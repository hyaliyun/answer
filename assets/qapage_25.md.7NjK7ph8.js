import{_ as d,o as a,c as o,a as e,t as s,C as p,F as l,p as g,e as y,f as w,q as b}from"./chunks/framework.DulMeQy4.js";const v={name:"PoemCard",props:{poem:{type:Object,required:!0}}},k={class:"card"},T={class:"question"},S={class:"answer"};function A(h,t,n,c,u,i){return a(),o("div",k,[e("div",T,s(n.poem.input),1),t[0]||(t[0]=e("div",{class:"separator"},null,-1)),e("div",S,s(n.poem.output)+"🚨"+s(n.poem.context),1)])}const C=d(v,[["render",A],["__scopeId","data-v-9f9f9217"]]),M=JSON.parse(`[{"output":"In 1898","context":"with the expansion of the rifle company to three platoons under mobilization for the SpanishÿAmerican War","input":"the company gained two officers (an additional first lieutenant as executive officer and an additional second lieutenant to command the third platoon). Additionally","field4":"there was an increase in the number of noncommissioned officers (NCOs) to serve as section leaders (sergeants) and squad leaders (corporals) to the point that in 1901 with the increase in privates to 127 (from 84 in 1898) that there were then authorized 18 corporals and six sergeants","field5":"along with two buglers (the bugle having replaced both the drum and the fife in infantry companies)","field6":"the wagoner","field7":"two senior NCOs (first sergeant and quartermaster sergeant)","field8":"and five officers for a total of 161 officers and men. In 1905 a mess sergeant was added to the company's senior NCO staff and the company quartermaster sergeant was renamed supply sergeant."},{"output":"Due to mobilization for World War I","context":"the army adopted its \\"\\"square division\\"\\" organization structure","input":"significantly increasing unit sizes from platoon up. In 1917 a fourth platoon was added to the company","field4":"increasing its size to 256 officers and men","field5":"including six officers (a captain as commanding officer","field6":"a first lieutenant as executive officer","field7":"and two first lieutenants and two second lieutenants as platoon commanders). Enlisted strength became: three senior NCOs (first sergeant","field8":"supply sergeant","field9":"and mess sergeant)","field10":"12 sergeants","field11":"33 corporals (one company clerk and 32 squad leaders with eight per platoon)","field12":"eight specialists (four cooks and four mechanics)","field13":"two buglers","field14":"64 privates first class","field15":"and 128 privates. Of the 12 sergeants","field16":"while eight of them continued to serve as section leaders (with two in each platoon)","field17":"the four senior ranking sergeants were assigned to a new position in each platoon headquarters as \\"\\"assistant to platoon commander\\"\\". This was the forerunner of the modern platoon sergeant slot created in 1943 (originally known in 1940 as the \\"\\"platoon leader\\"\\"","field18":"as the officer was styled as the \\"\\"platoon commander\\"\\" until 1943) to provide an experienced senior NCO as an advisor and second-in-command to the officer commanding the platoon."},{"output":"Cavalry companies (not officially re-designated as troops until 1883) had a similar organization to the infantry","context":"but with fewer men","input":"companies rarely exceeding around 70 men. In the Field Artillery","field4":"the company-equivalent unit is designated as a battery and historically consisted of a battery headquarters and two or three gun platoons","field5":"each with two gun sections. At full authorized strength","field6":"a typical battery of six gun sections would consist of approximately 100 officers and enlisted men."},{"output":"In the United States Army","context":"infantry companies are usually made up of three rifle platoons and a heavy weapons platoon; mechanized infantry companies are usually made up of three rifle platoons consisting of four infantry fighting vehicles (IFV) each and a command element containing two IFVs; tank companies are usually made up of three tank platoons consisting of four tanks each and a command element containing two tanks; support companies are typically divided into platoons of specialization that may contain additional special sections. A company is usually commanded by a captain","input":"although in some cases they may be commanded by a first lieutenant or a major. Unlike its component platoons","field4":"a company typically has additional positions of supporting staff","field5":"such as an executive officer (XO)","field6":"a first sergeant","field7":"a readiness/training NCO","field8":"and other positions (e.g.","field9":"supply sergeant","field10":"armorer). The corresponding unit of artillery is always called a battery. Similarly","field11":"the term troop is used for cavalry units","field12":"including both the horse-mounted units of history as well as modern armored cavalry and air cavalry units."},{"output":"Companies that are not separate from their parent battalion are identified by letterfor example","context":"\\"\\"Company A","input":"1st Battalion","field4":"15th Infantry Regiment\\"\\". This would commonly be abbreviated as \\"\\"A/1-15 INF\\"\\" in writing","field5":"but not in speaking. The dash in \\"\\"1-15\\"\\" indicates that the unit's history stems from the 15th Infantry Regiment of the Army","field6":"in its lineage. Companies normally do not have their own overhead","field7":"but share the overhead of the parent organization. When the regimental headquarters exists as a separate echelon of command (e.g.","field8":"the 75th Ranger Regiment","field9":"the 11th Armored Cavalry Regiment","field10":"and the 1st Marine Regiment)","field11":"as virtually all U.S. Army regiments did until after the Korean War","field12":"a slash separates the battalion/squadron number from the regimental number (i.e.","field13":"B/2/75 Ranger","field14":"C/3/11 ACR","field15":"E/2/1 Marines)."},{"output":"Although not official designations","context":"the letters are often pronounced in \\"\\"GI slang\\"\\" using the NATO phonetic alphabet or","input":"before that","field4":"the Joint Army/Navy Phonetic Alphabet","field5":"resulting in names such as \\"\\"Bravo Company\\"\\" and \\"\\"Echo Company\\"\\" (formerly \\"\\"Baker\\"\\" and \\"\\"Easy\\"\\" companies","field6":"respectively). Companies with a separate table of organization and equipment (TO&E) are identified by a number","field7":"and are able to operate completely independently from any other unit's support. Company-sized units that are organized under a table of distribution and allowance (TDA) are identified with a name or number."},{"output":"Company-sized units usually consist of four to six platoons each led by a lieutenant","context":"although there are examples of combat service and combat service support companies that have seven or more platoons. For example","input":"a transportation terminal service company normally has two ship platoons","field4":"two shore platoons","field5":"one documentation platoon","field6":"one maintenance platoon","field7":"and the headquarters platoon."},{"output":"While companies are typically commanded by captains","context":"some have a special operational capacity that requires them to be commanded by an officer with greater command authority and experience; such companies are commanded by majors","input":"and have platoons commanded by captains. Examples of this arrangement include aviation platoons","field4":"military intelligence companies","field5":"military police companies","field6":"and special forces companies. A captain reports to his commander","field7":"usually the battalion commander (a lieutenant colonel). However","field8":"there are some administrative and other duties at battalion level and larger (brigade or division) that are also handled by captains","field9":"for example the S-1","field10":"S-2","field11":"& S-4 officers of a battalion (S-3 is a major)","field12":"or some assistant staff positions in the G shops at division."},{"output":"The senior non-commissioned officer of a company is called a first sergeant. Any sergeant holding this position is referred to as \\"\\"first sergeant\\"\\" regardless of actual rank","context":"though the non-commissioned officer assigned ordinarily has the rank of first sergeant. A master sergeant assigned to this position will be \\"\\"laterally promoted\\"\\" to the rank of first sergeant","input":"unless the appointment is temporary. In some instances","field4":"a sergeant first class will be appointed to the job in lieu of a rank-qualified first sergeant or master sergeant. Again","field5":"in such situations","field6":"the NCO holds the duty position and title of \\"\\"First Sergeant\\"\\"","field7":"while retaining the rank of sergeant first class."},{"output":"A weapons company has in place of the three rifle platoons","context":"an 81?mm mortar platoon","input":"an anti-armor platoon","field4":"and a heavy machine gun platoon."},{"output":"USMC tank and LAR companies are organized similarly to US Army tank and mechanized infantry companies","context":"with the three line platoons consisting of four tanks or LAVs each","input":"and the company command element containing two tanks or LAVs."},{"output":"AAV companies have three platoons containing four sections of three AAVs each","context":"for a total of 12 AAVs per platoon","input":"and a headquarters section of three AAVs. The company also includes both command and recovery variants of the AAV","field4":"giving the company a grand total of approximately 42-45 AAVs."},{"output":"Some companies were well enough known that they have been identified with their company letter. Examples include:\\"","context":"How many soldiers in a company in india?"},{"output":"A clutch of one to three eggs is laid in March or April, depending upon latitude","context":"Buteo borealis\\r\\nButeo broealis (lapsus)\\r\\nFalco borealis Gmelin\\r\\nFalco harlani Audubon\\r\\nThe red-tailed hawk (Buteo jamaicensis) is a bird of prey, one of three species colloquially known in the United States as the \\"chickenhawk,\\" though it rarely preys on standard-sized chickens.[2] It breeds throughout most of North America, from western Alaska and northern Canada to as far south as Panama and the West Indies, and is one of the most common buteos in North America. Red-tailed hawks can acclimate to all the biomes within their range. The 14 recognized subspecies vary in appearance and range. It is one of the largest members of the genus Buteo in North America, typically weighing from 690 to 1,600?g (1.5 to 3.5?lb) and measuring 45ÿ65?cm (18ÿ26?in) in length, with a wingspan from 110ÿ145?cm (43ÿ57?in). The red-tailed hawk displays sexual dimorphism in size, with females averaging about 25% heavier than males.[3] The bird is sometimes referred to as the red-tail for short, when the meaning is clear in context.\\r\\nThe subspecies Harlan's hawk (B. j. harlani) is sometimes considered a separate species (B. harlani).[4]\\r\\nThe red-tailed hawk occupies a wide range of habitats and altitudes including deserts, grasslands, coniferous and deciduous forests, agricultural fields and urban areas. It lives throughout the North American continent, except in areas of unbroken forest or the high arctic. It is legally protected in Canada, Mexico and the United States by the Migratory Bird Treaty Act.\\r\\nBecause they are so common and easily trained as capable hunters, the majority of hawks captured for falconry in the United States are red-tails. Falconers are permitted to take only passage hawks (which have left the nest, are on their own, but are less than a year old) so as to not affect the breeding population. Adults, which may be breeding or rearing chicks, may not be taken for falconry purposes and it is illegal to do so. Passage red-tailed hawks are also preferred by falconers because these younger birds have not yet developed the adult behaviors which would make them more difficult to train.\\r\\n\\r\\n\\r\\nAs is the case with many raptors, the red-tailed hawk displays sexual dimorphism in size, as females are up to 25% larger than males.[5] As is typical in large raptors, frequently reported mean body mass for Red-tailed Hawks are somewhat higher than expansive research reveals.[6] Part of this weight is highly seasonally variable and due to clinal variation, male red-tailed hawks may weigh from 690 to 1,300?g (1.52 to 2.87?lb) and in females between 900 and 2,000?g (2.0 and 4.4?lb). However, research from nine studies occurring at migration sites in the United States and two breeding studies, one from the smallest race in Puerto Rico, the other from larger races in Wisconsin show that males weigh a mean of 837?g (1.845?lb) and females weigh a mean of 1,040.7?g (2.294?lb), about 15% lighter than prior species-wide published weights.[6][7][8] The heaviest surveyed weights came from migrants in Cape May, New Jersey, where females weighed a mean of 1,278?g (2.818?lb), males a mean of 990.8?g (2.184?lb).[7] The lightest were from the breeding population in forest openings of Puerto Rico, where the females and males weighed an average of 1,023?g (2.255?lb) and 795?g (1.753?lb), respectively, also the highest size sexual dimorphism in the species. Size variation in body mass reveals that the red-tailed hawks typically varies only a modest amount, racial variation in average weights of great horned owls show that mean body mass is nearly twice (the heaviest race is about 36% heavier than the lightest known race on average) as variable as that of the hawk (where the heaviest race is only just over 18% heavier on average than the lightest).[7][8][9] Males can reportedly measure 45 to 60?cm (18 to 24?in) in total length, females measuring 48 to 65?cm (19 to 26?in) long. The wingspan can range from 105 to 141?cm (41 to 56?in) and, in the standard scientific method of measuring wing size, the wing chord is 325.1ÿ444.5?mm (12.80ÿ17.50?in) long. The tail measures 188 to 258.7?mm (7.40 to 10.19?in) in length.[10][11] The exposed culmen was reported to range from 21.7 to 30.2?mm (0.85 to 1.19?in) and the tarsus averaged 74.7ÿ95.8?mm (2.94ÿ3.77?in).[6][7][12] The middle toe (excluding talon) can range from 38.3 to 53.8?mm (1.51 to 2.12?in), with the hallux-claw (the talon of the rear toe, which has evolved to be the largest in accipitrids) measuring from 24.1 to 33.6?mm (0.95 to 1.32?in) in length.[6][7]\\r\\nRed-tailed hawk plumage can be variable, depending on the subspecies and the region. These color variations are morphs, and are not related to molting. The western North American population, B. j. calurus, is the most variable subspecies and has three color morphs: light, dark, and intermediate or rufus. The dark and intermediate morphs constitute 10ÿ20% of the population.[13]\\r\\nThough the markings and hue vary across the subspecies, the basic appearance of the red-tailed hawk is consistent. Overall, this species is blocky and broad in shape, often appearing (and being) heavier than other Buteos of similar length.[10] A whitish underbelly with a dark brown band across the belly, formed by horizontal streaks in feather patterning, is present in most color variations. Especially in younger birds, the underside may be otherwise covered with dark brown spotting. The red tail, which gives this species its name, is uniformly brick-red above and light buff-orange below.[10][14] The bill is short and dark, in the hooked shape characteristic of raptors, and the head can sometimes appear small in size against the thick body frame.[10] They have a relatively short, broad tails and thick, chunky wings.[14] The cere, the legs, and the feet of the red-tailed hawk are all yellow.[5]\\r\\nImmature birds can be readily identified at close range by their yellowish irises. As the bird attains full maturity over the course of 3ÿ4 years, the iris slowly darkens into a reddish-brown hue. In both the light and dark morphs, the tail of the immature red-tailed hawk is patterned with numerous darker bars.[14]\\r\\nThe red-tailed hawk is a member of the genus Buteo, a group of medium-sized raptors with robust bodies and broad wings. Members of this genus are known as buzzards in Europe, but hawks in North America.[15]\\r\\nThere are at least 14 recognized subspecies of Buteo jamaicensis, which vary in range and in coloration:\\r\\nThe four island forms, jamaicensis, solitudinus, socorroensis, and fumosus, do not overlap in range with any other subspecies.\\r\\nThe red-tailed hawk is one of the most widely scattered hawks in the Americas. It breeds from central Alaska, the Yukon, and the Northwest Territories east to southern Quebec and the Maritime Provinces of Canada, and south to Florida, the West Indies, and Central America. The winter range stretches from southern Canada south throughout the remainder of the breeding range.[19]\\r\\nIts preferred habitat is mixed forest and field, with high bluffs or trees that may be used as perch sites. It occupies a wide range of habitats and altitudes, including deserts, grasslands, coastal regions, mountains, foothills, coniferous and deciduous woodlands, tropical rainforests, agricultural fields and urban areas.[1] It is second only to the peregrine falcon in the use of diverse habitats in North America.[22] It lives throughout the North American continent, except in areas of unbroken forest or the high Arctic.[13]\\r\\nAdult hawks have few natural predators, although their eggs and chicks are preyed on by a variety of organisms. The red-tailed hawk is widespread in North America,[22] partially due to historic settlement patterns, which have benefited it. The clearing of forests in the Northeast created hunting areas, while the preservation of woodlots left the species with viable nest sites. The increase in trees throughout the Great Plains during the past century due to fire suppression and tree planting facilitated the western range expansion of the red-tailed hawk[23] as well as range expansions of many other species of birds.[24][25][26] The construction of highways with utility poles alongside treeless medians provided perfect habitat for perch-hunting. Unlike some other raptors, the red-tailed hawk are seemingly unfazed by considerable human activity and can nest and live in close proximity to large numbers of humans.[10] Thus, the species can also be found in cities, where common prey such as rock pigeons and brown rats may support their populations.[27] One famous urban red-tailed hawk, known as \\"Pale Male\\", became the subject of a non-fiction book, Red-Tails in Love: A Wildlife Drama in Central Park, and is the first known red-tail in decades to successfully nest and raise young in the crowded New York City borough of Manhattan.[28][29][30]\\r\\nHawks in urban areas are threatened by the use of rat traps and poisoned bait to kill rodents. This generally consists of warfarin cookies which induce internal bleeding in rats and mice, and a hawk that ingests rodents who have consumed rat poison can itself be affected.[31]\\r\\nIn flight, this hawk soars with wings often in a slight dihedral, flapping as little as possible to conserve energy. Active flight is slow and deliberate, with deep wing beats. In wind, it occasionally hovers on beating wings and remains stationary above the ground.[13] When soaring or flapping its wings, it typically travels from 32 to 64?km/h (40?mph), but when diving may exceed 190?km/h (120?mph).[32]\\r\\nThe cry of the red-tailed hawk is a two to three second hoarse, rasping scream, described as kree-eee-ar,[27] that begins at a high pitch and slurs downward.[32] This cry is often described as sounding similar to a steam whistle.[5] The red-tailed hawk frequently vocalizes while hunting or soaring, but vocalizes loudest in annoyance or anger, in response to a predator or a rival hawk's intrusion into its territory.[27] At close range, it makes a croaking \\"guh-runk\\".[33] Young hawks may utter a wailing klee-uk food cry when parents leave the nest.[34] The fierce, screaming cry of the red-tailed hawk is frequently used as a generic raptor sound effect in television shows and other media, even if the bird featured is not a red-tailed hawk.[35][36]\\r\\nThe red-tailed hawk is carnivorous, and an opportunistic feeder. Their most common prey are small mammals such as rodents and lagomorphs, but they will also consume birds, fish, reptiles, and amphibians. Prey varies with regional and seasonal availability, but usually centers on rodents, comprising up to 85% of a hawk's diet.[5] Most commonly reported prey types include mice, including both native Peromyscus species and house mice; gophers, voles, chipmunks, ground squirrels and tree squirrels.[37][38] Additional prey (listed by descending likelihood of predation) include lagomorphs, shrews, bats,[39] pigeons, quail, corvids, waterfowl, other raptors, reptiles, fish, crustaceans, insects and earthworms.[10] Where found in Caribbean islands, red-tailed hawks prey mostly on reptiles such as snakes and lizards, since these are perhaps the most predominant native land animals of that region.[10] Prey specimens can range to as small a size as beetles and worms. However, they can also prey on marmots, white-tailed jackrabbits, or female wild turkey, all of which are at least easily double the weight of most red-tails.[10] Hawks will eat carrion if need be, although it is not a preferred food source. During winter in captivity, an average red-tail will eat about 135?g (4.8?oz) daily.[34]\\r\\nThe red-tailed hawk commonly employs one of two hunting techniques. Often, they scan for prey activity from an elevated perch site, swooping down from the perch to seize the prey. They also watch for prey while flying, either capturing a bird in flight or pursuing prey on the ground until they can pin them down in their talons.[10] Red-tailed hawks, like some other raptors, have been observed to hunt in pairs. This may consist of stalking opposites sides of a tree, in order to surround a tree squirrel and almost inevitably drive the rodent to be captured by one after being flushed by the other hawk.[40] They are opportunistically attracted to conspicuous meals, such as displaying male red-winged blackbirds.[5]\\r\\nThe great horned owl occupies a similar ecological niche nocturnally to the red-tail, taking similar prey. Competition may occur between the hawk and owl species during twilight, although the differing nesting season and activity times usually results in a lack of direct competition. Although the red-tail's prey is on average larger (due in part to the scarcity of diurnal squirrels in the owl's diet),[38] the owl is an occasional predator of red-tailed hawks themselves, of any age, while the hawks are not known to predate adult great horned owls.[37] Other competitors include other large Buteo species such as Swainson's hawks and rough-legged hawks, as well as the northern goshawk, since prey and foraging methods of these species occasionally overlap.[41][42] Hawks have been observed following American badgers to capture prey they flush and the two are considered potential competitors.[43] Competition over carcasses may occur with American crows, and several crows working together can displace a hawk.[44] Larger raptors, such as eagles and ferruginous hawks, may steal hawk kills.[5]\\r\\nThe red-tailed hawk reaches sexual maturity at two years of age. It is monogamous, mating with the same individual for many years. In general, the red-tailed hawk will only take a new mate when its original mate dies.[45] The same nesting territory may be defended by the pair for years. During courtship, the male and female fly in wide circles while uttering shrill cries. The male performs aerial displays, diving steeply, and then climbing again. After repeating this display several times, he sometimes grasps her talons briefly with his own. Courtship flights can last 10 minutes or more. Copulation often follows courtship flight sequences, although copulation frequently occurs in the absence of courtship flights.\\r\\nIn copulation, the female, when perched, tilts forward, allowing the male to land with his feet lodged on her horizontal back. The female twists and moves her tail feathers to one side, while the mounted male twists his cloacal opening around the female's cloaca. Copulation lasts 5 to 10 seconds and during pre-nesting courtship in late winter or early spring can occur numerous times each day.[46]\\r\\nIn the same period, the pair constructs a stick nest in a large tree 4 to 21?m (13 to 69?ft) off the ground or on a cliff ledge 35?m (115?ft) or higher above the ground, or may nest on man-made structures. The nest is generally 71 to 97?cm (28 to 38?in) in diameter and can be up to 90?cm (3.0?ft) tall. The nest is constructed of twigs, and lined with bark, pine needles, corn cobs, husks, stalks, aspen catkins, or other plant lining matter.\\r\\nGreat horned owls compete with the red-tailed hawk for nest sites. Each species has been known to kill the young and destroy the eggs of the other, but in general, both species nest in adjacent or confluent territories without conflict. Great horned owls are incapable of constructing nests and typically expropriate existing red-tail nests. Great horned owls begin nesting behaviors much earlier than red-tails, often as early as December. Red-tails are therefore adapted to constructing new nests when a previous year's nest has been overtaken by owls or otherwise lost. New nests are typically within a kilometer or less of the previous nest. Often, a new nest is only a few hundred meters or less from a previous one. Being a large predator, most predation of these hawks occurs with eggs and nestlings, which are taken by owls, corvids and raccoons.[47]\\r\\nA clutch of one to three eggs is laid in March or April, depending upon latitude. Clutch size depends almost exclusively on the availability of prey for the adults. Eggs are laid approximately every other day. The eggs are usually about 60?mm G?47?mm (2.4?in G?1.9?in). They are incubated primarily by female, with the male substituting when the female leaves to hunt or merely stretch her wings. The male brings most food to the female while she incubates. After 28 to 35 days, the eggs hatch over 2 to 4 days; the nestlings are altricial at hatching. The female broods them while the male provides most of the food to the female and the young, which are known as eyasses (pronounced \\"EYE-ess-ez\\"). The female feeds the eyasses after tearing the food into small pieces. After 42 to 46 days, the eyasses begin to leave the nest. The fledging period follows, with short flights engaged in, after another 3 weeks. About 6 to 7 weeks after fledging, the young begin to capture their own prey. Shortly thereafter, when the young are around 4 months of age, they become independent of their parents. However, the hawks do not generally reach breeding maturity until they are around 3 years of age. In the wild, red-tailed hawks have lived for at least 25 years, for example, Pale Male was born in 1990, and in Spring 2014 is still raising eyasses. The oldest captive hawk of this species was at least 29 and a half years of age.[5]\\r\\nThe red-tailed hawk is a popular bird in falconry, particularly in the United States where the sport of falconry is tightly regulated and where red-tailed hawks are both widely available and allowed to novice falconers. Red-tailed hawks are highly tameable and trainable, with a more social disposition than all other falcons or hawks other than the Harris's hawk.[48] They are also long lived and highly disease resistant, allowing a falconer to maintain a red-tailed hawk as a hunting companion for many years. There are fewer than 5,000 falconers in the United States, so despite their popularity any effect on the red-tailed hawk population, estimated to be about one million in the United States, is negligible.[49]\\r\\nNot being as swift as falcons or accipiters, red-tailed hawks are not the most effective of bird hawks and are usually used against ground game such as rabbits and squirrels. However, some individuals may learn to ambush birds on the ground with a swift surprise approach and capture them before they can accelerate to full speed and escape. Some have even learned to use a falcon-like diving stoop to capture challenging game such as pheasants. In the course of a typical hunt, a falconer using a red-tailed hawk most commonly releases the hawk and allows it to perch in a tree or other high vantage point. The falconer, who may be aided by a dog, then attempts to flush prey by stirring up ground cover. A well-trained red-tailed hawk will follow the falconer and dog, realizing that their activities produce opportunities to catch game. Once a raptor catches game, it does not bring it back to the falconer. Instead, the falconer must locate the bird and its captured prey, \\"make in\\" (carefully approach) and trade the bird its kill in exchange for a piece of offered meat.[50]\\r\\nThe feathers and other parts of the red-tailed hawk are considered sacred to many American indigenous people and, like the feathers of the bald eagle and golden eagle, are sometimes used in religious ceremonies and found adorning the regalia of many Native Americans in the United States; these parts, most especially their distinctive tail feathers, are a popular item in the Native American community.[51] As with the other two species, the feathers and parts of the red-tailed hawk are regulated by the eagle feather law,[52] which governs the possession of feathers and parts of migratory birds.[53]","input":"How many chicks do red tail hawks have?"},{"output":"Extrusive igneous rocks, also known as volcanic rocks","context":"Igneous rock (derived from the Latin word ignis meaning fire), or magmatic rock, is one of the three main rock types, the others being sedimentary and metamorphic. Igneous rock is formed through the cooling and solidification of magma or lava. The magma can be derived from partial melts of existing rocks in either a planet's mantle or crust. Typically, the melting is caused by one or more of three processes: an increase in temperature, a decrease in pressure, or a change in composition. Solidification into rock occurs either below the surface as intrusive rocks or on the surface as extrusive rocks. Igneous rock may form with crystallization to form granular, crystalline rocks, or without crystallization to form natural glasses.\\r\\n\\r\\n\\r\\nIgneous and metamorphic rocks make up 90ÿ95% of the top 16?km of the Earth's crust by volume.[1] Igneous rocks form about 15% of the Earth's current land surface.[note 1] Most of the Earth's oceanic crust is made of igneous rock.\\r\\nIgneous rocks are also geologically important because:\\r\\nIn terms of modes of occurrence, igneous rocks can be either intrusive (plutonic and hypabyssal) or extrusive (volcanic).\\r\\nIntrusive igneous rocks are formed from magma that cools and solidifies within the crust of a planet, surrounded by pre-existing rock (called country rock); the magma cools slowly and, as a result, these rocks are coarse-grained. The mineral grains in such rocks can generally be identified with the naked eye. Intrusive rocks can also be classified according to the shape and size of the intrusive body and its relation to the other formations into which it intrudes. Typical intrusive formations are batholiths, stocks, laccoliths, sills and dikes. When the magma solidifies within the earth's crust, it cools slowly forming coarse textured rocks, such as granite, gabbro, or diorite.\\r\\nThe central cores of major mountain ranges consist of intrusive igneous rocks, usually granite. When exposed by erosion, these cores (called batholiths) may occupy huge areas of the Earth's surface.\\r\\nIntrusive igneous rocks that form at depth within the crust are termed plutonic (or abyssal) rocks and are usually coarse-grained. Intrusive igneous rocks that form near the surface are termed subvolcanic or hypabyssal rocks and they are usually medium-grained. Hypabyssal rocks are less common than plutonic or volcanic rocks and often form dikes, sills, laccoliths, lopoliths, or phacoliths.\\r\\nExtrusive igneous rocks, also known as volcanic rocks, are formed at the crust's surface as a result of the partial melting of rocks within the mantle and crust. Extrusive igneous rocks cool and solidify quicker than intrusive igneous rocks. They are formed by the cooling of molten magma on the earth's surface. The magma, which is brought to the surface through fissures or volcanic eruptions, solidifies at a faster rate. Hence such rocks are smooth, crystalline and fine-grained. Basalt is a common extrusive igneous rock and forms lava flows, lava sheets and lava plateaus. Some kinds of basalt solidify to form long polygonal columns. The Giant's Causeway in Antrim, Northern Ireland is an example.\\r\\nThe molten rock, with or without suspended crystals and gas bubbles, is called magma. It rises because it is less dense than the rock from which it was created. When magma reaches the surface from beneath water or air, it is called lava. Eruptions of volcanoes into air are termed subaerial, whereas those occurring underneath the ocean are termed submarine. Black smokers and mid-ocean ridge basalt are examples of submarine volcanic activity.\\r\\nThe volume of extrusive rock erupted annually by volcanoes varies with plate tectonic setting. Extrusive rock is produced in the following proportions:[3]\\r\\nMagma that erupts from a volcano behaves according to its viscosity, determined by temperature, composition, crystal content and the amount of silica. High-temperature magma, most of which is basaltic in composition, behaves in a manner similar to thick oil and, as it cools, treacle. Long, thin basalt flows with pahoehoe surfaces are common. Intermediate composition magma, such as andesite, tends to form cinder cones of intermingled ash, tuff and lava, and may have a viscosity similar to thick, cold molasses or even rubber when erupted. Felsic magma, such as rhyolite, is usually erupted at low temperature and is up to 10,000 times as viscous as basalt. Volcanoes with rhyolitic magma commonly erupt explosively, and rhyolitic lava flows are typically of limited extent and have steep margins, because the magma is so viscous.\\r\\nFelsic and intermediate magmas that erupt often do so violently, with explosions driven by the release of dissolved gasestypically water vapour, but also carbon dioxide. Explosively erupted pyroclastic material is called tephra and includes tuff, agglomerate and ignimbrite. Fine volcanic ash is also erupted and forms ash tuff deposits, which can often cover vast areas.\\r\\nBecause lava usually cools and crystallizes rapidly, it is usually fine-grained. If the cooling has been so rapid as to prevent the formation of even small crystals after extrusion, the resulting rock may be mostly glass (such as the rock obsidian). If the cooling of the lava happened more slowly, the rock would be coarse-grained.\\r\\nBecause the minerals are mostly fine-grained, it is much more difficult to distinguish between the different types of extrusive igneous rocks than between different types of intrusive igneous rocks. Generally, the mineral constituents of fine-grained extrusive igneous rocks can only be determined by examination of thin sections of the rock under a microscope, so only an approximate classification can usually be made in the field.\\r\\nIgneous rocks are classified according to mode of occurrence, texture, mineralogy, chemical composition, and the geometry of the igneous body.\\r\\nThe classification of the many types of different igneous rocks can provide us with important information about the conditions under which they formed. Two important variables used for the classification of igneous rocks are particle size, which largely depends on the cooling history, and the mineral composition of the rock. Feldspars, quartz or feldspathoids, olivines, pyroxenes, amphiboles, and micas are all important minerals in the formation of almost all igneous rocks, and they are basic to the classification of these rocks. All other minerals present are regarded as nonessential in almost all igneous rocks and are called accessory minerals. Types of igneous rocks with other essential minerals are very rare, and these rare rocks include those with essential carbonates.\\r\\nIn a simplified classification, igneous rock types are separated on the basis of the type of feldspar present, the presence or absence of quartz, and in rocks with no feldspar or quartz, the type of iron or magnesium minerals present. Rocks containing quartz (silica in composition) are silica-oversaturated. Rocks with feldspathoids are silica-undersaturated, because feldspathoids cannot coexist in a stable association with quartz.\\r\\nIgneous rocks that have crystals large enough to be seen by the naked eye are called phaneritic; those with crystals too small to be seen are called aphanitic. Generally speaking, phaneritic implies an intrusive origin; aphanitic an extrusive one.\\r\\nAn igneous rock with larger, clearly discernible crystals embedded in a finer-grained matrix is termed porphyry. Porphyritic texture develops when some of the crystals grow to considerable size before the main mass of the magma crystallizes as finer-grained, uniform material.\\r\\nIgneous rocks are classified on the basis of texture and composition. Texture refers to the size, shape, and arrangement of the mineral grains or crystals of which the rock is composed.\\r\\nTexture is an important criterion for the naming of volcanic rocks. The texture of volcanic rocks, including the size, shape, orientation, and distribution of mineral grains and the intergrain relationships, will determine whether the rock is termed a tuff, a pyroclastic lava or a simple lava.\\r\\nHowever, the texture is only a subordinate part of classifying volcanic rocks, as most often there needs to be chemical information gleaned from rocks with extremely fine-grained groundmass or from airfall tuffs, which may be formed from volcanic ash.\\r\\nTextural criteria are less critical in classifying intrusive rocks where the majority of minerals will be visible to the naked eye or at least using a hand lens, magnifying glass or microscope. Plutonic rocks also tend to be less texturally varied and less prone to gaining structural fabrics. Textural terms can be used to differentiate different intrusive phases of large plutons, for instance porphyritic margins to large intrusive bodies, porphyry stocks and subvolcanic dikes (apophyses). Mineralogical classification is most often used to classify plutonic rocks. Chemical classifications are preferred to classify volcanic rocks, with phenocryst species used as a prefix, e.g. \\"olivine-bearing picrite\\" or \\"orthoclase-phyric rhyolite\\".\\r\\nIgneous rocks can be classified according to chemical or mineralogical parameters.\\r\\nChemical: total alkali-silica content (TAS diagram) for volcanic rock classification used when modal or mineralogic data is unavailable:\\r\\nChemical classification also extends to differentiating rocks that are chemically similar according to the TAS diagram, for instance:\\r\\nAn idealized mineralogy (the normative mineralogy) can be calculated from the chemical composition, and the calculation is useful for rocks too fine-grained or too altered for identification of minerals that crystallized from the melt. For instance, normative quartz classifies a rock as silica-oversaturated; an example is rhyolite. In an older terminology, silica oversaturated rocks were called silicic or acidic where the SiO2 was greater than 66% and the family term quartzolite was applied to the most silicic. A normative feldspathoid classifies a rock as silica-undersaturated; an example is nephelinite.\\r\\nIn 1902, a group of American petrographers proposed that all existing classifications of igneous rocks should be discarded and replaced by a \\"quantitative\\" classification based on chemical analysis. They showed how vague, and often unscientific, much of the existing terminology was and argued that as the chemical composition of an igneous rock was its most fundamental characteristic, it should be elevated to prime position.\\r\\nGeological occurrence, structure, mineralogical constitutionthe hitherto accepted criteria for the discrimination of rock specieswere relegated to the background. The completed rock analysis is first to be interpreted in terms of the rock-forming minerals which might be expected to be formed when the magma crystallizes, e.g., quartz feldspars, olivine, akermannite, Feldspathoids, magnetite, corundum, and so on, and the rocks are divided into groups strictly according to the relative proportion of these minerals to one another.[5][6]\\r\\nFor volcanic rocks, mineralogy is important in classifying and naming lavas. The most important criterion is the phenocryst species, followed by the groundmass mineralogy. Often, where the groundmass is aphanitic, chemical classification must be used to properly identify a volcanic rock.\\r\\nMineralogic contents ÿ felsic versus mafic\\r\\nFor intrusive, plutonic and usually phaneritic igneous rocks (where all minerals are visible at least via microscope), the mineralogy is used to classify the rock. This usually occurs on ternary diagrams, where the relative proportions of three minerals are used to classify the rock.\\r\\nThe following table is a simple subdivision of igneous rocks according to both their composition and mode of occurrence.\\r\\nFor a more detailed classification see QAPF diagram.\\r\\nGranite is an igneous intrusive rock (crystallized at depth), with felsic composition (rich in silica and predominately quartz plus potassium-rich feldspar plus sodium-rich plagioclase) and phaneritic, subeuhedral texture (minerals are visible to the unaided eye and commonly some of them retain original crystallographic shapes).\\r\\nThe Earth's crust averages about 35 kilometers thick under the continents, but averages only some 7ÿ10 kilometers beneath the oceans. The continental crust is composed primarily of sedimentary rocks resting on a crystalline basement formed of a great variety of metamorphic and igneous rocks, including granulite and granite. Oceanic crust is composed primarily of basalt and gabbro. Both continental and oceanic crust rest on peridotite of the mantle.\\r\\nRocks may melt in response to a decrease in pressure, to a change in composition (such as an addition of water), to an increase in temperature, or to a combination of these processes.\\r\\nOther mechanisms, such as melting from a meteorite impact, are less important today, but impacts during the accretion of the Earth led to extensive melting, and the outer several hundred kilometers of our early Earth was probably an ocean of magma. Impacts of large meteorites in the last few hundred million years have been proposed as one mechanism responsible for the extensive basalt magmatism of several large igneous provinces.\\r\\nDecompression melting occurs because of a decrease in pressure.[7]\\r\\nThe solidus temperatures of most rocks (the temperatures below which they are completely solid) increase with increasing pressure in the absence of water. Peridotite at depth in the Earth's mantle may be hotter than its solidus temperature at some shallower level. If such rock rises during the convection of solid mantle, it will cool slightly as it expands in an adiabatic process, but the cooling is only about 0.3?C per kilometer. Experimental studies of appropriate peridotite samples document that the solidus temperatures increase by 3?C to 4?C per kilometer. If the rock rises far enough, it will begin to melt. Melt droplets can coalesce into larger volumes and be intruded upwards. This process of melting from the upward movement of solid mantle is critical in the evolution of the Earth.\\r\\nDecompression melting creates the ocean crust at mid-ocean ridges. It also causes volcanism in intraplate regions, such as Europe, Africa and the Pacific sea floor. There, it is variously attributed either to the rise of mantle plumes (the \\"Plume hypothesis\\") or to intraplate extension (the \\"Plate hypothesis\\").[8]\\r\\nThe change of rock composition most responsible for the creation of magma is the addition of water. Water lowers the solidus temperature of rocks at a given pressure. For example, at a depth of about 100 kilometers, peridotite begins to melt near 800?C in the presence of excess water, but near or above about 1,500?C in the absence of water.[9] Water is driven out of the oceanic lithosphere in subduction zones, and it causes melting in the overlying mantle. Hydrous magmas composed of basalt and andesite are produced directly and indirectly as results of dehydration during the subduction process. Such magmas, and those derived from them, build up island arcs such as those in the Pacific Ring of Fire. These magmas form rocks of the calc-alkaline series, an important part of the continental crust.\\r\\nThe addition of carbon dioxide is relatively a much less important cause of magma formation than the addition of water, but genesis of some silica-undersaturated magmas has been attributed to the dominance of carbon dioxide over water in their mantle source regions. In the presence of carbon dioxide, experiments document that the peridotite solidus temperature decreases by about 200?C in a narrow pressure interval at pressures corresponding to a depth of about 70?km. At greater depths, carbon dioxide can have more effect: at depths to about 200?km, the temperatures of initial melting of a carbonated peridotite composition were determined to be 450?C to 600?C lower than for the same composition with no carbon dioxide.[10] Magmas of rock types such as nephelinite, carbonatite, and kimberlite are among those that may be generated following an influx of carbon dioxide into mantle at depths greater than about 70?km.\\r\\nIncrease in temperature is the most typical mechanism for formation of magma within continental crust. Such temperature increases can occur because of the upward intrusion of magma from the mantle. Temperatures can also exceed the solidus of a crustal rock in continental crust thickened by compression at a plate boundary. The plate boundary between the Indian and Asian continental masses provides a well-studied example, as the Tibetan Plateau just north of the boundary has crust about 80 kilometers thick, roughly twice the thickness of normal continental crust. Studies of electrical resistivity deduced from magnetotelluric data have detected a layer that appears to contain silicate melt and that stretches for at least 1,000 kilometers within the middle crust along the southern margin of the Tibetan Plateau.[11] Granite and rhyolite are types of igneous rock commonly interpreted as products of the melting of continental crust because of increases in temperature. Temperature increases also may contribute to the melting of lithosphere dragged down in a subduction zone.\\r\\nMost magmas only entirely melt for small parts of their histories. More typically, they are mixes of melt and crystals, and sometimes also of gas bubbles. Melt, crystals, and bubbles usually have different densities, and so they can separate as magmas evolve.\\r\\nAs magma cools, minerals typically crystallize from the melt at different temperatures (fractional crystallization). As minerals crystallize, the composition of the residual melt typically changes. If crystals separate from the melt, then the residual melt will differ in composition from the parent magma. For instance, a magma of gabbroic composition can produce a residual melt of granitic composition if early formed crystals are separated from the magma. Gabbro may have a liquidus temperature near 1,200?C, and the derivative granite-composition melt may have a liquidus temperature as low as about 700?C. Incompatible elements are concentrated in the last residues of magma during fractional crystallization and in the first melts produced during partial melting: either process can form the magma that crystallizes to pegmatite, a rock type commonly enriched in incompatible elements. Bowen's reaction series is important for understanding the idealised sequence of fractional crystallisation of a magma.\\r\\nMagma composition can be determined by processes other than partial melting and fractional crystallization. For instance, magmas commonly interact with rocks they intrude, both by melting those rocks and by reacting with them. Magmas of different compositions can mix with one another. In rare cases, melts can separate into two immiscible melts of contrasting compositions.\\r\\nThere are relatively few minerals that are important in the formation of common igneous rocks, because the magma from which the minerals crystallize is rich in only certain elements: silicon, oxygen, aluminium, sodium, potassium, calcium, iron, and magnesium. These are the elements that combine to form the silicate minerals, which account for over ninety percent of all igneous rocks. The chemistry of igneous rocks is expressed differently for major and minor elements and for trace elements. Contents of major and minor elements are conventionally expressed as weight percent oxides (e.g., 51% SiO2, and 1.50% TiO2). Abundances of trace elements are conventionally expressed as parts per million by weight (e.g., 420 ppm Ni, and 5.1 ppm Sm). The term \\"trace element\\" is typically used for elements present in most rocks at abundances less than 100 ppm or so, but some trace elements may be present in some rocks at abundances exceeding 1,000 ppm. The diversity of rock compositions has been defined by a huge mass of analytical dataover 230,000 rock analyses can be accessed on the web through a site sponsored by the U. S. National Science Foundation (see the External Link to EarthChem).\\r\\nThe word \\"igneous\\" is derived from the Latin ignis, meaning \\"of fire\\". Volcanic rocks are named after Vulcan, the Roman name for the god of fire. Intrusive rocks are also called \\"plutonic\\" rocks, named after Pluto, the Roman god of the underworld.\\r\\nVolcanic rocks:\\r\\nSubvolcanic rocks:\\r\\nPlutonic rocks:\\r\\nKomatiite, Picrite basalt\\r\\nKimberlite, Lamproite\\r\\nPeridotite\\r\\nBasalt\\r\\nDiabase (Dolerite)\\r\\nGabbro\\r\\nAndesite\\r\\nMicrodiorite\\r\\nDiorite\\r\\nDacite\\r\\nMicrogranodiorite\\r\\nGranodiorite\\r\\nRhyolite\\r\\nMicrogranite, Aplite\\r\\nGranite","input":"What igneous rock crystallizes at the earth's surface?"},{"output":"Nunavut","context":"Nunavut (/?nu?n??vu?t/;[8] French:?[nynavy(t)]; Inuktitut syllabics ???? [?nunavut]) is the newest, largest, and northernmost territory of Canada. It was separated officially from the Northwest Territories on April 1, 1999, via the Nunavut Act[9] and the Nunavut Land Claims Agreement Act,[10] though the boundaries had been contemplatively drawn in 1993. The creation of Nunavut resulted in the first major change to Canada's political map since the incorporation of the province of Newfoundland in 1949.\\r\\nNunavut comprises a major portion of Northern Canada, and most of the Canadian Arctic Archipelago. Its vast territory makes it the fifth-largest country subdivision in the world, as well as North America's second-largest (after Greenland). The capital Iqaluit (formerly \\"Frobisher Bay\\"), on Baffin Island in the east, was chosen by the 1995 capital plebiscite. Other major communities include the regional centres of Rankin Inlet and Cambridge Bay. Nunavut also includes Ellesmere Island to the far north, as well as the eastern and southern portions of Victoria Island in the west and Akimiski Island in James Bay far to the southeast of the rest of the territory. It is Canada's only geo-political region that is not connected to the rest of North America by highway.[11]\\r\\nNunavut is the largest in area and the second-least populous of Canada's provinces and territories. One of the world's most remote, sparsely settled regions, it has a population of 35,944,[1] mostly Inuit, spread over an area of just over 1,750,000?km2 (680,000?sq?mi), or slightly smaller than Mexico. Nunavut is also home to the world's northernmost permanently inhabited place, Alert.[12] A weather station farther down Ellesmere Island, Eureka, has the lowest average annual temperature of any Canadian weather station.[13]\\r\\n\\r\\n\\r\\nNunavut means \\"our land\\" in Inuktitut.[14]\\r\\nNunavut covers 1,877,787?km2 (725,018?sq?mi)[1] of land and 160,935?km2 (62,137?sq?mi)[15] of water in Northern Canada. The territory includes part of the mainland, most of the Arctic Archipelago, and all of the islands in Hudson Bay, James Bay, and Ungava Bay, including the Belcher Islands, which belonged to the Northwest Territories. This makes it the fifth-largest subnational entity (or administrative division) in the world. If Nunavut were a country, it would rank 15th in area.[16]\\r\\nNunavut has land borders with the Northwest Territories on several islands as well as the mainland, Manitoba to the south of the Nunavut mainland, Saskatchewan to the southwest (at a single four-corner point), and a small land border with Newfoundland and Labrador on Killiniq Island and with Ontario in two small locations in James Bay: the larger located west of Akimiski Island, and the smaller around the Albany River near Fafard Island. It also shares maritime borders with Greenland and the provinces of Quebec, Ontario, and Manitoba.\\r\\nNunavut's highest point is Barbeau Peak (2,616?m (8,583?ft)) on Ellesmere Island. The population density is 0.019?persons/km2 (0.05?persons/sq?mi), one of the lowest in the world. By comparison, Greenland has approximately the same area and nearly twice the population.[17]\\r\\nNunavut experiences a polar climate in most regions, owing to its high latitude and lower continental summertime influence than areas to the west. In more southerly continental areas very cold subarctic climates can be found, due to July being slightly milder than the required 10?C (50?F).\\r\\nThe region now known as Nunavut has supported a continuous indigenous population for approximately 4,000 years. Most historians identify the coast of Baffin Island with the Helluland described in Norse sagas, so it is possible that the inhabitants of the region had occasional contact with Norse sailors.\\r\\nIn September 2008, researchers reported on the evaluation of existing and newly excavated archaeological remains, including yarn spun from a hare, rats, tally sticks, a carved wooden face mask that depicts Caucasian features, and possible architectural material. The materials were collected in five seasons of excavation at Cape Tanfield. Scholars determined that these provide evidence of European traders and possibly settlers on Baffin Island, not later than 1000 CE (and thus older than or contemporaneous with L'Anse aux Meadows). They seem to indicate prolonged contact, possibly up to 1450. The origin of the Old World contact is unclear; the article states: \\"Dating of some yarn and other artifacts, presumed to be left by Vikings on Baffin Island, have produced an age that predates the Vikings by several hundred years. So [] you have to consider the possibility that as remote as it may seem, these finds may represent evidence of contact with Europeans prior to the Vikings' arrival in Greenland.\\"[19]\\r\\nThe written historical accounts of Nunavut begin in 1576, with an account by English explorer Martin Frobisher. While leading an expedition to find the Northwest Passage, Frobisher thought he had discovered gold ore around the body of water now known as Frobisher Bay on the coast of Baffin Island.[20] The ore turned out to be worthless, but Frobisher made the first recorded European contact with the Inuit. Other explorers in search of the elusive Northwest Passage followed in the 17th century, including Henry Hudson, William Baffin and Robert Bylot.\\r\\nCornwallis and Ellesmere Islands featured in the history of the Cold War in the 1950s. Concerned about the area's strategic geopolitical position, the federal government relocated Inuit from Nunavik (northern Quebec) to Resolute and Grise Fiord. In the unfamiliar and hostile conditions, they faced starvation[21] but were forced to stay.[22] Forty years later, the Royal Commission on Aboriginal Peoples issued a report titled The High Arctic Relocation: A Report on the 1953ÿ55 Relocation.[23] The government paid compensation to those affected and their descendents and on August 18, 2010 in Inukjuak, Nunavik, the Honourable John Duncan, PC, MP, previous Minister of Indian Affairs and Northern Development and Federal Interlocutor for Mtis and Non-Status Indians apologized on behalf of the Government of Canada for the relocation of Inuit to the High Arctic.[24][25]\\r\\nDiscussions on dividing the Northwest Territories along ethnic lines began in the 1950s, and legislation to do this was introduced in 1963. After its failure a federal commission recommended against such a measure.[26] In 1976, as part of the land claims negotiations between the Inuit Tapiriit Kanatami (then called the \\"Inuit Tapirisat of Canada\\") and the federal government, the parties discussed division of the Northwest Territories to provide a separate territory for the Inuit. On April 14, 1982, a plebiscite on division was held throughout the Northwest Territories. A majority of the residents voted in favour and the federal government gave a conditional agreement seven months later.[27]\\r\\nThe land claims agreement was completed in September 1992 and ratified by nearly 85% of the voters in Nunavut in a referendum. On July 9, 1993, the Nunavut Land Claims Agreement Act[10] and the Nunavut Act[9] were passed by the Canadian Parliament. The transition to establish Nunavut Territory was completed on April 1, 1999.[28] The creation of Nunavut has been followed by growth in the capital, Iqaluita modest increase from 5,200 in 2001 to 6,600 in 2011.\\r\\nAs of the 2016 Canada Census, the population of Nunavut was 35,944, a 12.7% increase from 2011.[1] In 2006, 24,640 people identified themselves as Inuit (83.6% of the total population), 100 as First Nations (0.3%), 130 Mtis (0.4%) and 4,410 as non-aboriginal (15.0%).[29]\\r\\nThe population growth rate of Nunavut has been well above the Canadian average for several decades, mostly due to birth rates significantly higher than the Canadian averagea trend that continues. Between 2011 and 2016, Nunavut had the highest population growth rate of any Canadian province or territory, at a rate of 12.7%.[1] The second-highest was Alberta, with a growth rate of 11.6%.\\r\\nAlong with the Inuit Language (Inuktitut and Inuinnaqtun) sometimes called Inuktut,[30] English and French are also official languages.[4][31]\\r\\nIn his 2000 commissioned report (Aajiiqatigiingniq Language of Instruction Research Paper) to the Nunavut Department of Education, Ian Martin of York University stated a \\"long-term threat to Inuit languages from English is found everywhere, and current school language policies and practices on language are contributing to that threat\\" if Nunavut schools follow the Northwest Territories model. He provided a 20-year language plan to create a \\"fully functional bilingual society, in Inuktitut and English\\" by 2020. The plan provides different models, including:\\r\\nOf the 29,025 responses to the census question concerning 'mother tongue', the most commonly reported languages were:\\r\\nAt the time of the census, only English and French were counted as official languages. Figures shown are for single-language responses and the percentage of total single-language responses.[33]\\r\\nIn the 2006 census it was reported that 2,305 people (7.9%) living in Nunavut had no knowledge of either official language of Canada (English or French).[34]\\r\\nThe largest denominations by number of adherents according to the 2001 census were the Anglican Church of Canada with 15,440 (58%); the Roman Catholic Church (Roman Catholic Diocese of Churchill-Hudson Bay) with 6,205 (23%); and Pentecostal with 1,175 (4%).[35] In total, 93% of the population were Christian.\\r\\nThe economy of Nunavut is Inuit and Territorial Government, mining, oil gas mineral exploration, arts crafts, hunting, fishing, whaling, tourism, transportation, education - Nunavut Arctic College, housing, military and research ÿ new Canadian High Arctic Research Station CHARS in planning for Cambridge Bay and high north Alert Bay Station. Iqaluit hosts the annual Nunavut Mining Symposium every April[36], this is a tradeshow that showcases many economic activities on going in Nunavut.\\r\\nThere are currently three major mines in operation in Nunavut.\\r\\nAgnico-Eagle Mines Ltd ÿ Meadowbank Division. Meadowbank is an open pit gold mine with an estimated mine life 2010ÿ2020 and employs 680 persons. The second recently opened mine in production is the Mary River Iron Ore mine operated by Baffinland Iron Mines. It is located close to Pond Inlet on North Baffin Island. They produce a high grade direct ship iron ore.\\r\\nThe most recent mine to open is Doris North or the Hope Bay Mine operated by TMAC Resource Ltd. This new high grade gold mine is the first in a series of potential mines in gold occurrences all along the Hope Bay greenstone belt.\\r\\nNunavut's people rely primarily on diesel fuel[42] to run generators and heat homes, with fossil fuel shipments from southern Canada by plane or boat because there are few to no roads or rail links to the region.[43] There is a government effort to use more renewable energy sources,[44] which is generally supported by the community.[45]\\r\\nThis support comes from Nunavut feeling the effects of global warming.[46][47] Former Nunavut Premier Eva Aariak said in 2011, \\"Climate change is very much upon us. It is affecting our hunters, the animals, the thinning of the ice is a big concern, as well as erosion from permafrost melting.\\"[43] The region is warming about twice as fast as the global average, according to the UN's Intergovernmental Panel on Climate Change.\\r\\n\\r\\nNunavut has a Commissioner appointed by the federal Minister of Indigenous and Northern Affairs. As in the other territories, the commissioner's role is symbolic and is analogous to that of a Lieutenant-Governor.[48] While the Commissioner is not formally a representative of Canada's head of state, a role roughly analogous to representing The Crown has accrued to the position.\\r\\nNunavut elects a single member of the House of Commons of Canada. This makes Nunavut the largest electoral district in the world by area.\\r\\nThe members of the unicameral Legislative Assembly of Nunavut are elected individually; there are no parties and the legislature is consensus-based.[49] The head of government, the premier of Nunavut, is elected by, and from the members of the legislative assembly. On June 14, 2018, Joe Savikataaq was elected as the Premier of Nunavut, after his predecessor Paul Quassa lost a non-confidence motion.[50][51] Former Premier Paul Okalik set up an advisory council of eleven elders, whose function it is to help incorporate \\"Inuit Qaujimajatuqangit\\" (Inuit culture and traditional knowledge, often referred to in English as \\"IQ\\") into the territory's political and governmental decisions.[52]\\r\\nDue to the territory's small population, and the fact that there are only a few hundred voters in each electoral district, the possibility of two election candidates finishing in an exact tie is significantly higher than in any Canadian province. This has actually happened twice in the five elections to date, with exact ties in Akulliq in the Nunavut general election, 2008 and in Rankin Inlet South in the Nunavut general election, 2013. In such an event, Nunavut's practice is to schedule a follow-up by-election rather than choosing the winning candidate by an arbitrary method. The territory has also had numerous instances where MLAs were directly acclaimed to office as the only person to register their candidacy by the deadline, as well as one instance where a follow-up by-election had to be held due to no candidates registering for the regular election in their district at all.\\r\\nOwing to Nunavut's vast size, the stated goal of the territorial government has been to decentralize governance beyond the region's capital. Three regionsKitikmeot, Kivalliq and Qikiqtaaluk/Baffinare the basis for more localized administration, although they lack autonomous governments of their own.[citation needed]\\r\\nThe territory has an annual budget of C$700 million, provided almost entirely by the federal government. Former Prime Minister Paul Martin designated support for Northern Canada as one of his priorities for 2004, with an extra $500 million to be divided among the three territories.[citation needed]\\r\\nIn 2001, the government of New Brunswick[citation needed] collaborated with the federal government and the technology firm SSI Micro to launch Qiniq, a unique network that uses satellite delivery to provide broadband Internet access to 24 communities in Nunavut. As a result, the territory was named one of the world's \\"Smart 25 Communities\\" in 2006 by the Intelligent Community Forum, a worldwide organization that honours innovation in broadband technologies. The Nunavut Public Library Services, the public library system serving the territory, also provides various information services to the territory.\\r\\nIn September 2012, Premier Aariak welcomed Prince Edward and Sophie, Countess of Wessex, to Nunavut as part of the events marking the Diamond Jubilee of Queen Elizabeth II.[53]\\r\\nThe Nunavut licence plate was originally created for the Northwest Territories in the 1970s. The plate has long been famous worldwide for its unique design in the shape of a polar bear. Nunavut was licensed by the NWT to use the same licence plate design in 1999 when it became a separate territory,[54] but adopted its own plate design in March 2012 for launch in August 2012a rectangle that prominently features the northern lights, a polar bear and an inuksuk.[54][55]\\r\\nThe flag and the coat of arms of Nunavut were designed by Andrew Karpik from Pangnirtung.[56]\\r\\nThe indigenous music of Nunavut includes Inuit throat singing and drum-led dancing, along with country music, bluegrass, square dancing, the button accordion and the fiddle, an infusion of European influence.\\r\\nThe Inuit Broadcasting Corporation is based in Nunavut. The Canadian Broadcasting Corporation (CBC) serves Nunavut through a radio and television production centre in Iqaluit, and a bureau in Rankin Inlet. The territory is also served by two regional weekly newspapers Nunatsiaq News published by Nortext and Nunavut News/North, published by Northern News Services, who also publish the regional Kivalliq News.[57] Broadband internet is provided by Qiniq and Northwestel through Netkaster.[58][59]\\r\\nThe film production company Isuma is based in Igloolik. Co-founded by Zacharias Kunuk and Norman Cohn in 1990, the company produced the 1999 feature Atanarjuat: The Fast Runner, winner of the Camra d'Or for Best First Feature Film at the 2001 Cannes Film Festival. It was the first feature film written, directed, and acted entirely in Inuktitut.\\r\\nIn November 2006, the National Film Board of Canada (NFB) and the Inuit Broadcasting Corporation announced the start of the Nunavut Animation Lab, offering animation training to Nunavut artists at workshops in Iqaluit, Cape Dorset and Pangnirtung.[60] Films from the Nunavut Animation Lab include Alethea Arnaquq-Baril's 2010 digital animation short Lumaajuuq, winner of the Best Aboriginal Award at the Golden Sheaf Awards and named Best Canadian Short Drama at the imagineNATIVE Film + Media Arts Festival.[61]\\r\\nIn November 2011, the government of Nunavut and the NFB jointly announced the launch of a DVD and online collection entitled Unikkausivut (Inuktitut: Sharing Our Stories), which will make over 100 NFB films by and about Inuit available in Inuktitut, Inuinnaqtun and other Inuit languages, as well as English and French. The Government of Nunavut is distributing Unikkausivut to every school in the territory.[62][63]\\r\\nArtcirq is a collective of Inuit circus performers based in Igloolik.[64] The group has performed around the world, including at the 2010 Olympic Winter Games in Vancouver, British Columbia.\\r\\nSusan Aglukark is an Inuit singer and songwriter. She has released six albums and has won several Juno Awards. She blends the Inuktitut and English languages with contemporary pop music arrangements to tell the stories of her people, the Inuit of Arctic.\\r\\nOn May 3, 2008, the Kronos Quartet premiered a collaborative piece with Inuit throat singer Tanya Tagaq, entitled Nunavut, based on an Inuit folk story. Tagaq is also known internationally for her collaborations with Icelandic pop star Bj?rk.\\r\\nJordin John Kudluk Tootoo (Inuktitut syllabics: ???? ??; born February 2, 1983 in Churchill, Manitoba, Canada) is a professional ice hockey player with the Chicago Blackhawks of the National Hockey League (NHL). Although born in Manitoba, Tootoo grew up in Rankin Inlet, where he was taught to skate and play hockey by his father, Barney.\\r\\nDue to prohibition laws influenced by local and traditional beliefs, Nunavut has a highly regulated alcohol market. It is the last outpost of prohibition in Canada, and it is often easier to obtain firearms than alcohol.[65] Every community in Nunavut has slightly differing regulations, but as a whole it is still very restrictive. Seven communities have bans against alcohol and another 14 have orders being restricted by local committees. Because of these laws, a lucrative bootlegging market has appeared where people mark up the prices of bottles by extraordinary amounts.[66] The RCMP estimate Nunavut's bootleg liquor market rakes in some $10 million a year.[65]\\r\\nDespite the restrictions, alcohol's availability leads to widespread alcohol related crime. One lawyer estimated some 95% of police calls are alcohol-related.[67] Alcohol is also believed to be a contributing factor to the territory's high rates of violence, suicide and homicide. A special task force created in 2010 to study and address the territory's increasing alcohol-related problems recommended the government ease alcohol restrictions. With prohibition shown to be highly ineffective historically, it is believed these laws contribute to the territory's widespread social ills. However, many residents are skeptical about the effectiveness of liquor sale liberalization and want to ban it completely. In 2014, Nunavut's government decided to move towards more legalization. A liquor store will be opened in Iqaluit, the capital, for the first time in 38 years.[65]\\r\\nNunavut has competed at the Arctic Winter Games and co-hosted the 2002 edition.\\r\\nHockey Nunavut was founded in 1999 and competes in the Maritime-Hockey North Junior C Championship.\\r\\n^1 Effective November 12, 2008.\\r\\nTourism\\r\\nJournalism\\r\\nCoordinates: 73N 091W? / ?73N 91W? / 73; -91","input":"What is the name of canada's newest territory?"},{"output":"compound","context":"Copper(II) nitrate, Cu(NO3)2, is an inorganic compound that forms a blue crystalline solid. Anhydrous copper nitrate forms deep blue-green crystals and sublimes in a vacuum at 150-200?C.[3] Copper nitrate also occurs as five different hydrates, the most common ones being the trihydrate and hexahydrate. These materials are more commonly encountered in commerce than in the laboratory.\\r\\n\\r\\n\\r\\nHydrated copper nitrate can be prepared by hydration of the anhydrous material or by treating copper metal with an aqueous solution of silver nitrate or concentrated nitric acid:[4]\\r\\nAnhydrous Cu(NO3)2 forms when copper metal is treated with N2O4:\\r\\nAttempted dehydration of any of the hydrated copper(II) nitrates by heating instead affords the oxides , not Cu(NO3)2. At 80?C, the hydrates convert to \\"basic copper nitrate\\" (Cu2(NO3)(OH)3), which converts to CuO at 180?C.[4] Exploiting this reactivity, copper nitrate can be used to generate nitric acid by heating it until decomposition and passing the fumes directly into water. This method is similar to the last step in the Ostwald process. The equations are as follows:\\r\\nNatural basic copper nitrates include the rare minerals gerhardtite and rouaite, both being polymorphs of Cu2(NO3)(OH)3 substance.[5]\\r\\nAnhydrous copper(II) nitrate has been crystallized in two solvate-free polymorphs.[6][7] ϫ- and -Cu(NO3)2 are fully 3D coordination polymer networks. The alpha form has only one Cu environment, with [4+1] coordination, but the beta form has two different copper centers, one with [4+1] and one that is square planar. The nitromethane solvate also features \\"[4+ 1] coordination\\", with four short Cu-O bonds of approximately 200 pm and one longer bond at 240 pm.[8] They are coordination polymers, with infinite chains of copper(II) centers and nitrate groups. In the gas phase, copper(II) nitrate features two bidentate nitrate ligands (see image at upper right).[9] Thus, evaporation of the solid entails \\"cracking\\" to give the copper(II) nitrate molecule.\\r\\nFive hydrates have been reported: the monohydrate (Cu(NO3)2{H2O),[7] the sesquihydrate (Cu(NO3)2{1.5H2O),[10] the hemipentahydrate (Cu(NO3)2{2.5H2O),[11] a trihydrate (Cu(NO3)2{3H2O),[12] and a hexahydrate (Cu(H2O)6](NO3)2).[13] The hexahydrate is interesting because the Cu-O distances are all equal, not revealing the usual effect of Jahn-Teller distortion that is otherwise characteristic of octahedral Cu(II) complexes. This non-effect is attributed to the strong hydrogen bonding that limits the elasticity of the Cu-O bonds.\\r\\nCopper(II) nitrate finds a variety of applications, the main one being its conversion to copper(II) oxide, which is used as catalyst for a variety of processes in organic chemistry. Its solutions are used in textiles and polishing agents for other metals. Copper nitrates are found in some pyrotechnics.[4] It is often used in school laboratories to demonstrate chemical voltaic cell reactions.\\r\\nCopper nitrate, in combination with acetic anhydride, is an effective reagent for nitration of aromatic compounds, under what are known as \\"Menke conditions\\", in honor of the Dutch chemist who discovered that metal nitrates are effective reagents for nitration.[14] Hydrated copper nitrate adsorbed onto clay affords a reagent called \\"Claycop\\". The resulting blue-colored clay is used as a slurry, for example for the oxidation of thiols to disulfides. Claycop is also used to convert dithioacetals to carbonyls.[15] A related reagent based on montmorillonite has proven useful for the nitration of aromatic compounds.[16]","input":"Is copper ii nitrate a compound or element?"},{"output":"at 8 weeks with abdominal ultrasound imaging","context":"The fetal pole is a thickening on the margin of the yolk sac of a fetus during pregnancy.\\r\\nIt is usually identified at 8 weeks with abdominal ultrasound imaging, and 6 weeks with vaginal ultrasound imaging. However it is quite normal for the fetal pole to not be visible until about 9 weeks. The fetal pole may be seen at 2ÿ4?mm crown-rump length (CRL).","input":"When do you normally see a fetal pole?"},{"output":"Most of the reabsorption (65%)","context":"Renal reabsorption of sodium (Na+) is a part of renal physiology. It uses Na-H antiport, Na-glucose symport, sodium ion channels (minor).[1] It is stimulated by angiotensin II and aldosterone, and inhibited by atrial natriuretic peptide.\\r\\n\\r\\nIt is very efficient, since more than 25,000 mmoles/day of sodium is filtered into the nephron, but only ~100 mmoles/day, or less than 0.4% remains in the final urine.\\r\\n\\r\\nMost of the reabsorption (65%) occurs in the proximal tubule. In the latter part it is favoured by an electrochemical driving force, but initially it needs the cotransporter SGLT and the Na-H antiporter. Sodium passes along an electrochemical gradient (passive transport) from the lumen into the tubular cell, together with water and chloride which also diffuse passively. Water is reabsorbed to the same degree, resulting in the concentration in the end of the proximal tubule being the same as in the beginning. In other words, the reabsorption in the proximal tubule is isosmotic.\\r\\n\\r\\nSodium is reabsorbed in the thick ascending limb of loop of Henle, by Na-K-2Cl symporter and\\r\\nNa-H antiporter. It goes against its chemical driving force, but the high electrical driving force renders the overall electrochemical driving force positive anyway, availing some sodium to diffuse passively either the transcellular or paracellular way..\\r\\n\\r\\nIn the distal convoluted tubule sodium is transported against an electrochemical gradient by sodium-chloride symporters.\\r\\n\\r\\nThe principal cells are the sodium-transporting cells in the collecting duct system.\\r\\n\\r\\nAlthough  only a fragment of total reabsorption happens here, it is the main part of intervention. This is e.g. done by endogenous production of aldosterone, increasing reabsorption. Since the normal excretion rate of sodium is ~100mmoles/day, then a regulation of the absorption of still more than 1000 mmoles/day entering the collecting duct system has a substantial influence of the total sodium excreted.\\r\\n\\r\\nBody water: Intracellular fluid/Cytosol","input":"How much sodium is actively reabsorbed by the proximal segment of the nephron?"},{"output":"natural rubber","context":"Agriculture in Liberia is a major sector of the country's economy worth 38.8% of GDP, employing more than 70% of the population and providing a valuable export for one of the worlds least developed countries (as defined by the UN).[1][2][3][4] Liberia has a climate favourable to farming, vast forests, and an abundance of water, yet low yields mean that over half of foodstuffs are imported, with net agricultural trade at -$73.12 million in 2010.[5] This was dismissed as a \\"misconception\\" by Liberias Minister of Agriculture.[6]\\r\\nThe major crops are natural rubber, rice, cassava, bananas and palm oil.[7] Timber is also a major export at $100 million annually, although much of this is the product of unsustainable habitat destruction, with Asian corporations criticised for their role.[3] Although agricultural activity occurs in most rural locations, it is particularly concentrated in coastal plains (subsistence crops) and tropical forest (cash crops). The sector is very important for women as they are widely employed in it in comparison to the economy as a whole.[8]\\r\\n\\r\\n\\r\\nDeys, Bassa, Kru, and the West Atlantic Gola and Kissi, are likely to be the first inhabitants of the region of present-day Liberia. Their migration was caused by the decline of the Malian Empire in 1375 and Songhay in 1591. Favourable arable land in comparison to their homelands also brought them in the area. Alongside their social and cultural customs, these immigrants also took with them rice, tuber and cotton cultivation. Evidence for this comes from early European merchants, such as Pedro de Cintra, who arrived in 1461.[9]\\r\\nWhen the first American slaves were repatriated to Liberia in 1822, they met hostile tribes. The people and their farming was little-changed from centuries previously. However, settlers developed commercial agriculture, growing most of the crops the country now grows today. The industry was still noted as inefficient and primitive in the 1850s, but had considerable potential for trade in produce: \\"If the land was cleared, it would be excellent land for corn, eddoes, cassada, cane, and rice\\".[10] However, in the 1870s the agricultural sector witnessed a prolonged crisis due to falling prices of cocoa and sugar as well as coffee being introduced to Brazil. In the first half of the 20th Century, the nation received loans from the United States, who created it, developing its economy and agriculture. In World War II, due to south east Asia's rubber's control by Japan, the United States invested heavily in Liberia, especially in agricultural infrastructure.[11]\\r\\nIn the 1990s the government was criticised for selling rights for deforestation to foreign companies, who exploited the bio-diverse hardwood rainforest. This resulted in many reafforestation programmes, although woodland is still destroyed for other uses, such as iron ore mining. Indeed, 98% of the population rely on firewood for fuel, and trees are still cut down for export to Europe and to grow plants. Liberia also lost 70% of its rare mangrove forest at the coast by the mid-1980s.[11] Only a quarter of the original evergreen rainforest is left and almost all the remainder of the land is a mosaic of crops, shrubs and planted trees, in contrast to the Great Plains of its founding country.[12]\\r\\nMuch of the industry was devastated by two civil wars ('89-'96 and '99-'03), with rice production falling by 76% between 1987 and 2005. However, the fighting completely destroyed almost all other sectors of the economy, causing agriculture's share of GDP to rise from 40% in 1990 to 95% in 1996.[12] Chronic malnutrition was (and still is) widespread, caused by a lack of infrastructure, poverty due to unemployment and low use of fertilisers and pesticides (below 1%).\\r\\nNevertheless, recent investment from foreign companies has resulted in cash crop production rising since 2006 with the percentage of households producing them doubling from 28% in 2006 to 46% in 2009 and 55% in 2012.[4] The main cash crops are rubber, coffee, and cocoa. This followed a successful election in 2005, which allowed a democratic government to rebuild infrastructure.\\r\\nHalf of Liberia's total landmass is forest (4.9 million hectares) and 47% is arable land, although most of this is upland pasture.[13] The 'slash and burn' technique is used extensively to convert woodland into rice paddies, often at the expense of biodiversity. The nation consists of flat, coastal plains and dwarf mountains further inland in the north east.\\r\\nThree quarters of the country's land is made up of latosol, a reddish, mineral-rich but infertile soil found in tropical rainforest.[13] While suitable for timber production, without leaf litter it quickly loses its fertility. This, in deforested area, combined with a lack of chemicals, causes Liberia's low yields.\\r\\nA tropical climate gives high temperatures all-year round (roughly 27?C), relative humidity of 65-80%, and heavy rainfall, especially in coastal regions with 3,500-4,600?mm. The rainy season lasts from May to October and leaves the region in water surplus for 5ÿ8 months.[13]\\r\\nThe livestock sector accounts for 14% of GDP despite large areas of pasture after being devastated by the civil war. Almost all animals were eaten.[14] This is also combined with a lack of rural credit and an unfavourable climate. For example, cattle population (per 1000) plummeted from 42 in 1985 to 36 a decade later. Grand Gedeh, Lofa, Bong, and Nimba counties harbour the majority of the cattle, sheep and goats. The most prevalent breeds of cattle are NDama and Muturu, although there have been limited attempts to cross breed these with larger cattle. The most common breed of ruminant in the West African Dwarf and there are also other types of goat.[15] Over 80% of ruminants are kept in traditional village animal husbandry systems. The government constructed ranches and encouraged commercial cattle production but have all been since deserted.\\r\\nThere is no dairy industry in Liberia and all milk is imported. Many of the problems with farming are also blamed on rural-urban migration: as the young move from the countryside to Monrovia and other cities, there is a shortage of labour for farming.\\r\\nThe Food and Agriculture Organization identified the following problems in 2002:\\r\\n[16]\\r\\nAlongside cassava, rice is the staple food of the republic, with 238,000 tonnes produced in 2013. The foodstuff accounts for 22% of agricultural GDP, far higher than all livestock.[7] However, the harvest still falls short of 1988 levels which were 299,000 tonnes even though the population has doubled.[13][17] Yields are also low with most rice at 3 tonnes per hectare, less than half the average in the United States.[12]\\r\\nIn 1979, rice riots resulted in the destruction of $40 million worth of private property. Forty people were killed and 500 wounded.[18] In the wake of the Ebola virus epidemic in West Africa, rice production has shifted away from the traditional 'breadbasket' Lofa, Nimba and Bong Counties to the south east following irrigation investment which tripled yields.[6][19] The epidemic has also caused large price rises and overgrown rice fields, causing the government to declare a state of emergency.[20]\\r\\nLiberia has over 230 species of tree and 14 million acres of forest.[18] Its tropical hardwoods are valuable, chiefly in luxury furniture abroad. The largest firm in the sector is the Indonesian-owned Oriental Timber Corporation, with about three million acres and it has invested over $100 million in sawmills.[21]\\r\\nThe rubber industry was born in the country when Hilary R. W. Johnson, the president, permitted a British firm to extract wild latex in 1890. Liberia Rubber Corporation, another British corporation, built a plantation at Mount Barclay two decades later, but both failed in their endeavours.\\r\\nDuring World War I, Britain and the Netherlands controlled 98% of the raw materials necessary for the production of rubber. In 1922, the Rubber Restriction Act was passed in Britain, increasing the cost of rubber. Then, in 1926, Mr. Harvey S. Firestone (a tyre company owner) obtained a 99-year lease for 4% of the countrys territory in exchange for a $5 million loan. This ensured the nation's rapid growth in the world natural rubber market in World War II. The 12,000 Liberian employees were paid low wages, because the Liberian government felt that \\"men with money in their pockets would eventually have demanded the ballot\\".[22] Firestone gave Charles Taylor, the warlord who caused the deaths of 300,000 Liberians, millions of dollars to secure the future of the firm's plantations, but production was still stopped for 6 years from 1990.[23][24]\\r\\nDespite making up 11% of its exports, there are estimated to be 600,000 ha. of overgrown rubber plantations, some 60 years old. Ergo, there is a large export potential in the redundant rubber-wood.[25] The rubber industry is also diversifying, using trees which no longer produce latex as biomass.[26]\\r\\nThe shrub makes up 23% of Liberian agricultural GDP and is now the second most important food crop. It can be grown throughout the country, although the area covered may vary considerably for different counties. The Liberian Ministry of Agriculture stated that the total area covered by the crop in 1977 was 86,000 ha with an average yield of 1800?kg/ha, and that 26 percent of all agricultural households in the country grew the crop. Its cultivation increased rapidly in the early 1980s. In 1985 283,000 tonnes of cassava was made, 23% larger than the previous year. The same increase happened with the area harvested (113 100 ha) and yield per ha (2500?kg).\\r\\nMost Liberian cassava is processed into various forms for human consumption. Cassava is normally left in the ground until it is ready to be sold, eaten, or processed into a more durable form. The leaves are also often eaten as a vegetable and the FAO encourages their use as animal feed.\\r\\nCassava can be ground into farina flour by removing the skin and grinding it in hammer mills or by hand in a traditional mortar. This is after it has been dried, most typically in the sun. Farina is produced throughout the country, especially in Bassa, Bomi and Nimba counties.\\r\\nThe coconut palm is a versatile element of every local village. Its branches facilitate house building and thatched roofs. The leaves are used in baskets, nets and crude clothes. The fruit itself is high in energy as fat, and can make cooking oil, candles, soap, palm wine and dried flakes. The ash from burnt shells is also another constituent of the soap-making process. Fences, brooms and spoons can also be made from this crop.[11]\\r\\nSince 1977 the Food and Agriculture Organization has given aid to the country. It has encouraged diversification of farms with projects such as horticulture and snail-farming as well as improved yields after Chinese investment.[27] Almost all investment in the country is dependent on foreign donations as the economy was all but destroyed after decades of conflict.\\r\\nThe Ministry of Agriculture (MOA) is the government ministry responsible for the governance, management and promotion of agriculture, founded on May 11, 1972. The Ministry is responsible for the oversight of agronomy, animal husbandry and other agriculture industries, the economic organization of the agriculture and food industries, and national food security. The work of the Ministry is divided into sectors of Livestock Production, Agricultural Chemicals and Crop Production.[28]\\r\\nThe current Minister of Agriculture is Florence A. Chenoweth, Ph.D. Main Ministry offices are located in Monrovia.","input":"What are the chief commercial tree crops of liberia?"},{"output":"houses offices of the Illinois state government","context":"The James R. Thompson Center (JRTC) is located at 100 W. Randolph Street in the Loop district of Chicago and houses offices of the Illinois state government.\\r\\n\\r\\n\\r\\nThe building opened in May 1985 as the State of Illinois Center. It was renamed in 1993 to honor former Illinois Republican Governor James R. Thompson. The property takes up the entire block bound by Randolph, Lake, Clark and LaSalle Streets, one of the 35 full-size city blocks within Chicago's Loop. In front of the Thompson Center is a sculpture, Monument With Standing Beast, by Jean Dubuffet. The Thompson Center is sometimes referred to as the State of Illinois Building.\\r\\nOn February 13, 2018, Chicago Police Commander Paul Bauer was shot and killed, allegedly by Shomari Legghette, while Bauer was pursuing a suspect down a stairwell of the Thompson Center.[1][2]\\r\\nThe Thompson Center was designed by Helmut Jahn of Murphy/Jahn now called JAHN Architects. It opened to mixed reviews by critics, ranging from \\"outrageous\\" to \\"wonderful\\". The color of the street-level panels were compared to tomato soup. The 17-story, all-glass exterior curves and slopes facing a plaza on the southeast corner of the property. The design simultaneity looks forward with advanced architectural tectonics (of the time) and back to recapture the grandeur of large public spaces. Visitors to the Thompson Center's interior can see all 17 floors layered partway around the building's immense skylit atrium. The open-plan offices on each floor are supposed to carry the message of \\"an open government in action.\\"[citation needed]\\r\\nOriginally, the design called for curved, insulated (double paned) glass panels, but these were found to be prohibitively expensive. Flat, insulated glass had been suggested, but was dismissed by Jahn. Single-paned (non-insulated), curved glass panels were eventually used, and resulted in the need for a more expensive air conditioning system, which remains very costly to operate, and is insufficient on hot days; internal temperatures have reached as high as 90?F (32?C).[3] The building is also bitterly cold in the winter; in its early years, ice formed on the interior of some of the wall panels. The marble floor of the atrium initially developed unsightly water stains, an issue which has since been resolved.\\r\\nThe Clark/Lake 'L' station, the busiest in the system, is housed between the Thompson Center and the 203 N. LaSalle building across the street. Orange, Green, Blue, Pink, Purple and Brown Line trains stop at the center. Three tunnels of the Chicago Pedway enter the building's food-court concourse, connecting from to 203 North LaSalle Street, the Chicago Title and Trust Company and Chicago City Hall.\\r\\nThe sculpture at the front entrance by French artist Jean Dubuffet[4] sets the tone for this building that houses a tremendous art collection. The collection includes nineteen specially commissioned artworks funded by the State of Illinois Art-in-Architecture Program.[5] The building also has over 150 of the state's 600 works collected under the Percent for Art program. Under this program 0.5% of the money designated for construction of state-funded public buildings is used for the purchase of art.[6] The Illinois Artisan's shop is also housed inside the building.\\r\\nWhen he first came to office, Illinois Governor Rod Blagojevich proposed selling the building to assuage the state budget.[7] The proposal was heavily criticized.[7] Lawmakers at first agreed to the plan,[8] but later a $200 million mortgage was agreed to instead, payable over 10 years.[9] The plan was declared unconstitutional by Illinois Attorney General Lisa Madigan in June 2004.[10] The plan was set aside, although it had already cost the state $532,000 in legal fees.[11]\\r\\nIn 2015,[12] and again in 2017,[13] Governor Bruce Rauner also proposed selling the property, and a legislative committee to explore his request was announced by Democratic House Speaker Michael Madigan in February 2017.\\r\\nThe Thompson Center has been a filming location in several motion pictures, including 2000's The Watcher and 1990's The Kid Who Loved Christmas, The climax of 1986's Running Scared was filmed there.\\r\\nThe location was previously the location of the Sherman House Hotel operated by Ernie Byfield.[17] The hotel was demolished in 1973 and the site was used as a parking lot until the Thompson Center was constructed.","input":"What is the thompson center in chicago illinois?"},{"output":"WrangellÿSt. Elias in Alaska","context":"The United States has 60 protected areas known as national parks[1] that are operated by the National Park Service, an agency of the Department of the Interior. National parks must be established by an act of the United States Congress. A bill creating the first national park, Yellowstone, was signed into law by President Ulysses S. Grant in 1872, followed by Mackinac National Park in 1875 (decommissioned in 1895), and then Rock Creek Park (later merged into National Capital Parks), Sequoia and Yosemite in 1890. The Organic Act of 1916 created the National Park Service \\"to conserve the scenery and the natural and historic objects and wildlife therein, and to provide for the enjoyment of the same in such manner and by such means as will leave them unimpaired for the enjoyment of future generations.\\"[2] Many current national parks had been previously protected as national monuments by the president under the Antiquities Act before being upgraded by Congress. Seven national parks (including six in Alaska) are paired with a national preserve, areas with different levels of protection that are administered together but considered separate units and whose areas are not included in the figures below.\\r\\nCriteria for the selection of national parks include natural beauty, unique geological features, unusual ecosystems, and recreational opportunities (though these criteria are not always considered together). National monuments, on the other hand, are frequently chosen for their historical or archaeological significance. Fourteen national parks are designated UNESCO World Heritage Sites (WHS),[3] while 21 national parks are designated UNESCO Biosphere Reserves (BR).[4] Eight national parks are designated in both UNESCO programs.\\r\\n\\r\\nTwenty-eight states have national parks, as do the territories of American Samoa and the United States Virgin Islands. California has the most (nine), followed by Alaska (eight), Utah (five), and Colorado (four). The largest national park is WrangellÿSt. Elias in Alaska: at over 8?million acres (32,375?km2), it is larger than each of the nine smallest states. The next three largest parks are also in Alaska. The smallest park is Gateway Arch National Park, Missouri, at approximately 192.83 acres (0.7804?km2). The total area protected by national parks is approximately 52.2?million acres (211,000?km2), for an average of 870?thousand acres (3,500?km2) but a median of only 229?thousand acres (930?km2).[5]\\r\\n\\r\\nThe national parks set a visitation record in 2017, with more than 84 million visitors.[6] The most-visited national park is Great Smoky Mountains National Park in North Carolina and Tennessee, with over 11.3 million visitors in 2017, followed by Arizona's Grand Canyon National Park, with over 6.2 million. In contrast, only 11,177 people visited the remote Gates of the Arctic National Park and Preserve in Alaska in the same year.[7]\\r\\n\\r\\nA few former national parks are no longer designated as such, or have been disbanded. Other units of the National Park Service (418 altogether) are broadly referred to as national parks within the National Park System.[8]","input":"What's the largest national park in the united states?"},{"output":"The battle of the Marne","context":"The 1916 Battle of the Somme was one of the largest battles of the First World War, with more than one million casualties, and also one of the bloodiest battles in human history. The Allied forces attempted to break through the German lines along a 25-mile (40 km) front north and south of the River Somme in northern France. One purpose of the battle was to draw German forces away from the Battle of Verdun; however, by its end the losses on the Somme had exceeded those at Verdun.\\r\\nWhile Verdun would bite deep in the national consciousness of France for generations, the Somme would have the same effect on generations of Britons. The battle is best remembered for its first day, 1 July 1916, on which the British suffered 57,470 casualties, including 19,240 dead  the bloodiest day in the history of the British Army to this day. As terrible as the battle was for the British Empire troops who suffered there, it naturally affected the other nationalities as well. One German officer famously described it as \\"the muddy grave of the German field army.\\" By the end of the battle, the British had learnt many lessons in modern warfare while the Germans had suffered irreplaceable losses. British official historian Sir James Edmonds stated, \\"It is not too much to claim that the foundations of the final victory on the Western Front were laid by the Somme offensive of 1916.\\"\\r\\nFor the first time the home front in Britain was exposed to the horrors of modern war with the release of the propaganda film The Battle of the Somme, which used actual footage from the first days of the battle.\\r\\n\\r\\nThe Battle of Verdun, fought from 21 February to 19 December 1916 around the city of Verdun-sur-Meuse in northeast France, was one of the most important battles in World War I on the Western Front. The battle was fought between the German and French armies. \\r\\n\\r\\nIt resulted in more than a quarter of a million deaths and about half a million wounded. It was the longest battle and one of the bloodiest in World War I. In both France and Germany it has come to represent the horrors of war, similar to the Somme in Britain. \\r\\n\\r\\nThe battle popularised the phrase \\"Ils ne passeront pas\\" (\\"They shall not pass\\"), uttered by Robert Nivelle, but often incorrectly attributed to Ptain.\\r\\n\\r\\n\\r\\n\\r\\nThe Treaty of Versailles (1919) was the peace treaty which officially ended World War I between the Allied and Central Powers and the German Empire. After six months of negotiations, which took place at the Paris Peace Conference, the treaty was signed as a follow-up to the armistice signed on November 11th, 1918 in the Compigne Forest (which had put an end to the actual fighting). Although there were many provisions in the treaty, one of the more important and recognized provisions required Germany to accept full responsibility for causing the war and, under the terms of articles 231-247, make reparations to certain countries that had formed the Allies.\\r\\n\\r\\nThe Brusilov Offensive (Russian: Ҷ־ҳ ŤѾ־) was the greatest Russian feat of arms during World War I, and among the most lethal battles in world history. It was a major offensive against the armies of the Central Powers on the Eastern Front, launched on June 4, 1916 and lasting until early August. It took place in what today is Ukraine, in the general vicinity of the towns of Lemberg, Kovel, and Lutsk. The offensive was named after the Russian commander in charge of the Southwestern Front, Aleksei Brusilov.  \\r\\n\\r\\nThe Battle of Tannenberg in 1914 was a decisive engagement between the Russian Empire and the German Empire in the first days of The Great War, fought by the Russian First and Second Armies and the German Eighth Army between August 17 and September 2, 1914. The battle resulted in the almost complete destruction of the Russian Second Army. A series of follow-up battles kept the Russians off-balance until the spring of 1915. The battle is notable particularly for a number of rapid movements of complete corps by train, allowing the single German Army to present a single front to both Russian Armies.\\r\\n\\r\\nThe Battle of Arras was an offensive during World War I by forces of the British Empire between 9 April and 16 May 1917. British, Canadian, and Australian troops attacked German trenches near the French city of Arras.\\r\\n\\r\\nAt this stage of the war, the Western Front was a continuous line of trenches stretching from the Belgian coast to the Swiss border. With minor adjustments caused by the ebb and flow of asaults and counter-assaults, this corresponded roughly to the final line of confrontation in the last phase of the war of movement of 1914. From behind barbed wire and fortifications, about 100 German divisions faced about 150 French and British Empire divisions in seeming deadlock. In its simplest terms, the Allied objective from early 1915 onwards was to break through the German defences into the open ground beyond and engage the numerically inferior German army in open battle. \\r\\n\\r\\nThe Battle of Arras was planned in conjunction with the French High Command, who were planning a massive attack (the Nivelle Offensive) about eighty kilometres to the south. This had the stated aim of ending the war in forty-eight hours. At Arras, the British Empire's immediate objectives were more modest: (i) to draw German troops away from the ground chosen for the French attack and (ii) to take the German-held high ground that dominated the plain of Douai. \\r\\n\\r\\nWhen the battle officially ended on 16 May, British Empire troops had made significant advances, but had been unable to achieve a major breakthrough at any point. New tactics had been battle-tested, particularly in the first phase, and had demonstrated that set-piece assaults against heavily fortified positions could be successful. This sector then reverted to the stalemate that typified most of the war on the Western Front.\\r\\n\\r\\nThe Armenian Genocide (Armenian: ????? ??????????????, Turkish: Ermeni Soyk?r?m?)  also known as the Armenian Holocaust, \\"Great Calamity\\" (??? ?????) or the Armenian Massacre  refers to the forced mass evacuation and related deaths of hundreds of thousands to over a million Armenians, during the government of the Young Turks from 1915 to 1917 in the Ottoman Empire.\\r\\n\\r\\nToday, the Republic of Turkey rejects the notion that the event constituted a genocide and instead claims that the deaths among the Armenians were a result of inter-ethnic strife, disease and famine during the turmoil of World War I.  However, most Armenian, Russian, Western, and an increasing number of Turkish scholars believe that it was indeed a genocide, or campaign of state-sponsored ethnic cleansing and mass extermination. For example, some Western sources point to the sheer scale of the death toll as evidence for a systematic, organized plan to eliminate the Armenians. The event is also said to be the second-most studied case of genocide, and often draws comparison with the Holocaust. To date 21 countries have here officially recognised it as genocide.\\r\\n\\r\\n\\r\\n\\r\\nThe Battle of Jutland (German: Skagerrakschlacht (Battle of the Skagerrak); Danish: S?slaget ved Jylland / S?slaget om Skagerrak), was the largest naval battle of World War I, and the only full-scale clash of battleships in that war. It was fought on May 31ÿJune 1, 1916, in the North Sea near Jutland, the mainland of Denmark. The combatants were the Kaiserliche Marines High Seas Fleet commanded by Vice Admiral Reinhard Scheer and the Royal Navys Grand Fleet commanded by Admiral of the Fleet Sir John Jellicoe. The intention of the German fleet was to break the British naval blockade of the North Sea and allow German mercantile shipping to operate again.\\r\\n\\r\\nOn the afternoon of 31 May, Beatty and Hipper encountered each other, and in a running battle to the south Hipper drew the British into the path of the High Seas Fleet. Beatty turned and fled towards the Grand Fleet and from 18:30 until nightfall at about 20:30 the two huge fleets  totaling 250 ships between them  were heavily engaged. Fourteen British and eleven German ships were sunk with great loss of life. Jellicoe tried to cut the Germans off from their base in the hope of continuing the battle in the morning, but under cover of darkness Scheer crossed the wake of the British fleet and returned to port.\\r\\n\\r\\nThe First Battle of the Marne (also known as the Miracle of the Marne) was a World War I battle fought from September 5 to September 12, 1914. It was a Franco-British victory against the German army under German Chief of Staff Helmuth von Moltke the Younger.\\r\\n\\r\\nThe battle of the Marne was a major turning point of World War I. By the end of August 1914, the whole Allied army on the Western Front had been forced into a general retreat back towards Paris. Meanwhile the two main German armies continued through France. It seemed that Paris would be taken as both the French and the British fell back towards the Marne River.\\r\\n\\r\\nThe Battle of Gallipoli (sometimes referred to as the first D-Day) took place on the Turkish peninsula of Gallipoli from April 1915 to January 1916 during the First World War. A joint British and French operation was mounted in order to eventually capture the Ottoman capital of Istanbul. The attempt failed, with heavy casualties on both sides. \\r\\n\\r\\nIn Turkey the campaign is known as the ?anakkale Sava?lar?, after the province of ?anakkale. In the United Kingdom it is called the Dardanelles Campaign or Gallipoli, and in France, Australia, New Zealand and Newfoundland it is usually known as the \\"Gallipoli Campaign\\".\\r\\n\\r\\nRead more...","input":"What was the turning point of world war 1?"},{"output":"In the final decade of the sixth century AD","context":"Coordinates: 375817N 234335E? / ?37.9714N 23.7265E? / 37.9714; 23.7265\\r\\nThe Parthenon (/?p?rĲ??n?n, -n?n/; Ancient Greek: ϫĲ?; Greek: ϫĲ?ϫ?, Parthen܇nas) is a former temple,[4][5] on the Athenian Acropolis, Greece, dedicated to the goddess Athena, whom the people of Athens considered their patron. Construction began in 447?BC when the Athenian Empire was at the peak of its power. It was completed in 438?BC although decoration of the building continued until 432?BC. It is the most important surviving building of Classical Greece, generally considered the zenith of the Doric order. Its decorative sculptures are considered some of the high points of Greek art. The Parthenon is regarded as an enduring symbol of Ancient Greece, Athenian democracy and western civilization,[6] and one of the world's greatest cultural monuments. To the Athenians who built it, the Parthenon and other Periclean monuments of the Acropolis, were seen fundamentally as a celebration of Hellenic victory over the Persian invaders and as a thanksgiving to the gods for that victory.[7] The Greek Ministry of Culture is currently carrying out a programme of selective restoration and reconstruction to ensure the stability of the partially ruined structure.[8]\\r\\nThe Parthenon itself replaced an older temple of Athena, which historians call the Pre-Parthenon or Older Parthenon, that was destroyed in the Persian invasion of 480?BC. The temple is archaeoastronomically aligned to the Hyades.[9] Like most Greek temples, the Parthenon served a practical purpose as the city treasury.[10][11] For a time, it served as the treasury of the Delian League, which later became the Athenian Empire. In the final decade of the sixth century AD, the Parthenon was converted into a Christian church dedicated to the Virgin Mary.\\r\\nAfter the Ottoman conquest, it was turned into a mosque in the early 1460s. On 26 September 1687, an Ottoman ammunition dump inside the building was ignited by Venetian bombardment. The resulting explosion severely damaged the Parthenon and its sculptures. From 1800 to 1803,[12] Thomas Bruce, 7th Earl of Elgin removed some of the surviving sculptures with the alleged permission of the Ottoman Empire.[citation needed] These sculptures, now known as the Elgin Marbles or the Parthenon Marbles, were sold in 1816 to the British Museum in London, where they are now displayed. Since 1983 (on the initiative of Culture Minister Melina Mercouri), the Greek government has been committed to the return of the sculptures to Greece.[13]\\r\\n\\r\\n\\r\\nThe origin of the Parthenon's name is from the Greek word ϫĲ? (parthenon), which referred to the \\"unmarried women's apartments\\" in a house and in the Parthenon's case seems to have been used at first only for a particular room of the temple;[14] it is debated which room this is and how the room acquired its name. The LiddellÿScottÿJones GreekÿEnglish Lexicon states that this room was the western cella of the Parthenon, as does J. B. Bury.[7] Jamauri D. Green holds that the parthenon was the room in which the peplos presented to Athena at the Panathenaic Festival was woven by the arrephoroi, a group of four young girls chosen to serve Athena each year.[15] Christopher Pelling asserts that Athena Parthenos may have constituted a discrete cult of Athena, intimately connected with, but not identical to, that of Athena Polias.[16] According to this theory, the name of the Parthenon means the \\"temple of the virgin goddess\\" and refers to the cult of Athena Parthenos that was associated with the temple.[17] The epithet parthnos (ϫĲ??) meant \\"maiden, girl\\", but also \\"virgin, unmarried woman\\"[18] and was especially used for Artemis, the goddess of wild animals, the hunt, and vegetation, and for Athena, the goddess of strategy and tactics, handicraft, and practical reason.[19] It has also been suggested that the name of the temple alludes to the maidens (parthenoi), whose supreme sacrifice guaranteed the safety of the city.[20] Parthnos has also been applied to the Virgin Mary, Parthnos Maria, and the Parthenon had been converted to a Christian church dedicated to the Virgin Mary in the final decade of the sixth century.[21]\\r\\nThe first instance in which Parthenon definitely refers to the entire building is found in the writings of the 4th century BC orator Demosthenes. In 5th-century building accounts, the structure is simply called ho naos (\\"the temple\\"). The architects Iktinos and Callicrates are said to have called the building Hekatompedos (\\"the hundred footer\\") in their lost treatise on Athenian architecture,[22] and, in the 4th century and later, the building was referred to as the Hekatompedos or the Hekatompedon as well as the Parthenon; the 1st-century-AD writer Plutarch referred to the building as the Hekatompedos Parthenon.[23]\\r\\nBecause the Parthenon was dedicated to the Greek goddess Athena, it has sometimes been referred to as the Temple of Minerva, the Roman name for Athena, particularly during the 19th century.[24]\\r\\nAlthough the Parthenon is architecturally a temple and is usually called so, it is not really one in the conventional sense of the word.[25] A small shrine has been excavated within the building, on the site of an older sanctuary probably dedicated to Athena as a way to get closer to the goddess,[25] but the Parthenon never hosted the cult of Athena Polias, patron of Athens: the cult image, which was bathed in the sea and to which was presented the peplos, was an olivewood xoanon, located at an older altar on the northern side of the Acropolis.[26]\\r\\nThe colossal statue of Athena by Phidias was not related to any cult[27] and is not known to have inspired any religious fervour.[26] It did not seem to have any priestess, altar or cult name.[28] According to Thucydides, Pericles once referred to the statue as a gold reserve, stressing that it \\"contained forty talents of pure gold and it was all removable\\".[29] The Athenian statesman thus implies that the metal, obtained from contemporary coinage,[30] could be used again without any impiety.[28] The Parthenon should then be viewed as a grand setting for Phidias' votive statue rather than a cult site.[31] It is said[by whom?] in many writings of the Greeks that there were many treasures stored inside the temple, such as Persian swords and small statue figures made of precious metals.\\r\\nArchaeologist Joan Breton Connelly has recently argued for the coherency of the Parthenons sculptural programme in presenting a succession of genealogical narratives that track Athenian identity back through the ages: from the birth of Athena, through cosmic and epic battles, to the final great event of the Athenian Bronze Age, the war of Erechtheus and Eumolpos.[32][33] She argues a pedagogical function for the Parthenons sculptured decoration, one that establishes and perpetuates Athenian foundation myth, memory, values and identity.[34][35] While some classicists, including Mary Beard, Peter Green, and Garry Wills[36][37] have doubted or rejected Connelly's thesis, an increasing number of historians, archaeologists, and classical scholars support her work. They include: J.J. Pollitt,[38] Brunilde Ridgway,[39] Nigel Spivey,[40] Caroline Alexander,[41] A.E. Stallings.[42]\\r\\nThe first endeavour to build a sanctuary for Athena Parthenos on the site of the present Parthenon was begun shortly after the Battle of Marathon (c. 490ÿ488?BC) upon a solid limestone foundation that extended and levelled the southern part of the Acropolis summit. This building replaced a hekatompedon (meaning \\"hundred-footer\\") and would have stood beside the archaic temple dedicated to Athena Polias (\\"of the city\\"). The Older or Pre-Parthenon, as it is frequently referred to, was still under construction when the Persians sacked the city in 480?BC and razed the Acropolis.[8][43]\\r\\nThe existence of both the proto-Parthenon and its destruction were known from Herodotus,[44] and the drums of its columns were plainly visible built into the curtain wall north of the Erechtheon. Further physical evidence of this structure was revealed with the excavations of Panagiotis Kavvadias of 1885ÿ90. The findings of this dig allowed Wilhelm D?rpfeld, then director of the German Archaeological Institute, to assert that there existed a distinct substructure to the original Parthenon, called Parthenon I by D?rpfeld, not immediately below the present edifice as had been previously assumed.[45] D?rpfeld's observation was that the three steps of the first Parthenon consisted of two steps of Poros limestone, the same as the foundations, and a top step of Karrha limestone that was covered by the lowest step of the Periclean Parthenon. This platform was smaller and slightly to the north of the final Parthenon, indicating that it was built for a wholly different building, now completely covered over. This picture was somewhat complicated by the publication of the final report on the 1885ÿ90 excavations, indicating that the substructure was contemporary with the Kimonian walls, and implying a later date for the first temple.[46]\\r\\nIf the original Parthenon was indeed destroyed in 480, it invites the question of why the site was left a ruin for thirty-three years. One argument involves the oath sworn by the Greek allies before the Battle of Plataea in 479?BC[47] declaring that the sanctuaries destroyed by the Persians would not be rebuilt, an oath from which the Athenians were only absolved with the Peace of Callias in 450.[48] The mundane fact of the cost of reconstructing Athens after the Persian sack is at least as likely a cause. However, the excavations of Bert Hodge Hill led him to propose the existence of a second Parthenon, begun in the period of Kimon after 468?BC.[49] Hill claimed that the Karrha limestone step D?rpfeld thought was the highest of Parthenon I was in fact the lowest of the three steps of Parthenon II, whose stylobate dimensions Hill calculated at 23.51 by 66.888 metres (77.13?ft G?219.45?ft).\\r\\nOne difficulty in dating the proto-Parthenon is that at the time of the 1885 excavation the archaeological method of seriation was not fully developed; the careless digging and refilling of the site led to a loss of much valuable information. An attempt to discuss and make sense of the potsherds found on the Acropolis came with the two-volume study by Graef and Langlotz published in 1925ÿ33.[50] This inspired American archaeologist William Bell Dinsmoor to attempt to supply limiting dates for the temple platform and the five walls hidden under the re-terracing of the Acropolis. Dinsmoor concluded that the latest possible date for Parthenon I was no earlier than 495?BC, contradicting the early date given by D?rpfeld.[51] Further, Dinsmoor denied that there were two proto-Parthenons, and held that the only pre-Periclean temple was what D?rpfeld referred to as Parthenon II. Dinsmoor and D?rpfeld exchanged views in the American Journal of Archaeology in 1935.[52]\\r\\nIn the mid-5th century BC, when the Athenian Acropolis became the seat of the Delian League and Athens was the greatest cultural centre of its time, Pericles initiated an ambitious building project that lasted the entire second half of the century. The most important buildings visible on the Acropolis today  the Parthenon, the Propylaia, the Erechtheion and the temple of Athena Nike  were erected during this period. The Parthenon was built under the general supervision of the artist Phidias, who also had charge of the sculptural decoration. The architects Ictinos and Callicrates began their work in 447?BC, and the building was substantially completed by 432, but work on the decorations continued until at least 431.\\r\\nThe Parthenon is a peripteral octastyle Doric temple with Ionic architectural features. It stands on a platform or stylobate of three steps. In common with other Greek temples, it is of post and lintel construction and is surrounded by columns (\\"peripteral\\") carrying an entablature. There are eight columns at either end (\\"octastyle\\") and seventeen on the sides. There is a double row of columns at either end. The colonnade surrounds an inner masonry structure, the cella, which is divided into two compartments. At either end of the building the gable is finished with a triangular pediment originally occupied by sculpted figures. The columns are of the Doric order, with simple capitals, fluted shafts and no bases. Above the architrave of the entablature is a frieze of carved pictorial panels (metopes), separated by formal architectural triglyphs, typical of the Doric order. Around the cella and across the lintels of the inner columns runs a continuous sculptured frieze in low relief. This element of the architecture is Ionic in style rather than Doric.[53]\\r\\nMeasured at the stylobate, the dimensions of the base of the Parthenon are 69.5 by 30.9 metres (228 by 101?ft). The cella was 29.8?metres long by 19.2?metres wide (97.8?G?63.0?ft). On the exterior, the Doric columns measure 1.9 metres (6.2?ft) in diameter and are 10.4 metres (34?ft) high. The corner columns are slightly larger in diameter. The Parthenon had 46 outer columns and 23 inner columns in total, each column containing 20 flutes. (A flute is the concave shaft carved into the column form.) The roof was covered with large overlapping marble tiles known as imbrices and tegulae.[citation needed][54]\\r\\nThe Parthenon is regarded as the finest example of Greek architecture. The temple, wrote John Julius Cooper, \\"enjoys the reputation of being the most perfect Doric temple ever built. Even in antiquity, its architectural refinements were legendary, especially the subtle correspondence between the curvature of the stylobate, the taper of the naos walls and the entasis of the columns.\\"[55] Entasis refers to the slight swelling, of 1/8 inch, in the centre of the columns to counteract the appearance of columns having a waist, as the swelling makes them look straight from a distance. The stylobate is the platform on which the columns stand. As in many other classical Greek temples,[56] it has a slight parabolic upward curvature intended to shed rainwater and reinforce the building against earthquakes. The columns might therefore be supposed to lean outwards, but they actually lean slightly inwards so that if they carried on, they would meet almost exactly a mile above the centre of the Parthenon; since they are all the same height, the curvature of the outer stylobate edge is transmitted to the architrave and roof above: \\"All follow the rule of being built to delicate curves\\", Gorham Stevens observed when pointing out that, in addition, the west front was built at a slightly higher level than that of the east front.[57]\\r\\nIt is not universally agreed what the intended effect of these \\"optical refinements\\" was; they may serve as a sort of \\"reverse optical illusion\\".[58] As the Greeks may have been aware, two parallel lines appear to bow, or curve outward, when intersected by converging lines. In this case, the ceiling and floor of the temple may seem to bow in the presence of the surrounding angles of the building. Striving for perfection, the designers may have added these curves, compensating for the illusion by creating their own curves, thus negating this effect and allowing the temple to be seen as they intended. It is also suggested that it was to enliven what might have appeared an inert mass in the case of a building without curves, but the comparison ought to be, according to Smithsonian historian Evan Hadingham, with the Parthenon's more obviously curved predecessors than with a notional rectilinear temple.[59]\\r\\nSome studies of the Acropolis, including the Parthenon, conclude that many of its proportions approximate the golden ratio. The Parthenon's fa?ade as well as elements of its fa?ade and elsewhere can be circumscribed by golden rectangles.[60] This view that the golden ratio was employed in the design has been disputed in more recent studies.[61]\\r\\nThe cella of the Parthenon housed the chryselephantine statue of Athena Parthenos sculpted by Phidias and dedicated in 439 or 438?BC. The appearance of this is known from other images. The decorative stonework was originally highly coloured.[62] The temple was dedicated to Athena at that time, though construction continued until almost the beginning of the Peloponnesian War in 432. By the year 438, the sculptural decoration of the Doric metopes on the frieze above the exterior colonnade, and of the Ionic frieze around the upper portion of the walls of the cella, had been completed. In the opisthodomus (the back room of the cella) were stored the monetary contributions of the Delian League, of which Athens was the leading member.\\r\\nOnly a very small number of the sculptures remain in situ; most of the surviving sculptures are today (controversially) in the British Museum in London as the Elgin Marbles, and the Athens Acropolis Museum, but a few pieces are also in the Louvre, and museums in Rome, Vienna and Palermo.[63]\\r\\nThe frieze of the Parthenon's entablature contained ninety-two metopes, fourteen each on the east and west sides, thirty-two each on the north and south sides. They were carved in high relief, a practice employed until then only in treasuries (buildings used to keep votive gifts to the gods).[citation needed] According to the building records, the metope sculptures date to the years 446ÿ440?BC. The metopes of the east side of the Parthenon, above the main entrance, depict the Gigantomachy (the mythical battle between the Olympian gods and the Giants). The metopes of the west end show the Amazonomachy (the mythical battle of the Athenians against the Amazons). The metopes of the south side show the Thessalian Centauromachy (battle of the Lapiths aided by Theseus against the half-man, half-horse Centaurs). Metopes 13ÿ21 are missing, but drawings from 1674 attributed to Jaques Carrey indicate a series of humans; these have been variously interpreted as scenes from the Lapith wedding, scenes from the early history of Athens and various myths.[64] On the north side of the Parthenon, the metopes are poorly preserved, but the subject seems to be the sack of Troy.\\r\\nThe metopes present examples of the Severe Style in the anatomy of the figures' heads, in the limitation of the corporal movements to the contours and not to the muscles, and in the presence of pronounced veins in the figures of the Centauromachy. Several of the metopes still remain on the building, but, with the exception of those on the northern side, they are severely damaged. Some of them are located at the Acropolis Museum, others are in the British Museum, and one is at the Louvre museum.[citation needed][65]\\r\\nIn March 2011, archaeologists announced that they had discovered five metopes of the Parthenon in the south wall of the Acropolis, which had been extended when the Acropolis was used as a fortress. According to Eleftherotypia daily, the archaeologists claimed the metopes had been placed there in the 18th century when the Acropolis wall was being repaired. The experts discovered the metopes while processing 2,250 photos with modern photographic methods, as the white Pentelic marble they are made of differed from the other stone of the wall. It was previously presumed that the missing metopes were destroyed during the Morosini explosion of the Parthenon in 1687.[66]\\r\\nThe most characteristic feature in the architecture and decoration of the temple is the Ionic frieze running around the exterior walls of the cella, which is the inside structure of the Parthenon. The bas-relief frieze was carved in situ; it is dated to 442?BC-438?BC.\\r\\nOne interpretation is that it depicts an idealized version of the Panathenaic procession from the Dipylon Gate in the Kerameikos to the Acropolis. In this procession held every year, with a special procession taking place every four years, Athenians and foreigners were participating to honour the goddess Athena, offering sacrifices and a new peplos (dress woven by selected noble Athenian girls called ergastines).\\r\\nJoan Breton Connelly offers a mythological interpretation for the frieze, one that is in harmony with the rest of the temples sculptural programme which shows Athenian genealogy through a series of succession myths set in the remote past. She identifies the central panel above the door of the Parthenon as the pre-battle sacrifice of the daughter of King Erechtheus, a sacrifice that ensured Athenian victory over Eumolpos and his Thracian army. The great procession marching toward the east end of the Parthenon shows the post-battle thanksgiving sacrifice of cattle and sheep, honey and water, followed by the triumphant army of Erechtheus returning from their victory. This represents the very first Panathenaia set in mythical times, the model on which historic Panathenaic processions was based.[67][68]\\r\\nThe traveller Pausanias, when he visited the Acropolis at the end of the 2nd century AD, only mentioned briefly the sculptures of the pediments (gable ends) of the temple, reserving the majority of his description for the gold and ivory statue of the goddess inside.\\r\\nThe figures on the corners of the pediment depict the passage of time over the course of a full day. Tethrippa of Helios and Selene are located on the left and right corners of the pediment respectively. The horses of Helios's chariot are shown with livid expressions as they ascend into the sky at the start of the day; whereas the Selene's horses struggle to stay on the pediment scene as the day comes to an end.[69][70]\\r\\nThe supporters of Athena are extensively illustrated at the back of the left chariot, while the defenders of Poseidon are shown trailing behind the right chariot. It is believed that the corners of the pediment are filled by Athenian water deities, such as Kephisos river, Ilissos river and nymph Callirhoe. This belief merges from the fluid character of the sculptures' body position which represents the effort of the artist to give the impression of a flowing river.,[71][72] Next to the left river god, there are the sculptures of the mythical king of Athens (Kekrops) with his daughters (Aglauros, Pandrosos, Herse). The statue of Poseidon was the largest sculpture in the pediment until it broke into pieces during Francesco Morosini's effort to remove it in 1688. The posterior piece of the torso was found by Lusieri in the groundwork of a Turkish house in 1801 and is currently held in British Museum. The anterior portion was revealed by Ross in 1835 and is now held in the Acropolis Museum of Athens.[73]\\r\\nEvery statue in the west pediment has a fully completed back, which would have been impossible to see when the sculpture was on the temple; this indicates that the sculptors put great effort into accurately portraying the human body.[72]\\r\\nThe only piece of sculpture from the Parthenon known to be from the hand of Phidias[74] was the statue of Athena housed in the naos. This massive chryselephantine sculpture is now lost and known only from copies, vase painting, gems, literary descriptions and coins.[75]\\r\\nA major fire broke out in the Parthenon shortly after the middle of the third century AD[76][77] which destroyed the Parthenon's roof and much of the sanctuary's interior.[78] Heruli pirates are also credited with sacking Athens in 276, and destroying most of the public buildings there, including the Parthenon.[79] Repairs were made in the fourth century AD, possibly during the reign of Julian the Apostate.[80] A new wooden roof overlaid with clay tiles was installed to cover the sanctuary. It sloped at a greater incline than the original roof and left the building's wings exposed.[78]\\r\\nThe Parthenon survived as a temple dedicated to Athena for nearly one thousand years until Theodosius II decreed in 435 AD that all pagan temples in the Eastern Roman Empire be closed.[81] At some point in the fifth century, Athena's great cult image was looted by one of the emperors and taken to Constantinople, where it was later destroyed, possibly during the siege and sack of Constantinople during the Fourth Crusade in 1204 AD.[82]\\r\\nThe Parthenon was converted into a Christian church in the final decade of the sixth century AD[21] to become the Church of the Parthenos Maria (Virgin Mary), or the Church of the Theotokos (Mother of God). The orientation of the building was changed to face towards the east; the main entrance was placed at the building's western end, and the Christian altar and iconostasis were situated towards the building's eastern side adjacent to an apse built where the temple's pronaos was formerly located.[83][84][85] A large central portal with surrounding side-doors was made in the wall dividing the cella, which became the church's nave, from the rear chamber, the church's narthex.[83] The spaces between the columns of the opisthodomus and the peristyle were walled up, though a number of doorways still permitted access.[83] Icons were painted on the walls and many Christian inscriptions were carved into the Parthenon's columns.[80] These renovations inevitably led to the removal and dispersal of some of the sculptures. Those depicting gods were either possibly re-interpreted according to a Christian theme, or removed and destroyed.[citation needed]\\r\\nThe Parthenon became the fourth most important Christian pilgrimage destination in the Eastern Roman Empire after Constantinople, Ephesos, and Thessalonica.[86] In 1018, the emperor Basil II went on a pilgrimage to Athens directly after his final victory over the Bulgarians for the sole purpose of worshipping at the Parthenon.[86] In medieval Greek accounts it is called the Temple of Theotokos Atheniotissa and often indirectly referred to as famous without explaining exactly which temple they were referring to, thus establishing that it was indeed well known.[86]\\r\\nAt the time of the Latin occupation, it became for about 250?years a Roman Catholic church of Our Lady. During this period a tower, used either as a watchtower or bell tower and containing a spiral staircase, was constructed at the southwest corner of the cella, and vaulted tombs were built beneath the Parthenon's floor.[87]\\r\\nIn 1456, Ottoman Turkish forces invaded Athens and laid siege to a Florentine army defending the Acropolis until June 1458, when it surrendered to the Turks.[88] The Turks may have briefly restored the Parthenon to the Greek Orthodox Christians for continued use as a church.[89] Some time before the close of the fifteenth century, the Parthenon became a mosque.[90][91]\\r\\nThe precise circumstances under which the Turks appropriated it for use as a mosque are unclear; one account states that Mehmed II ordered its conversion as punishment for an Athenian plot against Ottoman rule.[92] The apse became a mihrab,[93] the tower previously constructed during the Roman Catholic occupation of the Parthenon was extended upwards to become a minaret,[94] a minbar was installed,[83] the Christian altar and iconostasis were removed, and the walls were whitewashed to cover icons of Christian saints and other Christian imagery.[95]\\r\\nDespite the alterations accompanying the Parthenon's conversion into a church and subsequently a mosque, its structure had remained basically intact.[96] In 1667 the Turkish traveller Evliya ?elebi expressed marvel at the Parthenon's sculptures and figuratively described the building as \\"like some impregnable fortress not made by human agency\\".[97] He composed a poetic supplication that it, as \\"a work less of human hands than of Heaven itself, should remain standing for all time\\".[98] The French artist Jacques Carrey in 1674 visited the Acropolis and sketched the Parthenon's sculptural decorations.[99] Early in 1687, an engineer named Plantier sketched the Parthenon for the Frenchman Graviers dOrtires.[78] These depictions, particularly those made by Carrey, provide important, and sometimes the only, evidence of the condition of the Parthenon and its various sculptures prior to the devastation it suffered in late 1687 and the subsequent looting of its art objects.[99]\\r\\nIn 1687, the Parthenon was extensively damaged in the greatest catastrophe to befall it in its long history.[80] As part of the Great Turkish War (1683ÿ1699), the Venetians sent an expedition led by Francesco Morosini to attack Athens and capture the Acropolis. The Ottoman Turks fortified the Acropolis and used the Parthenon as a gunpowder magazine ÿ despite having been forewarned of the dangers of this use by the 1656 explosion that severely damaged the Propylaea ÿ and as a shelter for members of the local Turkish community.[100] On 26 September a Venetian mortar round, fired from the Hill of Philopappus, blew up the magazine, and the building was partly destroyed.[101] The explosion blew out the building's central portion and caused the cella's walls to crumble into rubble.[96] Greek architect and archaeologist Kornilia Chatziaslani writes that \\"...three of the sanctuarys four walls nearly collapsed and three-fifths of the sculptures from the frieze fell. Nothing of the roof apparently remained in place. Six columns from the south side fell, eight from the north, as well as whatever remained from eastern porch, except for one column. The columns brought down with them the enormous marble architraves, triglyphs and metopes.\\"[78] About three hundred people were killed in the explosion, which showered marble fragments over nearby Turkish defenders[100] and caused large fires that burned until the following day and consumed many homes.[78]\\r\\nAccounts written at the time conflict over whether this destruction was deliberate or accidental; one such account, written by the German officer Sobievolski, states that a Turkish deserter revealed to Morosini the use to which the Turks had put the Parthenon; expecting that the Venetians would not target a building of such historic importance. Morosini was said to have responded by directing his artillery to aim at the Parthenon.[78][100] Subsequently, Morosini sought to loot sculptures from the ruin and caused further damage in the process. Sculptures of Poseidon and Athena's horses fell to the ground and smashed as his soldiers tried to detach them from the building's west pediment.[84][102]\\r\\nThe following year, the Venetians abandoned Athens to avoid a confrontation with a large force the Turks had assembled at Chalcis; at that time, the Venetians had considered blowing up what remained of the Parthenon along with the rest of the Acropolis to deny its further use as a fortification to the Turks, but that idea was not pursued.[100]\\r\\nAfter the Turks had recaptured the Acropolis they used some of the rubble produced by this explosion to erect a smaller mosque within the shell of the ruined Parthenon.[103] For the next century and a half, portions of the remaining structure were looted for building material and any remaining objects of value.[104]\\r\\nThe 18th century was a period of Ottoman stagnation; as a result, many more Europeans found access to Athens, and the picturesque ruins of the Parthenon were much drawn and painted, spurring a rise in philhellenism and helping to arouse sympathy in Britain and France for Greek independence. Amongst those early travellers and archaeologists were James Stuart and Nicholas Revett, who were commissioned by the Society of Dilettanti to survey the ruins of classical Athens. What they produced was the first measured drawings of the Parthenon published in 1787 in the second volume of Antiquities of Athens Measured and Delineated. In 1801, the British Ambassador at Constantinople, the Earl of Elgin, obtained a questionable firman (edict) from the Sultan, whose existence or legitimacy has not been proved until today, to make casts and drawings of the antiquities on the Acropolis, to demolish recent buildings if this was necessary to view the antiquities, and to remove sculptures from them.[citation needed]\\r\\nWhen independent Greece gained control of Athens in 1832, the visible section of the minaret was demolished; only its base and spiral staircase up to the level of the architrave remain intact.[105] Soon all the medieval and Ottoman buildings on the Acropolis were destroyed. However, the image of the small mosque within the Parthenon's cella has been preserved in Joly de Lotbinire's photograph, published in Lerebours's Excursions Daguerriennes in 1842: the first photograph of the Acropolis.[106] The area became a historical precinct controlled by the Greek government. Today it attracts millions of tourists every year, who travel up the path at the western end of the Acropolis, through the restored Propylaea, and up the Panathenaic Way to the Parthenon, which is surrounded by a low fence to prevent damage.[citation needed]\\r\\nThe dispute centres around the Parthenon Marbles removed by Thomas Bruce, 7th Earl of Elgin, from 1801 to 1803, which are in the British Museum. A few sculptures from the Parthenon are also in the Louvre in Paris, in Copenhagen, and elsewhere, but more than half are in the Acropolis Museum in Athens.[17][107] A few can still be seen on the building itself. The Greek government has campaigned since 1983 for the British Museum to return the sculptures to Greece.[107] The British Museum has steadfastly refused to return the sculptures,[108] and successive British governments have been unwilling to force the Museum to do so (which would require legislation). Nevertheless, talks between senior representatives from Greek and British cultural ministries and their legal advisors took place in London on 4 May 2007. These were the first serious negotiations for several years, and there were hopes that the two sides may move a step closer to a resolution.[109]\\r\\nIn 1975, the Greek government began a concerted effort to restore the Parthenon and other Acropolis structures. After some delay, a Committee for the Conservation of the Acropolis Monuments was established in 1983.[110] The project later attracted funding and technical assistance from the European Union. An archaeological committee thoroughly documented every artifact remaining on the site, and architects assisted with computer models to determine their original locations. Particularly important and fragile sculptures were transferred to the Acropolis Museum. A crane was installed for moving marble blocks; the crane was designed to fold away beneath the roofline when not in use. In some cases, prior re-constructions were found to be incorrect. These were dismantled, and a careful process of restoration began.[111] Originally, various blocks were held together by elongated iron H pins that were completely coated in lead, which protected the iron from corrosion. Stabilizing pins added in the 19th century were not so coated, and corroded. Since the corrosion product (rust) is expansive, the expansion caused further damage by cracking the marble.[112]\\r\\nRestoration works in 2002.\\r\\nWork in progress in 2007.\\r\\nA reconstructed architrave block.\\r\\nWide-scale restoration in 2010.\\r\\nWestern side works in 2014.","input":"When was the parthenon converted into a christian church?"},{"output":"Epidermis","context":"The human skin is the outer covering of the body. In humans, it is the largest organ of the integumentary system. The skin has up to seven layers of ectodermal tissue and guards the underlying muscles, bones, ligaments and internal organs.[1] Human skin is similar to that of most other mammals, and human skin is very similar to pig skin.[2][3] Though nearly all human skin is covered with hair follicles, it can appear hairless. There are two general types of skin, hairy and glabrous skin (hairless).[4] The adjective cutaneous literally means \\"of the skin\\" (from Latin cutis, skin).\\r\\nBecause it interfaces with the environment, skin plays an important immunity role in protecting the body against pathogens[5] and excessive water loss.[6] Its other functions are insulation, temperature regulation, sensation, synthesis of vitamin D, and the protection of vitamin B folates. Severely damaged skin will try to heal by forming scar tissue. This is often discolored and depigmented.\\r\\nIn humans, skin pigmentation varies among populations, and skin type can range from dry to oily. Such skin variety provides a rich and diverse habitat for bacteria that number roughly 1000 species from 19 phyla, present on the human skin.[7][8]\\r\\nSkin has mesodermal cells, pigmentation, such as melanin provided by melanocytes, which absorb some of the potentially dangerous ultraviolet radiation (UV) in sunlight. It also contains DNA repair enzymes that help reverse UV damage, such that people lacking the genes for these enzymes suffer high rates of skin cancer. One form predominantly produced by UV light, malignant melanoma, is particularly invasive, causing it to spread quickly, and can often be deadly. Human skin pigmentation varies among populations in a striking manner. This has led to the classification of people(s) on the basis of skin color.[9]\\r\\nThe skin is the largest organ in the human body. For the average adult human, the skin has a surface area of between 1.5-2.0 square meters (16.1-21.5 sq ft.). The thickness of the skin varies considerably over all parts of the body, and between men and women and the young and the old. An example is the skin on the forearm which is on average 1.3?mm in the male and 1.26?mm in the female.[10] The average square inch (6.5?cm2) of skin holds 650 sweat glands, 20 blood vessels, 60,000 melanocytes, and more than 1,000 nerve endings.[11][better?source?needed] The average human skin cell is about 30 micrometers in diameter, but there are variants. A skin cell usually ranges from 25-40 micrometers (squared), depending on a variety of factors.\\r\\nSkin is composed of three primary layers: the epidermis, the dermis and the hypodermis.[10]\\r\\nEpidermis, \\"epi\\" coming from the Greek meaning \\"over\\" or \\"upon\\", is the outermost layer of the skin. It forms the waterproof, protective wrap over the body's surface which also serves as a barrier to infection and is made up of stratified squamous epithelium with an underlying basal lamina.\\r\\nThe epidermis contains no blood vessels, and cells in the deepest layers are nourished almost exclusively by diffused oxygen from the surrounding air[12] and to a far lesser degree by blood capillaries extending to the outer layers of the dermis. The main type of cells which make up the epidermis are Merkel cells, keratinocytes, with melanocytes and Langerhans cells also present. The epidermis can be further subdivided into the following strata (beginning with the outermost layer): corneum, lucidum (only in palms of hands and bottoms of feet), granulosum, spinosum, basale. Cells are formed through mitosis at the basale layer. The daughter cells (see cell division) move up the strata changing shape and composition as they die due to isolation from their blood source. The cytoplasm is released and the protein keratin is inserted. They eventually reach the corneum and slough off (desquamation). This process is called \\"keratinization\\". This keratinized layer of skin is responsible for keeping water in the body and keeping other harmful chemicals and pathogens out, making skin a natural barrier to infection.\\r\\nThe epidermis contains no blood vessels, and is nourished by diffusion from the dermis. The main type of cells which make up the epidermis are keratinocytes, melanocytes, Langerhans cells and Merkel cells. The epidermis helps the skin to regulate body temperature.\\r\\nEpidermis is divided into several layers where cells are formed through mitosis at the innermost layers. They move up the strata changing shape and composition as they differentiate and become filled with keratin. They eventually reach the top layer called stratum corneum and are sloughed off, or desquamated. This process is called keratinization and takes place within weeks. The outermost layer of the epidermis consists of 25 to 30 layers of dead cells.\\r\\nEpidermis is divided into the following 5 sublayers or strata:\\r\\nBlood capillaries are found beneath the epidermis, and are linked to an arteriole and a venule. Arterial shunt vessels may bypass the network in ears, the nose and fingertips.\\r\\nAbout 70% of all human protein-coding genes are expressed in the skin.[13][14] Almost 500 genes have an elevated pattern of expression in the skin. There are less than 100 genes that are specific for the skin and these are expressed in the epidermis.[15] An analysis of the corresponding proteins show that these are mainly expressed in keratinocytes and have functions related to squamous differentiation and cornification.\\r\\nThe dermis is the layer of skin beneath the epidermis that consists of connective tissue and cushions the body from stress and strain. The dermis is tightly connected to the epidermis by a basement membrane. It also harbors many nerve endings that provide the sense of touch and heat. It contains the hair follicles, sweat glands, sebaceous glands, apocrine glands, lymphatic vessels and blood vessels. The blood vessels in the dermis provide nourishment and waste removal from its own cells as well as from the Stratum basale of the epidermis.\\r\\nThe dermis is structurally divided into two areas: a superficial area adjacent to the epidermis, called the papillary region, and a deep thicker area known as the reticular region.\\r\\nThe papillary region is composed of loose areolar connective tissue. It is named for its fingerlike projections called papillae, that extend toward the epidermis. The papillae provide the dermis with a \\"bumpy\\" surface that interdigitates with the epidermis, strengthening the connection between the two layers of skin.\\r\\nIn the palms, fingers, soles, and toes, the influence of the papillae projecting into the epidermis forms contours in the skin's surface. These epidermal ridges occur in patterns (see: fingerprint) that are genetically and epigenetically determined and are therefore unique to the individual, making it possible to use fingerprints or footprints as a means of identification.\\r\\nThe reticular region lies deep in the papillary region and is usually much thicker. It is composed of dense irregular connective tissue, and receives its name from the dense concentration of collagenous, elastic, and reticular fibers that weave throughout it. These protein fibers give the dermis its properties of strength, extensibility, and elasticity.\\r\\nAlso located within the reticular region are the roots of the hairs, sebaceous glands, sweat glands, receptors, nails, and blood vessels.\\r\\nTattoo ink is held in the dermis. Stretch marks often from pregnancy and obesity, are also located in the dermis.\\r\\nThe subcutaneous tissue (also hypodermis and subcutis) is not part of the skin, and lies below the dermis of the cutis. Its purpose is to attach the skin to underlying bone and muscle as well as supplying it with blood vessels and nerves. It consists of loose connective tissue, adipose tissue and elastin. The main cell types are fibroblasts, macrophages and adipocytes (subcutaneous tissue contains 50% of body fat). Fat serves as padding and insulation for the body.\\r\\nHuman skin shows high skin color variety from the darkest brown to the lightest pinkish-white hues. Human skin shows higher variation in color than any other single mammalian species and is the result of natural selection. Skin pigmentation in humans evolved to primarily regulate the amount of ultraviolet radiation (UVR) penetrating the skin, controlling its biochemical effects.[16]\\r\\nThe actual skin color of different humans is affected by many substances, although the single most important substance determining human skin color is the pigment melanin. Melanin is produced within the skin in cells called melanocytes and it is the main determinant of the skin color of darker-skinned humans. The skin color of people with light skin is determined mainly by the bluish-white connective tissue under the dermis and by the hemoglobin circulating in the veins of the dermis. The red color underlying the skin becomes more visible, especially in the face, when, as consequence of physical exercise or the stimulation of the nervous system (anger, fear), arterioles dilate.[17]\\r\\nThere are at least five different pigments that determine the color of the skin.[18][19] These pigments are present at different levels and places.\\r\\nThere is a correlation between the geographic distribution of UV radiation (UVR) and the distribution of indigenous skin pigmentation around the world. Areas that highlight higher amounts of UVR reflect darker-skinned populations, generally located nearer towards the equator. Areas that are far from the tropics and closer to the poles have lower concentration of UVR, which is reflected in lighter-skinned populations.[20]\\r\\nIn the same population it has been observed that adult human females are considerably lighter in skin pigmentation than males. Females need more calcium during pregnancy and lactation, and vitamin D which is synthesized from sunlight helps in absorbing calcium. For this reason it is thought that females may have evolved to have lighter skin in order to help their bodies absorb more calcium.[21]\\r\\nThe Fitzpatrick scale[22][23] is a numerical classification schema for human skin color developed in 1975 as a way to classify the typical response of different types of skin to ultraviolet (UV) light:\\r\\nAs skin ages, it becomes thinner and more easily damaged. Intensifying this effect is the decreasing ability of skin to heal itself as a person ages.\\r\\nAmong other things, skin aging is noted by a decrease in volume and elasticity. There are many internal and external causes to skin aging. For example, aging skin receives less blood flow and lower glandular activity.\\r\\nA validated comprehensive grading scale has categorized the clinical findings of skin aging as laxity (sagging), rhytids (wrinkles), and the various facets of photoaging, including erythema (redness), and telangiectasia, dyspigmentation (brown discoloration), solar elastosis (yellowing), keratoses (abnormal growths) and poor texture.[24]\\r\\nCortisol causes degradation of collagen,[25] accelerating skin aging.[26]\\r\\nAnti-aging supplements are used to treat skin aging.\\r\\nPhotoaging has two main concerns: an increased risk for skin cancer and the appearance of damaged skin. In younger skin, sun damage will heal faster since the cells in the epidermis have a faster turnover rate, while in the older population the skin becomes thinner and the epidermis turnover rate for cell repair is lower which may result in the dermis layer being damaged.[27]\\r\\nSkin performs the following functions:\\r\\nThe human skin is a rich environment for microbes.[7][8] Around 1000 species of bacteria from 19 bacterial phyla have been found. Most come from only four phyla: Actinobacteria (51.8%), Firmicutes (24.4%), Proteobacteria (16.5%), and Bacteroidetes (6.3%). Propionibacteria and Staphylococci species were the main species in sebaceous areas. There are three main ecological areas: moist, dry and sebaceous. In moist places on the body Corynebacteria together with Staphylococci dominate. In dry areas, there is a mixture of species but dominated by b-Proteobacteria and Flavobacteriales. Ecologically, sebaceous areas had greater species richness than moist and dry ones. The areas with least similarity between people in species were the spaces between fingers, the spaces between toes, axillae, and umbilical cord stump. Most similarly were beside the nostril, nares (inside the nostril), and on the back.\\r\\nReflecting upon the diversity of the human skin researchers on the human skin microbiome have observed: \\"hairy, moist underarms lie a short distance from smooth dry forearms, but these two niches are likely as ecologically dissimilar as rainforests are to deserts.\\"[7]\\r\\nThe NIH has launched the Human Microbiome Project to characterize the human microbiota which includes that on the skin and the role of this microbiome in health and disease.[29]\\r\\nMicroorganisms like Staphylococcus epidermidis colonize the skin surface. The density of skin flora depends on region of the skin. The disinfected skin surface gets recolonized from bacteria residing in the deeper areas of the hair follicle, gut and urogenital openings.\\r\\nDiseases of the skin include skin infections and skin neoplasms (including skin cancer).\\r\\nDermatology is the branch of medicine that deals with conditions of the skin.[4]\\r\\nThe skin supports its own ecosystems of microorganisms, including yeasts and bacteria, which cannot be removed by any amount of cleaning. Estimates place the number of individual bacteria on the surface of one square inch (6.5 square cm) of human skin at 50 million, though this figure varies greatly over the average 20 square feet (1.9?m2) of human skin. Oily surfaces, such as the face, may contain over 500 million bacteria per square inch (6.5?cm2). Despite these vast quantities, all of the bacteria found on the skin's surface would fit into a volume the size of a pea.[30] In general, the microorganisms keep one another in check and are part of a healthy skin. When the balance is disturbed, there may be an overgrowth and infection, such as when antibiotics kill microbes, resulting in an overgrowth of yeast. The skin is continuous with the inner epithelial lining of the body at the orifices, each of which supports its own complement of microbes.\\r\\nCosmetics should be used carefully on the skin because these may cause allergic reactions. Each season requires suitable clothing in order to facilitate the evaporation of the sweat. Sunlight, water and air play an important role in keeping the skin healthy.\\r\\nOily skin is caused by over-active sebaceous glands, that produce a substance called sebum, a naturally healthy skin lubricant.[1] When the skin produces excessive sebum, it becomes heavy and thick in texture. Oily skin is typified by shininess, blemishes and pimples.[1] The oily-skin type is not necessarily bad, since such skin is less prone to wrinkling, or other signs of aging,[1] because the oil helps to keep needed moisture locked into the epidermis (outermost layer of skin).\\r\\nThe negative aspect of the oily-skin type is that oily complexions are especially susceptible to clogged pores, blackheads, and buildup of dead skin cells on the surface of the skin.[1] Oily skin can be sallow and rough in texture and tends to have large, clearly visible pores everywhere, except around the eyes and neck.[1]\\r\\nHuman skin has a low permeability; that is, most foreign substances are unable to penetrate and diffuse through the skin. Skin's outermost layer, the stratum corneum, is an effective barrier to most inorganic nanosized particles.[31][32] This protects the body from external particles such as toxins by not allowing them to come into contact with internal tissues. However, in some cases it is desirable to allow particles entry to the body through the skin. Potential medical applications of such particle transfer has prompted developments in nanomedicine and biology to increase skin permeability. One application of transcutaneous particle delivery could be to locate and treat cancer. Nanomedical researchers seek to target the epidermis and other layers of active cell division where nanoparticles can interact directly with cells that have lost their growth-control mechanisms (cancer cells). Such direct interaction could be used to more accurately diagnose properties of specific tumors or to treat them by delivering drugs with cellular specificity.\\r\\nNanoparticles 40?nm in diameter and smaller have been successful in penetrating the skin.[33][34][35] Research confirms that nanoparticles larger than 40?nm do not penetrate the skin past the stratum corneum.[33] Most particles that do penetrate will diffuse through skin cells, but some will travel down hair follicles and reach the dermis layer.\\r\\nThe permeability of skin relative to different shapes of nanoparticles has also been studied. Research has shown that spherical particles have a better ability to penetrate the skin compared to oblong (ellipsoidal) particles because spheres are symmetric in all three spatial dimensions.[35] One study compared the two shapes and recorded data that showed spherical particles located deep in the epidermis and dermis whereas ellipsoidal particles were mainly found in the stratum corneum and epidermal layers.[36] Nanorods are used in experiments because of their unique fluorescent properties but have shown mediocre penetration.\\r\\nNanoparticles of different materials have shown skins permeability limitations. In many experiments, gold nanoparticles 40?nm in diameter or smaller are used and have shown to penetrate to the epidermis. Titanium oxide (TiO2), zinc oxide (ZnO), and silver nanoparticles are ineffective in penetrating the skin past the stratum corneum.[37][38] Cadmium selenide (CdSe) quantum dots have proven to penetrate very effectively when they have certain properties. Because CdSe is toxic to living organisms, the particle must be covered in a surface group. An experiment comparing the permeability of quantum dots coated in polyethylene glycol (PEG), PEG-amine, and carboxylic acid concluded the PEG and PEG-amine surface groups allowed for the greatest penetration of particles. The carboxylic acid coated particles did not penetrate past the stratum corneum.[36]\\r\\nScientists previously believed that the skin was an effective barrier to inorganic particles. Damage from mechanical stressors was believed to be the only way to increase its permeability.[39] Recently, however, simpler and more effective methods for increasing skin permeability have been developed. For example, ultraviolet radiation (UVR) has been used to slightly damage the surface of skin, causing a time-dependent defect allowing easier penetration of nanoparticles.[40] The UVRs high energy causes a restructuring of cells, weakening the boundary between the stratum corneum and the epidermal layer.[40][41] The damage of the skin is typically measured by the transepidermal water loss (TEWL), though it may take 3ÿ5 days for the TEWL to reach its peak value. When the TEWL reaches its highest value, the maximum density of nanoparticles is able to permeate the skin. Studies confirm that UVR damaged skin significantly increases the permeability.[40][41] The effects of increased permeability after UVR exposure can lead to an increase in the number of particles that permeate the skin. However, the specific permeability of skin after UVR exposure relative to particles of different sizes and materials has not been determined.[40]\\r\\nOther skin damaging methods used to increase nanoparticle penetration include tape stripping, skin abrasion, and chemical enhancement. Tape stripping is the process in which tape is applied to skin then lifted to remove the top layer of skin. Skin abrasion is done by shaving the top 5-10 micrometers off the surface of the skin. Chemical enhancement is the process in which chemicals such as polyvinylpyrrolidone (PVP), dimethyl sulfoxide (DMSO), and oleic acid are applied to the surface of the skin to increase permeability.[42][43]\\r\\nElectroporation is the application of short pulses of electric fields on skin and has proven to increase skin permeability. The pulses are high voltage and on the order of milliseconds when applied. Charged molecules penetrate the skin more frequently than neutral molecules after the skin has been exposed to electric field pulses. Results have shown molecules on the order of 100 micrometers to easily permeate electroporated skin.[43]\\r\\nA large area of interest in nanomedicine is the transdermal patch because of the possibility of a painless application of therapeutic agents with very few side effects. Transdermal patches have been limited to administer a small number of drugs, such as nicotine, because of the limitations in permeability of the skin. Development of techniques that increase skin permeability has led to more drugs that can be applied via transdermal patches and more options for patients.[43]\\r\\nIncreasing the permeability of skin allows nanoparticles to penetrate and target cancer cells. Nanoparticles along with multi-modal imaging techniques have been used as a way to diagnose cancer non-invasively. Skin with high permeability allowed quantum dots with an antibody attached to the surface for active targeting to successfully penetrate and identify cancerous tumors in mice. Tumor targeting is beneficial because the particles can be excited using fluorescence microscopy and emit light energy and heat that will destroy cancer cells.[44]\\r\\nSunblock and sunscreen are different important skin-care products though both offer full protection from the sun.[45][46]\\r\\nSunblockSunblock is opaque and stronger than sunscreen, since it is able to block most of the UVA/UVB rays and radiation from the sun, and does not need to be reapplied several times in a day. Titanium dioxide and zinc oxide are two of the important ingredients in sunblock.[47]\\r\\nSunscreenSunscreen is more transparent once applied to the skin and also has the ability to protect against UVA/UVB rays, although the sunscreen's ingredients have the ability to break down at a faster rate once exposed to sunlight, and some of the radiation is able to penetrate to the skin. In order for sunscreen to be more effective it is necessary to consistently reapply and use one with a higher sun protection factor.\\r\\nVitamin A, also known as retinoids, benefits the skin by normalizing keratinization, downregulating sebum production which contributes to acne, and reversing and treating photodamage, striae, and cellulite.\\r\\nVitamin D and analogs are used to downregulate the cutaneous immune system and epithelial proliferation while promoting differentiation.\\r\\nVitamin C is an antioxidant that regulates collagen synthesis, forms barrier lipids, regenerates vitamin E, and provides photoprotection.\\r\\nVitamin E is a membrane antioxidant that protects against oxidative damage and also provides protection against harmful UV rays. [48]\\r\\nSeveral scientific studies confirmed that changes in baseline nutritional status affects skin condition. [49]\\r\\nThe Mayo Clinic lists foods they state help the skin: fruits and vegetables, whole-grains, dark leafy greens, nuts, and seeds.[50]","input":"What is the outer skin layer in human body called?"},{"output":"October 9, 1776","context":"Mission San Francisco de Ass, or Mission Dolores, is the oldest surviving structure in San Francisco and the sixth religious settlement established as part of the California chain of missions. The Mission was founded on October 9, 1776, by Lieutenant Jos Joaquin Moraga and Francisco Pal܇u (a companion of Junpero Serra), both members of the de Anza Expedition, which had been charged with bringing Spanish settlers to Alta (upper) California, and evangelizing the local Natives, the Ohlone. Some of the Mission's buildings have been turned into business including a print shop, and several saloons.\\r\\n\\r\\n\\r\\nThe settlement was named for St. Francis of Assisi, the founder of the Franciscan Order, but was also commonly known as \\"Mission Dolores\\" owing to the presence of a nearby creek named Arroyo de Nuestra Se?ora de los Dolores, meaning \\"Our Lady of Sorrows Creek.\\"[citation needed] During the expedition of Juan Bautista de Anza, this site was identified by Pedro Font as the most suitable site for a mission in the San Francisco area.[11]\\r\\nThe original Mission was a small structure dedicated on July 4, 1823, after the required church documents arrived. It was located near what is today the intersection of Camp and Albion Streets (according to most sources)[citation needed], about a block-and-a-half east of the surviving Adobe Mission building, and on the shores of a lake (supposedly long since filled) called Laguna de Los Dolores.[4] A historical marker at that location depicts this lake, but whether it ever actually existed is a matter of some dispute. (Creek geologists Janet Sowers and Christopher Richard propose that the legendary lake is the result of misunderstandings of Juan Bautista de Anza's 1776 writings. According to their 2011 hydrological map, there were no lakes in the area, only creeks.)[12]\\r\\nThe present Mission church, near what is now the intersection of Dolores and 16th Streets, was dedicated in 1791. At the time of dedication, a mural painted by native labor adorned the focal wall of the chapel. The Mission was constructed of Adobe and part of a complex of buildings used for housing, agricultural and manufacturing enterprises (see architecture of the California missions). Though most of the Mission complex, including the quadrangle and Convento, has either been altered or demolished outright during the intervening years, the fa?ade of the Mission chapel has remained relatively unchanged since its construction in 1782ÿ1791.[citation needed]\\r\\nAccording to Mission historian Brother Guire Cleary, the early 19th century saw the greatest period of activity at San Francisco de Ass:\\r\\nAt its peak in 1810ÿ1820, the average Indian population at Pueblo Dolores was about 1,100 people. The California missions were not only houses of worship. They were farming communities, manufacturers of all sorts of products, hotels, ranches, hospitals, schools, and the centers of the largest communities in the state...in 1810 the Mission owned 11,000 sheep, 11,000 cows, and thousands of horses, goats, pigs, and mules. Its ranching and farming operations extended as far south as San Mateo and east to Alameda. Horses were corralled on Potrero Hill, and the milking sheds for the cows were located along Dolores Creek at what is today Mission High School. Twenty looms were kept in operation to process wool into cloth. The circumference of the mission's holdings was said to have been about 125 miles.[13]\\r\\nThe Mission chapel, along with \\"Father Serra's Church\\" at Mission San Juan Capistrano, is one of only two surviving buildings where Junpero Serra is known to have officiated (although \\"Dolores\\" was still under construction at the time of Serra's visit). In 1817, Mission San Rafael Arcngel was established as an Asistencia to act as a hospital for the Mission, though it would later be granted full mission status in 1822. The Mexican War of Independence (1810ÿ1821) strained relations between the Mexican government and the California missions. Supplies were scant, and the Indians who worked at the missions continued to suffer terrible losses from disease and cultural disruption (more than 5,000 Indians are thought to have been buried in the cemetery adjacent to the Mission). In 1834, the Mexican government enacted secularization laws whereby most church properties were sold or granted to private owners. In practical terms, this meant that the missions would hold title only to the churches, the residences of the priests and a small amount of land surrounding the church for use as gardens. In the period that followed, Mission Dolores fell on very hard times. By 1842, only eight Christian Indians were living at the Mission.[13]\\r\\nThe California Gold Rush brought renewed activity to the Mission Dolores area. In the 1850s, two plank roads were constructed from what is today downtown San Francisco to the Mission, and the entire area became a popular resort and entertainment district.[14] Some of the Mission properties were sold or leased for use as saloons and gambling halls. Racetracks were constructed, and fights between bulls and bears were staged for crowds. The Mission complex also underwent alterations. Part of the Convento was converted to a two-story wooden wing for use as a seminary and priests' quarters, while another section became the \\"Mansion House,\\" a popular tavern and way station for travelers.[15] By 1876, the Mansion House portion of the Convento had been razed and replaced with a large Gothic Revival brick church, designed to serve the growing population of immigrants who were now making the Mission area their home.[citation needed]\\r\\nDuring this period, wood clapboard siding was applied to the original adobe chapel walls as both a cosmetic and a protective measure; the veneer was later removed when the Mission was restored. During the 1906 San Francisco earthquake, the adjacent brick church was destroyed. By contrast, the original adobe Mission, though damaged, remained in relatively good condition. However, the ensuing fire touched off by the earthquake reached almost to the Mission's doorstep. To prevent the spread of flames, the Convent and School of Notre Dame across the street was dynamited by firefighters; nevertheless, nearly all the blocks east of Dolores Street and north of 20th street were consumed by flames. In 1913, construction began on a new church (now known as the Mission Dolores Basilica) adjacent to the Mission, which was completed in 1918. This structure was further remodeled in 1926 with churrigueresque ornamentation inspired by the Panama-California Exposition held in San Diego's Balboa Park. A sensitive restoration of the original adobe Mission was undertaken in 1917 by architect Willis Polk. In 1952, San Francisco Archbishop John J. Mitty, announced that Pope Pius XII had elevated Mission Dolores to the status of a Minor Basilica. This was the first designation of a basilica west of the Mississippi and the fifth basilica named in the United States. Today, the larger, newer church is called \\"Mission Dolores Basilica\\" while the original adobe structure retains the name of Mission Dolores.[citation needed]\\r\\nThe San Francisco de Ass cemetery, which adjoins the property on the south side, was originally much larger than its present boundaries, running west almost to Church Street and north into what is today 16th Street. It was reduced in various stages, starting with the extension of 16th Street through the former mission grounds in 1889, and later by the construction of the Mission Dolores Basilica Center and the Chancery Building of the Archdiocese of San Francisco in the 1950s. Some remains were reburied on-site in a mass grave, while others were relocated to various Bay Area cemeteries. Today, most of the former cemetery grounds are covered by a paved playground behind the Mission Dolores School. The cemetery that currently remains underwent a careful restoration in the mid-1990s. The mission is still an active church, and hosts mass and other services in the mission church and the larger adjacent basilica. The mission is open to visitors, and is located on Dolores Street near its intersection with 16th Street. The San Francisco neighborhood closely surrounding the historic Mission is known as Mission Dolores, and the much larger Mission District is named for it as well. The current Pastor of Mission Dolores is Reverend Francis Mark P. Garbo. The current Curator of Mission Dolores is Andrew A. Galvan.[citation needed]\\r\\nA full-length portrait sculpture of Junpero Serra is on the property of the mission. The cast stone sculpture, by Arthur Putnam, was completed in 1909, cast between 1916ÿ1917 and installed in 1918 when the mission was remodeled. Funding for the piece came from D.J. McQuarry and it cost $500 to cast. It is approximately H. 6?ft. 6 in. The sculpture depicts Serra wearing a monk's robe belted at the waist with a knotted rope and a rosary around his neck. He looks down, with his head bowed and eyes downward. The sculpture is on a concrete base. It is one of a series of allegorical figures commissioned by the estate of E. W. Scripps to depict California history. In 1993 it was examined by the Smithsonian Institution's Save Outdoor Sculpture! program. The program determined that the sculpture was well maintained.[20]","input":"When was the mission san francisco de asis founded?"},{"output":"twenty-seven themed resort hotels, nine non-Disney hotels,","context":"Coordinates: 282307N 813350W? / ?28.385233N 81.563874W? / 28.385233; -81.563874\\r\\nWalt Disney World, officially known as the Walt Disney World Resort, also known as just Disney World, is an entertainment complex in Bay Lake and Lake Buena Vista, Florida, near Orlando and Kissimmee, Florida. Opened on October 1, 1971, the resort is owned and operated by Walt Disney Parks, Experiences and Consumer Products, a division of The Walt Disney Company. It was initially operated by Walt Disney World Company. The property covers 27,258 acres (43?sq?mi; 110?km2), featuring four theme parks, two water parks, twenty-seven themed resort hotels, nine non-Disney hotels, several golf courses, a camping resort, and other entertainment venues, including the outdoor shopping center Disney Springs.\\r\\nDesigned to supplement Disneyland in Anaheim, California, which had opened in 1955, the complex was developed by Walt Disney in the 1960s. \\"The Florida Project\\", as it was known, was intended to present a distinct vision with its own diverse set of attractions. Walt Disney's original plans also called for the inclusion of an \\"Experimental Prototype Community of Tomorrow\\" (EPCOT), a planned community intended to serve as a test bed for new city living innovations. After extensive lobbying, the government of Florida created the Reedy Creek Improvement District, a special government district that essentially gave The Walt Disney Company the standard powers and autonomy of an incorporated city. Walt Disney died on December 15, 1966, during construction of the complex. Without Disney spearheading the construction, the company created a resort similar to Disneyland, abandoning experimental concepts for a planned community. Magic Kingdom was the first theme park to open in the complex, in 1971, followed by Epcot in 1982, Disney's Hollywood Studios in 1989, and the most recent, Disney's Animal Kingdom in 1998.\\r\\nToday, Walt Disney World is the most visited vacation resort in the world, with an average annual attendance of over 52 million.[2] The resort is the flagship destination of Disney's worldwide corporate enterprise, and has become a popular staple in American culture.\\r\\n\\r\\n\\r\\nIn 1959, Walt Disney Productions began looking for land to house a second resort to supplement Disneyland in Anaheim, California, which had opened in 1955. Market surveys at the time revealed that only 5% of Disneyland's visitors came from east of the Mississippi River, where 75% of the population of the United States lived. Additionally, Walt Disney disliked the businesses that had sprung up around Disneyland and wanted more control over a larger area of land in the next project.[3]\\r\\nWalt Disney flew over a potential site in Orlando, Florida ÿ one of many ÿ in November 1963. After witnessing the well-developed network of roads and taking the planned construction of both Interstate 4 and Florida's Turnpike into account, with McCoy Air Force Base (later Orlando International Airport) to the east, Disney selected a centrally-located site near Bay Lake.[4] To avoid a burst of land speculation, Walt Disney World Company used various dummy corporations to acquire 30,500 acres (48?sq?mi; 123?km2) of land.[4] In May 1965, some of these major land transactions were recorded a few miles southwest of Orlando in Osceola County. In addition, two large tracts totaling $1.5 million were sold, and smaller tracts of flatlands and cattle pastures were purchased by exotically-named companies such as the \\"Ayefour Corporation\\", \\"Latin-American Development and Management Corporation\\" and the \\"Reedy Creek Ranch Corporation\\". Some are now memorialized on a window above Main Street, U.S.A. in Magic Kingdom. The smaller parcels of land acquired were called \\"outs\\". They were 5-acre (2?ha) lots platted in 1912 by the Munger Land Company and sold to investors. Most of the owners in the 1960s were happy to get rid of the land, which was mostly swamp at the time. Another issue was the mineral rights to the land, which were owned by Tufts University. Without the transfer of these rights, Tufts could come in at any time and demand the removal of buildings to obtain minerals. Eventually, Disney's team negotiated a deal with Tufts to buy the mineral rights for $15,000.[5]\\r\\nWorking strictly in secrecy, real estate agents unaware of their client's identity began making offers to landowners in April 1964 in parts of southwest Orange and northwest Osceola counties. The agents were careful not to reveal the extent of their intentions, and they were able to negotiate numerous land contracts with some including large tracts of land for as little as $100 an acre.[6] With the understanding that the recording of the first deeds would trigger intense public scrutiny, Disney delayed the filing of paperwork until a large portion of the land was under contract.[7]\\r\\nEarly rumors and speculation about the land purchases assumed possible development by NASA in support of the nearby Kennedy Space Center, as well as references to other famous investors such as Ford, the Rockefellers, and Howard Hughes.[7] An Orlando Sentinel news article published weeks later on May 20, 1965, acknowledged a popular rumor that Disney was building an \\"East Coast\\" version of Disneyland. However, the publication denied its accuracy based on an earlier interview with Disney at Kennedy Space Center, in which he claimed a $50 million investment was in the works for Disneyland, and that he had no interest in building a new park.[7] In October 1965, editor Emily Bavar from the Sentinel visited Disneyland during the park's 10th-anniversary celebration. In an interview with Disney, she asked him if he was behind recent land purchases in Central Florida; Bavar later described that Disney \\"looked like I had thrown a bucket of water in his face\\" before denying the story.[7] His reaction, combined with other research obtained during her Anaheim visit, led Bavar to author a story on October 21, 1965, where she predicted that Disney was building a second theme park in Florida.[7] Three days later after gathering more information from various sources, the Sentinel published another article headlined, \\"We Say: 'Mystery Industry' Is Disney\\".[7]\\r\\nWalt Disney had originally planned to publicly reveal Disney World on November 15, 1965, but in light of the Sentinel story, Disney asked Florida Governor Haydon Burns to confirm the story on October 25. His announcement called the new theme park \\"the greatest attraction in the history of Florida\\".[7] The official reveal was kept on the previously-planned November 15 date, and Disney joined Burns in Orlando for the event.[7]\\r\\nWalt Disney died from circulatory collapse caused by lung cancer on December 15, 1966, before his vision was realized. His brother and business partner, Roy O. Disney, postponed his retirement to oversee construction of the resort's first phase.\\r\\nOn February 2, 1967, Roy O. Disney held a press conference at the Park Theatres in Winter Park, Florida. The role of EPCOT was emphasized in the film that was played. After the film, it was explained that for Disney World, including EPCOT, to succeed, a special district would have to be formed: the Reedy Creek Improvement District with two cities inside it, Bay Lake and Reedy Creek, now Lake Buena Vista. In addition to the standard powers of an incorporated city, which include the issuance of tax-free bonds, the district would have immunity from any current or future county or state land-use laws. The only areas where the district had to submit to the county and state would be property taxes and elevator inspections.[3] The legislation forming the district and the two cities was signed into law by Florida Governor Claude R. Kirk, Jr. on May 12, 1967.[8] The Supreme Court of Florida then ruled in 1968 that the district was allowed to issue tax-exempt bonds for public projects within the district, despite the sole beneficiary being Walt Disney Productions.\\r\\nThe district soon began construction of drainage canals, and Disney built the first roads and the Magic Kingdom. The Contemporary Resort Hotel and Polynesian Village were also completed in time for the park's opening on October 1, 1971.[9][10] The Palm and Magnolia golf courses near Magic Kingdom had opened a few weeks before, while Fort Wilderness opened a month later. 24 days after the park opened, Roy O. Disney dedicated the property and declared that it would be known as \\"Walt Disney World\\" in his brother's honor. In his own words: \\"Everyone has heard of Ford cars. But have they all heard of Henry Ford, who started it all? Walt Disney World is in memory of the man who started it all, so people will know his name as long as Walt Disney World is here.\\" After the dedication, Roy Disney asked Walt's widow, Lillian, what she thought of Walt Disney World. According to biographer Bob Thomas, she responded, \\"I think Walt would have approved.\\" Roy Disney died at age 78 on December 20, 1971, less than three months after the property opened.[11]\\r\\nAdmission prices in 1971 were $3.50 for adults, $2.50 for juniors under age 18, and one dollar for children under twelve.[9]\\r\\nMuch of Walt Disney's plans for his Progress City were abandoned after his death after the company board decided that it did not want to be in the business of running a city. The concept evolved into the resort's second theme park, EPCOT Center,which opened in 1982 (renamed EPCOT in 1996). While still emulating Walt Disney's original idea of showcasing new technology, the park is closer to a world's fair than a \\"community of tomorrow\\". One of EPCOT's main attractions is their world's showcase which highlights 11 countries across the globe. Some of the urban planning concepts from the original idea of EPCOT would instead be integrated into the community of Celebration much later. The resort's third theme park, Disney-MGM Studios (renamed Disney's Hollywood Studios in 2008), opened in 1989 and is inspired by show business. The resort's fourth theme park, Disney's Animal Kingdom, opened in 1998.\\r\\nGeorge Kalogridis was named president of the resort in December 2012, replacing Meg Crofton, who had overseen the site since 2006.\\r\\nOn January 21, 2016, the resort's management structure was changed, with general managers within a theme park being in charge of an area or land, instead of on a functional basis as previously. Theme parks have already had a vice-president overseeing them. Disney Springs and Disney Sports were also affected. Now hotel general managers manage a single hotel instead of some managing multiple hotels.[12]\\r\\nOn October 18, 2017, it was announced that resort visitors could bring dogs to Disneys Yacht Club Resort, Disney Port Orleans Resort ÿ Riverside, Disneys Art of Animation Resort and Disneys Fort Wilderness Resort & Campground.[13]\\r\\nThe resort has a number of expansion projects planned or ongoing, including:\\r\\nThe Florida resort is not within Orlando city limits but is southwest of Downtown Orlando. Much of the resort is in southwestern Orange County, with the remainder in adjacent Osceola County. The property includes the cities of Lake Buena Vista and Bay Lake which are governed by the Reedy Creek Improvement District. The site is accessible from Central Florida's Interstate 4 via Exits 62B (World Drive), 64B (US 192 West), 65B (Osceola Parkway West), 67B (SR 536 West), and 68 (SR 535 North), and Exit 8 on SR 429, the Western Expressway. At its founding, the park occupied approximately 30,500 acres (48?sq?mi; 123?km2). Portions of the property have since been sold or de-annexed, including land now occupied by the Disney-built community of Celebration. Now the resort occupies 27,258 acres (43?sq?mi; 110?km2),[18] about the size of San Francisco, or twice the size of Manhattan.\\r\\nCinderella Castle at Magic Kingdom\\r\\nSpaceship Earth at Epcot\\r\\nThe Chinese Theatre at Disney's Hollywood Studios\\r\\nTree of Life at Disney's Animal Kingdom\\r\\nDisney's property includes four golf courses. The three 18-hole golf courses are Disney's Palm (4.5 stars), Disney's Magnolia (4 stars), and Disney's Lake Buena Vista (4 stars). There is also a nine-hole walking course (no electric carts allowed) called Oak Trail, designed for young golfers. The Magnolia and Palm courses played home to the PGA Tour's Children's Miracle Network Hospitals Classic. Arnold Palmer Golf Management manages the Disney golf courses.[21]\\r\\nAdditionally, there are two themed miniature golf complexes, each with two courses, Fantasia Gardens and Winter Summerland.[22] The two courses at Fantasia Gardens are Fantasia Garden and Fantasia Fairways. The Garden course is a traditional miniature-style course based on the \\"Fantasia\\" movies with musical holes, water fountains and characters. Fantasia Fairways is a traditional golf course on miniature scale having water hazards and sand traps.[23]\\r\\nThe two courses at Winter Summerland are Summer and Winter, both themed around Santa. Summer is the more challenging of the two 18-hole courses.[23]\\r\\nOf the thirty-four resorts and hotels on the Walt Disney World property, twenty-eight are owned and operated by Walt Disney Parks, Experiences and Consumer Products. These are classified into four categories? Deluxe, Moderate, Value, and Disney Vacation Club Villas? and are located in one of five resort areas: the Magic Kingdom, Epcot, Wide World of Sports, Animal Kingdom, or Disney Springs resort areas.\\r\\nWhile all of the Deluxe resort hotels have achieved an AAA Four Diamond rating, Disney's Grand Floridian Resort & Spa is considered the highest tier flagship luxury resort on the Walt Disney World Resort complex.[26]\\r\\nThe Grand Floridian Resort & Spa, Walt Disney World's flagship resort\\r\\nDisney's Polynesian Resort, a deluxe level resort\\r\\nCaribbean Beach Resort, the first moderate resort at Walt Disney World\\r\\nFort Wilderness, Disney's campground and cabin resort\\r\\nDisney's All Star Movies Resort, one of five value resorts\\r\\nShades of Green Resort, owned and operated by the United States Military\\r\\nThe Walt Disney World Dolphin\\r\\nThe Walt Disney World Swan\\r\\nThe Hilton at Walt Disney World, located at Hotel Plaza Boulevard\\r\\nGuests with a Disney Resort reservation (excluding the Walt Disney World Swan and Dolphin) that arrive at Orlando International Airport can be transported to their resort from the airport using the complimentary Disney Magical Express service, which is operated by Mears Destination Services. Guests can also have their bags picked up and transported to their resort for them through a contract with BAGS Incorporated on participating airlines. Many resorts feature Airline Check-in counters for guests returning to the airport. Here their bags will be checked all the way through to their final destination and they can also have boarding passes printed for them. Some participating airlines are Delta, United, American, Jet Blue, and Alaska Airlines.\\r\\nIn 2014, the resort's four theme parks all ranked in the top 8 on the list of the 25 most visited theme parks in the world; (1st) Magic Kingdom - 19,332,000 visitors, (6th) Epcot - 11,454,000 visitors, (7th) Disney's Animal Kingdom - 10,402,000 visitors, and (8th) Disney's Hollywood Studios - 10,312,000 visitors.[28]\\r\\nThe Walt Disney World Resort is serviced by Disney Transport, a complimentary mass transportation system allowing guest access across the property. The Walt Disney World Monorail System provides free transportation at Walt Disney World. Guests can aboard the monorail from select on property resorts including The Grand Floridian and The Polynesian. The system operates on three routes that interconnect at the Transportation and Ticket Center (TTC), adjacent to the Magic Kingdom's parking lot. A fleet of Disney-operated buses on property, branded Disney Transport, is also complimentary for guests.\\r\\nDisney Transport also operates a fleet of watercraft, ranging in size from water taxis, up to the ferries that connect the Magic Kingdom to the Transportation and Ticket Center. Disney Transport is also responsible for maintaining the fleet of parking lot trams that are used for shuttling visitors between the various theme park parking lots and their respective main entrances.\\r\\nWalt Disney World previously had its own small airport: the Walt Disney World Airport which was also known as the Lake Buena Vista STOLport. During the early 1970s, scheduled passenger service was operated by Shawnee Airlines with small de Havilland Canada DHC-6 Twin Otter commuter turboprops which had STOL (short take off and landing) capabilities on flights to Tampa and Orlando.[38][39] The airport is no longer in operation.\\r\\nWhen the Magic Kingdom opened in 1971, the site employed about 5,500 \\"cast members\\".[40] Today, Walt Disney World employs more than 74,000 cast members,[41] spending more than $1.2 billion on payroll and $474 million on benefits each year. The largest single-site employer in the United States,[42][43] Walt Disney World has more than 3,700 job classifications. The resort also sponsors and operates the Walt Disney World College Program, an internship program that offers American college students (CP's) the opportunity to live about 15 miles (24?km) off-site in four Disney-owned apartment complexes and work at the resort, and thereby provides much of the theme park and resort \\"front line\\" cast members. There is also the Walt Disney World International College Program, an internship program that offers international college students (ICP's) from all over the world the same opportunity.\\r\\nWalt Disney Worlds corporate culture uses jargon based on theatrical terminology.[44][45] For example, park visitors are always guests, employees are called cast members, rides are attractions or adventures, cast members costumed as famous Disney characters in a way that does not cover their faces are known as face characters, jobs are roles, and public and nonpublic areas are respectively labeled onstage and backstage.[44][45]\\r\\nDisney's security personnel are generally dressed in typical security guard uniforms, though some of the personnel are dressed as tourists in plain clothes. Since September 11, 2001, uniformed security has been stationed just outside each Disney park in Florida to search guests' bags as they enter the parks.\\r\\nThe land where Walt Disney World resides is part of the Reedy Creek Improvement District (RCID), a governing jurisdiction created in 1967 by the State of Florida at the request of Disney. RCID provides 911 services, fire, environmental protection, building code enforcement, utilities and road maintenance but does not provide law enforcement services. The approximately 800 security staff are instead considered employees of the Walt Disney Company. Arrests and citations are issued by the Florida Highway Patrol along with the Orange County and Osceola County sheriffs deputies who patrol the roads. Disney security does maintain a fleet of security vans equipped with flares, traffic cones, and chalk commonly used by police officers. These security personnel are charged with traffic control by the RCID and may only issue personnel violation notices to Disney and RCID employees, not the general public.[46][47]\\r\\nDespite the appearance of the uniformed security personnel, they are not considered a legal law enforcement agency. Disney and The Reedy Creek Improvement District were sued for access to Disney Security records by Bob and Kathy Sipkema following the death of their son at the resort in 1994. The court characterized Disney security as a \\"night watchman\\" service not a law enforcement agency and was not subject to Florida's open records laws. An appeals court later upheld the lower court's ruling.[48]\\r\\nIn late 2015, Disney confirmed the addition of randomized secondary screenings and dogs trained to detect body-worn explosives within parks, in addition to metal detectors at entrances. It has also increased the number of uniformed security personnel at Walt Disney World and Disneyland properties.[49]\\r\\nDisney Security personnel in Florida have investigated traffic accidents and issued accident reports. The forms used by Disney Security may be confused with official, government forms by some.[citation needed]\\r\\nThe Orange County Sheriff maintains an office on Disney property, but this is primarily to process guests accused of shoplifting by Disney security personnel.[50]","input":"How many walt disney world hotels are there?"},{"output":"by the 20 member clubs","context":"The Premier League is the top level of the English football league system. Contested by 20 clubs, it operates on a system of promotion and relegation with the English Football League (EFL).\\r\\nThe Premier League is a corporation in which the member clubs act as shareholders. Seasons run from August to May with each team playing 38 matches (playing each other home and away).[1] Most games are played on Saturday and Sunday afternoons. It is often known outside England as the English Premier League (EPL).\\r\\nThe competition was formed as the FA Premier League on 20 February 1992 following the decision of clubs in the Football League First Division to break away from the Football League, founded in 1888, and take advantage of a lucrative television rights deal.[2] The deal was worth S1?billion a year domestically as of 2013ÿ14, with BSkyB and BT Group securing the domestic rights to broadcast 116 and 38 games respectively.[3] The league generates ?2.2?billion per year in domestic and international television rights.[4] In 2014ÿ15, teams were apportioned revenues of S1.6?billion,[5] rising sharply to S2.4?billion in 2016ÿ17.[6]\\r\\nThe Premier League is the most-watched sports league in the world, broadcast in 212 territories to 643?million homes and a potential TV audience of 4.7?billion people.[7] In the 2014ÿ15 season, the average Premier League match attendance exceeded 36,000,[8] second highest of any professional football league behind the Bundesliga's 43,500.[9] Most stadium occupancies are near capacity.[10] The Premier League ranks third in the UEFA coefficients of leagues based on performances in European competitions over the past five seasons, as of 2016.[11]\\r\\nForty-nine clubs have competed since the inception of the Premier League in 1992. Six of them have won the title: Manchester United (13), Chelsea (5), Arsenal (3), Manchester City (3), Blackburn Rovers (1) and Leicester City (1).\\r\\n\\r\\n\\r\\nDespite significant European success in the 1970s and early 1980s, the late '80s marked a low point for English football. Stadiums were crumbling, supporters endured poor facilities, hooliganism was rife, and English clubs were banned from European competition for five years following the Heysel Stadium disaster in 1985.[12] The Football League First Division, the top level of English football since 1888, was behind leagues such as Italy's Serie A and Spain's La Liga in attendances and revenues, and several top English players had moved abroad.[13]\\r\\nBy the turn of the 1990s the downward trend was starting to reverse: at the 1990 FIFA World Cup, England reached the semi-finals; UEFA, European football's governing body, lifted the five-year ban on English clubs playing in European competitions in 1990, resulting in Manchester United lifting the UEFA Cup Winners' Cup in 1991, and the Taylor Report on stadium safety standards, which proposed expensive upgrades to create all-seater stadiums in the aftermath of the Hillsborough disaster, was published in January of that year.[14]\\r\\nThe 1980s also saw the major English clubs, led by the likes of Martin Edwards of Manchester United, Irving Scholar of Tottenham Hotspur and David Dein of Arsenal, beginning to be transformed into business ventures that applied commercial principles to the running of the clubs, which led to the increasing power of the elite clubs. By threatening to break away, the top clubs from Division One managed to increase their voting power, and took 50% share of all television and sponsorship income in 1986.[15] Revenue from television also became more important: the Football League received S6.3?million for a two-year agreement in 1986, but by 1988, in a deal agreed with ITV, the price rose to S44?million over four years with the leading clubs taking 75% of the cash.[16][17] The 1988 negotiations were conducted under the threat of ten clubs leaving to form a \\"super league\\", but were eventually persuaded to stay with the top clubs taking the lion share of the deal.[16][18][19] As stadiums improved and match attendance and revenues rose, the country's top teams again considered leaving the Football League in order to capitalise on the influx of money into the sport.[19]\\r\\nIn 1990, the managing director of London Weekend Television (LWT), Greg Dyke, met with the representatives of the \\"big five\\" football clubs in England (Manchester United, Liverpool, Tottenham, Everton and Arsenal) over a dinner.[20] The meeting was to pave the way for a break away from The Football League. Dyke believed that it would be more lucrative for LWT if only the larger clubs in the country were featured on national television and wanted to establish whether the clubs would be interested in a larger share of television rights money.[21] The five clubs decided it was a good idea and decided to press ahead with it; however, the league would have no credibility without the backing of The Football Association and so David Dein of Arsenal held talks to see whether the FA were receptive to the idea. The FA did not enjoy an amicable relationship with the Football League at the time and considered it as a way to weaken the Football League's position.[22]\\r\\nAt the close of the 1991 season, a proposal was tabled for the establishment of a new league that would bring more money into the game overall. The Founder Members Agreement, signed on 17 July 1991 by the game's top-flight clubs, established the basic principles for setting up the FA Premier League.[23] The newly formed top division would have commercial independence from The Football Association and the Football League, giving the FA Premier League licence to negotiate its own broadcast and sponsorship agreements. The argument given at the time was that the extra income would allow English clubs to compete with teams across Europe.[13] Although Dyke played a significant role in the creation of the Premier League, Dyke and ITV would lose out in the bidding for broadcast rights as BSkyB won with a bid of S304?million over five years with the BBC awarded the highlights package broadcast on Match of the Day.[20][21]\\r\\nIn 1992, the First Division clubs resigned from the Football League en masse and on 27 May 1992 the FA Premier League was formed as a limited company working out of an office at the Football Association's then headquarters in Lancaster Gate.[13] This meant a break-up of the 104-year-old Football League that had operated until then with four divisions; the Premier League would operate with a single division and the Football League with three. There was no change in competition format; the same number of teams competed in the top flight, and promotion and relegation between the Premier League and the new First Division remained the same as the old First and Second Divisions with three teams relegated from the league and three promoted.[19]\\r\\nThe league held its first season in 1992ÿ93. It was composed of 22 clubs for that season. The first Premier League goal was scored by Brian Deane of Sheffield United in a 2ÿ1 win against Manchester United.[24] The 22 inaugural members of the new Premier League were Arsenal, Aston Villa, Blackburn Rovers, Chelsea, Coventry City, Crystal Palace, Everton, Ipswich Town, Leeds United, Liverpool, Manchester City, Manchester United, Middlesbrough, Norwich City, Nottingham Forest, Oldham Athletic, Queens Park Rangers, Sheffield United, Sheffield Wednesday, Southampton, Tottenham Hotspur, and Wimbledon.[25] Luton Town, Notts County, and West Ham United were the three teams relegated from the old first division at the end of the 1991ÿ92 season, and did not take part in the inaugural Premier League season.[26]\\r\\nOne significant feature of the Premier League in the mid-2000s was the dominance of the so-called \\"Top Four\\" clubs: Arsenal, Chelsea, Liverpool and Manchester United.[27][28] During this decade, they dominated the top four spots, which came with UEFA Champions League qualification, taking all top-four places in 5 out of 6 seasons from 2003ÿ04 to 2008ÿ09 inclusive, while every season during the 2000s saw the \\"Big Four\\" always qualifying for European competition. Arsenal went as far as winning the league without losing a single game in 2003ÿ04, the only time it has ever happened in the Premier League.[29]\\r\\nDuring the 2000s, only four sides outside the \\"Top Four\\" managed to qualify for the Champions League: Leeds United (1999ÿ2000), Newcastle United (2001ÿ02 and 2002ÿ03), Everton (2004ÿ05) and Tottenham Hotspur (2009ÿ10) ÿ each occupying the final Champions League spot, with the exception of Newcastle in the 2002ÿ03 season, who finished third.\\r\\nIn May 2008 Kevin Keegan stated that \\"Top Four\\" dominance threatened the division, \\"This league is in danger of becoming one of the most boring but great leagues in the world.\\"[30] Premier League chief executive Richard Scudamore said in defence: \\"There are a lot of different tussles that go on in the Premier League depending on whether you're at the top, in the middle or at the bottom that make it interesting.\\"[31]\\r\\nBetween 2005 and 2012, there was a Premier League representative in seven of the eight Champions League finals, with only \\"Top Four\\" clubs reaching that stage. Liverpool (2005), Manchester United (2008) and Chelsea (2012) won the competition during this period, with Arsenal (2006), Liverpool (2007), Chelsea (2008) and Manchester United (2009 and 2011) all losing Champions League finals. Leeds United were the only non-\\"Top Four\\" side to reach the semi-finals of the Champions League, in the 2000ÿ01 season.\\r\\nAdditionally, between the 1999ÿ2000 and 2009ÿ10 seasons, four Premier League sides reached UEFA Cup or Europa League finals, with only Liverpool managing to win the competition in 2001. Arsenal (2000), Middlesbrough (2006) and Fulham (2010) all lost their finals.\\r\\nThe years following 2009 marked a shift in the structure of the \\"Top Four\\" with Tottenham Hotspur and Manchester City both breaking into the top four places on a regular basis.[32] In the 2009ÿ10 season, Tottenham finished fourth and became the first team to break the top four since Everton five years prior.[33] Criticism of the gap between an elite group of \\"super clubs\\" and the majority of the Premier League has continued, nevertheless, due to their increasing ability to spend more than the other Premier League clubs.[34] Since the continued presence of Manchester City and Tottenham Hotspur at the top end of the table, no side has won consecutive Premier League titles.\\r\\nBy the end of the 2009ÿ10 season, each of the \\"Top Four\\" clubs had revenues in excess of ?200?million, while Manchester City and Tottenham Hotspur had revenues of roughly ?150?million according to the Deloitte Football Money League. Manchester United had the largest revenue stream in the Premier League at ?349.8?million, while Aston Villa were the closest club to the new \\"Big Six\\" with revenues of ?109.4?million.[35]\\r\\nManchester City won the title in the 2011ÿ12 season, becoming the first club outside the \\"Big Four\\" to win since Blackburn Rovers in the 1994ÿ95 season. That season also saw two of the \\"Big Four\\" (Chelsea and Liverpool) finish outside the top four places for the first time since that season.[32] The return of Manchester City as a force in English football combined with Tottenham Hotspur finishing third and then second in the 2015ÿ16 and 2016ÿ17 seasons respectively meant that many viewed the \\"Big Four\\" as proliferating into the \\"Big Six\\".[citation needed]\\r\\nWith only four UEFA Champions League qualifying places available in the league, greater competition for qualification now exists, albeit from a narrow base of six clubs. In the following five seasons after the 2011ÿ12 campaign, Manchester United and Liverpool both found themselves outside of the top four three times while Chelsea finished 10th in the 2015ÿ16 season. Arsenal finished 5th in 2016ÿ17, ending their record of 20 consecutive top-four finishes.[36]\\r\\nIn the 2015ÿ16 season, the top four was breached by a non-Big Six side for the first time since Everton in 2005. Leicester City were the surprise winners of the league, qualifying for the Champions League as a result.[37] However, they were unable to follow up their title-winning season with another strong season ÿ finishing twelfth.[citation needed]\\r\\nOff the pitch, the \\"Big Six\\" wield financial power and influence, with these clubs arguing that they should be entitled to a greater share of revenue due to the greater stature of their clubs globally and the attractive football they aim to play.[38] Objectors argue that the egalitarian revenue structure in the Premier League helps to maintain a competitive league which is vital for its future success.[39]\\r\\nThe 2016ÿ17 Deloitte Football Money League report showed the financial disparity between the \\"Big Six\\" and the rest of the division. All of the \\"Big Six\\" had revenues greater than ?350?million, with Manchester United having the largest revenue in the league at ?676.3?million. Leicester City was the closest club to the \\"Big Six\\" in terms of revenue, recording a figure of ?271.1?million for that season ÿ helped by participation in the Champions League. The eighth largest revenue generator West Ham, who didn't play in European competition, had revenues of ?213.3?million, nearly half of the fifth largest club, Liverpool (?424.2?million).[40]\\r\\nDue to insistence by the International Federation of Association Football (FIFA), the international governing body of football, that domestic leagues reduce the number of games clubs played, the number of clubs was reduced to 20, down from 22, in 1995 when four teams were relegated from the league and only two teams promoted. The top flight had only been expanded to 22 teams at the start of the 1991ÿ92 season ÿ the year prior to the formation of the Premier League.\\r\\nOn 8 June 2006, FIFA requested that all major European leagues, including Italy's Serie A and Spain's La Liga, be reduced to 18 teams by the start of the 2007ÿ08 season. The Premier League responded by announcing their intention to resist such a reduction.[41] Ultimately, the 2007ÿ08 season kicked off again with 20 teams[42], as did the other major European leagues with 20 teams.\\r\\nThe league changed its name from the FA Premier League to simply the Premier League in 2007.[43]\\r\\nThe Football Association Premier League Ltd (FAPL)[44][45][46] is operated as a corporation and is owned by the 20 member clubs. Each club is a shareholder, with one vote each on issues such as rule changes and contracts. The clubs elect a chairman, chief executive, and board of directors to oversee the daily operations of the league.[47] The current chairman is Sir Dave Richards, who was appointed in April 1999, and the chief executive is Richard Scudamore, appointed in November 1999.[48] The former chairman and chief executive, John Quinton and Peter Leaver, were forced to resign in March 1999 after awarding consultancy contracts to former Sky executives Sam Chisholm and David Chance.[49] The Football Association is not directly involved in the day-to-day operations of the Premier League, but has veto power as a special shareholder during the election of the chairman and chief executive and when new rules are adopted by the league.[50]\\r\\nThe Premier League sends representatives to UEFA's European Club Association, the number of clubs and the clubs themselves chosen according to UEFA coefficients. For the 2012ÿ13 season the Premier League has 10 representatives in the Association: Arsenal, Aston Villa, Chelsea, Everton, Fulham, Liverpool, Manchester City, Manchester United, Newcastle United and Tottenham Hotspur.[51] The European Club Association is responsible for electing three members to UEFA's Club Competitions Committee, which is involved in the operations of UEFA competitions such as the Champions League and UEFA Europa League.[52]\\r\\nThere are 20 clubs in the Premier League. During the course of a season (from August to May) each club plays the others twice (a double round-robin system), once at their home stadium and once at that of their opponents', for a total of 38 games. Teams receive three points for a win and one point for a draw. No points are awarded for a loss. Teams are ranked by total points, then goal difference, and then goals scored. If still equal, teams are deemed to occupy the same position. If there is a tie for the championship, for relegation, or for qualification to other competitions, a play-off match at a neutral venue decides rank.[53] The three lowest placed teams are relegated into the EFL Championship, and the top two teams from the Championship, together with the winner of play-offs involving the third to sixth placed Championship clubs, are promoted in their place.[54]\\r\\nAs of the 2009ÿ10 season qualification for the UEFA Champions League changed, the top four teams in the Premier League qualify for the UEFA Champions League, with the top three teams directly entering the group stage. Previously only the top two teams qualified automatically. The fourth-placed team enters the Champions League at the play-off round for non-champions and must win a two-legged knockout tie in order to enter the group stage.[55]\\r\\nThe team placed fifth in the Premier League automatically qualifies for the UEFA Europa League, and the sixth and seventh-placed teams can also qualify, depending on the winners of the two domestic cup competitions i.e. the FA Cup and the EFL Cup. Two Europa League places are reserved for the winners of each tournament; if the winner of either the FA Cup or EFL Cup qualifies for the Champions League, then that place will go to the next-best placed finisher in the Premier League.[56][57]\\r\\nAn exception to the usual European qualification system happened in 2005, after Liverpool won the Champions League the year before, but did not finish in a Champions League qualification place in the Premier League that season. UEFA gave special dispensation for Liverpool to enter the Champions League, giving England five qualifiers.[58] UEFA subsequently ruled that the defending champions qualify for the competition the following year regardless of their domestic league placing. However, for those leagues with four entrants in the Champions League, this meant that if the Champions League winner finished outside the top four in its domestic league, it would qualify at the expense of the fourth-placed team in the league. At that time, no association could have more than four entrants in the Champions League.[59] This occurred in 2012, when Chelsea ÿ who had won the Champions League that summer, but finished sixth in the league ÿ qualified for the Champions League in place of Tottenham Hotspur, who went into the Europa League.[60]\\r\\nStarting with the 2015ÿ16 season, the Europa League champion automatically qualifies for the following season's Champions League, and the maximum number of Champions League places for any single association has increased to five. An association with four Champions League places, such as The FA, will only earn a fifth place if a club from that association that does not qualify for the Champions League through its league wins either the Champions League or Europa League.[61]\\r\\nIn 2007, the Premier League became the highest ranking European League based on the performances of English teams in European competitions over a five-year period. This broke the eight-year dominance of the Spanish league, La Liga.[62]\\r\\nBetween the 1992ÿ93 and the 2016ÿ17 seasons, Premier League clubs won the UEFA Champions League four times (and had five runners-up), behind Spain's La Liga with ten wins, and Italy's Serie A with five wins; ahead of, among others, Germany's Bundesliga with three wins. The FIFA Club World Cup (originally called the FIFA Club World Championship) has been won once by a Premier League club (Manchester United in 2008),[63] with two runners-up (Liverpool in 2005, Chelsea in 2012),[64][65] behind Spain's La Liga with five wins,[66][67] Brazil's Brasileir?o with four wins,[64][65][68][69] and Italy's Serie A with two wins[70][71] (see table here).\\r\\nA system of promotion and relegation exists between the Premier League and the EFL Championship. The three lowest placed teams in the Premier League are relegated to the Championship, and the top two teams from the Championship promoted to the Premier League,[72] with an additional team promoted after a series of play-offs involving the third, fourth, fifth and sixth placed clubs. The Premier League had 22 teams when it began in 1992, but this was reduced to the present 20-team format in 1995.\\r\\nA total of 49 clubs have played in the Premier League from its inception in 1992, up to and including the 2017ÿ18 season.[73]\\r\\nThe following 20 clubs will compete in the Premier League during the 2017ÿ18 season.\\r\\na: Founding member of the Premier League\\r\\nb: Never been relegated from Premier League\\r\\nc: One of the original 12 Football League teams\\r\\nd: Club based in Wales\\r\\nIn 2011, a Welsh club participated in the Premier League for the first time after Swansea City gained promotion.[74][75] The first Premier League match to be played outside England was Swansea City's home match at the Liberty Stadium against Wigan Athletic on 20 August 2011.[76] In 2012ÿ13, Swansea qualified for the Europa League by winning the League Cup.[77] The number of Welsh clubs in the Premier League increased to two for the first time in 2013ÿ14, as Cardiff City gained promotion,[78] but they were relegated after their maiden season.[79]\\r\\nBecause they are members of the Football Association of Wales (FAW), the question of whether clubs like Swansea should represent England or Wales in European competitions has caused long-running discussions in UEFA. Swansea took one of England's three available places in the Europa League in 2013ÿ14 by winning the League Cup in 2012ÿ13. The right of Welsh clubs to take up such English places was in doubt until UEFA clarified the matter in March 2012, allowing them to participate.[80]\\r\\nParticipation in the Premier League by some Scottish or Irish clubs has sometimes been discussed, but without result. The idea came closest to reality in 1998, when Wimbledon received Premier League approval to relocate to Dublin, Ireland, but the move was blocked by the Football Association of Ireland.[81][82][83][84] Additionally, the media occasionally discusses the idea that Scotland's two biggest teams, Celtic and Rangers, should or will take part in the Premier League, but nothing has come of these discussions.[85]\\r\\nFrom 1993 to 2016, the Premier League had title sponsorship rights sold to two companies, which were Carling Brewery and Barclays Bank PLC; Barclays was the most recent title sponsor, having sponsored the Premier League from 2001 through 2016 (until 2004, the title sponsorship was held through its Barclaycard brand before shifting to its main banking brand in 2004).[86]\\r\\nBarclays' deal with the Premier League expired at the end of the 2015ÿ16 season. The FA announced on 4 June 2015 that it would not pursue any further title sponsorship deals for the Premier League, arguing that they wanted to build a \\"clean\\" brand for the competition more in line with those of major U.S. sports leagues.[88]\\r\\nAs well as sponsorship for the league itself, the Premier League has a number of official partners and suppliers.[89] The official ball supplier for the league is Nike who have had the contract since the 2000ÿ01 season when they took over from Mitre.[90]\\r\\nThe Premier League has the highest revenue of any football league in the world, with total club revenues of ?2.48?billion in 2009ÿ10.[91][92] In 2013ÿ14, due to improved television revenues and cost controls, the Premier League had net profits in excess of S78?million, exceeding all other football leagues.[93] In 2010 the Premier League was awarded the Queen's Award for Enterprise in the International Trade category for its outstanding contribution to international trade and the value it brings to English football and the United Kingdom's broadcasting industry.[94]\\r\\nThe Premier League includes some of the richest football clubs in the world. Deloitte's \\"Football Money League\\" listed seven Premier League clubs in the top 20 for the 2009ÿ10 season,[95] and all 20 clubs were in the top 40 globally by the end of the 2013ÿ14 season, largely as a result of increased broadcasting revenue.[96] From 2013, the league generates ?2.2?billion per year in domestic and international television rights.[4]\\r\\nPremier League clubs agreed in principle in December 2012, to radical new cost controls. The two proposals consist of a break-even rule and a cap on the amount clubs can increase their wage bill by each season. With the new television deals on the horizon, momentum has been growing to find ways of preventing the majority of the cash going straight to players and agents.[97]\\r\\nCentral payments for the 2016ÿ17 season amounted to S2,398,515,773 across the 20 clubs, with each team receiving a flat participation fee of S35,301,989 and additional payments for TV broadcasts (S1,016,690 for general UK rights to match highlights, S1,136,083 for each live UK broadcast of their games and S39,090,596 for all overseas rights), commercial rights (a flat fee of S4,759,404) and a notional measure of \\"merit\\" which was based upon final league position.[6] The merit component was a nominal sum of S1,941,609 multiplied by each finishing place, counted from the foot of the table (e.g., Burnley finished 16th in May 2017, five places counting upwards, and received 5 G S1,941,609 = S9,708,045 merit payment).[6]\\r\\nTelevision has played a major role in the history of the Premier League. The League's decision to assign broadcasting rights to BSkyB in 1992 was at the time a radical decision, but one that has paid off. At the time pay television was an almost untested proposition in the UK market, as was charging fans to watch live televised football. However, a combination of Sky's strategy, the quality of Premier League football and the public's appetite for the game has seen the value of the Premier League's TV rights soar.[17]\\r\\nThe Premier League sells its television rights on a collective basis. This is in contrast to some other European Leagues, including La Liga, in which each club sells its rights individually, leading to a much higher share of the total income going to the top few clubs.[98] The money is divided into three parts:[99] half is divided equally between the clubs; one quarter is awarded on a merit basis based on final league position, the top club getting twenty times as much as the bottom club, and equal steps all the way down the table; the final quarter is paid out as facilities fees for games that are shown on television, with the top clubs generally receiving the largest shares of this. The income from overseas rights is divided equally between the twenty clubs.[100]\\r\\nThe first Sky television rights agreement was worth S304?million over five seasons.[101] The next contract, negotiated to start from the 1997ÿ98 season, rose to S670?million over four seasons.[101] The third contract was a S1.024?billion deal with BSkyB for the three seasons from 2001ÿ02 to 2003ÿ04. The league brought in S320?million from the sale of its international rights for the three-year period from 2004ÿ05 to 2006ÿ07. It sold the rights itself on a territory-by-territory basis.[102] Sky's monopoly was broken from August 2006 when Setanta Sports was awarded rights to show two out of the six packages of matches available. This occurred following an insistence by the European Commission that exclusive rights should not be sold to one television company. Sky and Setanta paid a total of S1.7?billion, a two-thirds increase which took many commentators by surprise as it had been widely assumed that the value of the rights had levelled off following many years of rapid growth. Setanta also hold rights to a live 3?pm match solely for Irish viewers. The BBC has retained the rights to show highlights for the same three seasons (on Match of the Day) for S171.6?million, a 63?per?cent increase on the S105?million it paid for the previous three-year period.[103] Sky and BT have agreed to jointly pay S84.3?million for delayed television rights to 242 games (that is the right to broadcast them in full on television and over the internet) in most cases for a period of 50 hours after 10?pm on matchday.[104] Overseas television rights fetched S625?million, nearly double the previous contract.[105] The total raised from these deals is more than S2.7?billion, giving Premier League clubs an average media income from league games of around S40?million-a-year from 2007 to 2010.[106]\\r\\nThe TV rights agreement between the Premier League and Sky has faced accusations of being a cartel, and a number of court cases have arisen as a result.[107] An investigation by the Office of Fair Trading in 2002 found BSkyB to be dominant within the pay TV sports market, but concluded that there were insufficient grounds for the claim that BSkyB had abused its dominant position.[108] In July 1999 the Premier League's method of selling rights collectively for all member clubs was investigated by the UK Restrictive Practices Court, who concluded that the agreement was not contrary to the public interest.[109]\\r\\nThe BBC's highlights package on Saturday and Sunday nights, as well as other evenings when fixtures justify, will run until 2016.[110] Television rights alone for the period 2010 to 2013 have been purchased for S1.782?billion.[111] On 22 June 2009, due to troubles encountered by Setanta Sports after it failed to meet a final deadline over a S30?million payment to the Premier League, ESPN was awarded two packages of UK rights containing a total of 46 matches that were available for the 2009ÿ10 season as well as a package of 23 matches per season from 2010ÿ11 to 2012ÿ13.[112] On 13 June 2012, the Premier League announced that BT had been awarded 38 games a season for the 2013ÿ14 through 2015ÿ16 seasons at S246?million-a-year. The remaining 116 games were retained by Sky who paid S760?million-a-year. The total domestic rights have raised S3.018?billion, an increase of 70.2% over the 2010ÿ11 to 2012ÿ13 rights.[113] The value of the licensing deal rose by another 70.2% in 2015, when Sky and BT paid a total of S5.136?billion to renew their contracts with the Premier League for another three years up to the 2018ÿ19 season.[114]\\r\\nUK highlights\\r\\nBetween the 1998ÿ99 season and the 2012ÿ13 season, RT broadcast highlights on Premier Soccer Saturday and occasionally Premier Soccer Sunday. During then between the 2004ÿ05 season and the 2006ÿ07 season, RT broadcast 15 live matches on a Saturday afternoon with each match being called Premiership Live.\\r\\nIn August 2016, it was announced that the BBC would be creating a new magazine-style show for the Premier League entitled The Premier League Show.[115]\\r\\nThe Premier League is the most-watched football league in the world, broadcast in 212 territories to 643?million homes and a potential TV audience of 4.7?billion people,[7]. The Premier League's production arm, Premier League Productions, is operated by IMG Productions and produces all content for its international television partners.\\r\\nThe Premier League is particularly popular in Asia, where it is the most widely distributed sports programme.[116] In Australia, Optus telecommunications holds exclusive rights to the Premier League, providing live broadcasts and online access (Fox Sports formerly held rights).[117] In India, the matches are broadcast live on STAR Sports. In China, the broadcast rights were awarded to Super Sports in a six-year agreement that began in the 2013ÿ14 season.[118] As of the 2013ÿ14 season, Canadian broadcast rights to the Premier League are jointly owned by Sportsnet and TSN, with both rival networks holding rights to 190 matches per season.[119]\\r\\nThe Premier League is broadcast in the United States through NBC Sports.[120] Premier League viewership has increased rapidly, with NBC and NBCSN averaging a record 479,000 viewers in the 2014ÿ15 season, up 118% from 2012ÿ13 when coverage still aired on Fox Soccer and ESPN/ESPN2 (220,000 viewers),[121] and NBC Sports has been widely praised for its coverage.[121][122][123] NBC Sports reached a six-year extension with the Premier League in 2015 to broadcast the league through the 2021ÿ22 season in a deal valued at $1?billion (S640?million).[124][125]\\r\\nThere has been an increasing gulf between the Premier League and the Football League. Since its split with the Football League, many established clubs in the Premier League have managed to distance themselves from their counterparts in lower leagues. Owing in large part to the disparity in revenue from television rights between the leagues,[126] many newly promoted teams have found it difficult to avoid relegation in their first season in the Premier League. In every season except 2001ÿ02 and 2011ÿ12, at least one Premier League newcomer has been relegated back to the Football League. In 1997ÿ98 all three promoted clubs were relegated at the end of the season.[127]\\r\\nThe Premier League distributes a portion of its television revenue to clubs that are relegated from the league in the form of \\"parachute payments\\". Starting with the 2013ÿ14 season, these payments are in excess of S60?million over four seasons.[128] Though designed to help teams adjust to the loss of television revenues (the average Premier League team receives S55?million[129] while the average Football League Championship club receives S2?million),[130] critics maintain that the payments actually widen the gap between teams that have reached the Premier League and those that have not,[131] leading to the common occurrence of teams \\"bouncing back\\" soon after their relegation. For some clubs who have failed to win immediate promotion back to the Premier League, financial problems, including in some cases administration or even liquidation have followed. Further relegations down the footballing ladder have ensued for several clubs unable to cope with the gap.[132][133]\\r\\nAs of the 2017ÿ18 season, Premier League football has been played in 58 stadiums since the formation of the division.[134] The Hillsborough disaster in 1989 and the subsequent Taylor Report saw a recommendation that standing terraces should be abolished; as a result all stadiums in the Premier League are all-seater.[135][136] Since the formation of the Premier League, football grounds in England have seen constant improvements to capacity and facilities, with some clubs moving to new-build stadiums.[137] Nine stadiums that have seen Premier League football have now been demolished. The stadiums for the 2017ÿ18 season show a large disparity in capacity: Wembley Stadium, the temporary home of Tottenham Hotspur, has a capacity of 90,000 with Dean Court, the home of Bournemouth, having a capacity of 11,360.[138][139] The combined total capacity of the Premier League in the 2017ÿ18 season is 806,033 with an average capacity of 40,302.[138]\\r\\nStadium attendances are a significant source of regular income for Premier League clubs.[140] For the 2016ÿ17 season, average attendances across the league clubs were 35,838 for Premier League matches with a total aggregate attendance figure of 13,618,596.[141] This represents an increase of 14,712 from the average attendance of 21,126 recorded in the league's first season (1992ÿ93).[142] However, during the 1992ÿ93 season the capacities of most stadiums were reduced as clubs replaced terraces with seats in order to meet the Taylor Report's 1994ÿ95 deadline for all-seater stadiums.[143][144] The Premier League's record average attendance of 36,144 was set during the 2007ÿ08 season.[145] This record was then beaten in the 2013ÿ14 season recording an average attendance of 36,695 with a total attendance of just under 14 million, the highest average in England's top flight since 1950.[146]\\r\\nManagers in the Premier League are involved in the day-to-day running of the team, including the training, team selection, and player acquisition. Their influence varies from club-to-club and is related to the ownership of the club and the relationship of the manager with fans.[147] Managers are required to have a UEFA Pro Licence which is the final coaching qualification available, and follows the completion of the UEFA 'B' and 'A' Licences.[148] The UEFA Pro Licence is required by every person who wishes to manage a club in the Premier League on a permanent basis (i.e. more than 12 weeks ÿ the amount of time an unqualified caretaker manager is allowed to take control).[149] Caretaker appointments are managers that fill the gap between a managerial departure and a new appointment. Several caretaker managers have gone on to secure a permanent managerial post after performing well as a caretaker; examples include Paul Hart at Portsmouth and David Pleat at Tottenham Hotspur.\\r\\nThe league's longest-serving manager was Alex Ferguson, who was in charge of Manchester United from November 1986 until his retirement at the end of the 2012ÿ13 season, meaning that he was manager for all of the first 21 seasons of the Premier League. Arsne Wenger is the league's longest-serving current manager, having been in charge of Arsenal in the Premier League since 1996.[150] As of the 22nd game-week of the 2017/2018 season, 7 managers have been sacked, the most recent being?Mark Hughes?of?Stoke City.[151]\\r\\nThere have been several studies into the reasoning behind, and effects of, managerial sackings. Most famously, Professor Sue Bridgewater of the?University of Liverpool?and Dr. Bas ter Weel of the?University of Amsterdam, performed two separate studies which helped to explain the statistics behind managerial sackings. Bridgewater's study found that clubs generally sack their managers upon dropping below an average of 1 point-per-game.[152]\\r\\nAt the inception of the Premier League in 1992ÿ93, just eleven players named in the starting line-ups for the first round of matches hailed from outside of the United Kingdom or Ireland.[154] By 2000ÿ01, the number of foreign players participating in the Premier League was 36?per?cent of the total. In the 2004ÿ05 season the figure had increased to 45?per?cent. On 26 December 1999, Chelsea became the first Premier League side to field an entirely foreign starting line-up,[155] and on 14 February 2005 Arsenal were the first to name a completely foreign 16-man squad for a match.[156] By 2009, under 40% of the players in the Premier League were English.[157]\\r\\nIn response to concerns that clubs were increasingly passing over young English players in favour of foreign players, in 1999, the Home Office tightened its rules for granting work permits to players from countries outside of the European Union.[158] A non-EU player applying for the permit must have played for his country in at least 75?per?cent of its competitive 'A' team matches for which he was available for selection during the previous two years, and his country must have averaged at least 70th place in the official FIFA world rankings over the previous two years. If a player does not meet those criteria, the club wishing to sign him may appeal.[159]\\r\\nPlayers may only be transferred during transfer windows that are set by the Football Association. The two transfer windows run from the last day of the season to 31 August and from 31 December to 31 January. Player registrations cannot be exchanged outside these windows except under specific licence from the FA, usually on an emergency basis.[160] As of the 2010ÿ11 season, the Premier League introduced new rules mandating that each club must register a maximum 25-man squad of players aged over 21, with the squad list only allowed to be changed in transfer windows or in exceptional circumstances.[161][162] This was to enable the 'home grown' rule to be enacted, whereby the League would also from 2010 require at least 8 of the named 25 man squad to be made up of 'home-grown players'.[161]\\r\\nThere is no team or individual salary cap in the Premier League. As a result of the increasingly lucrative television deals, player wages rose sharply following the formation of the Premier League when the average player wage was S75,000 per year.[163] The average salary stands at S1.1?million as of the 2008ÿ09 season.[164] As of 2015, average salaries in the Premier League are higher than for any other football league in the world.[165]\\r\\nThe record transfer fee for a Premier League player has risen steadily over the lifetime of the competition. Prior to the start of the first Premier League season Alan Shearer became the first British player to command a transfer fee of more than S3?million.[166] The record rose steadily in the Premier League's first few seasons, until Alan Shearer made a record breaking S15?million move to Newcastle United in 1996.[166] All three of the most expensive transfers in the sport's history had a Premier League club on the selling or buying end, with Juventus selling Paul Pogba to Manchester United in August 2016 for a fee of S89?million, Tottenham Hotspur selling Gareth Bale to Real Madrid for S85?million in 2013,[167] Manchester United's sale of Cristiano Ronaldo to Real Madrid for S80?million in 2009,[168] and Liverpool selling Luis Surez to Barcelona for S75?million in 2014.[169]\\r\\nItalics denotes players still playing professional football,\\r\\nBold denotes players still playing in the Premier League.\\r\\nThe Golden Boot is awarded to the top Premier League scorer at the end of each season. Former Blackburn Rovers and Newcastle United striker Alan Shearer holds the record for most Premier League goals with 260.[171] Twenty-five players have reached the 100-goal mark.[172] Since the first Premier League season in 1992ÿ93, 14 different players from 10 different clubs have won or shared the top scorers title.[173] Thierry Henry won his fourth overall scoring title by scoring 27 goals in the 2005ÿ06 season. Andrew Cole and Alan Shearer hold the record for most goals in a season (34) ÿ for Newcastle and Blackburn respectively.[174] Ryan Giggs of Manchester United holds the record for scoring goals in consecutive seasons, having scored in the first 21 seasons of the league.[175]\\r\\nThe Premier League maintains two trophies ÿ the genuine trophy (held by the reigning champions) and a spare replica. Two trophies are held in the event that two different clubs could win the League on the final day of the season.[176] In the rare event that more than two clubs are vying for the title on the final day of the season ÿ then a replica won by a previous club is used.[177]\\r\\nThe current Premier League trophy was created by Royal Jewellers Asprey of London. It consists of a trophy with a golden crown and a malachite plinth base. The plinth weighs 33 pounds (15?kg) and the trophy weighs 22 pounds (10.0?kg).[178] The trophy and plinth are 76?cm (30?in) tall, 43?cm (17?in) wide and 25?cm (9.8?in) deep.[179]\\r\\nIts main body is solid sterling silver and silver gilt, while its plinth is made of malachite, a semi-precious stone. The plinth has a silver band around its circumference, upon which the names of the title-winning clubs are listed. Malachite's green colour is also representative of the green field of play.[179] The design of the trophy is based on the heraldry of Three Lions that is associated with English football. Two of the lions are found above the handles on either side of the trophy ÿ the third is symbolised by the captain of the title-winning team as he raises the trophy, and its gold crown, above his head at the end of the season.[180] The ribbons that drape the handles are presented in the team colours of the league champions that year.\\r\\nIn 2004, a special gold version of the trophy was commissioned to commemorate Arsenal winning the title without a single defeat.[181]\\r\\nIn addition to the winner's trophy and the individual winner's medals awarded to players, the Premier League also awards the monthly Manager of the Month, Player of the Month and Goal of the Month awards,[182] as well as annual awards for Manager of the Season,[183] Player of the Season,[184] Golden Boot and the Golden Glove awards.[185]. From the 2017/18 season, players also receive a milestone award for 100 appearances and every century there after and also players who score 50 goals and multiples thereof. Each player to reach these milestones will receive a presentation box from the Premier League containing a special medallion and a plaque commemorating their achievement[186].\\r\\nIn 2012, the Premier League celebrated its second decade by holding the 20 Seasons Awards:[187]","input":"Who is in charge of the premier league?"},{"output":"Melvil Dewey","context":"Education for librarianship is the term for the educational preparation for professional librarians. This varies widely in different countries. In the United States and Canada, it generally consists of a master's degree program in library science. There are also bachelor's, associate, and certificate programs in library science, which provide formal training of paraprofessional library workers, library technicians, and clerksas well as preparation for graduate study in library science.\\r\\n\\r\\n\\r\\nUntil the 19th century, the librarian in charge of an academic collection was normally a scholar, often a university professor with a special interest in the library. There were no training programs, and the new librarian was expected to follow the practices of other similar libraries. (Popular libraries in the modern sense had not yet developed.) In the 19th century, although some librarians followed this older pattern, others prepared as apprentices under the direction of established librarians.\\r\\n\\"Library school\\" is a term which has been used to describe an institution of higher learning specializing in the professional training of librarians. The first library school in the United States was established by Melvil Dewey (the originator of the Dewey decimal system) in 1887 at Columbia University.[1] Since then many library schools have been founded in the United States and Canada,[2] with Canada's first formal librarianship program established at McGill University in 1904.[3] The development of library schools in other countries began in 1915, when librarians' schools were founded at Leipzig and Barcelona (currently, as a faculty of the Universitat de Barcelona, the latter is the oldest library school in Europe). Many others were founded during World War II. The University of Chicago Graduate Library School became the first library school to confer a master's degree in library science, which is now the standard professional degree, and later became the first to give a doctoral degree in the field. Other prominent American library schools are located at the University of Illinois at Urbana-Champaign and at the University of North Carolina at Chapel Hill.\\r\\nMost library schools in North America offer graduate programs only. Accreditation of these programs is granted by the American Library Association. The bachelor's degree in Library Science (or Library Economy as it was called in early days) was, for the most part, phased out several decades ago.[4] Librarians in North America typically earn a master's degree, typically the Master of Library Science (MLS) or the Master of Library and Information Science (MLIS). This degree allows one to work as a practicing librarian in public libraries, academic libraries, school library media centers, and special libraries, while many individuals with the MLS credential work with major library vendors. The degree is also applicable to related sectors such as publishing.\\r\\nMaster of Library Science programs are typically structured to offer a combination of required and elective courses in library science and information science. The required courses focus on core library skills such as cataloging, reference, collection development as well as related areas such as the philosophy underlying the profession, information technology and management. Elective courses may include information management, children's literature, genealogy and archives as well as specialized courses related to different types of libraries.\\r\\nIn recent decades, many schools offering librarianship education have changed their names to reflect the shift from print media to electronic media, and to information contained outside of traditional libraries. Some call themselves schools of library and information science (abbreviated to \\"SLIS\\", hence the term \\"SLISters\\" for their students), while others may have dropped the word \\"library\\" altogether. This trend began as early as the 1960s with the recognition that information and access to it was shifting to electronic resources with the development of telecommunications and computer networks and away from the traditional definition of librarianship. This shift led a number of library schools to change or broaden their mission to be more inclusive of information sciences across many disciplines including library sciences, archives, computer sciences and more, and led to the development by a number of schools of an iSchool organization, [5] to advance the field of information as a whole.[6]\\r\\nThe normal preparation for a faculty member in a department of library science (or other name) is a Ph.D. in Library science or Information science. In some fields of librarianship, a Ph.D. in another related subject, such as archival studies, is the equivalent, and some faculty have doctorates in various subject fields, as well as an MLS (or similar) degree.\\r\\nIn the United States and Canada, a professional librarian normally has a one or two-year master's degree in library and information science, library science or information science with abbreviations such as MLS, MSLS, MIS, MS-LIS, MISt, MI, MLIS, or MILS. Many professional librarians have degrees obtained from programs accredited by the American Library Association (ALA) and can have specializations within fields such as archives, records management, information architecture, information policy, knowledge management, public librarianship, medical librarianship, law librarianship, special librarianship, academic librarianship, or school (K-12) librarianship. School librarians often are required to have a teaching credential and school librarian license in addition to a library science degree. Master's degree programs for school library media specialist initial preparation are also accredited by the National Council for Accreditation of Teacher Education (NCATE), which ALA recognizes. Many, if not most, academic librarians also have a second, subject-based master's degree.\\r\\nThe early history of education for librarians in the U.S. has been explored by Churchwell.[7] The University of Chicago studies in library science assessed the state of education for librarians in 1948.[8] The pivotal role of the first doctoral degree at the University of Chicago Graduate Library School from 1921-1951 has been analyzed by John Richardson in his study, The Spirit of Inquiry.[9] Many faculty members at the University of Chicago, Graduate Library School (1928-1979) were at the forefront of the field's development in the twentieth century: Lester Asheim, Lee Pierce Butler, Leon Carnovsky, Herman H. Fussler, Frances E. Henne, Carleton B. Joeckel, Jesse Shera, Peggy Sullivan, Douglas Waples, Louis Round Wilson, Howard Winger, and Robert Wadsworth. The Library Quarterly was first published by the Graduate Library School in 1931.\\r\\nAt the turn of the millennium (1999ÿ2000), problems related to the graduate education of professional librarians pervaded professional and academic discourse. These were initially identified by the Council of the American Library Association as the growing elimination of the word \\"library\\" from the names of schools, the seeming lack of attention to core competencies (cataloguing was often mentioned), and the national shortage of professionals to work with particular groups (specifically young people in public libraries and disadvantaged populations), and in particular environments (such as schools). The Final Report of the Steering Committee on the Congress for Professional Education provides an analysis of these issues. The Coalition on Reinventing Information Science, Technology, and Literary Education supported by the Kellogg Foundation provided additional analysis of future educational needs and direction.[10] Bernie Sloan compiled an extensive 2004 bibliography on changes in LIS education.[11]\\r\\nDistinguished service to education for librarianship in the U.S. and Canada is recognized by the annual Beta Phi Mu Award sponsored by the International Honorary Society, Beta Phi Mu.[12] The first award was made in 1954 to Rudolph Hjalmar Gjelsness Dean of the University of Michigan's Library Science Department from 1940 to 1964.\\r\\nThe primary association for faculty teaching in library and information science programs is the Association for Library and Information Science Education.\\r\\nIn academic regalia in the United States, the color for library science is lemon.\\r\\nIn the UK and some other countries, a librarian can have a three- or four-year bachelor's degree in library and information studies or information science; separate master's degrees in librarianship, archive management, and records management are also available. In the United Kingdom, these degrees are accredited by the Chartered Institute of Library and Information Professionals and the Society of Archivists.\\r\\nIn Germany and some other countries, the first step for an academic librarian is a Ph.D. in a subject field, followed by additional training in librarianship.\\r\\nIn Denmark the first step to become a librarian is a 3-year long bachelor's degree in Library and Information Science (B.Sc.) at The Royal School of Library and Information Science. The students then have the choice between taking a half-year-long education for librarianship called Librarian D.B or take a 2-year master's degree called Master of Library and Information Science (M.L.I.Sc.). All the already mentioned courses takes place in Danish but The Royal School of Library and Information Science also offers an English-speaking 2-year master's degree called Master of Library and Information Science (M.L.I.Sc.). Students who complete the bachelor's degree, the librarianship or one of the master's degrees offered get work as librarians, information employees or organization staff. The students can also obtain a Ph.D. in Library and Information Science at The Royal School of Library and Information Science (first awarded at The Royal School of Library and Information Science in 2004). The students can also obtain a doctorate in Library and Information Science at The Royal School of Library and Information Science (first awarded at The Royal School of Library and Information Science in 2006).\\r\\nIn 1914, the University of the Philippines offered the first courses in Library education in the country. The University would later establish the first separate library school in the country in 1961; the Institute of Library Science, a former department of the now defunct College of Liberal Arts. Librarians usually have a four-year bachelor's degree in library and information studies, or a master's degree in LIS or one with a concentration in Library Science. It is also not uncommon for librarians to possess a degree in Education, with a specialization or major in Library Science. With passage of the Republic Act No. 6966 (Repealed in 2003 with the passing of R.A. 9246 or \\"The Philippine Librarianship Act of 2003\\") in 1990, graduates of library and information science are required to take the licensure examinations for librarians in order to practice librarianship in the Philippines or countries which have reciprocity as regards the practice of the field.[13]\\r\\nIn New Zealand, Victoria University of Wellington is the only university providing postgraduate education in librarianship. Qualifications include Master of Information Studies (course based) and Master of Arts (thesis based) as well as postgraduate diploma and certificate in librarianship. Open Polytechnic provides undergraduate librarianship education.","input":"Who established the first training program for librarians?"},{"output":"Kenneth Wayne \\"Ken\\" Jennings III","context":"Kenneth Wayne \\"Ken\\" Jennings III (born May 23, 1974) is an American game show contestant and author. Jennings holds the record for the longest winning streak on the U.S. syndicated game show Jeopardy! and as being the second highest-earning contestant in game show history. In 2004, Jennings won 74 Jeopardy! games (in a row) before he was defeated by challenger Nancy Zerg on his 75th appearance. His total earnings on Jeopardy! are $3,196,300, consisting of $2,520,700 over his 74 wins, a $2,000 second-place prize in his 75th appearance, a $500,000 second-place prize in the Jeopardy! Ultimate Tournament of Champions, a $100,000 win for second-place prize in the Jeopardy Battle of the Decades, as well as half of a $300,000 prize in the IBM Challenge, when he competed against Watson.\\r\\nDuring his first run of Jeopardy! appearances, Jennings earned the record for the highest American game show winnings. His total was surpassed by Brad Rutter, who defeated Jennings in the finals of the Jeopardy! Ultimate Tournament of Champions (first aired on May 25, 2005), adding $2,000,000 to Rutter's existing Jeopardy! winnings. Jennings regained the record after appearing on several other game shows, culminating in an appearance on Are You Smarter Than a 5th Grader? (first aired on October 10, 2008), though Rutter retained the record for highest Jeopardy! winnings and once again passed Jennings' total after his victory in the 2014 Jeopardy Battle of the Decades tournament.\\r\\nAfter his success on Jeopardy!, Jennings wrote about his experience and explored American trivia history and culture in his book Brainiac: Adventures in the Curious, Competitive, Compulsive World of Trivia Buffs, published in 2006.\\r\\n\\r\\n\\r\\nBorn in Edmonds, Washington, Jennings grew up in Seoul, South Korea (1981ÿ1992) and Singapore (1992ÿ1996), where his father worked for an international law firm and then as Asia Pacific Division Counsel of Oracle Corporation.[1] He watched Jeopardy! on American Forces Network television while growing up.[2]\\r\\nJennings went to Seoul Foreign School where he completed an International Baccalaureate diploma and then graduated with a degree in Computer Science and English at Brigham Young University, where he played on the school's quizbowl team for three and a half years. Jennings attended the University of Washington during his freshman year.[3]\\r\\nBefore 2003, Jeopardy! contestants were limited to five consecutive games. At the beginning of the show's twentieth season (in 2003), the rules were changed to allow contestants to remain on the show as long as they continued to win.[4] After this rule change, and until Jennings' run, the record winning streak was set by Tom Walsh, who won $186,900 in eight games in January 2004.\\r\\nBefore his Jeopardy! appearance, Jennings was a member of BYU's Quiz Bowl Team.[5] Jennings' run began during Jeopardy!'s 20th season with the episode aired on June 2, 2004, in which he unseated 2-time returning champion Jerry Harvey, and continued into season 21. In that first episode, Jennings's entire winning streak nearly ended before it even began. The Final Jeopardy answer was, \\"She's the first female track & field athlete to win medals in 5 different events at a single Olympics.\\" Jennings responded with \\"Who is Jones?\\", referring to Marion Jones (the shows were recorded before she was stripped of her medals as a result of admitted doping). Host Alex Trebek said, \\"We will accept that, in terms of female athletes, there aren't that many.\\" If the response had not been accepted, Jennings would have finished in 3rd place, and challenger Julia Lazarus would have won the game instead. Jennings' run was interrupted by the off-season break in July until September, 2004 Kids' Week, the Tournament of Champions (aired from September 20, 2004 through October 1, 2004), the 2004 U.S. Presidential Election (aired on Tuesday November 2, 2004, pushing his weeks of episodes to air from Wednesday to Saturday) and the College Championship (aired from November 10, 2004 through November 23, 2004). He did not participate in the Tournament of Champions, as invitations are only extended to champions who have already been defeated (with the exception of the winner(s) of the College Championship), which Jennings had not yet been.\\r\\nOn November 30, 2004, Jennings' reign as Jeopardy! champion ended when he lost his seventy-fifth game to challenger Nancy Zerg.[6] Jennings responded incorrectly to both Double Jeopardy! Daily Doubles, causing him to lose a combined $10,200 ($5,400 and $4,800, respectively) and leaving him with $14,400 at the end of the round. As a result, for only the tenth time in 75 games, Jennings did not have an insurmountable lead going into the Final Jeopardy! round.[7] Only Jennings and Zerg, who ended Double Jeopardy! with $10,000, were able to play Final Jeopardy! as third-place contestant David Hankins failed to finish with a positive score after Double Jeopardy!.\\r\\nThe Final Jeopardy! category was Business & Industry, and the clue was \\"Most of this firm's 70,000 seasonal white-collar employees work only four months a year\\". Jennings appeared perplexed during the time allowed to write a response, while Zerg finished her response quickly. Zerg responded correctly with \\"What is H&R Block?\\" and wagered $4,401 of her $10,000, giving her a $1 lead over Jennings with his response still to be revealed. Jennings incorrectly responded with \\"What is FedEx?\\", and lost the game with a final score of $8,799 after his $5,601 wager was deducted from his score. After his response was revealed to be incorrect, the audience audibly gasped. He was awarded $2,000 for his second-place finish, which gave him a final total of $2,522,700 for his run on Jeopardy!. Zerg, whom Jennings called a \\"formidable opponent\\", finished in third place on the next show. The audience gave a standing ovation in honor of both contestants, and Trebek called Zerg a \\"giant killer\\" as Jennings embraced her.\\r\\nJennings' 75 matches took place over a span of 182 calendar days.\\r\\nJeopardy! implemented some backstage changes during Jennings' run. Normally, players only get a short time to practice, but more rehearsal time was added so that the new players could get comfortable with the buzzers. Additionally, the person who managed the buzzer system was changed.[8] In his book Brainiac, Jennings says that the consistency of the original manager's timing had given an increasing advantage to continuing players, and that the change made a noticeable difference in the second season that he was on the show. At one point, announcer Johnny Gilbert stopped announcing Jennings' total wins during the show's opening.\\r\\nOn December 1, 2004, the day after his defeat, Jeopardy! broke with tradition by having Jennings make a guest appearance at the start of the broadcast, during which host Alex Trebek acknowledged his success and enumerated the various game show records he had broken.\\r\\nJennings appeared in The Guinness Book of World Records under \\"Most cash won on a game show.\\"\\r\\nAccording to the Nielsen TV National People Meter, Jeopardy!'s ratings were 22% higher during Jennings' run than they were during the same period the previous year. For several weeks of the winnings streak, Jeopardy! was ranked as TV's highest-rated syndicated program.[9] By the end of Jeopardy!'s 20th season several weeks later, the show had surpassed Wheel of Fortune in the ratings but Wheel, which is usually paired with Jeopardy! in programming, also benefited from Jennings' streak.[10]\\r\\nJennings has received a good deal of American media coverage. After his 38th win on Jeopardy!, during the summer break between tapings, Jennings made a guest appearance on Live with Regis and Kelly. There Jennings revealed that he had failed to qualify for Who Wants to Be a Millionaire, once hosted by Regis Philbin. During that guest appearance, Jennings said, \\"Jeopardy! is a man's game... it's not like Millionaire.\\"[11]\\r\\nJennings appeared on the Late Show with David Letterman to present Letterman's \\"Top Ten List\\" (Top ten ways to irritate Alex Trebek). He appeared again on the program on the night his final show was televised, in addition to interview segments airing that night on local late evening news programming and on Nightline. Barbara Walters selected Jennings as one of the \\"Ten Most Fascinating People of 2004\\" for her twelfth annual ABC News special, which aired on December 8, 2004. While on his media tour following his final game, Jennings taped a segment for Sesame Street. TV Guide featured a segment of \\"The Top Ten TV Moments of 2004\\", in which Jennings' loss placed third. On December 1, 2004, A&E aired an episode of Biography on Jennings and other Jeopardy! notables, including Frank Spangenberg and Eddie Timanus.\\r\\nOn December 28, 2004, Sony announced a 15-week, 75-show Jeopardy! Ultimate Tournament of Champions. It featured Tournament of Champions, College Championship, and Teen Tournament winners from the show's 21-year run, as well as over 100 five-time champions. Jeopardy!'s executive producer, Harry Friedman, explained:\\r\\nThe 2003 rule change, which allows Jeopardy! players to keep playing until they're defeated, raised the question about how other five-time champions might have played under this rule. This tournament is an opportunity to give those past champions another chance to shine.\\r\\nThe field totaled 145 players including Jennings, who, unlike the other competitors, was automatically placed in the finals. The Ultimate Tournament of Champions offered substantial cash prizes; with a grand prize of $2,000,000 to the winner, $500,000 for the first runner-up, and $250,000 for the second runner-up. Guaranteed prize money was offered to all contestants.\\r\\nIn the final round of the Ultimate Tournament, Brad Rutter decisively defeated Jennings and Jerome Vered, with respective final scores of $62,000, $34,599, and $20,600. Jennings won the $500,000 prize for second place, but as a result of the Ultimate Tournament, Rutter temporarily displaced him as the highest overall winner of money on a game show. Jennings has said he is still happy with his second-place finish.\\r\\nFrom February 14ÿ16, 2011, Jeopardy's \\"IBM Challenge\\" featured the computer company's Watson against Jennings and Rutter in two matches played over three days.[12] The winner of the competition was Watson, winning $1 million for two charities, while Jennings was second and Rutter was third, receiving $300,000 and $200,000, respectively. Jennings and Rutter each pledged to donate half of their winnings to charity.\\r\\nThis was the first ever man-versus-machine competition in the show's history. At the end of the first episode, in which only the first match's Jeopardy! Round was aired, Rutter was tied with Watson at $5,000, while Jennings was in third with $2,000. After the second episode in which the first game was completed, Jennings remained at third with $4,800 while Rutter at second had $10,400.[13] The competition ended with Watson with $77,147, Jennings with $24,000, and Rutter with $21,600.[14] Below his response during the Final Jeopardy! Round, Jennings wrote on his screen \\"I for one welcome our new computer overlords.\\" It was the first time Rutter had been defeated against any human player, although the defeat is not on Rutter's Jeopardy! official record, as the competition was deemed an exhibition.\\r\\nJennings wrote about playing against Watson for Slate.[15]\\r\\nIn 2014, Jeopardy! aired a special 5-week Jeopardy! Battle of the Decades tournament. Jennings made it to the finals along with Brad Rutter and Roger Craig. Jennings placed second, winning a $100,000 prize, and Rutter won first place, securing a $1,000,000 prize.\\r\\nTaking advantage of the notoriety that Jennings's losing Final Jeopardy! answer afforded, H&R Block offered Jennings free tax planning and financial services for the rest of his life.[16] H&R Block senior vice president David Byers estimated that Jennings owed approximately $1.04 million in taxes on his winnings.[17][18]\\r\\nIn a 2011 Reddit IAmA, Jennings recalled how in 2004 the Democratic politicians Chuck Schumer and Harry Reid unsuccessfully asked Jennings to run for the United States Senate from Utah. Jennings commented, \\"That was when I realized the Democratic Party was f@#$ed in '04.\\"[19]\\r\\nJennings has written several books. Brainiac: Adventures in the Curious, Competitive, Compulsive World of Trivia Buffs details his experiences on Jeopardy! and his research into trivia culture conducted after the completion of his run.[2] Ken Jennings's Trivia Almanac: 8,888 Questions in 365 Days, a hardcover book, is a compilation of trivia questionswith 3 categories and about 20 questions per day of the year.[20] Maphead: Charting the Wide, Weird World of Geography Wonks explores the world of map and geography enthusiasts.[21] Because I said so! is a humorous examination of 'The myths, tales & warnings every generation passes down to its kids'.[22] He also has written five books for his children's series, Junior Genius Guides. [23]\\r\\nJennings also had a column in Mental Floss magazine called \\"Six Degrees of Ken Jennings\\", in which readers submit two wildly different things and he has to connect them in exactly six moves, much in the same vein as the Six Degrees of Kevin Bacon game.[24] The column ran from November 2005[25] to the SeptemberÿOctober 2010 issue.[26]\\r\\nAccording to Variety.com, Jennings and television producer Michael Davies teamed up as executive producers on a new game show format for Comedy Central. According to Comedy Central execs, it was planned that Jennings would co-host and participate.[27] The series was planned to premiere late in 2005 or in the first quarter of 2006; as of April 2006, development had stalled, and the show's future remained uncertain. Jennings explained on his website that \\"Stephen Colbert's show was doing so well in its post-Daily Show spot that Comedy Central decided they weren't in the market for a quiz show anymore.\\" However, as of mid-2006, he was still shopping a potential game show titled, Ken Jennings vs. the Rest of the World.[28]\\r\\nJennings appeared on The Colbert Report on September 13, 2006. During the interview, Colbert discussed Jennings's book, Brainiac, and mocked him not knowing the number of pages the book contained. After Colbert coined a word to describe intellectual nerdiness, \\"poindexterity\\", Jennings was going back and forth of what is the correct noun for \\"poindexter.\\" Jennings noted, as he had done earlier that day on NPR's Talk of the Nation, that since his streak, people \\"seem to have an extra-hard trivia question\\" in case they run into him.[29]\\r\\nHe also appeared twice on NPR's Wait Wait... Don't Tell Me! program. In his Feb. 25, 2006 appearance on the \\"Not My Job\\" segment, he answered all three questions correctly, winning for a listener Carl Kasell's voice on that person's home answering machine. Jennings stated, \\"This is the proudest moment of my game show life.\\"[30] On June 1, 2013, Jennings made his debut as a panelist on Wait Wait Don't Tell Me.\\r\\nEntertainment Weekly put his performance on its end-of-the-decade \\"best of\\" list, saying, \\"Answer: A software engineer from Utah, he dominated the quizfest for a record 74 shows in 2004, amassing $2,520,700. Question: Who is Ken Jennings?\\"[31]\\r\\nJennings made the news in July 2006 when an article in the New York Post by Michael Starr criticized Jennings for what Starr perceived as lack of loyalty and gratitude on Jennings' part, citing an open letter to the producers of Jeopardy! and host Alex Trebek that Jennings wrote on his blog.[32][33] Jennings responded on his blog by explaining that the blog entry was satirical in tone, saying, \\"[Starr] knows there's no way I was genuinely calling for angry bees and ventriloquist's dummies to be added to the Jeopardy! format. It's a humor piece, and one which gets its laughs from the outrageous non sequiturs it proposes, not the ripeness of its target for criticism.\\"[34] Jennings had already posted a more serious comment defending Trebek that remains on his website.[35]\\r\\nJennings appeared on the first two episodes of the NBC game show 1 vs. 100 on October 13 and 20, 2006 as a mob member. He incorrectly answered the question, \\"what color is the number 1 space on a standard roulette wheel?\\" as \\"black\\" instead of \\"red\\" in his second episode, eliminating him from the game. (He explained that he did not know the answer because his Mormon faith prohibits gambling.)[36] He left the show with $714.29, his share of a $35,000 prize shared among 49 Mob members. Jennings returned to the show for a special \\"Last Man Standing\\" episode aired on February 9, 2007. He was eliminated on the final question, which asked which of the three options had been married the most times; he answered King Henry VIII, while the correct answer was Larry King. This episode was the first time Jennings had a chance at a rematch against rival Brad Rutter, who was also part of the mob and was eliminated before Jennings.\\r\\nIn 2007, Jennings was invited to be a contestant on the game show Grand Slam hosted by Dennis Miller and Amanda Byram, also a Sony Pictures production. The show debuted on Game Show Network (GSN) on August 4, 2007, and featured sixteen former game-show winners in a single-elimination tournament. Jennings, seeded second behind Brad Rutter, won the tournament and became the 2007 Grand Slam Champion after defeating Ogi Ogas (a second-round winner against Rutter) in the finals. He earned $100,000 for his victory.\\r\\nJennings was a contestant on an episode of Are You Smarter Than a 5th Grader? that aired on October 10, 2008, which held the possibility of exceeding Rutter's total game show winnings. After winning $500,000, enough to surpass Rutter's total, Jennings chose not to attempt the final $1,000,000 question, which would have deducted $475,000 from his winnings if he missed it. As is customary on the show, Jennings was then shown the question to see what would have happened, and he provided the correct answer. Had he risked his winnings and correctly answered the question, he would have become the show's second $1,000,000 winner.\\r\\nFrom 2008 to 2009, Jennings appeared on GSN on Fridays for the trivia game Stump the Master. Home viewers send questions via the GSN website. Four callers are put on hold and Jennings selects from one of the categories. The caller for the category he picked comes on the line and reads the question. If Jennings does not answer or is incorrect, the caller wins $1,000 or more. Any time Jennings is right, the jackpot is increased by $1,000. All callers are given a small prize, whether they participate on the air or not.\\r\\nJennings has appeared on multiple episodes of Doug Loves Movies, hosted by Doug Benson, and has won a few times.\\r\\nJennings also appeared on two other Sony Pictures Television game shows, Who Wants to Be a Millionaire, as a frequent expert for the lifeline \\"Ask the Expert\\", and also taped a pilot for the proposed 2009 CBS revival of Sony's The $25,000 Pyramid.[37]\\r\\nJennings appeared on Millionaire in 2014 as a contestant during Guinness World Records Edition themed week, where he won $100,000 after deciding to walk away on his $250,000 question. If he had gone for it, he would have been right and would have won $250,000.[38]\\r\\nJennings appeared on the second-season premiere of 500 Questions on May 26, 2016[39] and was eliminated on the fourth question, leaving with no winnings.\\r\\nJennings appeared on an episode of @midnight aired May 15, 2017 during the fourth season, which he won. As a result, he served as the funniest person on the internet for May 16, 2017.[40]\\r\\nJennings won the rookie division of the American Crossword Puzzle Tournament (ACPT) in 2006.[41] In his first time competing, Jennings placed 37th overall. He also served as the award's presenter, becoming the first contestant to present an award to himself. He has not competed in the tournament since.\\r\\nJennings had a weekly trivia column Kennections in Parade magazine.[42] In it, five questions are posed and their answers are all connected to a mystery topic, which the readers are to guess. Parade ceased publishing the quiz in early 2015, and removed links to archived quizzes in March 2015. Kennections now appear in the online version of Mental Floss magazine.\\r\\nEvery Tuesday, Jennings sends an email out containing seven questions, one of which is designed to be Google-resistant.[43] Subscribers respond with the answers to all seven questions and the results are maintained on a scoreboard on Jennings' blog.[44] At times he chooses to run multi-week tournaments, awarding the top responder with all seven answers correct with such things as a signed copy of his newest book.\\r\\nPaste named his Twitter one of \\"The 75 Best Twitter Accounts of 2014\\" ranking it at #10.[45]\\r\\nJennings was the subject of some controversy for posting a tweet when he wrote \\"Nothing sadder than a hot person in a wheelchair.\\"[46][47][48][49]\\r\\nJennings again faced controversy when on November 10, 2015, he tweeted a joke about the death of Daniel Fleetwood, a lifelong Star Wars fan who died of cancer. Fleetwood's dying wish was to see Star Wars: The Force Awakens fearing he likely would not live to see the film when it opened in theaters in December 2015. An online campaign was started on his behalf and his wish was granted only days before he died. Jennings said It cant be a good sign that every fan who has seen the new Star Wars movie died shortly thereafter.[50]\\r\\nJennings once more faced controversy when on May 31, 2017 he tweeted a joke involving Barron Trump, the youngest child of U.S. President Donald Trump. According to TMZ, after 11-year-old Barron Trump saw an image of Kathy Griffin holding a bloody Trump mask, he believed it was real and screamed. Jennings wrote, \\"Barron Trump saw a very long necktie on a heap of expired deli meat in a dumpster. He thought it was his dad & his little heart is breaking.\\"[51] After the tweet garnered controversy, Jennings said, \\"The joke doesn't mock Barron. It mocks using him for political cover.\\"[52]\\r\\nOn September 7, 2017, HowStuffWorks unveiled a new show entitled Omnibus, co-hosted by Jennings and John Roderick , frontman of the indie-rock band The Long Winters. They will pick topics they fear might be lost to history and discuss them.[53]\\r\\nJennings agreed to a deal with Microsoft to promote its Encarta encyclopedia software (which was later discontinued). He is also engaged in speaking deals through the Massachusetts-based speakers agency American Program Bureau.[54] In 2005, Cingular Wireless (now AT&T) featured Jennings in commercials portraying him as having lots of \\"friends and family\\" (coming out of the woodwork, because he is now \\"stinking rich\\").\\r\\nUniversity Games produced a Can You Beat Ken? board game, in which players vie against each other and Jennings in an attempt to earn $2.6 million first. Each question in the game was asked to Jennings, and his answers, both correct and incorrect, are recorded on the cards.[55]\\r\\nHe and his wife Mindy (ne Boam)[3] have a son, Dylan, born in 2002, and a daughter, Caitlin, born in 2006.[56]\\r\\nJennings currently resides in the Seattle metropolitan area. He has stated that he is an avid comic book and movie geek with a website listing his top 4,000 favorite movies. He also writes questions for, edits the literature and mythology categories of questions of, and is otherwise active in the National Academic Quiz Tournaments (NAQT), a quiz bowl organization;[57] in particular, he moderated (i.e., read questions) at the 2005, 2006, and 2009 NAQT National High School Tournaments in Chicago.\\r\\nDuring his Jeopardy! winning streak, Jennings was a software engineer for CHG Healthcare Services, a healthcare-placement firm in Holladay, Utah.[58]\\r\\nJennings regularly competes in LearnedLeague under the name \\"JenningsK\\".[59]","input":"What is the longest a contestant has been on jeopardy?"},{"output":"over 60","context":"Theodor Seuss Geisel, better known as Dr. Seuss, published over 60 children's books over the course of his long career. Though most were published under his well-known pseudonym, Dr. Seuss, he also authored over a dozen books as Theo. LeSieg and one as Rosetta Stone. As one of the most popular children's authors of all time, Geisel's books have topped many bestseller lists, sold over 222 million copies, and been translated into more than 15 languages.[1] In 2000, when Publishers Weekly compiled their list of the best-selling children's books of all time; 16 of the top 100 hardcover books were written by Geisel, including Green Eggs and Ham, at number 4, The Cat in the Hat, at number 9, and One Fish Two Fish Red Fish Blue Fish, at number 13, and Dr. Seuss's ABC.[2] In the years following his death in 1991, several additional books based on his sketches and notes were published, including Hooray for Diffendoofer Day! and Daisy-Head Mayzie. Although they were all published under the name Dr. Seuss, only My Many Colored Days, originally written in 1973, was entirely by Geisel.\\r\\nThe bulk of Theodor Seuss Geisel's books were published under the name of Dr. Seuss. Except for Great Day for Up!, My Book about ME and I Am Not Going to Get Up Today!, these books were illustrated and written by Geisel. Note only first edition information is given.\\r\\nGeisel also wrote several books that were posthumously published under his most recognizable pen name, Dr. Seuss.\\r\\nOriginal release was a mini version. It is scheduled to be re-released in a regular sized hardcover format on July 28, 2015 to coincide with the release of \\"What Pet Should I Get?, the newest Seuss book scheduled for release.[3][4] Adapted by Tish Rabe from the works of Dr. Seuss.\\r\\nGeisel also authored several books under the pen name Theo. LeSieg (Geisel spelled backward) and one book under the name Rosetta Stone. These books were written but not illustrated by Geisel.\\r\\nWhile Geisel was most famous for his literary works, he helped write several propaganda films, cartoon shorts, and feature-length film. Many of his literary works have also been adapted for the television and as feature-length films.\\r\\nThis Dr. Seuss collection was a series released by Random House. They are a video version of a \\"book on tape\\". None of these productions are animated. This section does not contain duplicate entries. While Horton Hatches The Egg, How The Grinch Stole Christmas, Horton Hears A Who, Green Eggs and Ham, and Because A Little Bug Went Ka-Choo!, were adapted into full animation, they were also adapted into a non-animated production for this Dr. Seuss Collection.","input":"How many books has dr seuss written and illustrated?"},{"output":"Assassin's Creed Origins","context":"","input":"Which is the latest assassin's creed game?"},{"output":"Andy Taylor's first regular girlfriend seen on the show","context":"Elinor \\"Ellie\\" Walker is a fictional character in the American television sitcom The Andy Griffith Show. She is notable for being Andy Taylor's first regular girlfriend seen on the show. Ellie makes 12 appearances in the first season. Despite the paucity of her appearances, she was intended as a series regular. Unlike girlfriends that would follow, her portrayer, Elinor Donahue, is given opening credit billing, alongside Andy Griffith.\\r\\nElinor Donahue was an established television star and had just finished her tour of duty as Betty \\"Princess\\" Anderson in the popular 1950s sitcom, Father Knows Best when she joined The Andy Griffith Show. Producers believed her name would have \\"drawing power\\" and gave her top billing in the opening credits with stars Andy Griffith, Ron Howard and Don Knotts. The only other performer to receive top billing during the course of the series' run was Frances Bavier (Aunt Bee) who began to appear in the opening credits in the 1965-1966 season after Don Knotts left the series.\\r\\nDonahue's wardrobe in the show was created by Mr. Burt of Encino, who received a nod in the closing credits. Donahue was dining out with her mother and three-year-old son when Mr. Burt approached her table and offered to provide the wardrobe. She approved Mr. Burt's designs and brought him to the show.\\r\\n\\r\\n\\r\\nEllie is a graduate of Bernard University and is a pharmacist. She takes up residence in Mayberry to assist her uncle, Fred Walker, at his pharmacy. Opie becomes Ellie's first friend in Mayberry when the two playfully make faces at each other through the glass window of the pharmacy's front door. Ellie is an intelligent woman with a good deal of spunkiness and a strong sense of fair play.\\r\\nAfter a short and somewhat turbulent getting acquainted period, Ellie and Mayberry sheriff Andy Taylor date and become steadies. In the episode \\"Cyrano Andy\\", Ellie and Andy are instrumental in bringing Andy's deputy Barney Fife and Mayberry townswoman, Thelma Lou together as romantic partners. The two couples often double date.\\r\\nIn her first appearance, \\"Ellie Comes to Town\\", Ellie refuses to dispense pills to Emma Brand without a prescription. Emma is unable to offer a prescription and black lists Ellie. Unknown to Ellie, her uncle regularly placated Emma with sugar pills. When Ellie discovers the ploy, she gives Emma the pills she demands. Emma then accepts Ellie into her good graces.\\r\\nIn a later episode, Ellie runs for town council and faces stiff resistance from the menfolk of Mayberry who cannot abide the idea of a woman holding a man's position. With a good word or two from Andy however, the men change their minds and Ellie is elected. In another episode, Ellie introduces a muddy looking farmer's daughter to makeup, perfume, hairdos, and attractive dresses. In the only Christmas episode in the show's run, Ellie and Andy sing \\"Away in a Manger\\" with guitar accompaniment.\\r\\nAfter twelve appearances in the first season, Ellie disappeared with no explanation offered the audience. It has long been rumored, however, that Ellie abandoned her Uncle's pharmacy and Andy Taylor to run away with Jim Lindsey and become a groupie of Bobby Fleet and his Band with a Beat.[citation needed] The last appearance of Ellie Walker strangely coincided with the episode where Jim Lindsey returned to Mayberry after becoming a famous guitarist with Bobby Fleet's band. At the end of \\"The Guitar Player Returns\\" (15 May 1961), Jim Lindsey reunites with Bobby Fleet to tour with his band, and neither Ellie, nor Jim, is ever seen again. Of course, the good people of Mayberry would never speak about such a scandal, and thus no explanation could be offered for Ellie's sudden departure.[citation needed] Over the years Donahue offered multiple explanations for her departure from the show. In one she believed that she and Griffith lacked the chemistry to make their on-screen romantic relationship credible and she decided to leave. Griffith later admitted that it was his own fault because he had a hard time showing affection on-screen, and as a result, the relationship didn't appear real or believable. However, Griffith's character showed no such problems with the arrival of the Helen Crump character in the 1964 season. A more credible story that Donahue relayed was that she was undergoing personal problems during her year on the show and simply lacked confidence in the quality of her performance. She asked the producers to release her from her contract, which they did. Donahue also admitted that she was a little hurt that they agreed so readily.","input":"Who was ellie on the andy griffith show?"},{"output":"Wilhelm Schickard","context":"A mechanical calculator, or calculating machine, was a mechanical device used to perform automatically the basic operations of arithmetic. Most mechanical calculators were comparable in size to small desktop computers and have been rendered obsolete by the advent of the electronic calculator.\\r\\nSurviving notes from Wilhelm Schickard in 1623 reveal that he designed and had built the earliest of the modern attempts at mechanizing calculation. His machine was composed of two sets of technologies: first an abacus made of Napier's bones, to simplify multiplications and divisions first described six years earlier in 1617, and for the mechanical part, it had a dialed pedometer to perform additions and subtractions. A study of the surviving notes shows a machine that would have jammed after a few entries on the same dial,[1] and that it could be damaged if a carry had to be propagated over a few digits (like adding 1 to 999).[2] Schickard abandoned his project in 1624 and never mentioned it again until his death eleven years later in 1635.\\r\\nTwo decades after Schickard's supposedly failed attempt, in 1642, Blaise Pascal decisively solved these particular problems with his invention of the mechanical calculator.[3] Co-opted into his father's labour as tax collector in Rouen, Pascal designed the calculator to help in the large amount of tedious arithmetic required.;[4] it was called Pascal's Calculator or Pascaline.[5]\\r\\nThomas' arithmometer, the first commercially successful machine, was manufactured two hundred years later in 1851; it was the first mechanical calculator strong enough and reliable enough to be used daily in an office environment. For forty years the arithmometer was the only type of mechanical calculator available for sale.[6]\\r\\nThe comptometer, introduced in 1887, was the first machine to use a keyboard which consisted of columns of nine keys (from 1 to 9) for each digit. The Dalton adding machine, manufactured from 1902, was the first to have a 10 key keyboard.[7] Electric motors were used on some mechanical calculators from 1901.[8] In 1961, a comptometer type machine, the Anita mk7 from Sumlock comptometer Ltd., became the first desktop mechanical calculator to receive an all electronic calculator engine, creating the link in between these two industries and marking the beginning of its decline. The production of mechanical calculators came to a stop in the middle of the 1970s closing an industry that had lasted for 120 years.\\r\\nCharles Babbage designed two new kinds of mechanical calculators, which were so big that they required the power of a steam engine to operate, and that were too sophisticated to be built in his lifetime. The first one was an automatic mechanical calculator, his difference engine, which could automatically compute and print mathematical tables. In 1855, Georg Scheutz became the first of a handful of designers to succeed at building a smaller and simpler model of his difference engine.[9] The second one was a programmable mechanical calculator, his analytical engine, which Babbage started to design in 1834; \\"in less than two years he had sketched out many of the salient features of the modern computer. A crucial step was the adoption of a punched card system derived from the Jacquard loom\\"[10] making it infinitely programmable.[11] In 1937, Howard Aiken convinced IBM to design and build the ASCC/Mark I, the first machine of its kind, based on the architecture of the analytical engine;[12] when the machine was finished some hailed it as \\"Babbage's dream come true\\".[13]\\r\\n\\r\\n\\r\\nThe desire to economize time and mental effort in arithmetical computations, and to eliminate human liability to error, is probably as old as the science of arithmetic itself. This desire has led to the design and construction of a variety of aids to calculation, beginning with groups of small objects, such as pebbles, first used loosely, later as counters on ruled boards, and later still as beads mounted on wires fixed in a frame, as in the abacus. This instrument was probably invented by the Semitic races and later adopted in India, whence it spread westward throughout Europe and eastward to China and Japan.\\r\\nAfter the development of the abacus, no further advances were made until John Napier devised his numbering rods, or Napier's Bones, in 1617. Various forms of the Bones appeared, some approaching the beginning of mechanical computation, but it was not until 1642 that Blaise Pascal gave us the first mechanical calculating machine in the sense that the term is used today.\\r\\nA short list of other precursors to the mechanical calculator must include a group of mechanical analog computers which, once set, are only modified by the continuous and repeated action of their actuators (crank handle, weight, wheel, water...). Before common era, there are odometers and the Antikythera mechanism, an out of place, unique, geared astronomical clock, followed more than a millennium later by early mechanical clocks, geared astrolabes and followed in the 15th century by pedometers?; These machines were all made of toothed gears linked by some sort of carry mechanisms. These machines always produce identical results for identical initial settings unlike a mechanical calculator where all the wheels are independent but are also linked together by the rules of arithmetic.\\r\\nThe 17th century marked the beginning of the history of mechanical calculators, as it saw the invention of its first machines, including Pascal's calculator, in 1642.[4][14] Blaise Pascal had invented a machine which he presented as being able to perform computations that were previously thought to be only humanly possible,[15] but he wasn't successful in creating an industry.\\r\\nIn a sense, Pascal's invention was premature, in that the mechanical arts in his time were not sufficiently advanced to enable his machine to be made at an economic price, with the accuracy and strength needed for reasonably long use. This difficulty was not overcome until well on into the nineteenth century, by which time also a renewed stimulus to invention was given by the need for many kinds of calculation more intricate than those considered by Pascal.\\r\\nThe 17th century also saw the invention of some very powerful tools to aid arithmetic calculations like Napier's bones, logarithmic tables and the slide rule which, for their ease of use by scientists in multiplying and dividing, ruled over and impeded the use and development of mechanical calculators[17] until the production release of the arithmometer in the mid 19th century.\\r\\nBlaise Pascal invented a mechanical calculator with a sophisticated carry mechanism in 1642. After three years of effort and 50 prototypes[19] he introduced his calculator to the public. He built twenty of these machines in the following ten years.[20] This machine could add and subtract two numbers directly and multiply and divide by repetition. Since, unlike Schickard's machine, the Pascaline dials could only rotate in one direction zeroing it after each calculation required the operator to dial in all 9s and then (method of re-zeroing) propagate a carry right through the machine.[21] This suggests that the carry mechanism would have proved itself in practice many times over. This is a testament to the quality of the Pascaline because none of the 17th and 18th century criticisms of the machine mentioned a problem with the carry mechanism and yet it was fully tested on all the machines, by their resets, all the time.[22]\\r\\nPascal's invention of the calculating machine, just three hundred years ago, was made while he was a youth of nineteen. He was spurred to it by seeing the burden of arithmetical labor involved in his father's official work as supervisor of taxes at Rouen. He conceived the idea of doing the work mechanically, and developed a design appropriate for this purpose; showing herein the same combination of pure science and mechanical genius that characterized his whole life. But it was one thing to conceive and design the machine, and another to get it made and put into use. Here were needed those practical gifts that he displayed later in his inventions...\\r\\nIn 1672, Gottfried Leibniz started working on adding direct multiplication to what he understood was the working of Pascal's calculator. However, it is doubtful that he had ever fully seen the mechanism and the method could not have worked because of the lack of reversible rotation in the mechanism. Accordingly, he eventually designed an entirely new machine called the Stepped Reckoner?; it used his Leibniz wheels, was the first two-motion calculator, the first to use cursors (creating a memory of the first operand) and the first to have a movable carriage. Leibniz built two Stepped Reckoners, one in 1694 and one in 1706.[23] Only the machine built in 1694 is known to exist, it was rediscovered at the end of the 19th century having been forgotten in an attic in the University of G?ttingen.[23]\\r\\nIn 1893, the German calculating machine inventor Arthur Burkhardt was asked to put Leibniz machine in operating condition if possible. His report was favorable except for the sequence in the carry.[24]\\r\\nLeibniz had invented his namesake wheel and the principle of a two motion calculator, but after forty years of development he wasn't able to produce a machine that was fully operational; this makes Pascal's calculator the only working mechanical calculator in the 17th century. Leibniz was also the first person to describe a pinwheel calculator.[25] He once said \\"It is unworthy of excellent men to lose hours like slaves in the labour of calculation which could safely be relegated to anyone else if machines were used.\\"[26]\\r\\nSchickard, Pascal and Leibniz were inevitably inspired by the role of clockwork which was highly celebrated in the seventeenth century.[27] However, simple minded application of interlinked gears was insufficient for any of their purposes. Schickard introduced the use of a single toothed \\"mutilated gear\\" to enable the carry to take place. Pascal improved on that with his famous weighted sautoir. Leibniz went even further in relation to the ability to use a moveable carriage to perform multiplication more efficiently, albeit at the expense of a fully working carry mechanism.\\r\\n...I devised a third which works by springs and which has a very simple design. This is the one, as I have already stated, that I used many times, hidden in the plain sight of an infinity of persons and which is still in operating order. Nevertheless, while always improving on it, I found reasons to change its design...\\r\\nWhen, several years ago, I saw for the first time an instrument which, when carried, automatically records the numbers of steps by a pedestrian, it occurred to me at once that the entire arithmetic could be subjected to a similar kind of machinery so that not only counting but also addition and subtraction, multiplication and division could be accomplished by a suitably arranged machine easily, promptly, and with sure results\\r\\nThe principle of the clock (input wheels and display wheels added to a clock like mechanism) for a direct entry calculating machine couldn't be implemented to create a fully effective calculating machine without additional innovation with the technological capabilities of the 17th century.[30] because their gears would jam when a carry had to be moved several places along the accumulator. The only 17th century calculating clocks that have survived to this day do not have a machine wide carry mechanism and therefore cannot be called fully effective mechanical calculators. A much more successful calculating clock was built by the Italian Giovanni Poleni in the 18th century and was a two-motion calculating clock (the numbers are inscribed first and then they are processed).\\r\\nThe 18th century saw the first mechanical calculator that could perform a multiplication automatically?; designed and built by Giovanni Poleni in 1709 and made of wood, it was the first successful calculating clock. For all the machines built in this century, division still required the operator to decide when to stop a repeated subtraction at each index, and therefore these machines were only providing a help in dividing, like an abacus. Both pinwheel calculators and Leibniz wheel calculators were built with a few unsuccessful attempts at their commercialization.\\r\\n The mechanical calculator industry started in 1851 when Thomas de Colmar released his simplified Arithmomtre which was the first machine that could be used daily in an office environment.\\r\\nFor 40 years,[54] the arithmometer was the only mechanical calculator available for sale and was sold all over the world. By then, in 1890, about 2,500 arithmometers had been sold[55] plus a few hundreds more from two licensed arithmometer clone makers (Burkhardt, Germany, 1878 and Layton, UK, 1883). Felt and Tarrant, the only other competitor in true commercial production, had sold 100 comptometers in three years.[56]\\r\\nThe 19th century also saw the designs of Charles Babbage calculating machines, first with his difference engine, started in 1822, which was the first automatic calculator since it continuously used the results of the previous operation for the next one, and second with his analytical engine, which was the first programmable calculator, using Jacquard's cards to read program and data, that he started in 1834, and which gave the blueprint of the mainframe computers built in the middle of the 20th century.[57]\\r\\nThe cash register, invented by James Ritty in 1879, addressed the old problems of disorganization and dishonesty in business transactions.[69] It was a pure adding machine coupled with a printer, a bell and a two sided display that showed the paying party and the store owner, if he wanted to, the amount of money exchanged for the current transaction.\\r\\nThe cash register was easy to use and, unlike genuine mechanical calculators, was needed and quickly adopted by a great number of businesses. \\"Eighty four companies sold cash registers between 1888 and 1895, only three survived for any length of time\\".[70]\\r\\nIn 1890, 6 years after John Patterson started NCR Corporation, 20,000 machines had been sold by his company alone against a total of roughly 3,500 for all genuine calculators combined.[71]\\r\\nBy 1900, NCR had built 200,000 cash registers[72] and there were more companies manufacturing them, compared to the \\"Thomas/Payen\\" arithmometer company that had just sold around 3,300[73] and Burroughs had only sold 1,400 machines.[74]\\r\\nTwo different classes of mechanisms had become established by this time, reciprocating and rotary. The former type of mechanism was operated typically by a limited-travel hand crank; some internal detailed operations took place on the pull, and others on the release part of a complete cycle. The illustrated 1914 machine is this type; the crank is vertical, on its right side. Later on, some of these mechanisms were operated by electric motors and reduction gearing that operated a crank and connecting rod to convert rotary motion to reciprocating.\\r\\nThe latter, type, rotary, had at least one main shaft that made one [or more] continuous revolution[s], one addition or subtraction per turn. Numerous designs, notably European calculators, had handcranks, and locks to ensure that the cranks were returned to exact positions once a turn was complete.\\r\\nThe first half of the 20th century saw the gradual development of the mechanical calculator mechanism.\\r\\nThe Dalton adding-listing machine introduced in 1902 was the first of its type to use only ten keys, and became the first of many different models of \\"10-key add-listers\\" manufactured by many companies.\\r\\nIn 1948 the miniature Curta calculator, which was held in one hand for operation, was introduced after being developed by Curt Herzstark in 1938. This was an extreme development of the stepped-gear calculating mechanism. It subtracted by adding complements; between the teeth for addition were teeth for subtraction.\\r\\nFrom the early 1900s through the 1960s, mechanical calculators dominated the desktop computing market (see History of computing hardware). Major suppliers in the USA included Friden, Monroe, and SCM/Marchant. (Some comments about European calculators follow below.) These devices were motor-driven, and had movable carriages where results of calculations were displayed by dials. Nearly all keyboards were full  each digit that could be entered had its own column of nine keys, 1..9, plus a column-clear key, permitting entry of several digits at once. (See the illustration below of a Marchant Figurematic.)One could call this parallel entry, by way of contrast with ten-key serial entry that was commonplace in mechanical adding machines, and is now universal in electronic calculators. (Nearly all Friden calculators, as well as some rotary (German) Diehls had a ten-key auxiliary keyboard for entering the multiplier when doing multiplication.) Full keyboards generally had ten columns, although some lower-cost machines had eight. Most machines made by the three companies mentioned did not print their results, although other companies, such as Olivetti, did make printing calculators.\\r\\nIn these machines, addition and subtraction were performed in a single operation, as on a conventional adding machine, but multiplication and division were accomplished by repeated mechanical additions and subtractions. Friden made a calculator that also provided square roots, basically by doing division, but with added mechanism that automatically incremented the number in the keyboard in a systematic fashion. The last of the mechanical calculators were likely to have short-cut multiplication, and some ten-key, serial-entry types had decimal-point keys. However, decimal-point keys required significant internal added complexity, and were offered only in the last designs to be made. Handheld mechanical calculators such as the 1948 Curta continued to be used until they were displaced by electronic calculators in the 1970s.\\r\\nTypical European four-operation machines use the Odhner mechanism, or variations of it. This kind of machine included the Original Odhner, Brunsviga and several following imitators, starting from Triumphator, Thales, Walther, Facit up to Toshiba. Although most of these were operated by handcranks, there were motor-driven versions. Hamann calculators externally resembled pinwheel machines, but the setting lever positioned a cam that disengaged a drive pawl when the dial had moved far enough.\\r\\nAlthough Dalton introduced in 1902 first ten-key printing adding (two operations, the other being subtraction) machine, these feature were not present in computing (four operations) machines for many decades. Facit-T (1932) was the first 10-key computing machine sold in large numbers. Olivetti Divisumma-14 (1948) was the first computing machine with both printer and a 10-key keyboard.\\r\\nFull-keyboard machines, including motor-driven ones, were also built until the 1960s. Among the major manufacturers were Mercedes-Euklid, Archimedes, and MADAS in Europe; in the USA, Friden, Marchant, and Monroe were the principal makers of rotary calculators with carriages. Reciprocating calculators (most of which were adding machines, many with integral printers) were made by Remington Rand and Burroughs, among others. All of these were key-set. Felt & Tarrant made Comptometers, as well as Victor, which were key-driven.\\r\\nThe basic mechanism of the Friden and Monroe, described above, was a modified Leibniz wheel (better known, perhaps informally, in the USA as a \\"stepped drum\\" or \\"stepped reckoner\\"). The Friden had an elementary reversing drive between the body of the machine and the accumulator dials, so its main shaft always rotated in the same direction. The Swiss MADAS was similar. The Monroe, however, reversed direction of its main shaft to subtract.\\r\\nThe earliest Marchants were pinwheel machines, but most of them were remarkably-sophisticated rotary types. They ran at 1,300 addition cycles per minute if the [+] bar is held down. Others were limited to 600 cycles per minute, because their accumulator dials started and stopped for every cycle; Marchant dials moved at a steady and proportional speed for continuing cycles. Most Marchants had a row of nine keys on the extreme right, as shown in the photo of the Figurematic. These simply made the machine add for the number of cycles corresponding to the number on the key, and then shifted the carriage one place. Even nine add cycles took only a short time.\\r\\nIn a Marchant, near the beginning of a cycle, the accumulator dials moved downward \\"into the dip\\", away from the openings in the cover. They engaged drive gears in the body of the machine, which rotated them at speeds proportional to the digit being fed to them, with added movement (reduced 10:1) from carries created by dials to their right. At the completion of the cycle, the dials would be misaligned like the pointers in a traditional watt-hour meter. However, as they came up out of the dip, a constant-lead disc cam realigned them by way of a (limited-travel) spur-gear differential. As well, carries for lower orders were added in by another, planetary differential. (The machine shown has 39 differentials in its (20-digit) accumulator!).\\r\\nIn any mechanical calculator, in effect, a gear, sector, or some similar device moves the accumulator by the number of gear teeth that corresponds to the digit being added or subtracted ÿ three teeth changes the position by a count of three. The great majority of basic calculator mechanisms move the accumulator by starting, then moving at a constant speed, and stopping. In particular, stopping is critical, because to obtain fast operation, the accumulator needs to move quickly. Variants of Geneva drives typically block overshoot (which, of course, would create wrong results).\\r\\nHowever, two different basic mechanisms, the Mercedes-Euklid and the Marchant, move the dials at speeds corresponding to the digit being added or subtracted; a [1] moves the accumulator the slowest, and a [9], the fastest. In the Mercedes-Euklid, a long slotted lever, pivoted at one end, moves nine racks (\\"straight gears\\") endwise by distances proportional to their distance from the lever's pivot. Each rack has a drive pin that is moved by the slot. The rack for [1] is closest to the pivot, of course. For each keyboard digit, a sliding selector gear, much like that in the Leibniz wheel, engages the rack that corresponds to the digit entered. Of course, the accumulator changes either on the forward or reverse stroke, but not both. This mechanism is notably simple and relatively easy to manufacture.\\r\\nThe Marchant, however, has, for every one of its ten columns of keys, a nine-ratio \\"preselector transmission\\" with its output spur gear at the top of the machine's body; that gear engages the accumulator gearing. When one tries to work out the numbers of teeth in such a transmission, a straightforward approach leads one to consider a mechanism like that in mechanical gasoline pump registers, used to indicate the total price. However, this mechanism is seriously bulky, and utterly impractical for a calculator; 90-tooth gears are likely to be found in the gas pump. Practical gears in the computing parts of a calculator cannot have 90 teeth. They would be either too big, or too delicate.\\r\\nGiven that nine ratios per column implies significant complexity, a Marchant contains a few hundred individual gears in all, many in its accumulator. Basically, the accumulator dial has to rotate 36 degrees (1/10 of a turn) for a [1], and 324 degrees (9/10 of a turn) for a [9], not allowing for incoming carries. At some point in the gearing, one tooth needs to pass for a [1], and nine teeth for a [9]. There is no way to develop the needed movement from a driveshaft that rotates one revolution per cycle with few gears having practical (relatively small) numbers of teeth.\\r\\nThe Marchant, therefore, has three driveshafts to feed the little transmissions. For one cycle, they rotate 1/2, 1/4, and 1/12 of a revolution. [2]. The 1/2-turn shaft carries (for each column) gears with 12, 14, 16, and 18 teeth, corresponding to digits 6, 7, 8, and 9. The 1/4-turn shaft carries (also, each column) gears with 12, 16, and 20 teeth, for 3, 4, and 5. Digits [1] and [2] are handled by 12 and 24-tooth gears on the 1/12-revolution shaft. Practical design places the 12th-rev. shaft more distant, so the 1/4-turn shaft carries freely-rotating 24 and 12-tooth idler gears. For subtraction, the driveshafts reversed direction.\\r\\nIn the early part of the cycle, one of five pendants moves off-center to engage the appropriate drive gear for the selected digit. If possible, see John Wolff's website [3] for a superb collection of photos with some accompanying explanations. He has similar sets of photos for several other notable calculators.\\r\\nSome machines had as many as 20 columns in their full keyboards. The monster in this field was the Duodecillion made by Burroughs for exhibit purposes.\\r\\nFor sterling currency, S/s/d (and even farthings), there were variations of the basic mechanisms, in particular with different numbers of gear teeth and accumulator dial positions. To accommodate shillings and pence, extra columns were added for the tens digit[s], 10 and 20 for shillings, and 10 for pence. Of course, these functioned as radix-20 and radix-12 mechanisms.\\r\\nA variant of the Marchant, called the Binary-Octal Marchant, was a radix-8 (octal) machine. It was sold to check very early vacuum-tube (valve) binary computers for accuracy. (Back then, the mechanical calculator was much more reliable than a tube/valve computer.)\\r\\nAs well, there was a twin Marchant, comprising two pinwheel Marchants with a common drive crank and reversing gearbox.[4] The article at the link describes them and shows a twin Brunsviga (side-by-side machines). Twin machines were relatively rare, and apparently were used for surveying calculations (The CORDIC algorithm was invented later, but these machine might be able to execute it.) At least one triple machine (Brunsviga(?)) was made. It is likely that a given accumulator could be engaged with either half of the twin.\\r\\nThe Facit calculator, and one similar to it, are basically pinwheel machines, but the array of pinwheels moves sidewise, instead of the carriage. The pinwheels are biquinary; digits 1 through 4 cause the corresponding number of sliding pins ot extend from the surface; digits 5 through 9 also extend a five-tooth sector as well as the same pins for 6 through 9.\\r\\nThe keys operate cams that operate a swinging lever to first unlock the pin-positioning cam that is part of the pinwheel mechanism; further movement of the lever (by an amount determined by the key's cam) rotates the pin-positioning cam to extend the necessary number of pins.[5]\\r\\nStylus-operated adders with circular slots for the stylus, and side-by -side wheels, as made by Sterling Plastics (USA), had an ingenious anti-overshoot mechanism to ensure accurate carries.\\r\\nMechanical calculators continued to be sold, though in rapidly decreasing numbers, into the early 1970s, with many of the manufacturers closing down or being taken over. Comptometer type calculators were often retained for much longer to be used for adding and listing duties, especially in accounting, since a trained and skilled operator could enter all the digits of a number in one movement of the hands on a Comptometer quicker than was possible serially with a 10-key electronic calculator. In fact, it was quicker to enter larger digits in two strokes using only the lower-numbered keys; for instance, a 9 would be entered as 4 followed by 5. Some key-driven calculators had keys for every column, but only 1 through 5; they were correspondingly compact. The spread of the computer rather than the simple electronic calculator put an end to the Comptometer. Also, by the end of the 1970s, the slide rule had become obsolete.","input":"Who invented the first gear driven calculating machine?"},{"output":"in 1924","context":"The California grizzly bear (Ursus arctos californicus) is an extinct subspecies of the grizzly bear, the very large North American brown bear. \\"Grizzly\\" could have meant \\"grizzled\\" (that is, with golden and grey tips of the hair) or \\"fear-inspiring\\" (this is actually spelled \\"grisly\\").[1] Nonetheless, after careful study, naturalist George Ord formally classified it in 1815 ÿ not for its hair, but for its character ÿ as Ursus horribilis (\\"terrifying bear\\").[2] Genetically, North American grizzlies are closely related;[3] in size and coloring, the California grizzly bear was much like the grizzly bear of the southern coast of Alaska. In California, it was particularly admired for its beauty, size and strength. The grizzly became a symbol of the Bear Flag Republic, a moniker that was attached to the short-lived attempt by a group of American settlers to break away from Mexico in 1846. Later, this rebel flag became the basis for the state flag of California, and then California was known as the \\"Bear State.\\"[4]\\r\\n\\r\\n\\r\\nA 1953 researcher stated, \\"The specific status of North American brown bears (or grizzly bears) is one of the most complex problems of mammalian taxonomy. The difficulty stems directly from the work of Merriam (1918), who concluded that there are 86 forms of grizzlies (and brown bears) in North America.\\"[5]\\r\\nNorth American grizzlies were taxonomically grouped as a species apart from other bear species, until DNA testing revealed that they should properly be grouped in the same species as the other brown bears.[3] Grizzlies living in California had been classified by Merriam into many subspecies, but the only genetically anomalous grouping in North America is the ABC Islands bears.[6]\\r\\nThe first recorded encounters of California grizzly bears by the Europeans are in the diaries kept by several members of the 1769 Portola expedition, first exploration by land of what is now the state of California. Several place names that include the Spanish word for bear (oso) trace their origins back to that first expedition (e.g. Los Osos).\\r\\nAs the settled frontier of New Spain was extended northward, settlers began to populate California and establish large cattle herds as the main industry. The grizzly bears killed livestock and so became enemies of the rancheros. Vaqueros hunted the grizzlies, sometimes roping and capturing them to be displayed in public battles with bulls.[7] This popular spectator sport inspired betting as to whether the bear or the bull would win.\\r\\nOne popular, though false[8] account is that Horace Greeley, after seeing such a fight, gave the modern stock market its \\"bear\\" and \\"bull\\" nicknames  based on the fighting styles of the two animals: the bear swipes downward while the bull hooks upward.\\r\\nIn 1866, a grizzly bear described as weighing as much as 2,200 pounds (1,000?kg) was killed in Valley Center, California, the biggest bear ever found in California, as recalled in 1932 by Catherine E. Lovett Smith, who had been six years old at the time and on whose family's ranch the bear had been found; Lovett Smith stated that this was where Valley Center got its original name of \\"Bear Valley\\" (other sources concurred as to the story of the bear, but differed on its size).[9]\\r\\nLess than 75 years after the discovery of gold in 1848, almost every grizzly bear in California had been tracked down and killed. One prospector in Southern California, William F. Holcomb (nicknamed \\"Grizzly Bill\\" Holcomb), was particularly well known for hunting grizzly bears in what is now San Bernardino County. The last hunted California grizzly bear was shot in Tulare County, California, in August 1922, although no body, skeleton or pelt was ever produced. Two years later in 1924, what was thought to be a grizzly was spotted in Sequoia National Park for the last time and thereafter, grizzlies were never seen again in California.[9][10][11]\\r\\nCalifornian grizzly bears had been pitted against other animals, such as bulls.[12]\\r\\nCalifornia still has habitat for about 500 grizzlies.[13] In 2014, the U.S. Fish and Wildlife Service received and rejected a petition to reintroduce grizzly bears to California.[14][15] In 2015, the Center for Biological Diversity launched a petition aimed at the California state legislature to reintroduce the grizzly bear to the state.[16][17][18] The California grizzly bear has been considered as a possible candidate for attempts at de-extinction, through the proposed use of back-breeding, cloning and genetic engineering to recreate extinct species.[19]\\r\\nThe California grizzly bear is one of the state's most visible and enduring symbols, adorning both the state flag and seal. The Bear Flag first flew in 1846 as a symbol of the short-lived California Republic. A second version was adopted as the state flag by the state legislature in 1911.[20] The bear symbol became a permanent part of the state seal in 1849. The California grizzly bear was designated the official state animal in 1953.[21][22] The bear is celebrated in name and as mascot of the sports teams of the University of California, Berkeley (the California Golden Bears), and of the University of California, Los Angeles (the UCLA Bruins) and in the mascot of University of California, Riverside (Scottie the Bear, dressed in a Highland kilt). The California Maritime Academy operates a training ship named \\"Golden Bear\\".[citation needed]\\r\\n Data related to Ursus arctos californicus at Wikispecies","input":"When did grizzly bears become extinct in california?"},{"output":"Ottoman victory","context":"","input":"What was the result of the gallipoli campaign?"},{"output":"Chevrolet Camaro ZL1","context":"William Clyde \\"Chase\\" Elliott II (born November 28, 1995) is an American professional stock car racing driver. He currently competes full-time in the Monster Energy NASCAR Cup Series, driving the No. 9 Chevrolet Camaro ZL1 for Hendrick Motorsports, and part-time in the NASCAR Xfinity Series, driving the No. 88 for JR Motorsports and No. 23 for GMS Racing. He is the son of 1988 Winston Cup Series champion Bill Elliott.[2]\\r\\n\\r\\nHe won the 2014 NASCAR Nationwide Series championship, becoming the first rookie to win a national series championship in NASCAR. He was the 2016 NASCAR Sprint Cup Series Rookie of the Year.\\r\\n\\r\\nAt 13 years old, Elliott was featured alongside thirteen other athletes, including future world number one golfer Jordan Spieth, as potential stars in the July 13, 2009 issue of Sports Illustrated.[3]\\r\\nElliott raced in 40 races in various series in 2010, winning twelve events over the course of the year and finishing in the top ten 38 times.[4] It was the third season of his racing career, and he won the Blizzard Series, Miller Lite and Gulf Coast championship en route to being named the Georgia Asphalt Pro Late Model Series Rookie of the Year.[4] He ended the season by winning the Winchester 400.[4] Sports Illustrated named Elliott as the high school player of the week in April 2011.[4] During the year, he competed in the Champion Racing Association, winning the series' National Super Late Model championship.[5] Later that year, just after his sixteenth birthday, he won the Snowball Derby and became the race's youngest winner.[6] He beat the second place driver, DJ Vanderley, by a record 0.229 seconds.[7] In 2012, he won the Alan Turner Snowflake 100, prelude to the Snowball Derby, for the second time in three years.[8]\\r\\n\\r\\nIn November 2013, Elliott won the All American 400, becoming the first driver to win all four of the country's largest short-track races: the All American 400, the Snowball Derby, the World Crown 300 and the Winchester 400.[9]  In December, it appeared as though Elliott had become the first driver to sweep the Snowball Derby and Snowflake 100 in the same weekend. Upon post-race inspection, however, a piece of tungsten was found in Elliott's car, which was prohibited by the Derby rulebook. Elliott was accordingly disqualified and the victory awarded to Erik Jones.[10] Elliott won the Snowball Derby in 2015 after initial winner Christopher Bell was disqualified.[1]\\r\\n\\r\\nElliott signed a three-year driver development contract with Hendrick Motorsports in February 2011.[4][11] He competed in the K&N Pro Series East in 2011 with number 9, finishing 9th in season points.[5] Elliott returned to the K&N Pro Series East in 2012,[5] winning his first career race in the series at Iowa Speedway in May.[12] He finished fourth in series points.\\r\\n\\r\\nIn 2011 and 2012, Elliott competed in three K&N Pro Series West races (once in 2011, twice in 2012), all at Phoenix International Raceway. In his lone 2011 event, he finished third, and in the 2012 races, he finished 17th (due to a crash) and fourth.[13]\\r\\n\\r\\nElliott competed in six ARCA Racing Series races in 2012 and five races in 2013 with number 9, in order to gain experience at larger circuits. ARCA allows 17-year old drivers to race at Pocono Raceway and Kentucky Speedway, two circuits where NASCAR has a minimum age of 18;  the minimum age for ovals longer than 2,000 meters, or 1.25 miles, is 18 years of age;  shorter tracks and road courses have a minimum age of 16.[14] On June 8, 2013, Elliott became the youngest winner in ARCA superspeedway history following his Pocono victory.[15]\\r\\n\\r\\nIn January 2013, it was announced that Elliott would compete in nine NASCAR Camping World Truck Series events for Hendrick Motorsports during the 2013 racing season, using trucks prepared by Turner Scott Motorsports.[14]\\r\\n\\r\\nIn qualifying for the UNOH 200 at Bristol Motor Speedway, Elliott won his first career NASCAR pole position with a lap speed of 125.183?mph (201.463?km/h), and became the youngest pole-sitter in Truck Series history.[16]\\r\\n\\r\\nElliott would win his first race in the Chevrolet Silverado 250 at Canadian Tire Motorsport Park, in the first road course truck race outside the US; he was at the time the youngest winner in Truck Series history, at the age of 17 years, 9 months, and 4 days.[17][N 1] The win was however controversial as Elliott made contact with leader Ty Dillon in the last corner. Dillon hit the tire barrier while Elliott went into the grass though recovered enough to be able to coast to the finish line ahead of Kyle Busch Motorsports driver Chad Hackenbracht.[19]\\r\\n\\r\\nDillon afterwards stated that the next time they raced each other \\"he won't finish the race\\";[19] later Elliott stated he had attempted to apologize to Dillon but without any response.[20] The following week at Iowa Speedway, Elliott cut down a tire early in the race and crashed without involvement from Dillon.[21]\\r\\n\\r\\nIn October 2016, Elliott entered the Alpha Energy Solutions 200 at Martinsville Speedway, his first truck race since 2013, driving the #71 for Contreras Motorsports, leasing owners points and the truck chassis from JR Motorsports, where he led the most laps with 109 and finished 2nd.[22]\\r\\n\\r\\nElliott joined GMS Racings No. 23 entry for two races (Atlanta and Martinsville) in 2017. Elliott won at Martinsville.\\r\\n\\r\\nIn January 2014, it was announced that Elliott would be competing full-time in the Nationwide Series in 2014, driving the No. 9 Chevrolet for JR Motorsports, with sponsorship from NAPA Auto Parts.[23] On April 4, 2014, Elliott won the O'Reilly Auto Parts 300 at Texas Motor Speedway, holding Kevin Harvick and Kyle Busch off after taking the lead with 16 laps to go.[24] On April 11, 2014, Elliott won the VFW Sport Clips Help a Hero 200 at Darlington Raceway by passing Elliott Sadler on the last lap after restarting sixth with two laps to go.[25] Elliott won the EnjoyIllinois.com 300 at Chicagoland Speedway after holding off Trevor Bayne.[26] At Phoenix, Elliott clinched the Nationwide Series championship with a 53-point lead over teammate Regan Smith, becoming the first rookie and youngest driver to win a NASCAR national series title.[27] Later in the year, he was named the Nationwide Series' Most Popular Driver.[28]\\r\\n\\r\\nIn 2015, Elliott received his first DNF in his career after being involved in the second big one at Daytona, finishing 28th. On September 11, Elliott won his first race of the season at Richmond, snapping his 36-race winless streak. He battled with Chris Buescher for the championship, but was unable to catch up and finished 2nd in points.\\r\\n\\r\\nFollowing Elliott's move up to the Cup Series in 2016, he continued driving for JR Motorsports part-time in the Xfinity Series. In 2016 he drove the No. 88 car in 5 races, including the season opening PowerShares QQQ 300 at Daytona, which he won. He also drove the No. 5 car at Texas.\\r\\n\\r\\nIn 2018 he began the season driving the No. 88 car at Daytona, which teammate Tyler Reddick won in a photofinish with teammate Elliott Sadler. Following Spencer Gallagher's suspension from NASCAR, it was announced that Elliott would pilot the No. 23 car for GMS Racing in select events, including the races at Charlotte, Pocono, Chicagoland, Daytona, and Bristol.[29]\\r\\n\\r\\nOn January 29, 2015, Hendrick Motorsports announced Elliott would make his Sprint Cup Series debut in 2015, driving the No. 25 with Kenny Francis as crew chief. He was scheduled to race in five races at Martinsville, Richmond, Charlotte, Indianapolis, and Darlington. The team also announced that he would take over Jeff Gordon's No. 24 starting in 2016.[28]\\r\\n\\r\\nElliott's Cup debut in the STP 500 was threatened by potential rain; due to a lack of owner's points and race attempts, had qualifying been rained out, he would have failed to qualify. Elliott eventually recorded a lap speed of 96.919?mph (155.976?km/h), qualifying 27th.[30] During the race, contact with Brett Moffitt on lap 75 forced his car to drop debris onto the track and damage to hang from its rear, while his power steering was damaged. Dropping to 37th, Elliott entered the garage, and returned to the race on lap 144, 69 laps behind the leader and in last. Elliott would ultimately finish 38th, 73 laps down.[31] On May 5, 2015, it was announced that Elliott would be entering the Sprint All-Star Race's Sprint Showdown.[32] He finished 8th and 5th in the event's two segments.[33]\\r\\n\\r\\nElliott joined the Sprint Cup Series full-time in 2016, driving the No. 24 with Alan Gustafson as crew chief.[28] Elliott carried primary sponsorship from NAPA (twenty-four races),[34] 3M (five races),[35] SunEnergy1 (four races),[36] Kelley Blue Book (two races),[37] and Mountain Dew (two races).[38] He won the 2016 Rookie of the Year over Ryan Blaney, Chris Buescher, Jeffrey Earnhardt, and Brian Scott.[39]\\r\\n\\r\\nIn his Daytona 500 debut, Elliott won the pole with a speed of 196.314 miles per hour (315.937?km/h). At the age of 20, he became the youngest pole-sitter in 500 history.[40] Elliott led three laps in the race, but on lap 18, spun exiting turn four and slid into the grass, damaging the front of the car. Elliott returned to the race on lap 59, 40 laps down, and finished 37th.[41] The next week he finished 8th at Atlanta for his first Sprint Cup top-ten finish. The following week, at Las Vegas, Elliott showed a strong car all day and even had his car inside the top-five with 40 laps to go, but crashed and finished 38th. Elliott picked up more top tens during the spring, finishing 5th at Texas for his first career Top-5, 4th at Bristol, 5th at Talladega, 9th at Kansas, 3rd at Dover, 8th in the Coca-Cola 600, and a career best 2nd at Michigan.[42] At Pocono for the running of the Axalta \\"We Paint Winners\\" 400, Elliott would have his breakout race of his Sprint Cup career, Elliott would start 13th and later get the lead in the race and he would lead a race high of 51 laps, leading the most laps in a Sprint Cup race for the first time in his career. On a restart, Elliott would lose the lead and the race came down to fuel mileage but the fuel would hold and he would finish 4th. At Michigan in June, Elliott finished second after he missed a shift in the lead. He won the fan vote to advance into the All-Star Race along with Danica Patrick where he finished a respectable 7th after nearly winning the final segment of the Sprint Showdown, losing to Kyle Larson in a photo finish. Fifteen races into his rookie season, he sat 6th in the standings, the highest without a victory, with two poles for the Daytona 500 and at Talladega, six Top-5s and eleven Top-10s. Two weeks later at Sonoma, Elliott started 16th, but would ultimately finish 21st.  He was one of the first rookies to qualify for the Chase along with Chris Buescher since Denny Hamlin in 2006. On September 18, at the Teenage Mutant Ninja Turtles 400, he had a chance at his first win but a late caution wiped out his 3-second lead over Martin Truex Jr. who would go on to win the race while Elliott would finish in 3rd. He was eliminated after the Round of 12, but managed to finish 10th in the final standings.\\r\\n\\r\\nElliott started the 2017 season by winning the pole for the Daytona 500 for the second year in a row.[43] He followed it up with a win in the first Can-Am Duel race, becoming the first driver since Dale Earnhardt in 1996 to win both the Daytona 500 pole and a qualifying race and the third in NASCAR history (Davey Allison is the first after doing so in 1990). At Martinsville a little later in the year, he snuck past Kyle Busch after Ricky Stenhouse Jr bumped the #18 out of the way, allowing Chase to steal the stage 2 victory. At Talladega on May 7, 2017, he was involved in a 16 car pileup that nearly saw him flip over, as his car got airborne. At Michigan in June, Elliott got his 3rd second-place finish in a row at the track. On October 1, Elliott had another chance at his first career win leading his first 138 laps at Dover and having a 4-second lead over Kyle Busch with 50 laps to go, but caught lap traffic and was stuck behind Ryan Newman, who has long held the reputation as one of the hardest guys to pass, allowing Busch to pass Elliott with 2 laps to go for the win while Elliott finished second. Jeff Gordon, the previous driver of the #24 car and a mentor to Elliott, confronted Newman after the race because he cost Elliott his first career win. At the fall race at Martinsville, Elliott was able to take the lead from Brad Keselowski with 4 to go, but his winning chances were ruined after being hit by Denny Hamlin from behind and spun out with 3 to go. Unhappy with Hamlin, he drove him to the outside wall after the race ended on the cooling lap. My mom always said if you dont have anything nice to say dont say anything at all, Elliott told NBCSN. Hes not even worth my time.  We had a good opportunity. I cant control his decisions and whatever the hell that was. On to Texas. He later got an apology from Hamlin after the race via Twitter.[44] At Phoenix, Elliott was in a must-win situation to advance to Miami. He did lead 34 laps of the race but once again, he had to settle for second as Matt Kenseth passed him with 10 laps to go, ending his championship hopes.[45] However, he wound up finishing 5th in the final standings.[46]\\r\\n\\r\\nIn 2018 Hendrick Motorsports switched Elliott's car number from the No. 24 to the No. 9, the number that his father drove during most of his racing career, and also Chase's number in NASCAR's lower series. Elliott retained his crew, including crew chief Alan Gustafson.[47][48]\\r\\nIn qualifying for the Daytona 500, Elliott posted the 9th fastest time, ending his streak of consecutive Daytona 500 poles at two. Only a few hours  later in the Advance Auto Parts Clash, he would be up front for most of the race, leading 17 out of the 75 laps but dropped back after an incident in the backstretch and was later caught up in a wreck on the final lap. Elliott won the second Can-Am Duel to earn a spot in the second row for the Daytona 500. Elliott ran towards the front during the first half of the Daytona 500, even leading four laps, before getting caught up in an accident on lap 102 and finishing 33rd.[49] At Richmond in April, Elliott finished second in the Toyota Owners 400. This was his best finish of the season to date and the eighth second-place finish of his career, tying the number of second-place finishes his father had before his first win. The following week at Talladega, he finished third in the GEICO 500 after starting the race at the rear of the field due to an unapproved tire change.[50] Chase had a strong race at Dover, starting 6th, finishing in the top 10 in both stages before slipping to 12th at the end. Chase scored another twelfth place finish at Kansas, and rallied from a disappointing 22nd qualification to finish 11th at the Coca-Cola 600. Elliott had what he called his team's \\"best race of the year\\"[51] to date in the Pocono 400 where he finished tenth and earned additional points with top tens in both stages.[52] Elliott scored a race-high 49 points with two top five stage finishes and a fourth-place overall finish in the Toyota/Save Mart 350 at Sonoma,[53] which he considers \\"one of [his] worst\\" tracks.[54] He scored his first pole of the 2018 season at the Coke Zero Sugar 400 at Daytona, with a lap that was 0.240 seconds faster than anyone else in qualifying. The following day, his race ended early when he was collected in the big one on lap 54 along with 25 other drivers after Ricky Stenhouse Jr. turned Brad Keselowski near the front of the field, resulting in a 34th-place finish for Elliott.[55] At Watkins Glen, Elliott started third, won Stage 2 for third week in a row (Loudon, Pocono, Watkins Glen), and led the final 33 laps to finally capture his first career Monster Cup Series Series victory. Chase's win emulated his father, Bill, by finishing second eight times before winning, and winning his first race on a road course (Bill won his first career race at the now-defunct Riverside International Raceway).[56]\\r\\n\\r\\nElliott has made appearances on television, including CMT's The Dude Perfect Show[57] and MTV's Ridiculousness.[58] He voices the character Mark Set-Go on Nickelodeon's Blaze and the Monster Machines[59] and Chase Racelott in the 2017 Pixar film Cars 3.[60]\\r\\n\\r\\nIn 2017, Elliott served as a Fox NASCAR analyst for the Xfinity Series race at Atlanta.[61]\\r\\n\\r\\nElliott has appeared on the cover of magazines, including NASCAR Illustrated;[62] NASCAR Pole Position;[63][64] and Georgia Magazine.[65]\\r\\n\\r\\nElliott is featured as a playable driver in Forza Motorsport 6, via the NASCAR expansion pack.[66] The expansion features twenty-four paint schemes from the 2016 Sprint Cup Series season, including Elliott's No. 24 NAPA SS.[66] Elliott, along with Jimmie Johnson and Kyle Busch, provide commentary in the expansion as the \\"voices of motorsport.\\"[66] Elliott and Johnson also had roles in developing the expansion.[67]\\r\\n\\r\\nElliott has been a driver in all of the NASCAR Heat series of games by 704Games which were released beginning in 2016. All four 2018 HMS drivers, including Elliott, were on the cover of NASCAR Heat 3 which will be released Sept. 7, 2018.[68]\\r\\n\\r\\nNOTE:  The asterisk denotes Elliott won a Daytona 500 qualifying race, which counts as a stage win for championship purposes (ten points) but not a stage win point.\\r\\n\\r\\n(key) (Bold?ÿ Pole position awarded by qualifying time. Italics?ÿ Pole position earned by points standings or practice time. *?ÿ Most laps led.)\\r\\n\\r\\n* Season still in progress\\r\\n1 Ineligible for series points\\r\\n\\r\\n(key) (Bold?ÿ Pole position awarded by qualifying time. Italics?ÿ Pole position earned by points standings or practice time. *?ÿ Most laps led.)\\r\\n\\r\\n1982? J. Ingram \\r\\n1983? S. Ard \\r\\n1984? S. Ard\\r\\n1985? J. Ingram\\r\\n1986? L. Pearson\\r\\n1987? L. Pearson\\r\\n1988? T. Ellis\\r\\n\\r\\n1989? R. Moroso\\r\\n1990? C. Bown\\r\\n1991? B. Labonte\\r\\n1992? J. Nemechek\\r\\n1993? S. Grissom\\r\\n1994? D. Green\\r\\n1995? J. Benson Jr.\\r\\n\\r\\n1996? R. LaJoie\\r\\n1997? R. LaJoie\\r\\n1998? D. Earnhardt Jr.\\r\\n1999? D. Earnhardt Jr.\\r\\n2000? J. Green\\r\\n2001? K. Harvick\\r\\n2002? G. Biffle\\r\\n\\r\\n2003? B. Vickers\\r\\n2004? M. Truex Jr.\\r\\n2005? M. Truex Jr.\\r\\n2006? K. Harvick\\r\\n2007? C. Edwards\\r\\n2008? C. Bowyer\\r\\n2009? K. Busch\\r\\n\\r\\n2010? B. Keselowski\\r\\n2011? R. Stenhouse Jr.\\r\\n2012? R. Stenhouse Jr.\\r\\n2013? A. Dillon\\r\\n2014? C. Elliott\\r\\n2015? C. Buescher\\r\\n2016? D. Surez\\r\\n\\r\\n2017? W. Byron","input":"What kind of car does chase elliot drive?"},{"output":"Nestor Studios, opened in 1911 by Al Christie for David Horsley","context":"A film studio (also known as movie studio or simply studio) is a major entertainment company or motion picture company that has its own privately owned studio facility or facilities that are used to make films, which is handled by the production company. The majority of firms in the entertainment industry have never owned their own studios, but have rented space from other companies.\\r\\nThere are also independently owned studio facilities, who have never produced a motion picture of their own because they are not entertainment companies or motion picture companies; they are companies who sell only studio space.\\r\\nThe largest film studio in the world is Ramoji Film City, in Hyderabad, India.[2][3]\\r\\n\\r\\n\\r\\nIn 1893, Thomas Edison built the first movie studio in the United States when he constructed the Black Maria, a tarpaper-covered structure near his laboratories in West Orange, New Jersey, and asked circus, vaudeville, and dramatic actors to perform for the camera. He distributed these movies at vaudeville theaters, penny arcades, wax museums, and fairgrounds. The pioneering Thanhouser film studio was founded in New Rochelle, New York in 1909 by American theatrical impresario Edwin Thanhouser. The company produced and released 1,086 films between 1910 and 1917, successfully distributing them around the world. The first film serial ever, The Million Dollar Mystery, was released by the Thanhouser company in 1914.\\r\\nIn the early 1900s, companies started moving to Los Angeles, California. Although electric lights were by then widely available, none were yet powerful enough to adequately expose film; the best source of illumination for motion picture production was natural sunlight. Some movies were shot on the roofs of buildings in Downtown Los Angeles. Early movie producers also relocated to Southern California to escape Edison's Motion Picture Patents Company, which controlled almost all the patents relevant to movie production at the time.\\r\\nThe first movie studio in the Hollywood area was Nestor Studios, opened in 1911 by Al Christie for David Horsley. In the same year, another 15 independents settled in Hollywood. Other production companies eventually settled in the Los Angeles area in places such as Culver City, Burbank, and what would soon become known as Studio City in the San Fernando Valley.\\r\\nThe Big 5\\r\\nBy the mid-1920s, the evolution of a handful of American production companies into wealthy motion picture industry conglomerates that owned their own studios, distribution divisions, and theaters, and contracted with performers and other filmmaking personnel, led to the sometimes confusing equation of \\"studio\\" with \\"production company\\" in industry slang. Five large companies, 20th Century Fox, RKO Pictures, Paramount Pictures, Warner Bros., and Metro-Goldwyn-Mayer came to be known as the \\"Big Five,\\" the \\"majors,\\" or \\"the Studios\\" in trade publications such as Variety, and their management structures and practices collectively came to be known as the \\"studio system.\\"\\r\\nThe Little 3\\r\\nAlthough they owned few or no theaters to guarantee sales of their films, Universal Pictures, Columbia Pictures, and United Artists also fell under these rubrics, making a total of eight generally recognized \\"major studios\\". United Artists, although its controlling partners owned not one but two production studios during the Golden Age, had an often-tenuous hold on the title of \\"major\\" and operated mainly as a backer and distributor of independently produced films.\\r\\nSmaller studios operated simultaneously with \\"the majors.\\" These included operations such as Republic Pictures, active from 1935, which produced films that occasionally matched the scale and ambition of the larger studio, and Monogram Pictures, which specialized in series and genre releases. Together with smaller outfits such as PRC TKO and Grand National, the minor studios filled the demand for B movies and are sometimes collectively referred to as Poverty Row.\\r\\nThe Big Five's ownership of movie theaters was eventually opposed by eight independent producers, including Samuel Goldwyn, David O. Selznick, Walt Disney, Hal Roach, and Walter Wanger. In 1948, the federal government won a case against Paramount in the Supreme Court, which ruled that the vertically integrated structure of the movie industry constituted an illegal monopoly. This decision, reached after twelve years of litigation, hastened the end of the studio system and Hollywood's \\"Golden Age\\".\\r\\nBy the 1950s, the physical components of a typical major film studio had become standardized. Since then, a major film studio has usually been housed inside a physically secure compound with a high wall, which protects filmmaking operations from unwanted interference from paparazzi and crazed fans of leading movie stars. Movement in and out of the studio is normally limited to specific gates (often capped with grand decorative arches), where visitors must stop at a boom barrier and explain the purpose of their visit to a security guard. Studio premises generally feature multiple sound stages along with an outside backlot, as well as offices for studio executives and production companies. There is normally a studio \\"commissary\\", which is the traditional term in the film industry for what other industries call a company cafeteria. Early nitrate film was notoriously flammable, and sets were and are still very flammable, which is why film studios built in the early-to-mid 20th century have water towers to facilitate firefighting.\\r\\nHalfway through the 1950s, with television proving to be a lucrative enterprise not destined to disappear any time soonas many in the film industry had once hopedmovie studios were increasingly being used to produce programming for the burgeoning medium. Some midsize film companies, such as Republic Pictures, eventually sold their studios to TV production concerns, which were eventually bought by larger studios, such as the American Broadcasting Company which was purchased by The Walt Disney Company in 1996.\\r\\nWith the growing diversification of studios into such fields as video games, television, theme parks, home video and publishing, they have become multi-national corporations. As the studios increased in size they began to rely on production companies, like J.J. Abrams' Bad Robot Productions, to handle many of the creative and physical production details of their feature films. Instead the studios transformed into financing and distribution entities for the films made by their affiliated production companies. With the decreasing cost of CG and visual effects, many studios sold large chunks of their once massive studio spaces or backlots to private real-estate developers. Century City in Los Angeles was once part of the 20th Century Fox backlot, which was among the largest and most famous of the studio lots. In most cases portions of the backlots were retained and are available for rental by various film and television productions. Some studios offer tours of their backlots, while Universal Pictures allows visitors to its adjacent Universal Studios Hollywood theme park to take a tram tour of the backlot where films such as Psycho and Back to the Future were once shot.\\r\\nIn the 1980s and 90s, as the cost of professional 16mm film equipment decreased, along with the emergence of non-film innovations such as S-VHS and Mini-DV cameras, many young filmmakers began to make films outside the \\"studio system\\". Filmmakers such as Jim Jarmusch, Robert Rodriguez, Steven Soderbergh, Quentin Tarantino, Kevin Smith and Richard Linklater made films that pushed boundaries in ways the studios were then reluctant to do. In response to these films, many distributed by \\"mini-studios\\" like Miramax, the \\"majors\\" created their own in-house mini-studios meant to focus on edgier \\"independent\\" content. Focus Features was created by Universal Pictures and Fox Searchlight was created by 20th Century Fox for this purpose.","input":"Who built the first film studio in hollywood?"},{"output":"Mnchen Hauptbahnhof","context":"Mnchen Hauptbahnhof (German for Munich main railway station) is the main railway station in the city of Munich, Germany. It is one of the three long distance stations in Munich, the others being Mnchen Ost and Mnchen-Pasing. Mnchen Hauptbahnhof sees about 450,000 passengers a day, which puts it on par with other large stations in Germany, such as Hamburg Hauptbahnhof and Frankfurt Hauptbahnhof. It is classified by Deutsche Bahn as a category 1 station, one of 21 in Germany and two in Munich, the other being Mnchen Ost.[5] The mainline station is a terminal station with 32 platforms. The subterranean S-Bahn with 2 platforms and U-Bahn stations with 6 platforms are through stations.[2][3]\\r\\nThe first Munich station was built about 800 metres to the west in 1839. A station at the current site was opened in 1849 and it has been rebuilt numerous times, including to replace the main station building, which was badly damaged during World War II.\\r\\n\\r\\n\\r\\nThe station is located close to Munich's city centre in the north of the borough of Ludwigsvorstadt-Isarvorstadt. The main entrance to the east of the station is via the Prielmayerstra?e or Bayerstra?e to Karlsplatz (Stachus). In the station forecourt (Bahnhofsplatz) in front of the main entrance are tram stops on several lines.[8]\\r\\nThe station is bordered to the north by the Arnulfstra?e and to the west by Paul-Heyse-Stra?e, which passes through a tunnel near the end of the platforms. The station is bordered to the south by Bayerstra?e. The station precinct extends some distance to the west and ends at Donnersbergerbrcke.\\r\\nDuring the industrialisation of the mid-19th century a new, more efficient system was needed to accelerate the transport of passengers and goods. Horse-drawn carts on the mostly poor roads were no longer sufficient. As a solution, the construction of a railway, as was being developed in England, was considered. However, the Bavarian King, Ludwig I preferred the extension of canals. Construction of railways was left to private companies and associations.[9]\\r\\nAfter the opening of the approximately 6?km-long railway from Nuremberg to Frth on 28 November 1835, interested citizens founded railway committees in Munich and Augsburg. The two committees soon joined together to facilitate the construction of a railway line from Augsburg to Munich. The two major cities would be connected by a faster service than could be provided by stagecoach over a distance that in 1835 was measured as 17 Poststunden (post hours, which were each half a Bavarian mile, that is about 3,707 metres), equivalent to about 63?km. Based on the travel speed of a locomotive, a railway could be expected to reduce travel time to one-third of a stage coachs time. The railway committee commissioned a state official to plan the approximate route of the line. The state was to build the railway.[10] The government turned down the proposal, but indicated that Bavaria would financially support its construction.\\r\\nJoseph Anton von Maffei founded the Munich-Augsburg Railway Company (Mnchen-Augsburger Eisenbahn-Gesellschaft) as a private company on 23 July 1837. After further support from shareholders had been found, construction began in the spring of 1838.[9][11]\\r\\nIn 1838, the initial planning began for the station in Munich. The Planning Director of the MunichÿAugsburg railway, Ulrich Himbsel, and his deputy, Joseph Pertsch, proposed a railway layout with an entrance building and a warehouse for freight. Behind the entrance building, a semicircular building was followed by four radially arranged halls. This was based on English models.[12] Joseph Pertsch preferred a location on today's Sonnenstra?e, while Ulrich Himbsel favoured a station at Spatzenstra?e. This would have been at the location of the current station.[13]\\r\\nThe Munich-Augsburg railway company could not afford the building and the land on either site. A temporary wooden building was put into operation with the opening of the first section of the line from Munich to Lochhausen on the MunichÿAugsburg line on 1 September 1839.[14] This station was built in Marsfeld at the present site of Hackerbrcke.[15] It consisted of a simple wooden station building and two toll booths. In the entrance building there were two waiting rooms and several work spaces. Attached to this building there was a 75.4 G 15.37 metre wide station hall with two tracks with a turntable at the end of each.[14][16] There was also a locomotive workshop in the station area. A year later, on 4 October 1840, the entire line to Augsburg was opened. The line was used by about 400 passengers daily.[14]\\r\\nThe first complaints were made about the location of the station in 1841. The station was too far from the city centre, so the trip to the station was too costly.[17] The wooden building was considered to be too small for a city like Munich and not very impressive.[18] King Ludwig commissioned the architect Friedrich von G?rtner to redesign the station in 1843. It would be closer to the city centre, as the old station was half an hour away from the city.[19] When, in 1844, the Munich-Augsburg Railway Company was nationalised, the first steps for the realisation of a new station building were carried out. Three new plans were presented. The station under the first option would have been at the shooting range, under the second option it would have been on the Marsfeld plain and under the third it would have been on Sonnenstra?e. In the following years, the state and the city could not choose between the three proposals. The station suffered a major fire on 4 April 1847, although its cause could not be determined. No one was injured. Parts of the freight and operations facilities were destroyed.[13] The decision on where to construct a new station had to be taken now.[20] On 5 April 1847, the king of Bavaria decided to build the new station at the shooting range. The station at Marsfeld was to be restored in the autumn of 1847 to serve until the completion of the new station.[14][21] Due to a delay in the construction, the tracks were extended to the buildings of the former shooting range. The building of the shooting range now served as an entrance building to the new station, which was opened on 15 November 1847.[13]\\r\\nDirection of the construction was transferred to the architect Friedrich Brklein, a disciple of Friedrich von G?rtner. The new station hall was opened in 1848. It was 111 metres long, 29 metres wide and 20 metres high and had room for five tracks.[14] The station building was opened a year later, on 1 October 1849. The station was used daily by around 1,500 passengers.[19] The building was built of red and yellow brick in the Rundbogenstil with Romanesque revival and Italian Renaissance forms; sand and limestone were also used for individual components. The station building was a basilica-like building, which was extended with a pavilion on the east side.[22] It was equipped with the latest technology, a central hot water heater and a mechanical clock with a central drive, with dials that were up to 130 metres from the central mechanism. The station was illuminated from 1851 by coal gas.\\r\\nThe new building proved again to be too small with the opening of the railway to Landshut in 1858. This meant that the Royal Bavarian Eastern Railway Company (K?niglich privilegirte Actiengesellschaft der bayerischen Ostbahnen) built a station north of the actual station. The new station, also called the Ostbahnhof, consisted of a 145 metre long and 24 metre wide platform hall with four tracks. This became a carriage house with three tracks, a goods shed and other outbuildings.[14] In 1859, the Bavarian Eastern Railways line to Nuremberg was commissioned. On 12 August 1860, the RosenheimÿSalzburg railway was opened, adding extra importance to the station. As no more platforms were available in the main hall, trains had to use the Ostbahnhof. The station was also used by international passengers and, in 1860, it was already used by 3,500 passengers daily.[19] To the south a station was also built for postal services in the same style as the other buildings.[23]\\r\\nThe opening of the line from Munich to Ingolstadt in 1867, the MunichÿMhldorfÿSimbach and the MunichÿGrafingÿRosenheim lines in 1871 and the MunichÿBuchloe in 1873 created further capacity problems.[14] Thus, two projects were developed: Friedrich Brklein planned another wing station. The other option was a new building, requiring the demolition of the Ostbahnhof. They chose the second option. So from 1877 to 1883 under the leadership of Carl Schnorr von Carlsfeld, Jacob Graff and Heinrich Gerber, a new concourse was built with 16 tracks.[19] Carl Schnorr von Carlsfeld was responsible for the redesign of the tracks, Jacob Graff was the site manager of the building and Heinrich Gerber was in charge of the construction.[14] The old hall was twice the size of the new, so that the front part of it remained as the main hall. The other premises were extended. The project was completed at the end of the 1883.[25]\\r\\nThe Munich Centralbahnhof precinct was divided into three station sections. The first section, which was also called the inner section, took over passenger, express freight, and small freight operations. The middle section at Arbeitersteg (workers bridge, now called Donnersberger Bridge) contained wagonload operations and the marshalling yard. The outer section ended at the Friedenheimer Bridge and included locomotive and carriage sheds and the central workshop.[26] The station was 2.9?km long up to its last crossover and 580 metres wide at its widest point. There were 226 sets of points, 42 turntables and 82.3?km of tracks.[14]\\r\\nA few years later, the station again proved to be too small. The architect Friedrich Graf suggested the station be relocated to Landsbergerstra?e to create a circle line from South station via Schwabing station to a planned North station. The plans were not realised, instead, freight was separated from passenger operations so that the Hauptbahnhof became a passenger-only station. Now freight was handled at the Laim marshalling yard.[19] Construction began in 1891.[14]\\r\\nIn 1893, the Royal Bavarian State Railways opened the Starnberg wing station (Starnberger Flgelbahnhof), partly serving traffic on the line to Starnberg. It had six tracks and only had a temporary wooden building. Long-distance traffic was now concentrated in the main hall and local traffic towards Pasing was moved to the wing station. In 1897, the wing station received Bavarias first electro-mechanical interlocking.[14] In 1896, the Laim marshalling yard was opened; only the handling of small goods could not be moved to Laim. In addition, the line to Landshut was moved to a new course running to the west of Nymphenburg Park to allow a connection to the Laim yard. Next new flyovers were built on the line towards Pasing.[25]\\r\\nOn 1 May 1904 was the station's name was changed from Mnchen Centralbahnhof (\\"central station\\") to Mnchen Hauptbahnhof (\\"main station\\"). The station now had 22 tracks and handled 300 trains daily. In subsequent years, the station, which then served a city of 407,000, handled 18,000 passengers per day.[14] The passenger numbers continued to rise, and further extensions were planned. FX Liebig and Theodore Lechner recommended a new through station on the Kohleninsel (Coal Island, now called Museum Island) to improve connections to the Isar Valley Railway. This is now the location of the Deutsches Museum.[27] Other possibilities considered were a through station west of Hackerbrcke (Hacker Bridge), on the site of the current S-Bahn station, and connected to the East station by a tunnel, transferring local traffic only to an underground station and moving the main station to the South Station.[28]\\r\\nIn a memorandum of September 1911, the Bavarian government discarded all these options in favour of an extension of the Starnberg wing station and the construction of Holzkirchen wing station (Holzkirchner Bahnhof), partly serving the line to Holzkirchen.[14] It was also planned to relocate all the local traffic to the wing stations. It was assumed from the outset that in the future a through station would be appropriate.[29] Construction began in 1914, and continued through the First World War, but it was delayed. The wing stations finally opened on 30 April 1921. Local traffic was largely shifted to the wing stations. The station reached 36 tracks in its largest expansion since the Holzkirchen wing station included an additional ten tracks.[30] The trains were controlled by nine electromechanical interlockings built from 1922 to 1929.\\r\\nBetween 1925 and 1927, six of the lines beginning in Munich were electrified so that all parts of the station except the Holzkirchen wing station received overhead lines.[19] This was part of the Deutsche Reichsbahns new restructuring plan. The Reichsbahn planned to move the station to the west of the Hacker Bridge. A connection to the South Ring (Sdring) by a 1,900 metre long tunnel under the Theresienwiese was part of the plan. Local traffic would still terminate at an adjacent terminal station. Laim marshalling yard would have to be demolished under these plans and a new marshalling yard would be built in Milbertshofen instead. As a result of the Great Depression during the following years, none of these plans were realised.[31]\\r\\nFrom 1933, Adolf Hitler directed Hermann Alker to create new plans for rebuilding the station. A new station would be built between Laim and Pasing stations and the old railway tracks would be replaced by a boulevard from Karlsplatz to the new station. In addition, a U-Bahn was planned from the new station to the central city under the boulevard. Alkers presented his plans but his client was not satisfied, as the station building would not look impressive at the end of the 120 metre wide boulevard. In 1938, Hermann Giesler, solved the problem by turning the station to a 45-degree angle to the road. He planned a huge domed building with a height of 136 metres and a diameter of 265 metres.[31] In May 1942, Deutsche Reichsbahn began on Hitlers instruction to develop plans for a broad-gauge railway that would connect the whole of Europe.[25] This was planned to have a track gauge of three metres with a structure gauge of eight by eight metres. Munich would be on broad gauge lines between Berlin and Munich and between Paris and Vienna. The ten standard gauge tracks and the four broad gauge tracks would be laid in an underground tunnel seven metres below the surface. These plans were not realised, however.[32][33]\\r\\nThe timetable of the summer 1939 showed the station had a total of 112 arrivals and departures by scheduled long-distance services each day. It was the eleventh busiest node of Deutsche Reichsbahns long-distance network.[34]\\r\\nDuring World War II the station suffered heavy damage from American bombing, but train services resumed after each air raid.[14] However, after bombings from 11 July to 13 July 1944, trains had to be diverted because of the impact of 112 bombs. It was only possible for trains to reach Pasing. All trains had to either run around Munich at a distance or use the North Ring as a bypass. Overall, the loss amounted to 7.1 million Reichsmarks. In addition, there were numerous deaths and injuries. On 30 April 1945, American troops entered Munich and initially German troops were ordered to defend the station.[19] Since a counter-attack would have been pointless, it was not carried out. Reconstruction started on 6 May 1945 on the building despite shortages and a complicated approval process. On 24 July 1945 it was possible to operate 128 trains. From 16 December there were 235 trains per day.[25]\\r\\nThe train shed was demolished from 16 May to 16 August 1949, due to the danger of it collapsing, and then the remaining buildings were demolished to enable their reconstruction. A new beginning after the war was marked in May 1950 by the construction of the new Starnberg wing station, designed by Heinrich Gerbl. Its monumental neoclassicism was seen as backward looking and the pillared hall were criticised for being reminiscent of the Nazi period. The main hall had a width of 240 metres and a length of 222 metres. In the same year, the first four areas of the new main hall were completed. A hotel was opened in 1951 in the southern part of the station. From 26 July 1952 pushÿpull operations were introduced to avoid a change of locomotives. The main hall was put in operation in 1953. The electrification of the Holzkirchen wing station followed in May 1954. The commissioning of radio for shunting operations on 6 February 1956 simplified shunting in the station area. A roof was completed on the concourse of the Holzkirchen wing station on 1 August 1958.[14] The construction of the hall in the main station building, based on plans by Franz Hart, was completed on 1 August 1960.[25] The hall is 140 metres wide and 222 metres long. In addition to the columns at the edge of a span of 70 metres, it has a middle row of columns, which was unusual at the time. The current station building was completed on 1 August 1960.[35]\\r\\nThe central signalling centre was brought into operation on 11 October 1964 at 4 AM.[36] The new signal box controlled 295 sets of points and 446 signals and detected occupancy on 300 sections of track and seven automatic block sections.[14] In the signalling centre there were four interlockings, one controlling the Holzkirchen wing station, two controlling the tracks of the main hall and the other one controlling the Starnberg wing station. The new interlockings needed only 38 staff for operations and 12 for maintaining the signal technology, saving 93 jobs.[25]\\r\\nIn the following years, postal operations, which included the station's own underground post office railway, had growing problem due to the interference of passengers. On 18 August 1969, a separate package handling facility was brought into operation at Wilhelm-Hale-Stra?e, which was connected with the station by a double-track line.[37]\\r\\nThe Starnberg wing station was affected by the construction of the S-Bahn trunk line from 1967 because the trunk line was built under it. The trunk line and the new underground station were taken into operation on 28 April 1972 in time for the 1972 Summer Olympics. During the Summer Olympics the station had a high volume of passengers. On 2 September 1972, there were, for example, 35,000 passengers, excluding S-Bahn operations.[14] The first U-Bahn lines, U8/U1 (now U2/U1) commenced operations through the station on 18 October 1980. As a further development of the S-Bahn, the line to Wolfratshausen as S-Bahn line S 7 was connected to the trunk line with a 260-metre-long tunnel under all the tracks on 31 May 1981.[14] Until then the S-Bahn trains to and from Wolfratshausen, then called line S 10, ended and started in Holzkirchen wing station. The U-Bahn platform on lines U4/U5 opened on 10 March 1984. In the 1980s, the entrance building was converted under the leadership of Ekkehard Fahr, Dieter Schaich and Josef Reindl into a circulating hall with a travel centre in order to create a transparent and open environment. In the timetable of the summer of 1989, the station was the twelfth largest node in the network of Deutsche Bundesbahn, with 269 arrivals and departures by scheduled long-distance services per day.[38]\\r\\nThe platforms were thin with a width of 5.4 to 6 metres and were 20 centimetres too low. After the elimination of the 3.2 metre-wide baggage platforms, new passenger platforms were built that are up to 76 centimetres high and up to 10.2 metres wide. In addition, the facilities of the platforms, such as benches, were renewed and some platforms were extended to be 430 metres long. A baggage tunnel was put into operation under tracks 12 and 13.[14] The northern and southern carriage sidings and the maintenance facilities were extended. The construction work began in August 1976. It was completed at Christmas 1987.[14]\\r\\nA new split-flap display was installed in 1981 at the cross platform concourse. The individual platforms, except for the Holzkirchen wing station platforms, were given split-flap destination displays. These replaced panels that were once attached to the buffer stops. Some still exist at the Holzkirchen wing station, but are no longer used. An additional 37 monitors were installed at internal sites such as the ticket office. All displays are controlled by a computer, on which all changes to the basic timetable are stored. They are updated by the signal centre.[14] A washing plant was established to the south of the tracks for Intercity-Express trains in 1991 and in the following years it was expanded into an ICE depot.[39] Since 2004, the entire area of the station has video monitoring.[40] The 70 cameras are controlled by the control center of DB Security in the station.[41] Meanwhile, the split-flap displays have been replaced with more modern LCD displays. The loudspeaker systems have also been modernised.\\r\\nThe construction of a second S-Bahn route (a second main tunnel route through the centre of Munich) with a new S-Bahn station is being planned for the station hall. Because of challenges with the planning and financing of the route, it will probably not be finished until 2020.\\r\\nA new front for the railway station and service hall are to be built to a design by Auer+Weber+Assoziierte. Because of difficulties in financing, it is questionable when the project will be started.[42]\\r\\nA Transrapid route to Munich Airport was under consideration for some time and intended to be operational around 2011. However, construction never started due to rising costs caused by increasing prices for steel and other materials. It was finally suspended when cost estimates reached ?3.2 billion, up from an original estimate of ?1.85 billion.\\r\\nApart from Lindau Hauptbahnhof, Mnchen Hauptbahnhof is the only major terminal station in Bavaria. The station is used by about 450,000 passengers a day[6] and is one of 21 stations classified by Deutsche Bahn as a category 1 station.[5] There are 32 tracks, split over the original three stations:[2][3]\\r\\nThe subterranean Munich S-Bahn station is separated operationally from the mainline station and known as Mnchen Hbf (tief). To optimise passenger flow, separate platforms for entering (centre) and disembarking (outer) trains exist. The subway station is situated near the U-Bahn lines for the U1 and U2 trains, but if one wishes to change from the S-Bahn to U4/U5 trains, it is more practical to stay on the S-Bahn to Karlsplatz (Stachus), as the U4/U5 station is on the opposite side of the station.\\r\\nDue to the station's size, walking from one platform to another may take a considerable amount of time. Deutsche Bahn recommends planning for a minimum walking time of 10 minutes from the central hall to Starnberger Bahnhof or Holzkirchner Bahnhof; 15 minutes between Starnberger and Holzkirchner Bahnhof; and 15 minutes between the S-Bahn station and Holzkirchner Bahnhof.\\r\\nThe two outlying parts of the station have shorter tracks than the main hall, which means passengers always have to walk down most of the length of either platform 11 or 26 when changing from there. Unlike Frankfurt Hbf or Leipzig Hbf, there is no passenger tunnel under the tracks.\\r\\nThe mainline station lobby is only closed between 1:30 and 3:00, but platforms can be reached all the time. The S-Bahn station operate almost 24/7, and the U-Bahn station closes only between 1:30 and 4:00 (2:30-4:00 on weekends).\\r\\nOn the ground floor of this station many shops exist where you can shop for daily household needs, dressing, and you will find almost all major brands of places to eat.\\r\\nThe station is the southern point of the InterCityExpress line to Hamburg-Altona via the Hanover-Wrzburg high-speed rail line. It also has frequent links to Dortmund via Frankfurt and Cologne using the Cologne-Frankfurt high-speed rail line. The most recent addition is the Nuremberg-Ingolstadt high-speed rail line, which has greatly benefited from Munich traffic. Additional ICE services using mainly ordinary lines on their run exist to Vienna, Berlin and a number of other cities. There are also numerous InterCity and EuroCity services to most parts of Germany as well as neighbouring Austria, Switzerland, France and Italy. The station has a number of DB NachtZug and CityNightLine services to northern Germany, the Netherlands, Denmark, France and Italy, though facilities for the autoracks in these night services are located at Mnchen Ost railway station. Night services operated by other railway companies also can be seen at the station, for example to Rome, Budapest or Zagreb.\\r\\nThere are numerous RegionalExpress and RegionalBahn services to Landshut, Regensburg, Plattling, Passau, Kempten, Lindau, Garmisch-Partenkirchen and Nuremberg among others. The Bayerische Oberlandbahn operates services to Bayrischzell, Lenggries and Tegernsee.\\r\\nAll lines are electrified, except the ones to Mhldorf, Kempten and Lindau and the lines of the Bayerische Oberlandbahn. To minimise pollution, services using these lines preferably end at tracks 5-10 and 27-36.\\r\\nThe Munich S-Bahn operates through a separate part of the station as a S-Bahn station on the S-Bahn trunk line (S-Bahn-Stammstrecke) with two tracks and three platforms in the Spanish solution, which is in the northern basement at level -2. This station is served by seven S-Bahn lines S 1, S 2, S 3, S 4, S 6, S 7 and S 8. The planned construction of a new S-Bahn station as part of the construction of the second trunk line (zweiten Stammstrecke) at level -5, formerly intended to start in 2006, has been delayed due to financing issues.\\r\\nIn the east of the main hall at ground level and on the first floor there are several food shops, newsagents, flower and gift shops, etc. There is also an extensive shopping arcade in the basement to the north and east, as well as direct access to adjacent stores in the inner city through a shopping arcade. Since 1995, the Children and Youth Museum of the City of Munich (Kinder- und Jugendmuseum Mnchen) has been located in the Starnberg wing station. In the southern part of the building there is an InterCityHotel. As with many stations, a few hotels are located around the station, including the luxury hotel Sofitels Munich Bayerpost and Le Mridien. At the southernmost platform 11 there is an office of the Bahnhofsmission charity, which provides travellers and the homeless with around the clock assistance, food and rest facilities. In the northern section there is a police station of the Munich and Federal Police. In the first floor of the northern wing there is a canteen (\\"Casino\\") for employees of the DB and their guests. Two parking decks on the fourth and fifth floors of the main building are accessible from Bayerstra?e and Arnulfstra?e.\\r\\nAt the Hauptbahnhof there are two underground stations of the Munich U-Bahn.\\r\\nThe underground station of Munich U-Bahn trunk line 2 is at level -4 and is orientated in a north-south direction under the station forecourt and has four tracks. It branches to the north as line U1 to Olympia-Einkaufszentrum and line U 2 to Feldmoching. It was originally planned to build the station under the Kaufhaus Hertie department store. To enable shorter connections to the main hall and the underground station of lines U 4 and U 5 it was decided instead to build it directly next to the main station. Construction of the U-Bahn station began in the spring of 1975, which required the closure of the station forecourt to surface traffic. The building was built because of its great breadth and depth by the cut and cover method. First the side walls and the roof were built and then the individual levels were built from top to bottom. The U-Bahn station was opened on 18 October 1980. The station is differentiated from the other U-Bahn stations opened in 1980 on line U 2 by the silver lining of the walls opposite the platform and on the pillars in the middle of the station. The platforms connect at the northern end via a mezzanine level to the S-Bahn station and at the south end there is another mezzanine connecting with the U-Bahn station of lines U 4 and U 5. In the middle of the platform escalators lead a mezzanine level connecting with the station forecourt.[44]\\r\\nThe station of U-Bahn trunk line 3 is on level -2 and is orientated in an east-west direction under Bayerstra?e south of the main station. The station was opened on 10 March 1984.The silver-coloured tunnel-like walls opposite the platforms are curved inward, which give the station a tubular character. The platform does not have columns and is on a slight curve. The lighting is on struts arranged in a square under a retracted ceiling. At the eastern end of the platform is a connection via a mezzanine to the underground station of lines 1 and 2. There is a connection to the southern entrance of the mainline station at the entrance level at the western end of the station. In addition, there is a lift at this end, which provides the disabled with access to the U-Bahn platform.[45]\\r\\nThere are also four stops of the Munich tramway in the vicinity of the station, called Hauptbahnhof, Hauptbahnhof Nord, Hauptbahnhof Sd and Holzkirchner Bahnhof. Hauptbahnhof Nord is served by routes 16, 17, 20, 21 and 22. The Hauptbahnhof stop in the station forecourt is served by almost all lines (16, 17, 19, 20, 21 and 22), with lines 20, 21 and 22 only stopping towards the city. The Hauptbahnhof Sd and Holzkirchner Bahnhof stops are only used by lines 18 and 19.[46]\\r\\nThe first tram line to operate through the station forecourt, was a horse tram line that ran from Promenadenplatz to Maillingerstra?e and was opened on 21 October 1876. In the next few years the horse tram network was expanded so that by 1900 four tram lines served the station. The tram network was further expanded and electrified as trams became the main mode of transport. By 1938, nine lines operated to the Hauptbahnhof, which was one of the focal points of the Munich tram network. In 1966, the Hauptbahnhof was served by ten tram lines. In the following period the number of trams in Munich was reduced as a result of the construction of the U-Bahn, but the lines that served Munich Hauptbahnhof were almost unaffected, so that the number even increased to eleven at one point.[47]\\r\\nMetro bus line 58 of the Mnchner Verkehrsgesellschaft (Munich Transport Company) stops at all tram stops except Hauptbahnhof Sd. The Hauptbahnhof Nord stop is also served by museum bus line 100 and some regional bus services.","input":"What is the main train station in munich?"},{"output":"curling","context":"Olympic sports are sports contested in the Summer and Winter Olympic Games. The 2016 Summer Olympics included 28 sports, with five additional sports due to be added to the 2020 Summer Olympics. The 2014 Winter Olympics included seven sports.[1] The number and kinds of events may change slightly from one Olympiad to another. Each Olympic sport is represented by an international governing body, namely an International Federation (IF).[2] The International Olympic Committee (IOC) establishes a hierarchy of sports, disciplines, and events.[2] According to this hierarchy, the Olympic sports can be subdivided into multiple disciplines, which are often assumed to be distinct sports. Examples include swimming and water polo (disciplines of aquatics, represented by the International Swimming Federation),[3] or figure skating and speed skating (disciplines of skating, represented by the International Skating Union).[4] In their turn, disciplines can be subdivided into events, for which medals are actually awarded.[2] A sport or discipline is included in the Olympic program if the IOC determines it is widely practiced around the world, that is, the number of countries that compete in a given sport is the indicator of the sport's prevalence. The IOC's requirements reflect participation in the Olympic Games as wellmore stringent toward men (as they are represented in higher numbers) and summer sports (as more nations compete in the Summer Olympics).\\r\\nPrevious Olympic Games included sports which are no longer present on the current program, like polo and tug of war.[5] These sports, known as \\"discontinued sports\\", were later removed either because of lack of interest or absence of an appropriate governing body.[2] Archery and tennis are examples of sports that were competed at the early Games and were later dropped by the IOC, but managed to return to the Olympic program (in 1972 and 1988, respectively). Demonstration sports have often been included in the Olympic Games, usually to promote a local sport from the host country or to gauge interest and support for the sport.[6] Some such sports, like baseball and curling, were added to the official Olympic program (in 1992 and 1998, respectively). Baseball, however, was discontinued after the 2008 Summer Olympics.\\r\\n\\r\\n\\r\\nThe term \\"sport\\" in Olympic terminology refers to all the events that are sanctioned by one international sport federation, a definition that may be different from the common meaning of the word sport. One sport, by Olympic definition, may be divided into several disciplines, which are often regarded as separate sports in common language.\\r\\nFor example, aquatics is a summer Olympic sport that includes six disciplines: swimming, synchronized swimming, diving, water polo, open water swimming, and high diving (the last of which is a non-Olympic discipline), since all these disciplines are governed at international level by the International Swimming Federation.[1] Skating is a winter Olympic sport represented by the International Skating Union, and includes four disciplines: figure skating, speed skating (on a traditional long track), short track speed skating, and synchronized skating (the latter is a non-Olympic discipline).[1] The sport with the largest number of Olympic disciplines is skiing, with six: alpine skiing, cross-country skiing, ski jumping, nordic combined, snowboarding, and freestyle skiing.\\r\\nOther notable multi-discipline sports are gymnastics (artistic, rhythmic, and trampoline), cycling (road, track, mountain, and BMX), volleyball (indoors and beach), wrestling (freestyle and Greco-Roman), canoeing (flatwater and slalom), and bobsleigh (includes skeleton). The disciplines listed here are only those contested in the Olympicsgymnastics has two non-Olympic disciplines, while cycling and wrestling have three each.\\r\\nIt should also be noted that the IOC definition of a \\"discipline\\" may differ from that used by an international federation. For example, the IOC considers artistic gymnastics a single discipline, but the International Federation of Gymnastics (FIG) classifies men's and women's artistic gymnastics as separate disciplines.[7] Similarly, the IOC considers freestyle wrestling to be a single discipline, but United World Wrestling uses \\"freestyle wrestling\\" strictly for the men's version, classifying women's freestyle wrestling as the separate discipline of \\"female wrestling\\".[8]\\r\\nOn some occasions, notably in the case of snowboarding, the IOC agreed to add sports which previously had a separate international federation to the Olympics on condition that they dissolve their governing body and instead affiliate with an existing Olympic sport federation, therefore not increasing the number of Olympic sports.\\r\\nAn event, by IOC definition, is a competition that leads to the award of medals. Therefore, the sport of aquatics includes a total of 46 Olympic events, of which 32 are in the discipline of swimming, eight in diving, and two each in synchronized swimming, water polo, and open water swimming. The number of events per sport ranges from a minimum of two (until 2008, there were sports with only one event) to a maximum of 47 in athletics, which despite its large number of events and its diversity is not divided into disciplines.\\r\\nThe list of Olympic sports has changed considerably during the course of Olympic history, and has gradually increased until the early 2000s, when the IOC decided to cap the number of sports in the Summer Olympics at 28.\\r\\nThe only summer sports that have never been absent from the Olympic program are athletics, aquatics (the discipline of swimming has been in every Olympics), cycling, fencing, and gymnastics (the discipline of artistic gymnastics has been in every Olympics).\\r\\nThe only winter sports that were included in all Winter Olympic Games are skiing (only nordic skiing), skating (figure skating and speed skating), and ice hockey. Figure skating and ice hockey were also included in the Summer Olympics before the Winter Olympics were introduced in 1924.\\r\\nFor most of the 20th century, demonstration sports were included in many Olympic Games, usually to promote a non-Olympic sport popular in the host country, or to gauge interest and support for the sport.[6] The competitions and ceremonies in these sports were identical to official Olympic sports, except that the medals were not counted in the official record. Some demonstration sports, like baseball and curling, were later added to the official Olympic program. This changed when the International Olympic Committee decided in 1989 to eliminate demonstration sports from Olympics Games after 1992.[9] An exception was made in 2008, when the Beijing Organizing Committee received permission to organize a wushu tournament.[10][11]\\r\\nA sport or discipline may be included in the Olympic program if the IOC determines that it is widely practiced around the world, that is, the number of countries and continents that regularly compete in a given sport is the indicator of the sport's prevalence. The requirements for winter sports are considerably lower than for summer sports since many fewer nations compete in winter sports. The IOC also has lower requirements for inclusion of sports and disciplines for women for the same reason. Women are still barred from several disciplines; but on the other hand, there are women-only disciplines, such as rhythmic gymnastics and synchronized swimming.\\r\\nSports that depend primarily on mechanical propulsion, such as motor sports, may not be considered for recognition as Olympic sports, though there were power-boating events in the early days of the Olympics before this rule was enacted by the IOC.[2][12] Part of the story of the founding of aviation sports' international governing body, the FAI, originated from an IOC meeting in Brussels, Belgium on June 10, 1905.[13]\\r\\nThese criteria are only a threshold for consideration as Olympic sport. In order to be admitted to the Olympic program, the IOC Session has to approve its inclusion. There are many sports that easily make the required numbers but are not recognized as Olympic sports, mainly because the IOC has decided to put a limit on the number of sports, as well as events and athletes, in the Summer Olympics in order not to increase them from the 28 sports, 300 events, and 10,000 athletes of the 2000 Summer Olympics.\\r\\nNo such limits exist in the Winter Olympics and the number of events and athletes continue to increase, but no sport has been added since 1998. The latest winter sport added to the Winter Olympics was curling in 1998.\\r\\nPrevious Olympic Games included sports which are no longer present on the current program, like polo and tug of war.[1] In the early days of the modern Olympics, the organizers were able to decide which sports or disciplines were included on the program, until the IOC took control of the program in 1924. As a result, a number of sports were on the Olympic program for relatively brief periods before 1924.[2] These sports, known as discontinued sports, were removed because of lack of interest or absence of an appropriate governing body, or because they became fully professional at the time that the Olympic Games were strictly for amateurs, as in the case of tennis.[2] Several discontinued sports, such as archery and tennis, were later readmitted to the Olympic program (in 1972 and 1984, respectively). Curling, which was an official sport in 1924 and then discontinued, was reinstated as Olympic sport in 1998.\\r\\nThe Olympic Charter decrees that Olympic sports for each edition of the Olympic Games should be decided at an IOC Session no later than seven years prior to the Games.\\r\\nThe only sports that have been dropped from the Olympics since 1936 are baseball and softball, which were both voted out by the IOC Session in Singapore on July 11, 2005,[14] a decision that was reaffirmed on February 9, 2006.[15] These sports were last included in 2008, although officially they remain recognized as Olympic sports in the Olympic Charter. Therefore, the number of sports in the 2012 Summer Olympics was dropped from 28 to 26.\\r\\nFollowing the addition of women's boxing in 2012, and women's ski jumping in 2014, there are only Greco-Roman wrestling and nordic combined, respectively, that are only for men in those games.\\r\\nTwo previously discontinued sports, golf and rugby, returned for the 2016 Summer Olympics. On August 13, 2009, the IOC Executive Board proposed that golf and rugby sevens be added to the Olympic program for the 2016 Games.[16] On 9 October 2009, during the 121st IOC Session in Copenhagen, the IOC voted to admit both sports as official Olympic sports and to include them in the 2016 Summer Olympics.[17] The IOC voted 81ÿ8 in favor of including rugby sevens and 63ÿ27 in favor of reinstating golf, thus bringing the number of sports back to 28.[17]\\r\\nIn February 2013, the IOC considered dropping a sport from the 2020 Summer Olympics to make way for a new sport. Modern pentathlon and taekwondo were thought to be vulnerable, but instead the IOC recommended dismissing wrestling.[18] On September 8, 2013, the IOC added wrestling to the 2020 and 2024 Summer Games.[19]\\r\\nOn August 3, 2016, the IOC voted to add baseball/softball, karate, sport climbing, surfing, and skateboarding for the 2020 Summer Olympics.[20]\\r\\nAt the first Olympic Games, nine sports were contested.[21] Since then, the number of sports contested at the Summer Olympic Games has gradually risen to twenty-eight on the program for 2000ÿ2008. At the 2012 Summer Olympics, however, the number of sports fell back to twenty-six following an IOC decision in 2005 to remove baseball and softball from the Olympic program. These sports retain their status as Olympic sports with the possibility of a return to the Olympic program in future games.[14] At the 121st IOC Session in Copenhagen on 9 October 2009, the IOC voted to reinstate both golf and rugby to the Olympic program, meaning that the number of sports to be contested in 2016 was once again 28.[22]\\r\\nIn order for a sport or discipline to be considered for inclusion in the list of Summer Olympics sports, it must be widely practiced in at least 75 countries, spread over four continents.\\r\\nThe following sports (or disciplines of a sport) make up the current and discontinued Summer Olympic Games official program and are listed alphabetically according to the name used by the IOC. The discontinued sports were previously part of the Summer Olympic Games program as official sports, but are no longer on the current program. The figures in each cell indicate the number of events for each sport contested at the respective Games; a bullet (?) denotes that the sport was contested as a demonstration sport.\\r\\nEight of the 34 sports consist of multiple disciplines. Disciplines from the same sport are grouped under the same color:\\r\\n???? Aquatics ÿ ???? Basketball ÿ ???? Canoeing/Kayak ÿ ???? Cycling ÿ ???? Gymnastics ÿ ???? Volleyball ÿ ???? Equestrian ÿ ???? Wrestling\\r\\nThe following sports or disciplines have been demonstrated at the Summer Olympic Games for the years shown, but have never been included on the official Olympic program:\\r\\nGliding was promoted from demonstration sport to an official Olympic sport in 1936 in time for the 1940 Summer Olympics, but the Games were cancelled due to the outbreak of World War II.[23][24]\\r\\nSummer Olympic sports are divided into categories based on popularity, gauged by: television viewing figures (40%), Internet popularity (20%), public surveys (15%), ticket requests (10%), press coverage (10%), and number of national federations (5%). The category determines the share the sport's International Federation receives of Olympic revenue.[25][26]\\r\\nThe current categories, as of 2013, are as follows, with the pre-2013 categorizations also being available.[27] Category A represents the most popular sports; category E lists either the sports that are the least popular or that are new to the Olympics (golf and rugby).\\r\\nBefore 1924, when the first Winter Olympic Games were celebrated, sports held on ice, like figure skating and ice hockey, were held at the Summer Olympics.[28] These two sports made their debuts at the 1908 and the 1920 Summer Olympics, respectively, but were permanently integrated in the Winter Olympics program as of the first edition. The International Winter Sports Week, later dubbed the I?Olympic Winter Games and retroactively recognized as such by the IOC, consisted of nine sports. The number of sports contested at the Winter Olympics has since been decreased to seven, comprising a total of fifteen disciplines.[29]\\r\\nA sport or discipline must be widely practiced in at least 25 countries on three continents in order to be included on the Winter Olympics program.[2]\\r\\nThe following sports (or disciplines of a sport) make up the current Winter Olympic Games official program and are listed alphabetically, according to the name used by the IOC. The figures in each cell indicate the number of events for each sport that were contested at the respective Games (the red cells indicate that those sports were held at the Summer Games); a bullet denotes that the sport was contested as a demonstration sport. On some occasions, both official medal events and demonstration events were contested in the same sport at the same Games.\\r\\nThree out of the seven sports consist of multiple disciplines. Disciplines from the same sport are grouped under the same color:\\r\\n???? Skating ÿ ???? Skiing ÿ ???? Bobsleigh\\r\\nMilitary patrol was an official skiing event in 1924 but the IOC currently considers it an event of biathlon in those games, and not as a separate sport.\\r\\nThe following sports have been demonstrated at the Winter Olympic Games for the years shown, but have never been included on the official Olympic program:\\r\\nIce climbing was showcased in 2014, and will be demonstrated at the 2018 Winter Olympics, with a plan to be included official competition sport for the 2022 Winter Olympics.[30] Ski ballet, similarly to Military Patrol, was simply a demonstration event falling under the scope of freestyle skiing. Disabled sports are now part of the Winter Paralympic Games.\\r\\nMany sports are not recognized as Olympic sports although their governing bodies are recognized by the IOC.[31] Such sports, if eligible under the terms of the Olympic Charter, may apply for inclusion in the program of future Games, through a recommendation by the IOC Olympic Programme Commission, followed by a decision of the IOC Executive Board and a vote of the IOC Session. When Olympic demonstration sports were allowed, a sport usually appeared as such before being officially admitted.[6] An International Sport Federation (IF) is responsible for ensuring that the sport's activities follow the Olympic Charter. When a sport is recognized the IF become an official Olympic sport federation and can assemble with other Olympic IFs in the Association of Summer Olympic International Federations (ASOIF, for summer sports contested in the Olympic Games), Association of International Olympic Winter Sports Federations (AIOWS, for winter sports contested in the Olympic Games) or Association of IOC Recognised International Sports Federations (ARISF, for sports not contested in the Olympic Games).[1] A number of recognized sports are included in the program of the World Games, a multi-sport event run by the International World Games Association, an organization that operates under the patronage of the IOC. Since the start of the World Games in 1981, a number of sports, including badminton, taekwondo, and triathlon have all subsequently been incorporated into the Olympic program.\\r\\nIn 2020, the IOC altered the way it plans the Olympic Games from one based around a maximum number of sports, to taking total events into account, opening the schedule up for the inclusion on a Games by Games basis of additional sports to the 25 \\"core\\" sports. For the 2020 Summer Olympics, the local organizing committee was thus permitted to add a total of five sports to the programme in addition to the existing 28, taking the total to 33.[32][33]\\r\\nThe governing bodies of the following sports, though not contested in the Olympic Games, are recognized by the IOC:[34]\\r\\n\\r\\n1 Official sport at the World Games\\r\\n2 Discontinued Olympic sport\\r\\n3 Ineligible to be included because the Olympic Charter bans sports with motorization elements\\r\\n4 The governing bodies for baseball and softball merged into a single international federation in 2013.\\r\\n5 Included at the 2020 Summer Olympics","input":"What is the newest winter olympic sport added?"},{"output":"\\"Love Will Keep Us Together\\" by Captain & Tennille","context":"These are the Billboard magazine Hot 100 number one hits of 1975. Both 1974 and 1975 hold the Hot 100 record for the year with the most #1 hits with 35 songs reaching the # 1 spot. Additionally, the period beginning January 11 and ending April 12 constitutes the longest run of a different #1 song every week (14 weeks) in Billboard history. Coincidentally, it both begins and ends with songs by Elton John. The longest running number one song of 1975 is \\"Love Will Keep Us Together\\" by Captain & Tennille.","input":"What was the number 1 song in 1975?"},{"output":"Erik the Red","context":"The exploration of North America by non-indigenous people was a continuing effort to map and explore the continent of North America. It spanned centuries, and consisted of efforts by numerous people and expeditions from various foreign countries to map the continent. The European colonization of the Americas followed.\\r\\n\\r\\n\\r\\nAccording to the Sagas of Icelanders, Norse sailors (often called Vikings) from Iceland first settled Greenland in the 980s. Erik the Red explored and settled southwestern Greenland, which he named to Garrett Spears potential Icelandic settlers, eventually establishing the Eastern and Western Settlements, which were abandoned around 1350.\\r\\nL'Anse aux Meadows, an archaeological site on the northernmost tip of Newfoundland, and a second site in southwestern Newfoundland, are the only known sites of a Norse village in North America outside of Greenland. These sites are notable for their possible connections with the attempted colony of Vinland established by Leif Erikson in 1003.\\r\\nThe Viking voyages did not become common knowledge in the Old World, and Europeans remained ignorant of the existence of the Americas as a whole, until the first decades following the year 1492. Many expeditions were launched from European nations in search of a Northwest Passage to East Asia (or \\"the Indies\\" as the region was called) in order to establish a shorter trade route to China than the Silk Road, a trade route which had become desperately needed and yet exacerbated by the Fall of Constantinople. Also, the Castilian crown needed an alternative to the Portuguese controlled eastern maritime trade route around Africa to India and East Asia.\\r\\nOn August 3, 1492, Christopher Columbus set sail from the Port of Palos de la Frontera in the Province of Huelva, from the newly los Reyes Cat܇licos coordinated Kingdoms of Castile and Aragon, in present-day Spain, financed by Queen Isabella I of Castille. Columbus's Letter on the First Voyage of his discovery of the Bahamas, Cuba, and Hispaniola spread the news across Europe quickly. Columbus rediscovered and explored much of the Lesser Antilles in his second voyage then discovered both Trinidad and Tobago on his third voyage whilst skirting the northern South American coast. His fourth voyage was spent scanning the Central American coast, searching for a strait to the Pacific Ocean. The Voyages of Christopher Columbus opened the New World.\\r\\nItalian navigator and explorer Giovanni Caboto (known in English as John Cabot) is credited with the discovery of continental North America on June 24, 1497, under the commission of Henry VII of England. Though the exact location of his discovery remains disputed, the Canadian and United Kingdom governments' official position is that he landed on the island of Newfoundland. The English presence through Giovanni Caboto was signaled in Juan de la Cosa's map of 1500.\\r\\nIn 1499 Jo?o Fernandes Lavrador was licensed by the King of Manuel I of Portugal and together with Pro de Barcelos they reached Greenland and sighted Labrador for the first time since Leif Erikson, which was granted and named after Lavrador. After returning he possibly went to Bristol to sail in the name of England.[1] Nearly at the same time, between 1499 and 1502 the brothers Gaspar and Miguel Corte Real explored and named the coasts of Greenland, Labrador and also Newfoundland, naming \\"Terra Verde\\" the explored North American coasts.[2] Both explorations were signaled in 1502 Cantino planisphere.\\r\\nIt was soon understood that Columbus had not reached Asia, but rather found what was to Europeans a New World, which in 1507 was named \\"America\\", probably after Amerigo Vespucci, on the Waldseemller map.\\r\\nIn 1500, Pedro lvares Cabral was sent by Portugal to explore South America. He is considered to be the discoverer of Brazil.\\r\\nAragon King Ferdinand II sent Juan Ponce de Le܇n from the fledgling colony on Hispaniola to verify rumors of undiscovered land to the northwest. On April 2, 1513, Ponce de Le܇n disembarked on the northeast coast of what he named Florida for the crown. The exact location is disputed, but historians have offered the possibilities of St. Augustine, Ponce de Le܇n Inlet, and Melbourne Beach. He encountered the powerful Gulf Stream and found a passage through the Florida Keys to land on the southwestern Gulf Coast of Florida on the Gulf of Mexico. Again, the exact location is disputed.[3] He was the first known European to reach the present-day United States.[4]\\r\\nOn September 25, 1513, Spanish conquistador Vasco N~?ez de Balboa was the first European to see the Pacific Ocean once he crossed the Isthmus of Panama. He claimed all the territory touching it for the Crown, later to affect colonization of Las Californias.\\r\\nAround 1519-1521, Portuguese explorer Jo?o lvares Fagundes explored the coasts of Newfoundland, Labrador and Nova Scotia, with a mission to create colonies.\\r\\nIn 1524, Italian explorer Giovanni da Verrazzano sailed for King Francis I of France and is known as the first European since the Norse to explore the Atlantic coast of North America. Arriving near the Cape Fear River delta, he explored the coastlines of present-day states of South Carolina and North Carolina, entering the Pamlico Sound, and bypassing entrances to the Chesapeake Bay. Believing the New York Harbor to be a lake, he sailed past Long Island, exploring Narragansett Bay and Newfoundland.\\r\\nIn 1524-1525, Portuguese explorer Estev?o Gomes, on behalf of Charles I of Spain, explored present-day Nova Scotia sailing South along the Maine coast. Gomes entered New York Harbor and saw the Hudson River(which he named the \\"San Antonio River\\"). Because of his expedition, the 1529 Diogo Ribeiro world map outlines the East coast of North America almost perfectly.\\r\\nIn 1534, Jacques Cartier planted a cross in the Gasp Peninsula, on the Gulf Gulf of Saint Lawrence and claimed the land in the name of Francis I. In 1535 Cartier explored the St. Lawrence river and also claimed the region for France.\\r\\nAfter two failed attempts to reach East Asia by circumnavigating Siberia, Henry Hudson sailed west in 1609 under the Dutch East India Company. He, too, passed Cape Cod, Chesapeake Bay and the Delaware Bay, instead sailing up the Hudson River on September 11, 1609 in search of a fabled connection to the Pacific via what was actually the Great Lakes. In Hudson's fourth and final voyage, he discovered, mapped, and explored the Hudson Strait, Hudson Bay and James Bay.\\r\\nOther major sea-based explorers were Captain James Cook, George Vancouver, and Charles Wilkes.\\r\\nThere were numerous Spanish explorers and conquistadors who explored the Southwest of North America (including present-day west and central United States) and cross the continent (east to west) in its southern regions, mainly from the second quarter to the middle of the 16th century, such as lvar N~?ez Cabeza de Vaca and Francisco Vsquez de Coronado, but also the North American Southeast and south-central regions, s Soto]].\\r\\nIn 1608 Samuel de Champlain founded what is now Quebec City, which would become the first permanent settlement and the capital of New France. He took personal administration over the city and its affairs, and sent out expeditions to explore the interior. Champlain himself discovered Lake Champlain in 1609. By 1615, he had travelled by canoe up the Ottawa River through Lake Nipissing and Georgian Bay to the centre of Huron country near Lake Simcoe. During these voyages, Champlain aided the Wendat (aka 'Hurons') in their battles against the Iroquois Confederacy. As a result, the Iroquois would become enemies of the French and be involved in multiple conflicts.\\r\\nFrom 1679 to 1682 Ren-Robert Cavelier, Sieur de La Salle explored the Great Lakes region of the United States and Canada, and the entire course of Mississippi River to the Gulf of Mexico.\\r\\nFrom 1697 to 1702 Eusebio Kino explored the Sonoran Desert and on his journey to the Colorado River Delta discovered an overland route to Baja California that was then commonly believed to be an island. In 1683 Kino lead the first European overland crossing of Baja California.\\r\\nEuropean exploration of western Canada was largely motivated by the fur trade and the search for the elusive Northwest Passage. Hudson's Bay Company explorer Henry Kelsey has the distinction of being the first European to see the northern Great Plains in 1690.\\r\\nAnthony Henday was the first to have seen the Rocky Mountains, in 1754, but curiously did not mention it in his journals. From his westernmost geographic position (roughly near the town of Olds, Alberta, halfway between Calgary and Red Deer, Alberta) the Rockies should have been quite conspicuous, but he was likely trying to disguise the disappointing fact that an unknown range of seemingly impassible mountains now stood between the HBC and the Pacific. Samuel Hearne found the Coppermine River in 1769-71 in his failed search for copper ore deposits. Burned by these shortfalls, the HBC largely quit exploration.\\r\\nThe North West Company, on the other hand, used a business model that required constant expansion into untapped areas. Under the auspices of the NWC, Alexander Mackenzie discovered the Mackenzie River in 1789 and was the first European to reach the North-American Pacific overland, via the Bella Coola River, in 1793. Simon Fraser reached the Pacific in 1808 via the Fraser River.\\r\\nDavid Thompson, widely regarded as the greatest land geographer that ever lived, traveled over 90,000?km during his lifetime. In 1797, Thompson was sent south by his employers to survey part of the Canada-U.S. boundary along the water routes from Lake Superior to Lake of the Woods to satisfy unresolved questions of territory arising from the Jay Treaty between Great Britain and the United States. By 1798 Thompson had completed a survey of 6,750?km (4,190?mi) from Grand Portage, through Lake Winnipeg, to the headwaters of the Assiniboine and Mississippi Rivers, as well as two sides of Lake Superior.[5] In 1798, the company sent him to Red Deer Lake (in present-day Alberta) to establish a trading post. The English translation of Lac La Biche-Red Deer Lake-first appeared on the Mackenzie map of 1793.[6] Thompson spent the next few seasons trading based in Fort George (now in Alberta), and during this time led several expeditions into the Rocky Mountains. In 1811/1812 he followed the Columbia River to the Pacific, and in 1814 used his notes and measurements to draft the first European-style map of western Canada, covering 3.9 million square kilometres.\\r\\nLewis and Clark were the first Americans to venture into the newly acquired territory of the Louisiana Purchase, at the order of President Thomas Jefferson. They discovered many new geographical features, Indian tribes, and animal and plant species. John Colter was a member of the expedition who subsequently became a guide for others in the 'Old West,' and did some explorations of his own.\\r\\nJohn C. Frmont led many important explorations in the Great Plains, Great Basin, Oregon territory, and Mexican Alta California.\\r\\nJoseph Reddeford Walker was one of the most prominent of the explorers, and charted many new paths through the West, which often were then utilized by emigrants crossing to settle in Western towns and communities. In 1833, his exploring party discovered a route along the Humboldt River across present-day Nevada, ascending the Sierra Nevada following the Carson River and descending via Stanislaus River drainages to Monterey. His return route across the southern Sierra was via Walker Pass, named after Walker by John Charles Fremont. The approach of the Sierra via the Carson River route later became known as the California Trail, the primary route for the emigrants to the gold fields during the California gold rush.\\r\\nAs the American population of the West increased, the US government launched ongoing official explorations mainly through the US Army Corps of Topographical Engineers. One of the main officers and explorers in this unit was George Wheeler. In 1872, the US Congress authorized an ambitious plan to map the portion of the United States west of the 100th meridian at a scale of 8?miles to the inch. This plan necessitated what became known as the Wheeler Survey, along with the Clarence King and John Wesley Powell Surveys, and expeditions by Ferdinand Vandeveer Hayden. In 1879, all such efforts were reorganized as the United States Geological Survey.\\r\\nUniversalis Cosmographia, the Waldseemller map dated 1507, depicts the Americas, Africa, Europe, Asia, and the Pacific Ocean separating Asia from the Americas.\\r\\nThe Vinland map surfaced in 1965 and is claimed, though widely discredited, to be a 15th-century map depicting the Norse colony of Vinland.","input":"Who was the first explorer in north america?"},{"output":"the south-western coast of the country","context":"\\r\\n\\r\\nThe Western Cape (Afrikaans: Wes-Kaap; Xhosa: iNtshona-Koloni) is a province of South Africa, situated on the south-western coast of the country. It is the fourth largest of the nine provinces with an area of 129,449 square kilometres (49,981?sq?mi), and the third most populated, with an estimated 6.6 million inhabitants in 2018.[3] About two-thirds of these inhabitants live in the metropolitan area of Cape Town, which is also the provincial capital. The Western Cape was created in 1994 from part of the former Cape Province.\\r\\n\\r\\nThe Western Cape Province is roughly L-shaped, extending north and east from the Cape of Good Hope, in the southwestern corner of South Africa. It stretches about 400 kilometres (250?mi) northwards along the Atlantic coast and about 500 kilometres (300?mi) eastwards along the South African south coast (Southern Indian Ocean). It is bordered on the north by the Northern Cape and on the east by the Eastern Cape. The total land area of the province is 129,462 square kilometres (49,986?sq?mi),[1]:9 about 10.6% of the country's total. It is roughly the size of England or the State of Louisiana. Its capital city and largest city is Cape Town, and some other major cities include Stellenbosch, Worcester, Paarl, and George. The Garden Route and the Overberg are popular coastal tourism areas.\\r\\n\\r\\nThe Western Cape is the southernmost region of the African continent with Cape Agulhas as its southernmost point, only 3800?km from the Antarctic coastline. The coastline varies from sandy between capes, to rocky to steep and mountainous in places. The only natural harbour is Saldanha Bay on the west coast, about 140?km north of Cape Town. However a lack of fresh water in the region meant that it has only recently been used as a harbour. The province's main harbour was built in Table Bay, which in its natural state was fully exposed to the northwesterly storms that bring rain to the province in winter, as well as the almost uninterrupted dry southeasterly winds in summer. But fresh water coming off Table Mountain and Devil's Peak allowed the early European settlers to build Cape Town on the shores of this less than satisfactory anchorage.\\r\\n\\r\\nThe province is topographically exceptionally diverse. Most of the province falls within the Cape Fold Belt, a set of nearly parallel ranges of sandstone folded mountains of Cambrian-Ordovician age (the age of the rocks is from 510 to about 330?million years ago; their folding into mountains occurred about 350 to about 270?million years ago).[4][5][6] The height of the mountain peaks in the different ranges vary from 1000m to 2300m. The valleys between ranges are generally very fertile as they contain the weathered loamy soils of the Bokkeveld mudstones (see the diagrams on the left).[5]\\r\\n\\r\\nThe far interior forms part of the Karoo. This region of the Province is generally arid and hilly with a prominent escarpment that runs close to the Province's most inland boundary.\\r\\n\\r\\nThe Escarpment marks the southwestern edge of South Africa's central plateau (see the middle and bottom diagrams on the left).[5][7] It runs parallel to the entire South African coastline except in the very far northeast, where it is interrupted by the Limpopo River valley, and the far northwest, where it is interrupted by the Orange River valley. The 1000?km-long northeastern stretch of the escarpment is called the Drakensberg, which is geographically and geologically quite distinct from the Cape Fold Mountains, which originated much earlier and totally independently of the origin of the escarpment.[5][6][8][9]\\r\\n\\r\\nThe principal rivers of the province are the Berg and Olifants which drain into the Atlantic Ocean, and the Breede and Gourits which drain into the Indian Ocean.\\r\\n\\r\\nThe vegetation is also extremely diverse, with one of the world's seven floral kingdoms almost exclusively endemic to the province, namely the Cape Floral Kingdom, most of which is covered by Fynbos (from the Afrikaans meaning \\"Fine Bush\\" (Dutch: Fijnbosch), though precisely how it came to be referred to as such, is uncertain.).[10][11] These evergreen heathlands are extremely rich in species diversity,[10][11] with at least as many plant species occurring on Table Mountain as in  the entire United Kingdom.[11] It is characterised by various types of shrubs, thousands of flowering plant species and some grasses.[10] With the exception of the Silver tree, Leucadendron argenteum, which only grows on the granite and clay soils of the Cape Peninsula,[12] open fynbos is generally treeless except in the wetter mountain ravines where patches of Afromontane forest persist.[10][11]\\r\\n\\r\\nThe arid interior is dominated by Karoo drought-resistant shrubbery. The West Coast and Little Karoo are semi-arid regions and are typified by many species of succulents and drought-resistant shrubs and acacia trees. The Garden Route on the south coast (between the Outeniqua Mountains and the Southern Indian Ocean) is extremely lush, with temperate rainforest (or Afromontane Forest) covering many areas adjacent to the coast, in the deep river valleys and along the southern slopes of the Outeniqua mountain range. Typical species are hardwoods of exceptional height, such as Yellowwood, Stinkwood and Ironwood trees.\\r\\n\\r\\nThe Western Cape is also climatologically diverse, with many distinct micro- and macroclimates created by the varied topography and the influence of the surrounding ocean currents. These are the warm Agulhas Current which flows southwards along South Africa's east coast, and the cold Benguela Current which is an upwelling current from the depths of the South Atlantic Ocean along South Africa's west coast.[13][14] Thus climatic statistics can vary greatly over short distances. Most of the province is considered to have a Mediterranean climate with cool, wet winters and warm, dry summers. Both the Great Karoo and Little Karoo, in the interior, have an arid to semi-arid climate with cold, frosty winters and hot summers with occasional thunderstorms. The Garden Route and the Overberg on the south coast have a maritime climate with cool, moist winters and mild, moist summers. Mossel Bay in the Garden Route is considered to have the second mildest climate worldwide after Hawaii.\\r\\n\\r\\nThunderstorms are generally rare in the province (except in the Karoo) with most precipitation being of a frontal or orographic nature. Extremes of heat and cold are common inland, but rare near the coast. Snow is a common winter occurrence on the Western Cape Mountains occasionally reaching down into the more inland valleys. Otherwise, frost is relatively rare in coastal areas and many of the heavily cultivated valleys.\\r\\n\\r\\nPopulation over 3 million:\\r\\n\\r\\nPopulation 100,000ÿ1,000,000:\\r\\n\\r\\nPopulation 50,000ÿ100,000:\\r\\n\\r\\nPopulation 10,000ÿ50,000:\\r\\n\\r\\nPopulation 5,000ÿ10,000:\\r\\n\\r\\nThe Black Consciousness Movement (BCM) was a grassroots anti-Apartheid activist movement that emerged in South Africa in the mid-1960s out of the political vacuum created by the jailing and banning of the African National Congress and Pan Africanist Congress leadership after the Sharpeville Massacre in 1960. The BCM represented a social movement for political consciousness.\\r\\n\\r\\nIn December 1968, the South African Student Organization (SASO) was formed at a conference held in Marianhill, Natal. The conference was exclusively attended by Black students. After its launch, SASO became the medium through which black consciousness ideology spread to schools and other university campuses across the country.[15]\\r\\n\\r\\nIn 1974, the then South African Minister of Bantu Education and Development, MC Botha, constituted the imposition of using Afrikaans as a medium of instruction in black schools, effective with students in Grade 7 (Standard 5) upwards. As early as March 1976, students began passive resistance against Afrikaans, fueling the outbreak of the Soweto Uprising on 16 June 1976. Consequently, the student protests spread to other parts of the country, and Cape Town became a pivotal site for Western Cape student revolt.\\r\\n\\r\\nStudent leaders at the University of the Western Cape (UWC) and the University of Cape Town (UCT) organised marches. Poster parades by UWC and Black Power Salute marches by UCT was broken by the police, resulting in 73 students getting arrested  and detained at Victor Verster Prison, near Paarl.\\r\\n\\r\\nOn 1 September 1976 the unrest spread to the city of Cape Town itself. Approximately 2000 black students from Western Cape townships, namely Langa, Nyanga and Gugulethu, matched the Cape Town central business district (CBD). Coloured students also contributed to the protests by peacefully marching to the city, but were blockaded by the police in the CBD. The protests turned violent when coloured students started burning schools, libraries and a magistrate's court in support of the student revolt. Thereafter, 200,000 coloured workers partook in a two-day strike staying away from work in the Cape Town area.\\r\\n\\r\\nAccording to a report by the Truth and Reconciliation Commission (TRC), the Western Cape experienced the second highest number of deaths and casualties associated with the 1976 uprising protests.[16]\\r\\n\\r\\nIn 1994, at the introduction of the Interim Constitution and the first non-racial election, South Africa's original provinces and bantustans were abolished and nine new provinces were established. The former Cape Province was divided into the Western Cape, Northern Cape, Eastern Cape and part of North West.\\r\\n\\r\\nIn the 1994 election the Western Cape was one of two provinces that did not elect an African National Congress (ANC) provincial government (the other being KwaZulu-Natal). The National Party (NP) won 53% of the votes and 23 seats in the 42-seat provincial legislature, and Hernus Kriel, a former Minister of Law and Order, was elected Premier. He resigned in 1998 and was replaced by Gerald Morkel.\\r\\n\\r\\nThe 1999 election marked the beginning of a period of great turbulence in Western Cape politics. No party achieved an absolute majority in the provincial parliament, as the ANC won 18 seats while the New National Party (NNP), successor to the NP, won 17. The NNP went into coalition with the Democratic Party (DP), which won 5 seats, to form a government, and Morkel remained Premier. In 2000 the DP and the NNP formalised their coalition by forming the Democratic Alliance (DA).\\r\\n\\r\\nIn 2001, however, the NNP broke with the DA over the removal of Peter Marais from office as Mayor of Cape Town by DA leader Tony Leon. The NNP instead went into coalition with the ANC; Gerald Morkel, who was opposed to the split, resigned as Premier and was replaced by Peter Marais. In 2002 Marais resigned as Premier due to a sexual harassment scandal, and was replaced by NNP leader Marthinus van Schalkwyk. During the 2003 floor-crossing period four members of the provincial parliament crossed to the ANC, giving it an absolute majority of 22 seats in the 42-seat house. However, the ANC remained in coalition with the NNP and van Schalkwyk remained as Premier.\\r\\n\\r\\nIn the 2004 election there was again no absolute winner in the provincial parliament; this time the ANC won 19 seats, the DA won 12, and the NNP won 5. The ANC-NNP coalition continued in power, but van Schalkwyk took up a ministerial post in the national cabinet and was replaced as Premier by the ANC's Ebrahim Rasool. The NNP was finally dissolved after the 2005 floor-crossing period and its members joined the ANC, again giving that party an absolute majority of 24 seats. In the 2007 floor-crossing period the ANC gained a further three members of the provincial parliament. In 2008 Rasool resigned as Premier due to internal party politics, and was replaced by Lynne Brown.\\r\\n\\r\\nThe 2009 election marked a significant change in Western Cape politics, as the Democratic Alliance won 51% of the votes and an absolute majority of 22 seats in the provincial parliament, while the ANC won 14 seats with 31% of the vote. The DA leader Helen Zille was elected Premier. In 2010 the Independent Democrats, which had won 3 seats with 5% of the vote, merged with the DA. In the 2014 election the DA won 59% of the votes and an absolute majority of 26 seats in the provincial parliament, while the ANC won 14 seats with 32% of the vote.\\r\\nIn 2018 King Khoebaha Cornelius III Declared the independence of the Sovereign State of Good Hope [17][18]\\r\\n\\r\\nThe provincial government is established under the Constitution of the Western Cape, which was adopted in 1998. The people of the province elect the 42-member Western Cape Provincial Parliament every five years by a system of party-list proportional representation. The fifth provincial parliament was elected in the election of 7 May 2014; 26 seats are held by the Democratic Alliance, 14 by the African National Congress, and 1 each by the Economic Freedom Fighters and the African Christian Democratic Party. The provincial parliament is responsible for legislating within its responsibilities as set out by the national constitution; these responsibilities include agriculture, education, environment, health services, housing, language policies, tourism, trade, and welfare.\\r\\n\\r\\nThe provincial parliament also elects the Premier of the Western Cape to lead the provincial executive. Since 2009 the Premiership has been held by Helen Zille, former leader of the Democratic Alliance. The Premier appoints ten members of the provincial legislature to serve as a cabinet of ministers, overseeing the departments of the provincial government. These departments are Agriculture, Community Safety, Cultural Affairs and Sport, Economic Development and Tourism, Education, Environmental Affairs and Development Planning, Health, Human Settlements, Local Government, Social Development, Transport and Public Works, and the Provincial Treasury.\\r\\n\\r\\nThe Western Cape Province is divided into one metropolitan municipality and five district municipalities. The district municipalities are in turn divided into 24 local municipalities:\\r\\n\\r\\nThe Western Cape's total GDP for 2008 was R268bn, making the province the joint 2nd largest contributor to the country's total GDP, at 14%. It also has one of the fastest growing economies in the country, growing at 4% in 2008[19] and is expected to grow by 3.2% in 2011.[20] At 20% the province has a substantially lower unemployment rate than the national average standing at 23% in 2009.[21] The province's Gini coefficient of 0.63 is lower than South Africa's Gini coefficient of 0.7 making it more equal than the rest of the country[22] whilst still being extremely high and unequal by international standards. The Western Cape's Human Development Index is the highest in South Africa at 0.7708 compared to the South African average of 0.6675 in 2003.[23]\\r\\n\\r\\nThe biggest sector in the Western Cape's economy is the financial, business services and realestate sectors contributing approximately R77?billion in 2008. Manufacturing was the second largest contributor valued at R43.7?billion in 2008 with the agricultural sector being the fastest growing at 10.6% in the same year.[19]  High-tech industries, international call centres, fashion design, advertising and TV production are niche industries rapidly gaining in importance.[24] The city of Cape Town is ranked as the most entrepreneurial city in South Africa with Early-Stage Entrepreneurial Activity being 190% greater than South Africa's national average.[25]\\r\\n\\r\\nThe Western Cape has an excellent network of highways comparable with any first-world country. The primary highways are the N1 (from Cape Town to Three Sisters, continuing outside the province towards Bloemfontein and Johannesburg), N2 (from Cape Town to Bloukrans River, towards Port Elizabeth), N7 (from Cape Town to Bitterfontein, continuing towards Springbok and Namibia) and N12 (from George to Three Sisters, continuing towards Kimberley and Johannesburg). Other routes are the \\"R\\" roads which connect the smaller towns. All major roads are tarred with major rural gravel roads well maintained. Limited access motorways are limited to the Cape Metropolitan Area, Winelands and Garden Route, however due to the low population density of the remainder of the province, the highways remain efficient and high-speed, except during peak holiday travel seasons, when travel can be slow-going in places due to heavy traffic.[citation needed]\\r\\n\\r\\nTelecommunications in the province are highly sophisticated. Landline telephones are available extensively, and the majority of large urban nodes have access to ADSL and other high-speed internet services.[citation needed] Mobile cellular networks are world-class, with reception extending from cities to highways and many remote rural areas.[citation needed] Mobile networks also play an important role in the internet space due to their speed and widespread availability. Major cities and towns have access to mobile internet speeds in excess of 21 Mbit/s (HSDPA+). In areas where HSDPA+ is not available, networks make provisions for HSDPA, 3G, EDGE or finally GPRS if demand does not warrant higher speed investment.[citation needed]\\r\\n\\r\\nThe 2011 Census recorded the population of the Western Cape as 5,822,734 people living in 1,634,000 households.[1]:8, 63 As the province covers an area of 129,462 square kilometres (49,986?sq?mi),[1]:9 the population density was 45.0 inhabitants per square kilometre (117/sq?mi) and the household density 12.6 per square kilometre (33/sq?mi).\\r\\n\\r\\n49% of the people of the Western Cape described themselves as \\"Coloured\\", while 33% described themselves as \\"Black African\\", 17% as \\"White\\", and 1% as \\"Indian or Asian\\".[1]:21 Afrikaans is the plurality language, spoken as the first language of 50% of the province's population. IsiXhosa is the first language of 25% of the population, while English is the first language of 20%.[1]:25\\r\\n\\r\\nRoughly 16% (894,289 people) of the Western Cape's population in 2011 were born in the Eastern Cape, 3% (167,524) in Gauteng and 1% (61,945) in KwaZulu-Natal. People born outside of South Africa amounted to 4% of the province's population or 260,952 people.[26]\\r\\n\\r\\nThe age distribution of the province was as follows: 25.1% were under the age of 15, 18.3% from 15 to 24, 32.7% from 25 to 44, 18.0% from 45 to 64, and 5.9% who are 65 years of age or older.[1]:28 The median age is 28 years.[27]:20 For every 100 women there are 96 men.[27]:18\\r\\n\\r\\n90% of households in the province have a flush toilet[1]:84 and 90% have refuse removed by the local council at least once a week.[1]:96 75% of households have piped tap water inside the dwelling, while a further 13% have piped water on their property; 11% receive piped water at a community tap, while 1% have no access to piped water.[1]:77 One in seven people live in an informal dwelling.[26]\\r\\n\\r\\n86.9% of households use electricity for cooking,[1]:84 and 93% use it for lighting.[1]:93 89% of households have a cellphone and 31% have a landline telephone, while 86% own a television, 81% own a refrigerator, and 34% own a computer.[1]:99 44% of households have access to the Internet.[1]:101\\r\\n\\r\\nThe average annual household income was R143,460, the second-highest in the country after Gauteng.[27]:37 As of  September?2012[update], 69% of the population aged 15ÿ64 are economically active, and of these 25% are unemployed. Overall, 52% of the working-age population are employed.[28] Around 2 million people in the Western Cape labour market (those aged 16 to 64) are employed, 1.3 million are not economically active, 552,733 are unemployed with an additional 122,753 who are discouraged work seekers who want to work but have given up looking for it.[26]\\r\\n\\r\\nAccording to research conducted by Plus94, the Western Cape is the least racist province in South Africa.[29]\\r\\n\\r\\n2.7% of residents aged 20 and over have received no schooling, 10.7% have had only some primary, 5.6% have completed primary school but gone no further, 38% have had some secondary education without finishing Grade 12, 28% have finished Grade 12 but gone no further, and 14% have higher education beyond the secondary level. Overall, 43% of residents have completed high school.[1]:49\\r\\n\\r\\nThe Western Cape province has the most highly educated residents with a very skilled workforce in comparison to any other African region.[30] The high school graduation rate is consistently around 80%, higher than any other province. The proportion of adults with a degree or higher was 4.8% (2005),[24] the highest in the country.\\r\\n\\r\\nThe province also boasts four universities:\\r\\n\\r\\nThe province is also home to the South African Military Academy.\\r\\n\\r\\nThe University of Cape Town\\r\\n\\r\\nOver 50% of all cheese in South Africa is produced in the Western Cape.[31]\\r\\n\\r\\nThe Western Cape is famous for wine production and some of the most beautiful vineyards in the world. The winelands are divided into six main regions: Boberg, Breede River Valley, Cape South Coast, Coastal Region, Klein Karoo and Olifants River. Each has unique climate, topography and fertile soils which produce wines of excellent quality.\\r\\n\\r\\nCoordinates: 34S 20E? / ?34S 20E? / -34; 20","input":"Where is the western cape in south africa?"},{"output":"c. 5500 and 4000 BC","context":"Sumer (/?su?m?r/)[note 1] is the earliest known civilization in the historical region of southern Mesopotamia, modern-day southern Iraq, during the Chalcolithic and Early Bronze ages, and arguably the first civilization in the world with Ancient Egypt and the Indus Valley.[1] Living along the valleys of the Tigris and Euphrates, Sumerian farmers were able to grow an abundance of grain and other crops, the surplus of which enabled them to settle in one place. Proto-writing in the prehistory dates back to c. 3000 BC. The earliest texts come from the cities of Uruk and Jemdet Nasr and date back to 3300 BC; early cuneiform script writing emerged in 3000 BC.[2]\\r\\nModern historians have suggested that Sumer was first permanently settled between c. 5500 and 4000 BC by a West Asian people who spoke the Sumerian language (pointing to the names of cities, rivers, basic occupations, etc., as evidence), an agglutinative language isolate.[3][4][5][6] These conjectured, prehistoric people are now called \\"proto-Euphrateans\\" or \\"Ubaidians\\",[7] and are theorized to have evolved from the Samarra culture of northern Mesopotamia.[8][9][10][11] The Ubaidians (though never mentioned by the Sumerians themselves) are assumed by modern-day scholars to have been the first civilizing force in Sumer, draining the marshes for agriculture, developing trade, and establishing industries, including weaving, leatherwork, metalwork, masonry, and pottery.[7]\\r\\nSome scholars contest the idea of a Proto-Euphratean language or one substrate language. It has been suggested by them, and others, that the Sumerian language was originally that of the hunter and fisher peoples, who lived in the marshland and the Eastern Arabia littoral region, and were part of the Arabian bifacial culture.[12] Reliable historical records begin much later; there are none in Sumer of any kind that have been dated before Enmebaragesi (c. 26th century BC). Juris Zarins believes the Sumerians lived along the coast of Eastern Arabia, today's Persian Gulf region, before it flooded at the end of the Ice Age.[13]\\r\\nSumerian civilization took form in the Uruk period (4th millennium BC), continuing into the Jemdet Nasr and Early Dynastic periods. During the 3rd millennium BC, a close cultural symbiosis developed between the Sumerians, who spoke a language isolate, and Akkadian-speakers, which included widespread bilingualism.[14] The influence of Sumerian on Akkadian (and vice versa) is evident in all areas, from lexical borrowing on a massive scale, to syntactic, morphological, and phonological convergence.[14] This has prompted scholars to refer to Sumerian and Akkadian in the 3rd millennium BC as a Sprachbund.[14] Sumer was conquered by the Semitic-speaking kings of the Akkadian Empire around 2270 BC (short chronology), but Sumerian continued as a sacred language.\\r\\nNative Sumerian rule re-emerged for about a century in the Third Dynasty of Ur at approximately 2100ÿ2000 BC, but the Akkadian language also remained in use. The Sumerian city of Eridu, on the coast of the Persian Gulf, is considered to have been the world's first city, where three separate cultures may have fused: that of peasant Ubaidian farmers, living in mud-brick huts and practicing irrigation; that of mobile nomadic Semitic pastoralists living in black tents and following herds of sheep and goats; and that of fisher folk, living in reed huts in the marshlands, who may have been the ancestors of the Sumerians.[15]\\r\\n\\r\\n\\r\\nThe term Sumerian is the common name given to the ancient non-Semitic-speaking inhabitants of Mesopotamia, Sumer, by the East Semitic-speaking Akkadians. The Sumerians referred to themselves as ? sa? gg ga (cuneiform: ?? ?? ?? ??), phonetically /u? sa? gi ga/, literally meaning \\"the black-headed people\\", and to their land as ki-en-gi(-r) (cuneiform: ??????) ('place' + 'lords' + 'noble'), meaning \\"place of the noble lords\\".[16] The Akkadian word Shumer may represent the geographical name in dialect, but the phonological development leading to the Akkadian term ?umer? is uncertain.[17] Hebrew Shinar, Egyptian Sngr, and Hittite ?anhar(a), all referring to southern Mesopotamia, could be western variants of Shumer.[17]\\r\\nIn the late 4th millennium BC, Sumer was divided into many independent city-states, which were divided by canals and boundary stones. Each was centered on a temple dedicated to the particular patron god or goddess of the city and ruled over by a priestly governor (ensi) or by a king (lugal) who was intimately tied to the city's religious rites.\\r\\nThe five \\"first\\" cities, said to have exercised pre-dynastic kingship \\"before the flood\\":\\r\\nOther principal cities:\\r\\n(1location uncertain)\\r\\n(2an outlying city in northern Mesopotamia)\\r\\nMinor cities (from south to north):\\r\\n(2an outlying city in northern Mesopotamia)\\r\\nApart from Mari, which lies full 330 kilometres (205 miles) north-west of Agade, but which is credited in the king list as having exercised kingship in the Early Dynastic II period, and Nagar, an outpost, these cities are all in the Euphrates-Tigris alluvial plain, south of Baghdad in what are now the Bbil, Diyala, Wsit, Dhi Qar, Basra, Al-Muthann and Al-Qdisiyyah governorates of Iraq.\\r\\nThe Sumerian city-states rose to power during the prehistoric Ubaid and Uruk periods. Sumerian written history reaches back to the 27th century BC and before, but the historical record remains obscure until the Early Dynastic III period, c. the 23rd century BC, when a now deciphered syllabary writing system was developed, which has allowed archaeologists to read contemporary records and inscriptions. Classical Sumer ends with the rise of the Akkadian Empire in the 23rd century BC. Following the Gutian period, there was a brief Sumerian Renaissance in the 21st century BC, cut short in the 20th century BC by invasions by the Amorites. The Amorite \\"dynasty of Isin\\" persisted until c. 1700 BC, when Mesopotamia was united under Babylonian rule. The Sumerians were eventually absorbed into the Akkadian (Assyro-Babylonian) population.[citation needed]\\r\\nThe Ubaid period is marked by a distinctive style of fine quality painted pottery which spread throughout Mesopotamia and the Persian Gulf. During this time, the first settlement in southern Mesopotamia was established at Eridu (Cuneiform: nun.ki ????), c. 6500 BC, by farmers who brought with them the Hadji Muhammed culture, which first pioneered irrigation agriculture. It appears that this culture was derived from the Samarran culture from northern Mesopotamia. It is not known whether or not these were the actual Sumerians who are identified with the later Uruk culture. The rise of the city of Uruk may be reflected in the story of the passing of the gifts of civilization (me) to Inanna, goddess of Uruk and of love and war, by Enki, god of wisdom and chief god of Eridu, may reflect the transition from Eridu to Uruk.[19]:174\\r\\nThe archaeological transition from the Ubaid period to the Uruk period is marked by a gradual shift from painted pottery domestically produced on a slow wheel to a great variety of unpainted pottery mass-produced by specialists on fast wheels. The Uruk period is a continuation and an outgrowth of Ubaid with pottery being the main visible change.[20][21]\\r\\nBy the time of the Uruk period (c. 4100ÿ2900 BC calibrated), the volume of trade goods transported along the canals and rivers of southern Mesopotamia facilitated the rise of many large, stratified, temple-centered cities (with populations of over 10,000 people) where centralized administrations employed specialized workers. It is fairly certain that it was during the Uruk period that Sumerian cities began to make use of slave labor captured from the hill country, and there is ample evidence for captured slaves as workers in the earliest texts. Artifacts, and even colonies of this Uruk civilization have been found over a wide areafrom the Taurus Mountains in Turkey, to the Mediterranean Sea in the west, and as far east as central Iran.[22]\\r\\nThe Uruk period civilization, exported by Sumerian traders and colonists (like that found at Tell Brak), had an effect on all surrounding peoples, who gradually evolved their own comparable, competing economies and cultures. The cities of Sumer could not maintain remote, long-distance colonies by military force.[22]\\r\\nSumerian cities during the Uruk period were probably theocratic and were most likely headed by a priest-king (ensi), assisted by a council of elders, including both men and women.[23] It is quite possible that the later Sumerian pantheon was modeled upon this political structure. There was little evidence of organized warfare or professional soldiers during the Uruk period, and towns were generally unwalled. During this period Uruk became the most urbanized city in the world, surpassing for the first time 50,000 inhabitants.\\r\\nThe ancient Sumerian king list includes the early dynasties of several prominent cities from this period. The first set of names on the list is of kings said to have reigned before a major flood occurred. These early names may be fictional, and include some legendary and mythological figures, such as Alulim and Dumizid.[23]\\r\\nThe end of the Uruk period coincided with the Piora oscillation, a dry period from c. 3200 ÿ 2900 BC that marked the end of a long wetter, warmer climate period from about 9,000 to 5,000 years ago, called the Holocene climatic optimum.[24]\\r\\nThe dynastic period begins c. 2900 BC and was associated with a shift from the temple establishment headed by council of elders led by a priestly \\"En\\" (a male figure when it was a temple for a goddess, or a female figure when headed by a male god)[25] towards a more secular Lugal (Lu = man, Gal = great) and includes such legendary patriarchal figures as Enmerkar, Lugalbanda and Gilgameshwho are supposed to have reigned shortly before the historic record opens c. 2700 BC, when the now deciphered syllabic writing started to develop from the early pictograms. The center of Sumerian culture remained in southern Mesopotamia, even though rulers soon began expanding into neighboring areas, and neighboring Semitic groups adopted much of Sumerian culture for their own.\\r\\nThe earliest dynastic king on the Sumerian king list whose name is known from any other legendary source is Etana, 13th king of the first dynasty of Kish. The earliest king authenticated through archaeological evidence is Enmebaragesi of Kish (c. 26th century BC), whose name is also mentioned in the Gilgamesh epicleading to the suggestion that Gilgamesh himself might have been a historical king of Uruk. As the Epic of Gilgamesh shows, this period was associated with increased war. Cities became walled, and increased in size as undefended villages in southern Mesopotamia disappeared. (Both Enmerkar and Gilgamesh are credited with having built the walls of Uruk[26]).\\r\\nc. 2500ÿ2270 BC\\r\\nThe dynasty of Lagash, though omitted from the king list, is well attested through several important monuments and many archaeological finds.\\r\\nAlthough short-lived, one of the first empires known to history was that of Eannatum of Lagash, who annexed practically all of Sumer, including Kish, Uruk, Ur, and Larsa, and reduced to tribute the city-state of Umma, arch-rival of Lagash. In addition, his realm extended to parts of Elam and along the Persian Gulf. He seems to have used terror as a matter of policy.[27] Eannatum's Stele of the Vultures depicts vultures pecking at the severed heads and other body parts of his enemies. His empire collapsed shortly after his death.\\r\\nLater, Lugal-Zage-Si, the priest-king of Umma, overthrew the primacy of the Lagash dynasty in the area, then conquered Uruk, making it his capital, and claimed an empire extending from the Persian Gulf to the Mediterranean. He was the last ethnically Sumerian king before Sargon of Akkad.[15]\\r\\nc. 2270ÿ2083 BC (short chronology)\\r\\nThe Eastern Semitic Akkadian language is first attested in proper names of the kings of Kish c. 2800 BC,[27] preserved in later king lists. There are texts written entirely in Old Akkadian dating from c. 2500 BC. Use of Old Akkadian was at its peak during the rule of Sargon the Great (c. 2270ÿ2215 BC), but even then most administrative tablets continued to be written in Sumerian, the language used by the scribes. Gelb and Westenholz differentiate three stages of Old Akkadian: that of the pre-Sargonic era, that of the Akkadian empire, and that of the \\"Neo-Sumerian Renaissance\\" that followed it. Akkadian and Sumerian coexisted as vernacular languages for about one thousand years, but by around 1800 BC, Sumerian was becoming more of a literary language familiar mainly only to scholars and scribes. Thorkild Jacobsen has argued that there is little break in historical continuity between the pre- and post-Sargon periods, and that too much emphasis has been placed on the perception of a \\"Semitic vs. Sumerian\\" conflict.[28] However, it is certain that Akkadian was also briefly imposed on neighboring parts of Elam that were previously conquered, by Sargon.\\r\\nc. 2083ÿ2050 BC (short chronology)\\r\\nc. 2093ÿ2046 BC (short chronology)\\r\\nFollowing the downfall of the Akkadian Empire at the hands of Gutians, another native Sumerian ruler, Gudea of Lagash, rose to local prominence and continued the practices of the Sargonid kings' claims to divinity. The previous Lagash dynasty, Gudea and his descendants also promoted artistic development and left a large number of archaeological artifacts.\\r\\nc. 2047ÿ1940 BC (short chronology)\\r\\nLater, the 3rd dynasty of Ur under Ur-Nammu and Shulgi, whose power extended as far as southern Assyria, was the last great \\"Sumerian renaissance\\", but already the region was becoming more Semitic than Sumerian, with the resurgence of the Akkadian speaking Semites in Assyria and elsewhere, and the influx of waves of Semitic Martu (Amorites) who were to found several competing local powers in the south, including Isin, Larsa, Eshnunna and some time later Babylonia. The last of these eventually came to briefly dominate the south of Mesopotamia as the Babylonian Empire, just as the Old Assyrian Empire had already done so in the north from the late 21st century BC. The Sumerian language continued as a sacerdotal language taught in schools in Babylonia and Assyria, much as Latin was used in the Medieval period, for as long as cuneiform was utilized.\\r\\nThis period is generally taken to coincide with a major shift in population from southern Mesopotamia toward the north. Ecologically, the agricultural productivity of the Sumerian lands was being compromised as a result of rising salinity. Soil salinity in this region had been long recognized as a major problem.[citation needed]Poorly drained irrigated soils, in an arid climate with high levels of evaporation, led to the buildup of dissolved salts in the soil, eventually reducing agricultural yields severely. During the Akkadian and Ur III phases, there was a shift from the cultivation of wheat to the more salt-tolerant barley, but this was insufficient, and during the period from 2100 BC to 1700 BC, it is estimated that the population in this area declined by nearly three fifths.[29] This greatly upset the balance of power within the region, weakening the areas where Sumerian was spoken, and comparatively strengthening those where Akkadian was the major language. Henceforth, Sumerian would remain only a literary and liturgical language, similar to the position occupied by Latin in medieval Europe.\\r\\nFollowing an Elamite invasion and sack of Ur during the rule of Ibbi-Sin (c. 1940 BC)[citation needed], Sumer came under Amorites rule (taken to introduce the Middle Bronze Age). The independent Amorite states of the 20th to 18th centuries are summarized as the \\"Dynasty of Isin\\" in the Sumerian king list, ending with the rise of Babylonia under Hammurabi c. 1700 BC.\\r\\nLater rulers who dominated Assyria and Babylonia occasionally assumed the old Sargonic title \\"King of Sumer and Akkad\\", such as Tukulti-Ninurta I of Assyria after c. 1225 BC.\\r\\nUruk, one of Sumer's largest cities, has been estimated to have had a population of 50,000-80,000 at its height;[30] given the other cities in Sumer, and the large agricultural population, a rough estimate for Sumer's population might be 0.8 million to 1.5 million. The world population at this time has been estimated at about 27 million.[31]\\r\\nThe Sumerians spoke a language isolate, but a number of linguists have claimed to be able to detect a substrate language of unknown classification beneath Sumerian because names of some of Sumer's major cities are not Sumerian, revealing influences of earlier inhabitants.[32] However, the archaeological record shows clear uninterrupted cultural continuity from the time of the early Ubaid period (5300 ÿ 4700 BC C-14) settlements in southern Mesopotamia. The Sumerian people who settled here farmed the lands in this region that were made fertile by silt deposited by the Tigris and the Euphrates.\\r\\nSome archaeologists have speculated that the original speakers of ancient Sumerian may have been farmers, who moved down from the north of Mesopotamia after perfecting irrigation agriculture there. The Ubaid period pottery of southern Mesopotamia has been connected via Choga Mami transitional ware to the pottery of the Samarra period culture (c. 5700 ÿ 4900 BC C-14) in the north, who were the first to practice a primitive form of irrigation agriculture along the middle Tigris River and its tributaries. The connection is most clearly seen at Tell Awayli (Oueilli, Oueili) near Larsa, excavated by the French in the 1980s, where eight levels yielded pre-Ubaid pottery resembling Samarran ware. According to this theory, farming peoples spread down into southern Mesopotamia because they had developed a temple-centered social organization for mobilizing labor and technology for water control, enabling them to survive and prosper in a difficult environment.[citation needed]\\r\\nOthers have suggested a continuity of Sumerians, from the indigenous hunter-fisherfolk traditions, associated with the bifacial assemblages found on the Arabian littoral. Juris Zarins believes the Sumerians may have been the people living in the Persian Gulf region before it flooded at the end of the last Ice Age.[33]\\r\\nSumerian myths suggest a prohibition against premarital sex.[34] Marriages were often arranged by the parents of the bride and groom; engagements were usually completed through the approval of contracts recorded on clay tablets. These marriages became legal as soon as the groom delivered a bridal gift to his bride's father. Nonetheless, evidence suggests that premarital sex was a common, but surreptitious, occurrence.[35]:78\\r\\nIn the early Sumerian period, the primitive pictograms suggest[36] that\\r\\nThere is considerable evidence concerning Sumerian music. Lyres and flutes were played, among the best-known examples being the Lyres of Ur.[37]\\r\\nInscriptions describing the reforms of king Urukagina of Lagash (c. 2300 BC) say that he abolished the former custom of polyandry in his country, prescribing that a woman who took multiple husbands be stoned with rocks upon which her crime had been written.[38]\\r\\nSumerian culture was male-dominated and stratified. The Code of Ur-Nammu, the oldest such codification yet discovered, dating to the Ur III, reveals a glimpse at societal structure in late Sumerian law. Beneath the lu-gal (\\"great man\\" or king), all members of society belonged to one of two basic strata: The \\"lu\\" or free person, and the slave (male, arad; female geme). The son of a lu was called a dumu-nita until he married. A woman (munus) went from being a daughter (dumu-mi), to a wife (dam), then if she outlived her husband, a widow (numasu) and she could then remarry another man who was from the same tribe.\\r\\nProstitution existed but it is not clear if sacred prostitution did.[39]:151\\r\\nThe most important archaeological discoveries in Sumer are a large number of clay tablets written in cuneiform script. Sumerian writing, while proven to not be the oldest example of writing on earth, is considered to be a great milestone in the development of humanity's ability to not only create historical records but also in creating pieces of literature, both in the form of poetic epics and stories as well as prayers and laws. Although pictures  that is, hieroglyphs  were used first, cuneiform and then ideograms (where symbols were made to represent ideas) soon followed. Triangular or wedge-shaped reeds were used to write on moist clay. A large body of hundreds of thousands of texts in the Sumerian language have survived, such as personal and business letters, receipts, lexical lists, laws, hymns, prayers, stories, and daily records. Full libraries of clay tablets have been found. Monumental inscriptions and texts on different objects, like statues or bricks, are also very common. Many texts survive in multiple copies because they were repeatedly transcribed by scribes in training. Sumerian continued to be the language of religion and law in Mesopotamia long after Semitic speakers had become dominant.\\r\\nA prime example of cuneiform writing would be a lengthy poem that was discovered in the ruins of Uruk. The Epic of Gilgamesh was written in the standard Sumerian cuneiform. It tells of a king from the early Dynastic II period named Gilgamesh or \\"Bilgamesh\\" in Sumerian. The story is based around the fictional adventures of Gilgamesh and his companion, Enkidu. It was laid out on several clay tablets and is claimed to be the earliest example of a fictional, written piece of literature discovered so far.\\r\\nThe Sumerian language is generally regarded as a language isolate in linguistics because it belongs to no known language family; Akkadian, by contrast, belongs to the Semitic branch of the Afroasiatic languages. There have been many failed attempts to connect Sumerian to other language families. It is an agglutinative language; in other words, morphemes (\\"units of meaning\\") are added together to create words, unlike analytic languages where morphemes are purely added together to create sentences. Some authors have proposed that there may be evidence of a substratum or adstratum language for geographic features and various crafts and agricultural activities, called variously Proto-Euphratean or Proto Tigrean, but this is disputed by others.\\r\\nUnderstanding Sumerian texts today can be problematic even for experts.[citation needed] Most difficult are the earliest texts, which in many cases do not give the full grammatical structure of the language and seem to have been used as an \\"aide-mmoire\\" for knowledgeable scribes.\\r\\nDuring the 3rd millennium BC a cultural symbiosis developed between the Sumerians and the Akkadians, which included widespread bilingualism.[14] The influences between Sumerian on Akkadian are evident in all areas including lexical borrowing on a massive scaleand syntactic, morphological, and phonological convergence.[14] This mutual influence has prompted scholars to refer to Sumerian and Akkadian of the 3rd millennium BC as a Sprachbund.[14]\\r\\nAkkadian gradually replaced Sumerian as a spoken language somewhere around the turn of the 3rd and the 2nd millennium BC,[40] but Sumerian continued to be used as a sacred, ceremonial, literary, and scientific language in Babylonia and Assyria until the 1st century AD.[41]\\r\\nThe Sumerians credited their divinities for all matters pertaining to them and exhibited humility in the face of cosmic forces, such as death and divine wrath.[35]:3ÿ4\\r\\nSumerian religion seems to have been founded upon two separate cosmogenic myths. The first saw creation as the result of a series of hieroi gamoi or sacred marriages, involving the reconciliation of opposites, postulated as a coming together of male and female divine beings; the gods. This continued to influence the whole Mesopotamian mythos. Thus, in the later Akkadian Enuma Elish, the creation was seen as the union of fresh and salt water; as male Abzu, and female Tiamat. The products of that union, Lahm and Lahmu, \\"the muddy ones\\", were titles given to the gate keepers of the E-Abzu temple of Enki, in Eridu, the first Sumerian city. Describing the way that muddy islands emerge from the confluence of fresh and salty water at the mouth of the Euphrates, where the river deposited its load of silt, a second hieros gamos supposedly created Anshar and Kishar, the \\"sky-pivot\\" or axle, and the \\"earth pivot\\", parents in turn of Anu (the sky) and Ki (the earth). Another important Sumerian hieros gamos was that between Ki, here known as Ninhursag or \\"Lady of the Mountains\\", and Enki of Eridu, the god of fresh water which brought forth greenery and pasture.\\r\\nAt an early stage, following the dawn of recorded history, Nippur, in central Mesopotamia, replaced Eridu in the south as the primary temple city, whose priests exercised political hegemony on the other city-states. Nippur retained this status throughout the Sumerian period.\\r\\nSumerians believed in an anthropomorphic polytheism, or the belief in many gods in human form. There was no common set of gods; each city-state had its own patrons, temples, and priest-kings. Nonetheless, these were not exclusive; the gods of one city were often acknowledged elsewhere. Sumerian speakers were among the earliest people to record their beliefs in writing, and were a major inspiration in later Mesopotamian mythology, religion, and astrology.\\r\\nThe Sumerians worshiped:\\r\\nThese deities formed a core pantheon; there were additionally hundreds of minor ones. Sumerian gods could thus have associations with different cities, and their religious importance often waxed and waned with those cities' political power. The gods were said to have created human beings from clay for the purpose of serving them. The temples organized the mass labour projects needed for irrigation agriculture. Citizens had a labor duty to the temple, though they could avoid it by a payment of silver.\\r\\nSumerians believed that the universe consisted of a flat disk enclosed by a dome. The Sumerian afterlife involved a descent into a gloomy netherworld to spend eternity in a wretched existence as a Gidim (ghost).[46]\\r\\nThe universe was divided into four quarters:\\r\\nTheir known world extended from The Upper Sea or Mediterranean coastline, to The Lower Sea, the Persian Gulf and the land of Meluhha (probably the Indus Valley) and Magan (Oman), famed for its copper ores.\\r\\nZiggurats (Sumerian temples) each had an individual name and consisted of a forecourt, with a central pond for purification.[47] The temple itself had a central nave with aisles along either side. Flanking the aisles would be rooms for the priests. At one end would stand the podium and a mudbrick table for animal and vegetable sacrifices. Granaries and storehouses were usually located near the temples. After a time the Sumerians began to place the temples on top of multi-layered square constructions built as a series of rising terraces, giving rise to the Ziggurat style.[48]\\r\\nIt was believed that when people died, they would be confined to a gloomy world of Ereshkigal, whose realm was guarded by gateways with various monsters designed to prevent people entering or leaving. The dead were buried outside the city walls in graveyards where a small mound covered the corpse, along with offerings to monsters and a small amount of food. Those who could afford it sought burial at Dilmun.[49] Human sacrifice was found in the death pits at the Ur royal cemetery where Queen Puabi was accompanied in death by her servants.\\r\\nThe Sumerians adopted an agricultural lifestyle perhaps as early as c. 5000 BC ÿ 4500 BC. The region demonstrated a number of core agricultural techniques, including organized irrigation, large-scale intensive cultivation of land, mono-cropping involving the use of plough agriculture, and the use of an agricultural specialized labour force under bureaucratic control. The necessity to manage temple accounts with this organization led to the development of writing (c. 3500 BC).\\r\\nIn the early Sumerian Uruk period, the primitive pictograms suggest that sheep, goats, cattle, and pigs were domesticated. They used oxen as their primary beasts of burden and donkeys or equids as their primary transport animal and \\"woollen clothing as well as rugs were made from the wool or hair of the animals. ... By the side of the house was an enclosed garden planted with trees and other plants; wheat and probably other cereals were sown in the fields, and the shaduf was already employed for the purpose of irrigation. Plants were also grown in pots or vases.\\"[36]\\r\\nThe Sumerians were one of the first known beer drinking societies. Cereals were plentiful and were the key ingredient in their early brew. They brewed multiple kinds of beer consisting of wheat, barley, and mixed grain beers. Beer brewing was very important to the Sumerians. It was referenced in the Epic of Gilgamesh when Enkidu was introduced to the food and beer of Gilgamesh's people: \\"Drink the beer, as is the custom of the land... He drank the beer-seven jugs! and became expansive and sang with joy!\\"[50]\\r\\nThe Sumerians practiced similar irrigation techniques as those used in Egypt.[51] American anthropologist Robert McCormick Adams says that irrigation development was associated with urbanization,[52] and that 89% of the population lived in the cities.\\r\\nThey grew barley, chickpeas, lentils, wheat, dates, onions, garlic, lettuce, leeks and mustard. Sumerians caught many fish and hunted fowl and gazelle.[53]\\r\\nSumerian agriculture depended heavily on irrigation. The irrigation was accomplished by the use of shaduf, canals, channels, dykes, weirs, and reservoirs. The frequent violent floods of the Tigris, and less so, of the Euphrates, meant that canals required frequent repair and continual removal of silt, and survey markers and boundary stones needed to be continually replaced. The government required individuals to work on the canals in a corvee, although the rich were able to exempt themselves.\\r\\nAs is known from the \\"Sumerian Farmer's Almanac\\", after the flood season and after the Spring Equinox and the Akitu or New Year Festival, using the canals, farmers would flood their fields and then drain the water. Next they made oxen stomp the ground and kill weeds. They then dragged the fields with pickaxes. After drying, they plowed, harrowed, and raked the ground three times, and pulverized it with a mattock, before planting seed. Unfortunately, the high evaporation rate resulted in a gradual increase in the salinity of the fields. By the Ur III period, farmers had switched from wheat to the more salt-tolerant barley as their principal crop.\\r\\nSumerians harvested during the spring in three-person teams consisting of a reaper, a binder, and a sheaf handler.[54] The farmers would use threshing wagons, driven by oxen, to separate the cereal heads from the stalks and then use threshing sleds to disengage the grain. They then winnowed the grain/chaff mixture.\\r\\nThe Tigris-Euphrates plain lacked minerals and trees. Sumerian structures were made of plano-convex mudbrick, not fixed with mortar or cement. Mud-brick buildings eventually deteriorate, so they were periodically destroyed, leveled, and rebuilt on the same spot. This constant rebuilding gradually raised the level of cities, which thus came to be elevated above the surrounding plain. The resultant hills, known as tells, are found throughout the ancient Near East.\\r\\nAccording to Archibald Sayce, the primitive pictograms of the early Sumerian (i.e. Uruk) era suggest that \\"Stone was scarce, but was already cut into blocks and seals. Brick was the ordinary building material, and with it cities, forts, temples and houses were constructed. The city was provided with towers and stood on an artificial platform; the house also had a tower-like appearance. It was provided with a door which turned on a hinge, and could be opened with a sort of key; the city gate was on a larger scale, and seems to have been double. The foundation stones  or rather bricks  of a house were consecrated by certain objects that were deposited under them.\\"[36]\\r\\nThe most impressive and famous of Sumerian buildings are the ziggurats, large layered platforms that supported temples. Sumerian cylinder seals also depict houses built from reeds not unlike those built by the Marsh Arabs of Southern Iraq until as recently as 400 CE. The Sumerians also developed the arch, which enabled them to develop a strong type of dome. They built this by constructing and linking several arches. Sumerian temples and palaces made use of more advanced materials and techniques,[citation needed] such as buttresses, recesses, half columns, and clay nails.\\r\\nThe Sumerians developed a complex system of metrology c. 4000 BC. This advanced metrology resulted in the creation of arithmetic, geometry, and algebra. From c. 2600 BC onwards, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period.[55] The period c. 2700 ÿ 2300 BC saw the first appearance of the abacus, and a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system.[56] The Sumerians were the first to use a place value numeral system. There is also anecdotal evidence the Sumerians may have used a type of slide rule in astronomical calculations. They were the first to find the area of a triangle and the volume of a cube.[57]\\r\\nDiscoveries of obsidian from far-away locations in Anatolia and lapis lazuli from Badakhshan in northeastern Afghanistan, beads from Dilmun (modern Bahrain), and several seals inscribed with the Indus Valley script suggest a remarkably wide-ranging network of ancient trade centered on the Persian Gulf. For example, Imports to Ur came from many parts of the world. In particular, the metals of all types had to be imported.\\r\\nThe Epic of Gilgamesh refers to trade with far lands for goods, such as wood, that were scarce in Mesopotamia. In particular, cedar from Lebanon was prized. The finding of resin in the tomb of Queen Puabi at Ur, indicates it was traded from as far away as Mozambique.\\r\\nThe Sumerians used slaves, although they were not a major part of the economy. Slave women worked as weavers, pressers, millers, and porters.\\r\\nSumerian potters decorated pots with cedar oil paints. The potters used a bow drill to produce the fire needed for baking the pottery. Sumerian masons and jewelers knew and made use of alabaster (calcite), ivory, iron, gold, silver, carnelian, and lapis lazuli.[58]\\r\\nLarge institutions kept their accounts in barley and silver, often with a fixed rate between them. The obligations, loans and prices in general were usually denominated in one of them. Many transactions involved debt, for example goods consigned to merchants by temple and beer advanced by \\"ale women\\".[59]\\r\\nCommercial credit and agricultural consumer loans were the main types of loans. The trade credit was usually extended by temples in order to finance trade expeditions and was nominated in silver. The interest rate was set at 1/60 a month (one shekel per mina) some time before 2000 BC and it remained at that level for about two thousand years.[59] Rural loans commonly arose as a result of unpaid obligations due to an institution (such as a temple), in this case the arrears were considered to be lent to the debtor.[60] They were denominated in barley or other crops and the interest rate was typically much higher than for commercial loans and could amount to 1/3 to 1/2 of the loan principal.[59]\\r\\nPeriodically, rulers signed \\"clean slate\\" decrees that cancelled all the rural (but not commercial) debt and allowed bondservants to return to their homes. Customarily, rulers did it at the beginning of the first full year of their reign, but they could also be proclaimed at times of military conflict or crop failure. The first known ones were made by Enmetena and Urukagina of Lagash in 2400-2350 BC. According to Hudson, the purpose of these decrees was to prevent debts mounting to a degree that they threatened the fighting force, which could happen if peasants lost the subsistence land or became bondservants due to the inability to repay the debt.[59]\\r\\nThe almost constant wars among the Sumerian city-states for 2000 years helped to develop the military technology and techniques of Sumer to a high level.[61] The first war recorded in any detail was between Lagash and Umma in c. 2525 BC on a stele called the Stele of the Vultures. It shows the king of Lagash leading a Sumerian army consisting mostly of infantry. The infantry carried spears, wore copper helmets, and carried rectangular shields. The spearmen are shown arranged in what resembles the phalanx formation, which requires training and discipline; this implies that the Sumerians may have made use of professional soldiers.[62]\\r\\nThe Sumerian military used carts harnessed to onagers. These early chariots functioned less effectively in combat than did later designs, and some have suggested that these chariots served primarily as transports, though the crew carried battle-axes and lances. The Sumerian chariot comprised a four or two-wheeled device manned by a crew of two and harnessed to four onagers. The cart was composed of a woven basket and the wheels had a solid three-piece design.\\r\\nSumerian cities were surrounded by defensive walls. The Sumerians engaged in siege warfare between their cities, but the mudbrick walls were able to deter some foes.\\r\\nExamples of Sumerian technology include: the wheel, cuneiform script, arithmetic and geometry, irrigation systems, Sumerian boats, lunisolar calendar, bronze, leather, saws, chisels, hammers, braces, bits, nails, pins, rings, hoes, axes, knives, lancepoints, arrowheads, swords, glue, daggers, waterskins, bags, harnesses, armor, quivers, war chariots, scabbards, boots, sandals, harpoons and beer. The Sumerians had three main types of boats:\\r\\nEvidence of wheeled vehicles appeared in the mid 4th millennium BC, near-simultaneously in Mesopotamia, the Northern Caucasus (Maykop culture) and Central Europe. The wheel initially took the form of the potter's wheel. The new concept quickly led to wheeled vehicles and mill wheels. The Sumerians' cuneiform script is the oldest (or second oldest after the Egyptian hieroglyphs) which has been deciphered (the status of even older inscriptions such as the Jiahu symbols and Tartaria tablets is controversial). The Sumerians were among the first astronomers, mapping the stars into sets of constellations, many of which survived in the zodiac and were also recognized by the ancient Greeks.[63] They were also aware of the five planets that are easily visible to the naked eye.[64]\\r\\nThey invented and developed arithmetic by using several different number systems including a mixed radix system with an alternating base 10 and base 6. This sexagesimal system became the standard number system in Sumer and Babylonia. They may have invented military formations and introduced the basic divisions between infantry, cavalry, and archers. They developed the first known codified legal and administrative systems, complete with courts, jails, and government records. The first true city-states arose in Sumer, roughly contemporaneously with similar entities in what are now Syria and Lebanon. Several centuries after the invention of cuneiform, the use of writing expanded beyond debt/payment certificates and inventory lists to be applied for the first time, about 2600 BC, to messages and mail delivery, history, legend, mathematics, astronomical records, and other pursuits. Conjointly with the spread of writing, the first formal schools were established, usually under the auspices of a city-state's primary temple.\\r\\nFinally, the Sumerians ushered in domestication with intensive agriculture and irrigation. Emmer wheat, barley, sheep (starting as mouflon), and cattle (starting as aurochs) were foremost among the species cultivated and raised for the first time on a grand scale.\\r\\nGeography\\r\\nLanguage\\r\\nCoordinates: 3200N 4530E? / ?32.0N 45.5E? / 32.0; 45.5","input":"When did the sumerians emerge as a civilization?"},{"output":"126,326","context":"Cedar Rapids /?si?d?r ?r?p?dz/ is the second-largest city in Iowa and is the county seat of Linn County. The city lies on both banks of the Cedar River, 20 miles (32?km) north of Iowa City and 100 miles (160?km) northeast of Des Moines, the state's capital and largest city. It is a part of the Cedar Rapids/Iowa City Corridor of Linn, Benton, Cedar, Jones, Johnson, and Washington counties.[5]\\r\\nAs of the 2010 United States Census, the city population was 126,326.[6][7] The estimated population of the three-county Metropolitan Statistical Area, which includes the nearby cities of Marion and Hiawatha, was 255,452 in 2008.[8] Cedar Rapids is an economic hub of the state, located in the core of the Interstate 380.\\r\\nA flourishing center for arts and culture in Eastern Iowa, the city is home to the Cedar Rapids Museum of Art, the National Czech & Slovak Museum & Library, the Paramount Theatre, Orchestra Iowa, Theatre Cedar Rapids, the African-American Historical Museum and Cultural Center of Iowa, the Iowa Cultural Corridor Alliance. In the 1990s and 2000s, several Cedar Rapidians became well-known actors, including Bobby Driscoll, Ashton Kutcher, Elijah Wood, and Ron Livingston. The city is the setting for the musical The Pajama Game and the comedy film Cedar Rapids.\\r\\nAmong the famous people who have lived in Cedar Rapids are American Gothic painter Grant Wood, journalist and historian William L. Shirer, writer and photographer Carl Van Vechten, and aerodynamics pioneer Dr. Alexander Lippisch. The area has also produced professional athletes such as Landon Cassill, Ryan Sweeney, Trent Green, Zach Johnson, and Kurt Warner, as well as Mark Walter, co-owner and chairman of baseball's Los Angeles Dodgers.\\r\\nCedar Rapids is nicknamed the \\"City of Five Seasons\\", for the \\"fifth season\\", which is time to enjoy the other four.[1] The symbol of the five seasons is the Tree of Five Seasons sculpture in downtown along the north river bank. The name \\"Five Seasons\\" and representations of the sculpture appear throughout the city in many forms.[1]\\r\\n\\r\\n\\r\\nThe location of present-day Cedar Rapids was in the territory of the Fox and Sac tribes.\\r\\nThe first permanent settler, Osgood Shepherd, arrived in 1838. When Cedar Rapids was first established in 1838, William Stone named the town Columbus. In 1841 it was resurveyed and renamed by N.B. Brown and his associates. They named the town Cedar Rapids for the rapids in the Cedar River at the site, and the river itself was named for the large number of red cedar trees that grew along its banks.[9] Cedar Rapids was incorporated on January 15, 1849.[10] Cedar Rapids annexed the community of Kingston in 1870.\\r\\nThe economic growth of Cedar Rapids increased in 1871 upon the founding of the Sinclair meatpacking company.\\r\\nIn 2010, the Census Bureau reported Cedar Rapids' population as 87.98% white, and 5.58% black.[11]\\r\\nDuring the Iowa flood of 2008, the Cedar River reached a record high of 31.12 feet (9.49?m) on June 13, 2008, the previous record was 20 feet (6.1?m) surpassing the 500-year flood plain. 1,126 city blocks were flooded, or more than 10 square miles (26?km2), 561 city blocks were severely damaged, on both banks of the Cedar River comprising 14% of the city's total area. There were a total of 7,749 flooded properties that had to be evacuated, 5,900 were homes, and 310 were city facilities including the City Hall, Central Fire Station, Main Public Library, Ground Transportation Center, Public Works building, and Animal Control building. It is estimated 1300 or more properties need to be demolished in the Cedar Rapids area because of the flood, which caused several billions of dollars in damages. More than 4000 members of the Iowa National Guard were activated to assist the city. The temporary levees became saturated not only with the flood waters but also with additional rainfall causing the levies to fail.[12][13]\\r\\nUntil the 2008 flood, the city's government was headquartered in the Veterans Memorial Building, near the Linn County Courthouse and jail on Mays Island in the Cedar River, making Cedar Rapids one of a few cities in the world, along with Paris, France, with governmental offices on a municipal island.[14]\\r\\nDuring the flood of 2016, remnants of Hurricane Paine from the eastern Pacific Ocean via the Gulf of California caused the second highest recorded crest of the Cedar River in Cedar Rapids reaching 22 feet (6.7?m) on Tuesday, September 27, 2016.[15][16][17] The inundation of southern Minnesota, central and western Wisconsin, and northeastern Iowa by Hurricane Paine's remnants began on September 21 and 22 and continued until the end of September 2016.[18][19][20][21] This cresting in Cedar Rapids was below the initial estimate of 25 feet (7.6?m) and the revised estimate of 23 feet (7.0?m), but more than 10 feet (3.0?m) above the flood stage of 12 feet (3.7?m).[22][23][24] This flood was above levels considered to have about a 1 percent chance of occurring in a given year.[25][a] More than 5,000 homes were affected including the flooded areas around Time Check Park, the Czech Village, and the New Bohemia district causing over 5,000 persons to be evacuated.[25][13][26][27] The Cedar Rapids Schools did not have school for a week.[28]\\r\\nIn 2015, the city of Cedar Rapids approved a $625 million flood protection plan over 20 years for levee improvements.[25] Although the improvement to the levee system in Cedar Rapids had not been completed due to over $80 million in funding not appropriated by the United States Congresses of 2014 and 2016 and the voting down by local residents of a temporary increase in the local sales tax to pay for the levee improvements,[b] Out of school students along with hundreds of thousands of volunteers and 412 Iowa National Guard troops filled more than a quarter of a million sand bags in a successful effort that prevented any major flooding of the city outside of the evacuation zone.[25][29] A 9.8-mile (15.8?km) system of Hesco barriers, earthen berms, and over 400,000 sand bags were used to plug the gaps in the levee system.[25][30][31] The city of Cedar Rapids purchased additional Hesco barriers from Iowa City for $1.4 million.[32] Numerous upstream cities which had been earlier affected by the September flooding and mandatory evacuations including Charles City, Greene, Manchester, Clarksville, Shell Rock, Vinton, Janesville, Cedar Falls and Waterloo sent hundreds of thousands of unused sand bags to support efforts in Cedar Rapids and nearby communities.[19][27] The remnants of Hurricane Paine did not produce any rain to saturate the temporary earth berms and sand bags, which would have greatly increased the likelihood of breach in the temporary levee structures and thus causing a much greater flooded area; the river crested during very sunny weather in Cedar Rapids. Additionally beginning on the evening of Sunday, September 25, 300 to 400 National Guard troops along with the Iowa State Patrol, other law enforcement agencies, and 60 duly sworn law enforcement officials enforced an 8pm to 7am curfew, nightly.[27][28][29]\\r\\nThe city is divided into four quadrants, used in addressing. 1st Avenue (U.S. Route 151 Business) divides the north and south sides of the city, and the Cedar River divides east and west. Mays Island, in the middle of the river, is the only area of the city where addresses have no quadrant. Areas outside of the city limit that use the \\"Cedar Rapids\\" city name on their mailing address also do not use the quadrants.\\r\\nExcept in the downtown area, 1st Avenue and the Cedar River tend to run diagonally instead of along the cardinal directions. Due to the curving of 1st Avenue, there are some areas in western Cedar Rapids where NW addresses are actually south of SW addresses.\\r\\nCedar Rapids is divided into fourteen ZIP Codes. Mays Island and the downtown area are covered by 52401. The northeast quadrant is covered by 52402 and 52411. The southeast quadrant is covered by 52403. The southwest quadrant is covered by 52404. The northwest quadrant is covered by 52405. Post office boxes are covered by ZIP codes 52406, 52407, 52408, 52409, and 52410. Several other ZIP codes are for specific business (Aegon USA, Rockwell Collins, etc.).\\r\\nAccording to the United States Census Bureau, the city has a total area of 72.07 square miles (186.66?km2), of which, 70.8 square miles (183.37?km2) is land and 1.27 square miles (3.29?km2) is water.[2]\\r\\nThere are twelve active neighborhood associations in Cedar Rapids. The neighborhoods nearest downtown include Wellington Heights and Oakhill Jackson in the southeast quadrant and Moundview in the northeast quadrant. Also farther north in the northeast quadrant is the Kenwood Park, which was independent until it was incorporated into the Cedar Rapids city limits, and Noelridge Park neighborhood. The boundaries of Kenwood are 32nd Street to Oakland Road to Old Marion Road to C Avenue to 40th Street then 1st Avenue between 40th street and 32nd Street.[33]\\r\\nIn addition to the neighborhood associations in Cedar Rapids, there are many informal, unofficial neighborhoods, such as Bowman Woods, Vernon Heights, Stoney Point, New Bohemia (NewBo) and Wilderness Estates.\\r\\nCzech Village is located along 16th Avenue SW, which is south of the Cedar River. It is home to such Czech-related businesses as The Czech Cottage, Sykora Bakery, and White Lion Treasures. The National Czech & Slovak Museum & Library is one of the major tourist attractions in Cedar Rapids, and the nearby Bohemian National Cemetery may also be of interest to visitors. The National Czech & Slovak Museum's main building was located directly on the river and was badly damaged by the 2008 floods. 1400 The museum moved a few blocks after the flood to Inspiration Pl SW, Cedar Rapids, IA 52404.\\r\\nThe Cedar Rapids Czech Heritage Foundation is one of many local organizations working to promote and preserve Czech heritage in Cedar Rapids. They support and sponsor many programs and events throughout the year. One of these programs is the Miss Czech-Slovak Iowa pageant. Two Miss Czech-Slovak US queens can claim this community as home: Lisa Volesky and Stasia Krivanek.\\r\\nOlga Drahozal was the famed band leader of the Czech Plus Polka Band, a performing group that frequents the Kosek Band Stand in Czech Village. She, along with Bessie Duggena and Leona Podu?ka, taught Czech School (?esk ?kola) at Wilson Middle School.\\r\\nIn 2003, the African-American Historical Museum and Cultural Center of Iowa opened its doors. Cedar Rapids is also home to the historic 26 acre (105,000 m2) Brucemore Estate, on which sits a 21-room mansion, and the Masonic Library and Museum.\\r\\nIn 2009, Cedar Rapids was rated one of the \\"Top 10 cities to Grow Up In\\" in the United States, partly due to a low crime rate and a good public school system.[34]\\r\\nCedar Rapids has a humid continental climate with long, cold, sometimes brutal winters with plenty of snow, while summers are hot and humid, with frequent severe thunderstorms.\\r\\nThe record low temperature in Cedar Rapids is ?29?F (?34?C), set on January 15, 2009,[35] while the record high temperature of 104?F (40?C) was set on July 31, 1988.\\r\\nThe Cedar Rapids Metropolitan Statistical Area consists of Linn, Benton, and Jones counties. The MSA had a 2000 census population of 237,230, with an estimated 2008 population of 255,452;[8] Linn County was the only county in the MSA before the MSA was redefined after the 2000 census.\\r\\nAs a growing job center, Cedar Rapids pulls commuters from nearby Marion and Hiawatha. Other towns that have become bedroom communities include Ely, Swisher, Shueyville, Palo, Atkins, Fairfax, Walford, Robins and Bertram.\\r\\nBased on the 2010 American Community Survey[37] 1 Year Estimates, the median income for a household in the city was $51,186, and the median income for a family was $63,265. Males had a median income of $40,413 versus $26,402 for females. The per capita income for the city is $26,370. About 6.3% of families and 11.7% of the population were below the poverty line, including 15.5% of those under the age of 18 and 4.3% of those 65 or older.\\r\\nAs of the census[38] of 2010, there were 126,326 people, 53,236 households, and 30,931 families residing in the city. The population density was 1,784.3 people per square mile (688.9/km2). There were 57,217 housing units at an average density of 808.2 per square mile (312.0/km2). The racial makeup of the city was 87.98% White, 5.58% African American, 0.31% Native American, 2.21% Asian, 0.12% Pacific Islander, 0.93% from other races, and 2.87% from two or more races. Hispanic or Latino of any race were 3.31% of the population.\\r\\nThere were 53,236 households of which 28.9% had children under the age of 18 living with them, 42.8% were married couples living together, 11.0% had a female householder with no husband present, and 41.9% were non-families. 32.5% of all households were made up of individuals and 10.3% had someone living alone who was 65 years of age or older. The average household size was 2.31 and the average family size was 2.95.\\r\\nAge spread: 23.5% under the age of 18, 11.2% from 18 to 24, 27.4% from 25 to 44, 24.8% from 45 to 64, and 13.1% who were 65 years of age or older. The median age was 35.3 years. For every 100 females there were 96.6 males. For every 100 females age 18 and over, there were 94.4 males.[39]\\r\\nIn the 2000 census, Cedar Rapids was 91.9% non-Hispanic white, with well over half of the population claiming a specific ethnic European ancestry, such as Germans (35.5%), Irish (17.1%), English (9.4%), Czechs (7.8%), Norwegians (5.1%), and French from either France or Canada (3.2%).[40] The city also has a growing minority population: for example, in the three-year period from 2006 to 2008, the U.S. Census Bureau estimated that 4.9% of the Cedar Rapids population identified as African Americans, up from 3.7% in the 2000 census.[40][41]\\r\\nCedar Rapids has played an important role in Muslim culture in the United States. The National Muslim Cemetery, on 12 acres (49,000?m2) of land donated by Haj. Yahya William Aossey in 1948, is said to be the first exclusively Muslim cemetery in North America.[citation needed] Graves in the cemetery face Mecca.[42] The Mother Mosque of America, dedicated on June 16, 1934, is the longest standing mosque in North America.[43][44] In 1972, another mosque was built and the original mosque was sold and fell into disrepair before being purchased in 1990 by the Islamic Council of Iowa and renovated.[45][46] It is on the National Register of Historic Places. The Iowa flood of 2008 extensively damaged the basement, destroying many historic documents.\\r\\nMuslim presence in the area dates to 1895 when the first immigrants arrived from the Beqaa Valley in today's Lebanon and Syria.[47] Islamic Services of America (I.S.A.) was established in Cedar Rapids in 1975 and provides Halal Certification and supervision throughout the world.[48]\\r\\nCedar Rapids is one of the largest cities in the world for corn processing. The grain processing industry is Cedar Rapids' most important sector, directly providing 4,000 jobs that pay on average $85,000, and also providing 8,000 indirectly.[50] Fortune 500 company Rockwell Collins and trucking company CRST are based in Cedar Rapids,[51] and Aegon has its United States headquarters there. A large Quaker Oats mill, one of the four that merged in 1901 to form Quaker Oats, dominates the north side of downtown. Other large companies that have facilities in Cedar Rapids include Archer Daniels Midland, Cargill, General Mills, Toyota Financial Services and Nordstrom.[50] Newspaperarchive, based in Cedar Rapids, is the largest newspaper archive in North America with a repository of more than 150 million pages assembled over 250 years; it was taken offline for two days by the 2008 flood.\\r\\nAccording to Cedar Rapids' 2014 Comprehensive Annual Financial Report,[52] the top employers in the area are:\\r\\nCedar Rapids is home to Orchestra Iowa, the Paramount Theatre, Theatre Cedar Rapids, Cedar Rapids Opera Theatre, and Brucemore, a National Trust Historic Site, among others.\\r\\nCedar Rapids is also home to the Cedar Rapids Museum of Art, The Cedar Rapids Ceramics Center, Legion Art's CSPS Hall, the National Czech & Slovak Museum & Library, the African American Historical Museum, Kirkwood Community College's Iowa Hall Gallery, and the legendary Grant Wood Studio at 5 Turner Alley. These Cedar Rapids venues have recently hosted world class and award nominated exhibitions, including the works of Andy Warhol, Grant Wood, and the Iowa Biennial, among others.\\r\\nThe Cedar Rapids Museum of Art houses the largest collection of Grant Wood paintings in the world. The 1920s Paramount Theatre is home to the Orchestra Iowa and the Cedar Rapids Area Theatre Organ Society. Concerts and events such as high school graduations, sporting events, exhibitions, and political rallies are held in the U.S. Cellular Center, formerly known as The Five Seasons Center.\\r\\nMany arts centers in Cedar Rapids sustained severe damage during the June 2008 flood. Among those severely damaged are the Paramount Theatre, Theatre Cedar Rapids, the National Czech & Slovak Museum, and the African American Historical Museum. Two Wurlitzer organs were damaged, located at the Paramount Theatre and Theatre Cedar Rapids. The Cedar Rapids Museum of Art suffered minor damage. It is expected to cost $25 million to repair the Paramount;[53][54] Theatre Cedar Rapids reopened in February 2010.[55]\\r\\nCedar Rapids became a popular internet meme during the 2016 Presidential Election, after Hillary Clinton posted a video saying, \\"I'm just chilling in Cedar Rapids\\" using a popular mobile app called Snapchat.[56]\\r\\nCedar Rapids is home to one major league, and three minor league sports franchises:\\r\\nCedar Rapids has over 3,360 acres (13.6?km2) of city owned property for undeveloped green space and recreational use. There are 74 formally named parks or recreational facilities. These include baseball and softball fields, all-weather basketball courts, two frisbee golf courses, sand volleyball courts, the Tuma Soccer Complex, a BMX dirt track, two off-leash dog exercise areas, the Old MacDonald's Farm (a children's zoo), 10 splash pads, and many parks that have pavilions, picnicking areas and restroom facilities. The various trail systems in Cedar Rapids have a total of 24 miles (39?km) for walking, running or bicycling.[57]\\r\\nThe YMCA has had a local chapter since 1868. It has many facilities including Camp Wapsie.[58]\\r\\nFrom April 6, 1908, to December 31, 2005, Cedar Rapids used the city commission form of government. It was one of the few larger American cities remaining to operate under this model. Under this form of government, the council was made up of a public safety commissioner, a streets commissioner, a finance commissioner, a parks commissioner, and a mayor. The council members worked on a full-time basis, served two-year terms, and were considered department heads. Don Canney, the longest serving mayor in city history, served for twenty-two years under this system.[61] The last mayor of Cedar Rapids under this form of government was Paul Pate.\\r\\nIn 2005 the Cedar Rapids Area Chamber of Commerce spearheaded a movement to change from the commission form of government. A panel was appointed by Mayor Pate and the City Council to study the issue, and recommended that voters be presented with three options:\\r\\nOn June 14, 2005, voters went to the polls to decide whether to adopt a new form of government or continue with the commission form. 28,818 of the 83,514 registered voters (29.72%) cast ballots on the issue. 68.80% of the voters decided to adopt a new form of government.[62] Elections were held on November 8, 2005 and 30 candidates ran. Kay Halloran, a retired attorney and state legislator, became the first mayor elected under the new system. Several members of the city council were elected outright; however, the remaining races were close enough to require a runoff election, which took place in December.\\r\\nCedar Rapids now has an Iowa \\"Home Rule\\" charter which establishes a weak mayor system with a part-time City Council and Mayor both on four-year terms.[63]\\r\\nCedar Rapids is home to two four-year colleges: Coe College and Mount Mercy University. The University of Iowa also has an evening MBA facility there.[64] Kirkwood Community College is the area's only two-year college, while Kaplan University (formerly Hamilton College) and Upper Iowa University also have campuses there. Cornell College in Mount Vernon and the University of Iowa's main campus in Iowa City are both within 30 miles (48?km) of Cedar Rapids.\\r\\nThe Cedar Rapids Community School District is the largest school district in the metropolitan area with an enrollment of 17,263 in the 2006ÿ2007 school year.[65] The district contains 24 elementary schools, six middle schools, and four high schools: Jefferson, Washington, Kennedy, and Metro High School (an alternative high school).[66] Two neighboring school districts draw students from within the Cedar Rapids city limits. The Linn-Mar Community School District serves part of the northeast quadrant of the city and has seven elementary schools inside the city limits.[67] The College Community School District serves part of the southwest quadrant of Cedar Rapids as well as neighboring rural portions of Linn, Benton and Johnson counties. Located in a central campus off Interstate 380 are College Community's five elementary schools, Prairie Creek Intermediate, Prairie Point Middle School & Ninth Grade Academy, and Prairie High School.[68]\\r\\nThe Cedar Rapids Metro Catholic Education System, which is affiliated with the Roman Catholic Archdiocese of Dubuque, consists of six elementary schools,[69] two middle schools,[70] and one high school (Xavier). The Cedar Rapids Catholic Education System and Cedar Rapids Community School District are synonymous with each other in the Cedar Rapids Public and Parochial School System.\\r\\nThe city hosts several private schools, including Summit Schools, Cedar Valley Christian School, Trinity Lutheran School, Isaac Newton Christian Academy, Faith Christian Learning Center, and Good Shepherd Lutheran School[71] of the WELS.\\r\\nCedar Rapids' radio market, which consists of Linn County,[72] is ranked 211th by Arbitron with 172,000 listeners aged 12 and older.[73]\\r\\niHeart Media owns four stations in the Cedar Rapids area, including WMT 600 AM, a news/talk station that has broadcast since 1922. Clear Channel also owns KKSY-FM 96.5, a modern country music station; KMJM 1360 AM, a classic country station; and KOSY-FM 95.7 FM, a hit music station.[74] Townsquare Media owns four radio stations in Cedar Rapids, which were formerly owned by Cumulus Media: KDAT 104.5 FM (adult contemporary), KHAK 98.1 FM (country music), and KRNA 94.1 FM (classic rock). Townsquare also operates KRQN 107.1 under a Lease-Management Agreement. KRQN broadcasts a (contemporary hits) format.[75] Three other stations in Cedar Rapids are independently owned: KZIA 102.9 FM (contemporary hits), KGYM 1600 AM (sports radio), and KMRY 1450 AM/93.1 FM (Classic Hits).[76] Several stations from Davenport, Waterloo, and Iowa City also figure into ratings in Cedar Rapids.[77] These stations include Waterloo-licensed contemporary Christian \\"Life 101.9,\\" KNWS-FM; KFMW 107.9 FM, known as \\"Rock 108,\\" with an active rock format; and KOKZ 105.7 FM, which has a classic hits format. Clear Channel-owned KKRQ 100.7 FM, with a classic rock format, is the Iowa City station that is typically highly rated in Cedar Rapids.\\r\\nThe only noncommercial station licensed to Cedar Rapids is KCCK-FM 88.3 FM, a jazz station licensed to Kirkwood Community College. KXGM 89.1 is a non-commercial contemporary Christian music station licensed to neighboring Hiawatha.[76] NPR stations from Cedar Falls (KUNI (FM) 90.9 FM) and Iowa City (KSUI 91.7 FM and WSUI 910 AM) reach Cedar Rapids.[78]\\r\\nThe Cedar Rapids-Waterloo-Iowa City-Dubuque media market consists of 21 eastern Iowa counties: Allamakee, Benton, Black Hawk, Bremer, Buchanan, Butler, Cedar, Chickasaw, Clayton, Delaware, Dubuque, Fayette, Grundy, Iowa, Johnson, Jones, Keokuk, Linn, Tama, Washington, and Winneshiek.[72] It is ranked 90th by Nielsen Media Research for the 2016ÿ17 television season with 346,330 television households.[79]\\r\\nCedar Rapids is home to four network-affiliated stations: KGAN channel 2 (CBS), KCRG channel 9 (ABC), KFXA channel 28 (Fox), and KPXR-TV channel 48 (ION). NBC affiliate KWWL channel 7 and The CW affiliate KWWL-DT2 are based in Waterloo and maintain a newsroom inside the Alliant Energy tower in downtown Cedar Rapids. Other stations in the market are KWKB channel 20 (This TV), licensed to Iowa City and KFXB-TV channel 40 (CTN), licensed to Dubuque. Public television is provided by Iowa Public Television, which has two stations in the area: KIIN channel 12 in Iowa City and KRIN channel 32 in Waterloo. KWWF channel 22 (RTN), which operated from Waterloo, ceased broadcasting in 2013. Mediacom and local company ImOn Communications provide cable television service to Cedar Rapids.\\r\\nThe Gazette is the primary daily newspaper for Cedar Rapids. The Cedar Rapids Gazette won a Pulitzer Prize in 1936, under editor Verne Marshall and primarily due to his efforts and articles, for its campaign against corruption and misgovernment in the State of Iowa.[80]\\r\\nCedar Rapids is an American comedy film about a naive insurance agent, played by Ed Helms, who is sent to represent his company at a regional conference in big town Cedar Rapids.[81] Although the film is set in Cedar Rapids, it was actually mostly shot in Ann Arbor, Michigan, although exterior shots were done in Cedar Rapids.[82]\\r\\nThe 2017 film Amelia 2.0 is a scifi drama set in a nameless fictional city. The majority of the movie was filmed in Cedar Rapids, using iconic locations such as the Cedar Rapids Public Library and Theater Cedar Rapids as important set pieces.\\r\\nReleased on February 26, 2010,The Crazies is a 2010 film which fictionally takes place near Cedar Rapids at Odgen Marsh, Iowa. In the film, residents both in and near Ogden Marsh become infected with a \\"Rhabdoviridae prototype\\" biological weapon called Trixie. At the end of the movie, David and Judy, who are unaffected victims within the affected area, arrive at a truck stop to search for a vehicle but discover that the National Guard have also executed those who were evacuated. Fending off three Trixie infected persons, they escape in a semi-truck. In order to stop the infection from spreading, the National Guard destoys Ogden Marsh in a massive explosion. As the couple flees, their truck flips in the passing shockwave. They then exit their wrecked semi and begin walking towards Cedar Rapids. A view from a military satellite highlights first the couple, then the city of Cedar Rapids. The words \\"Initiate containment protocol\\" appear, signifying a new containment attempt by the National Guard. Later, during the credits, Bruce Aune, a real newscaster from KCRG-TV 9 in Cedar Rapids reports that an explosion originating from the Dakon Pendrill chemical plant started a massive fire in Ogden Marsh which is merely a cover story for the National Guard's actual destruction of the town. Bruce Aune then reports that a perimeter has been set and civilians are not being allowed into the area. A Trixie-infected individual appears on camera just before the signal is lost.[83][84][85][86][87][88][89][90]\\r\\nCedar Rapids is served by Cedar Rapids Transit, consisting of an extensive bus system and taxis. Cedar Rapids Transit operates scheduled bus service throughout the city and to Marion and Hiawatha.[91] A series of enclosed pedestrian skywalks connect several downtown buildings.[92]\\r\\nThe city is also served by The Eastern Iowa Airport (formerly known as the Cedar Rapids Airport), a regional airport that connects with other regional and international airports. Cedar Rapids Transit and private bus lines also connect at the airport.[93]\\r\\nInterstate 380, part of the Avenue of the Saints, runs north-south through Cedar Rapids. U.S. Highways 30, 151, and 218 and Iowa Highway 13 and Iowa Highway 100 also serve the city.[94]\\r\\nCedar Rapids is served by four major railroads. They are the Union Pacific, the Cedar Rapids and Iowa City Railway (Crandic), the Canadian National, and the Iowa Northern Railway Company [IANR]. The Iowa Northern Railway has its headquarters in the historic Paramount Theater Building. The Crandic and the Iowa Interstate Railroad also are headquartered in Cedar Rapids. The Iowa Interstate reaches the city via the Crandic tracks, running a daily train from Iowa City, Iowa to Cedar Rapids.[citation needed]\\r\\nCedar Rapids is linked to other Midwestern cities by the Burlington Trailways bus hub at the Eastern Iowa Airport.[95]\\r\\nThere are two hospitals in Cedar Rapids, St. Luke's and Mercy Medical Center.","input":"What is the population of cedar rapids iowa?"},{"output":"organ system","context":"The human musculoskeletal system (also known as the locomotor system, and previously the activity system[1]) is an organ system that gives humans the ability to move using their muscular and skeletal systems. The musculoskeletal system provides form, support, stability, and movement to the body.\\r\\nIt is made up of the bones of the skeleton, muscles, cartilage,[2] tendons, ligaments, joints, and other connective tissue that supports and binds tissues and organs together. The musculoskeletal system's primary functions include supporting the body, allowing motion, and protecting vital organs.[3] The skeletal portion of the system serves as the main storage system for calcium and phosphorus and contains critical components of the hematopoietic system.[4]\\r\\nThis system describes how bones are connected to other bones and muscle fibers via connective tissue such as tendons and ligaments. The bones provide stability to the body. Muscles keep bones in place and also play a role in the movement of bones. To allow motion, different bones are connected by joints. Cartilage prevents the bone ends from rubbing directly onto each other. Muscles contract to move the bone attached at the joint.\\r\\nThere are, however, diseases and disorders that may adversely affect the function and overall effectiveness of the system. These diseases can be difficult to diagnose due to the close relation of the musculoskeletal system to other internal systems. The musculoskeletal system refers to the system having its muscles attached to an internal skeletal system and is necessary for humans to move to a more favorable position. Complex issues and injuries involving the musculoskeletal system are usually handled by a physiatrist (specialist in physical medicine and rehabilitation) or an orthopaedic surgeon.\\r\\n\\r\\n\\r\\nThe skeletal system serves many important functions; it provides the shape and form for the body, support and protection, allows bodily movement, produces blood for the body, and stores minerals.[5] The number of bones in the human skeletal system is a controversial topic. Humans are born with over 300 bones; however, many bones fuse together between birth and maturity. As a result, an average adult skeleton consists of 206 bones. The number of bones varies according to the method used to derive the count. While some consider certain structures to be a single bone with multiple parts, others may see it as a single part with multiple bones.[6] There are five general classifications of bones. These are long bones, short bones, flat bones, irregular bones, and sesamoid bones. The human skeleton is composed of both fused and individual bones supported by ligaments, tendons, muscles and cartilage. It is a complex structure with two distinct divisions; the axial skeleton, which includes the vertebral column, and the appendicular skeleton.[7]\\r\\nThe skeletal system serves as a framework for tissues and organs to attach themselves to. This system acts as a protective structure for vital organs. Major examples of this are the brain being protected by the skull and the lungs being protected by the rib cage.\\r\\nLocated in long bones are two distinctions of bone marrow (yellow and red). The yellow marrow has fatty connective tissue and is found in the marrow cavity. During starvation, the body uses the fat in yellow marrow for energy.[8] The red marrow of some bones is an important site for blood cell production, approximately 2.6 million red blood cells per second in order to replace existing cells that have been destroyed by the liver.[5] Here all erythrocytes, platelets, and most leukocytes form in adults. From the red marrow, erythrocytes, platelets, and leukocytes migrate to the blood to do their special tasks.\\r\\nAnother function of bones is the storage of certain minerals. Calcium and phosphorus are among the main minerals being stored. The importance of this storage \\"device\\" helps to regulate mineral balance in the bloodstream. When the fluctuation of minerals is high, these minerals are stored in bone; when it is low it will be withdrawn from the bone.\\r\\nThere are three types of musclescardiac, skeletal, and smooth. Smooth muscles are used to control the flow of substances within the lumens of hollow organs, and are not consciously controlled. Skeletal and cardiac muscles have striations that are visible under a microscope due to the components within their cells. Only skeletal and smooth muscles are part of the musculoskeletal system and only the skeletal muscles can move the body. Cardiac muscles are found in the heart and are used only to circulate blood; like the smooth muscles, these muscles are not under conscious control. Skeletal muscles are attached to bones and arranged in opposing groups around joints.[9] Muscles are innervated, to communicate nervous energy to,[10] by nerves, which conduct electrical currents from the central nervous system and cause the muscles to contract.[11]\\r\\nIn mammals, when a muscle contracts, a series of reactions occur. Muscle contraction is stimulated by the motor neuron sending a message to the muscles from the somatic nervous system. Depolarization of the motor neuron results in neurotransmitters being released from the nerve terminal. The space between the nerve terminal and the muscle cell is called the neuromuscular junction. These neurotransmitters diffuse across the synapse and bind to specific receptor sites on the cell membrane of the muscle fiber. When enough receptors are stimulated, an action potential is generated and the permeability of the sarcolemma is altered. This process is known as initiation.[12]\\r\\nA tendon is a tough, flexible band of fibrous connective tissue that connects muscles to bones.[13] The extra-cellular connective tissue between muscle fibers binds to tendons at the distal and proximal ends, and the tendon binds to the periosteum of individual bones at the muscle's origin and insertion. As muscles contract, tendons transmit the forces to the relatively rigid bones, pulling on them and causing movement. Tendons can stretch substantially, allowing them to function as springs during locomotion, thereby saving energy.\\r\\nJoints are structures that connect individual bones and may allow bones to move against each other to cause movement. There are two divisions of joints, diarthroses which allow extensive mobility between two or more articular heads, and false joints or synarthroses, joints that are immovable, that allow little or no movement and are predominantly fibrous. Synovial joints, joints that are not directly joined, are lubricated by a solution called synovial fluid that is produced by the synovial membranes. This fluid lowers the friction between the articular surfaces and is kept within an articular capsule, binding the joint with its taut tissue.[7]\\r\\nA ligament is a small band of dense, white, fibrous elastic tissue.[7] Ligaments connect the ends of bones together in order to form a joint. Most ligaments limit dislocation, or prevent certain movements that may cause breaks. Since they are only elastic they increasingly lengthen when under pressure. When this occurs the ligament may be susceptible to break resulting in an unstable joint.\\r\\nLigaments may also restrict some actions: movements such as hyper extension and hyper flexion are restricted by ligaments to an extent. Also ligaments prevent certain directional movement.[14]\\r\\nA bursa is a small fluid-filled sac made of white fibrous tissue and lined with synovial membrane. Bursa may also be formed by a synovial membrane that extends outside of the joint capsule.[8] It provides a cushion between bones and tendons and/or muscles around a joint; bursa are filled with synovial fluid and are found around almost every major joint of the body.\\r\\nBecause many other body systems, including the vascular, nervous, and integumentary systems, are interrelated, disorders of one of these systems may also affect the musculoskeletal system and complicate the diagnosis of the disorder's origin. Diseases of the musculoskeletal system mostly encompass functional disorders or motion discrepancies; the level of impairment depends specifically on the problem and its severity. In a study of hospitalizations in the United States, the most common inpatient OR procedures in 2012 involved the musculoskeletal system: knee arthroplasty, laminectomy, hip replacement, and spinal fusion.[16]\\r\\nArticular (of or pertaining to the joints)[17] disorders are the most common. However, also among the diagnoses are: primary muscular diseases, neurologic (related to the medical science that deals with the nervous system and disorders affecting it)[18] deficits, toxins, endocrine abnormalities, metabolic disorders, infectious diseases, blood and vascular disorders, and nutritional imbalances.\\r\\nDisorders of muscles from another body system can bring about irregularities such as: impairment of ocular motion and control, respiratory dysfunction, and bladder malfunction. Complete paralysis, paresis, or ataxia may be caused by primary muscular dysfunctions of infectious or toxic origin; however, the primary disorder is usually related to the nervous system, with the muscular system acting as the effector organ, an organ capable of responding to a stimulus, especially a nerve impulse.[4]\\r\\nOne understated disorder that begins during pregnancy is Pelvic girdle pain, it is complex and multi-factorial and likely to be also represented by a series of sub-groups driven by pain varying from peripheral or central nervous system,[19] altered laxity/stiffness of muscles,[20] laxity to injury of tendinous/ligamentous structures[21] to mal-adaptive body mechanics.[19]","input":"What is the musculoskeletal system and what does it do?"},{"output":"Barry Allen","context":"The Flash (or simply Flash) is the name of several superheroes appearing in comic books published by DC Comics. Created by writer Gardner Fox and artist Harry Lampert, the original Flash first appeared in Flash Comics #1 (cover date January 1940/release month November 1939).[1] Nicknamed the \\"Scarlet Speedster\\", all incarnations of the Flash possess \\"super speed\\", which includes the ability to run, move, and think extremely fast, use superhuman reflexes, and seemingly violate certain laws of physics.\\r\\nThus far, at least four different characterseach of whom somehow gained the power of \\"the speed force\\"have assumed the mantle of the Flash in DC's history: college athlete Jay Garrick (1940ÿ1951, 1961ÿ2011, 2017ÿpresent), forensic scientist Barry Allen (1956ÿ1985, 2008ÿpresent), Barry's nephew Wally West (1986ÿ2011, 2016ÿpresent), and Barry's grandson Bart Allen (2006ÿ2007). Each incarnation of the Flash has been a key member of at least one of DC's premier teams: the Justice Society of America, the Justice League, and the Teen Titans.\\r\\nThe Flash is one of DC Comics' most popular characters and has been integral to the publisher's many reality-changing \\"crisis\\" storylines over the years. The original meeting of the Golden Age Flash Jay Garrick and Silver Age Flash Barry Allen in \\"Flash of Two Worlds\\" (1961) introduced the Multiverse storytelling concept to DC readers, which would become the basis for many DC stories in the years to come.\\r\\nLike his Justice League colleagues Wonder Woman, Superman and Batman, the Flash has a distinctive cast of adversaries, including the various Rogues (unique among DC supervillains for their code of honor) and the various psychopathic \\"speedsters\\" who go by the names Reverse-Flash or Zoom. Other supporting characters in Flash stories include Barry's wife Iris West, Wally's wife Linda Park, Bart's girlfriend Valerie Perez, friendly fellow speedster Max Mercury, and Central City police department members David Singh and Patty Spivot.\\r\\nA staple of the comic book DC Universe, the Flash has been adapted to numerous DC films, video games, animated series, and live-action television shows. In live action, Barry Allen has been portrayed by Rod Haase for the 1979 television special Legends of the Superheroes, John Wesley Shipp and Grant Gustin in the 1990 The Flash series and the 2014 The Flash series, respectively, as well as by Ezra Miller in the DC Extended Universe series of films, beginning with Batman v Superman: Dawn of Justice (2016). Shipp also portrays a version of Jay Garrick in the 2014 The Flash series. The various incarnations of the Flash also feature in animated series such as Superman: The Animated Series, Justice League, Batman: The Brave and the Bold and Young Justice, as well as the DC Universe Animated Original Movies series.\\r\\n\\r\\n\\r\\nThe Flash first appeared in the Golden Age Flash Comics #1 (January 1940), from All-American Publications, one of three companies that would eventually merge to form DC Comics. Created by writer Gardner Fox and artist Harry Lampert, this Flash was Jay Garrick, a college student who gained his speed through the inhalation of hard water vapors. When re-introduced in the 1960s Garrick's origin was modified slightly, gaining his powers through exposure to heavy water.\\r\\nJay Garrick was a popular character in the 1940s, supporting both Flash Comics and All-Flash Quarterly (later published bi-monthly as simply All-Flash); co-starring in Comic Cavalcade; and being a charter member of the Justice Society of America, the first superhero team, whose adventures ran in All Star Comics. With superheroes' post-war decline in popularity, Flash Comics was canceled with issue #104 (1949) which featured an Evil version of the Flash called the Rival. The Justice Society's final Golden Age story ran in All Star Comics #57 (1951; the title itself continued, as All Star Western).\\r\\nIn 1956, DC Comics successfully revived superheroes, ushering in what became known as the Silver Age of comic books. Rather than bringing back the same Golden Age heroes, DC rethought them as new characters for the modern age. The Flash was the first revival, in the aptly named tryout comic book Showcase #4 (Oct. 1956).\\r\\nThis new Flash was (Barry Allen), a police scientist who gained super-speed when bathed by chemicals after a shelf of them was struck by lightning. He adopted the name The Scarlet Speedster after reading a comic book featuring the Golden Age Flash.[1] After several more appearances in Showcase, Allen's character was given his own title, The Flash, the first issue of which was #105 (resuming where Flash Comics had left off). Barry Allen and the new Flash were created by writers Robert Kanigher and John Broome and cartoonist Carmine Infantino.\\r\\nThe Silver Age Flash proved popular enough that several other Golden Age heroes were revived in new incarnations (see: Green Lantern). A new superhero team, the Justice League of America, was also created, with the Flash as a main, charter member.\\r\\nBarry Allen's title also introduced a much-imitated plot device into superhero comics when it was revealed that Garrick and Allen existed on fictional parallel worlds. Their powers allowed them to cross the dimensional boundary between worlds, and the men became good friends. Flash of Two Worlds (The Flash (vol. 1) #123) was the first crossover in which a Golden Age character met a Silver Age character. Soon, there were crossovers between the entire Justice League and the Justice Society; their respective teams began an annual get-together which endured from the early 1960s until the mid-1980s.\\r\\nAllen's adventures continued in his own title until the event of Crisis on Infinite Earths. The Flash ended as a series with issue #350. Allen's life had become considerably confused in the early 1980s, and DC elected to end his adventures and pass the mantle on to another character. Allen died heroically in Crisis on Infinite Earths #8 (1985). Thanks to his ability to travel through time, he would continue to appear occasionally in the years to come.\\r\\nThe third Flash was Wally West, introduced in The Flash (vol. 1) #110 (Dec. 1959) as Kid Flash. West, Allen's nephew by marriage, gained the Flash's powers through an accident identical to Allen's. Adopting the identity of Kid Flash, he maintained membership in the Teen Titans for years. Following Allen's death, West adopted the Flash identity in Crisis on Infinite Earths #12 and was given his own series, beginning with The Flash (vol. 2) #1 in 1987.[1] Many issues began with the catchphrase: \\"My name is Wally West. I'm the fastest man alive.\\"\\r\\nDue to the Infinite Crisis miniseries and the \\"One Year Later\\" jump in time in the DC Universe, DC canceled The Flash (vol. 2) in January 2006 at #230. A new series, The Flash: The Fastest Man Alive, began on June 21, 2006. The initial story arc of this series, written by Danny Bilson and Paul De Meo with art by Ken Lashley, focused on Bart Allen's acceptance of the role of the Flash.\\r\\nFlash: Fastest Man Alive was canceled with issue #13. In its place The Flash (vol. 2) was revived with issue #231, with Mark Waid as the initial writer. Waid also wrote All-Flash #1, which acted as the bridge between the two series.[2] DC had solicited Flash: Fastest Man Alive through issue #15. All Flash #1 replaced issue #14 and The Flash (vol. 2) #231 replaced issue #15 in title and interior creative team only. The covers and cover artists were as solicited by DC, and the information text released was devoid of any plot information.[3][4]\\r\\nIn 2009, Barry Allen made a full-fledged return to the DCU-proper in The Flash: Rebirth, a six-issue miniseries by Geoff Johns and Ethan Van Sciver.[5]\\r\\nWhile several other individuals have used the name Flash, these have lived either on other parallel worlds, or in the future. Jay Garrick, Barry Allen, and Wally West are the best-known exemplars of the identity. The signature wingdings are never absent.\\r\\nJay Garrick was a college student in 1938 who accidentally inhaled heavy water vapors after taking a smoke break inside his laboratory where he had been working.[6] As a result, he found that he could run at superhuman speed and had similarly fast reflexes. After a brief career as a college football star, he donned a red shirt with a lightning bolt and a stylized metal helmet with wings (based on images of the Greek deity Hermes), and began to fight crime as the Flash. His first case involved battling the \\"Faultless Four\\", a group of blackmailers. Garrick kept his identity secret for years without a mask by continually vibrating his body while in public so that any photograph of his face would be blurred. Although originally from Earth-Two, he was incorporated into the history of New Earth following the Crisis on Infinite Earths and is still active as the Flash operating out of Keystone City. He is a member of the Justice Society.\\r\\nBarry Allen is an assistant scientist from the Criminal and Forensic Science Division of Central City Police Department. Barry had a reputation for being very slow, deliberate, and frequently late, which frustrated his fiance, Iris West. One night, as he was preparing to leave work, a freak lightning bolt struck a nearby shelf in his lab and doused him with a cocktail of unnamed chemicals. As a result, Barry found that he could run extremely fast and had matching reflexes. He donned a set of red tights sporting a lightning bolt (reminiscent of the original Fawcett Comics Captain Marvel), dubbed himself the Flash (after his childhood hero, Jay Garrick), and became a crimefighter active in Central City. In his civilian identity, he stores the costume compressed in a special ring via the use of a special gas that could compress cloth fibers to a very small fraction of their normal size.\\r\\nBarry sacrificed his life for the universe in the 1985 maxi-series Crisis on Infinite Earths, and remained dead for over twenty years after that story's publication. With the 2008 series Final Crisis, Barry returned to the DC Universe and returned to full prominence as the Flash in the 2009 series The Flash: Rebirth, which was soon after followed by a new volume of The Flash ongoing series, where Barry's adventures as the Scarlet Speedster are currently published.[7][8]\\r\\nWallace Rudolph \\"Wally\\" West is the nephew of both Iris West and Barry Allen, by marriage, and was introduced in The Flash (vol. 1) #110 (1959). When West was about ten years old, he was visiting his uncle's police laboratory, and the freak accident that gave Allen his powers repeated itself, bathing West in electrically-charged chemicals. Now possessing the same powers as his uncle, West donned a copy of his uncle's outfit and became the young, crime fighter, Kid Flash. After the events of Crisis on Infinite Earths where Barry Allen was killed, Wally took over as the fastest man alive. Following the events of Infinite Crisis, Wally, his wife Linda, and their twins left Earth for an unknown dimension.\\r\\nWally, his wife and twins were pulled back from the Speed Force by the Legion of Super-Heroes at the conclusion of The Lightning Saga.[9] This set the stage for Wally West's return as the Flash after the events of The Flash: Fastest Man Alive #13 (see Bart Allen), in All Flash #1, and with The Flash (vol. 2) series, which resumed with issue #231 in August 2007. It subsequently ends with issue #247, and West, along with all the other Flash characters, play a large role in 2009's The Flash: Rebirth. [7] He briefly appears in the Blackest Night story arc but shortly after the New 52 was launched and the character was nowhere to be seen. He is back as the Flash in DC Rebirth and joined the Titans.\\r\\nBartholomew Henry \\"Bart\\" Allen II is the grandson of Barry Allen and his wife Iris. Bart suffered from accelerated aging and, as a result, was raised in a virtual reality machine until Iris took him back in time to get help from the then-current Flash, Wally West. With Wally's help, Bart's aging slowed, and he took the name Impulse. After he was shot in the knee by Deathstroke, Bart changed both his attitude and his costume, taking the mantle of Kid Flash. During the events of Infinite Crisis, the Speed Force vanished, taking with it all the speedsters save Jay Garrick. Bart returned, four years older, and for a year claimed that he was depowered from the event. However, the Speed Force had not disappeared completely, but had been absorbed into Bart's body; essentially, he now contained all of the Speed Force.\\r\\nBart's costume as the Flash was a clone of his grandfather's, similarly stylized to Wally West's. Not long after taking the mantle of the Flash, Bart was killed by the Rogues in the thirteenth (and final) issue of The Flash: The Fastest Man Alive. However, he was later resurrected in the 31st century in Final Crisis: Legion of 3 Worlds #3 by Brainiac 5 to combat Superboy-Prime and the Legion of Super-Villains. Bart returned to the past and played a large role in The Flash: Rebirth.[10]\\r\\nDark Flash is a Doppelganger of The Flash from Earth-X. His true identity has not yet been revealed. The Dark Flash is scheduled to be a villain for the crossover event between The Flash, Supergirl, Green Arrow and The Legends. He isn't a Flash from the future that becomes the good Flash, He is just a version.\\r\\nDaughter of the speedster Johnny Quick, Jesse Chambers becomes a speeding superhero like her father. She later meets Wally West, the Flash, who asks her to be his replacement if something were to happen to him (as part of an elaborate plan on his part, trying to force Bart Allen to take his role in the legacy of the Flash more seriously). She briefly assumes the mantle of the Flash, after Wally enters the Speed Force.[11]\\r\\nJohn Fox was a historian for the National Academy of Science in Central City in the 27th Century. He was sent back in time to get the help of one or more of the three Flashes (Garrick, Allen, West), in order to defeat the radioactive villain Mota back in Fox's own time period. (Each Flash had individually fought Mota over the course of several years in the 20th century.) Fox's mission was a failure, but during his return trip, the tachyon radiation that sent him through the time stream gave him superspeed. He defeated Mota as a new iteration of the Flash and operated as his century's Flash for a time. Shortly after, he moved to the 853rd century and joined \\"Justice Legion A\\" (also known as Justice Legion Alpha) as seen in the DC One Million series of books. The name \\"John Fox\\" is combined from the names of seminal comic book writers John Broome, who co-created the Barry Allen and Wally West Flashes, and Gardner Fox, who co-created the Jay Garrick Flash.\\r\\nThe father of Sela Allen, his wife and daughter were captured by Cobalt Blue. He is forced to watch his wife die and his daughter become crippled. As he and Max Mercury kill Cobalt Blue, a child takes Cobalt Blue's power gem and kills Allen. This Flash is one of the two destined Flashes to be killed by Cobalt Blue.\\r\\nSela Allen is an ordinary human in the 23rd century until Cobalt Blue steals electrical impulses away from her, causing her to become as slow to the world as the world is to the Flash. Hoping to restore her, her father takes her into the Speed Force. When her father is killed, she appears as a living manifestation of the Speed Force, able to lend speed to various people and objects, but unable to physically interact with the world.[1]\\r\\nBlaine Allen and his son live on the colony world of Petrus in the 28th century. In an attempt to end the Allen blood line, Cobalt Blue injects Allen's son Jace with a virus. Lacking super speed, Jace was unable to shake off the virus. In despair, Blaine takes his son to the Speed Force in the hopes that it would accept him. It takes Blaine instead and grants super speed to Jace so that he can shake off the sickness.[12]\\r\\nJace Allen gains super speed when his father brings him into the Speed Force to attempt to cure him of a virus injected into his body by Cobalt Blue in an attempt to end the Allen bloodline.[12] In memory of his father, Jace assumes the mantle of the Flash and continues the feud against Cobalt Blue.[13]\\r\\nAfter an alien creature invades Earth, a history buff named Kriyad travels back in time from the 98th century to acquire a Green Lantern power ring. He fails, so he tries to capture the Flash's speed instead. After being beaten by Barry Allen (The Flash (vol. 1) #309, May 1982), he travels back further in time and uses the chemicals from the clothes Barry Allen was wearing when he gained his powers to give himself super speed. Kryiad later sacrifices his life to defeat the alien creature.\\r\\nBizarro-Flash was created when Bizarro cloned Flash. He had a costume the reverse colors of Flash's, however he had a mallet symbol because Flash was holding one when he was cloned. The modern version of Bizarro Flash has the symbol of a thunderbolt-shaped mustard stain. He has the powers of the Flash but he is completely intangible.\\r\\nAll incarnations of the Flash can move, think, and react at light speeds as well as having superhuman endurance that allows them to run incredible distances. Some, notably later versions, can vibrate so fast that they can pass through walls in a process called quantum tunneling,[14] travel through time and can also lend and borrow speed. Speedsters can also heal more rapidly than an average human.\\r\\nOn several occasions, the Flash has raced against Superman, either to determine who is faster or as part of a mutual effort to thwart some type of threat; these races, however, often resulted in ties because of outside circumstances. Writer Jim Shooter and artist Curt Swan crafted the story \\"Superman's Race With the Flash!\\" in Superman #199 (Aug. 1967) which featured the first race between the Flash and Superman.[15] Writer E. Nelson Bridwell and artist Ross Andru produced \\"The Race to the End of the Universe\\", a follow-up story four months later in The Flash #175 (Dec. 1967).[16] However, after the DC Universe revision after Crisis on Infinite Earths, the Flash does successfully beat Superman in a race in Adventures of Superman #463 with the explanation that Superman is not accustomed to running at high speed for extended periods of time since flying is more versatile and less strenuous, which means the far more practiced Flash has the advantage. After Final Crisis in Flash: Rebirth #3 the Flash is shown as being significantly faster than Superman, able to outrun him as Superman struggles to keep up with him. He reveals that all the close races between them before had been \\"for charity\\". In the Smallville episode \\"Run\\", Flash is not only able to run faster than a pre-Superman Clark Kent but can match Clark's top speed while running backwards.\\r\\nWhile various incarnations of the Flash have proven their ability to run at light speed, the ability to steal speed from other objects allows respective Flashes to even significantly surpass this velocity. In Flash: The Human Race[17] Wally is shown absorbing kinetic energy to an extent enabling him to move faster than teleportation and run from the end of the universe back to earth in less than a Planck instant (Planck time).\\r\\nSpeedsters may at times use the ability to speed-read at incredible rates and in doing so, process vast amounts of information. Whatever knowledge they acquire in this manner is usually temporary. Their ability to think fast also allows them some immunity to telepathy, as their thoughts operate at a rate too rapid for telepaths such as Martian Manhunter or Gorilla Grodd to read or influence their minds.\\r\\nFlashes and other super-speedsters also have the ability to speak to one another at a highly accelerated rate. This is often done to have private conversations in front of non-fast people (as when Flash speaks to Superman about his ability to serve both the Titans and the JLA in The Titans #2). Speed-talking is also sometimes used for comedic effect where Flash becomes so excited that he begins talking faster and faster until his words become a jumble of noise. He also has the ability to change the vibration of his vocal cords making it so he can change how his voice sounds to others.\\r\\nWhile not having the physical strength of his comrades and enemies, Flash has shown to be able to use his speed to exert incredible momentum into physical attacks. In Injustice: Gods Among Us, Flash uses these kinds of attacks as many of his special moves.\\r\\nThe Flash has also claimed that he can process thoughts in less than an attosecond. At times he is able to throw lightning created by his super speed and make speed vortexes.\\r\\nSome flashes also have the ability to create speed avatars(i.e. duplicates) and these avatars have sometimes been sent to different timelines to complete a particular mission.(Barry Allen exhibits this ability in the live action series \\"The Flash\\").\\r\\nHe can also be seen negating the effects of the anti-life equation, when he freed Iris-West from its control (probably due to his connection with the speed force).\\r\\nIt is said that Wally West has reached the velocity of 23,759,449,000,000,000,000,000,000,000,000,000,000,000,000 (about 24 tredecillion) G c (the speed of light) and he could only do this with the help of every human being on earth moving so the speed force was joined through everyone.[citation needed] With that speed he was able to not only run from planet to planet but different galaxies and universes at what would be considered a blink of an eye.\\r\\nIn the final issue of 52, a new Multiverse is revealed, originally consisting of 52 identical realities. Among the parallel realities shown is one designated \\"Earth-2\\". As a result of Mister Mind \\"eating\\" aspects of this reality, it takes on visual aspects similar to the pre-Crisis Earth-2, including the Flash among other Justice Society of America characters. The names of the characters and the team are not mentioned in the panel in which they appear, but the Flash is visually similar to the Jay Garrick Flash.[18] Based on comments by Grant Morrison, this alternate universe is not the pre-Crisis Earth-2.[19]\\r\\nA variant of the Flasha superfast college student named Mary Maxwellwas seen in the Elseworld book Just Imagine Stan Lee Creating The Flash.\\r\\nThe Flash of Earth-D, Rei was a Japanese man who idolized Barry Allen, whose stories only existed in comic books on this world. Rei was inspired by Allen to become the Flash, much like Allen was inspired to become the Flash by his idol, Jay Garrick. Allen and Rei met during the \\"Crisis on Infinite Earths\\" when Barry was coming back from the 30th century and arrived in the wrong universe. As Earth-D was under attack by the shadow demons, Barry called on the Justice League and Tanaka called on the Justice Alliance, his world's version of the Justice League. They built a cosmic treadmill and were able to evacuate much of Earth-D's population. The Justice League left, but 39 seconds later, Earth-D perished.\\r\\nRei made his only appearance in Legends of the DC Universe: Crisis on Infinite Earths (February 1999). The story was written by Marv Wolfman, with art by Paul Ryan (pencils) and Bob McLeod (ink).\\r\\nThe young, female Flash of the Tangent Universe is not a speedster, but instead \\"the first child born in space\\" and a being made up of and able to control light. As a side effect, she can move at the speed of light, which actually makes her faster than most of the other Post-Crisis Flashes, as only Wally West has ever survived a light-speed run without becoming trapped in the Speed Force.[20] She recently reappeared in Justice League of America #16, somehow summoned out of the paper 'green lantern' of her universean artifact that survived the Crisis that erased the Tangent Universe from existence.[21] Lia Nelson also appeared in Countdown: Arena battling two versions of the Flash from other Earths within the Multiverse.[22] In the 52-Earth Multiverse, the Tangent Universe is designated Earth-9.\\r\\nIn Superman & Batman: Generations 2, three different Flashes appear: Wally West as Kid Flash in 1964, Wally's cousin Carrie as Kid Flash in 1986, and Jay West, the son of Wally and his wife Magda as the fifth Flash in 2008. Barry Allen makes a cameo appearance out of costume in 1964.\\r\\nAli Rayner-West, aka Green Lightning, is a descendant of both Kyle Rayner and Wally West. She has both a power ring and superspeed, as seen in Green Lantern: Circle of Fire. She was a living construct created by Kyle Rayner's subconscious, who later re-fused into his mind.[23]\\r\\nA teenage version of Jesse Chambers appears as the Flash of the Ame-Comi universe. As with most of the other characters of that Earth, she sports an Anime-inspired costume.[24]\\r\\nThe 1980s series Captain Carrot and His Amazing Zoo Crew! presented the parallel Earth of \\"Earth-C-Minus\\", a world populated by funny animal superheroes that paralleled the mainstream DC Universe. Earth-C-Minus was the home of the Crash, a turtle with super-speed powers similar to those of Barry Allen's, and a member of his world's superhero team, the Just'a Lotta Animals. The Crash as a youth had read comics about Earth-C's Terrific Whatzit, similar to how Barry Allen enjoyed comics about Earth-Two's Jay Garrick.[25]\\r\\nAn African-American teenager named Danica Williams appears as the Flash in the Justice League Beyond series, acting as Wally West's successor during the 2040s (following the events of Batman Beyond). She is employed at the Flash Museum in Central City, and like Barry Allen, is chronically late.[26] She later enters into a relationship with Billy Batson, who is the secret identity of the superhero, Captain Marvel.\\r\\nThe following writers have been involved in the ongoing The Flash and Flash Comics series:\\r\\nThe comics and characters have been nominated for and won several awards over the years, including:\\r\\nThroughout his 70-year history, the Flash has appeared in numerous media. The Flash has been included in multiple animated features, such as Superfriends and Justice League, as well as his own live action television series and some guest star appearances on Smallville (as the Bart Allen/Impulse version.) There are numerous videos that feature the character.\\r\\nNumerous references to the Flash are presented on the television show The Big Bang Theory. A particular reference is main character Sheldon Cooper's Flash t-shirt, which has become a staple of merchandise clothing. In the season 1 episode The Middle-Earth Paradigm, the four main male characters on the show all independently dress up for a Halloween party as the Flash before deciding that they can't all be the Flash so no one gets to. In the season 10 episode The Birthday Synchronicity, Sheldon bought a Flash onesie for Howard & Bernadette's newborn.\\r\\nIn season 3 of Lost, in the episode \\"Catch-22\\", Charlie and Hurley debate over who would win a footrace between The Flash and Superman.\\r\\nThe false name Barry Allen is used by character of con artist Frank Abagnale, Jr.(posing as a Secret Service Agent), in the movie Catch Me If You Can. When a coffee shop waiter notices the notes of FBI agent Carl Hanratty, he reveals that Barry Allen is the Flash, giving Carl a vital clue to his unknown subject's identity.\\r\\nIn 2006, a near-pristine copy of Flash Comics #1 was sold in a Heritage Auction for $273,125. The same book was then sold privately for $450,000 in 2010.[29]\\r\\nRenan Kanbay wears a Flash costume while playing Carrie, the manager of a comic book store, in Joe Lipari's Dream Job (2011).[30]\\r\\nThe band Jim's Big Ego wrote the song \\"The Ballad of Barry Allen\\" detailing the hardship having to watch time moving so slowly from the perspective of Allen. The frontman of the band, Jim Infantino is the nephew of Flash artist Carmine Infantino.\\r\\nIn the film Daddy Day Care, one of the day care kids named Tony wore a Flash costume for the majority of the film.\\r\\nIn the My Little Pony: Friendship Is Magic episode \\"Power Ponies\\", Pinkie Pie becomes a superhero based on the Flash called Fili-Second.\\r\\nIn an episode of The Simpsons, Comic Book Guy dresses as The Flash while running in a marathon. He says \\"No one can outrun the Flash\\" but ends up falling in a pothole and gets stuck.\\r\\nLike Batman, Superman, and Green Lantern, the Flash has a reputation for having fought a distinctive and memorable rogues gallery of supervillains. In the Flash's case, some of these villains have adopted the term \\"Flash's Rogues Gallery\\" as an official title, and insist on being called \\"Rogues\\" rather than \\"supervillains\\" or similar names. At times, various combinations of the Rogues have banded together to commit crimes or take revenge on the Flash, usually under the leadership of Captain Cold.\\r\\nThe Rogues are known for their communal style relationship, socializing together and operating under a strict moral code, sometimes brutally enforced by Captain Cold. Such \\"rules\\" include \\"no drugs\\" and, except in very dire situations or on unique occasions, \\"no killing\\".","input":"Who came first barry allen or wally west?"},{"output":"enthusiasm, emotion, and an appeal to the super-natural","context":"The Second Great Awakening was a Protestant religious revival during the early 19th century in the United States. The movement began around 1790, gained momentum by 1800 and, after 1820, membership rose rapidly among Baptist and Methodist congregations whose preachers led the movement. It was past its peak by the late 1850s. The Second Great Awakening reflected Romanticism characterized by enthusiasm, emotion, and an appeal to the super-natural. It rejected the skeptical rationalism and deism of the Enlightenment.\\r\\nThe revivals enrolled millions of new members in existing evangelical denominations and led to the formation of new denominations. Many converts believed that the Awakening heralded a new millennial age. The Second Great Awakening stimulated the establishment of many reform movements designed to remedy the evils of society before the anticipated Second Coming of Jesus Christ.[1]\\r\\nHistorians named the Second Great Awakening in the context of the First Great Awakening of the 1730s and 1740s and of the Third Great Awakening of the late 1850s to early 1900s. These revivals were part of a much larger Romantic religious movement that was sweeping across Europe at the time, mainly throughout England, Scotland, and Germany.[2]\\r\\n\\r\\n\\r\\nLike the First Great Awakening a half century earlier, the Second reflected Romanticism characterized by enthusiasm, emotion, and an appeal to the super-natural.[3] It rejected the skepticism, deism, and rationalism left over from the Enlightenment.[4] At about the same time, similar movements flourished in Europe. Pietism was sweeping German countries.[5] Evangelicalism was waxing strong in England.[6]\\r\\nThe Second Great Awakening occurred in several episodes and over different denominations; however, the revivals were very similar.[4] As the most effective form of evangelizing during this period, revival meetings cut across geographical boundaries,[7] and the movement quickly spread throughout Kentucky, Tennessee and southern Ohio. Each denomination had assets that allowed it to thrive on the frontier. The Methodists had an efficient organization that depended on itinerant ministers, known as circuit riders, who sought out people in remote frontier locations. The circuit riders came from among the common people, which helped them establish rapport with the frontier families they hoped to convert.\\r\\nPostmillennialism theology dominated American Protestantism in the first half of the 19th century. Postmillennialists believed that Christ will return to earth after the \\"millennium\\", which could entail either a literal 1,000 years or a figurative \\"long period\\" of peace and happiness. Christians thus had a duty to purify society in preparation for that return. This duty extended beyond American borders to include Christian Restorationism. George Fredrickson argues that Postmillennial theology \\"was an impetus to the promotion of Progressive reforms, as historians have frequently pointed out.\\"[8] During the Second Great Awakening of the 1830s, some diviners expected the millennium to arrive in a few years. By the 1840s, however, the great day had receded to the distant future, and postmillennialism became a more passive religious dimension of the wider middle-class pursuit of reform and progress.[8]\\r\\nIn the early nineteenth century, western New York State was called the \\"burned-over district\\" because of the highly publicized revivals that crisscrossed the region.[9][10] Charles Finney, a leading revivalist active in the area, coined the term.[11] Linda K. Pritchard uses statistical data to show that compared to the rest of New York State, the Ohio River Valley in the lower Midwest, and the country as a whole, the religiosity of the Burned-over District was typical rather than exceptional.[12]\\r\\nOn the American Frontier, evangelical denominations sent missionary preachers and exhorters out to the people in the backcountry, which supported the growth of membership among Methodists and Baptists. Revivalists' techniques were based on the camp meeting, with its Scottish Presbyterian roots. Most of the Scots-Irish immigrants before the American Revolutionary War settled in the backcountry of Pennsylvania and down the spine of the Appalachian Mountains.[13]\\r\\nThese denominations were based on an interpretation of man's spiritual equality before God, which led them to recruit members and preachers from a wide range of classes and all races. Baptists and Methodist revivals were successful in some parts of the Tidewater in the South, where an increasing number of common planters, plain folk, and slaves were converted.\\r\\nIn the newly settled frontier regions, the revival was implemented through camp meetings. These often provided the first encounter for some settlers with organized religion, and they were important as social venues. The camp meeting was a religious service of several days' length with preachers. Settlers in thinly populated areas gathered at the camp meeting for fellowship as well as worship. The sheer exhilaration of participating in a religious revival with crowds of hundreds and perhaps thousands of people inspired the dancing, shouting, and singing associated with these events. The revivals followed an arc of great emotional power, with an emphasis of the individual's sins and need to turn to Christ, restored by a sense of personal salvation. Upon their return home, most converts joined or created small local churches, which grew rapidly.[14] The Second Great Awakening marked a religious transition in society in America. Many Americans from the Calvinist sect emphasized man's inability to save themselves and that their only way to be saved was from grace from God.[15]\\r\\nThe Revival of 1800 in Logan County, Kentucky, began as a traditional Presbyterian sacramental occasion. The first informal camp meeting began there in June, when people began camping on the grounds of the Red River Meeting House. Subsequent meetings followed at the nearby Gasper River and Muddy River congregations, all three under the ministry of James McGready. One year later, an even larger sacrament occasion was held at Cane Ridge, Kentucky under Barton Stone, attracting perhaps as many as 20,000 people. Numerous Presbyterian, Baptist and Methodist ministers participated in the services. Thanks to such leaders as Barton W. Stone (1772ÿ1844) and Alexander Campbell (1788ÿ1866), the camp meeting revival became a major mode of church expansion for the Methodists and Baptists.[16]\\r\\nThe Cumberland Presbyterian Church emerged in Kentucky. Cane Ridge was also instrumental in fostering what became known as the Restoration Movement. This was made up of non-denominational churches committed to what they saw as the original, fundamental Christianity of the New Testament. They were committed to individuals' achieving a personal relationship with Christ. Churches with roots in this movement include the Churches of Christ, Christian Church (Disciples of Christ), and the Evangelical Christian Church in Canada[17]\\r\\nThe Methodist circuit riders and local Baptist preachers made enormous gains; to a lesser extent the Presbyterians gained members, particularly with the Cumberland Presbyterian Church in sparsely settled areas. As a result, the numerical strength of the Baptists and Methodists rose relative to that of the denominations dominant in the colonial periodthe Anglicans, Presbyterians, Congregationalists. Among the new denominations that grew from the religious ferment of the Second Great Awakening are the Churches of Christ, Christian Church (Disciples of Christ), the Seventh-day Adventist Church, and the Evangelical Christian Church in Canada.[17][18] The converts during the Second Great Awakening were predominantly female. A 1932 source estimated at least three female converts to every two male converts between 1798 and 1826. Young people (those under 25) also converted in greater numbers, and were the first to convert.[19]\\r\\nThe Advent Movement emerged in the 1830s and 1840s in North America, and was preached by ministers such as William Miller, whose followers became known as Millerites. The name refers to belief in the soon Second Advent of Jesus (popularly known as the Second coming) and resulted in several major religious denominations, including Seventh-day Adventists and Advent Christians.[20]\\r\\nThough its roots are in the First Great Awakening and earlier, a re-emphasis on Wesleyan teachings on sanctification emerged during the Second Great Awakening, leading to a distinction between Mainline Methodism and Holiness churches.\\r\\nThe idea of restoring a \\"primitive\\" form of Christianity grew in popularity in the U.S. after the American Revolution.[21]:89ÿ94 This desire to restore a purer form of Christianity without an elaborate hierarchy contributed to the development of many groups during the Second Great Awakening, including the Mormons, Baptists and Shakers.[21]:89 Several factors made the restoration sentiment particularly appealing during this time period:[21]:90ÿ94\\r\\nThe Restoration Movement began during, and was greatly influenced by, the Second Great Awakening.[22]:368 While the leaders of one of the two primary groups making up this movement, Thomas Campbell and Alexander Campbell, resisted what they saw as the spiritual manipulation of the camp meetings, the revivals contributed to the development of the other major branch, led by Barton W. Stone.[22]:368 The Southern phase of the Awakening \\"was an important matrix of Barton Stone's reform movement\\" and shaped the evangelistic techniques used by both Stone and the Campbells.[22]:368\\r\\nEfforts to apply Christian teaching to the resolution of social problems presaged the Social Gospel of the late 19th century. Converts were taught that to achieve salvation they needed not just to repent personal sin but also work for the moral perfection of society, which meant eradicating sin in all its forms. Thus, evangelical converts were leading figures in a variety of 19th century reform movements.[23]\\r\\nCongregationalists set up missionary societies to evangelize the western territory of the northern tier. Members of these groups acted as apostles for the faith, and also as educators and exponents of northeastern urban culture. The Second Great Awakening served as an \\"organizing process\\" that created \\"a religious and educational infrastructure\\" across the western frontier that encompassed social networks, a religious journalism that provided mass communication, and church-related colleges.[22]:368 Publication and education societies promoted Christian education; most notable among them was the American Bible Society, founded in 1816. Women made up a large part of these voluntary societies.[24] The Female Missionary Society and the Maternal Association, both active in Utica, NY, were highly organized and financially sophisticated women's organizations responsible for many of the evangelical converts of the New York frontier.[25]\\r\\nThere were also societies that broadened their focus from traditional religious concerns to larger societal ones. These organizations were primarily sponsored by affluent women. They did not stem entirely from the Second Great Awakening, but the revivalist doctrine and the expectation that one's conversion would lead to personal action accelerated the role of women's social benevolence work.[26] Social activism influenced abolition groups and supporters of the Temperance movement. They began efforts to reform prisons and care for the handicapped and mentally ill. They believed in the perfectibility of people and were highly moralistic in their endeavors.\\r\\nBaptists and Methodists in the South preached to slaveholders and slaves alike. Conversions and congregations started with the First Great Awakening, resulting in Baptist and Methodist preachers being authorized among slaves and free African Americans more than a decade before 1800. \\"Black Harry\\" Hosier, an illiterate freedman who drove Francis Asbury on his circuits, proved to be able to memorize large passages of the Bible verbatim and became a cross-over success, as popular among white audiences as the black ones Asbury had originally intended for him to minister.[27] His sermon at Thomas Chapel in Chapeltown, Delaware, in 1784 was the first to be delivered by a black preacher directly to a white congregation.[28]\\r\\nDespite being called the \\"greatest orator in America\\" by Benjamin Rush[29] and one of the best in the world by Bishop Thomas Coke,[28] Hosier was repeatedly passed over for ordination and permitted no vote during his attendance at the Christmas Conference that formally established American Methodism. Richard Allen, the other black attendee, was ordained by the Methodists in 1799, but his congregation of free African Americans in Philadelphia left the church there because of its discrimination. They founded the African Methodist Episcopal Church (AME) in Philadelphia. After first submitting to oversight by the established Methodist bishops, several AME congregations finally left to form the first independent African-American denomination in the United States in 1816. Soon after, the African Methodist Episcopal Zion Church (AME Zion) was founded as another denomination in New York City.\\r\\nEarly Baptist congregations were formed by slaves and free African Americans in South Carolina and Virginia. Especially in the Baptist Church, African Americans were welcomed as members and as preachers. By the early 19th century, independent African American congregations numbered in the several hundred in some cities of the South, such as Charleston, South Carolina, and Richmond and Petersburg, Virginia.[30] With the growth in congregations and churches, Baptist associations formed in Virginia, for instance, as well as Kentucky and other states.\\r\\nThe revival also inspired slaves to demand freedom. In 1800, out of African American revival meetings in Virginia, a plan for slave rebellion was devised by Gabriel Prosser, although the rebellion was discovered and crushed before it started.[31] Despite white attempts to control independent African American congregations, especially after the Nat Turner Uprising of 1831, a number of African American congregations managed to maintain their separation as independent congregations in Baptist associations. State legislatures passed laws requiring them always to have a white man present at their worship meetings.[30]\\r\\nWomen, who made up the majority of converts during the Awakening, played a crucial role in its development and focus. It is not clear why women converted in larger numbers than men. Various scholarly theories attribute the discrepancy to a reaction to the perceived sinfulness of youthful frivolity, an inherent greater sense of religiosity in women, a communal reaction to economic insecurity, or an assertion of the self in the face of patriarchal rule. Husbands, especially in the South, sometimes disapproved of their wives' conversion, forcing women to choose between submission to God or their spouses. Church membership and religious activity gave women peer support and place for meaningful activity outside the home, providing many women with communal identity and shared experiences.[32]\\r\\nDespite the predominance of women in the movement, they were not formally indoctrinated or given leading ministerial positions. However, women took other public roles; for example, relaying testimonials about their conversion experience, or assisting sinners (both male and female) through the conversion process. Leaders such as Charles Finney saw women's public prayer as a crucial aspect in preparing a community for revival and improving their efficacy in conversion.[33] Women also took crucial roles in the conversion and religious upbringing of children. During the period of revival, mothers were seen as the moral and spiritual foundation of the family, and were thus tasked with instructing children in matters of religion and ethics.[34]\\r\\nThe greatest change in women's roles stemmed from participation in newly formalized missionary and reform societies. Women's prayer groups were an early and socially acceptable form of women's organization. Through their positions in these organizations, women gained influence outside of the private sphere.[35][36]\\r\\nChanging demographics of gender also affected religious doctrine. In an effort to give sermons that would resonate with the congregation, ministers stressed Christ's humility and forgiveness, in what the historian Barbara Welter calls a \\"feminization\\" of Christianity.[37]\\r\\nRevivals and perfectionist hopes of improving individuals and society continued to increase from 1840 to 1865 across all major denominations, especially in urban areas. Evangelists often directly addressed issues such as slavery, greed, and poverty, laying the groundwork for later reform movements.[1] The influence of the Awakening continued in the form of more secular movements.[38] In the midst of shifts in theology and church polity, American Christians began progressive movements to reform society during this period. Known commonly as antebellum reform, this phenomenon included reforms in against the consumption of alcohol, for women's rights and abolition of slavery, and a multitude of other issues faced by society.[39]\\r\\nThe religious enthusiasm of the Second Great Awakening was echoed by the new political enthusiasm of the Second Party System.[40] More active participation in politics by more segments of the population brought religious and moral issues into the political sphere. The spirit of evangelical humanitarian reforms was carried on in the antebellum Whig party.[41]\\r\\nHistorians stress the understanding common among participants of reform as being a part of God's plan. As a result, local churches saw their roles in society in purifying the world through the individuals to whom they could bring salvation, and through changes in the law and the creation of institutions. Interest in transforming the world was applied to mainstream political action, as temperance activists, antislavery advocates, and proponents of other variations of reform sought to implement their beliefs into national politics. While Protestant religion had previously played an important role on the American political scene, the Second Great Awakening strengthened the role it would play.[1]","input":"What characterized the second period of christian expansion?"},{"output":"Le Griffon","context":"Le Griffon (French pronunciation:??[l? ?if??], The Griffin) was a 17th-century barque built by Ren-Robert Cavelier, Sieur de La Salle in his quest to find the Northwest Passage to China and Japan.\\r\\nLe Griffon was constructed and launched at or near Cayuga Creek on the Niagara River as a seven-cannon, 45-ton barque. La Salle and Father Louis Hennepin set out on Le Griffon's' maiden voyage on August 7, 1679 with a crew of 32, sailing across Lake Erie, Lake Huron and Lake Michigan through uncharted waters that only canoes had previously explored. The ship landed at a location on an island in Lake Michigan where the local tribes had gathered with animal pelts to trade with the French. La Salle disembarked and on September 18 sent the ship back toward Niagara. On its return trip from the island, said to be located in the mouth of the body of water which is now known as Green Bay (Lake Michigan), it vanished with all six crew members and its load of furs.\\r\\nIn late December 2014, treasure hunting divers Kevin Dykstra and Frederick Monroe alerted media outlets that they found indisputable proof of Le Griffon's location. They happened upon the wreckage while searching the floor of Lake Michigan for Confederate gold. Evidently, they spotted the wreck in 2011, but waited until 2014 to reveal the discovery of what some call the \\"holy grail\\" of Great Lakes shipwrecks while they consulted experts. There are \\"no cables, no cabin, and no smokestacks,\\" no mechanical devices of any kind, and a carving on the front of the ship strongly resembles 17th-century French carvings of griffins, Dykstra says. On January 2, 2015, Frederick Monroe told an interviewer for public radio that he believed Le Griffon was built in Canada, below Niagara Falls, and brought to the upper Great Lakes. He went on to say that the wreck is \\"the one that got in the way.\\" Their claim was quickly debunked when Michigan authorities dove down on June 9, 2015 after receiving the coordinates to verify its authenticity. Michigan state maritime archaeologist Wayne R. Lusardi presented evidence that the wreck was, in fact, a tugboat due to its 90-foot (27?m) length and presence of a steam boiler.[3]\\r\\nPrior to this, wreckage from Le Griffon was thought to have possibly been located near Fairport, Michigan by US wreck diver Steve Libert in 2004. Since then, ownership of the potential remains has been the subject of lawsuits involving the discoverers, the state of Michigan, the U.S. federal government and the government of France.[4] Some scientists concluded it was a bowsprit detached from a ship dating hundreds of years old,[5] while others believe it is a 19th-century pound net (fishing) stake.[6]\\r\\n\\r\\n\\r\\nLe Griffon was the first full-sized sailing ship on the upper Great Lakes of North America and she led the way to modern commercial shipping in that part of the world. Historian J. B. Mansfield reported that this \\"excited the deepest emotions of the Indian tribes, then occupying the shores of these inland waters\\".[1]\\r\\nFrench explorer Ren Robert Cavelier, Sieur de La Salle, sought a Northwest Passage to China and Japan to extend France's trade. Creating a fur trade monopoly with the Native Americans would finance his quest and building Le Griffon was an \\"essential link in the scheme\\".[7] While work continued on Le Griffon in the spring of 1679 as soon as the ice began to break up along the shores of Lake Erie, La Salle sent out men from Fort Frontenac in 15 canoes laden with supplies and merchandise to trade with the Illinois for furs at the trading posts of the upper Huron and Michigan Lakes.[7]\\r\\nLe Griffon may or may not be considered the first ship on the Great Lakes, depending on what factors one deems necessary to qualify a vessel for that designation. Decking, permanent masts, and bearing a name are a few of the criteria one might use.[notes 1]\\r\\nBefore 1673, the most common vessel on the lakes was the canoe. While smaller canoes were used on rivers and streams, lake canoes were more commonly larger vessels measuring up to about 35 feet (11?m) long. While some of these were made from a single carved log (\\"dug out\\" or \\"pirogue\\"), most were bark canoes. Bateaux were also common. They were open vessels (no deck) made of wood measuring up to about 35 feet (11?m) long and capable of carrying three or four tons of cargo. While they were at times fitted with mast and sails, their primary propulsion was either oars or poles. The sails were merely supplemental for traveling down wind. Their inefficiency at beating to windward made them impractical as sailing vessels, and they were not very safe in open water.\\r\\nJames Mansfield[1] says that in the fall of 1678, La Salle built a vessel of about 10 tons burden at Fort Frontenac and that this vessel, named Frontenac, was the first real sailing vessel on the Great Lakes; specifically, on Lake Ontario (which some at the time called Lac de Frontenac). Many authors since Mansfield have followed suit. There is reason, however, to question his assertion.\\r\\nJustin Windsor notes that Count Frontenac by August 1, 1673, \\"had already ordered the construction of a vessel on Ontario to be used as an auxiliary force to Fort Frontenac.\\"[8] He also says that at Fort Frontenac in 1676, La Salle \\"laid the keels of the vessels which he depended on to frighten the English.\\"[8] J. C. Mills [7] quotes a letter from La Salle to the Minister of Marine that says, \\"The fort at Cataraqui (Fort Frontenac) with the aid of a vessel now building, will command Lake Ontario...\\"[7] While no date is given for the letter, the location of Mill's reference to it suggests that it was sent before 1677, perhaps as early as 1675. Francis Parkman says that by 1677, \\"four vessels of 25 to 40 tons had been built for the lake Ontario and the river St. Lawrence.\\"[9] H. W. Beckwith says that in September, 1678, La Salle \\"already had three small vessels on Lake Ontario, which he had made use of in a coasting trade with the Indians.\\"[10] None of these sources ascribe a name to any of these vessels. While the journals of Tonti, Hennepin, and LeClercq (participants with La Salle) do mention a little vessel of 10 tons, none of them apply a name to it.\\r\\nLa Salle's prime focus in 1678 was building Le Griffon. Arriving at Fort Frontenac in late September, he had neither the time for nor the interest in building a vessel at Fort Frontenac to transport building materials, some of which he had recently obtained in France, to a site above Niagara Falls where he could build his new ship. Beckwith's conclusion was that he chose one of his existing vessels, one of about ten tons burden, for sending the first group of men to Niagara.[9] Some of La Salle's associates called this vessel a brigantine; others called it a bark. The accounts agree that this little vessel played a part in the building of Le Griffon.\\r\\nOn November 18, 1678, after just over a month of preparations at Fort Frontenac, La Salle dispatched Captain La Motte and Father Louis Hennepin together with 15 men and supplies in a vessel of 10 tons. Their mission was to begin selecting a site for the construction of Le Griffon and to erect necessary structures for shelter, storage, and defense. Because the wind was strong from the north, they sailed close to the north shore of the lake, putting in for the nights in various bays along the way. Somewhere near present-day Toronto they were frozen in and had to chop their way out of the ice. From there they struck out across the lake toward the mouth of the Niagara River. They arrived late on December 5, but the weather was rough and they did not want to run the surf and outflow of the river at night, so they stayed a few miles off shore. On December 6, they landed safely on the east bank of the river at about where Lewiston, New York is today. They attempted to sail further upstream, but the current was too strong. Ice flowing down the river threatened to damage their little brigantine and after a cable was broken, they hauled the vessel ashore and into a small ravine for protection.[11]\\r\\nRelations with the Iroquois were uneasy, so beginning on Christmas Day, 1678, La Motte and Hennepin together with four of their men, went by snowshoe to a prominent Seneca chief who resided at Tagarondies [notes 2] a village about 75 miles (121?km) east of Niagara[notes 3] and about 20 miles (32?km) south of Lake Ontario.[12][page?needed] They wished to secure a reliable truce lest the natives interfere with their projects. Those left behind proceeded with needed building projects. Negotiations with the Senecas were only moderately successful, so when they left the village they still wondered if the natives would permit them to finish their project. They reached Niagara again on January 14.[11]\\r\\nMeanwhile, La Salle and Henri de Tonti, had departed Fort Frontenac in a second vessel some days after La Mothe and Hennepin. This was a \\"great bark\\" (Hennepin's words) of about 20 tons burden[11] ÿ although Tonti's journal says this was a 40-ton vessel.[13] The vessel carried anchors, chain, guns, cordage, and cable for Le Griffon, as well as supplies and provisions for the anticipated journey. La Salle followed the southern shore of the lake. La Salle decided to visit the Senecas at Tagarondies himself. He put ashore near present-day Rochester, New York, and arrived at Tagarondies very shortly after La Motte and Hennepin had left. He was more successful in securing the Indians' tolerance of his proposed \\"big canoe\\" and support buildings. With La Salle back aboard their vessel, the company again sailed west until, about 25 miles (40?km) from Niagara, weather checked their progress.[notes 4] There was some disagreement between La Salle and the ship's pilot, and La Salle and Tonti went ahead on foot to Niagara. When they arrived there La Motte and Hennepin had not yet returned. While there La Salle selected a site for building Le Griffon.\\r\\nAfter La Salle and Tonti left, the pilot and the rest of the crew were to follow with the supply vessel. On January 8, 1679, the pilot and crew decided to spend the night ashore where they could light a fire and sleep in some warmth. It was a calm night and they believed the vessel was securely moored. When a strong wind suddenly arose, they could not make it back to the ship. The vessel dragged its anchor for about nine miles to the east before grounding and breaking up near present-day Thirty Mile Point.[notes 5] When La Salle heard of the loss (through a messenger or one of the natives), he left Niagara and joined in the salvage effort. They recovered the anchors, chain, and most of the materials critical for Le Griffon, but most of the supplies and provisions were lost. They dragged the materials to the mouth of the Niagara, rested and warmed up a few days in an Indian village, then carried the materials single file through the snow to their settlement above the falls. They arrived there on January 20, and were welcomed by La Motte and Hennepin.\\r\\nThe site La Salle had selected for building Le Griffon has conclusively been identified as at or near the mouth of Cayuga Creek.[9][11][14] There the keel was laid on January 26, 1679. La Salle offered Hennepin the honor of driving the first spike, but Hennepin deferred to his leader. Having lost needed supplies, La Salle left the building of Le Griffon under Tonti's care, and set out on foot to return to Fort Frontenac. While frozen rivers made traveling easy, finding food was not. He arrived there nearly starved only to find that his detractors had succeeded in stirring up doubt and opposition with his creditors. Addressing his problems long delayed his return to the expedition.[notes 6]\\r\\nAfter La Salle's departure, Tonti refloated the little brigantine, and attempted to use it for more salvage work at the wreck, but the winter weather prevented success. He then charged La Motte with salvage by use of canoes. Some time later, Hennepin would use this little vessel to sail to Fort Frontenac and again back to Niagara.[11]\\r\\nProgress on Le Griffon was fraught with problems. Suffering from cold and low on supplies, the men were close to mutiny. The uneasy truce with the Indians was tested by threats and attempts of sabotage and murder. Tonti learned of a plan to burn the ship before it could be launched, so he launched ahead of schedule and Le Griffon entered the waters in early May, 1679. Hennepin's first account says she was a vessel of about 45 tons; his second says 60 tons. Because his second account has numerous exaggerations and cases where he credits himself for things that La Salle had done, Hennepin's first account is considered more reliable. In any case, Le Griffon was larger than any other vessel on the lakes at the time, and as far as contemporary reports can confirm, the first named vessel. Her fitting out continued the next couple of months and when La Salle arrived in July he was encouraged by her readiness.\\r\\nLa Salle had Le Griffon built in the winter of 1678ÿ79 at a distance of several hundred miles from any settlements on Cayuga Creek (at Cayuga Island) on the Niagara River. Le Griffon's pattern closely followed the prevailing type used by explorers to cross the Atlantic Ocean to the New World.[1][7] The exact size and construction of Le Griffon is not known. The widely referenced antique woodcutting of Le Griffon shows her with 2 masts but many researchers believe she was a 45-ton barque with a single mast with several square sails and 30 to 40 feet (9.1 to 12.2?m) long with a 10-to-15-foot (3.0 to 4.6?m) beam.[1][2]\\r\\nLa Salle's men first had to build their lodging and then guard against the Iroquois who were hostile to this invasion of their ancient homeland.[7] La Salle had instructed Hennepin and La Mothe to go 75 miles into wilderness in knee-deep snow on an embassy to the great village of the Seneca tribe, bringing gifts and promises in order to obtain their good will to build \\"the big canoe\\" (Le Griffon), but many tribal members did not approve.[1][7] La Salle arrived on 20 January 1679 from Fort Frontenac with the full rigging, anchors, chains, cordage, and cannon that were transported by barge, then salvaged and dragged 30 miles overland to the construction site. La Salle oversaw the laying of Le Griffon's keel and drove her first bolt. Crude tools, green and wet timbers, and the cold winter months caused slow progress in the construction of Le Griffon.[7] La Salle left Italian officer Henri de Tonti and Father Hennepin in charge while he journeyed to Fort Frontenac to secure replacements for lost supplies.[1][7]\\r\\nA female Native informant who was of the tribe foiled the plans of hostile Senecas to burn Le Griffon as she grew on her stocks. The unrest of the Seneca and dissatisfied workmen were continually incited by secret agents of merchants and traders who feared La Salle would break their monopoly on the fur trade.[7] When the Seneca again threatened to burn the ship, she was launched earlier than planned in Cayuga Creek channel of the upper Niagara River with ceremony and the roar of her cannons. A party from the Iroquois tribe who witnessed the launching were so impressed by the \\"large floating fort\\" that they named the French builders Ot-kon, meaning \\"penetrating minds\\", which corresponds to the Seneca word Ot-goh, meaning supernatural beings or spirits.[1] The tumultuous sound of Le Griffon's cannons so amazed the Native Americans that the Frenchmen were able to sleep at ease for the first time in months when they anchored off shore. After Le Griffon was launched, she was rigged with sails and provisioned with seven cannon of which two were brass.[1] The French flag flew above the cabin placed on top of the main deck that was elevated above the hull.[7] She had the figure of a griffin mounted on her jib-boom and an eagle flying above.[1][7] Some say Le Griffon was named for Count Frontenac whose coat of arms was ornamented with the mythical griffin. Hennepin said she was named to protect her from the fire that threatened her.[1]\\r\\nIn July 1679, La Salle directed 12 men to tow Le Griffon through the rapids of the Niagara River with long lines stretched from the bank. They moored in quiet water off Squaw Island three miles from Lake Erie waiting for favorable northeast winds. La Salle sent Tonti ahead on 22 July 1679 with a few selected men, canoes, and trading goods to secure furs and supplies. Le Griffon set off on 7 August with unfurled sails, a 34-man crew, and a salute from her cannon and musketry.[7] They were navigating Le Griffon through uncharted waters that only canoes had previously explored. They made their way around Long Point, Ontario, constantly sounding as they went through the first moonless, fog-laden night to the sound of breaking waves and guided only by La Salle's knowledge of Galine's crude, 10-year-old chart. They sailed across the open water of Lake Erie whose shores were forested and \\"unbroken by the faintest signs of civilization\\".[1] They reached the mouth of the Detroit River on 10 August 1679 where they were greeted by three columns of smoke signaling the location of Tonti's camp whom they received on board.[7] They entered Lake St. Clair on 12 August, the feast day of Saint Clare of Assisi, and named the lake after her. They again sounded their way through the narrow channel of the St. Clair River to its mouth where they were delayed by contrary winds until 24 August. For the second time, they used a dozen men and ropes to tow Le Griffon over the rapids of the St. Clair River into lower Lake Huron. They made their way north and west to Saginaw Bay on Lake Huron where they were becalmed until noon of 25 August. La Salle took personal command at this point due to evidence that the pilot was negligent.[1][7]\\r\\nOn noon of 25 August they started out northwest with a favoring northerly wind. When the wind suddenly veered to the southeast they changed course to avoid Presque Isle. However, the ferocity of the gale forced them to retreat windward and lie-to until morning. By 26 August the violence of the gale caused them to \\"haul down their topmasts, to lash their yards to the deck, and drift at the mercy of storm. At noon the waves ran so high, and the lake became so rough, as to compel them to stand in for land.\\"[1] Father Hennepin wrote that during the fearful crisis of the storm, La Salle vowed that if God would deliver them, the first chapel erected in Louisiana would be dedicated to the memory of Saint Anthony of Padua, the patron of the sailor. The wind did slightly decrease but they drifted slowly all night, unable to find anchorage or shelter. They were driven northwesterly until the evening of 27 August when under a light southerly breeze they finally rounded Point St. Ignace and anchored in the calm waters of the natural harbor at Mackinac Island where there was a settlement of Hurons, Ottawas, and a few Frenchmen.[1]\\r\\nUpon Le Griffon's safe arrival at Mackinac Island, the voyagers fired a salute from her deck that the Hurons on shore volleyed three times with their firearms. More than 100 Native American bark canoes gathered around Le Griffon to look at the \\"big wood canoe\\".[7] La Salle dressed in a scarlet cloak bordered with lace and a highly plumed cap, laid aside his arms in charge of a sentinel and attended mass with his crew in the chapel of the Ottawas and then made a visit of ceremony with the chiefs.[1][7]\\r\\nLa Salle found some of the 15 men he sent ahead from Fort Frontenac to trade with the Illinois but they had listened to La Salle's enemies who said he would never reach Mackinac Island. La Salle seized two of the deserters and sent Tonti with six men to arrest two more at Sault Ste. Marie.[1][7]\\r\\nThe short open-water season of the upper Great Lakes compelled La Salle to depart for Green Bay on 12 September, five days before Tonti's return. They sailed from the Straits of Mackinac to an island (either Washington Island or Rock Island)[1] located at the entrance of Green Bay (Lake Michigan). They anchored on the south shore of the island and found it occupied by friendly Pottawatomies and 15 of the fur traders La Salle sent ahead. The traders had collected 12,000 pounds (5,400?kg) of furs in anticipation of the arrival of the Le Griffon. La Salle decided to stay behind with four canoes to explore the head of Lake Michigan. La Salle gave instructions for Le Griffon to off-load merchandise for him at Mackinac Island that would be picked up on the return trip. Le Griffon rode out a violent storm for four days and then on 18 September, the pilot Luc and five crew sailed under a favorable wind for the Niagara River with a parting salute from a single gun. She carried a cargo of furs valued at from 50,000 to 60,000 francs ($10,000 ÿ $12,000) and the rigging and anchors for another vessel that La Salle intended to build to find passage to the West Indies. La Salle never saw Le Griffon again.[1][7]\\r\\nFather Hennepin wrote that Le Griffon was lost in a violent storm.[7] Some charged fur traders, and even Jesuits with her destruction. Some said that the Ottawas or Pottawatomies boarded her, murdered her crew, and then burned her. La Salle was convinced that the pilot and crew treacherously sank her and made off with the goods. There is no conclusive evidence about any of the theories about Le Griffon's loss.[1]\\r\\nLe Griffon is reported to be the \\"Holy Grail\\" of Great Lakes shipwreck hunters.[15] A number of sunken old sailing ships have been suggested to be Le Griffon but, except for the ones proven to be other ships, there has been no positive identification. One candidate is a wreck at the western end of Manitoulin Island in Lake Huron, with another wreck near Escanaba, Michigan, also proposed.\\r\\nLe Griffon is considered by some to have been the first ship lost on the Great Lakes. It was another vessel used by La Salle and Tonti, however, that was the first loss on January 8, 1679. As noted above, sources give its size as either 20 tons or 40 tons. It dragged anchor and ran aground near Thirty Mile Point on Lake Ontario, where it broke apart. Some say that this vessel was named the Frontenac, while others say the other vessel used on La Salle's expedition was Frontenac. Some sources confuse the two vessels.\\r\\nLe Griffon may have been found by the Great Lakes Exploration Group but the potential remains were the subject of lawsuits involving the discoverers, the state of Michigan, the U.S. federal government, and the Government of France.[4] Originally discovered in 2001 near Poverty Island, Michigan sonar has shown an object approximately 40 by 18 feet (12.2 by 5.5?m) (similar to the dimensions of Le Griffon) located under several feet of sediment. In July, 2010 the Great Lakes Exploration Group issued a press release stating that they, the state of Michigan and France had reached agreement to co-operate in the next phase of an archaeological site assessment for identifying the shipwreck.[16] After years of legal squabbles the Michigan Department of Natural Resources issued a permit, and on June 16, 2013, an underwater pit was dug allowing US and French archeologists to examine the object for the first time. They discovered a 15-inch slab of blackened wood that might have been a human-fashioned cultural artifact.[17] On June 19, 2013, teams of scientists determined the wood pole discovered was not attached to a ship, after it came loose and was placed on the lake bed during an excavation. They concluded it was likely a bowsprit dating from a ship hundreds of years old, although some think it was a common pound net stake used for fishing nets in the 19th century. At the time, no other wreckage was found, but scientists noted other wreckage may not be far away.[5][6]\\r\\nOn June 23, 2014, Steve Libert told the Associated Press he believed he found Le Griffon in Lake Michigan after extensive searching, in a debris field near where a wood slab was found the previous year.[18]\\r\\nOn December 27, 2014, two divers, Kevin Dykstra and Frederick Monroe, announced the discovery of a wreck that they believe is Le Griffon, based on the bowstem, which to some resembles an ornamental griffin.[19][unreliable source?][20] As noted above and with reference #3, this \\"discovery\\" has proven false.\\r\\nThere has yet to be any consensus regarding the location of the shipwreck of Le Griffon. A 2015 book The Wreck of the Griffon by Cris Kohl and Joan Forsberg argues that the best \\"discovery\\" proposed to date remains the 1898 find by Albert Cullis, lighthouse keeper on the western edge of Manitoulin Island in northern Lake Huron. While they recognize that conclusive evidence has not been found, the evidence that has been found there fits with what is known of the history of that time and they postulate that if Le Griffon is found elsewhere, that would deepen the mystery of the find by Cullis.[21]\\r\\nCoordinates: 4536N 8239W? / ?45.60N 82.65W? / 45.60; -82.65","input":"What is the holy grail of great lakes shipwrecks?"},{"output":"The New Kingdom of Egypt","context":"The New Kingdom of Egypt, also referred to as the Egyptian Empire, is the period in ancient Egyptian history between the 16th century BC and the 11th century BC, covering the 18th, 19th, and 20th Dynasties of Egypt. Radiocarbon dating places the exact beginning of the New Kingdom between 1570 BC and 1544 BC.[1] The New Kingdom followed the Second Intermediate Period and was succeeded by the Third Intermediate Period. It was Egypt's most prosperous time and marked the peak of its power.[2]\\r\\nThe later part of this period, under the 19th and 20th Dynasties (1292ÿ1069 BC), is also known as the Ramesside period. It is named after the 11 Pharaohs that took the name Ramesses, after Ramesses I, the founder of the 19th Dynasty.[2]\\r\\nPossibly as a result of the foreign rule of the Hyksos during the Second Intermediate Period, the New Kingdom saw Egypt attempt to create a buffer between the Levant and Egypt proper, and during this time Egypt attained its greatest territorial extent. Similarly, in response to very successful 17th century attacks during the Second Intermediate Period by the powerful Kingdom of Kush,[3] the rulers of the New Kingdom felt compelled to expand far south into Nubia and to hold wide territories in the Near East. In the north, Egyptian armies fought Hittite armies for control of modern-day Syria.\\r\\n\\r\\n\\r\\nThe 18th Dynasty contained some of Egypt's most famous Pharaohs, including Ahmose I, Hatshepsut, Thutmose III, Amenhotep III, Akhenaten and Tutankhamun. Queen Hatshepsut concentrated on expanding Egypt's external trade by sending a commercial expedition to the land of Punt.\\r\\nThutmose III (\\"the Napoleon of Egypt\\") expanded Egypt's army and wielded it with great success to consolidate the empire created by his predecessors. This resulted in a peak in Egypt's power and wealth during the reign of Amenhotep III. During the reign of Thutmose III (c. 1479ÿ1425 BC), Pharaoh, originally referring to the king's palace, became a form of address for the person who was king.[4]\\r\\nOne of the best-known 18th Dynasty pharaohs is Amenhotep IV, who changed his name to Akhenaten in honor of the Aten, a representation of the Egyptian god, Ra. His exclusive worship of the Aten is often interpreted as history's first instance of monotheism. Akhenaten's wife, Nefertiti, contributed a great deal to his new take on the Egyptian religion. Nefertiti was bold enough to perform rituals to Aten. Akhenaten's religious fervor is cited as the reason why he and his wife were subsequently written out of Egyptian history.[5] Under his reign, in the 14th century BC, Egyptian art flourished under a distinctive style. (See Amarna Period.)\\r\\nTowards the end of the 18th Dynasty, the situation had changed radically. Aided by Akhenaten's apparent lack of interest in international affairs, the Hittites had gradually extended their influence into Phoenicia and Canaan to become a major power in international politics  a power that both Seti I and his son Ramesses II would need to deal with during the 19th Dynasty.\\r\\nThe Nineteenth Dynasty was founded by the Vizier Ramesses I, whom the last ruler of the 18th dynasty, Pharaoh Horemheb, had chosen as his successor. His brief reign marked a transition period between the reign of Horemheb the powerful pharaohs of this dynasty, in particular, his son Seti I and grandson Ramesses II, who would bring Egypt up to new heights of imperial power.\\r\\nRamesses II (\\"the Great\\") sought to recover territories in the Levant that had been held by the 18th?Dynasty. His campaigns of reconquest culminated in the Battle of Kadesh, where he led Egyptian armies against those of the Hittite king Muwatalli II. Ramesses was caught in history's first recorded military ambush, although he was able to rally his troops and turn the tide of battle against the Hittites thanks to the arrival of the Ne'arin (possibly mercenaries in the employ of Egypt). The outcome of the battle was undecided, with both sides claiming victory at their home front, and ultimately resulting in a peace treaty between the two nations. Egypt was able to obtain wealth and stability under Ramesses' rule of over half a century.[6] His immediate successors continued the military campaigns, although an increasingly troubled courtwhich at one point put a usurper (Amenmesse) on the thronemade it increasingly difficult for a pharaoh to effectively retain control of the territories.\\r\\nRamesses II was also famed for the huge number of children he sired by his various wives and concubines; the tomb he built for his sons, many of whom he outlived, in the Valley of the Kings has proven to be the largest funerary complex in Egypt.\\r\\nEgyptian and Hittite Empires, around the time of the Battle of Kadesh.\\r\\nThe last \\"great\\" pharaoh from the New Kingdom is widely considered to be Ramesses III, a 20th Dynasty pharaoh who reigned several decades after Ramesses II.[7]\\r\\nIn the eighth year of his reign the Sea Peoples invaded Egypt by land and sea. Ramesses III defeated them in two great land and sea battles (the Battle of Djahy and the Battle of the Delta). He incorporated them as subject peoples and settled them in Southern Canaan although there is evidence that they forced their way into Canaan. Their presence in Canaan may have contributed to the formation of new states, such as Philistia, in this region after the collapse of the Egyptian Empire. He was also compelled to fight invading Libyan tribesmen in two major campaigns in Egypt's Western Delta in his sixth year and eleventh year respectively.[8]\\r\\nThe heavy cost of this warfare slowly drained Egypt's treasury and contributed to the gradual decline of the Egyptian Empire in Asia. The severity of the difficulties is indicated by the fact that the first known labor strike in recorded history occurred during the 29th year of Ramesses III's reign, when the food rations for Egypt's favored and elite royal tomb-builders and artisans in the village of Deir el Medina could not be provisioned.[9] Air pollutants prevented much sunlight from reaching the ground and also arrested global tree growth for almost two full decades until 1140 BC.[10] One proposed cause is the Hekla 3 eruption of the Hekla volcano in Iceland but the dating of this remains disputed.\\r\\nRameses III's death was followed by years of bickering among his heirs. Three of his sons ascended the throne successively as Ramesses IV, Rameses VI and Rameses VIII. Egypt was increasingly beset by droughts, below-normal flooding of the Nile, famine, civil unrest and official corruption. The power of the last pharaoh of the dynasty, Ramesses XI, grew so weak that in the south the High Priests of Amun at Thebes became the de facto rulers of Upper Egypt, and Smendes controlled Lower Egypt even before Rameses XI's death. Smendes eventually founded the 21st Dynasty at Tanis.\\r\\nRelief of a Nobleman, c. 1295ÿ1070 B.C.E. Brooklyn Museum\\r\\nQueen Ahmose-Nefertari\\r\\nHatshepsut as a Sphinx. Daughter of Thutmose I, she ruled jointly as her stepson's (Thutmose III) co-regent. She soon took the throne for herself, and declared herself pharaoh.\\r\\nQueen Hatshepsut's Temple at Deir el-Bahari, was called Djeser-Djeseru, meaning the Holy of Holies, in ancient times.\\r\\nThutmosis III, a military man and member of the Thutmosid royal line is commonly called the Napoleon of Egypt. His conquests of the Levant brought Egypt's territories and influence to its greatest extent.\\r\\nColossi of Memnon. Representing Amenhotep III, this statue sits outside Luxor.\\r\\nTiye, born a commoner, became queen through her marriage to Amenhotep III. In the New Kingdom, women gained influence in court, and Tiye soon helped run affairs of state for both her husband and son during their reigns.\\r\\nAkhenaten, born Amenhotep IV, was the son of Queen Tiye. He rejected the old Egyptian religion and went about promoting the Aten as a supreme deity.\\r\\nAkhenaten\\r\\nBust of Nefertiti. The wife of Akhenaten, she held position as co-regent with Akhenaten. She may also have ruled as pharaoh in her own right as she is one of few candidates for the identity of Pharaoh Neferneferuaten.\\r\\nTutankhamun's mask. King Tutankhamun, son of Akhenaten, restored Egypt to its former religion. Though he died young and was not considered significant in his own time, the 1922 discovery of his KV62 intact tomb by Howard Carter, made him relevant as a symbol of ancient Egypt in the modern world.\\r\\nDetail Temple of Rameses II\\r\\nNefertari's Temple at Abu Simbel\\r\\nGiant Ramses II\\r\\nAbu Simbel Temple of Ramesses II\\r\\nAbu Simbel\\r\\nKing Tutanhkamun Guardian Statue","input":"Which of egypts kingdoms was the most powerful?"},{"output":"1951ÿ52","context":"Jawaharlal Nehru\\r\\nINC\\r\\nJawaharlal Nehru\\r\\nINC\\r\\nThe Indian general election of 1951ÿ52 elected the first Lok Sabha since India became independent in August 1947.[1][2][3] Until this point, the Indian Constituent Assembly had served as an interim legislature. See the 'Durations' section below to find the time-range associated with these elections.\\r\\nThe Indian National Congress (INC) won a landslide victory, winning 364 of the 489 seats and 45% of the total votes polled. This was over four times as many votes as the second-largest party. Jawaharlal Nehru became the first democratically elected Prime Minister of the country. In the first Lok Sabha polls held in 1951, India had around 173 million voters, out of an overall population of about 360 million.[4] Voter turnout was 45.7%.[5]\\r\\n\\r\\n\\r\\nBefore Independent India went to the polls, two former cabinet colleagues of Nehru established separate political parties to challenge the INC's supremacy. While Shyama Prasad Mookerjee went on to found the Jana Sangh in October 1951, Dalit leader Dr. B. R. Ambedkar revived the Scheduled Castes Federation (which was later named the Republican Party). Other parties which started coming to the forefront included the Kisan Mazdoor Praja Parishad, whose prime mover was Acharya Kripalani; the Socialist Party, which had Ram Manohar Lohia and Jayaprakash Narayan's leadership to boast of; and the Communist Party of India. However, these smaller parties were unable to make an electoral stand against the Indian National Congress.\\r\\nThe first general elections, which were conducted for 489 seats in 401 constituencies, represented 25 Indian states. At that time, there were 314 constituencies with one seat, 86 with two seats and one with three seats.[6] The multi-seat constituencies were abolished in the 1960s. There were also 2 nominated Anglo-Indian members.\\r\\nScheduled Caste leader B. R. Ambedkar was defeated in the Bombay (North Central) (reserved seat) constituency as Scheduled Castes Federation candidate by his little-known former assistant and Congress Candidate Narayan Sadoba Kajrolkar, who polled 1,38,137 votes compared to Ambedkar's 1,23,576 votes.[7]:156 Dr Ambedkar then entered the parliament as a Rajya Sabha member. He contested by-poll from Bhandara in 1954 to try to enter Lok Sabha but again lost to Mr Borkar of Congress.\\r\\nAcharya Kripalani lost from Faizabad in UP as KMPP candidate, but his wife Sucheta Kripalani defeated the Congress candidate Manmohini Sahgal in Delhi.[8]\\r\\nThe Speaker of the first Lok Sabha was Ganesh Vasudev Mavalankar. The first Lok Sabha also witnessed 677 sittings (3,784 hours), the highest recorded count of the number of sittings. The Lok Sabha lasted its full term from 17 April 1952 until 4 April 1957.\\r\\nWhile Indian Government's official websites and official documents assign the year 1951 to these polls, it is a misrepresentation because all territories except Himachal Pradesh and Jammu & Kashmir voted in FebruaryÿMarch 1952; no polls were held for Lok Sabha seats in Kashmir until 1967, and only Himachal Pradesh voted in 1951 for the first Lok Sabha because weather tends to be inclement in February and March, heavy snow impending free movement.[9] The rest of the India voted only in FebruaryÿMarch 1952 for the Lok Sabha and Vidhan Sabha elections. Polling was held between 25 October 1951 and 27 March 1952. The very first votes of the election were cast in the tehsil (district) of Chini in Himachal Pradesh.[7]","input":"When did the first election held in india?"},{"output":"Olympia","context":"","input":"9. what is the capital of washington state?"},{"output":"519,682 elected officials in the United States as of 2012","context":" \\r\\n\\r\\n\\r\\n\\r\\nElections in the United States are held for government officials at the federal, state, and local levels. At the federal level, the nation's head of state, the President, is elected indirectly by the people of each state, through an Electoral College. Today, these electors almost always vote with the popular vote of their state. All members of the federal legislature, the Congress, are directly elected by the people of each state. There are many elected offices at state level, each state having at least an elective Governor and legislature. There are also elected offices at the local level, in counties, cities, towns, townships, boroughs, and villages. According to a study by political scientist Jennifer Lawless, there were 519,682 elected officials in the United States as of 2012.[1]\\r\\nWhile the United States Constitution does set parameters for the election of federal officials, state law, not federal, regulates most aspects of elections in the U.S., including primaries, the eligibility of voters (beyond the basic constitutional definition), the running of each state's electoral college, as well as the running of state and local elections. All electionsfederal, state, and localare administered by the individual states.[2]\\r\\nThe restriction and extension of voting rights to different groups has been a contested process throughout United States history. The federal government has also been involved in attempts to increase voter turnout, by measures such as the National Voter Registration Act of 1993. The financing of elections has also long been controversial, because private sources make up substantial amounts of campaign contributions, especially in federal elections. Voluntary public funding for candidates willing to accept spending limits was introduced in 1974 for presidential primaries and elections. The Federal Elections Commission, created in 1975 by an amendment to the Federal Election Campaign Act, has the responsibility to disclose campaign finance information, to enforce the provisions of the law such as the limits and prohibitions on contributions, and to oversee the public funding of U.S. presidential elections.\\r\\nThe most common method used in U.S. elections is the first-past-the-post system, where the highest polling candidate wins the election. Some may use a two-round system, where if no candidate receives a required number of votes then there is a runoff between the two candidates with the most votes.[citation needed]\\r\\nSince 2002, several cities have adopted instant-runoff voting in their elections. Voters rank the candidates in order of preference rather than voting for a single candidate. If a candidate secures more than half of votes cast, that candidate wins. Otherwise, the candidate with the fewest votes is eliminated. Ballots assigned to the eliminated candidate are recounted and assigned to those of the remaining candidates who rank next in order of preference on each ballot. This process continues until one candidate wins by obtaining more than half the votes.[citation needed]\\r\\nThe eligibility of an individual for voting is set out in the constitution and also regulated at state level. The constitution states that suffrage cannot be denied on grounds of race or color, sex, or age for citizens eighteen years or older. Beyond these basic qualifications, it is the responsibility of state legislatures to regulate voter eligibility. Some states ban convicted criminals, especially felons, from voting for a fixed period of time or indefinitely.[3] The number of American adults who are currently or permanently ineligible to vote due to felony convictions is estimated to be 5.3 million.[4] Some states also have legacy constitutional statements barring legally declared incompetent from voting; such references are generally considered obsolete and are being considered for review or removal where they appear.[5]\\r\\nWhile the federal government has jurisdiction over federal elections, most election laws are decided at the state level. All U.S. states except North Dakota require that citizens who wish to vote be registered. Traditionally, voters had to register at state offices to vote, but in the mid-1990s efforts were made by the federal government to make registering easier, in an attempt to increase turnout. The National Voter Registration Act of 1993 (the \\"Motor Voter\\" law) required state governments that receive certain types of federal funding to make the voter registration process easier by providing uniform registration services through drivers' license registration centers, disability centers, schools, libraries, and mail-in registration. Other states allow citizens same-day registration on Election Day.\\r\\nIn many states, citizens registering to vote may declare an affiliation with a political party.[6] This declaration of affiliation does not cost money, and does not make the citizen a dues-paying member of a party. A party cannot prevent a voter from declaring his or her affiliation with them, but it can refuse requests for full membership. In some states, only voters affiliated with a party may vote in that party's primary elections (see below). Declaring a party affiliation is never required. Some states, including Georgia, Michigan, Minnesota, Virginia, Wisconsin, and Washington, practice non-partisan registration.[7]\\r\\nVoters unable or unwilling to vote at polling stations on Election Day can vote via absentee ballots. Absentee ballots are most commonly sent and received via the United States Postal Service. Despite their name, absentee ballots are often requested and submitted in person. About half of all states and U.S. territories allow \\"no excuse absentee,\\" where no reason is required to request an absentee ballot. Others require a valid reason, such as infirmity or travel, be given before a voter can participate using an absentee ballot. Some states, including California,[8] and Washington[9][10] allow citizens to apply for permanent absentee voter status, which will automatically receive an absentee ballot for each election. Typically a voter must request an absentee ballot before the election occurs.\\r\\nA significant source of absentee ballots is the population of Americans living outside the United States. In 1986 Congress enacted the Uniformed and Overseas Citizens Absentee Voting Act (UOCAVA). UOCAVA requires that the states and territories allow members of the United States Uniformed Services and merchant marine, their family members, and United States citizens residing outside the United States to register and vote absentee in elections for Federal offices. Though many states had pre-existing statutes in place UOCAVA made it mandatory and nationally uniform. \\"Generally, all U.S. citizens 19 years or older who are or will be residing outside the United States during an election period are eligible to vote absentee in any election for Federal office. In addition, all members of the Uniformed Services, their family members and members of the Merchant Marine and their family members, who are U.S. citizens, may vote absentee in Federal, state and local elections.\\"[11] Absentee ballots from these voters can often be transmitted private delivery services, fax, or email.[12]\\r\\nMail ballots are similar in many respects to an absentee ballot. However they are used for Mailing Precincts where on Election Day no polling place is opened for a specific precinct.[13] In Oregon, Washington, and Colorado, all ballots are delivered through the mail.\\r\\nEarly voting is a formal process where voters can cast their ballots prior to the official Election Day. Early voting in person is allowed in 33 states and in Washington, D.C., with no excuse required.[14]\\r\\nVoters casting their ballots in polling places record their votes most commonly with optical scan voting machines or DRE voting machines. Voting machine selection is typically done through a state's local election jurisdiction including counties, cities, and townships. Many of these local jurisdictions have changed their voting equipment since 2000 due to the passage of the Help America Vote Act (HAVA), which allocated funds for the replacement of lever machine and punch card voting equipment.\\r\\nSince the 1980s many jurisdictions and voting locations have given out \\"I Voted\\" stickers to people casting ballots. In the state of Illinois it is a state law to have stickers available to voters after they have cast their ballots. State and local governments pay $30 million per year for the stickers that cost about one cent each.[15]\\r\\nThe United States has a presidential system of government, which means that the executive and legislature are elected separately. Article One of the United States Constitution requires that any election for the U.S. President must occur on a single day throughout the country; elections for Congressional offices, however, can be held at different times. Congressional and presidential elections take place simultaneously every four years, and the intervening Congressional elections, which take place every two years, are called Midterm elections.\\r\\nThe constitution states that members of the United States House of Representatives must be at least 25 years old, a citizen of the United States for at least seven years, and be a (legal) inhabitant of the state they represent. Senators must be at least 30 years old, a citizen of the United States for at least nine years, and be a (legal) inhabitant of the state they represent. The President must be at least 35 years old, a natural born citizen of the United States and a resident in the United States for at least fourteen years. It is the responsibility of state legislatures to regulate the qualifications for a candidate appearing on a ballot paper, although in order to get onto the ballot, a candidate must often collect a legally defined number of signatures.\\r\\nThe President and the Vice President are elected together in a Presidential election.[16] It is an indirect election, with the winner being determined by votes cast by electors of the Electoral College. In modern times, voters in each state select a slate of electors from a list of several slates designated by different parties or candidates, and the electors typically promise in advance to vote for the candidates of their party (whose names of the presidential candidates usually appear on the ballot rather than those of the individual electors). The winner of the election is the candidate with at least 270 Electoral College votes. It is possible for a candidate to win the electoral vote, and lose the (nationwide) popular vote (receive fewer votes nationwide than the second ranked candidate). Prior to ratification of the Twelfth Amendment to the United States Constitution (1804), the runner-up in a Presidential election[17] became the Vice President.\\r\\nElectoral College votes are cast by individual states by a group of electors; each elector casts one electoral college vote. Until the Twenty-third Amendment to the United States Constitution of 1961 the District of Columbia citizens did not have representation and/or electors in the electoral college. In modern times, with electors usually committed to vote for a party candidate in advance, electors that vote against the popular vote in their state are called faithless electors, and occurrences are rare. State law regulates how states cast their electoral college votes. In all states except Maine and Nebraska, the candidate that wins the most votes in the state receives all its electoral college votes (a \\"winner takes all\\" system). From 1969 in Maine, and from 1991 in Nebraska, two electoral votes are awarded based on the winner of the statewide election, and the rest (two in Maine, three in Nebraska) go to the highest vote-winner in each of the state's congressional districts.\\r\\nCongress has two chambers: the Senate and the House of Representatives.\\r\\nThe Senate has 100 members, elected for a six-year term in dual-seat constituencies (2 from each state), with one-third being renewed every two years. The group of the Senate seats that is up for election during a given year is known as a \\"class\\"; the three classes are staggered so that only one of the three groups is renewed every two years. Until the Seventeenth Amendment to the United States Constitution in 1913, States chose how to elect Senators, and they were often elected by state legislatures, not the electorate of states.\\r\\nThe House of Representatives has 435 members, elected for a two-year term in single-seat constituencies. House of Representatives elections are held every two years on the first Tuesday after November 1 in even years. Special House elections can occur between if a member dies or resigns during a term. House elections are first-past-the-post elections that elect a Representative from each of 435 House districts which cover the United States. The non-voting delegates of Washington, D.C. and the territories of American Samoa, Guam, the Northern Mariana Islands, Puerto Rico and the United States Virgin Islands are also elected.\\r\\nHouse elections occur every two years, correlated with presidential elections or halfway through a President's term. The House delegate of Puerto Rico, officially known as the Resident Commissioner of Puerto Rico, is elected to a four-year term, coinciding with those of the President.\\r\\nAs the redistricting commissions of states are often partisan, districts are often drawn which benefit incumbents. An increasing trend has been for incumbents to have an overwhelming advantage in House elections, and since the 1994 election, an unusually low number of seats has changed hands in each election.[citation needed] Due to gerrymandering, fewer than 10% of all House seats are contested in each election cycle. Over 90% of House members are reelected every two years, due to lack of electoral competition. Gerrymandering of the House, combined with the divisions inherent in the design of the Senate and of the Electoral College, result in a discrepancy between the percentage of popular support for various political parties and the actual level of the parties' representation.\\r\\nState law and state constitutions, controlled by state legislatures regulate elections at state level and local level. Various officials at state level are elected. Since the separation of powers applies to states as well as the federal government, state legislatures and the executive (the governor) are elected separately. Governors and lieutenant governor are elected in all states, in some states on a joint ticket and in some states separately, some separately in different electoral cycles. The governors of the territories of American Samoa, Guam, the Northern Mariana Islands, Puerto Rico and the United States Virgin Islands are also elected. In some states, executive positions such as Attorney General and Secretary of State are also elected offices. All members of state legislatures and territorial jurisdiction legislatures are elected. In some states, members of the state supreme court and other members of the state judiciary are elected. Proposals to amend the state constitution are also placed on the ballot in some states.\\r\\nAs a matter of convenience and cost saving, elections for many of these state and local offices are held at the same time as either the federal presidential or midterm elections. There are a handful of states, however, that instead hold their elections during odd-numbered \\"off years.\\"\\r\\nAt the local level, county and city government positions are usually filled by election, especially within the legislative branch. The extent to which offices in the executive or judicial branches are elected vary from county-to-county or city-to-city. Some examples of local elected positions include sheriffs at the county level and mayors and school board members at the city level. Like state elections, an election for a specific local office may be held at the same time as either the presidential, midterm, or off-year elections.\\r\\nIn the US elections are actually conducted by local authorities, working under local, state, and federal law and regulation, as well as the US Constitution. It is a highly decentralized system.[18]\\r\\nIn around half of US states, the Secretary of State is the official in charge of elections; in other states it is someone appointed for the job, or a commission.[18] It is this person or commission who is responsible for certifying, tabulating, and reporting votes for the state.[18]\\r\\nAmericans vote for a specific candidate instead of directly selecting a particular political party. The United States Constitution has never formally addressed the issue of political parties. The Founding Fathers such as Alexander Hamilton and James Madison did not support domestic political factions at the time the Constitution was written.[19] In addition, the first President of the United States, George Washington, was not a member of any political party at the time of his election or throughout his tenure as president. Furthermore, he hoped that political parties would not be formed, fearing conflict and stagnation. Nevertheless, the beginnings of the American two-party system emerged from his immediate circle of advisers, with Hamilton and Madison ending up being the core leaders in this emerging party system.\\r\\nThus, it is up to the candidate to decide under what party he/she should run, registers to run, pays the fees, etc. In the primary elections, the party organization stays neutral until one candidate has been elected. The platform of the party is written by the winning candidate (in presidential elections; in other elections no platform is involved). Each candidate has his or her own campaign, fund raising organization, etc. The primary elections in the main parties are organized by the states, who also register the party affiliation of the voters (this also makes it easier to gerrymander the congressional districts). The party is thus little more than a campaign organization for the main elections.\\r\\nHowever, elections in the United States often do become de facto national races between the political parties. In what is known as \\"presidential coattails\\", candidates in presidential elections usually bring out supporters who then vote for his or her party's candidates for other offices, usually resulting in the presidential winner's party gaining seats in Congress. On the other hand, midterm elections are sometimes regarded as a referendum on the sitting president's and/or incumbent party's performance.[20][21] There is a historical pattern that the incumbent president's party loses seats in midterm elections. This may be because the President's popularity has slipped since election, or because the President's popularity encouraged supporters to come out to vote for him or her in the presidential election, but these supporters are less likely to vote when the President is not up for election.\\r\\nBallot access refers to the laws which regulate under what conditions access is granted for a candidate or political party to appear on voters' ballots. Each State has its own ballot access laws to determine who may appear on ballots and who may not. According to Article I, Section 4, of the United States Constitution, the authority to regulate the time, place, and manner of federal elections is up to each State, unless Congress legislates otherwise. Depending on the office and the state, it may be possible for a voter to cast a write-in vote for a candidate whose name does not appear on the ballot, but it is extremely rare for such a candidate to win office.\\r\\nThe funding of electoral campaigns has always been a controversial issue in American politics. Infringement of free speech (First Amendment) is an argument against restrictions on campaign contributions, while allegations of corruption arising from unlimited contributions and the need for political equality are arguments for the other side.[22] Private funds are a major source of finance, from individuals and organizations. The first attempt to regulate campaign finance by legislation was in 1867, but major legislation, with the intention to widely enforce, on campaign finance was not introduced until the 1970s.\\r\\nMoney contributed to campaigns can be classified into \\"hard money\\" and \\"soft money\\". Hard money is money contributed directly to a campaign, by an individual or organization. Soft money is money from an individual or organization not contributed to a campaign, but spent in candidate specific advertising or other efforts that benefits that candidate by groups supporting the candidate, but legally not coordinated by the official campaign.\\r\\nThe Federal Election Campaign Act of 1971 required candidates to disclose sources of campaign contributions and campaign expenditure. It was amended in 1974 to legally limit campaign contributions. It banned direct contributing to campaigns by corporations and trade unions and limited individual donations to $1,000 per campaign. It introduced public funding for Presidential primaries and elections. The Act also placed limits of $5,000 per campaign on PACs (political action committees). The limits on individual contributions and prohibition of direct corporate or labor union campaigns led to a huge increase in the number of PACs. Today many labor unions and corporations have their own PACs, and over 4,000 in total exist. The 1974 amendment also specified a Federal Election Commission, created in 1975 to administer and enforce campaign finance law. Various other provisions were also included, such as a ban on contributions or expenditures by foreign nationals (incorporated from the Foreign Agents Registration Act (FARA) (1966)).\\r\\nThe case of Buckley v. Valeo (1976) challenged the Act. Most provisions were upheld, but the court found that the mandatory spending limit imposed was unconstitutional, as was the limit placed on campaign spending from the candidate's personal fortune and the provision that limited independent expenditures by individuals and organizations supporting but not officially linked to a campaign. The effect of the first decision was to allow candidates such as Ross Perot and Steve Forbes to spend enormous amounts of their own money in their own campaigns. The effect of the second decision was to allow the culture of \\"soft money\\" to develop.\\r\\nA 1979 amendment to the Federal Election Campaign Act allowed political parties to spend without limit on get-out-the-vote and voter registration activities conducted primarily for a presidential candidate. Later, they were permitted by FECA to use \\"soft money\\", unregulated, unlimited contributions to fund this effort. Increasingly, the money began to be spent on issue advertising, candidate specific advertising that was being funded mostly by soft money.\\r\\nThe Bipartisan Campaign Reform Act of 2002 banned local and national parties from spending \\"soft money\\" and banned national party committees from accepting or spending soft money. It increased the limit of contributions by individuals from $1,000 to $2,000. It banned corporations or labor unions from funding issue advertising directly, and banned the use of corporate or labor money for advertisements that mention a federal candidate within 60 days of a general election or 30 days of a primary. The constitutionality of the bill was challenged and in December 2003, the Supreme Court upheld most provisions of the legislation. (See McConnell v. FEC.)\\r\\nA large number of \\"527 groups\\" were active for the first time in the 2004 election. These groups receive donations from individuals and groups and then spend the money on issue advocacy, such as the anti-Kerry ads by Swift Boat Veterans for Truth. This is a new form of soft money, and not surprisingly it is controversial. Many 527 groups have close links with the Democratic or Republican parties, even though legally they cannot coordinate their activities with them. John McCain, one of the senators behind the Bipartisan Campaign Reform Act, and President Bush have both declared a desire to ban 527s.\\r\\nChanging campaign finance laws is a highly controversial issue. Some reformers wish to see laws changed in order to improve electoral competition and political equality. Opponents wish to see the system stay as it is, whereas other reformers wish even fewer restrictions on the freedom to spend and contribute money. The Supreme Court has made it increasingly difficult for those who wish to regulate election financing, but options like partial public funding of campaigns are still possible and offer the potential to address reformers' concerns with minimal restrictions on the freedom to contribute.[23]\\r\\nIn partisan elections, candidates are chosen by primary elections (abbreviated to \\"primaries\\") and caucuses in the states, the District of Columbia, Puerto Rico, American Samoa, Guam, and the U.S. Virgin Islands.\\r\\nA primary election is an election in which registered voters in a jurisdiction (nominating primary) select a political party's candidate for a later election. There are various types of primary: either the whole electorate is eligible, and voters choose one party's primary at the polling booth (an open primary); or only independent voters can choose a party's primary at the polling booth (a semi-closed primary); or only registered members of the party are allowed to vote (closed primary). The blanket primary, when voters could vote for all parties' primaries on the same ballot was struck down by the United States Supreme Court as violating the First Amendment guarantee of freedom of assembly in the case California Democratic Party v. Jones. Primaries are also used to select candidates at the state level, for example in gubernatorial elections.\\r\\nCaucuses also nominate candidates by election, but they are very different from primaries. Caucuses are meetings that occur at precincts and involve discussion of each party's platform and issues such as voter turnout in addition to voting. Eleven states: Iowa, New Mexico, North Dakota, Maine, Nevada, Hawaii, Minnesota, Kansas, Alaska, Wyoming, Colorado and the District of Columbia use caucuses, for one or more political parties.\\r\\nThe primary and caucus season in Presidential elections lasts from the Iowa caucus in January to the last primaries in June. Front-loading - when larger numbers of contests take place in the opening weeks of the seasoncan have an effect on the nomination process, potentially reducing the number of realistic candidates, as fund-raisers and donors quickly abandon those they see as untenable. However, it is not the case that the successful candidate is always the candidate that does the best in the early primaries. There is also a period dubbed the \\"invisible primary\\" that takes place before the primary season, when candidates attempt to solicit media coverage and funding well before the real primary season begins.\\r\\nA state's presidential primary election or caucus usually is an indirect election: instead of voters directly selecting a particular person running for President, it determines how many delegates each party's national political convention will receive from their respective state. These delegates then in turn select their party's presidential nominee. Held in the summer, a political convention's purpose is also to adopt a statement of the party's principles and goals known as the platform and adopt the rules for the party's activities.\\r\\nThe day on which primaries are held for congressional seats, and state and local offices may also vary between states. The only federally mandated day for elections is Election Day for the general elections of the President and Congress; all other elections are at the discretion of the individual state and local governments.\\r\\nIn most states of the U.S., the chief election officer is the Secretary of state. In some states, local officials like a county Registrar of Voters or Supervisor of Elections manages the conduct of elections under the supervision of (or in coordination with) the chief election officer of the state. Many of these state and county offices have web sites that provide information to help voters obtain information on their polling places for each election, the various districts to which they belong (e.g., house and senate districts in the state and federal legislature, school boards, water districts, municipalities, etc.), as well as dates of elections and deadlines to file to run or register to vote. Some allow voters to download a sample ballot in advance of the election.\\r\\nBeyond this, various media outlets provide information they think will interest their audience.\\r\\nMore systematic coverage is provided by web sites devoted specifically to collecting election information and making it available to the public. Two of the better known such sites are Ballotpedia and Vote Smart. These are run by non-profit, non-partisan organizations. They have paid staffs and are much more tightly controlled than Wikipedia.\\r\\nUSElections.com[24] tries to provide similar information but relies on volunteers in a way that is more like Wikipedia than Ballotpedia and Vote Smart.\\r\\nThe website 270towin provides actual electoral college maps (both current and historic) but also the ability to use an interactive map in order to make election predictions. Ongoing election news is reported as well as data on Senate and House races.[25]\\r\\nThe Center for Responsive Politics (opensecrets.org) and the National Institute on Money in State Politics (followthemoney.org) also provide useful election information focusing especially on campaign finance.\\r\\nIn 2014 scientists from Princeton University did a study on the influence of the \\"elite\\", and their derived power from special interest lobbying, versus the \\"ordinary\\" US citizen within the US political system. They found that the US was looking more like an oligarchy than a real representative democracy; thus eroding a government of the people, by the people, for the people as stated by Abraham Lincoln in his Gettysburg Address. In fact, the study found that average citizens had an almost nonexistent influence on public policies and that the ordinary citizen had little or no independent influence on policy at all.[26]\\r\\nThere were many US presidential elections in which foreign countries manipulate the voters.[27] Electoral College is criticised for being un-democratic (it can choose candidate who did not win the popular vote) and for focusing campaigns only on swing states.[28]","input":"How many elected officials in the united states?"},{"output":"high level in painting and sculpture","context":"Ancient Egyptian art is the painting, sculpture, architecture and other arts produced by the civilization of ancient Egypt in the lower Nile Valley from about 3000 BC to 30 AD. Ancient Egyptian art reached a high level in painting and sculpture, and was both highly stylized and symbolic. It was famously conservative, and Egyptian styles changed remarkably little over more than three thousand years. Much of the surviving art comes from tombs and monuments and thus there is an emphasis on life after death and the preservation of knowledge of the past.\\r\\nAncient Egyptian art included paintings, sculpture in wood (now rarely surviving), stone and ceramics, drawings on papyrus, faience, jewelry, ivories, and other art media. It displays an extraordinarily vivid representation of the ancient Egyptian's socioeconomic status and belief systems.\\r\\n\\r\\n\\r\\nEgyptian art is famous for its distinctive figure convention, used for the main figures in both relief and painting, with parted legs (where not seated) and head shown as seen from the side, but the torso seen as from the front, and a standard set of proportions making up the figure, using 18 \\"fists\\" to go from the ground to the hair-line on the forehead.[1] This appears as early as the Narmer Palette from Dynasty I, but there as elsewhere the convention is not used for minor figures shown engaged in some activity, such as the captives and corpses.[2] Other conventions make statues of males darker than females ones. Very conventionalized portrait statues appear from as early as Dynasty II, before 2,780 BC,[3] and with the exception of the art of the Amarna period of Ahkenaten,[4] and some other periods such as Dynasty XII, the idealized features of rulers, like other Egyptian artistic conventions, changed little until after the Greek conquest.[5]\\r\\nEgyptian art uses hierarchical proportion, where the size of figures indicates their relative importance. The gods or the divine pharaoh are usually larger than other figures and the figures of high officials or the tomb owner are usually smaller, and at the smallest scale any servants and entertainers, animals, trees, and architectural details.[6]\\r\\nSymbolism can be observed throughout Egyptian art and played an important role in establishing a sense of order. The pharaoh's regalia, for example, represented his power to maintain order. Animals were also highly symbolic figures in Egyptian art. Some colors were expressive: blue or gold indicated divinity because of its unnatural appearance and association with precious materials, and the use of black for royal figures expressed the fertility of the Nile from which Egypt was born.[7]\\r\\nNot all Egyptian reliefs were painted, and less prestigious works in tombs, temples and palaces were merely painted on a flat surface. Stone surfaces were prepared by whitewash, or if rough, a layer of coarse mud plaster, with a smoother gesso layer above; some finer limestones could take paint directly. Pigments were mostly mineral, chosen to withstand strong sunlight without fading. The binding medium used in painting remains unclear: egg tempera and various gums and resins have been suggested. It is clear that true fresco, painted into a thin layer of wet plaster, was not used. Instead the paint was applied to dried plaster, in what is called \\"fresco a secco\\" in Italian. After painting, a varnish or resin was usually applied as a protective coating, and many paintings with some exposure to the elements have survived remarkably well, although those on fully exposed walls rarely have.[8] Small objects including wooden statuettes were often painted using similar techniques.\\r\\nMany ancient Egyptian paintings have survived in tombs, and sometimes temples, due to Egypt's extremely dry climate. The paintings were often made with the intent of making a pleasant afterlife for the deceased. The themes included journey through the afterworld or protective deities introducing the deceased to the gods of the underworld (such as Osiris). Some tomb paintings show activities that the deceased were involved in when they were alive and wished to carry on doing for eternity.\\r\\nIn the New Kingdom and later, the Book of the Dead was buried with the entombed person. It was considered important for an introduction to the afterlife.\\r\\nEgyptian paintings are painted in such a way to show a profile view and a side view of the animal or person at the same time. For example, the painting to the right shows the head from a profile view and the body from a frontal view. Their main colors were red, blue, green, gold, black and yellow.\\r\\nPaintings showing scenes of hunting and fishing can have lively close-up landscape backgrounds of reeds and water, but in general Egyptian painting did not develop a sense of depth, and neither landscapes nor a sense of visual perspective are found, the figures rather varying in size with their importance rather than their location.\\r\\nThe monumental sculpture of ancient Egypt's temples and tombs is world-famous,[9] but refined and delicate small works exist in much greater numbers. The Egyptians used the distinctive technique of sunk relief, which is well suited to very bright sunlight. The distinctive pose of standing statues facing forward with one foot in front of the other was helpful for the balance and strength of the piece. It was adopted very early and remained unchanged until the arrival of the Greeks. Seated statues were also very common.\\r\\nEgyptian pharaohs were always regarded as gods, but other deities are much less common in large statues, except when they represent the pharaoh as another deity; however the other deities are frequently shown in paintings and reliefs. The famous row of four colossal statues outside the main temple at Abu Simbel each show Rameses II, a typical scheme, though here exceptionally large.[10] Most larger sculptures survive from Egyptian temples or tombs; massive statues were built to represent gods and pharaohs and their queens, usually for open areas in or outside temples. The very early colossal Great Sphinx of Giza was never repeated, but avenues lined with very large statues including sphinxes and other animals formed part of many temple complexes. The most sacred cult image of a god in a temple, usually held in the naos, was in the form of a relatively small boat or barque holding an image of the god, and apparently usually in precious metal ÿ none have survived.\\r\\nBy Dynasty IV (2680ÿ2565 BC) at the latest the idea of the Ka statue was firmly established. These were put in tombs as a resting place for the ka portion of the soul, and so we have a good number of less conventionalized statues of well-off administrators and their wives, many in wood as Egypt is one of the few places in the world where the climate allows wood to survive over millennia, and many block statues. The so-called reserve heads, plain hairless heads, are especially naturalistic, though the extent to which there was real portraiture in ancient Egypt is still debated.\\r\\nEarly tombs also contained small models of the slaves, animals, buildings and objects such as boats necessary for the deceased to continue his lifestyle in the afterworld, and later Ushabti figures.[11] However the great majority of wooden sculpture has been lost to decay, or probably used as fuel. Small figures of deities, or their animal personifications, are very common, and found in popular materials such as pottery. There were also large numbers of small carved objects, from figures of the gods to toys and carved utensils. Alabaster was often used for expensive versions of these; painted wood was the most common material, and normal for the small models of animals, slaves and possessions placed in tombs to provide for the afterlife.\\r\\nVery strict conventions were followed while crafting statues and specific rules governed appearance of every Egyptian god. For example, the sky god (Horus) was essentially to be represented with a falcon's head, the god of funeral rites (Anubis) was to be always shown with a jackal's head. Artistic works were ranked according to their compliance with these conventions, and the conventions were followed so strictly that, over three thousand years, the appearance of statues changed very little. These conventions were intended to convey the timeless and non-aging quality of the figure's ka.[citation needed]\\r\\nWooden tomb models, Dynastry XI; a high administrator counts his cattle.\\r\\nThe Gold Mask of Tutankhamun, c. late Eighteenth dynasty, Egyptian Museum\\r\\nThe Younger Memnon c. 1250 BC, British Museum\\r\\nSunk relief of the crocodile god Sobek\\r\\nOsiris on a lapis lazuli pillar in the middle, flanked by Horus on the left, and Isis on the right, 22nd dynasty, Louvre\\r\\nThe ka statue provided a physical place for the ka to manifest. Egyptian Museum, Cairo\\r\\nBlock statue of Pa-Ankh-Ra, ship master, bearing a statue of Ptah. Late Period, ca. 650ÿ633 BC, Cabinet des Mdailles.\\r\\nA sculpted head of Amenhotep III\\r\\nQueen Ankhnes-meryre II and her Son, Pepy II,c. 2200 BC. Brooklyn Museum\\r\\nEgyptian faience, made from sand and chemicals, produced relatively cheap and very attractive small objects in a variety of colours, and was used for a variety of types of objects including jewellery. Ancient Egyptian glass goes back to very early Egyptian history, but was at first very much a luxury material. In later periods it became common, and highly decorated small jars for perfume and other liquids are often found as grave goods.\\r\\nAncient Egyptians used steatite (some varieties were called soapstone) and carved small pieces of vases, amulets, images of deities, of animals and several other objects. Ancient Egyptian artists also discovered the art of covering pottery with enamel. Covering by enamel was also applied to some stone works. The colour blue, first used in the very expensive imported stone lapis lazuli, was highly regarded by ancient Egypt, and the pigment Egyptian blue was widely used to colour a variety of materials.\\r\\nDifferent types of pottery items were deposited in tombs of the dead. Some such pottery items represented interior parts of the body, like the lungs, the liver and smaller intestines, which were removed before embalming. A large number of smaller objects in enamel pottery were also deposited with the dead. It was customary to craft on the walls of the tombs cones of pottery, about six to ten inches tall, on which were engraved or impressed legends relating to the dead occupants of the tombs. These cones usually contained the names of the deceased, their titles, offices which they held, and some expressions appropriate to funeral purposes.\\r\\nPapyrus was used by ancient Egyptians (and exported to much of the ancient Mediterranean world) for writing and painting. Papyrus is relatively fragile, lasting at most a century or two in a library, and though used all over the classical world has only survived when buried in the very dry conditions of Egypt, and even then is often in poor condition. Papyrus texts illustrate all dimensions of ancient Egyptian life and include literary, religious, historical and administrative documents.\\r\\nThe Amarna period and the years before the pharaoh Akhenaten moved the capital there in the late Eighteenth Dynasty form the most drastic interruption to the continuity of style in the Old and New Kingdoms. Amarna art is characterized by a sense of movement and activity in images, with figures having raised heads, many figures overlapping and many scenes full and crowded. As the new religion was a monotheistic worship of the sun, sacrifices and worship were apparently conducted in open courtyards, and sunk relief decoration was widely used in these.\\r\\nThe human body is portrayed differently in the Amarna style than Egyptian art on the whole. For instance, many depictions of Akhenaten's body give him distinctly feminine qualities, such as large hips, prominent breasts, and a larger stomach and thighs. This is a divergence from the earlier Egyptian art which shows men with perfectly chiseled bodies. Faces are still shown exclusively in profile.\\r\\nNot many buildings from this period have survived the ravages of later kings, partially as they were constructed out of standard size blocks, known as Talatat, which were very easy to remove and reuse. Temples in Amarna, following the trend, did not follow traditional Egyptian customs and were open, without ceilings, and had no closing doors. In the generation after Akhenaten's death, artists reverted to their old styles. There were still traces of this period's style in later art, but in most respects Egyptian art, like Egyptian religion, resumed its usual characteristics after the death of Akhenaten as though the period had never happened. Amarna itself was abandoned and considerable trouble was gone to in defacing monuments from the reign, including dis-assembling buildings and reusing the blocks with their decoration facing inwards, as has recently been discovered in one later building.\\r\\nDiscoveries made since the end of the 19th century surrounding the (now submerged) ancient Egyptian city of Heracleum at Alexandria include a 4th-century BC, unusually sensual, detailed and feministic (as opposed to deified) depiction of Isis, marking a combination of Egyptian and Hellenistic forms beginning around the time of Egypt's conquest by Alexander the Great in 332-331 BC. However this was untypical of Ptolemaic sculpture, which generally avoided mixing Egyptian styles with the general Hellenistic style which was used in the court art of the Ptolemaic Dynasty,[12] while temples in the rest of the country continued using late versions of traditional Egyptian formulae.[13] Scholars have proposed an \\"Alexandrian style\\" in Hellenistic sculpture, but there is in fact little to connect it with Alexandria.[14]\\r\\nMarble was extensively used in court art, although it all had to be imported, and use was made of various marble-saving techniques, such as making even heads up from a number of pieces, and using stucco for beards, the back of heads and hair.[15] In contrast to the art of other Hellenistic kingdoms, Ptolemaic royal portraits are generalized and idealized, with little concern for achieving an individual portrait, though thanks to coins some portrait sculpture can be identified as one of the 15 King Ptolemys.[16] Many later portraits have clearly had the face reworked to show a later king.[17] One Egyptian trait was to give much greater prominence to the queens than other successor dynasties to Alexander, with the royal couple often shown as a pair. This predated the 2nd century, a series of queens did indeed exercise real power.[18]\\r\\nIn the 2nd century, Egyptian temple sculptures did begin to reuse court models in their faces, and sculptures of priest often used a Hellenistic style to achieve individually distinctive portrait heads.[19] Many small statuettes were produced, with Alexander, as founder of the dynasty, a generalized \\"King Ptolemy\\", and a naked Aphrodite among the most common types. Pottery figurines included grotesques and fashionable ladies of the Tanagra figurine style.[20] Erotic groups featured absurdly large phalluses. Some fittings for wooden interiors include very delicately patterned polychrome falcons in faience.\\r\\nAncient Egyptian architects used sun-dried and kiln-baked bricks, fine sandstone, limestone and granite. Architects carefully planned all their work. The stones had to fit precisely together, since there was no mud or mortar. When creating the pyramids, ramps were used to allow workmen to move up as the height of the construction grew. When the top of the structure was completed, the artists decorated from the top down, removing ramp sand as they went down. Exterior walls of structures like the pyramids contained only a few small openings. Hieroglyphic and pictorial carvings in brilliant colors were abundantly used to decorate Egyptian structures, including many motifs, like the scarab, sacred beetle, the solar disk, and the vulture. They described the changes the Pharaoh would go through to become a god.[21]\\r\\nHieroglyphs are the ancient Egyptian writing system in which pictures and symbols stand for sounds and words. Jean-Francois Champollion first decoded hieroglyphs from the Rosetta Stone, which was found in 1799. Hieroglyphs have more than 700 symbols.\\r\\nHill, Marsha (2007). Gifts for the gods: images from Egyptian temples. New York: The Metropolitan Museum of Art. ISBN?9781588392312.?","input":"What art forms were common in ancient egypt?"},{"output":"Dmitri Ivanovich Mendeleev","context":"\\r\\n\\r\\n\\r\\n\\r\\nDmitri Ivanovich Mendeleev[2] (English: /?m?nd?l?e??f/ MEN-d?l-AY-?f;[3] Russian: Ұҳ ־־ 춶־[note 1], tr. Dmtriy Ivnovich Mendelyev, IPA:?[?dm?itr??j ??van?v??t? m??nd???l?ej?f]?(?listen); 8 February 1834?ÿ 2 February 1907 O.S. 27?January 1834?ÿ 20 January 1907) was a Russian chemist and inventor. He formulated the Periodic Law, created a farsighted version of the periodic table of elements, and used it to correct the properties of some already discovered elements and also to predict the properties of eight elements yet to be discovered.\\r\\n\\r\\nMendeleev was born in the village of Verkhnie Aremzyani, near Tobolsk in Siberia, to Ivan Pavlovich Mendeleev (1783ÿ1847) and Maria Dmitrievna Mendeleeva (ne Kornilieva) (1793ÿ1850).[4][5] His paternal grandfather Pavel Maximovich Sokolov was a Russian Orthodox priest from the Tver region.[6] Ivan, along with his brothers and sisters, obtained new family names while attending the theological seminary.[7] He worked as a school principal and a teacher of fine arts, politics and philosophy at the Tambov and Saratov gymnasiums.[8]\\r\\n\\r\\nMaria Kornilieva came from a well-known dynasty of Tobolsk merchants, founders of the first Siberian printing house who traced their ancestry to Yakov Korniliev, a 17th-century posad man turned a wealthy merchant.[9][10] In 1889 a local librarian published an article in the Tobolsk newspaper where he claimed that Yakov was a baptized Teleut, an ethnic minority known as \\"white Kalmyks\\" at the time.[11] Since no sources were provided and no documented facts of Yakov's life were ever revealed, biographers generally dismiss it as a myth.[12][13] In 1908, shortly after Mendeleev's death, one of his nieces published Family Chronicles. Memories about D. I. Mendeleev where she voiced \\"a family legend\\" about Maria's grandfather who married \\"a Kyrgyz or Tatar beauty whom he loved so much that when she died, he also died from grief\\".[14] This, however, contradicts the documented family chronicles, and neither of those legends is supported by Mendeleev's autobiography, his daughter's or his wife's memoirs.[5][15][16] Yet some Western scholars still refer to Mendeleev's supposed \\"Mongol\\", \\"Tatar\\", \\"Tartarian\\" or simply \\"Asian\\" ancestry as a fact.[17][18][19][20]\\r\\n\\r\\nMendeleev was raised as an Orthodox Christian, his mother encouraging him to \\"patiently search divine and scientific truth.\\"[21] His son would later inform that he departed from the Church and embraced a form of \\"romanticized deism\\".[22]\\r\\n\\r\\nMendeleev was the youngest of 17 siblings, of whom \\"only 14 stayed alive to be baptized\\" according to Mendeleev's brother Pavel, meaning the others died soon after their birth.[8] The exact number of Mendeleev's siblings differs among sources and is still a matter of some historical dispute.[23][24] Unfortunately for the family's financial well being, his father became blind and lost his teaching position. His mother was forced to work and she restarted her family's abandoned glass factory. At the age of?13, after the passing of his father and the destruction of his mother's factory by fire, Mendeleev attended the Gymnasium in Tobolsk.\\r\\n\\r\\nIn 1849, his mother took Mendeleev across Russia from Siberia to Moscow with the aim of getting Mendeleev a higher education. The university in Moscow did not accept him. The mother and son continued to Saint Petersburg to the fathers alma mater. The now poor Mendeleev family relocated to Saint Petersburg, where he entered the Main Pedagogical Institute in 1850. After graduation, he contracted tuberculosis, causing him to move to the Crimean Peninsula on the northern coast of the Black Sea in 1855. While there, he became a science master of the Simferopol gymnasium 1. In 1857, he returned to Saint Petersburg with fully restored health.\\r\\n\\r\\nBetween 1859 and 1861, he worked on the capillarity of liquids and the workings of the spectroscope in Heidelberg. Later in 1861, he published a textbook named Organic Chemistry.[25] This won him the Demidov Prize of the Petersburg Academy of Sciences.[25]\\r\\n\\r\\nOn 4?April 1862 he became engaged to Feozva Nikitichna Leshcheva, and they married on 27?April 1862 at Nikolaev Engineering Institute's church in Saint Petersburg (where he taught).[26]\\r\\n\\r\\nMendeleev became a professor at the Saint Petersburg Technological Institute and Saint Petersburg State University in 1864,[25] and 1865, respectively. In 1865 he became Doctor of Science for his dissertation \\"On the Combinations of Water with Alcohol\\". He achieved tenure in 1867 at St. Petersburg University and started to teach inorganic chemistry, while succeeding Voskresenskii to this post.[25] and by 1871 he had transformed Saint Petersburg into an internationally recognized center for chemistry research.\\r\\n\\r\\nIn 1876, he became obsessed with Anna Ivanova Popova and began courting her; in 1881 he proposed to her and threatened suicide if she refused. His divorce from Leshcheva was finalized one month after he had married Popova (on 2?April[27]) in early 1882. Even after the divorce, Mendeleev was technically a bigamist; the Russian Orthodox Church required at least seven years before lawful remarriage. His divorce and the surrounding controversy contributed to his failure to be admitted to the Russian Academy of Sciences (despite his international fame by that time). His daughter from his second marriage, Lyubov, became the wife of the famous Russian poet Alexander Blok. His other children were son Vladimir (a sailor, he took part in the notable Eastern journey of Nicholas?II) and daughter Olga, from his first marriage to Feozva, and son Ivan and twins from Anna.\\r\\n\\r\\nThough Mendeleev was widely honored by scientific organizations all over Europe, including (in 1882) the Davy Medal from the Royal Society of London (which later also awarded him the Copley Medal in 1905),[28] he resigned from Saint Petersburg University on 17?August 1890. He was elected a Foreign Member of the Royal Society (ForMemRS) in 1892,[1] and in 1893 he was appointed director of the Bureau of Weights and Measures, a post which he occupied until his death.[29]\\r\\n\\r\\nMendeleev also investigated the composition of petroleum, and helped to found the first oil refinery in Russia. He recognized the importance of petroleum as a feedstock for petrochemicals. He is credited with a remark that burning petroleum as a fuel \\"would be akin to firing up a kitchen stove with bank notes.\\"[30]\\r\\n\\r\\nIn 1905, Mendeleev was elected a member of the Royal Swedish Academy of Sciences. The following year the Nobel Committee for Chemistry recommended to the Swedish Academy to award the Nobel Prize in Chemistry for 1906 to Mendeleev for his discovery of the periodic system. The Chemistry Section of the Swedish Academy supported this recommendation. The Academy was then supposed to approve the Committee's choice, as it has done in almost every case. Unexpectedly, at the full meeting of the Academy, a dissenting member of the Nobel Committee, Peter Klason, proposed the candidacy of Henri Moissan whom he favored. Svante Arrhenius, although not a member of the Nobel Committee for Chemistry, had a great deal of influence in the Academy and also pressed for the rejection of Mendeleev, arguing that the periodic system was too old to acknowledge its discovery in 1906. According to the contemporaries, Arrhenius was motivated by the grudge he held against Mendeleev for his critique of Arrhenius's dissociation theory. After heated arguments, the majority of the Academy chose Moissan by a margin of one vote.[31]  The attempts to nominate Mendeleev in 1907 were again frustrated by the absolute opposition of Arrhenius.[32]\\r\\n\\r\\nIn 1907, Mendeleev died at the age of 72 in Saint Petersburg from influenza. The crater Mendeleev on the Moon, as well as element number?101, the radioactive mendelevium, are named after him.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nIn 1863 there were 56 known elements with a new element being discovered at a rate of approximately one per year. Other scientists had previously identified periodicity of elements. John Newlands described a Law of Octaves, noting their periodicity according to relative atomic weight in 1864, publishing it in 1865. His proposal identified the potential for new elements such as germanium. The concept was criticized and his innovation was not recognized by the Society of Chemists until 1887. Another person to propose a periodic table was Lothar Meyer, who published a paper in 1864 describing 28 elements classified by their valence, but with no prediction of new elements.\\r\\n\\r\\nAfter becoming a teacher in 1867, Mendeleev wrote the definitive textbook of his time: Principles of Chemistry (two volumes, 1868ÿ1870). It was written as he was preparing a textbook for his course.[25] This is when he made his most important discovery.[25] As he attempted to classify the elements according to their chemical properties, he noticed patterns that led him to postulate his periodic table; he claimed to have envisioned the complete arrangement of the elements in a dream:[33][34][35][36][37]\\r\\n\\r\\n\\"I saw in a dream a table where all elements fell into place as required. Awakening, I immediately wrote it down on a piece of paper, only in one place did a correction later seem necessary.\\"\\r\\nUnaware of the earlier work on periodic tables going on in the 1860s, he made the following table:\\r\\n\\r\\nBy adding additional elements following this pattern, Mendeleev developed his extended version of the periodic table.[40][41] On 6 March 1869, he made a formal presentation to the Russian Chemical Society, titled The Dependence between the Properties of the Atomic Weights of the Elements, which described elements according to both atomic weight and valence.[42][43] This presentation stated that\\r\\n\\r\\nMendeleev published his periodic table of all known elements and predicted several new elements to complete the table in a Russian-language journal. Only a few months after, Meyer published a virtually identical table in a German-language journal.[44][45]   Mendeleev has the distinction of accurately predicting the qualities of what he called ekasilicon, ekaaluminium and ekaboron (germanium, gallium and scandium, respectively).\\r\\n\\r\\nFor his predicted eight elements, he used the prefixes of eka, dvi, and tri (Sanskrit one, two, three) in their naming.  Mendeleev questioned some of the currently accepted atomic weights (they could be measured only with a relatively low accuracy at that time), pointing out that they did not correspond to those suggested by his Periodic Law. He noted that tellurium has a higher atomic weight than iodine, but he placed them in the right order, incorrectly predicting that the accepted atomic weights at the time were at fault. He was puzzled about where to put the known lanthanides, and predicted the existence of another row to the table which were the actinides which were some of the heaviest in atomic mass. Some people dismissed Mendeleev for predicting that there would be more elements, but he was proven to be correct when Ga (gallium) and Ge (germanium) were found in 1875 and 1886 respectively, fitting perfectly into the two missing spaces.[46]\\r\\n\\r\\nBy giving Sanskrit names to his \\"missing\\" elements, Mendeleev showed his appreciation and debt to the Sanskrit grammarians of ancient India, who had created sophisticated theories of language based on their discovery of the two-dimensional patterns in basic sounds. Mendeleev was a friend and colleague of the Sanskritist B?htlingk, who was preparing the second edition of his book on P?ini[47] at about this time, and Mendeleev wished to honor P?ini with his nomenclature.[48] Noting that there are striking similarities between the periodic table and the introductory ?iva Stras in P?ini's grammar, Prof. Kiparsky says:\\r\\n\\r\\n\\"[T]he analogies between the two systems are striking. Just as Panini found that the phonological patterning of sounds in the language is a function of their articulatory properties, so Mendeleev found that the chemical properties of elements are a function of their atomic weights.\\r\\n\\r\\nLike Panini, Mendeleev arrived at his discovery through a search for the grammar of the elements (using what he called the principle of isomorphism, and looking for general formulas to generate the possible chemical compounds).\\r\\n\\r\\nJust as Panini arranged the sounds in order of increasing phonetic complexity (e.g. with the simple stops k,p preceding the other stops, and representing all of them in expressions like kU, pU) so Mendeleev arranged the elements in order of increasing atomic weights, and called the first row (oxygen, nitrogen, carbon etc.) typical (or representative) elements.\\r\\n\\r\\nJust as Panini broke the phonetic parallelism of sounds when the simplicity of the system required it, e.g. putting the velar to the right of the labial in the nasal row, so Mendeleev gave priority to isomorphism over atomic weights when they conflicted, e.g. putting beryllium in the magnesium family because it patterns with it even though by atomic weight it seemed to belong with nitrogen and phosphorus. In both cases, the periodicities they discovered would later be explained by a theory of the internal structure of the elements.[49]\\"\\r\\n\\r\\nThe original draft made by Mendeleev would be found years later and published under the name Tentative System of Elements.[50]\\r\\n\\r\\nDmitri Mendeleev is often referred to as the Father of the Periodic Table. He called his table or matrix, \\"the Periodic System\\".[51]\\r\\n\\r\\nMendeleev made other important contributions to chemistry. The Russian chemist and science historian Lev Chugaev has characterized him as \\"a chemist of genius, first-class physicist, a fruitful researcher in the fields of hydrodynamics, meteorology, geology, certain branches of chemical technology (explosives, petroleum, and fuels, for example) and other disciplines adjacent to chemistry and physics, a thorough expert of chemical industry and industry in general, and an original thinker in the field of economy.\\" Mendeleev was one of the founders, in 1869, of the Russian Chemical Society. He worked on the theory and practice of protectionist trade and on agriculture.\\r\\n\\r\\nIn an attempt at a chemical conception of the Aether, he put forward a hypothesis that there existed two inert chemical elements of lesser atomic weight than hydrogen.[29] Of these two proposed elements, he thought the lighter to be an all-penetrating, all-pervasive gas, and the slightly heavier one to be a proposed element, coronium.\\r\\n\\r\\nMendeleev devoted much study and made important contributions to the determination of the nature of such indefinite compounds as solutions.\\r\\n\\r\\nIn another department of physical chemistry, he investigated the expansion of liquids with heat, and devised a formula similar to Gay-Lussac's law of the uniformity of the expansion of gases, while in 1861 he anticipated Thomas Andrews' conception of the critical temperature of gases by defining the absolute boiling-point of a substance as the temperature at which cohesion and heat of vaporization become equal to zero and the liquid changes to vapor, irrespective of the pressure and volume.[29]\\r\\n\\r\\nMendeleev is given credit for the introduction of the metric system to the Russian Empire.\\r\\n\\r\\nHe invented pyrocollodion, a kind of smokeless powder based on nitrocellulose. This work had been commissioned by the Russian Navy, which however did not adopt its use. In 1892 Mendeleev organized its manufacture.\\r\\n\\r\\nMendeleev studied petroleum origin and concluded hydrocarbons are abiogenic and form deep within the earth ÿ see Abiogenic petroleum origin.\\r\\nHe wrote: \\"The capital fact to note is that petroleum was born in the depths of the earth, and it is only there that we must seek its origin.\\" (Dmitri Mendeleev, 1877)[52]\\r\\n\\r\\nA very popular Russian story is that it was Mendeleev who came up with the 40% standard strength of vodka in 1894, after having been appointed Director of the Bureau of Weights and Measures with the assignment to formulate new state standards for the production of vodka. This story has, for instance, been used in marketing claims by the Russian Standard vodka brand that, \\"In 1894, Dmitri Mendeleev, the greatest scientist in all Russia, received the decree to set the Imperial quality standard for Russian vodka and the 'Russian Standard' was born\\",[53] or that the vodka is \\"compliant with the highest quality of Russian vodka approved by the royal government commission headed by Mendeleev in 1894.\\"[54]\\r\\n\\r\\nWhile it is true that Mendeleev in 1892 became head of the Archive of Weights and Measures in Saint Petersburg, and evolved it into a government bureau the following year, that institution was never involved in setting any production quality standards, but was issued with standardising Russian trade weights and measuring instruments. Furthermore, the 40% standard strength was already introduced by the Russian government in 1843, when Mendeleev was nine years old.[54]\\r\\n\\r\\nThe basis for the whole story is a popular myth that Mendeleev's 1865 doctoral dissertation \\"A Discourse on the combination of alcohol and water\\" contained a statement that 38% is the ideal strength of vodka, and that this number was later rounded to 40% to simplify the calculation of alcohol tax. However, Mendeleev's dissertation was about alcohol concentrations over 70% and he never wrote anything about vodka.[54][55][56]\\r\\n\\r\\nA number of places and objects are associated with the name and achievements of the scientist.\\r\\n\\r\\nIn Saint Petersburg his name was given to the National Metrology Institute[57] dealing with establishing and supporting national and worldwide standards for precise measurements. Next to it there is a monument to him that consists of his sitting statue and a depiction of his periodic table on the wall of the establishment.\\r\\n\\r\\nIn the Twelve Collegia building, now being the centre of Saint Petersburg State University and in Mendeleev's time ÿ Head Pedagogical Institute ÿ there is Dmitry Mendeleev's Memorial Museum Apartment[58] with his archives. The street in front of these is named after him as Mendeleevskaya liniya  (Mendeleev Line).\\r\\n\\r\\nIn Moscow, there is the D. Mendeleyev University of Chemical Technology of Russia.[59]\\r\\n\\r\\nAfter him was also named mendelevium, which is a synthetic chemical element with the symbol Md (formerly Mv) and the atomic number 101. It is a metallic radioactive transuranic element in the actinide series, usually synthesized by bombarding einsteinium with alpha particles.\\r\\n\\r\\nThe newly discovered mineral mendeleevite-Ce, Cs6(Ce22Ca6)(Si70O175)(OH,F)14(H2O)21, was named in Mendeleev's honor in 2010.[60]  The related species mendeleevite-Nd, Cs6[(Nd,REE)23Ca7](Si70O175)(OH,F)19(H2O)16, was described in 2015.[61]\\r\\n\\r\\nA large lunar impact crater Mendeleev that is located on the far side of the Moon, as seen from the Earth, also bears the name of the scientist.\\r\\n\\r\\nThe Russian Academy of Sciences has occasionally awarded a Mendeleev Golden Medal since 1965.[62]","input":"Who is credited with developing the periodic table?"},{"output":"2,430 games, plus the postseason","context":"The Major League Baseball (MLB) season schedule consists of 162 games for each of the 30 teams in the American League (AL) and National League (NL), played over approximately six monthsa total of 2,430 games, plus the postseason. The regular season runs from late March/early April to late September/early October, followed by the postseason which can run to early November. The season begins with the official Opening Day, and, as of 2018, runs 26? weeks through the last Sunday of September or first Sunday of October. One or more International Opener games may be scheduled outside the United States before the official Opening Day.[1] It is possible for a given team to play a maximum of 20 games in the postseason in a given year, provided the team is a wild card and advances to each of the Division Series, Championship Series, and World Series with each series going the distance (5 games in the Division Series, 7 games each in the League Championship Series/World Series).\\r\\nThe regular season is constructed from series. Due to travel concerns and the sheer number of games, pairs of teams are never scheduled to play single games against each other (except in the instance of making up a postponed game); instead they play games on several consecutive days in the same ballpark. Most often the series are of three or four games, but two-game series are also scheduled. Teams play one mid-week series and one weekend series per week. Depending on the length of the series, mid-week series games are usually scheduled between Monday and Thursday, while weekend games are scheduled between Thursday and Monday. Beginning in 2018, teams start and end their season on a weekend for a total of 26? weeks. Due to the mid-week all-star break in July, teams are scheduled to play 27 weekend series and 25 mid-week series for a total of 52 series (24 divisional series, 20 inter-divisional series, 8 inter-league series). A team's road games are usually grouped into a multi-series road trip; home series are grouped into homestands.\\r\\nNote that rainouts and other cancellations are often rescheduled ad hoc during the season, sometimes as doubleheaders. However, if two teams are scheduled to meet for the final time in the last two weeks of the season, and the game is cancelled, it may not be rescheduled if there is no impact on the divisional or wild card races. For example, in 2016, the September 29 game between the Cleveland Indians and Detroit Tigers was cancelled due to rain because the teams were unable to reschedule a make-up date before the end of the season on October 2, and it did not affect the divisional race. In contrast, a 2008 AL Central division game between Detroit and the Chicago White Sox needed to be made up following the last day of the regular season because it affected a division race involving the White Sox and the Minnesota Twins.\\r\\n\\r\\n\\r\\nThis account gives the length of the major league \\"championship season\\" schedule by league and year. It does not cover the curtailment of play by war (1918) or by strikes and lockouts (1972, 1981, 1994). The schedules for 1995 were revised and shortened from 162 to 144 games, after late resolution of the strike that had begun in 1994 required a delay in the season to accommodate limited spring training.\\r\\nThe listed years are those in which the league revised its schedule. For example, the National League (NL) scheduled 84 games during 1879, 1880, 1881, and 1882 ÿ that is, four seasons from 1879, ending before 1883, the next listing. 1876 is listed here for convenience although the NL did not schedule games (see 1871 to 1876, below).\\r\\n1882 ÿ 1891\\r\\nThus the AA expanded its schedule to 140 games two years before the National League did so. After 1891 four AA clubs joined the NL and four were bought out, nominally creating one big league, the \\"National League and American Association\\" of 12 clubs.\\r\\n1884\\r\\n1890\\r\\n1914: 1915\\r\\nThe National Association of Professional Base Ball Players (1871ÿ1875) did not schedule games, nor did it control the number of teams, a major reason for its demise after the 1875 season. Clubs paid a $10 entry fee, later $20, to enter the Association for one season, and thereby declare for that year's national championship. Without continuing membership or heavy investment, there was little to deter a team from breaking a commitment, and though it happened, it was mainly due to clubs going out of business.\\r\\nThe National League organized for 1876 on a different basis, granting exclusive memberships to eight clubs that would continue from year to year  it was generally expected, if only because membership would be profitable. But the new league followed its predecessor in merely agreeing that each club would play a certain number of matches to a decision (excluding ties) by a certain date. Boston played 70 games with its quota of ten decisions against every rival. The others achieved 56 to 68 decisions, 64 to 66 for the four western teams as the teams from New York and Philadelphia (eastern) abandoned their schedule-concluding road trips.\\r\\nFor all six early seasons, prior to the first league schedule in 1877, member clubs scheduled their own matches by mutual arrangement, including championship games necessarily with member clubs, other games with members, and games with non-member clubs. Some may have practically dictated their arrangements with some others, but there was no central control or coordination.\\r\\nThis listing gives the greatest number of games played by any club for each season. Naturally, the leader by games played was always a strong club fielding one of the better gate attractions.\\r\\nThe leading numbers of games played to a decision were 33, 54, 59, 71, 82, and 70 decisions; by the listed teams except the Mutuals in 1872.\\r\\nSince 1998, there have been 30 major league teams with a single advance schedule for every season that comprises 2430 games. Each team plays 162 games, 81 as the \\"home\\" team, 81 as the \\"visitor\\". (This is true even on the rare occasion when a game is played at a ballpark not home to either team.) Occasionally, the advance schedule is subsequently altered due to a game postponement or a one-game tie-breaker to determine which team will play in the postseason.\\r\\nBefore 2013 the schedule included 252 \\"interleague games\\" that matched one team from the American League and one from the National League; the other 2178 games matched a pair from within one league. About half of the latter matched teams from within one division and about half matched teams from different divisions in one league. In the Central Division of the National League, which alone had six teams, every pair of division rivals played 15 or 16 games. Within the other, smaller divisions every pair of teams played 18 or 19 games.\\r\\nDivision games (1091). There are 61 pairs of teams from within one division.\\r\\nOther intraleague games (1087). There are 150 pairs of teams from two different divisions within one league.\\r\\nThe schedule for interleague play comprised 84 three-game series in each season from 1998 to 2012, divided as six series (18 games) for each of fourteen AL teams and as many as six for each of sixteen NL teams.\\r\\nAmong the 224 interleague pairs of teams, 11 played six games every year, which were scheduled in two three-game series \\"home and home\\", or one at each home ballpark. Five of these 11 special arrangements matched two teams in the same city or in neighboring cities, where they wholly or partly share territorial rights. Six were regional matches at greater distance, four of which were in the same state.\\r\\nThese special local and regional series accounted for 66 interleague games annually from 1998-2012, and the other 186 games were determined by rotation.\\r\\nThe 2001 season was suspended for one week due to the September 11 terrorist attacks and resulting disruptions in travel, resulting in games scheduled for September 11ÿ16 being rescheduled to the first week of October and the playoffs and World Series being rescheduled one week later than their originally planned dates, which resulted in the World Series continuing into early November.\\r\\nSchedule changes for 2013, precipitated by realignment that created two equal-sized leagues of 15 teams each, gave every team 20 interleague games.[2] Sixteen of which were determined by a match of divisions, one from each league; all teams in a given division play all teams in a given division from the other league. (Each plays a three-game series against four teams from the designated division and two two-game series against the remaining team.)\\r\\nThe matched divisions rotate annually:\\r\\nEach team played its four other interleague games against a designated \\"natural rival\\", with two games in each club's city. Thus all 30 teams, rather than 22 of 30 as previously, were deemed to have a natural rival in the other league. In 2013 the natural rivalry games were all scheduled for May 27 to May 30 (Memorial Day weekend) but in 2014 their scheduled dates range from May to August.\\r\\nTen of the natural rivalries from 2012 and earlier continued, while the HoustonÿTexas \\"Lone Star\\" rivalry had been transformed into an intra-division one with 19 games played. Five of the special arrangements were new in 2013 , including one each for Houston and Texas.\\r\\nFor 2014, four (4) of the five (5) new rivalries have been revised (?), all except Detroit and Pittsburgh.\\r\\nEvery team now plays 19 games against each of 4 opponents within its division (76 games), and 6 or 7 games against each of 10 opponents from other divisions within its own league (66 games).\\r\\nWhen corresponding divisions (i.e. NL East vs. AL East) play each other, a slight adjustment was made to the interleague games. Teams now play 6 games against their rival and 4 games (home and home) against two opponents plus one home and one away 3 game series (14 total) against the other four teams in the opposing division. This was done in 2015, and will next occur in 2018.\\r\\nUnder the new collective bargaining agreement reached in December 2016, several changes were done to the scheduling pattern for the 2018 season. The overall length of the season has been extended to 187 days with the addition of four off-days for all teams. All teams will play on Opening Day, which for 2018 will be held on March 29. Sunday Night Baseball will no longer be played on the final Sunday before the All-Star Game, in order to ease travel time for those who are participating in the Home Run Derby. A single, nationally televised afternoon game will be played the following Thursday, with all other teams returning to play on Friday.[3][4]\\r\\nStart of Major League Baseball games depends on days of the week, game number in series, holidays, and other factors. Most games start at 7pm in the local time zone, so there are more night games than day games even though baseball is traditionally played during the day. The reason why there are more night baseball games is to attract more fans to ballparks as well as viewers from home because most fans would be at work or school during the day. On Mondays (excluding Opening Day and holidays), Tuesdays, and Fridays, games are almost exclusively played at night except for Cubs home games. Getaway days, days on which teams play their last game of the series before departing for another series in another city the next day, are usually day games, mainly Sundays, Wednesdays, and Thursdays. On Sundays, usually all but one are day games, with the final game reserved for ESPN's Sunday Night Baseball.\\r\\nAbout half of Saturday games are day games (1, 2 or 4pm ET). In some markets, Saturday night start an hour earlier than usual night start times, but other cities start Saturday night games at the same time as weeknight games. In conclusion, weekday games are only played at night except for getaway days while many weekend games are played during the day.\\r\\nThe initial pitch typically occurs 5 or 10 minutes after the hour, in order to allow time for pre-game ceremonies.","input":"How many games in major league baseball season?"},{"output":"10 September 2008","context":"The Large Hadron Collider (LHC) is the world's largest and most powerful particle collider, the most complex experimental facility ever built, and the largest single machine in the world.[1] It was built by the European Organization for Nuclear Research (CERN) between 1998 and 2008 in collaboration with over 10,000 scientists and engineers from over 100 countries, as well as hundreds of universities and laboratories.[2] It lies in a tunnel 27 kilometres (17?mi) in circumference, as deep as 175 metres (574?ft) beneath the FranceÿSwitzerland border near Geneva, Switzerland. Its first research run took place from March 2010 to early 2013 at an energy of 3.5 to 4?teraelectronvolts (TeV) per beam (7 to 8?TeV total), about 4 times the previous world record for a collider.[3][4] Afterwards, the accelerator was upgraded for two years. It was restarted in early 2015 for its second research run, reaching 6.5 TeV per beam (13 TeV total, the current world record).[5][6][7][8]\\r\\nThe aim of the LHC is to allow physicists to test the predictions of different theories of particle physics, including measuring the properties of the Higgs boson[9] and searching for the large family of new particles predicted by supersymmetric theories,[10] as well as other unsolved questions of physics.\\r\\nThe collider has four crossing points, around which are positioned seven detectors, each designed for certain kinds of research. The LHC primarily collides proton beams, but it can also use beams of heavy ions. Protonÿlead collisions were performed for short periods in 2013 and 2016, leadÿlead collisions took place in 2010, 2011, 2013, and 2015, and a short run of xenonÿxenon collisions took place in 2017.[11]\\r\\nThe LHC's computing grid is a world record holder. Data from collisions was produced at an unprecedented rate for the time of first collisions, tens of petabytes per year, a major challenge at the time, to be analysed by a grid-based computer network infrastructure connecting 170 computing centres in 42 countries as of 2017[12][13] ÿ by 2012 the Worldwide LHC Computing Grid was also the world's largest distributed computing grid, comprising over 170 computing facilities in a worldwide network across 36 countries.[14][15][16]\\r\\n\\r\\n\\r\\nThe term hadron refers to composite particles composed of quarks held together by the strong force (as atoms and molecules are held together by the electromagnetic force).[17] The best-known hadrons are the baryons such as protons and neutrons; hadrons also include mesons such as the pion and kaon, which were discovered during cosmic ray experiments in the late 1940s and early 1950s.[18]\\r\\nA collider is a type of a particle accelerator with two directed beams of particles. In particle physics, colliders are used as a research tool: they accelerate particles to very high kinetic energies and let them impact other particles.[19] Analysis of the byproducts of these collisions gives scientists good evidence of the structure of the subatomic world and the laws of nature governing it. Many of these byproducts are produced only by high-energy collisions, and they decay after very short periods of time. Thus many of them are hard or nearly impossible to study in other ways.[20]\\r\\nPhysicists hope that the Large Hadron Collider will help answer some of the fundamental open questions in physics, concerning the basic laws governing the interactions and forces among the elementary objects, the deep structure of space and time, and in particular the interrelation between quantum mechanics and general relativity.\\r\\nData is also needed from high-energy particle experiments to suggest which versions of current scientific models are more likely to be correct ÿ in particular to choose between the Standard Model and Higgsless model and to validate their predictions and allow further theoretical development. Many theorists expect new physics beyond the Standard Model to emerge at the TeV energy level, as the Standard Model appears to be unsatisfactory. Issues explored by LHC collisions include:[21][22]\\r\\nOther open questions that may be explored using high-energy particle collisions:\\r\\nThe LHC is the world's largest and highest-energy particle accelerator.[35][36] The collider is contained in a circular tunnel, with a circumference of 27 kilometres (17?mi), at a depth ranging from 50 to 175 metres (164 to 574?ft) underground.\\r\\nThe 3.8-metre (12?ft) wide concrete-lined tunnel, constructed between 1983 and 1988, was formerly used to house the Large ElectronÿPositron Collider.[37] It crosses the border between Switzerland and France at four points, with most of it in France. Surface buildings hold ancillary equipment such as compressors, ventilation equipment, control electronics and refrigeration plants.\\r\\nThe collider tunnel contains two adjacent parallel beamlines (or beam pipes) each containing a beam, which travel in opposite directions around the ring. The beams intersect at four points around the ring, which is where the particle collisions take place. Some 1,232 dipole magnets keep the beams on their circular path (see image[38]), while an additional 392 quadrupole magnets are used to keep the beams focused, with stronger quadrupole magnets close to the intersection points in order to maximize the chances of interaction where the two beams cross. Magnets of higher multipole orders are used to correct smaller imperfections in the field geometry. In total, about 10,000 superconducting magnets are installed, with the dipole magnets having a mass of over 27 tonnes.[39] Approximately 96 tonnes of superfluid helium-4 is needed to keep the magnets, made of copper-clad niobium-titanium, at their operating temperature of 1.9?K (?271.25?C), making the LHC the largest cryogenic facility in the world at liquid helium temperature.\\r\\nWhen running at the current energy record of 6.5 TeV per proton,[40] once or twice a day, as the protons are accelerated from 450?GeV to 6.5?TeV, the field of the superconducting dipole magnets will be increased from 0.54 to 7.7 teslas (T). The protons each have an energy of 6.5?TeV, giving a total collision energy of 13?TeV. At this energy the protons have a Lorentz factor of about 6,930 and move at about 0.999999990?c, or about 3.1?m/s (11?km/h) slower than the speed of light (c). It takes less than 90 microseconds (s) for a proton to travel once around the main ring?ÿ a speed of about 11,000 revolutions per second. Rather than having continuous beams, the protons are bunched together, into up to 2,808 bunches, with 115 billion protons in each bunch so that interactions between the two beams take place at discrete intervals, mainly 25 nanoseconds (ns) apart, providing a bunch collision rate of 40?MHz. It was operated with fewer bunches in the first years. The design luminosity of the LHC is 1034 cm?2s?1,[41] which was first reached in June 2016.[42] By 2017 twice this value was achieved.[43]\\r\\nPrior to being injected into the main accelerator, the particles are prepared by a series of systems that successively increase their energy. The first system is the linear particle accelerator LINAC 2 generating 50-MeV protons, which feeds the Proton Synchrotron Booster (PSB). There the protons are accelerated to 1.4 GeV and injected into the Proton Synchrotron (PS), where they are accelerated to 26 GeV. Finally the Super Proton Synchrotron (SPS) is used to further increase their energy to 450 GeV before they are at last injected (over a period of several minutes) into the main ring. Here the proton bunches are accumulated, accelerated (over a period of 20 minutes) to their peak energy, and finally circulated for 5 to 24 hours while collisions occur at the four intersection points.[44]\\r\\nThe LHC physics programme is mainly based on protonÿproton collisions. However, shorter running periods, typically one month per year, with heavy-ion collisions are included in the programme. While lighter ions are considered as well, the baseline scheme deals with lead ions[45] (see A Large Ion Collider Experiment). The lead ions are first accelerated by the linear accelerator LINAC 3, and the Low Energy Ion Ring (LEIR) is used as an ion storage and cooler unit. The ions are then further accelerated by the PS and SPS before being injected into LHC ring, where they reached an energy of 2.3 TeV per nucleon (or 522 TeV per ion),[46] higher than the energies reached by the Relativistic Heavy Ion Collider. The aim of the heavy-ion programme is to investigate quarkÿgluon plasma, which existed in the early universe.\\r\\nSeven detectors have been constructed at the LHC, located underground in large caverns excavated at the LHC's intersection points. Two of them, the ATLAS experiment and the Compact Muon Solenoid (CMS), are large, general purpose particle detectors.[36] ALICE and LHCb have more specific roles and the last three, TOTEM, MoEDAL and LHCf, are very much smaller and are for very specialized research. The BBC's summary of the main detectors is:[47]\\r\\nData produced by LHC, as well as LHC-related simulation, was estimated at approximately 15 petabytes per year (max throughput while running not stated)[48] - a major challenge in its own right at the time.\\r\\nThe LHC Computing Grid[49] was constructed as part of the LHC design, to handle the massive amounts of data expected for its collisions. It is an international collaborative project that consists of a grid-based computer network infrastructure initially connecting 140 computing centres in 35 countries (over 170 in 36 countries as of 2012). It was designed by CERN to handle the significant volume of data produced by LHC experiments,[12][13] incorporating both private fibre optic cable links and existing high-speed portions of the public Internet to enable data transfer from CERN to academic institutions around the world.[50] The Open Science Grid is used as the primary infrastructure in the United States, and also as part of an interoperable federation with the LHC Computing Grid.\\r\\nThe distributed computing project LHC@home was started to support the construction and calibration of the LHC. The project uses the BOINC platform, enabling anybody with an Internet connection and a computer running Mac OS X, Windows or Linux, to use their computer's idle time to simulate how particles will travel in the beam pipes. With this information, the scientists are able to determine how the magnets should be calibrated to gain the most stable \\"orbit\\" of the beams in the ring.[51] In August 2011, a second application went live (Test4Theory) which performs simulations against which to compare actual test data, to determine confidence levels of the results.\\r\\nBy 2012 data from over 6 quadrillion (6 x 1015) LHC proton-proton collisions had been analysed,[52] LHC collision data was being produced at approximately 25 petabytes per year, and the LHC Computing Grid had become the world's largest computing grid (as of 2012), comprising over 170 computing facilities in a worldwide network across 36 countries.[14][15][16]\\r\\nThe LHC first went live on 10 September 2008,[53] but initial testing was delayed for 14 months from 19 September 2008 to 20 November 2009, following a magnet quench incident that caused extensive damage to over 50 superconducting magnets, their mountings, and the vacuum pipe.[54][55][56][57][58]\\r\\nDuring its first run (2010ÿ2013) the LHC collided two opposing particle beams of either protons at up to 4?teraelectronvolts (4 TeV or 0.64 microjoules), or lead nuclei (574?TeV per nucleus, or 2.76 TeV per nucleon).[35][59] Its first run discoveries included the long sought Higgs boson, several composite particles (hadrons) like the b (3P) bottomonium state, the first creation of a quarkÿgluon plasma, and the first observations of the very rare decay of the Bs meson into two muons (Bs0 L +?), which challenged the validity of existing models of supersymmetry.[60]\\r\\nThe size of the LHC constitutes an exceptional engineering challenge with unique operational issues on account of the amount of energy stored in the magnets and the beams.[44][61] While operating, the total energy stored in the magnets is 10?GJ (2,400 kilograms of TNT) and the total energy carried by the two beams reaches 724?MJ (173 kilograms of TNT).[62]\\r\\nLoss of only one ten-millionth part (10?7) of the beam is sufficient to quench a superconducting magnet, while each of the two beam dumps must absorb 362?MJ (87 kilograms of TNT). These energies are carried by very little matter: under nominal operating conditions (2,808 bunches per beam, 1.15G1011 protons per bunch), the beam pipes contain 1.0G10?9 gram of hydrogen, which, in standard conditions for temperature and pressure, would fill the volume of one grain of fine sand.\\r\\nWith a budget of ?7.5 billion (approx. $9bn or S6.19bn as of June?2010), the LHC is one of the most expensive scientific instruments[63] ever built.[64] The total cost of the project is expected to be of the order of 4.6bn Swiss francs (SFr) (approx. $4.4bn, ?3.1bn, or S2.8bn as of Jan?2010) for the accelerator and 1.16bn (SFr) (approx. $1.1bn, ?0.8bn, or S0.7bn as of Jan?2010) for the CERN contribution to the experiments.[65]\\r\\nThe construction of LHC was approved in 1995 with a budget of SFr 2.6bn, with another SFr 210M towards the experiments. However, cost overruns, estimated in a major review in 2001 at around SFr 480M for the accelerator, and SFr 50M for the experiments, along with a reduction in CERN's budget, pushed the completion date from 2005 to April 2007.[66] The superconducting magnets were responsible for SFr 180M of the cost increase. There were also further costs and delays due to engineering difficulties encountered while building the underground cavern for the Compact Muon Solenoid,[67] and also due to magnet supports which were insufficiently strongly designed and failed their initial testing (2007) and damage from a magnet quench and liquid helium escape (inaugural testing, 2008) (see: Construction accidents and delays).[68] Due to lower electricity costs during the summer, the LHC normally does not operate over the winter months,[69] although exceptions over the 2009/10 and 2012/2013 winters were made to make up for the 2008 start-up delays and to improve precision of measurements of the new particle discovered in 2012, respectively.\\r\\nIn both of its runs (2010 to 2012 and 2015), the LHC was initially run at energies below its planned operating energy, and ramped up to just 2 x 4 TeV energy on its first run and 2 x 6.5 TeV on its second run, below the design energy of 2 x 7 TeV. This is because massive superconducting magnets require considerable magnet training to handle the high currents involved without losing their superconducting ability, and the high currents are necessary to allow a high proton energy. The \\"training\\" process involves repeatedly running the magnets with lower currents to provoke any quenches or minute movements that may result. It also takes time to cool down magnets to their operating temperature of around 1.9 K (close to absolute zero). Over time the magnet \\"beds in\\" and ceases to quench at these lesser currents and can handle the full design current without quenching; CERN media describe the magnets as \\"shaking out\\" the unavoidable tiny manufacturing imperfections in their crystals and positions that had initially impaired their ability to handle their planned currents. The magnets over time and with training, gradually become able to handle their full planned currents without quenching.[78][79]\\r\\nThe first beam was circulated through the collider on the morning of 10 September 2008.[47] CERN successfully fired the protons around the tunnel in stages, three kilometres at a time. The particles were fired in a clockwise direction into the accelerator and successfully steered around it at 10:28 local time.[53] The LHC successfully completed its major test: after a series of trial runs, two white dots flashed on a computer screen showing the protons travelled the full length of the collider. It took less than one hour to guide the stream of particles around its inaugural circuit.[80] CERN next successfully sent a beam of protons in an anticlockwise direction, taking slightly longer at one and a half hours due to a problem with the cryogenics, with the full circuit being completed at 14:59.\\r\\nOn 19 September 2008, a magnet quench occurred in about 100 bending magnets in sectors 3 and 4, where an electrical fault led to a loss of approximately six tonnes of liquid helium (the magnets' cryogenic coolant), which was vented into the tunnel. The escaping vapour expanded with explosive force, damaging total 53 superconducting magnets and their mountings, and contaminating the vacuum pipe, which also lost vacuum conditions.[54][55][81]\\r\\nShortly after the incident CERN reported that the most likely cause of the problem was a faulty electrical connection between two magnets, and that?ÿ due to the time needed to warm up the affected sectors and then cool them back down to operating temperature?ÿ it would take at least two months to fix.[82] CERN released an interim technical report[81] and preliminary analysis of the incident on 15 and 16 October 2008 respectively,[83] and a more detailed report on 5 December 2008.[75] The analysis of the incident by CERN confirmed that an electrical fault had indeed been the cause. The faulty electrical connection had led (correctly) to a failsafe power abort of the electrical systems powering the superconducting magnets, but had also caused an electric arc (or discharge) which damaged the integrity of the supercooled helium's enclosure and vacuum insulation, causing the coolant's temperature and pressure to rapidly rise beyond the ability of the safety systems to contain it,[81] and leading to a temperature rise of about 100 degrees Celsius in some of the affected magnets. Energy stored in the superconducting magnets and electrical noise induced in other quench detectors also played a role in the rapid heating. Around two tonnes of liquid helium escaped explosively before detectors triggered an emergency stop, and a further four tonnes leaked at lower pressure in the aftermath.[81] A total of 53 magnets were damaged in the incident and were repaired or replaced during the winter shutdown.[84] This accident was thoroughly discussed in a 22?February 2010 Superconductor Science and Technology article by CERN physicist Lucio Rossi.[85]\\r\\nIn the original timeline of the LHC commissioning, the first \\"modest\\" high-energy collisions at a centre-of-mass energy of 900 GeV were expected to take place before the end of September 2008, and the LHC was expected to be operating at 10 TeV by the end of 2008.[86] However, due to the delay caused by the above-mentioned incident, the collider was not operational until November 2009.[87] Despite the delay, LHC was officially inaugurated on 21 October 2008, in the presence of political leaders, science ministers from CERN's 20 Member States, CERN officials, and members of the worldwide scientific community.[88]\\r\\nMost of 2009 was spent on repairs and reviews from the damage caused by the quench incident, along with two further vacuum leaks identified in July 2009 which pushed the start of operations to November of that year.[77]\\r\\nOn 20 November 2009, low-energy beams circulated in the tunnel for the first time since the incident, and shortly after, on 30 November, the LHC achieved 1.18?TeV per beam to become the world's highest-energy particle accelerator, beating the Tevatron's previous record of 0.98?TeV per beam held for eight years.[90]\\r\\nThe early part of 2010 saw the continued ramp-up of beam in energies and early physics experiments towards 3.5?TeV per beam and on 30 March 2010, LHC set a new record for high-energy collisions by colliding proton beams at a combined energy level of 7?TeV. The attempt was the third that day, after two unsuccessful attempts in which the protons had to be \\"dumped\\" from the collider and new beams had to be injected.[91] This also marked the start of the main research programme.\\r\\nThe first proton run ended on 4 November 2010. A run with lead ions started on 8 November 2010, and ended on 6 December 2010,[92] allowing the ALICE experiment to study matter under extreme conditions similar to those shortly after the Big Bang.[93]\\r\\nCERN originally planned that the LHC would run through to the end of 2012, with a short break at the end of 2011 to allow for an increase in beam energy from 3.5 to 4?TeV per beam.[4] At the end of 2012 the LHC was planned to get shut down until around 2015 to allow upgrade to a planned beam energy of 7?TeV per beam.[94] In late 2012, in light of the July 2012 discovery of the Higgs boson, the shutdown was postponed for some weeks into early 2013, to allow additional data to be obtained prior to shutdown.\\r\\nThe LHC was shut down on 13 February 2013 for its 2-year upgrade, which would touch on many aspects of the LHC: enabling collisions at 14 TeV, enhancing its detectors and pre-accelerators (the Proton Synchrotron and Super Proton Synchrotron), as well as replacing its ventilation system and 100?km of cabling impaired by high-energy collisions from its first run.[95] The upgraded collider began its long start-up and testing process in June 2014, with the Proton Synchrotron Booster starting on 2 June 2014, the final interconnection between magnets completing and the Proton Synchrotron circulating particles on 18 June 2014, and the first section of the main LHC supermagnet system reaching operating temperature of 1.9?K (?271.25?C), a few days later.[96] Due to the slow progress with \\"training\\" the superconducting magnets, it was decided to start the second run with a lower energy of 6.5 TeV per beam, corresponding to a current of 11,000 amperes. The first of the main LHC magnets were reported to have been successfully trained by 9 December 2014, while training the other magnet sectors was finished in March 2015.[97]\\r\\nOn 5 April 2015, the LHC restarted after a two-year break, during which the electrical connectors between the bending magnets were upgraded to safely handle the current required for 7 TeV per beam (14 TeV).[5][98] However, the bending magnets were only trained to handle up to 6.5 TeV per beam (13 TeV total), which became the operating energy for 2015 to 2017.[99] The energy was first reached on 10 April 2015.[100] The upgrades culminated in colliding protons together with a combined energy of 13?TeV.[101] On 3 June 2015 the LHC started delivering physics data after almost two years offline.[102] In the following months it was used for proton-proton collisions, while in November the machine switched to collisions of lead ions and in December the usual winter shutdown started.\\r\\nIn 2016, the machine operators focused on increasing the luminosity for proton-proton collisions. The design value was first reached 29 June,[42] and further improvements increased the collision rate to 40% above the design value.[103] The total number of collisions in 2016 exceeded the number from Run 1 - at a higher energy per collision. The proton-proton run was followed by four weeks of proton-lead collisions.[104]\\r\\nIn 2017 the luminosity could be increased further and reached twice the design value. The total number of collisions was higher than in 2016 as well.[43]\\r\\n20 May 2015\\r\\nAn initial focus of research was to investigate the possible existence of the Higgs boson, a key part of the Standard Model of physics which is predicted by theory but had not yet been observed before due to its high mass and elusive nature. CERN scientists estimated that, if the Standard Model were correct, the LHC would produce several Higgs bosons every minute, allowing physicists to finally confirm or disprove the Higgs boson's existence. In addition, the LHC allowed the search for supersymmetric particles and other hypothetical particles as possible unknown areas of physics.[35] Some extensions of the Standard Model predict additional particles, such as the heavy W' and Z' gauge bosons, which are also estimated to be within reach of the LHC to discover.[118]\\r\\nThe first physics results from the LHC, involving 284 collisions which took place in the ALICE detector, were reported on 15 December 2009.[105] The results of the first protonÿproton collisions at energies higher than Fermilab's Tevatron protonÿantiproton collisions were published by the CMS collaboration in early February 2010, yielding greater-than-predicted charged-hadron production.[119]\\r\\nAfter the first year of data collection, the LHC experimental collaborations started to release their preliminary results concerning searches for new physics beyond the Standard Model in proton-proton collisions.[120][121][122][123] No evidence of new particles was detected in the 2010 data. As a result, bounds were set on the allowed parameter space of various extensions of the Standard Model, such as models with large extra dimensions, constrained versions of the Minimal Supersymmetric Standard Model, and others.[124][125][126]\\r\\nOn 24 May 2011, it was reported that quarkÿgluon plasma (the densest matter thought to exist besides black holes) had been created in the LHC.[108]\\r\\nBetween July and August 2011, results of searches for the Higgs boson and for exotic particles, based on the data collected during the first half of the 2011 run, were presented in conferences in Grenoble[127] and Mumbai.[128] In the latter conference it was reported that, despite hints of a Higgs signal in earlier data, ATLAS and CMS exclude with 95% confidence level (using the CLs method) the existence of a Higgs boson with the properties predicted by the Standard Model over most of the mass region between 145 and 466 GeV.[129] The searches for new particles did not yield signals either, allowing to further constrain the parameter space of various extensions of the Standard Model, including its supersymmetric extensions.[130][131]\\r\\nOn 13 December 2011, CERN reported that the Standard Model Higgs boson, if it exists, is most likely to have a mass constrained to the range 115ÿ130 GeV. Both the CMS and ATLAS detectors have also shown intensity peaks in the 124ÿ125 GeV range, consistent with either background noise or the observation of the Higgs boson.[132]\\r\\nOn 22 December 2011, it was reported that a new composite particle had been observed, the b (3P) bottomonium state.[111]\\r\\nOn 4 July 2012, both the CMS and ATLAS teams announced the discovery of a boson in the mass region around 125ÿ126 GeV, with a statistical significance at the level of 5 sigma each. This meets the formal level required to announce a new particle. The observed properties were consistent with the Higgs boson, but scientists were cautious as to whether it is formally identified as actually being the Higgs boson, pending further analysis.[133]\\r\\nOn 8 November 2012, the LHCb team reported on an experiment seen as a \\"golden\\" test of supersymmetry theories in physics,[114] by measuring the very rare decay of the Bs meson into two muons (Bs0 L +?). The results, which match those predicted by the non-supersymmetrical Standard Model rather than the predictions of many branches of supersymmetry, show the decays are less common than some forms of supersymmetry predict, though could still match the predictions of other versions of supersymmetry theory. The results as initially drafted are stated to be short of proof but at a relatively high 3.5 sigma level of significance.[134] The result was later confirmed by the CMS collaboration.[135]\\r\\nIn August 2013 the LHCb team revealed an anomaly in the angular distribution of B meson decay products which could not be predicted by the Standard Model; this anomaly had a statistical certainty of 4.5 sigma, just short of the 5 sigma needed to be officially recognized as a discovery. It is unknown what the cause of this anomaly would be, although the Z' boson has been suggested as a possible candidate.[136]\\r\\nOn 19 November 2014, the LHCb experiment announced the discovery of two new heavy subatomic particles, ּ?\\r\\nb and ּ??\\r\\nb. Both of them are baryons that are composed of one bottom, one down, and one strange quark. They are excited states of the bottom Xi baryon.[137][138]\\r\\nThe LHCb collaboration has observed multiple exotic hadrons, possibly pentaquarks or tetraquarks, in the Run 1 data. On 4 April 2014, the collaboration confirmed the existence of the tetraquark candidate Z(4430) with a significance of over 13.9 sigma.[139][140] On 13 July 2015, results consistent with pentaquark states in the decay of bottom Lambda baryons (0\\r\\nb) were reported.[141][142][143] On 28 June 2016, the collaboration announced four tetraquark-like particles decaying into a J/ and a  meson, only one of which was well established before (X(4274), X(4500) and X(4700) and X(4140)).[144][145]\\r\\nIn December 2016, ATLAS presented a measurement of the W boson mass, researching the precision of analyses done at the Tevatron.[146]\\r\\nAt the conference EPS-HEP 2015 in July, the collaborations presented first cross-section measurements of several particles at the higher collision energy.\\r\\nOn 15 December 2015, the ATLAS and CMS experiments both reported a number of preliminary results for Higgs physics, supersymmetry (SUSY) searches and exotics searches using 13 TeV proton collision data. Both experiments saw a moderate excess around 750 GeV in the two-photon invariant mass spectrum,[147][148][149] but the experiments did not confirm the existence of the hypothetical particle in an August 2016 report.[150][151][152]\\r\\nIn July 2017, many analysis based on the large dataset collected in 2016 were shown. The properties of the Higgs boson were studied in more detail and the precision of many other results was improved.[153]\\r\\nAfter some years of running, any particle physics experiment typically begins to suffer from diminishing returns: as the key results reachable by the device begin to be completed, later years of operation discover proportionately less than earlier years. A common response is to upgrade the devices involved, typically in collision energy, luminosity, or improved detectors. In addition to a possible increase to 14?TeV collision energy in 2018, a luminosity upgrade of the LHC, called the High Luminosity LHC, is planned,[154] to be made after 2022.\\r\\nThe experiments at the Large Hadron Collider sparked fears that the particle collisions might produce doomsday phenomena, involving the production of stable microscopic black holes or the creation of hypothetical particles called strangelets.[155] Two CERN-commissioned safety reviews examined these concerns and concluded that the experiments at the LHC present no danger and that there is no reason for concern,[156][157][158] a conclusion expressly endorsed by the American Physical Society.[159]\\r\\nThe reports also noted that the physical conditions and collision events that exist in the LHC and similar experiments occur naturally and routinely in the universe without hazardous consequences,[157] including ultra-high-energy cosmic rays observed to impact Earth with energies far higher than those in any man-made collider.\\r\\nThe Large Hadron Collider gained a considerable amount of attention from outside the scientific community and its progress is followed by most popular science media. The LHC has also inspired works of fiction including novels, TV series, video games and films.\\r\\nCERN employee Katherine McAlpine's \\"Large Hadron Rap\\"[160] surpassed 7?million YouTube views.[161][162] The band Les Horribles Cernettes was founded by women from CERN. The name was chosen so to have the same initials as the LHC.[163][164]\\r\\nNational Geographic Channel's World's Toughest Fixes, Season 2 (2010), Episode 6 \\"Atom Smasher\\" features the replacement of the last superconducting magnet section in the repair of the collider after the 2008 quench incident. The episode includes actual footage from the repair facility to the inside of the collider, and explanations of the function, engineering, and purpose of the LHC.[165]\\r\\nThe Large Hadron Collider was the focus of the 2012 student film Decay, with the movie being filmed on location in CERN's maintenance tunnels.[166]\\r\\nThe feature documentary Particle Fever follows the experimental physicists at CERN who run the experiments, as well as the theoretical physicists who attempt to provide a conceptual framework for the LHC's results. It won the Sheffield International Doc/Fest in 2013.\\r\\nThe novel Angels & Demons, by Dan Brown, involves antimatter created at the LHC to be used in a weapon against the Vatican. In response, CERN published a \\"Fact or Fiction?\\" page discussing the accuracy of the book's portrayal of the LHC, CERN, and particle physics in general.[167] The movie version of the book has footage filmed on-site at one of the experiments at the LHC; the director, Ron Howard, met with CERN experts in an effort to make the science in the story more accurate.[168]\\r\\nIn the visual novel/manga/anime-series \\"Steins;Gate\\", the primary antagonist controls a global research organization called SERN, the name of which is a deliberate misspelling of CERN. In the series SERN designs a particle accelerator and uses the miniature black holes created from experiments to master time travel and take over the world.\\r\\nThe novel FlashForward, by Robert J. Sawyer, involves the search for the Higgs boson at the LHC. CERN published a \\"Science and Fiction\\" page interviewing Sawyer and physicists about the book and the TV series based on it.[169]\\r\\nCoordinates: 4614N 0603E? / ?46.233N 6.050E? / 46.233; 6.050","input":"When was the large hadron collider first turned on?"},{"output":"28,218","context":"","input":"How many starbucks stores are in the world?"},{"output":"the Greek engineer Ctesibius (c. 270 BC)","context":"","input":"When was the first robot invented and by whom?"},{"output":"after seven to ten days","context":"\\r\\n\\r\\nA kitten is a juvenile cat. After being born, kittens are totally dependent on their mother for survival and they do not normally open their eyes until after seven to ten days. After about two weeks, kittens quickly develop and begin to explore the world outside the nest. After a further three to four weeks, they begin to eat solid food and grow adult teeth. Domestic kittens are highly social animals and usually enjoy human companionship.\\r\\n\\r\\nThe word \\"kitten\\" derives from the Middle English word kitoun, which in turn came from the Old French chitoun or cheton.[1] Juvenile big cats are called \\"cubs\\" rather than kittens; either term (but usually more commonly \\"kitten\\") may be used for the young of smaller wild felids, such as ocelots, caracals and lynxes.[2]\\r\\n\\r\\nA feline litter usually consists of two to five kittens[3] born after a gestation lasting between 64 and 67 days, with an average length of 66 days,[3] but from one to more than ten are known.[4] Kittens emerge in a sac called the amnion, which is bitten off and eaten by the mother cat.[5]\\r\\n\\r\\nFor the first several weeks, kittens cannot urinate or defecate without being stimulated by their mother.[6] They also cannot regulate their body temperature for the first three weeks, so kittens born in temperatures less than 27?C (81?F) can die from hypothermia if their mother does not keep them warm.[7] The mother's milk is very important for the kittens' nutrition and proper growth. This milk transfers antibodies to the kittens, which helps protect them against infectious disease.[8] Newborn kittens are unable to produce concentrated urine, and so have a very high requirement for fluids.[9] Kittens open their eyes about seven to ten days after birth. At first, the retina is poorly developed and vision is poor. Kittens cannot see as well as adult cats until about ten weeks after birth.[10]\\r\\n\\r\\nKittens develop very quickly from about two weeks of age until their seventh week. Their coordination and strength improve. They play-fight with their litter-mates and begin to explore the world outside the nest or den. They learn to wash themselves and others as well as play hunting and stalking games, showing their inborn ability as predators. These innate skills are developed by the kittens' mother or other adult cats, who bring live prey to the nest. Later, the adult cats demonstrate hunting techniques for the kittens to emulate.[11] As they reach three to four weeks old, the kittens are gradually weaned and begin to eat solid food, with weaning usually complete by six to eight weeks.[12] Kittens generally begin to lose their baby teeth around three months of age, and have a complete set of adult teeth by nine months.[13] Kittens live primarily on solid food after weaning, but usually continue to suckle from time to time until separated from their mothers. Some mother cats will scatter their kittens as early as three months of age, while others continue to look after them until they approach sexual maturity.[14]\\r\\n\\r\\nThe sex of kittens is usually easy to determine at birth. By six to eight weeks they are harder to sex because of the growth of fur in the genital region. The male's urethral opening is round, whereas the female's urethral opening is a slit. Another marked difference is the distance between anus and urethral opening, which is greater in males than in females.[15]\\r\\n\\r\\nKittens are highly social animals and spend most of their waking hours interacting with available animals and playing on their own. Play with other kittens peaks in the third or fourth month after birth, with more solitary hunting and stalking play peaking later, at about five months.[16]\\r\\n\\r\\nKittens are vulnerable because they like to find dark places to hide, sometimes with fatal results if they are not watched carefully. Cats have a habit of seeking refuge under or inside cars or on top of car tires during stormy or cold weather. This often leads to broken bones, burns, heat stroke, damaged internal organs or death.[17]\\r\\n\\r\\nDomestic kittens are commonly sent to new homes at six to eight weeks of age, but it has been suggested that being with their mother and litter-mates from six to twelve weeks is important for a kitten's social and behavioural development.[16] Usually, breeders and foster/rescue homes will not sell or adopt out a kitten that is younger than twelve weeks. In many jurisdictions, it is illegal to give away kittens younger than eight weeks of age.[18] Kittens generally reach sexual maturity at around seven months old. A cat reaches full \\"adulthood\\" around one year of age.[19]\\r\\n\\r\\nDomestic kittens in developed societies are usually vaccinated against common illnesses from two to three months of age. The usual combination vaccination protects against feline viral rhinotracheitis (FVR), feline calicivirus (C), and feline panleukopenia (P). This FVRCP inoculation is usually given at eight, twelve, and sixteen weeks, and an inoculation against rabies may be given at sixteen weeks. Kittens are usually spayed or neutered at seven months of age, but kittens may be neutered as young as seven weeks (if large enough), especially in animal shelters.[20] Such early neutering does not appear to have any long-term health risks to cats, and may even be beneficial in male cats.[21] Kittens are commonly wormed against roundworms from about four weeks.[22]\\r\\n\\r\\nFelines are carnivores and have adapted to animal-based diets and low carbohydrate inclusion. Kittens are categorized in a growth life stage, and have high energy and protein requirements.[23] When feeding a kitten, it is often recommended to use highly digestible ingredients and various components to aid in development in order to produce a healthy adult.[24] In North America, diets certified by the Association of American Feed Control Officials (AAFCO) are accepted as adequate nutrition, thus kitten diets should be AAFCO approved to ensure full supplementation.[25] Key components of the diet are high fat content to meet caloric requirements of growth, high protein to meet requirements for muscle growth as well as supplementation of certain nutrients such as docosahexaenoic acid to benefit the development of the brain and optimization of cognition.[26]\\r\\n\\r\\nPart of the kitten's immune system is the mucosal immune system, which is within the gastrointestinal tract. The mucosal immune system is largely responsible for coordinating proper immune responses by tolerating innocuous antigens and attacking foreign pathogens.[27] In order to optimize kitten health and increase chances of survival, it is important to optimize the link between the gut-associated lymphoid tissue and the microbiota of the gastrointestinal tract. Lasting health and longevity can be accomplished partly through proper nutrition[28] and establishing a healthy gut from birth through utilizing colostrum.[29]\\r\\n\\r\\nWithin the first 2 days after birth, kittens acquire passive immunity  from their mothers milk.[30] Milk within the first few days of parturition is called colostrum, and contains high concentrations of immunoglobulins.[30] These include immunoglobulin A and immunoglobulin G which cross the intestinal barrier of the neonate.[29] The immunoglobulins and growth factors found in the colostrum begin to establish and strengthen the weak immune system of the offspring.[31] Kittens are able to chew solid food around 5ÿ6 weeks after birth, and it is recommended that 30% of their diet should consist of solid food at this time.[32] The kitten remains on the mothers milk until around eight weeks of age when weaning is complete and a diet of solid food is the primary food source.[23]\\r\\n\\r\\nUp until approximately one year of age the kitten is undergoing a growth phase where energy requirements are up to 2.5 times higher than maintenance.[33] Pet nutritionists often suggest that a commercial cat food designed specifically for kittens by offered beginning at 4 weeks of age.[28] Fat has a higher caloric value than carbohydrates and protein, supplying 8.5kcal/g.[34] The growing kitten requires arachidonic and linoleic acid which can be provided in omega-3 fatty acids.[23] Docosahexaenoic acid (DHA) is another vital nutrient that can be supplied through omega 3 fatty acid. Addition of DHA to the diet benefits the cognition, brain and visual development of kittens.[28]\\r\\n\\r\\nCats are naturally carnivores and require high amounts of protein in the diet. Kittens are undergoing growth and require high amounts of protein to provide essential amino acids that enable the growth of tissues and muscles.[30] It is recommended that kittens consume a diet containing approximately 30% protein on a dry matter basis for proper growth.[35]\\r\\n\\r\\nTaurine is an essential amino acid found only in animal tissues and cannot be produced in sufficient amounts by the cat.[36] As it is an indispensable amino acid, it must be provided exogenously through the diet at 10?mg/kg bodyweight/day.[37] Kittens deprived of taurine can experience poor growth[36] and can result in retinal degeneration in cats.[38]\\r\\n\\r\\nFelines are natural carnivores and do not intentionally consume large quantities of carbohydrates. The domestic cat's liver has adapted to the lack of carbohydrates in the diet by using amino acids to produce glucose to fuel the brain and other tissues.[39]   Studies have shown that carbohydrate digestion in young kittens is much less effective than that of a mature feline with a developed gastrointestinal tract.[40] Highly digestible carbohydrates can be found in commercial kitten food as a source of additional energy as well as a source of fiber to stimulate the immature gut tissue. Soluble fibre such as beet pulp is a common ingredient used as a fibrous stool hardener and has been proven to strengthen intestinal muscles and to thicken the gut mucosal layer to prevent diarrhea.[41]\\r\\n\\r\\nThe lack of readily available glucose from the limited carbohydrates in the diet has resulted to the adaptation of the liver to produce glucose from the breakdown components of proteinamino acids. The enzymes that breakdown amino acids are constantly active in cats and thus, cats need a constant source of protein in their diet.[24] Kittens, require an increased amount of protein to supply readily available amino acids for daily maintenance and for building new body components seeing as they are constantly growing.[24] There are many required amino acids for kittens. Histidine is required at no greater than 30% in kitten diets since consuming histidine-free diets causes weight loss.[25]Tryptophan is required at 0.15% seeing as it maximized performance at this level.[25] Kittens also need the following amino acids supplemented in their diet: arginine to avoid an excess of ammonia in the blood otherwise known as hyperammonemia, isoleucine, leucine, valine, lysine, methionine as a sulfur containing amino acid, asparagine for maximal growth in the early post-weaning kitten, threonine and taurine to prevent from central retinal degeneration.[25]\\r\\n\\r\\nFat-Soluble Vitamins\\r\\n\\r\\nVitamin A is required in kitten diets because cats cannot convert carotenes to retinol in the intestinal mucosa because they lack the enzyme so this vitamin must be supplement in the diet.[24][42] Vitamin E is another required vitamin in kitten diets seeing as deficiency leads to steatitis, causing the depot fat to become firm and yellow-orange in colour, which is painful and leads to death.[42] Also, Vitamin D is an essential vitamin because cats cannot convert it from precursors in the skin.[24]\\r\\n\\r\\nWater-Soluble Vitamins\\r\\n\\r\\nCats can synthesize niacin, but their breakdown exceeds the rate that it can be synthesized and thus, have a higher need for it, which can be fulfilled through an animal-based diet.[24] Pyridoxine (vitamin B6) is required in increased amounts seeing as it is needed to produce amino acids.[24] To continue, vitamin B12 is an AAFCO recommended vitamin essential in the metabolism of carbohydrates and protein and maintains a healthy nervous system, healthy mucous membranes, healthy muscle and heart function and in general, promotes normal growth and development.[42] Choline is also a AAFCO recommended ingredient for kittens, which is important for neurotransmission in the brain and as a component of membrane phospholipids.[24] Biotin is another AAFCO recommended vitamin to support thyroid and adrenal glands and the reproductive and nervous systems.[24] Kittens also require riboflavin (vitamin B2) for heart health, pantothenic acid (vitamin B5), and folacin.[42]\\r\\n\\r\\nSince kitten diets are very high in calories, ingredients must be implemented to ensure adequate digestion and utilization of these calories. Choline chloride is an ingredient that maintains fat metabolism.[42] Biotin and niacin are also active in the metabolism of fats, carbs and protein.[42] Riboflavin is also necessary for the digestion of fats and carbohydrates.[42] These are the main metabolism aids incorporated into kitten diets to ensure nutrient usage is maximized.\\r\\n\\r\\nA combination of required nutrients is used to satisfy the overall growth and development of the kitten body, there are many ingredients that kittens do not require, but are included in diet formulation to encourage healthy growth and development. These ingredients include: dried egg as source of high quality protein and fatty acids, flaxseed as source of flaxseed oil which is rich in omega 3 fatty acid and aids in digestion, calcium carbonate as a source of calcium, and calcium pantothenate (vitamin B5) acts as a coenzyme in the conversion of amino acids and is important for healthy skin.[42]\\r\\n\\r\\nAntioxidants help support the development of a healthy immune system through inhibiting the oxidation of other molecules, which are essential for a growing kitten.[24] Antioxidants can be derived from ingredients, such as: carrots, sweet potatoes, spinach, vitamin E and vitamin E supplement, and zinc proteinate.\\r\\n\\r\\nKittens require a high-calorie diet that contains more protein than the diet of adult cats.[43] Young orphaned kittens require cat milk every two to four hours, and they need physical stimulation to defecate and urinate.[6] Cat milk replacement is manufactured to feed to young kittens, because cow's milk does not provide all the necessary nutrients.[44] Human-reared kittens tend to be very affectionate with humans as adults and sometimes more dependent on them than kittens reared by their mothers, but they can also show volatile mood swings and aggression.[45] Depending on the age at which they were orphaned and how long they were without their mothers, these kittens may be severely underweight and can have health problems later in life, such as heart conditions. The compromised immune system of orphaned kittens (from lack of antibodies found naturally in the mother's milk) can make them especially susceptible to infections, making antibiotics a necessity.[46]","input":"When do newborn baby kittens open their eyes?"},{"output":"encouraging settlement","context":"The Maxwell Land Grant, also known as the Beaubien-Miranda Land Grant, was a 1,714,765-acre (6,939.41?km2) Mexican land grant in Colfax County, New Mexico and part of adjoining Las Animas County, Colorado. This 1841 land grant was one of the largest contiguous private landholdings in the history of the United States. The New Mexico towns of Cimarron, Colfax, Dawson, Elizabethtown, French, Lynn, Maxwell, Miami, Raton, Rayado, Springer, Ute Park and Vermejo Park, came to be located within the grant,[1][2] as well as numerous other towns that are now ghost towns.[3]\\r\\n\\r\\n\\r\\nThe lands covered in the Maxwell Land Grant were originally tribal lands belonging to Jicarilla Apache Indians.[4] The region of northern New Mexico was claimed by Spain in 1524, but there were few settlements east of the Sangre de Cristo Range. In 1821, the government of Mexico was established, and the new government retained the Spanish policy of encouraging settlement by making land grants.[citation needed]\\r\\nCarlos Beaubien was a French-Canadian trapper who became a Mexican citizen. His partner, Guadalupe Miranda was the secretary to Governor Manuel Armijo in Santa Fe. On January 8, 1841, Beaubien and Miranda petitioned Armijo for a land grant.[5] They had to swear that they would colonize and cultivate the land. Three days later, Armijo granted them the land on the condition that they put it to good use. However, Beaubien and Miranda failed to prove up the grant for the next two years. On February 13, 1843, they asked the Justice of the Peace in Taos to sign an order promising them possession of the land. The justice affirmed that he had marked the boundaries of the grant and that Beaubien and Miranda were in full possession of the land grant.\\r\\nLucien Bonaparte Maxwell was a pioneer, explorer and adventurer who married Luz Beaubien, the daughter of Carlos Beaubien. Beaubien hired Maxwell to manage his interests, and Maxwell and his wife settled in Rayado, New Mexico in 1849. In 1860, Maxwell built a large home in Cimarron, a stop on the Mountain Branch of the Santa Fe Trail.[6]\\r\\nIn 1875, a range war known as the Colfax County War soon erupted between Maxwell and the settlers whom he perceived as squatters and tried to remove. There seems to be a discrepancy as to Lucien's middle name. A 1945 book lists it as Benjamin rather than Bonaparte.[7]\\r\\nIn 1870, for reasons that are not clear, Maxwell decided to sell the Grant.[8] A group of financiers, representing an English syndicate, purchased the Grant for a reported price of $1,350,000. Maxwell moved to Santa Fe, and then to Fort Sumner, where he died in 1875.[9]\\r\\nThe new owners formed the Maxwell Land Grant and Railway Company. They attempted to remove the squatters from the Grant. Some of the squatters felt that they had Maxwell's unwritten permission to live on the Grant. Many people left, but some stayed and fought. This struggle between owners and squatters came to be called the Colfax County War. F.J. Tolby, a minister sympathetic to the squatters was murdered on September 14, 1875.\\r\\nThe English company was bankrupt by 1874, and it went into foreclosure in 1879. A new group of owners from the Netherlands formed the Maxwell Land Grant Company installing future Senator and Secretary of War Stephen Benton Elkins as president.[10] In 1885, the new owners convinced the Territorial Governor Lionel Allen Sheldon to use the National Guard to suppress the squatters.[citation needed]\\r\\nBecause of a variety of financial problems, the Dutch company went bankrupt in 1888. In the early 1880s, the United States sued the company for making claims of lands in the Public Domain in Colorado. In 1887, this case reached the US Supreme Court, and was decided as United States v. Maxwell Land Grant Company. The court decision affirmed the company's ownership of the land. At this point, the settlers and squatters realized that they could not obtain good title to the land, and most of them left.[11]\\r\\nIn 1867, Lucien Maxwell sold what he thought was a 1,000 acres (400?ha) claim to J.B. Dawson. When Dawson had the land surveyed it turned out to be 20,000 acres (8,100?ha) underlain by coal. Phelps Dodge bought the Dawson Homestead and underlying coal in 1906. The company named the town Dawson, New Mexico and it grew to have about 2000 people.[citation needed]\\r\\nThe struggles over the grant continued, especially in the Colorado portion of the grant, where quite a bit of homesteading had taken place. On August 25, 1888, there was a violent incident at Stonewall, Colorado, in which several people were killed. The Maxwell Land Grant Company continued to sue homesteaders, and in many cases made them pay for their homesteads a second time. In 1894, the US Supreme Court decided Russell v. Maxwell Land Grant Company, which completely rejected the homesteaders claims in favor of the company.[citation needed]\\r\\nMany other sales of lands in the Grant took place in the early 1900s.[citation needed]\\r\\nIn 1902, William Bartlett, a wealthy grain operator from Chicago bought 205,000 acres (830?km2) of the grant along the drainage of the Vermejo River. Under the agreement, he withheld part of the last payment until the Maxwell Land Grant Company evicted the last of the squatters. In his words, \\"They are given two years to get the Mexicans off and I hold back $10,000.\\" Bartlett's Vermejo Park portion of the Grant has passed through several owners during the Twentieth Century. Pennzoil bought the Vermejo Park Ranch in 1973, and expanded its size. In 1982, Pennzoil donated a 100,000 acres (400?km2) portion of the ranch known as Valle Vidal to the US Government. This area is managed as a wilderness by the US Forest Service. In 1992, Ted Turner bought Vermejo Park Ranch (590,823 acres (2,390.98?km2) from Pennzoil. Ted Turner did not buy the mineral rights, so Atlas Energy Group, LLC produces gas on the Vermejo Park Ranch, while Ted Turner raises buffalo.[citation needed]\\r\\nBeginning in 1922, Waite Phillips, an oilman from Tulsa, Oklahoma also assembled a block of land on the Maxwell Land Grant. Phillips bought over 300,000 acres (1,200?km2), and named his ranch Philmont. In two separate gifts in 1938 and 1941, Phillips donated 127,395 acres (515.55?km2) as a wilderness camping area for the Boy Scouts of America.\\r\\nIn 1963, Norton Clapp, an officer of the National Council of the Boy Scouts of America, donated another piece of the Maxwell Land Grant to Philmont.[12] This was the Baldy Mountain mining area consisting of 10,098 acres (40.87?km2).[12]\\r\\nManley M. Chase purchased land in Chase Ranch in 1866, where M. M. Chase purchased a one-third interest in John B. Dawson's ranch (part of the Maxwell Land Grant) on the Vermejo River. Chase raised both sheep and cattle. In 1871, Chase purchased another part of the original Maxwell grant. He paid 50 cents an acre for 2,000 acres along Po?il Creek, an area which included the old Kit Carson homestead. The two-story adobe house which he built about three miles northeast of Cimarron is still the ranch headquarters and the family home.\\r\\nThe Cimarron Canyon State Park extends along Cimarron Canyon from Eagle Nest Lake to Ute Park and along US Route 64. The park is part of the Colin Neblett State Wildlife Area, which consists of 33,116 acres (134.02?km2) acres of former Grant land. This area was purchased by the State of New Mexico in the early 1950s.\\r\\nThe Whittington Center, founded in 1973, is the largest shooting and hunting complex in the world. It is owned by the National Rifle Association, and covers 33,000 acres (130?km2) of the Maxwell Land Grant.[citation needed]\\r\\nFive cases involving the land grant went to the United States Supreme Court:","input":"What was the purpose of the maxwell land grant?"},{"output":"New Zealand","context":"The Rugby World Cup is a men's rugby union tournament contested every four years between the top international teams. The tournament was first held in 1987, when the tournament was co-hosted by New Zealand and Australia. New Zealand are the current champions, having defeated Australia in the final of the 2015 tournament in England.\\r\\nThe winners are awarded the William Webb Ellis Cup, named after William Webb Ellis, the Rugby School pupil who  according to a popular legend  invented rugby by picking up the ball during a football game. Four countries have won the trophy; New Zealand have won it three times, two teams have won twice, Australia and South Africa, while England have won it once.\\r\\nThe tournament is administered by World Rugby, the sport's international governing body. Sixteen teams were invited to participate in the inaugural tournament in 1987, however since 1999 twenty teams have taken part. Japan will host the next event in 2019 and France will host in 2023.\\r\\n\\r\\n\\r\\nQualifying tournaments were introduced for the second tournament, where eight of the sixteen places were contested in a twenty-four-nation tournament.[1] The inaugural World Cup in 1987, did not involve any qualifying process; instead, the 16 places were automatically filled by seven eligible International Rugby Football Board (IRFB, now World Rugby) member nations, and the rest by invitation.[2]\\r\\nIn 2003 and 2007, the qualifying format allowed for eight of the twenty available positions to be filled by automatic qualification, as the eight quarter finalists of the previous tournament enter its successor. The remaining twelve positions were filled by continental qualifying tournaments.[3] Positions were filled by three teams from the Americas, one from Asia, one from Africa, three from Europe and two from Oceania.[3] Another two places were allocated for repechage. The first repechage place was determined by a match between the runners-up from the Africa and Europe qualifying tournaments, with that winner then playing the Americas runner-up to determine the place.[4] The second repechage position was determined between the runners-up from the Asia and Oceania qualifiers.[4]\\r\\nThe current format allows for 12 of the 20 available positions to be filled by automatic qualification, as the teams who finish third or better in the group (pool) stages of the previous tournament enter its successor (where they will be seeded).[5][6] The qualification system for the remaining eight places is region-based, with a total eight teams allocated for Europe, five for Oceania, three for the Americas, two for Africa, and one for Asia. The last place is determined by an intercontinental play-off.[7]\\r\\nThe 2015 tournament involved twenty nations competing over six weeks.[6][8] There were two stages, a pool and a knockout. Nations were divided into four pools, A through to D, of five nations each.[8][9] The teams were seeded before the start of the tournament, with the seedings taken from the World Rankings in December 2012. The four highest-ranked teams were drawn into pools A to D. The next four highest-ranked teams were then drawn into pools A to D, followed by the next four. The remaining positions in each pool were filled by the qualifiers.[6][10]\\r\\nNations play four pool games, playing their respective pool members once each.[9] A bonus points system is used during pool play. If two or more teams are level on points, a system of criteria is used to determine the higher ranked; the sixth and final criterion decides the higher rank through the official World Rankings.[9]\\r\\nThe winner and runner-up of each pool enter the knockout stage. The knockout stage consists of quarter- and semi-finals, and then the final. The winner of each pool is placed against a runner-up of a different pool in a quarter-final. The winner of each quarter-final goes on to the semi-finals, and the respective winners proceed to the final. Losers of the semi-finals contest for third place, called the 'Bronze Final'. If a match in the knockout stages ends in a draw, the winner is determined through extra time. If that fails, the match goes into sudden death and the next team to score any points is the winner. As a last resort, a kicking competition is used.[9]\\r\\nPrior to the Rugby World Cup, there was no truly global rugby union competition, but there were a number of other tournaments. One of the oldest is the annual Six Nations Championship, which started in 1883 as the Home Nations Championship, a tournament between England, Ireland, Scotland and Wales. It expanded to the Five Nations in 1910, when France joined the tournament. France did not participate from 1931 to 1939, during which period it reverted to a Home Nations championship. In 2000, Italy joined the competition, which became the Six Nations.[11]\\r\\nRugby union was also played at the Summer Olympic Games, first appearing at the 1900 Paris games and subsequently at London in 1908, Antwerp in 1920, and Paris again in 1924. France won the first gold medal, then Australasia, with the last two being won by the United States. However rugby union ceased to be on Olympic program after 1924.[12][13][a]\\r\\nThe idea of a Rugby World Cup had been suggested on numerous occasions going back to the 1950s, but met with opposition from most unions in the IRFB.[14] The idea resurfaced several times in the early 1980s, with the Australian Rugby Union (ARU) in 1983, and the New Zealand Rugby Union (NZRU) in 1984 independently proposing the establishment of a world cup.[15] A proposal was again put to the IRFB in 1985 and this time successfully passed 10ÿ6. The delegates from Australia, France, New Zealand and South Africa all voted for the proposal, and the delegates from Ireland and Scotland against; the English and Welsh delegates were split, with one from each country for and one against.[14][15]\\r\\nThe inaugural tournament, jointly hosted by Australia and New Zealand, was held in May and June 1987, with sixteen nations taking part.[16] New Zealand became the first ever champions, defeating France 29ÿ9 in the final.[17] The subsequent 1991 tournament was hosted by England, with matches played throughout Britain, Ireland and France. This tournament saw the introduction of a qualifying tournament; eight places were allocated to the quarter-finalists from 1987, and the remaining eight decided by a thirty-five nation qualifying tournament.[1] Australia won the second tournament, defeating England 12ÿ6 in the final.[18]\\r\\nIn 1992, eight years after their last official series,[b] South Africa hosted New Zealand in a one-off test match. The resumption of international rugby in South Africa came after the dismantling of the apartheid system, and was only done with permission of the African National Congress.[19][20] With their return to test rugby, South Africa were selected to host the 1995 Rugby World Cup.[21] After upsetting Australia in the opening match, South Africa continued to advance through the tournament until they met New Zealand in the final.[22][23] After a tense final that went into extra time, South Africa emerged 15ÿ12 winners,[24] with then President Nelson Mandela, wearing a Springbok jersey,[23] presenting the trophy to South Africa's captain, Francois Pienaar.[25]\\r\\nThe tournament in 1999 was hosted by Wales with matches also being held throughout the rest of the United Kingdom, Ireland and France. The tournament included a repechage system, alongside specific regional qualifying places, and an increase from sixteen to twenty participating nations. Australia claimed their second title, defeating France in the final.\\r\\nThe 2003 event was hosted by Australia, although it was originally intended to be held jointly with New Zealand. England emerged as champions defeating Australia in extra time. England's win was unique in that it broke the southern hemisphere's dominance in the event. Such was the celebration of England's victory, that an estimated 750,000 people gathered in central London to greet the team, making the day the largest sporting celebration of its kind ever in the United Kingdom.[26]\\r\\nThe 2007 competition was hosted by France, with matches also being held in Wales and Scotland. South Africa claimed their second title by defeating defending champions England 15ÿ6. The 2011 tournament was awarded to New Zealand in November 2005, ahead of bids from Japan and South Africa. The All Blacks reclaimed their place atop the rugby world with a narrow 8ÿ7 win over France in the 2011 final.\\r\\nIn the 2015 edition of tournament, hosted by England, New Zealand once again won the final, this time against established rivals, Australia. In doing so, they became the first team in World Cup history to win three titles, as well as the first to successfully defend a title. It was also New Zealand's first title victory on foreign soil.\\r\\nThe Webb Ellis Cup is the prize presented to winners of the Rugby World Cup, named after William Webb Ellis. The trophy is also referred to simply as the Rugby World Cup. The trophy was chosen in 1987 as an appropriate cup for use in the competition, and was created in 1906 by Garrard's Crown Jewellers.[27][28] The trophy is restored after each game by fellow Royal Warrant holder Thomas Lyte.[29][30] The words 'The International Rugby Football Board' and 'The Webb Ellis Cup' are engraved on the face of the cup. It stands thirty-eight centimetres high and is silver gilded in gold, and supported by two cast scroll handles, one with the head of a satyr, and the other a head of a nymph.[31] In Australia the trophy is colloquially known as \\"Bill\\"  a reference to William Webb Ellis.\\r\\nTournaments are organised by Rugby World Cup Ltd (RWCL), which is itself owned by World Rugby. The selection of host is decided by a vote of World Rugby Council members.[32][33] The voting procedure is managed by a team of independent auditors, and the voting kept secret. The allocation of a tournament to a host nation is now made five or six years prior to the commencement of the event, for example New Zealand were awarded the 2011 event in late 2005.\\r\\nThe tournament has been hosted by multiple nations. For example, the 1987 tournament was co-hosted by Australia and New Zealand. World Rugby requires that the hosts must have a venue with a capacity of at least 60,000 spectators for the final.[34] Host nations sometimes construct or upgrade stadia in preparation for the World Cup, such as Millennium Stadium ÿ purpose built for the 1999 tournament ÿ and Eden Park, upgraded for 2011.[34][35] The first country outside of the traditional rugby nations of SANZAAR or the Six Nations to be awarded the hosting rights was Japan, who will host the 2019 tournament. France will host the 2023 tournament.\\r\\nOrganizers of the Rugby World Cup, as well as the Global Sports Impact, state that the Rugby World Cup is the third largest sporting event in the World, behind only the FIFA World Cup and the Olympics,[36][37] although other sources question whether this is accurate.[38]\\r\\nReports emanating from World Rugby and its business partners have frequently touted the tournament's media growth, with cumulative worldwide television audiences of 300 million for the inaugural 1987 tournament, 1.75 billion in 1991, 2.67 billion in 1995, 3 billion in 1999,[39] 3.5 billion in 2003,[40] and 4 billion in 2007.[41] The 4 billion figure was widely dismissed as the global audience for television is estimated to be about 4.2 billion.[42]\\r\\nHowever, independent reviews have called into question the methodology of those growth estimates, pointing to factual inconsistencies.[43] The event's supposed drawing power outside of a handful of rugby strongholds was also downplayed significantly, with an estimated 97 percent of the 33 million average audience produced by the 2007 final coming from Australasia, South Africa, the British Isles and France.[44] Other sports have been accused of exaggerating their television reach over the years; such claims are not exclusive to the Rugby World Cup.\\r\\nWhile the event's global popularity remains a matter of dispute, high interest in traditional rugby nations is well documented. The 2003 final, between Australia and England, became the most watched rugby union match in the history of Australian television.[45]\\r\\nNotes:\\r\\nTwenty-five nations have participated at the Rugby World Cup (excluding qualifying tournaments). Of the eight tournaments that have been held, all but one have been won by a national team from the southern hemisphere.[47] The southern hemisphere's dominance has been broken only in 2003, when England beat Australia in the final.[47]\\r\\nThus far the only nations to host and win a tournament are New Zealand (1987 and 2011) and South Africa (1995). The performance of other host nations includes England (1991 final hosts) and Australia (2003 hosts) finishing runners-up. France (2007 hosts) finished fourth, while Wales (1999 hosts) failed to reach the semi-finals. Wales became the first host nation to be eliminated at the pool stages in 1991, while, England became the first solo host nation to be eliminated at the pool stages in 2015. Of the twenty-five nations that have ever participated in at least one tournament, twelve of them have never missed a tournament.[c]\\r\\nThe record for most points overall is held by English player Jonny Wilkinson, who scored 277 over his World Cup career.[48] Grant Fox of New Zealand holds the record for most points in one competition, with 126 in 1987;[48] Jason Leonard of England holds the record for most World Cup matches: 22 between 1991 and 2003.[48] Simon Culhane holds the record for most points in a match by one player, 45, as well as the record for most conversions in a match, 20.[49] Marc Ellis holds the record for most tries in a match, six, which he scored against Japan in 1995.[50]\\r\\nAll Black Jonah Lomu is the youngest player to appear in a final ÿ aged 20 years and 43 days at the 1995 Final.[51] Lomu shares 2 records with South African Bryan Habana. Most tries in a tournament (8): Lomu in 1999 and Habana in 2007 and total world cup tournament tries, both scored 15.[50] The record for most penalties in a match is 8, held by Matt Burke, Gonzalo Quesada, Gavin Hastings and Thierry Lacroix,[49] and the record for most penalties in a tournament, 31, is held by Gonzalo Quesada. South Africa's Jannie de Beer kicked five drop-goals against England in 1999 ÿ an individual record for a single World Cup match.[51]\\r\\nThe most points scored in a game is 145  by the All Blacks against Japan in 1995, while the widest winning margin is 142, held by Australia in a match against Namibia in 2003.[52]\\r\\nA total of 16 players have been sent off (red carded) in the tournament. Welsh lock Huw Richards was the first, while playing against New Zealand in 1987. No player has been red carded more than once.[49]","input":"Who won the first rugby union world cup?"},{"output":"a few centuries","context":"\\r\\n\\r\\nQuercus robur, commonly known as common oak, pedunculate oak, European oak or English oak, is a species of flowering plant in the beech and oak family, Fagaceae. It is native to most of Europe west of the Caucasus. The tree is widely cultivated in temperate regions and has escaped into the wild in scattered parts of China and North America.[3][4]\\r\\n\\r\\nQuercus robur (Latin quercus, \\"oak\\" + robur \\"strength, hard timber\\") is the type species of the genus (the species by which the oak genus Quercus is defined), and a member of the white oak section (Quercus section Quercus). The populations in Italy, southeast Europe, and Asia Minor and the Caucasus are sometimes treated as separate species, Q. brutia Tenore, Q. pedunculiflora K. Koch and Q. haas Kotschy respectively.\\r\\n\\r\\nA close relative is the sessile oak (Q. petraea), which shares much of its range. Q. robur is distinguished from this species by its leaves having only a very short stalk (petiole) 3ÿ8?mm (0.12ÿ0.31?in) long, and by its pedunculate (stalked) acorns. The two often hybridise in the wild, the hybrid being known as Quercus G rosacea.\\r\\n\\r\\nQ. robur should not be confused with Q. rubra, the red oak, which is a native of North America and only distantly related.\\r\\n\\r\\nQuercus robur is a large deciduous tree, with circumference of grand oaks from 4?m (13?ft) to exceptional 12?m (39?ft).[citation needed] The Majesty Oak with a circumference of 12.2?m (40?ft) is the thickest tree in Great Britain,[citation needed] and the Kaive Oak in Latvia with a circumference of 10.2?m (33?ft) is the thickest tree in Northern Europe.[citation needed] Quercus robur has lobed and nearly sessile (very short-stalked) leaves 7ÿ14?cm (2.8ÿ5.5?in) long. Flowering takes place in mid spring, and the fruit, called acorns, ripen by the following autumn. The acorns are 2ÿ2.5?cm (0.79ÿ0.98?in) long, pedunculate (having a peduncle or acorn-stalk, 3ÿ7?cm (1.2ÿ2.8?in) long) with one to four acorns on each peduncle.\\r\\n\\r\\nQ. robur is very tolerant to soil conditions and the continental climate but it prefers fertile and well-watered soils. Mature trees tolerate flooding.[5]\\r\\n\\r\\nIt is a long-lived tree, with a large wide spreading crown of rugged branches. While it may naturally live to an age of a few centuries, many of the oldest trees are pollarded or coppiced, both pruning techniques that extend the tree's potential lifespan, if not its health. Two individuals of notable longevity are the Stelmu?? Oak in Lithuania and the Granit Oak in Bulgaria, which are believed to be more than 1500 years old, possibly making them the oldest oaks in Europe; another specimen, called the 'Kongeegen' ('Kings Oak'), estimated to be about 1200 years old, grows in Jaegerspris, Denmark.[6] Yet another can be found in Kvilleken, Sweden, that is over 1000 years old and 14 metres (46?ft) around.[7] Of maiden (not pollarded) specimens, one of the oldest is the great oak of Ivenack, Germany. Tree-ring research of this tree and other oaks nearby gives an estimated age of 700 to 800 years. Also the Bowthorpe Oak in Lincolnshire, England is estimated to be 1000 years old, making it the oldest in the UK, although there is Knightwood Oak in the New Forest that is also said to be as old. Highest density of the grand oak trees Q. robur with a circumference 4 metres (13?ft) and more is in Latvia.[8]\\r\\n\\r\\nWithin its native range Q. robur is valued for its importance to insects and other wildlife. Numerous insects live on the leaves, buds, and in the acorns. Q. robur supports the highest biodiversity of insect herbivores of any British plant (>400 spp).  The acorns form a valuable food resource for several small mammals and some birds, notably Eurasian jays Garrulus glandarius. Jays were overwhelmingly the primary propagators[9] of oaks before humans began planting them commercially (and still remain the principal propagators for wild oaks), because of their habit of taking acorns from the umbra of its parent tree and burying them undamaged elsewhere. Mammals, notably squirrels who tend to hoard acorns and other nuts usually leave them too abused to grow in the action of moving or storing them.\\r\\n\\r\\nQuercus robur is planted for forestry, and produces a long-lasting and durable heartwood, much in demand for interior and furniture work. The wood of Q. robur is identified by a close examination of a cross-section perpendicular to fibres. The wood is characterised by its distinct (often wide) dark and light brown growth rings. The earlywood displays a vast number of large vessels (~0.5?mm (0.020?in) diameter). There are rays of thin (~0.1?mm (0.0039?in)) yellow or light brown lines running across the growth rings. The timber is around 720?kg (1,590?lb) per cubic meter in density.[10]\\r\\n\\r\\nA number of cultivars are grown in gardens and parks and in arboreta and botanical gardens.  The most common cultivar is Quercus robur 'Fastigiata', and is the exception among Q. robur cultivars that are generally smaller than the standard tree, growing to between 10ÿ15?m and exhibit unusual leaf or crown shape characteristics.\\r\\n\\r\\nEnglish oak is one of the most common park trees in south-eastern Australia, noted for its vigorous, luxuriant growth. In Australia, it grows very quickly[citation needed] to a tree of 20?m (66?ft) tall by up to 20?m (66?ft) broad, with a low-branching canopy. Its trunk and secondary branches are very thick and solid and covered with deep-fissured blackish-grey bark.[11]\\r\\nThe largest example in Australia is in Donnybrook, Western Australia.[12]\\r\\n\\r\\nAlong with the naturally occurring  Q. G rosacea, several hybrids with other white oak species have also been produced in cultivation, including Turner's Oak Q. G turnerii, Heritage Oak Q. G macdanielli, and Two Worlds Oak Q. G bimundorum, the latter two developed by nurseries in the United States.\\r\\n\\r\\n[13] (Turner had died in January 1776, and the nursery grounds, on extended lease, returned to the landowner.)[14] An early specimen is at the Royal Botanic Gardens, Kew.[15]\\r\\n\\r\\nIn the Basque Country (Spain and France) the oak symbolises the traditional basque liberties. This is based on the 'tree of Gernika', an ancient oak tree located in Gernika, below which since at least the 13th century the Lords of Biscay first, and afterwards their successors the Kings of Castile and the Kings of Spain solemnly swore to uphold the charter of Biscay, which secured widespread rights to the inhabitants of Biscay. Since the 14th century, the Juntas Generales (the parliament of Biscay) gathers in a building next to the oak tree, and symbolically passes its laws under the tree as well. Nowadays, the Lehendakari (Basque prime minister) swears his oath of office under the tree.\\r\\n\\r\\nThe national coat of arms of Bulgaria includes two crossed oak branches with fruits - as shield (escutcheon) compartment.\\r\\n\\r\\nOak leaves with acorns are depicted on the reverse of the Croatian 5 lipa coin, minted since 1993.[19] The pedunculate oak of the Croatian region of Slavonia (considered a separate subspecies - Slavonian oak) is a regional symbol of Slavonia and a national symbol of Croatia.[20]\\r\\n\\r\\nIn England, the English oak has assumed the status of a national emblem. This has its origins in the oak tree at Boscobel House, where the future King Charles II hid from his Parliamentarian pursuers in 1650 during the English Civil War; the tree has since been known as the Royal Oak. This event was celebrated nationally on 29 May as Oak Apple Day, which is continued to this day in some communities.[21] The Royal Oak is the third most popular pub name in Britain (541 in 2007)[22] and has been the name of eight major Royal Navy warships. The naval associations are strengthened by the fact that oak was the main construction material for sailing warships. The Royal Navy was often described as The Wooden Walls of Old England[23] (a paraphrase of the Delphic Oracle) and the Navys official quick march is \\"Heart of Oak\\". In folklore, the Major Oak is where Robin Hood is purportedly to have taken shelter[24]. Furthermore, the oak is the most common woodland tree in England.[25] An oak tree has been depicted on the reverse of the pound coin (the 1987 and 1992 issues) and a sprig of oak leaves and acorns is the emblem of the National Trust.\\r\\n\\r\\nIn Germany, the oak tree is used as a typical object and symbol in romanticism. It can be found in several paintings of Caspar David Friedrich and in \\"Of the life of a Good-For-Nothing\\" written by Joseph Freiherr von Eichendorff as a symbol of the state protecting every man. In those works the oak is shown in different situations, with leaves and flowers or dead without any of its previous beauty. Those conditions are mostly symbols for the conditions Germany is in or going through. Furthermore, the oak's stem is a symbol for Germany's strength and stability. Oak branches were displayed on the reverse of coins of the old Deutsche Mark currency (1 through 10 Pfennigs; the 50 Pfennigs coin showed a woman planting an oak seedling), and are now also displayed on the reverse of German-issue Euro currency coins (1 through 5 cents).\\r\\n\\r\\nIn Ireland, at Birr Castle, an example, over 400 years old has a girth of 6.5?m. It is known as the Carroll Oak, referring to the local Chieftains, Ely O'Carroll who ruled prior to Norman occupation.[26]\\r\\n\\r\\nThe Romanian Rugby Union side is known as The Oaks.\\r\\n\\r\\nGrandinin/roburin E, castalagin/vescalagin, gallic acid, monogalloyl glucose (glucogallin) and valoneic acid dilactone, monogalloyl glucose, digalloyl glucose, trigalloyl glucose, rhamnose, quercitrin and ellagic acid are phenolic compounds found in Q. robur.[27] The heartwood contains triterpene saponins.[28]","input":"How long does an english oak tree live?"},{"output":"Kalpana Chawla","context":"Kalpana Chawla (March 17, 1962 ÿ February 1, 2003) was an American astronaut[2] and the first woman of Indian origin in space.[3] She first flew on Space Shuttle Columbia in 1997 as a mission specialist and primary robotic arm operator. In 2003, Chawla was one of the seven crew members who died in the Space Shuttle Columbia disaster [4] when the craft disintegrated during its re-entry into the Earth's atmosphere. Chawla is a recipient of the Congressional Space Medal of Honor.\\r\\n\\r\\n\\r\\nChawla was born on 17 March 1962, but her official date of birth was altered to 1 July 1961 to allow her to become eligible for the matriculation exam.[5] As a child, Kalpana liked to draw pictures of airplanes.[6]After getting a Bachelor of Engineering degree in Aeronautical Engineering from Punjab Engineering College, Chandigarh,[7] she moved to the United States in 1982 where she obtained a Master of Science degree in Aerospace Engineering from the University of Texas at Arlington in 1984.[8] Chawla went on to earn a second Masters in 1986 and a PhD[9] in aerospace engineering in 1988 from the University of Colorado Boulder.\\r\\nIn 1988, she began working at NASA, where she did computational fluid dynamics (CFD) research on vertical and/or short take-off and landing (V/STOL) concepts. In 1993, she joined Overset Methods, Inc. as Vice President and Research Scientist specializing in simulation of moving multiple body problems.[10] Chawla held a Certificated Flight Instructor rating for airplanes, gliders and Commercial Pilot licenses for single and multi-engine airplanes, seaplanes and gliders.[11] After becoming a naturalized U.S. citizen in April 1991, Chawla applied for the NASA Astronaut Corps.[12] She joined the corps in March 1995 and was selected for her first flight in 1996.\\r\\nHer first space mission began on November 19, 1997, as part of the six-astronaut crew that flew the Space Shuttle Columbia flight STS-87. Chawla was the first Indian-born woman and the second Indian person to fly in space, following astronaut Rakesh Sharma who flew in 1984 on the Soyuz T-11. She spoke the following words while traveling in the weightlessness of space, \\"You are just your intelligence\\". On her first mission, Chawla traveled over 10.4 million miles (16737177.6 K M) in 252 orbits of the earth, logging more than 372 hours (15 Days and 12 Hours) in space.[10] During STS-87, she was responsible for deploying the Spartan satellite which malfunctioned, necessitating a spacewalk by Winston Scott and Takao Doi to capture the satellite. A five-month NASA investigation fully exonerated Chawla by identifying errors in software interfaces and the defined procedures of flight crew and ground control. After the completion of STS-87 post-flight activities, Chawla was assigned to technical positions in the astronaut office to work on the space station.\\r\\nIn 2000, Chawla was selected for her second flight as part of the crew of STS-107. This mission was repeatedly delayed due to scheduling conflicts and technical problems such as the July 2002 discovery of cracks in the shuttle engine flow liners. On January 16, 2003, Chawla finally returned to space aboard Space Shuttle Columbia on the ill-fated STS-107 mission. The crew performed nearly 80 experiments studying earth and space science, advanced technology development, and astronaut health and safety. During the launch of STS-107, Columbia's 28th mission, a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter. Previous shuttle launches had seen minor damage from foam shedding,[13] but some engineers suspected that the damage to Columbia was more serious. NASA managers limited the investigation, reasoning that the crew could not have fixed the problem if it had been confirmed.[14] When Columbia re-entered the atmosphere of Earth, the damage allowed hot atmospheric gases to penetrate and destroy the internal wing structure, which caused the spacecraft to become unstable and break apart.[15] After the disaster, Space Shuttle flight operations were suspended for more than two years, similar to the aftermath of the Challenger disaster. Construction of the International Space Station (ISS) was put on hold; the station relied entirely on the Russian Roscosmos State Corporation for resupply for 29 months until Shuttle flights resumed with STS-114 and 41 months for crew rotation.\\r\\nChawla died in the Space Shuttle Columbia disaster which occurred on February 1, 2003 when the Columbia disintegrated over Texas during re-entry into the Earth's atmosphere, with the death of all of seven crew members, shortly before it was scheduled to conclude its 28th mission, STS-107.[16]\\r\\nChawla's remains were identified along with the rest of the crew members and were cremated and scattered at National Park in Utah in accordance with her wishes.","input":"Who was the first india born woman nasa astronaut?"},{"output":"to attract and build a strong and effective force of men dedicated to its Cardinal Principles of manhood, scholarship, perseverance, and uplift","context":"Omega Psi Phi (ζ) is an international fraternity with over 750 undergraduate and graduate chapters. The fraternity was founded on November 17, 1911 by three Howard University juniors, Edgar Amos Love, Oscar James Cooper and Frank Coleman, and their faculty adviser, Dr. Ernest Everett Just. Omega Psi Phi is the first predominantly African-American fraternity to be founded at a historically black university.[1]\\r\\n\\r\\n\\r\\nSince its founding in 1911, Omega Psi Phi's stated purpose has been to attract and build a strong and effective force of men dedicated to its Cardinal Principles of manhood, scholarship, perseverance, and uplift. Throughout the world, many notable members are recognized as leaders in the arts, academics, athletics, entertainment, business, civil rights, education, government, and science fields. A few notable members include Bill Cosby, Samuel M. Nabrit, Walter E. Massey, Benjamin Mays, Bayard Rustin, Langston Hughes, Count Basie, Roy Wilkins, Benjamin Hooks, Vernon Jordan, Dr. Robert Henry Lawrence, Jr., Malcolm Jenkins, Rev. Jesse Jackson, Rev. Dr. Mack King Carter, William H. Hastie (U.S. Virgin Islands) and L. Douglas Wilder, Representative James Clyburn, Earl Graves, Tom Joyner, Charles Bolden, Ronald McNair, General William \\"Kip\\" Ward, Michael Jordan, Shaquille O'Neal, Roger Kingdom, Terrence Trammell, Shammond Williams, Vince Carter, Steve Harvey, Rickey Smiley, Ray Lewis, Stephen A. Smith, and numerous presidents of colleges and universities. Over 250,000 men have been initiated into Omega Psi Phi throughout the United States, Bermuda, Bahamas, Virgin Islands, South Korea, Japan, Liberia, Germany, and Kuwait.[1] On the 2013 Super Bowl champion Baltimore Ravens, six players and GM Ozzie Newsome are members.[2]\\r\\nIn 1924, at the urging of fraternity member Carter G. Woodson, the fraternity launched Negro History and Literature Week in an effort to publicize the growing body of scholarship on African-American history.[3] Encouraged by public interest, the event was renamed \\"Negro Achievement Week\\" in 1925 and given an expanded national presence in 1926 by Woodson's Association for the Study of Negro Life as \\"Negro History Week.\\"[3] Expanded to the full month of February from 1976, this event continues today as Black History Month.\\r\\nSince 1945, the fraternity has undertaken a National Social Action Program to meet the needs of African Americans in the areas of health, housing, civil rights, and education. Omega Psi Phi has been a patron of the United Negro College Fund (UNCF) since 1955, providing an annual gift of $350,000.00 to the program.\\r\\nOmega Psi Phi is a member of the National Pan-Hellenic Council (NPHC), which is composed of nine predominately African-American Greek-letter sororities and fraternities that promote interaction through forums, meetings, and other media for the exchange of information, and engage in cooperative programming and initiatives throughout the world. The (NPHC) currently represents over 2.5 million members.[4]\\r\\nOmega Psi Phi celebrated its centennial during the week of July 27ÿ31, 2011 in Washington D.C., becoming distinguished as only the third African-American collegiate fraternity to reach the century mark.[4] The Centennial Celebration recognized the impact of the Fraternity in communities over the past 100 years, honored Omega Men for achievement in all walks of life, reiterated Omega Psi Phi's commitment to providing unparalleled community service and scholarship, and charted the Fraternity's future activities.\\r\\nEach Chapter administers Internationally Mandated Programs every year:[5]\\r\\nAchievement Week ÿ A week in November that seeks to recognize individuals who have made notable contributions to society. During the Achievement Week, a High School Essay Contest is held and the winner usually receives a scholarship award.\\r\\nScholarship ÿ The Charles R. Drew Scholarship Program encourages academic progress among the organization's undergraduate members. A portion of the fraternity's budget is designated for the Charles R. Drew Scholarship Commission, which awards scholarships to members and non-members.\\r\\nSocial Action Programs ÿ All chapters are required to participate in programs that uplift their society. Many participate in activities like: voter registration, illiteracy programs, mentoring programs, fundraisers, and charitable organizations such as American Diabetes Association, United Way, and the Sickle Cell Anemia Foundation.\\r\\nTalent Hunt Program ÿ Each chapter is required to hold a yearly talent contest, to encourage young people to expose themselves to the Performing Arts. Individuals who win these talent contests receive an award, such as a scholarship.\\r\\nMemorial Service ÿ March 12 is Omega Psi Phi Memorial Day. Every chapter of the Fraternity performs a ritualistic memorial service to remember members who have died.\\r\\nReclamation and Retention ÿ This program is an effort to encourage inactive members to become fully active and participate in the fraternity's programs.\\r\\nCollege Endowment Funds ÿ The fraternity donates thousands of dollars to Historically Black Colleges and Universities each year.\\r\\nHealth Initiatives ÿ Chapters are required to coordinate programs that will encourage good health practices. Programs that members involve themselves in include HIV/AIDS awareness, blood drives, prostate cancer awareness, and sickle cell anemia awareness programs.\\r\\nVoter Registration, Education and Motivation ÿ Coordination activities that promote voter registration and mobilization.\\r\\nNAACP ÿ A Life Membership at Large in the NAACP is required by all chapters and districts.[6]\\r\\nOmega Psi Phi recognizes undergraduate and graduate membership. College students must be working toward a bachelor's degree at a four-year institution, have at least 31 semester credits, and maintain at least a 2.5 grade point average. For the graduate chapter, an applicant must already possess a bachelor's degree.[7] The fraternity grants honorary membership to men who have contributed to society in a positive way on a national or international level. For example, Charles Young (March 12, 1864 ÿ January 2, 1922) was the third African American graduate of West Point, the first black U.S. national park superintendent, the first African American military attach, and the highest ranking black officer (Colonel) in the United States Army until his death in 1922.\\r\\nIn 1930, Omega Psi Phi became one of 5 founding members of the National Pan-Hellenic Council (NPHC). Today, the NPHC is composed of nine international black Greek-letter sororities and fraternities and promotes interaction through forums, meetings, and other mediums for the exchange of information, and engages in cooperative programming and initiatives through various activities and functions.[8]\\r\\na. Finished unexpired term of Atkins[10]\\r\\nIn 1977, Robert Brazile, a student at the University of Pennsylvania, collapsed and died at a fraternity house meeting due to injuries and beatings he sustained while pledging the fraternity.[36]\\r\\nIn 1978, Nathaniel Swimson, a student at North Carolina Central University, died during an off-campus initiation activity. He was asked to run several miles before he collapsed and died.[36]\\r\\nIn 1983, Vann Watts, a student at Tennessee State University, died of an alcohol overdose following an initiation party. It was reported that prior to his death, he was severely beaten and verbally abused by fraternity members.[37]\\r\\nIn 1984, a Hampton University student was killed participating in an Omega Psi Phi ritual. The family of the deceased student privately settled with the fraternity for an undisclosed amount as a result of his wrongful death.[38]\\r\\nIn 1986, Thomas Harold, a student at Lamar University, died as a result of running miles on Lamar's track as part of a pledging task.[39][40]\\r\\nIn 1993, 24 Omegas were arrested for making pledges from University of Maryland at College Park eat vomit and dog biscuits, dropping hot wax on their necks, and beating them so badly that they needed medical attention.[41]\\r\\nIn 1997, the fraternity was court ordered to pay a former Indiana University pledge $774,500 for injuries he sustained while pledging in 1994. [42]\\r\\nIn 1999, Omega Psi Phi was court ordered to pay a former University of Louisville student nearly $1 million for suffering kidney failure due to hazing activity in 1997.[43]\\r\\nIn 2001, Joseph T. Green, a student at Tennessee State University, died as result of an asthma attack he developed from being asked to run long distances while pledging. In 2002, his family filed a $15 million wrongful death lawsuit against the men of Omega Psi Phi Incorporated.[44] [45]\\r\\nIn 2009, a former pledge at the University of Houston (UH) settled with the fraternity for an undisclosed amount after being hit with a baseball bat, wood board, and TV antenna while pledging. The UH student wanted to join the fraternity because his father was a member. The chapter was placed on suspension following this incident. [46]\\r\\nIn 2014, the chapter at Valdosta State University was banned from campus until at least August 2022 due to severe hazing and violating the school's code of conduct.[47]\\r\\nIn 2015, six Omega Psi Phi members at Johnson C. Smith University were arrested and charged with assault for severely beating pledges over a two-month span.[48]\\r\\nIn 2015, four Omega Psi Phi members at Saginaw Valley State University were arrested and charged for striking pledges with open hands and paddles. One known pledge sustained a serious injury after losing consciousness one night pledging.[49]\\r\\nIn 2015, a Florida Atlantic University student reported to the police she was gang-raped at an Omega Psi Phi \\"Oil Spill\\" step show after party. Inside the party, she stated she was suddenly and forcefully pulled behind curtains and raped by a group of men in a dark area.[50][51]\\r\\nIn 2016, the fraternity at Florida State University was suspended for severely abusing pledges and violating the university's code of conduct.[52] Criminal charges are pending for members of the fraternity.\\r\\nIn April 2017, Omega Psi Phi member and alleged spree killer Steve Stephens referenced the fraternity multiple times in videos he posted during his killing spree. Stephens claimed that he was going to shoot \\"Greeks\\" in the head until he was caught. [53]\\r\\nLike many fraternal organizations, Omega Psi Phi has a rich tradition of practices. While some traditions are naturally secret, many are freely expressed in public. A popular one is referring to members as \\"Que Dogs\\" or \\"Ques\\". Another is the practice of members voluntarily undergoing branding of the letters, or variations and designs based on them (such as two linked Omega symbols), on their skin. The brands often are displayed in public as a matter of pride; some prospects first learn of the fraternity by seeing members bearing brands.[54]","input":"What is the purpose of omega psi phi?"},{"output":"Gulf of Mexico","context":"\\r\\n\\r\\nThe Mississippi River is the chief river of the second-largest drainage system on the North American continent, second only to the Hudson Bay drainage system.[13][14] The stream is entirely within the United States (although its drainage basin reaches into Canada), its source is Lake Itasca in northern Minnesota and it flows generally south for 2,320 miles (3,730?km)[14] to the Mississippi River Delta in the Gulf of Mexico. With its many tributaries, the Mississippi's watershed drains all or parts of 31 U.S. states and two Canadian provinces between the Rocky and Appalachian Mountains. The Mississippi ranks as the fourth-longest and fifteenth-largest river by discharge in the world. The river either borders or passes through the states of Minnesota, Wisconsin, Iowa, Illinois, Missouri, Kentucky, Tennessee, Arkansas, Mississippi, and Louisiana.[15][16]\\r\\n\\r\\nNative Americans long lived along the Mississippi River and its tributaries. Most were hunter-gatherers, but some, such as the Mound Builders, formed prolific agricultural societies. The arrival of Europeans in the 16th century changed the native way of life as first explorers, then settlers, ventured into the basin in increasing numbers.[17] The river served first as a barrier, forming borders for New Spain, New France, and the early United States, and then as a vital transportation artery and communications link. In the 19th century, during the height of the ideology of manifest destiny, the Mississippi and several western tributaries, most notably the Missouri, formed pathways for the western expansion of the United States.\\r\\n\\r\\nFormed from thick layers of the river's silt deposits, the Mississippi embayment is one of the most fertile agricultural regions of the country, which resulted in the river's storied steamboat era. During the American Civil War, the Mississippi's capture by Union forces marked a turning point towards victory due to the river's importance as a route of trade and travel, not least to the Confederacy. Because of substantial growth of cities and the larger ships and barges that supplanted riverboats, the first decades of the 20th century saw the construction of massive engineering works such as levees, locks and dams, often built in combination.\\r\\n\\r\\nSince modern development of the basin began, the Mississippi has also seen its share of pollution and environmental problems ÿ most notably large volumes of agricultural runoff, which has led to the Gulf of Mexico dead zone off the Delta. In recent years, the river has shown a steady shift towards the Atchafalaya River channel in the Delta; a course change would be an economic disaster for the port city of New Orleans.\\r\\n\\r\\nThe word  Mississippi itself comes from Misi zipi, the French rendering of the Anishinaabe (Ojibwe or Algonquin) name for the river, Misi-ziibi (Great River).\\r\\n\\r\\nIn the 18th century, the river was the primary western boundary of the young United States, and since the country's expansion westward, the Mississippi River has been widely considered a convenient if approximate dividing line between the Eastern, Southern, and Midwestern United States, and the Western United States. This is exemplified by the Gateway Arch in St. Louis and the phrase \\"Trans-Mississippi\\" as used in the name of the Trans-Mississippi Exposition.\\r\\n\\r\\nIt is common to qualify a regionally superlative landmark in relation to it, such as \\"the highest peak east of the Mississippi\\"[18] or \\"the oldest city west of the Mississippi\\".[19] The FCC also uses it as the dividing line for broadcast callsigns, which begin with W to the east and K to the west, mixing together in media markets along the river.\\r\\n\\r\\nThe geographical setting of the Mississippi River includes considerations of the course of the river itself, its watershed, its outflow, its prehistoric and historic course changes, and possibilities of future course changes. The New Madrid Seismic Zone along the river is also noteworthy. These various basic geographical aspects of the river in turn underlie its human history and present uses of the waterway and its adjacent lands.\\r\\n\\r\\nThe Mississippi River can be divided into three sections: the Upper Mississippi, the river from its headwaters to the confluence with the Missouri River; the Middle Mississippi, which is downriver from the Missouri to the Ohio River; and the Lower Mississippi, which flows from the Ohio to the Gulf of Mexico.\\r\\n\\r\\nThe Upper Mississippi runs from its headwaters to its confluence with the Missouri River at St. Louis, Missouri. It is divided into two sections:\\r\\n\\r\\nThe source of the Upper Mississippi branch is traditionally accepted as Lake Itasca, 1,475 feet (450?m) above sea level in Itasca State Park in Clearwater County, Minnesota. The name \\"Itasca\\" was chosen to designate the \\"true head\\" of the Mississippi River as a combination of the last four letters of the Latin word for truth (veritas) and the first two letters of the Latin word for head (caput).[20] However, the lake is in turn fed by a number of smaller streams.\\r\\n\\r\\nFrom its origin at Lake Itasca to St. Louis, Missouri, the waterway's flow is moderated by 43 dams. Fourteen of these dams are located above Minneapolis in the headwaters region and serve multiple purposes, including power generation and recreation. The remaining 29 dams, beginning in downtown Minneapolis, all contain locks and were constructed to improve commercial navigation of the upper river. Taken as a whole, these 43 dams significantly shape the geography and influence the ecology of the upper river. Beginning just below Saint Paul, Minnesota, and continuing throughout the upper and lower river, the Mississippi is further controlled by thousands of wing dikes that moderate the river's flow in order to maintain an open navigation channel and prevent the river from eroding its banks.\\r\\n\\r\\nThe head of navigation on the Mississippi is the Coon Rapids Dam in Coon Rapids, Minnesota. Before it was built in 1913, steamboats could occasionally go upstream as far as Saint Cloud, Minnesota, depending on river conditions.\\r\\n\\r\\nThe uppermost lock and dam on the Upper Mississippi River is the Upper St. Anthony Falls Lock and Dam in Minneapolis. Above the dam, the river's elevation is 799 feet (244?m). Below the dam, the river's elevation is 750 feet (230?m). This 49-foot (15?m) drop is the largest of all the Mississippi River locks and dams. The origin of the dramatic drop is a waterfall preserved adjacent to the lock under an apron of concrete. Saint Anthony Falls is the only true waterfall on the entire Mississippi River. The water elevation continues to drop steeply as it passes through the gorge carved by the waterfall.\\r\\n\\r\\nAfter the completion of the St. Anthony Falls Lock and Dam in 1963, the river's head of navigation moved upstream, to the Coon Rapids Dam. However, the Locks were closed in 2015 to control the spread of invasive Asian carp, making Minneapolis once again the site of the head of navigation of the river.[21]\\r\\n\\r\\nThe Upper Mississippi has a number of natural and artificial lakes, with its widest point being Lake Winnibigoshish, near Grand Rapids, Minnesota, over 11 miles (18?km) across. Lake Onalaska, created by Lock and Dam No. 7, near La Crosse, Wisconsin, is more than 4 miles (6.4?km) wide. Lake Pepin, a natural lake formed behind the delta of the Chippewa River of Wisconsin as it enters the Upper Mississippi, is more than 2 miles (3.2?km) wide.[22]\\r\\n\\r\\nBy the time the Upper Mississippi reaches Saint Paul, Minnesota, below Lock and Dam No. 1, it has dropped more than half its original elevation and is 687 feet (209?m) above sea level. From St. Paul to St. Louis, Missouri, the river elevation falls much more slowly, and is controlled and managed as a series of pools created by 26 locks and dams.[23]\\r\\n\\r\\nThe Upper Mississippi River is joined by the Minnesota River at Fort Snelling in the Twin Cities; the St. Croix River near Prescott, Wisconsin; the Cannon River near Red Wing, Minnesota; the Zumbro River at Wabasha, Minnesota; the Black, La Crosse, and Root rivers in La Crosse, Wisconsin; the Wisconsin River at Prairie du Chien, Wisconsin; the Rock River at the Quad Cities; the Iowa River near Wapello, Iowa; the Skunk River south of Burlington, Iowa; and the Des Moines River at Keokuk, Iowa. Other major tributaries of the Upper Mississippi include the Crow River in Minnesota, the Chippewa River in Wisconsin, the Maquoketa River and the Wapsipinicon River in Iowa, and the Illinois River in Illinois.\\r\\n\\r\\nThe Upper Mississippi is largely a multi-thread stream with many bars and islands. From its confluence with the St. Croix River downstream to Dubuque, Iowa, the river is entrenched, with high bedrock bluffs lying on either side. The height of these bluffs decreases to the south of Dubuque, though they are still significant through Savanna, Illinois. This topography contrasts strongly with the Lower Mississippi, which is a meandering river in a broad, flat area, only rarely flowing alongside a bluff (as at Vicksburg, Mississippi).\\r\\n\\r\\nThe Mississippi River is known as the Middle Mississippi from the Upper Mississippi River's confluence with the Missouri River at St. Louis, Missouri, for 190 miles (310?km) to its confluence with the Ohio River at Cairo, Illinois.[24][25]\\r\\n\\r\\nThe Middle Mississippi is relatively free-flowing. From St. Louis to the Ohio River confluence, the Middle Mississippi falls 220 feet (67?m) over 180 miles (290?km) for an average rate of 1.2 feet per mile (23?cm/km). At its confluence with the Ohio River, the Middle Mississippi is 315 feet (96?m) above sea level. Apart from the Missouri and Meramec rivers of Missouri and the Kaskaskia River of Illinois, no major tributaries enter the Middle Mississippi River.\\r\\n\\r\\nThe Mississippi River is called the Lower Mississippi River from its confluence with the Ohio River to its mouth at the Gulf of Mexico, a distance of about 1,000 miles (1,600?km). At the confluence of the Ohio and the Middle Mississippi, the long-term mean discharge of the Ohio at Cairo, Illinois is 281,500 cubic feet per second (7,970 cubic meters per second),[26] while the long-term mean discharge of the Mississippi at Thebes, Illinois (just upriver from Cairo) is 208,200?cu?ft/s (5,900?m3/s).[27] Thus, by volume, the main branch of the Mississippi River system at Cairo can be considered to be the Ohio River (and the Allegheny River further upstream), rather than the Middle Mississippi.\\r\\n\\r\\nIn addition to the Ohio River, the major tributaries of the Lower Mississippi River are the White River, flowing in at the White River National Wildlife Refuge in east central Arkansas; the Arkansas River, joining the Mississippi at Arkansas Post; the Big Black River in Mississippi; and the Yazoo River, meeting the Mississippi at Vicksburg, Mississippi. The widest point of the Mississippi River is in the Lower Mississippi portion where it exceeds 1 mile (1.6?km) in width in several places.\\r\\n\\r\\nDeliberate water diversion at the Old River Control Structure in Louisiana allows the Atchafalaya River in Louisiana to be a major distributary of the Mississippi River, with 30% of the Mississippi flowing to the Gulf of Mexico by this route, rather than continuing down the Mississippi's current channel past Baton Rouge and New Orleans on a longer route to the Gulf.[28][29][30] Although the Red River is commonly thought to be a tributary, it is actually not, because its water flows separately into the Gulf of Mexico through the Atchafalaya River.\\r\\n\\r\\nThe Mississippi River has the world's fourth-largest drainage basin (\\"watershed\\" or \\"catchment\\"). The basin covers more than 1,245,000 square miles (3,220,000?km2), including all or parts of 31[31] U.S. states and two Canadian provinces. The drainage basin empties into the Gulf of Mexico, part of the Atlantic Ocean. The total catchment of the Mississippi River covers nearly 40% of the landmass of the continental United States. The highest point within the watershed is also the highest point of the Rocky Mountains, Mount Elbert at 14,440 feet (4,400?m).[32]\\r\\n\\r\\nIn the United States, the Mississippi River drains the majority of the area between the crest of the Rocky Mountains and the crest of the Appalachian Mountains, except for various regions drained to Hudson Bay by the Red River of the North; to the Atlantic Ocean by the Great Lakes and the Saint Lawrence River; and to the Gulf of Mexico by the Rio Grande, the Alabama and Tombigbee rivers, the Chattahoochee and Appalachicola rivers, and various smaller coastal waterways along the Gulf.\\r\\n\\r\\nThe Mississippi River empties into the Gulf of Mexico about 100 miles (160?km) downstream from New Orleans. Measurements of the length of the Mississippi from Lake Itasca to the Gulf of Mexico vary somewhat, but the United States Geological Survey's number is 2,320 miles (3,730?km). The retention time from Lake Itasca to the Gulf is typically about 90 days.[33]\\r\\n\\r\\nThe Mississippi River discharges at an annual average rate of between 200 and 700 thousand cubic feet per second (7,000ÿ20,000?m3/s).[34] Although it is the fifth-largest river in the world by volume, this flow is a small fraction of the output of the Amazon, which moves nearly 7 million cubic feet per second (200,000?m3/s) during wet seasons. On average, the Mississippi has only 8% the flow of the Amazon River.[35]\\r\\n\\r\\nFresh river water flowing from the Mississippi into the Gulf of Mexico does not mix into the salt water immediately. The images from NASA's MODIS (to the right) show a large plume of fresh water, which appears as a dark ribbon against the lighter-blue surrounding waters. These images demonstrate that the plume did not mix with the surrounding sea water immediately. Instead, it stayed intact as it flowed through the Gulf of Mexico, into the Straits of Florida, and entered the Gulf Stream. The Mississippi River water rounded the tip of Florida and traveled up the southeast coast to the latitude of Georgia before finally mixing in so thoroughly with the ocean that it could no longer be detected by MODIS.\\r\\n\\r\\nBefore 1900, the Mississippi River transported an estimated 400?million metric tons of sediment per year from the interior of the United States to coastal Louisiana and the Gulf of Mexico. During the last two decades, this number was only 145?million metric tons per year. The reduction in sediment transported down the Mississippi River is the result of engineering modification of the Mississippi, Missouri, and Ohio rivers and their tributaries by dams, meander cutoffs, river-training structures, and bank revetments and soil erosion control programs in the areas drained by them.[36]\\r\\n\\r\\nOver geologic time, the Mississippi River has experienced numerous large and small changes to its main course, as well as additions, deletions, and other changes among its numerous tributaries, and the lower Mississippi River has used different pathways as its main channel to the Gulf of Mexico across the delta region.\\r\\n\\r\\nThrough a natural process known as avulsion or delta switching, the lower Mississippi River has shifted its final course to the mouth of the Gulf of Mexico every thousand years or so. This occurs because the deposits of silt and sediment begin to clog its channel, raising the river's level and causing it to eventually find a steeper, more direct route to the Gulf of Mexico. The abandoned distributaries diminish in volume and form what are known as bayous. This process has, over the past 5,000 years, caused the coastline of south Louisiana to advance toward the Gulf from 15 to 50 miles (24 to 80?km). The currently active delta lobe is called the Birdfoot Delta, after its shape, or the Balize Delta, after La Balize, Louisiana, the first French settlement at the mouth of the Mississippi.\\r\\n\\r\\nThe current form of the Mississippi River basin was largely shaped by the Laurentide Ice Sheet of the most recent Ice Age. The southernmost extent of this enormous glaciation extended well into the present-day United States and Mississippi basin. When the ice sheet began to recede, hundreds of feet of rich sediment were deposited, creating the flat and fertile landscape of the Mississippi Valley. During the melt, giant glacial rivers found drainage paths into the Mississippi watershed, creating such features as the Minnesota River, James River, and Milk River valleys. When the ice sheet completely retreated, many of these \\"temporary\\" rivers found paths to Hudson Bay or the Arctic Ocean, leaving the Mississippi Basin with many features \\"oversized\\" for the existing rivers to have carved in the same time period.\\r\\n\\r\\nIce sheets during the Illinoian Stage, about 300,000 to 132,000 years before present, blocked the Mississippi near Rock Island, Illinois, diverting it to its present channel farther to the west, the current western border of Illinois. The Hennepin Canal roughly follows the ancient channel of the Mississippi downstream from Rock Island to Hennepin, Illinois. South of Hennepin, to Alton, Illinois, the current Illinois River follows the ancient channel used by the Mississippi River before the Illinoian Stage.[37][38]\\r\\n\\r\\nTimeline of outflow course changes[39]\\r\\n\\r\\nIn March 1876, the Mississippi suddenly changed course near the settlement of Reverie, Tennessee, leaving a small part of Tipton County, Tennessee, attached to Arkansas and separated from the rest of Tennessee by the new river channel. Since this event was an avulsion, rather than the effect of incremental erosion and deposition, the state line still follows the old channel.[40]\\r\\n\\r\\nThe town of Kaskaskia, Illinois once stood on a peninsula at the confluence of the Mississippi and Kaskaskia (Okaw) Rivers. Founded as a French colonial community, it later became the capital of the Illinois Territory and was the first state capital of Illinois until 1819. Beginning in 1844, successive flooding caused the Mississippi River to slowly encroach east. A major flood in 1881 caused it to overtake the lower 10 miles of the Kaskaskia River, forming a new Mississippi channel and cutting off the town from the rest of the state. Later flooding destroyed most of the remaining town, including the original State House. Today, the remaining 2,300 acre island and community of 14 residents is known as an enclave of Illinois and is accessible only from the Missouri side.[41]\\r\\n\\r\\nThe New Madrid Seismic Zone, along the Mississippi River near New Madrid, Missouri, between Memphis and St. Louis, is related to an aulacogen (failed rift) that formed at the same time as the Gulf of Mexico. This area is still quite active seismically. Four great earthquakes in 1811 and 1812, estimated at approximately 8 on the Richter magnitude scale, had tremendous local effects in the then sparsely settled area, and were felt in many other places in the midwestern and eastern U.S. These earthquakes created Reelfoot Lake in Tennessee from the altered landscape near the river.\\r\\n\\r\\nWhen measured from its traditional source at Lake Itasca, the Mississippi has a length of 2,320 miles (3,730 km). When measured from its longest stream source (most distant source from the sea), Brower's Spring in Montana, the source of the Missouri River, it has a length of 3,710 miles, making it the fourth longest river in the world after the Nile, Amazon, and Yangtze.[42] When measured by the largest stream source (by water volume), the Ohio River, by extension the Allegheny River, would be the source, and the Mississippi would begin in Pennsylvania.[citation needed]\\r\\n\\r\\nThe Mississippi River runs through or along 10 states, from Minnesota to Louisiana, and is used to define portions of these states' borders, with Wisconsin, Illinois, Kentucky, Tennessee, and Mississippi along the east side of the river, and Iowa, Missouri, and Arkansas along its west side. Substantial parts of both Minnesota and Louisiana are on either side of the river, although the Mississippi defines part of the boundary of each of these states.\\r\\n\\r\\nIn all of these cases, the middle of the riverbed at the time the borders were established was used as the line to define the borders between adjacent states.[43][44] In various areas, the river has since shifted, but the state borders have not changed, still following the former bed of the Mississippi River as of their establishment, leaving several small isolated areas of one state across the new river channel, contiguous with the adjacent state. Also, due to a meander in the river, a small part of western Kentucky is contiguous with Tennessee, but isolated from the rest of its state.\\r\\n\\r\\nMany of the communities along the Mississippi River are listed below; most have either historic significance or cultural lore connecting them to the river. They are sequenced from the source of the river to its end.\\r\\n\\r\\nThe road crossing highest on the Upper Mississippi is a simple steel culvert, through which the river (locally named \\"Nicolet Creek\\") flows north from Lake Nicolet under \\"Wilderness Road\\" to the West Arm of Lake Itasca, within Itasca State Park.[45]\\r\\n\\r\\nThe earliest bridge across the Mississippi River was built in 1855. It spanned the river in Minneapolis where the current Hennepin Avenue Bridge is located.[46] No highway or railroad tunnels cross under the Mississippi River.\\r\\n\\r\\nThe first railroad bridge across the Mississippi was built in 1856. It spanned the river between the Rock Island Arsenal in Illinois and Davenport, Iowa. Steamboat captains of the day, fearful of competition from the railroads, considered the new bridge a hazard to navigation. Two weeks after the bridge opened, the steamboat Effie Afton rammed part of the bridge, setting it on fire. Legal proceedings ensued, with Abraham Lincoln defending the railroad. The lawsuit went to the Supreme Court of the United States, which ruled in favor of the railroad.[47]\\r\\n\\r\\nBelow is a general overview of selected Mississippi bridges which have notable engineering or landmark significance, with their cities or locations. They are sequenced from the Upper Mississippi's source to the Lower Mississippi's mouth.\\r\\n\\r\\nA clear channel is needed for the barges and other vessels that make the main stem Mississippi one of the great commercial waterways of the world. The task of maintaining a navigation channel is the responsibility of the United States Army Corps of Engineers, which was established in 1802.[48] Earlier projects began as early as 1829 to remove snags, close off secondary channels and excavate rocks and sandbars.\\r\\n\\r\\nSteamboats entered trade in the 1820s, so the period 1830ÿ1850 became the golden age of steamboats. As there were few roads or rails in the lands of the Louisiana Purchase, river traffic was an ideal solution. Cotton, timber and food came down the river, as did Appalachian coal. The port of New Orleans boomed as it was the trans-shipment point to deep sea ocean vessels. As a result, the image of the twin stacked, wedding cake Mississippi steamer entered into American mythology. Steamers worked the entire route from the trickles of Montana, to the Ohio River; down the Missouri and Tennessee, to the main channel of the Mississippi. Only with the arrival of the railroads in the 1880s did steamboat traffic diminish. Steamboats remained a feature until the 1920s. Most have been superseded by pusher tugs. A few survive as iconsthe Delta Queen and the River Queen for instance.\\r\\n\\r\\nA series of 29 locks and dams on the upper Mississippi, most of which were built in the 1930s, is designed primarily to maintain a 9-foot-deep (2.7?m) channel for commercial barge traffic.[49][50] The lakes formed are also used for recreational boating and fishing. The dams make the river deeper and wider but do not stop it. No flood control is intended. During periods of high flow, the gates, some of which are submersible, are completely opened and the dams simply cease to function. Below St. Louis, the Mississippi is relatively free-flowing, although it is constrained by numerous levees and directed by numerous wing dams.\\r\\n\\r\\nOn the lower Mississippi, from Baton Rouge to the mouth of the Mississippi, the navigation depth is 45 feet (14?m), allowing container ships and cruise ships to dock at the Port of New Orleans and bulk cargo ships shorter than 150-foot (46?m) air draft that fit under the Huey P. Long Bridge to traverse the Mississippi to Baton Rouge.[51] There is a feasibility study to dredge this portion of the river to 50 feet (15?m) to allow New Panamax ship depths.[52]\\r\\n\\r\\nIn 1829, there were surveys of the two major obstacles on the upper Mississippi, the Des Moines Rapids and the Rock Island Rapids, where the river was shallow and the riverbed was rock. The Des Moines Rapids were about 11 miles (18?km) long and just above the mouth of the Des Moines River at Keokuk, Iowa. The Rock Island Rapids were between Rock Island and Moline, Illinois. Both rapids were considered virtually impassable.\\r\\n\\r\\nIn 1848, the Illinois and Michigan Canal was built to connect the Mississippi River to Lake Michigan via the Illinois River near Peru, Illinois. The canal allowed shipping between these important waterways. In 1900, the canal was replaced by the Chicago Sanitary and Ship Canal. The second canal, in addition to shipping, also allowed Chicago to address specific health issues (typhoid fever, cholera and other waterborne diseases) by sending its waste down the Illinois and Mississippi river systems rather than polluting its water source of Lake Michigan.\\r\\n\\r\\nThe Corps of Engineers recommended the excavation of a 5-foot-deep (1.5?m) channel at the Des Moines Rapids, but work did not begin until after Lieutenant Robert E. Lee endorsed the project in 1837. The Corps later also began excavating the Rock Island Rapids. By 1866, it had become evident that excavation was impractical, and it was decided to build a canal around the Des Moines Rapids. The canal opened in 1877, but the Rock Island Rapids remained an obstacle. In 1878, Congress authorized the Corps to establish a 4.5-foot-deep (1.4?m) channel to be obtained by building wing dams which direct the river to a narrow channel causing it to cut a deeper channel, by closing secondary channels and by dredging. The channel project was complete when the Moline Lock, which bypassed the Rock Island Rapids, opened in 1907.\\r\\n\\r\\nTo improve navigation between St. Paul, Minnesota, and Prairie du Chien, Wisconsin, the Corps constructed several dams on lakes in the headwaters area, including Lake Winnibigoshish and Lake Pokegama. The dams, which were built beginning in the 1880s, stored spring run-off which was released during low water to help maintain channel depth.\\r\\n\\r\\nIn 1907, Congress authorized a 6-foot-deep (1.8?m) channel project on the Mississippi River, which was not complete when it was abandoned in the late 1920s in favor of the 9-foot-deep (2.7?m) channel project.\\r\\n\\r\\nIn 1913, construction was complete on Lock and Dam No. 19 at Keokuk, Iowa, the first dam below St. Anthony Falls. Built by a private power company (Union Electric Company of St. Louis) to generate electricity (originally for streetcars in St. Louis), the Keokuk dam was one of the largest hydro-electric plants in the world at the time. The dam also eliminated the Des Moines Rapids. Lock and Dam No. 1 was completed in Minneapolis, Minnesota in 1917. Lock and Dam No. 2, near Hastings, Minnesota, was completed in 1930.\\r\\n\\r\\nBefore the Great Mississippi Flood of 1927, the Corps's primary strategy was to close off as many side channels as possible to increase the flow in the main river. It was thought that the river's velocity would scour off bottom sediments, deepening the river and decreasing the possibility of flooding. The 1927 flood proved this to be so wrong that communities threatened by the flood began to create their own levee breaks to relieve the force of the rising river.\\r\\n\\r\\nThe Rivers and Harbors Act of 1930 authorized the 9-foot (2.7?m) channel project, which called for a navigation channel 9 feet (2.7?m) feet deep and 400 feet (120?m) wide to accommodate multiple-barge tows.[53][54] This was achieved by a series of locks and dams, and by dredging. Twenty-three new locks and dams were built on the upper Mississippi in the 1930s in addition to the three already in existence.\\r\\n\\r\\nUntil the 1950s, there was no dam below Lock and Dam 26 at Alton, Illinois. Chain of Rocks Lock (Lock and Dam No. 27), which consists of a low-water dam and an 8.4-mile-long (13.5?km) canal, was added in 1953, just below the confluence with the Missouri River, primarily to bypass a series of rock ledges at St. Louis. It also serves to protect the St. Louis city water intakes during times of low water.\\r\\n\\r\\nU.S. government scientists determined in the 1950s that the Mississippi River was starting to switch to the Atchafalaya River channel because of its much steeper path to the Gulf of Mexico. Eventually the Atchafalaya River would capture the Mississippi River and become its main channel to the Gulf of Mexico, leaving New Orleans on a side channel. As a result, the U.S. Congress authorized a project called the Old River Control Structure, which has prevented the Mississippi River from leaving its current channel that drains into the Gulf via New Orleans.[56]\\r\\n\\r\\nBecause the large scale of high-energy water flow threatened to damage the structure, an auxiliary flow control station was built adjacent to the standing control station. This $300?million project was completed in 1986 by the Corps of Engineers. Beginning in the 1970s, the Corps applied hydrological transport models to analyze flood flow and water quality of the Mississippi. Dam 26 at Alton, Illinois, which had structural problems, was replaced by the Mel Price Lock and Dam in 1990. The original Lock and Dam 26 was demolished.\\r\\n\\r\\nThe Corps now actively creates and maintains spillways and floodways to divert periodic water surges into backwater channels and lakes, as well as route part of the Mississippi's flow into the Atchafalaya Basin and from there to the Gulf of Mexico, bypassing Baton Rouge and New Orleans. The main structures are the Birds Point-New Madrid Floodway in Missouri; the Old River Control Structure and the Morganza Spillway in Louisiana, which direct excess water down the west and east sides (respectively) of the Atchafalaya River; and the Bonnet Carr Spillway, also in Louisiana, which directs floodwaters to Lake Pontchartrain (see diagram). Some experts blame urban sprawl for increases in both the risk and frequency of flooding on the Mississippi River.[57]\\r\\n\\r\\nSome of the pre-1927 strategy is still in use today, with the Corps actively cutting the necks of horseshoe bends, allowing the water to move faster and reducing flood heights.[58]\\r\\n\\r\\nThe area of the Mississippi River basin was first settled by hunting and gathering Native American peoples and is considered one of the few independent centers of plant domestication in human history.[59] Evidence of early cultivation of sunflower, a goosefoot, a marsh elder and an indigenous squash dates to the 4th millennium BCE. The lifestyle gradually became more settled after around 1000 BCE during what is now called the Woodland period, with increasing evidence of shelter construction, pottery, weaving and other practices. A network of trade routes referred to as the Hopewell interaction sphere was active along the waterways between about 200 and 500 CE, spreading common cultural practices over the entire area between the Gulf of Mexico and the Great Lakes. A period of more isolated communities followed, and agriculture introduced from Mesoamerica based on the Three Sisters (maize, beans and squash) gradually came to dominate. After around 800 CE there arose an advanced agricultural society today referred to as the Mississippian culture, with evidence of highly stratified complex chiefdoms and large population centers. The most prominent of these, now called Cahokia, was occupied between about 600 and 1400 CE[60] and at its peak numbered between 8,000 and 40,000 inhabitants, larger than London, England of that time. At the time of first contact with Europeans, Cahokia and many other Mississippian cities had dispersed, and archaeological finds attest to increased social stress.[61][62][63]\\r\\n\\r\\nModern American Indian nations inhabiting the Mississippi basin include Cheyenne, Sioux, Ojibwe, Potawatomi, Ho-Chunk, Fox, Kickapoo, Tamaroa, Moingwena, Quapaw and Chickasaw.\\r\\n\\r\\nThe word Mississippi itself comes from Messipi, the French rendering of the Anishinaabe (Ojibwe or Algonquin) name for the river, Misi-ziibi (Great River).[64][65] The Ojibwe called Lake Itasca Omashkoozo-zaaga'igan (Elk Lake) and the river flowing out of it Omashkoozo-ziibi (Elk River). After flowing into Lake Bemidji, the Ojibwe called the river Bemijigamaag-ziibi (River from the Traversing Lake). After flowing into Cass Lake, the name of the river changes to Gaa-miskwaawaakokaag-ziibi (Red Cedar River) and then out of Lake Winnibigoshish as Wiinibiigoonzhish-ziibi (Miserable Wretched Dirty Water River), Gichi-ziibi (Big River) after the confluence with the Leech Lake River, then finally as Misi-ziibi (Great River) after the confluence with the Crow Wing River.[66] After the expeditions by Giacomo Beltrami and Henry Schoolcraft, the longest stream above the juncture of the Crow Wing River and Gichi-ziibi was named \\"Mississippi River\\". The Mississippi River Band of Chippewa Indians, known as the Gichi-ziibiwininiwag, are named after the stretch of the Mississippi River known as the Gichi-ziibi. The Cheyenne, one of the earliest inhabitants of the upper Mississippi River, called it the M?xe-?ometaa?e (Big Greasy River) in the Cheyenne language. The Arapaho name for the river is Beesniice.[67] The Pawnee name is Kickatit.[68]\\r\\n\\r\\nThe Mississippi was spelled Mississipi or Missisipi during French Louisiana and was also known as the Rivire Saint-Louis.[69][70][71]\\r\\n\\r\\nOn May 8, 1541, Spanish explorer Hernando de Soto became the first recorded European to reach the Mississippi River, which he called Ro del Espritu Santo (\\"River of the Holy Spirit\\"), in the area of what is now Mississippi. In Spanish, the river is called Ro Mississippi.[72]\\r\\n\\r\\nFrench explorers Louis Jolliet and Jacques Marquette began exploring the Mississippi in the 17th century. Marquette traveled with a Sioux Indian who named it Ne Tongo (\\"Big river\\" in Sioux language) in 1673. Marquette proposed calling it the River of the Immaculate Conception.\\r\\n\\r\\nWhen Louis Jolliet explored the Mississippi Valley in the 17th century, natives guided him to a quicker way to return to French Canada via the Illinois River. When he found the Chicago Portage, he remarked that a canal of \\"only half a league\\" (less than 2 miles (3.2?km), 3?km) would join the Mississippi and the Great Lakes.[73] In 1848, the continental divide separating the waters of the Great Lakes and the Mississippi Valley was breached by the Illinois and Michigan canal via the Chicago River.[74] This both accelerated the development, and forever changed the ecology of the Mississippi Valley and the Great Lakes.\\r\\n\\r\\nIn 1682, Ren-Robert Cavelier, Sieur de La Salle and Henri de Tonti claimed the entire Mississippi River Valley for France, calling the river Colbert River after Jean-Baptiste Colbert and the region La Louisiane, for King Louis XIV. On March 2, 1699, Pierre Le Moyne d'Iberville rediscovered the mouth of the Mississippi, following the death of La Salle.[75] The French built the small fort of La Balise there to control passage.[76]\\r\\n\\r\\nIn 1718, about 100 miles (160?km) upriver, New Orleans was established along the river crescent by Jean-Baptiste Le Moyne, Sieur de Bienville, with construction patterned after the 1711 resettlement on Mobile Bay of Mobile, the capital of French Louisiana at the time.\\r\\n\\r\\nFollowing Britain's victory in the Seven Years War the Mississippi became the border between the British and Spanish Empires. The Treaty of Paris (1763) gave Great Britain rights to all land east of the Mississippi and Spain rights to land west of the Mississippi. Spain also ceded Florida to Britain to regain Cuba, which the British occupied during the war. Britain then divided the territory into East and West Florida.\\r\\n\\r\\nArticle 8 of the Treaty of Paris (1783) states, \\"The navigation of the river Mississippi, from its source to the ocean, shall forever remain free and open to the subjects of Great Britain and the citizens of the United States\\". With this treaty, which ended the American Revolutionary War, Britain also ceded West Florida back to Spain to regain the Bahamas, which Spain had occupied during the war. In 1800, under duress from Napoleon of France, Spain ceded an undefined portion of West Florida to France. When France then sold the Louisiana Territory to the U.S. in 1803, a dispute arose again between Spain and the U.S. on which parts of West Florida exactly had Spain ceded to France, which would in turn decide which parts of West Florida were now U.S. property versus Spanish property. These aspirations ended when Spain was pressured into signing Pinckney's Treaty in 1795.\\r\\n\\r\\nFrance reacquired 'Louisiana' from Spain in the secret Treaty of San Ildefonso in 1800. The United States then secured effective control of the river when it bought the Louisiana Territory from France in the Louisiana Purchase of 1803. The last serious European challenge to U.S. control of the river came at the conclusion of War of 1812 when British forces mounted an attack on New Orleans ÿ the attack was repulsed by an American army under the command of General Andrew Jackson.\\r\\n\\r\\nIn the Treaty of 1818, the U.S. and Great Britain agreed to fix the border running from the Lake of the Woods to the Rocky Mountains along the 49th parallel north. In effect, the U.S. ceded the northwestern extremity of the Mississippi basin to the British in exchange for the southern portion of the Red River basin.\\r\\n\\r\\nSo many settlers traveled westward through the Mississippi river basin, as well as settled in it, that Zadok Cramer wrote a guide book called The Navigator, detailing the features and dangers and navigable waterways of the area. It was so popular that he updated and expanded it through 12 editions over a period of 25 years.\\r\\n\\r\\nThe colonization of the area was barely slowed by the three earthquakes in 1811 and 1812, estimated at approximately 8 on the Richter magnitude scale, that were centered near New Madrid, Missouri.\\r\\n\\r\\nMark Twain's book, Life on the Mississippi, covered the steamboat commerce which took place from 1830 to 1870 on the river before more modern ships replaced the steamer. The book was published first in serial form in Harper's Weekly in seven parts in 1875. The full version, including a passage from the then unfinished Adventures of Huckleberry Finn and works from other authors, was published by James R. Osgood & Company in 1885.\\r\\n\\r\\nThe first steamboat to travel the full length of the Lower Mississippi from the Ohio River to New Orleans was the New Orleans in December 1811. Its maiden voyage occurred during the series of New Madrid earthquakes in 1811ÿ12. The Upper Mississippi was treacherous, unpredictable and to make traveling worse, the area was not properly mapped out or surveyed. Until the 1840s only two trips a year to the Twin Cities landings were made by steamboats which suggests it was not very profitable.[77]\\r\\n\\r\\nSteamboat transport remained a viable industry, both in terms of passengers and freight until the end of the first decade of the 20th century. Among the several Mississippi River system steamboat companies was the noted Anchor Line, which, from 1859 to 1898, operated a luxurious fleet of steamers between St. Louis and New Orleans.\\r\\n\\r\\nItalian explorer Giacomo Beltrami, wrote about his journey on the Virginia, which was the first steam boat to make it to Fort St. Anthony in Minnesota. He referred to his voyage as a promenade that was once a journey on the Mississippi. The steamboat era changed the economic and political life of the Mississippi, as well as the nature of travel itself. The Mississippi was completely changed by the steamboat era as it transformed into a flourishing tourists trade.[78]\\r\\n\\r\\nControl of the river was a strategic objective of both sides in the American Civil War. In 1862 Union forces coming down the river successfully cleared Confederate defenses at Island Number 10 and Memphis, Tennessee, while Naval forces coming upriver from the Gulf of Mexico captured New Orleans, Louisiana. The remaining major Confederate stronghold was on the heights overlooking the river at Vicksburg, Mississippi, and the Union's Vicksburg Campaign (December 1862 to July 1863), and the fall of Port Hudson, completed control of the lower Mississippi River. The Union victory ending the Siege of Vicksburg on July 4, 1863, was pivotal to the Union's final victory of the Civil War.\\r\\n\\r\\nThe \\"Big Freeze\\" of 1918ÿ19 blocked river traffic north of Memphis, Tennessee, preventing transportation of coal from southern Illinois. This resulted in widespread shortages, high prices, and rationing of coal in January and February.[79]\\r\\n\\r\\nIn the spring of 1927, the river broke out of its banks in 145 places, during the Great Mississippi Flood of 1927 and inundated 27,000?sq?mi (70,000?km2) to a depth of up to 30 feet (9.1?m).\\r\\n\\r\\nIn 1962 and 1963, industrial accidents spilled 3.5?million US gallons (13,000,000?L) of soybean oil into the Mississippi and Minnesota rivers. The oil covered the Mississippi River from St. Paul to Lake Pepin, creating an ecological disaster and a demand to control water pollution.[80]\\r\\n\\r\\nOn October 20, 1976, the automobile ferry, MV George Prince, was struck by a ship traveling upstream as the ferry attempted to cross from Destrehan, Louisiana, to Luling, Louisiana. Seventy-eight passengers and crew died; only eighteen survived the accident.\\r\\n\\r\\nIn 1988, the water level of the Mississippi fell to 10 feet (3.0?m) below zero on the Memphis gauge. The remains of wooden-hulled water craft were exposed in an area of 4.5 acres (18,000?m2) on the bottom of the Mississippi River at West Memphis, Arkansas. They dated to the late 19th to early 20th centuries. The State of Arkansas, the Arkansas Archeological Survey, and the Arkansas Archeological Society responded with a two-month data recovery effort. The fieldwork received national media attention as good news in the middle of a drought.[81]\\r\\n\\r\\nThe Great Flood of 1993 was another significant flood, primarily affecting the Mississippi above its confluence with the Ohio River at Cairo, Illinois.\\r\\n\\r\\nTwo portions of the Mississippi were designated as American Heritage Rivers in 1997: the lower portion around Louisiana and Tennessee, and the upper portion around Iowa, Illinois, Minnesota and Missouri. The Nature Conservancy's project called \\"America's Rivershed Initiative\\" announced a 'report card' assessment of the entire basin in October 2015 and gave the grade of D+. The assessment noted the aging navigation and flood control infrastructure along with multiple environmental problems.[82]\\r\\n\\r\\nIn 2002, Slovenian long-distance swimmer Martin Strel swam the entire length of the river, from Minnesota to Louisiana, over the course of 68 days. In 2005, the Source to Sea Expedition[83] paddled the Mississippi and Atchafalaya Rivers to benefit the Audubon Society's Upper Mississippi River Campaign.[84][85]\\r\\n\\r\\nGeologists believe that the lower Mississippi could take a new course to the Gulf. Either of two new routesthrough the Atchafalaya Basin or through Lake Pontchartrainmight become the Mississippi's main channel if flood-control structures are overtopped or heavily damaged during a severe flood.[86][87][88][89][90]\\r\\n\\r\\nFailure of the Old River Control Structure, the Morganza Spillway, or nearby levees would likely re-route the main channel of the Mississippi through Louisiana's Atchafalaya Basin and down the Atchafalaya River to reach the Gulf of Mexico south of Morgan City in southern Louisiana. This route provides a more direct path to the Gulf of Mexico than the present Mississippi River channel through Baton Rouge and New Orleans.[88] While the risk of such a diversion is present during any major flood event, such a change has so far been prevented by active human intervention involving the construction, maintenance, and operation of various levees, spillways, and other control structures by the U.S. Army Corps of Engineers.\\r\\n\\r\\nThe Old River Control Structure, between the present Mississippi River channel and the Atchafalaya Basin, sits at the normal water elevation and is ordinarily used to divert 30% of the Mississippi's flow to the Atchafalaya River. There is a steep drop here away from the Mississippi's main channel into the Atchafalaya Basin. If this facility were to fail during a major flood, there is a strong concern the water would scour and erode the river bottom enough to capture the Mississippi's main channel. The structure was nearly lost during the 1973 flood, but repairs and improvements were made after engineers studied the forces at play. In particular, the Corps of Engineers made many improvements and constructed additional facilities for routing water through the vicinity. These additional facilities give the Corps much more flexibility and potential flow capacity than they had in 1973, which further reduces the risk of a catastrophic failure in this area during other major floods, such as that of 2011.\\r\\n\\r\\nBecause the Morganza Spillway is slightly higher and well back from the river, it is normally dry on both sides.[91] Even if it failed at the crest during a severe flood, the flood waters would have to erode to normal water levels before the Mississippi could permanently jump channel at this location.[citation needed] During the 2011 floods, the Corps of Engineers opened the Morganza Spillway to 1/4 of its capacity to allow 150,000?ft3/sec of water to flood the Morganza and Atchafalaya floodways and continue directly to the Gulf of Mexico, bypassing Baton Rouge and New Orleans.[92] In addition to reducing the Mississippi River crest downstream, this diversion reduced the chances of a channel change by reducing stress on the other elements of the control system.[93]\\r\\n\\r\\nSome geologists have noted that the possibility for course change into the Atchafalaya also exists in the area immediately north of the Old River Control Structure. Army Corps of Engineers geologist Fred Smith once stated, \\"The Mississippi wants to go west. 1973 was a forty-year flood. The big one lies out there somewherewhen the structures can't release all the floodwaters and the levee is going to have to give way. That is when the river's going to jump its banks and try to break through.\\"[94]\\r\\n\\r\\nAnother possible course change for the Mississippi River is a diversion into Lake Pontchartrain near New Orleans. This route is controlled by the Bonnet Carr Spillway, built to reduce flooding in New Orleans. This spillway and an imperfect natural levee about 4ÿ6 meters (12 to 20 feet) high are all that prevents the Mississippi from taking a new, shorter course through Lake Pontchartrain to the Gulf of Mexico.[95] Diversion of the Mississippi's main channel through Lake Pontchartrain would have consequences similar to an Atchafalaya diversion, but to a lesser extent, since the present river channel would remain in use past Baton Rouge and into the New Orleans area.\\r\\n\\r\\nThe sport of water skiing was invented on the river in a wide region between Minnesota and Wisconsin known as Lake Pepin.[96] Ralph Samuelson of Lake City, Minnesota, created and refined his skiing technique in late June and early July 1922. He later performed the first water ski jump in 1925 and was pulled along at 80?mph (130?km/h) by a Curtiss flying boat later that year.[96]\\r\\n\\r\\nThere are seven National Park Service sites along the Mississippi River. The Mississippi National River and Recreation Area is the National Park Service site dedicated to protecting and interpreting the Mississippi River itself. The other six National Park Service sites along the river are (listed from north to south):\\r\\n\\r\\nThe Mississippi basin is home to a highly diverse aquatic fauna and has been called the \\"mother fauna\\" of North American fresh water.[97]\\r\\n\\r\\nAbout 375 fish species are known from the Mississippi basin, far exceeding other North Hemisphere river basin exclusively within temperate/subtropical regions,[97] except the Yangtze.[98] Within the Mississippi basin, streams that have their source in the Appalachian and Ozark highlands contain especially many species. Among the fish species in the basin are numerous endemics, as well as relicts such as paddlefish, sturgeon, gar and bowfin.[97]\\r\\n\\r\\nBecause of its size and high species diversity, the Mississippi basin is often divided into subregions. The Upper Mississippi River alone is home to about 120 fish species, including walleye, sauger, large mouth bass, small mouth bass, white bass, northern pike, bluegill, crappie, channel catfish, flathead catfish, common shiner, freshwater drum and shovelnose sturgeon.[99][100]\\r\\n\\r\\nIn addition to fish, several species of turtles (such as snapping, musk, mud, map, cooter, painted and softshell turtles), American alligator, aquatic amphibians (such as hellbender, mudpuppy, three-toed amphiuma and lesser siren),[101] and cambarid crayfish (such as the red swamp crayfish) are native to the Mississippi basin.[102]\\r\\n\\r\\nNumerous introduced species are found in the Mississippi and some of these are invasive. Among the introductions are fish such as Asian carp, including the silver carp that have become infamous for outcompeting native fish and their potentially dangerous jumping behavior. They have spread throughout much of the basin, even approaching (but not yet invading) the Great Lakes.[103] The Minnesota Department of Natural Resources has designated much of the Mississippi River in the state as infested waters by the exotic species zebra mussels and Eurasian watermilfoil.[104]","input":"What is at the mouth of the mississippi river?"},{"output":"eight","context":"The Sinner is an American drama television series based on the novel of the same name by Petra Hammesfahr. The series was ordered on January 17, 2017.[1] The USA Network eight-episode limited series premiered on August 2, 2017 and concluded on September 20, 2017.[2]\\r\\n\\r\\n\\r\\nThe series follows the events that happen after a young mother kills someone in public but has no idea why she did so.\\r\\nThe series has received mostly positive reviews from critics. On the review aggregator website Rotten Tomatoes, the series has a rating of 94%, based on 33 reviews. The critical consensus is \\"Smartly unpredictable and led by powerful performances from a talented cast, the darkly compelling The Sinner sinks its hooks in fast and doesn't let go.\\"[14] Metacritic, which assigns a rating out of 100 to reviews from mainstream critics, reported that there were \\"generally favorable reviews\\" for the series, with an average score of 71 based on 23 reviews.[15] In August 2017, it was announced as the number one new cable series of 2017, according to Nielsen delayed viewing data.[16]","input":"How many parts are in the sinner series?"},{"output":"Colt Firearms","context":"The M4 carbine is a shorter and lighter variant of the M16A2 assault rifle. The M4 is a 5.56G45mm NATO, air-cooled, direct impingement gas-operated, magazine-fed carbine. It has a 14.5?in (370?mm) barrel and a telescoping stock.\\r\\n\\r\\nThe M4 carbine is extensively used by the United States Armed Forces and is largely replacing the M16 rifle in United States Army and United States Marine Corps combat units as the primary infantry weapon[7][8] and service rifle.\\r\\n\\r\\nThe M4 is also capable of mounting the M203 and M320 grenade launchers. The distinctive step in its barrel is for mounting the M203 with the standard hardware. The M4 is capable of firing in semi-automatic and three-round burst modes (like the M16A2 and M16A4), while the M4A1 is capable of firing in semi-auto and fully automatic modes (like the M16A1 and M16A3).\\r\\n\\r\\nFollowing the adoption of the M16 rifle, carbine variants were also adopted for close quarters operations. The CAR-15 family of weapons served through the Vietnam War. However, these carbines had design issues, as \\"the barrel length was halved\\" to 10 inches, which \\"upset the ballistics\\", reducing its range and accuracy and leading \\"to considerable muzzle flash and blast, so that a large flash suppressor had to be fitted\\".[9] \\"Nevertheless, as a short-range weapon it is quite adequate and thus, [despite] its caliber, [the XM177 'Commando'] is classed as a submachine gun.\\"[9] In 1984, Colt began work on a new carbine design called the XM4 combining the best features of the Colt Commando and later the M16A2 rifles.\\r\\n\\r\\nIn 1984, the first model was made, and it was tested in May 1985. The first models had an upper receiver with an A1 sight, and were given a shorter 11.5-inch barrel, but later ones were given a longer 14.5-inch barrel for the bayonet and the M203 Grenade Launcher. The second model was made in May 1986, and it was tested from May 1986 though May 1987; at the time it had an A2 Upper Sight, and it had the M16A2's 1:7?inch rifle twist, to use the heavier 62-grain M855 rounds. The extended barrel improved the XM4's ballistics, reduced muzzle blast and gave the XM4 the ability to mount a bayonet and the M203 grenade launcher. The XM4 was also given the cartridge deflector, as well as other minor refinements.[citation needed] In May 1991, the XM4 was renamed to the M4, and Colt made a manual.\\r\\n\\r\\nThe M4 was officially accepted into service by the U.S. military in 1994, and was first used operationally by U.S. troops deployed to Kosovo in 1999 in support of the NATO-led KFOR peacekeeping force. It would subsequently be used heavily by U.S. forces during the Global War on Terrorism, including Operation Enduring Freedom and Operation Iraqi Freedom. In the U.S. Army, the M4 had largely replaced M16A2s as the primary weapon of forward deployed personnel by 2005.[10] The M4 carbine also replaced most submachine guns and selected handguns in U.S. military service,[10] as it fires more effective rifle ammunition that offers superior stopping power and is better able to penetrate modern body armor.[citation needed]\\r\\n\\r\\nThe United States Marine Corps has ordered its officers (up to the rank of Lieutenant Colonel) and staff non-commissioned officers to carry the M4 carbine instead of the M9 handgun.[11] This is in keeping with the Marine Corps doctrine, \\"Every Marine a rifleman\\". The Marine Corps, however, chose the full-sized M16A4 over the M4 as its standard infantry rifle. United States Navy corpsmen E5 and below are also issued M4s instead of the M9.[12] While ordinary riflemen in the Marine Corps were armed with M16A4s, M4s were fielded by troops in positions where a full-length rifle would be too bulky, including vehicle operators and fireteam and squad leaders. As of 2013, the U.S. Marine Corps had 80,000 M4 carbines in their inventory.[13][14]\\r\\n\\r\\nBy July 2015, major Marine Corps commands were endorsing switching to the M4 over the M16A4 as the standard infantry rifle, just as the Army had done. This is because of the carbine's lighter weight, compact length, and ability to address modern combat situations that happen mostly within close quarters; if a squad needs to engage at longer ranges, the M27 IAR can be used as a designated marksman rifle. Approval of the change would move the M16 to support personnel, while armories already had the 17,000 M4s in the inventory needed to outfit all infantrymen who needed one.[15] In October 2015, Commandant Robert Neller formally approved of making the M4 carbine the primary weapon for all infantry battalions, security forces, and supporting schools in the U.S. Marine Corps. The switch was to begin in early 2016 and be completed by September 2016.[16] In December 2017, the Marine Corps revealed a decision to equip every Marine in an infantry squad with the M27, replacing the M4 in that part of the service.[17] MARSOC will retain the M4, as its shorter barrel is more suited to how they operate in confined spaces.[18]\\r\\n\\r\\nOn 1 July 2009, the U.S. Army took complete ownership of the M4 design.[19] This allowed companies other than Colt to compete with their own M4 designs. The Army planned on fielding the last of its M4 requirement in 2010.[19] On 30 October 2009, Army weapons officials proposed a series of changes to the M4 to Congress. Requested changes included an electronic round counter that records the number of shots fired, a heavier barrel, and possibly replacing the direct impingement system with a gas piston system.\\r\\n\\r\\nThe benefits of this, however, have come under scrutiny from both the military and civilian firearms community.[20][21] According to a PDF detailing the M4 Carbine improvement plans released by PEO Soldier, the direct impingement system would be replaced only after reviews were done comparing the direct impingement system to commercial gas piston operating system to find out and use the best available operating system in the U.S. Army's improved M4A1.[22]\\r\\n\\r\\nIn September 2010, the Army announced it would buy 12,000 M4A1s from Colt Firearms by the end of 2010, and would order 25,000 more M4A1s by early 2011. The service branch planned to buy 12,000 M4A1 conversion kits in early 2011. In late 2011, the Army bought 65,000 more conversion kits. From there the Army had to decide if it would upgrade all of its?M4s.[23]\\r\\n\\r\\nOn 21 April 2012, the U.S. Army announced to begin purchasing over 120,000 M4A1 carbines to start reequipping front line units from the original M4 to the new M4A1 version. The first 24,000 were to be made by Remington Arms Company. Remington was to produce the M4A1s from mid-2013 to mid-2014.[24] After completion of that contract, it was to be between Colt and Remington to produce over 100,000 more M4A1s for the U.S. Army. Because of efforts from Colt to sue the Army to force them not to use Remington to produce M4s, the Army reworked the original solicitation for new M4A1s to avoid legal issues from Colt.[25] On 16 November 2012, Colt's protest of Remington receiving the M4A1 production contract was dismissed.[26] Instead of the contract being re-awarded to Remington, the Army awarded the contract for 120,000 M4A1 carbines worth $77 million to FN Herstal on 22 February 2013.[27][28] The order is expected to be completed by 2018.[29]\\r\\n\\r\\nThe M4 product improvement program (PIP) is the effort by the U.S. Army to modernize its inventory of M4 service rifles. Phase I consists of converting and replacing regular M4s with the M4A1 version. This variant of the rifle is fully automatic and has a heavier barrel, and is given ambidextrous fire controls. Phase II of the PIP explored developing a new bolt carrier. 11 designs were submitted. The competition was scheduled to conclude in summer 2013, but ended in April 2012. Over six months of testing revealed that the current bolt carrier assembly outperformed the competing designs, especially in the areas of reliability, durability, and high-temp and low-temp tests. Phase II also includes a competition for a free-floating forward rail assembly. The Army may award contracts to up to three finalists in early 2013, with the selection of a final winner in early 2014. If the Army determines that the winning rail system should be procured, delivery of new rail is anticipated by the summer of 2014.[30]\\r\\n\\r\\nIn March 2015, the Army launched a market survey to see what the small-arms industry could offer to further enhance the M4A1 to an \\"M4A1+\\" standard. Several upgrade options include an extended forward rail that will allow for a free-floated barrel for improved accuracy with a low-profile gas block that would do away with the traditional triangular fixed front sight, removable front and rear flip-up back-up iron sights, a coyote tan or \\"neutral color\\" rail for reduced visual detection, a more effective flash suppressor/muzzle brake, an improved charging handle, and a new single-stage trigger module.[31] In June 2016, the M4A1+ was canceled after reviewing the offerings and determining that there were no major upgrades currently offered.[32]\\r\\n\\r\\nThe M4 and its variants fire 5.56G45mm NATO (and .223 Remington) ammunition, and are gas-operated, magazine-fed, selective fire firearms with either a multi-position telescoping stock or a fixed A2 or LE tactical stock.[33]\\r\\n\\r\\nThe M4 is a shorter and lighter variant of the M16A2 rifle, with 80% parts commonality.[34] The M4 is similar to much earlier compact M16 versions, such as the 1960s-era XM177 family. Some of those visual similarities are obvious in both weapons.\\r\\n\\r\\nAs with many carbines, the M4 is handy and more convenient to carry than a full-length rifle. The price is slightly inferior ballistic performance compared to the full-size M16, with its 5.5\\" (14?cm) longer barrel. This becomes most apparent at ranges of 200 yards (180 m) and beyond.\\r\\n\\r\\nWhile the M4's maneuverability makes it a candidate for non-infantry troops (vehicle crews, clerks and staff officers), it also makes it ideal for close quarters battle (CQB). The M4, along with the M16A4, have mostly replaced the M16A2 in the Army and Marines. The U.S. Air Force, for example, has transitioned completely to the M4 for Security Forces squadrons, while other armed personnel retain the M16A2. The US Navy uses M4A1s for Special Operations and vehicle crews.\\r\\n\\r\\nSome features of the M4 and M4A1 compared to a full-length M16-series rifle include:\\r\\n\\r\\nHowever, there have been some criticisms of the carbine, such as lower muzzle velocities and louder report due to the shorter barrel, additional stress on parts because of the shorter gas system, and a tendency to overheat faster than the M16A2.\\r\\n\\r\\nLike all the variants of the M16, the M4 and the M4A1 can be fitted with many accessories, such as night vision devices, suppressors, laser pointers, telescopic sights, bipods, either the M203 or M320 grenade launchers, the M26 MASS shotgun, forward hand grips, and anything else compatible with a MIL-STD-1913 Picatinny rail.\\r\\n\\r\\nOther common accessories include the AN/PEQ-2, AN/PEQ-15 multi-mode laser, AN/PEQ-16 Advanced Combat Optical Gunsight (ACOG), and M68 CCO. EOTech holographic weapon sights are part of the SOPMOD II package. Visible and IR (infrared) lights of various manufacturers are also commonly attached using various mounting methods. As with all versions of the M16, the M4 accepts a blank-firing attachment (BFA) for training purposes.\\r\\n\\r\\nIn January 2017, a USMC unit deployed with suppressors mounted to every infantry M4 service weapon. Exercises showed that having all weapons suppressed improved squad communication and surprise during engagements; disadvantages included additional heat and weight, increased maintenance, and the greater cost of equipping so many troops with the attachment.[35]\\r\\n\\r\\nM4 feedramps are extended from the barrel extension into the upper receiver. This can help alleviate feeding problems that may occur as a result of the increased pressure of the shortened gas system of the M4. This problem is primarily seen in full-auto applications.\\r\\n\\r\\nU.S. Special Operations Command (USSOCOM) developed the Special Operations Peculiar Modification (SOPMOD) Block I kit for the carbines used by units under its jurisdiction. The kit features an M4A1, a Rail Interface System (RIS) handguard developed by Knight's Armament Company, a shortened quick-detachable M203 grenade launcher and leaf sight, a KAC sound suppressor, a KAC back-up rear sight, an Insight Technologies AN/PEQ-2A visible laser/infrared designator, along with Trijicon's ACOG TA-01NSN model and Reflex sights, and a night vision sight. This kit was designed to be configurable (modular) for various missions, and the kit is currently in service with special operations units.\\r\\n\\r\\nA second-generation SOPMOD kit (now known as SOPMOD II) includes innovative optics, such as the Elcan Specter DR, Trijicon's ACOG TA01 ECOS model, and the Eotech 553. Block II uses the RIS II rails manufactured by Daniel Defense in both a 9.5 and 12.5 length.\\r\\n\\r\\nExcept for the very first delivery order, all U.S. military-issue M4 and M4A1 carbines possess a flat-top NATO M1913-specification (Picatinny) rail on top of the receiver for attachment of optical sights and other aiming devices  Trijicon TA01 and TA31 Advanced Combat Optical Gunsights (ACOG), EOTech 550 series holographic sights, and Aimpoint M68 Close Combat Optic (M68 CCO) being the favorite choices  and a detachable rail-mounted carrying handle. Standards are the Colt Model 920 (M4) and 921 (M4A1).\\r\\n\\r\\nVariants of the carbine built by different manufacturers are also in service with many other foreign special forces units, such as the Australian Special Air Service Regiment (SASR). While the SASR uses weapons of essentially the same pattern built by Colt for export (Colt uses different models to separate weapons for the U.S. military and those for commercial/export purposes), the British SAS uses a variant on the basic theme, the Colt Canada (formerly Diemaco) C8SFW.\\r\\n\\r\\nColt Model 925 carbines were tested and fitted with the Knight's Armament Corporation (KAC) M4 RAS under the designation M4E2, but this designation appears to have been scrapped in favor of mounting this system to existing carbines without changing the designation. The U.S. Army Field Manual specifies for the Army that adding the Rail Adapter System (RAS) turns the weapon into the M4 MWS or Modular Weapon System.\\r\\n\\r\\nThe M4A1 carbine is a fully automatic variant of the basic M4 carbine intended for special operations use. The M4A1 was introduced in May 1991, and was in service in 1994. The M4A1 has a \\"S-1-F\\" (safe/semi-automatic/fully automatic) trigger group, while the M4 has a \\"S-1-3\\" (safe/semi-automatic/3-round burst) trigger group. The M4A1 is used by almost all U.S special operation units including, but not limited to, Marine Force Recon, Army Rangers, Army Special Forces, Navy SEALs, United States Air Force Pararescue and Air Force Combat Control Teams. It has a maximum effective range of about 500?to 600 meters (550ÿ660?yd).[6] The fully automatic trigger gives a more consistent trigger pull, which leads to better accuracy.[36] According to Mark A. Westrom, owner of ArmaLite, Inc., automatic fire is better for clearing rooms than burst fire.[37]\\r\\n\\r\\nIn the last few years, M4A1 carbines have been refitted or received straight from the factory with barrels with a thicker profile under the handguard. This is for a variety of reasons such as heat dissipation during full-auto, and accuracy as a byproduct of barrel weight. These heavier barrel weapons are also fitted with a heavier buffer known as the H2. Out of three sliding weights inside the buffer, the H2 possesses two tungsten weights and one steel weight, versus the standard H buffer, which uses one tungsten weight and two steel weights. These weapons, known by Colt as the Model 921HB (for Heavy Barrel), have also been designated M4A1, and as far as the government is concerned the M4A1 represents both the 921 and 921HB.\\r\\n\\r\\nConversion of M4s to the M4A1 began in 2014, the start of all U.S. Army forces being equipped with the automatic variant.[38] Though in service with special forces, combat in Afghanistan showed the need for providing automatic suppression fires during fire and movement for regular soldiers. The 101st Airborne Division began fielding new-built M4A1s in 2012, and the U.S. 1st Infantry Division became the first unit to convert their M4s to M4A1-standard in May 2014. Upgrades included a heavier barrel to better dissipate heat from sustained automatic firing, which also helps the rifles use the M855A1 EPR that has higher proof pressures and puts more strain on barrels. The full-auto trigger group has a more consistent trigger pull, whereas the burst group's pull varies on where the fire control group is set, resulting in more predictable and better accuracy on semi-automatic fire. Another addition is an ambidextrous selector lever for easier use with left-handed shooters. The M4-M4A1 conversion only increases weapon weight from 7.46?lb (3.38?kg) to 7.74?lb (3.51?kg), counting a back-up iron sight, forward pistol grip, empty magazine, and sling. Each carbine upgrade costs $240 per rifle, for a total cost of $120 million for half a million conversions. 300 conversions can be done per day to equip a brigade combat team per week, with all M4A1 conversions to be completed by 2019.[39][40]\\r\\n\\r\\nThe Mk 18 Close Quarters Battle Receiver is an M4A1 with a 10.3-inch barrel upper receiver.[41] Current contractors for the Mark 18 are Colt and Lewis Machine & Tool (LMT) NSN 1005-01-527-2288.\\r\\n\\r\\nFor the Individual Carbine competition, Colt submitted their Enhanced M4 design, also known as the Colt Advanced Piston Carbine (APC). The weapon has a suppression-ready fluted barrel, which is lighter and cools better than previous M4 barrels. It is claimed to have \\"markedly better\\" accuracy. To improve reliability, Colt used an articulating link piston (ALP), which \\"reduces the inherent stress in the piston stroke by allowing for deflection and thermal expansion\\".[42] In traditional gas piston operating systems, the force of the piston striking the bolt carrier can push the bolt carrier downwards and into the wall of the buffer tube, leading to accelerated wear and even chipped metal. This is known as carrier tilt. The ALP allows the operating rod to wiggle to correct for the downward pressure on the bolt and transfers the force straight backwards in line with the bore and buffer assembly, eliminating the carrier tilt. This relieves stress on parts and helps to increase accuracy.[43] The Individual Carbine competition was canceled before a winning weapon was chosen.[36]\\r\\n\\r\\nThough Colt has focused its attention on carbines with 14.5-inch barrels and rifles with 20-inch barrels, Colt continues to make carbines with 11.5-inch barrels, which it calls Commandos. The Colt Model 733, is their first design, and it was made in 1987. It was referred to as the M16A2 Commando, and later the M4 Commando. Unlike the XM177, the Colt Commando was a shorter variant of the M16A2. Originally, Commandos were assembled from whatever spare parts are available, so Model 733 Commandos could have A1-style upper receivers with case deflectors or A2-style upper receivers, and M16A1-profile 1:7 or M16A2-profile 1:7 barrels. Depending on the specific models, Commandos may have had three-position fire control groups (safe/semi-automatic/three-round burst), or four-position having both full-automatic and burst. The modern Model 933 has a \\"flattop\\" receiver, with a removable carrying handle and a MIL-STD-1913 Picatinny rail, with semi-automatic and automatic fire. The Model 935 Commando has the features of the Model 933, but has three-round burst fire instead of automatic. Though originally called the M16A2 Commando, Colt markets them as the M4 Commando around 1995.\\r\\n\\r\\nIn 2014, American firearms designer Jim Sullivan provided a video interview regarding his contributions to the M16/M4 family of rifles when working for Armalite. A noted critic of the M4, he illustrates the deficiencies found in the rifle in its current configuration. In the video, he demonstrates his \\"Arm West LLC modified M4\\", with enhancements he believes necessary to rectify the issues with the weapon. Proprietary issues aside, the weapon is said to borrow features in his prior development, the Ultimax. Sullivan has stated (without exact details as to how) the weapon can fire from the closed bolt in semi-automatic and switch to open bolt when firing in fully automatic, improving accuracy. The weight of the cyclic components of the gun has been doubled (while retaining the weapon's weight at less than 8 pounds). Compared to the standard M4, which in automatic fires 750-950 rounds a minute, the rate of fire of the Arm West M4 is heavily reduced both to save ammunition and reduce barrel wear. The reduced rate also renders the weapon more controllable and accurate in automatic firing.[44]\\r\\n\\r\\nThe M4 carbine has been used for close quarters operations where the M16 would be too long and bulky to use effectively. It has been a compact, light, customizable, and accurate weapon. This has come at the cost of reliability and maintainability. Failure to maintain the M4 causes malfunctions. This became apparent as it saw continued use in the sandy environments of Iraq and Afghanistan.[45] Despite this, in post-combat surveys, 94 percent of soldiers rated the M4 as an effective weapons system.[46]\\r\\n\\r\\nBy late 2002, 89 percent of U.S. troops reported they were confident with the M4, but they had a range of problems. 34 percent of users said the handguards rattled and became excessively hot when firing, and 15 percent had trouble zeroing the M68 Close Combat Optic. 35 percent added barber brushes and 24 percent added dental picks to their cleaning kits. There were many malfunctions, including 20 percent of users experiencing a double feed, 15 percent experiencing feeding jams, and 13 percent saying that feeding problems were due to magazines. 20 percent of users were dissatisfied with weapon maintenance. Some had trouble locking the magazine into the weapon and having to chamber a round in order to lock the magazine. Soldiers also asked for a larger round to be able to kill targets with one shot. New optics and handguards made usage of the M4 easier, and good weapon maintenance reduced the number of misfeeds.[47]\\r\\n\\r\\nIn December 2006, the Center for Naval Analyses released a report on U.S. small arms in combat. The CNA conducted surveys on 2,608 troops returning from combat in Iraq and Afghanistan over the past 12 months. Only troops who fired their weapons at enemy targets were allowed to participate. 917 troops were armed with M4 Carbines, making up 35 percent of the survey. 89 percent of M4 users (816 troops) reported they were satisfied with the weapon. 90 percent (825 troops) were satisfied with handling qualities such as handguards, size, and weight. M4 users had the highest levels of satisfaction with weapon performance, including 94 percent (862 troops) with accuracy, 92 percent (844 troops) with range, and 93 percent (853 troops) with rate of fire. Only 19 percent of M4 users (174 troops) reported a stoppage, while 82 percent of those that experienced a stoppage said it had little impact on their ability to clear the stoppage and re-engage their target. 53 percent of the M4 users (486 troops) never experienced failures of their magazines to feed. 81 percent (743 troops) did not need their rifles repaired while in theater. 80 percent (734 troops) were confident in the M4's reliability, defined as level of soldier confidence their weapon will fire without malfunction, and 83 percent (761 troops) were confident in its durability, defined as level of soldier confidence their weapon will not break or need repair. Both factors were attributed to high levels of soldiers performing their own maintenance. 54 percent of M4 users offered recommendations for improvements. 20 percent of requests were for greater bullet lethality, and 10 percent was better quality magazines, as well as other minor recommendations. Some M16 users expressed their desire to be issued the M4.[48] Some issues have been addressed with the issuing of the improved STANAG magazine in March 2009,[49][50] and the M855A1 Enhanced Performance Round in June 2010.[51]\\r\\n\\r\\nIn the fall 2007, the Army tested the M4 against three other carbines in \\"sandstorm conditions\\" at Aberdeen Proving Ground, Maryland: the Heckler & Koch XM8, Fabrique Nationale de Herstal SOF Combat Assault Rifle (SCAR) and the Heckler & Koch HK416. Ten of each type of rifle were used to fire 6,000 rounds each, for a total of 60,000 rounds per rifle type.[52] The M4 suffered far more stoppages than its competitors: 882 stoppages, 19 requiring an armorer to fix. The XM8 had the fewest stoppages, 116 minor stoppages and 11 major ones, followed by the FN SCAR with 226 stoppages and the HK416 with 233.[53][54]\\r\\n\\r\\nDespite 863 minor stoppagestermed \\"class one\\" stoppages, which require 10 seconds or less to clear, or \\"class two\\" stoppages, which require more than ten seconds to clearthe M4 functioned well, with over 98 percent of the 60,000 total rounds firing without a problem. The Army said it planned to improve the M4 with a new cold-hammer-forged barrel to give longer life and more reliable magazines to reduce the stoppages. Magazine failures caused 239 of the M4's 882 failures. Army officials said the new magazines could be combat-ready by spring if testing went well.[55] The Army began issuing an improved STANAG magazine in March 2009.[49][50]\\r\\n\\r\\nAccording to the Army, the M4 only suffered 296 stoppages, and said that the high number reported could be attributed to discrepancies in the scoring process. The Army testing command stated that, if the number of stoppages caused by a broken part met some threshold, they would be eliminated from the final report pending redesign of the part. Colt also claimed that the testing conditions were unfair to the M4, as the M4s used in the test were normal guns from active inventory, with remaining service life varying randomly. Further, the trial M4s had burst-mode fire groups, which are more complicated and prone to failure than the fully automatic fire groups the other manufacturers presented for testing.[56]\\r\\n\\r\\nThere were three extreme dust tests performed in 2007. The 2nd Summer 2007 results showed a large difference from the later fall test with the M4 having 148 class 1 stoppages due to rifle malfunctions and 148 class 1 stoppages due to magazine stoppages. The full-size M16 rifle had 61 stoppages during the same extreme dust test.[57]\\r\\n\\r\\nIn early 2010, two journalists from the New York Times spent three months with soldiers and Marines in Afghanistan. While there, they questioned around 100 infantrymen about the reliability of their M4 Carbines, as well as the M16 rifle. Troops did not report to be suffering reliability problems with their rifles. While only 100 troops were asked, they fought at least a dozen intense engagements in Helmand Province, where the ground is covered in fine powdered sand (called \\"moon dust\\" by troops) that can stick to firearms. Weapons were often dusty, wet, and covered in mud. Intense firefights lasted hours with several magazines being expended. Only one soldier reported a jam when his M16 was covered in mud after climbing out of a canal. The weapon was cleared and resumed firing with the next chambered round. Furthermore, a Marine Chief Warrant Officer reported that there were no issues with his battalion's 700 M4s and 350 M16s.[58]\\r\\n\\r\\nThe reliability of the M4 has increased as the design was upgraded. In 1990, the M4 was required to fire 600 mean rounds between stoppages using M855 ammunition. In 2013, the current M4A1 version can fire 1,691 mean rounds between stoppages using M855A1 ammunition.[59]\\r\\n\\r\\nDuring the 2009 Marine Corps Infantry Automatic Rifle testing, the Colt IAR displayed a MRBS of CLASS I/II Stoppages of 952 rounds, with a MRBEFF of Class III Stoppages of 60,000 rounds.[60]\\r\\n\\r\\nAn array of firearms accessory makers have offered gas piston conversion kits for the M4. The claimed benefits include less needed lubrication for the bolt carrier group to run reliably and reduced fouling.[61] The argument against it is increased weight and reduced accuracy.[citation needed] The Enhanced M4 uses an articulating link piston operating system.\\r\\n\\r\\nComplicating the Army search for higher reliability in the M4 is a number of observations of M4 gas piston alternatives that suffer unintended design problems. The first is that many of the gas piston modifications for the M4 isolate the piston so that piston jams or related malfunction require the entire weapon be disassembled, such disassembly cannot be performed by the end user and requires a qualified armorer to perform out of field, whereas almost any malfunction with the direct-impingement system can be fixed by the end user in field. The second is that gas piston alternatives use an off-axis operation of the piston that can introduce carrier tilt, whereby the bolt carrier fails to enter the buffer tube at a straight angle, resulting in part wearing. This can also tilt the bolt during extraction, leading to increased bolt lug failures. The third is that the use of a sound suppressor results in hot gases entering the chamber, regardless of a direct-gas impingement or gas piston design choice. The gas piston system may also cause the firearm to become proprietary to the manufacturer, making modifications and changes with parts from other manufacturers difficult.[21][62]\\r\\n\\r\\nThe M4 was developed and produced for the United States government by Colt Firearms, which had an exclusive contract to produce the M4 family of weapons through 2011.[65] However, a number of other manufacturers offer M4-like firearms. Colt previously held a U.S. trademark on the term \\"M4\\".[66] Many manufacturers have production firearms that are essentially identical to a military M4, but with a 16\\" barrel. The Bushmaster M4 Type Carbine is a popular example. Civilian models are sometimes colloquially referred to as \\"M4gery\\" (/?m?f??rd??ri/ em-FOR-j?r-ee, a portmanteau of \\"M4\\" and \\"forgery\\"). Colt had maintained that it retains sole rights to the M4 name and design. Other manufacturers had long maintained that Colt had been overstating its rights, and that \\"M4\\" had now become a generic term for a shortened AR-15. In April 2004, Colt filed a lawsuit against Heckler & Koch and Bushmaster Firearms, claiming acts of trademark infringement, trade dress infringement, trademark dilution, false designation of origin, false advertising, patent infringement, unfair competition, and deceptive trade practices. Heckler & Koch later settled out of court, changing one product's name from \\"HK M4\\" to \\"HK416\\". However, on December 8, 2005, a District court judge in Maine granted a summary judgment in favor of Bushmaster Firearms, dismissing all of Colt's claims except for false advertising. On the latter claim, Colt could not recover monetary damages. The court also ruled that \\"M4\\" was now a generic name, and that Colt's trademark should be revoked.[67]\\r\\n\\r\\nSales of select-fire or fully automatic M4s by Colt are restricted to military and law enforcement agencies. No private citizen can own an M4 in a select-fire or fully automatic configuration, as this model of fully automatic rifle was developed after the 1986 ban on full automatic weapons available to be purchased by US citizens under the Firearms Owners' Protection Act of 1986, which was signed prior to the development of the M4. While many machine guns can be legally owned with a proper tax stamp from the Bureau of Alcohol, Tobacco, Firearms and Explosives, an amendment to the Firearm Owners Protection Act of 1986 barred the transfer to private citizens of machine guns made or registered in the U.S. after May 19, 1986. The only exception was for Special Occupational Taxpayers (SOT), who are licensed machine gun dealers with demonstration letters, manufacturers, and those dealing in exports and imports. As such, only the earliest Colt M4 prototypes built prior to 1986 would be legal to own by non-SOT civilians.[citation needed] The modular nature of the AR-15 design, however, makes it relatively simple to fit M4-specific components to a \\"pre-'86\\" select-fire AR-15 lower receiver, producing an \\"M4\\" in all but name.\\r\\n\\r\\nThe M4 falls under restrictions of Title II of the National Firearms Act. The 14.5?inch barrel makes the M4 a Short Barrel Rifle (SBR) and select fire capability (semi-automatic and full automatic or burst-automatic) makes the M4 a machine gun. Civilian replicas of the M4 typically have 16?inch barrels (or standard 14.5 inch M4 barrels with permanently attached flash suppressors with a total length of 16 inches) and are semi-automatic-only to meet the legal definition of a rifle under the Gun Control Act. Civilian-legal M4s are also popular with police as a patrol carbine.","input":"What brand of m4 does the military use?"},{"output":"The Oprah Winfrey Network (OWN)","context":"The Oprah Winfrey Network (OWN), named after the former daytime talk-show host Oprah Winfrey, is an American television channel owned by Harpo Studios and Discovery Inc. It debuted on January 1, 2011, in approximately 80 million homes, replacing the Discovery Health Channel (DHC).\\r\\nIn late 2012, Tyler Perry partnered with OWN to provide it with scripted television programming. One of Perry's series, The Haves and the Have Nots has consistently posted very successful ratings and has set a preeminent record on OWN, providing the network with its highest ratings to date.[2] A March 11, 2014 episode of the series brought in 3.6 million viewers, surpassing the 3.5 million that tuned in for an Oprahs Next Chapter episode which was the network's previous highest rated viewing.[2]\\r\\nAs of February 2015, OWN is available to approximately 81.9 million pay television households (70.3% of households with television) in the United States.[3]\\r\\nOn December 4, 2017, Winfrey sold a majority of her stake in OWN to Discovery Communications, rendering them majority shareholder of OWN, and will continue in her role as Chief Executive Officer of the network.[4][5]\\r\\n\\r\\n\\r\\nOWN broadcasts in both standard-definition resolution (480i) and high-definition resolution (1080i). All markets of DirecTV, AT&T Uverse, Dish Network, Cablevision and Bright House Networks carry the channel in HD. Charter Communications, Comcast, Cox, Spectrum and Verizon FiOS carry the HD feed of the channel only in some markets. PlayStation Vue carries the channel at (720p) on its service, which is broadcast over the internet.\\r\\nAs of March 2014, the network's programming primarily consists of marathons, with eight-hour daytime blocks airing multiple episodes of the same series, usually a different show each day. The primetime programming varies and is repeated in late night, followed by an overnight movie and a morning block of talk shows including Dr. Phil, Rachel Ray, and The Nate Berkus Show. The programming includes a mix of original programs, specials, documentaries, and acquired movies. The channel does not air infomercials.\\r\\nIn Canada, the former Canadian version of Discovery Health, owned by Shaw Media, did not become a Canadian version of OWN, instead shifting to a reality-based format known as Twist TV.[6] Instead, Corus Entertainment agreed to launch a Canadian version of OWNits existing channel Viva was re-launched as OWN on March 1, 2011.[7] During January and February 2011, selected OWN programming aired on both Viva and the co-owned W Network.[8] Both Shaw Media and Corus share common ownership interests from the J.R. Shaw family, but are considered separate companies.[9] Winfrey revealed to The New York Times that she is in final negotiations to release the channel in Brazil and Argentina.[10]\\r\\nAn OWN Programming Block that features popular series such as Oprah's Next Chapter, Oprah's Lifeclass and Super Soul Sunday began airing on TLC UK in April 2013.[11][12] The same programming block began airing Discovery Home & Health in Australia on August 4, 2013.[13] It was announced on September 20, 2013, that the OWN Programming Block would begin airing in South Africa on DStv beginning October 17, 2013.[14] OWN Programming Block also debuted in Poland and Russia in August and September; it also debuted on October 28, 2013, in Balkans/Romania/Bulgaria.[15]\\r\\nIn October 2012, Tyler Perry entered into a partnership with OWN. According to the terms, all of Perry's new TV projects would air on the OWN network.[16] These TV projects would comprise a total of 90 episodes and help the network fill its programming schedule with original content. These shows include a drama, The Haves and the Have Nots, a new original comedy, Love Thy Neighbor, as well as a third season of the comedy series Tyler Perry's For Better or Worse which formerly aired on TBS. All three series were renewed after their respective first seasons on OWN.\\r\\nWithin its first season, the drama series The Haves and the Have Nots has been critically acclaimed as being \\"one of OWNs biggest success stories\\" and achieved numerous heavily publicized season highs in ratings.[17]\\r\\nOn January 9, 2014, it was announced that OWN ordered a drama series If Loving You Is Wrong. The show premiered on September 9, 2014.[18]\\r\\nAs of 2015, OWN has ordered even more Perry-produced shows because all of Perry's shows gross more than a million viewers each episode. According to network president Erik Logan, Oprah Winfreys partnership with Tyler Perry has been a tremendous success, garnering millions of viewers for OWN and setting ratings records,Tylers original series have quickly become appointment viewing for a passionate audience and OWN is pleased to deliver 87 new episodes. Other shows on the network like LifeClass and Lisa Ling's 'Our America' have done fairly well, but nowhere near the success of Perry's shows.[19]\\r\\nDuring its first week after launch in January 2011, OWN was watched by an average of 505,000 viewers.[20]\\r\\nOn March 8, 2011, the channel announced that it would undergo a major overhaul.[21] On May 6, 2011, Discovery executive Peter Liguori replaced Christina Norman as chief executive officer.[22]\\r\\nOn July 13, 2011, Winfrey announced she herself would take over as chief executive officer and chief creative officer of the channel.[23]\\r\\nThe average prime-time audience for 2011 was 264,000. As a result, Lisa Erspamer, Executive Vice President of Production and Development, left the network effective January 26, 2012.[24]\\r\\nThe channel's losses were estimated to be $330 million as of May 2012.[25] The channel responded by raising the channel's retransmission consent fee (OWN was provided to cable providers effectively for free for its first two years on air) to a level more in line with other basic cable channels, a move that Discovery expected to allow the channel to break even in late 2013.[26]\\r\\nThe heavily promoted Oprah's Next Chapter episodes featuring former cyclist Lance Armstrong, in which Armstrong publicly confessed his usage of performance-enhancing drugs for the first time, drew 4.3 million viewers over two nights according to Nielsen ratings data.[27] The Thursday night installment drew 3.2 million viewers, the second-highest total in the network's history to date,[28] and the second installment on Friday night was seen by 1.1 million viewers.[27] Oprah's interview with Whitney Houston's family in March 2012 retains the distinction of being OWN's highest-rated show,[28] while the first installment of Armstrong's interview on January 17, 2013 and its repeat later that evening provided the network with its highest-rated evening to date.[27]","input":"What is the name of oprah's television network?"},{"output":"late 1970s","context":"Nursing in Australia has evolved in training and regulation since the 19th century.\\r\\nNurses are vital to the community. A career in nursing is challenging, while also rewarding as it involves helping sick individuals become healthy again. There are many ways of becoming a nurse in Australia, including TAFE or university courses. As there are many people in need of nurses nationwide, there are always jobs available for nurses. Nurses in Australia have many places they can work in, this includes hospitals and schools, with the opportunity to have flexible work hours. The salary of a nurse in Australia depends on what type of nurse they are and their years of experience.[1] There are multiple career pathways one may follow within nursing such as paediatrics, intensive care, oncology, and emergency nursing.[2]\\r\\n\\r\\n\\r\\nPrior to the transfer of nursing education to the university sector, nurses were trained on the job in a three-year course of theory and practice instruction in hospital nursing schools and on the wards commencing with a 6-week preliminary training school (PTS) with the earliest accepted entry age being 17 years. First, second and third year student nurses were often distinguished by the number of stripes on their uniform caps and or belts. Hospital-based examinations were held each year and a successful pass meant progression to the next year and a fail meant a student nurse was cut, no second chances. Three years of student nursing culminated in final year exams. Student Nurses were paid employees of the parent hospital and there was no sick leave. If a student nurse missed time off through sick leave over the three years of training this time was added on to be worked at the end of the three years and if not worked a nurse was not allowed to graduate. Hospitals awarded distinctive badges and veils (think the flying nun) upon graduation. In addition, state registering authorities awarded a badge of registration. These were generally worn with pride on the uniform collars.\\r\\nAs early as the 1930s, attempts were made to establish university credentialed nursing courses in Australia, most notably by then director of nursing at the Royal Melbourne Hospital and the University of Melbourne. As recently as the 1970s, Sandra Stacy, one of the first Australian nurses to attain a PhD enrolled in a school of anthropology to submit her thesis.\\r\\nIn the late 1970s, the Royal College of Nursing Australia pioneered a course that became the Diploma of Applied Science (Nursing), awarded by the Lincoln Institute in Melbourne and Cumberland College in Sydney. The transfer of nursing education to the university sector continued throughout the 1980s, and gradually hospital schools ceased operating. In the early 1990s, universities finally granted nursing education the same status as allied health, and granted bachelor degrees in nursing rather than diplomas for entry-level courses.\\r\\nThe first move towards baccalaureate recognition was the development of the Bachelor of Applied Science (Advanced Nursing), a post graduate degree that required registration as a registered nurse as a prerequisite to admission and completion of 16 units. This course is no longer offered, and has been superseded by the transition of \\"post basic courses\\" conducted by various hospitals as a form of in-service training to the tertiary sector. The College of Nursing still runs post graduate certificate courses for nurses in many specialities.\\r\\nThe transfer of nursing education to the university sector from the hospital setting was the result of long-time efforts by leaders in Australian nursing. It was opposed by the medical hierarchy who viewed the development of highly trained professional nurses as a threat to their monopoly on the delivery of high-level health care.[citation needed] Many nurses themselves opposed the transfer on the grounds that \\"hands on experience in hospitals\\" would be lost.[citation needed] One underlying cause of the opposition was that of societal views toward appropriate gender roles: nursing as a \\"female\\" profession and medicine as a \\"male\\" profession.\\r\\nHistorically, a \\"double\\" or \\"triple certificated sister\\" would have been a registered nurse who held general, midwifery, psychiatric, or other range of certificates. The post nominal \\"RN (DC)\\" or \\"RN (TC)\\" was used by some nurses to signify this attainment.\\r\\nNurse practitioners are being introduced into the Australian healthcare community. It was only in December 2000, that the first Nurse Practitioner was authorised to work in Australia. Since then there are approximately 700 practicing throughout Australia. To become a Nurse Practitioner you must firstly apply to the Nursing and Midwifery Board of Australia, NMBA. After applying to the NMBA you are then required to provide affirmation of the following to be considered:[3]\\r\\n{ General registration as a nurse\\r\\n{ The required hours of experience in a leading practice, which is 3 years full-time within the space of the last 6 years\\r\\n{ A Masters degree which is from an approved provided\\r\\nAs of 1 July 2010, nurses are regulated by the Nursing and Midwifery Board of Australia, an agency under the Australian Health Practitioner Regulation Agency (AHPRA) under the National Registration and Accreditation Scheme. The practice of nursing was previously governed by state and territorial nursing regulation authorities. The Australian Nursing and Midwifery Accreditation Council (ANMAC) was established in 1992 and works with these authorities to facilitate a national approach to nursing and midwifery regulation. Which has now been superseded by the Nursing and Midwifery Board of Australia (NMBA).\\r\\nNurses fall into the following major categories:\\r\\nBefore being able to work as a nurse you must become registered in Australia. There are specific registration requirements that all new applicants and applicants renewing their registration[5] must meet:\\r\\n{ Criminal history check ÿ international as of the 4th of February 2015\\r\\n{ Professional indemnity insurance arrangements\\r\\n{ English skills\\r\\n{ Continuation of professional development\\r\\n{ Recency of practice\\r\\nRegistration as a registered nurse now requires a Bachelor of Nursing, considered the foundation for any future specialisation within nursing. Postgraduate diplomas provide further vocational training for specialist areas. Masters level courses are available in both research and course work streams; a specialist course has been developed to provide preparation for registration as a nurse practitioner. Professional doctorates are also available.\\r\\nAustralia has a long tradition of post-basic courses, usually of a six-month (minor) or twelve-month (major) duration, which included midwifery, maternal and child welfare, psychiatric, perioperative (theatre nursing), intensive care, and coronary care in later years, as well as a myriad of other courses. They are now provided by the university sector as postgraduate diplomas or post graduate certificates, depending on the length and complexity.\\r\\nThere are options available for hospital trained nurses to upgrade their qualifications to a Bachelor of Nursing (post registration). However, most opt instead to undertake specialist courses such as a postgraduate diploma or certification in the area of their clinical interest.\\r\\nEnrolled nurses (Endorsed) complete a Diploma of Nursing and are trained in the \\"technical and further education\\" (TAFE) sector and also Universities, although still obtaining the same degree. Course length has been increased to 18 months to include a module that permits enrolled nurses to dispense oral medications, as well as perform intramuscular injections, subcutaneous injections, and intradermal injections when. Additional post-graduate certificates are offered. Diploma (Endorsed) and Certificate nurses can attend University to gain a Bachelor of Nursing in just 2 years, as their previous qualification allows most to enter at 2nd year.\\r\\nThe professional courtesy title \\"sister\\" has fallen into disuse and disapproval, even though it was formerly used by both male and female registered general nurses. The title \\"nurse\\" was used when addressing enrolled nurses.\\r\\nThe salary of a nurse depends on two factors: the type of nurse and the years of experience in the position.\\r\\nThe initial figure would be the commencing salary, then after years of experience the nurse could potentially receive the larger figure. The salary rates[6] above are the most current rates in Australia, which are from the 1st of April 2016.","input":"When did nursing become a degree in australia?"},{"output":"7 feet","context":"Gigantism, also known as giantism (from Greek ?ϫ? gigas, \\"giant\\", plural ?ϫ? gigantes), is a condition characterized by excessive growth and height significantly above average. In humans, this condition is caused by over-production of growth hormone[1] in childhood resulting in people between 7 feet (2.13 m) and 9 feet (2.72 m) in height.[2][3][4][5]\\r\\nIt?is a rare disorder resulting from increased levels of?growth hormone?before the fusion of the?growth plate?which usually occurs at some point soon after puberty. This is most often due to abnormal tumor growths on the?pituitary gland.[6][7] Gigantism should not be confused with?acromegaly, the adult form of the disorder, characterized by somatic enlargement specifically in the extremities and face.[8][9]\\r\\n\\r\\n\\r\\nGigantism is characterized by an excess of growth hormone (GH). This overproduction of growth hormone that brings about gigantism is virtually always caused by pituitary growths (adenomas).[7] These adenomas are on the anterior pituitary gland. They can also cause overproduction of GH's hypothalamic precursor known as growth hormone releasing hormone (GHRH).[10]\\r\\nAs a result of the excessive amounts of growth hormone, children achieve heights that are well above normal ranges.[11] The specific age of onset for gigantism varies between patients and gender, but the common age that excessive growth symptoms start to appear has been found to be around 13 years.[6] Other health complications may occur in pediatric patients with hyper-secretion of growth hormone such as hypertension. Characteristics more similar to those seen in acromegaly may occur in patients that are closer in age to adolescence since they are nearing growth plate fusion.[12]\\r\\nGrowth hormone (GH) and insulin-like growth factor-I (IGF-I) are two different substances that have been identified as influencing growth plate formation and bone growth and, therefore, gigantism. The specific mechanisms of these are still not completely understood.[6][13]\\r\\nMore broadly, GH and IGF have both been identified to be involved most stages of growth: embryonic, prenatal, and postnatal.[14][15] Moreover, the receptor gene for IGF has been shown to be particularly influential throughout various stages of development, especially prenatally. This is the same for GH receptor genes which have been known to drive overall growth throughout various pathways.[14][16]\\r\\nGrowth hormone is a precursor (upstream) of IGF-I, but each have their own independent roles in hormonal pathways. Yet both seem to ultimately come together to have a joint effect on growth.[15]\\r\\nEvaluation of growth hormone hyper-secretion cannot be excluded with a single normal GH level due to diurnal variation. However, a random blood sample showing markedly elevated GH is adequate for diagnosis of GH hyper-secretion. Additionally, a high-normal GH level that fails to suppress with administration of glucose is also sufficient for a diagnosis of GH hyper-secretion.[17]\\r\\nInsulin-like Growth Factor-1 (IGF-1) is an excellent test for evaluation of GH hyper-secretion. It does not undergo diurnal variation and will thus be consistently elevated in GH hyper-secretion and therefore patients with gigantism. A single normal IGF-1 value will reliably exclude GH hyper-secretion.[17]\\r\\nFinding a specific genetic cause for gigantism has proven to be difficult. Gigantism is the primary example of growth hormone hyper-secretion disorders, a group of illnesses that are not yet deeply understood.[6]\\r\\nSome common mutations have been associated with gigantism. Pediatric gigantism patients have shown to have duplications of genes on a specific chromosome, Xq26. Typically, these patients also experienced an onset of typical gigantism symptoms before reaching the age of 5. This indicates a possible linkage between gene duplications and the gigantism.[18]\\r\\nAdditionally, DNA mutations in the aryl hydrocarbon receptor interacting protein (AIP) gene are common in gigantism patients. They have been found to be present in about 29 percent of patients with gigantism.[7] AIP is labeled as a tumor suppressor gene and a pituitary adenoma disposition gene.[7][19]\\r\\nMutations in AIP sequencing can have deleterious effects by inducing the development of pituitary adenomas which in turn can cause gigantism.[7][19]\\r\\nTwo specific mutations in the AIP gene have been identified as possible causes of pituitary adenomas. These mutations also have the ability to cause adenoma growth to occur early in life.[20] This is typical in gigantism.\\r\\nAdditionally, a large variety of other known genetic disorders have been found to influence the development of gigantism such as multiple endocrine neoplasia type 1 and 4, McCune-Albright syndrome, Carney complex, familial isolated pituitary adenoma, X-linked acrogigantism (X-LAG).[7][21]\\r\\nAlthough various gene mutations have been associated with gigantism, over 50 percent of cases cannot be linked to genetic causes, showing the complex nature of the disorder.[6]\\r\\nMany treatments for gigantism receive criticism and are not accepted as ideal. Various treatments involving surgery and drugs have been used to treat gigantism.[22]\\r\\nPegvisomant is one pharmaceutical drug which has received attention for being a possible treatment route for Gigantism. Reduction of the levels of IGF-I as a result of pegvisomant administration can be incredibly beneficial for the pediatric gigantism patients.[23]\\r\\nAfter treatment with pegvisomant, high growth rates, a feature characteristic of gigantism, can be significantly decreased.[23] Pegvisomant has been seen to be a powerful alternative to other treatments such as somatostatin analogues, a common treatment method for acromegaly, if drug treatment is paired with radiation.[24]\\r\\nFinding the optimal level of pegvisomant is important so normal body growth is not negatively affected. In order to do this, titration of the medication can be used as a way to find the proper administration level.[22]\\r\\nSee acromegaly for additional treatment possibilities.\\r\\nThe term is typically applied to those whose height is not just in the upper 1% of the population but several standard deviations above mean for persons of the same sex, age, and ethnic ancestry. The term is seldom applied to those who are simply \\"tall\\" or \\"above average\\" whose heights appear to be the healthy result of normal genetics and nutrition. Gigantism is usually caused by a tumor on the pituitary gland of the brain. It causes growth of the hands, face, and feet.[25][better?source?needed] In some cases the condition can be passed on genetically through a mutated gene.[26]\\r\\nOther names somewhat obsolete for this pathology are hypersoma (Greek: hyper over the normal level; soma body) and somatomegaly (Greek; soma body, genitive somatos of the body; megas, gen. megalou great). In the past, while many of them were social outcasts because of their height, some (usually unintentionally) found employment in Friedrich Wilhelm I's famous Potsdam Giants regiment.\\r\\nMany of those who have been identified with gigantism have suffered from multiple health issues involving their circulatory or skeletal system, as the strain of maintaining a large, heavy body places abnormal demands on both the bones and the heart.\\r\\nReports of gigantism exist throughout history, with some nations and tribes taller than others. The giants of Crete are listed in various historic sources, beginning with Titan, a Greek mythological giant, and including Gigantus, after whom giants and gigantism are named. Rhodes is another island where giants were said to have lived, with the Colossus of Rhodes, a giant statue of a giant patron god Helios. Goliath, a giant mentioned in the Bible, was a Philistine warrior who was killed by David in a battle between the Israelites and the Philistines. A member of Goliath's family is also recorded as having six fingers on each hand and six toes on each foot.[27]","input":"How tall do you need to be to be a giant?"},{"output":"Jack Sugden","context":"Emmerdale (known as Emmerdale Farm until 1989) is a British soap opera set in Emmerdale (known as Beckindale until 1994), a fictional village in the Yorkshire Dales. Created by Kevin Laffan, Emmerdale Farm was first broadcast on 16 October 1972. Produced by ITV Yorkshire, it has been filmed at their Leeds studio since its inception. The programme has been broadcast in every ITV region.\\r\\nThe series originally appeared during the afternoon until 1978, when it was moved to an early-evening time slot in most regions; London and Anglia followed during the mid-1980s. Until December 1988, Emmerdale took seasonal breaks; since then, it has been broadcast year-round.\\r\\nEpisodes air on ITV weekday evenings at 19:00, with a second Thursday episode at 20:00. The programme began broadcasting in high definition on 10 October 2011. Emmerdale is the United Kingdom's second-longest-running television soap opera (after ITV's Coronation Street), and attracts an average of five to seven million viewers per episode.\\r\\nOctober 2012 marked the 40th anniversary of the show. During that month, the show made a live episode to mark the anniversary.\\r\\nThe premise of Emmerdale Farm was similar to the BBC radio soap opera The Archers, focusing on a family, a farm and characters in a nearby village. The programme's farmyard filming was originally modelled on RT's The Riordans, an Irish soap opera which was broadcast from the mid-1960s to the end of the 1970s. The Riordans broke new ground for soap operas by being filmed largely outdoors (on a farm, owned on the programme by Tom and Mary Riordan) rather than in a studiothe usual practice of British and American soap operas. The programme pioneered farmyard location shooting, with farm animals and equipment. During the 1960s and 1970s, outdoor filming of television programmes with outdoor broadcast units (OBUs) was in its infancy due to higher costs and reliance on the weather. The Riordans' success demonstrated that a soap opera could be filmed largely outdoors, and Yorkshire Television sent people to its set in County Meath to see the programme's production firsthand.[3][4]\\r\\nEmmerdale has had a large number of characters since it began, with its cast gradually expanding in size. The programme has also had changing residences and businesses for its characters, including a bed-and-breakfast and a factory.\\r\\nThe Miffield estate was the largest employer in the village of Beckindale, 39 miles (63?km) from Bradford and 52 miles (84?km) from Leeds. Lord Miffield leased Emmerdale Farm, on the edge of the village, to the Sugden family during the 1850s in gratitude after Josh Sugden sacrificed his life for the earl's son in the Crimean War. Josh's grandson Joseph married Margaret Oldroyd and their son, Jacob, was born in January 1916. During the 1930s, Jacob Sugden purchased Emmerdale Farm. In 1945 he married Annie Pearson, daughter of farm labourer Sam Pearson. Margaret Sugden died in 1963, and Joseph died the following year.\\r\\nJacob Sugden ran the farm into the ground, drinking away its profits. The badly-maintained farm's future looked bleak at his death on 10 October 1972. He was survived by his wife Annie, two sons and a daughter: Jack, the eldest; Peggy and Joe, the youngest of the three. These characters formed the basis of Emmerdale Farm.\\r\\nCharacter types on Emmerdale have included \\"bad boys\\", such as Cain Dingle, Ross Barton, Carl King, Robert Sugden and Aaron Livesy; \\"bitches\\", such as Kim Tate, Charity Tate, Nicola King, Chrissie White, Kelly Windsor and Sadie King; \\"villains\\", such as Cameron Murray, Lachlan White, Pierce Harris, Steph Stokes, Rosemary King, Gordon Livesy, Emma Barton and Sally Spode; caring characters, such as Laurel Thomas, Emily Kirk, Lisa Dingle, Paddy Kirk, Ashley Thomas and Ruby Haswell; sassy females, such as Chas Dingle, Val Pollard, Viv Hope, Nicola King, Faith Dingle, Rebecca White and Leyla Harding, and comedy characters such as Kerry Wyatt, Bernice Blackstock, David Metcalfe, Val Pollard, Paddy Kirk, Faith Dingle, Seth Armstrong, Dan Spencer and Jimmy King. The show has had a number of matriarchs, including Diane Sugden, Viv Hope, Lisa Dingle, Annie Sugden, Faith Dingle and Moira Barton. Older characters in Emmerdale include Edna Birch, Betty Eagleton, Eric Pollard, Pearl Ladderbanks, Sandy Thomas, Seth Armstrong, Alan Turner, Sam Pearson, Lily Butterfield and Len Reynolds.\\r\\nThe first episode of Emmerdale Farm, aired on 16 October 1972, began with Jacob Sugden's funeral. Jacob upset the family when he left the farm to his eldest son, Jack, who left home at 18 in 1964 and had not returned. Jack appeared in the opening episode, avoiding the funeral and waiting for the Sugdens at Emmerdale Farm. Over the next few months Jack sold a share of the farm to his mother Annie, brother Joe, sister Peggy and grandfather Sam Pearson. Emmerdale Farm Ltd was formed when Henry Wilks bought Sam's share of the estate. The first episode, along with the others, have been repeated and released on a variety of media.[5]\\r\\nCharacters introduced in the first episode were:\\r\\nThe show's focus, initially on the farm and the Sugden family, moved to the nearby village of Beckindale. Reflecting this change, on 14 November 1989 its title was changed to Emmerdale. Coinciding with the title change was the introduction of the Tate family. These changes and more exciting storylines and dramatic episodes, such as Pat Sugden's 1986 car crash and the 1988 Crossgill fire, gradually began to improve the soap's popularity under new executive producer Keith Richardson. Richardson produced the programme for 24 years, overseeing its transformation from a minor, daytime, rural drama into a major UK soap opera.[6] The Windsor family arrived in 1993.\\r\\nBy 1993 Emmerdale was beginning its third decade on the air. In December of that year, one particular episode emerged as a major turning point for the show's history. Meant to evoke memories of the Lockerbie Disaster whose fifth anniversary was just nine days prior, the Emmerdale episode written for 30 December attracted its highest-ever audience (over 18?million) by featuring a plane crashing into the village, killing four people.[7] According to Nick Smurthwaite, the episode brought forth not just ratings but \\"complaints from aghast viewers.\\"[7] Nevertheless, the episode proved to be \\"brilliant television\\", as the highly-rated episode \\"allowed the writers to get rid of much dead wood, and reinvent the soap virtually from scratch\\", which included survivors changing the village name from Beckindale to Emmerdale.[7]\\r\\nEmmerdale had dramatic storylines for the rest of the 1990s and new long-term characters, such as the Dingle family, were introduced. The Tates became the soap's leading family during the decade, overshadowing the Sugdens and remaining at Home Farm for 16 years. Family members left or died and the last, Zoe, left in 2005. The early and mid-2000s included episodes with a storm (a similar, less-major storyline 10 years after the plane crash), a bus crash, the Kings River explosion, Sarah Sugden's death in a barn fire and the Sugden house fire (set in 2007 by Victoria Sugden, who was seeking the truth about her mother's death). It also saw the introduction of major long-term characters, including the King family and Cain and Charity Dingle (who left before returning in 2009).[8]\\r\\nIn 2009 the longest-tenured character, Jack Sugden, was killed off after the death of actor Clive Hornby (who had played Jack since 1980). Jack's funeral featured the first on-screen appearance in 13 years of Annie Sugden (Sheila Mercier). Early that year, executive producer Keith Richardson was replaced by former series producer Steve November (later replaced by John Whiston). Gavin Blyth became the series producer, followed by Stuart Blackburn after his death.\\r\\nEmmerdale celebrated its 40th anniversary on 16 October 2012. On 1 May 2012, it was announced that the show would have its first-ever live episode.[9] On 25 June 2012, it was announced that Tony Prescott, who directed the 50th anniversary live episode of Coronation Street in December 2010 would direct the episode.[10] On 23 July it was reported that an ITV2 backstage show, Emmerdale Uncovered: Live, would be broadcast after the live episode.[11] On 14 August, it was announced that the production team was building a new Woolpack set for the live episode. Although Emmerdale's village and interior sets are miles apart, its producers wanted The Woolpack to feature in the live episode.[12] On 31 August, it was announced that Emmerdale had created and filmed a live music festival with performances by Scouting for Girls and The Proclaimers.[13] On 6 September, it was confirmed that the One-hour live episode would include an unexpected death, two weddings and two births.[14]\\r\\nEmmerdale Live aired on 17 October 2012, in the middle of the 40th anniversary week, with the death revealed to be Carl King's. The story of Carl's death took the show into 2013, when a new series producer replaced Blackburn (who became producer of Coronation Street).\\r\\nAt the beginning of August 2015, Emmerdale introduced a new storyline: \\"Summer Fate\\", with the tagline \\"The choices we make are the paths we take. Who will meet their summer fate?\\". A promo for the storyline was released on 13 July. A disaster storyline had been rumoured, confirmed by the promo. The disaster was identified on 1 August, two days before the disaster week began, as a helicopter crash. The crash was triggered by an argument between Chrissie and Robert Sugden; Chrissie set Robert's car ablaze, causing exploding gas canisters to collide with a helicopter. The helicopter crashed into the village hall during Debbie Dingle and Pete Barton's wedding reception. Regular characters Ruby Haswell and Val Pollard were killed in the aftermath of the crash. Although Ross Barton was apparently murdered by his brother Pete, it was learned three weeks later that he survived.\\r\\nEmmerdale has featured a number of families, some defining an era of the show:\\r\\nThe Sugdens and their relatives, the Merricks and the Skilbecks, were at the centre of the show during the series' first two decades in the 1970s and 1980s (the Emmerdale Farm era). The Sugdens, owners of Emmerdale Farm, were its first family. Many of its members, and those of the Merrick and Skilbeck families, have left or been killed off since the mid-1990s.\\r\\nDecember 1984 saw the arrival of Caroline Bates; her teenage children, Kathy and Nick, followed in late 1985. Caroline left the show in 1989, returning for guest appearances in 1991, 1993-1994 and 1996. Nick was written out of the show when he was sentenced to ten years in prison in 1997. Kathy and her niece, Alice, remained in the village until late 2001; by then, Kathy had outlived two husbands. Through her, the Bateses are related to two of Emmerdale's central families: the Sugdens (through Jackie Merrick) and the Tates (through Chris Tate).\\r\\nSugdens remaining in the village include Jack's widow, Diane; his three children, Andy, Robert and Victoria Barton; Andy's children Sarah and Jack (the latter born on the show's 40th anniversary), and Robert's son Sebastian. Other families followed: the middle-class Windsors in 1993 (known as the Hope family after Viv's 2001 remarriage to Bob Hope) and the ne'er-do-well Dingles in 1994.\\r\\nThe Tate, Windsor-Hope and Dingle families predominated during the 1990s and 2000s. The era's storylines included the 1993 plane crash, the 1994 Home Farm siege, the 1998 post-office robbery, the 2000 bus crash, the 2003ÿ04 storm and the 2006 King show-home collapse. By the mid- to late-2000s, the last of the Tates (Zoe, daughter Jean and nephew Joseph) had emigrated to New Zealand. In 2009, Chris Tate's ex-wife Charity and their son Noah returned to the village. In 2017, Joseph Tate returned to the village. Members of the Windsor-Hope family left the village in early 2006, and Viv Hope was killed off in a village fire in February 2011 after nearly 18 years on the show. As of 2017, only Donna Windsor's daughter, April, and the Hope branch of the family (Bob and his children, twins Cathy and Heathcliff) remain.\\r\\nThe King family arrived in 2004 (as the Tates departed), but, apart from Jimmy King and his three children, Elliott, Angelica and Carl, its members have been killed off.\\r\\nIn 2018, most of the Dingles remained, with having actually increased their numbers in Emmerdale over recent years. Their circumstances had changed in their two decades in the village; Chas Dingle owned half of The Woolpack, with Charity Dingle owning the other half, and Marlon was a chef there. In 2014, the Dingles, Bartons and Whites are the central families; the Bartons are a farming family, and the Whites currently own Home Farm. In 2018, the Barton and White families had slowly been diminished, while the Sugden, and later the Tate, family had been brought back into front-burner storylines.\\r\\nOver the years, Emmerdale has highlighted a range of different social issues, including rape, cancer, miscarriage, dementia, homosexuality, arson, murder, HIV, sexual assault, post traumatic stress disorder, brain aneurysm, adultery, domestic violence, financial problems, embezzlement, sexual abuse, alcoholism, drug addiction, anorexia, teenage pregnancy, gambling addiction, bereavement, fraud, suicide, mesothelioma, schizophrenia, manslaughter, becoming a parent in later life, sudden infant death syndrome, self-harming, assisted suicide, epilepsy and premature births.\\r\\nAn average Emmerdale episode generally attracts 6ÿ8?million viewers, making it one of Britain's most popular television programmes. During the 1990s, the series had an average of 10ÿ11?million viewers per episode. On 30 December 1993, Emmerdale had its largest audience (18?million) when a plane crashed into the village. On 27 May 1997, 13?million viewers saw Frank Tate die of a heart attack after the return of wife Kim. On 20 October 1998, 12.5?million viewers saw The Woolpack explode after a fire.\\r\\nThe village storm on 1 January 2004 attracted 11.19?million viewers. 18 May 2004 episode in which Jack Sugden was shot by his adopted son, Andy, attracted 8.27?million viewers. On 17 March 2005, 9.39?million watched Shelly Williams fall from the Isle of Arran ferry. Zoe Tate left the show after 16 years on 22 September 2005 before 8.58 million viewers, marking her departure by blowing up Home Farm. On 13 July 2006, the Kings River house collapse was seen by 6.90?million viewers. Cain Dingle left on 21 September 2006, before an audience of 8.57?million viewers. On Christmas Day 2006, 7.69?million saw Tom King murdered on his wedding day. Billy Hopwood crashed his truck into a lake on 1 February 2007, attracting 8.15?million viewers. The end of the \\"Who Killed Tom King?\\" storyline on 17 May 2007, had an audience of 8.92 million.\\r\\nOn 14 January 2010, 9.96?million saw Mark Wylde shot dead by wife Natasha. Natasha's 27 October confession to daughter Maisie attracted an audience of nearly 8?million. On 13 January 2011, 9.15?million saw a fire kill Viv Hope and Terry Woods. The live 40th-anniversary episode on 17 October 2012, drew an audience of 8.83?million. On 16 October 2013, 8.37?million watched Cameron take the occupants of The Woolpack hostage and shoot Alicia. The next day, 9.28?million viewers saw Cameron Murray die.[15]\\r\\nLocation shooting was originally filmed in the village of Arncliffe in Littondale, a quiet valley in the Yorkshire Dales. The Falcon, the village hotel, served as the fictional Woolpack Inn. When the filming location became public it was moved to the village of Esholt in 1976, where it remained for 22 years.\\r\\nFilming returned to Esholt for a one off episode in 2016 for the Ashley Thomas dementia special which aired December 2016. The location was used to represent Ashley's onset of dementia to the viewer.\\r\\nThe original Emmerdale Farm buildings are near the village of Leathley. Creskeld Hall, in Arthington, (Home Farm). The buildings are one of the few original filming locations used for the entire series, and have been involved in many storylines.\\r\\nConstruction of a purpose-built set began on the Harewood estate in 1996, and it has been used since 1997. The first scenes filmed on the set (the front of The Woolpack) were broadcast on 17 February 1998. The Harewood set is a replica of Esholt, with minor alterations.\\r\\nThe Harewood houses are timber-framed and stone-faced. The village is built on green-belt land, with its buildings classified as \\"temporary structures\\" which must be demolished within ten years unless new planning permission is received. There is no plan to demolish the set, and a new planning application has been drawn up. The set includes a church and churchyard, where the characters who have died on the series are buried.\\r\\nButlers Farm is Brookland Farm, a working farm in the nearby village of Eccup. Farmyard and building exteriors are filmed at Brookland, with interior house shots filmed in the studio.\\r\\nLocation filming is also done in the City of Leeds and other West Yorkshire locations; the fictional market town of Hotten is Otley, 10 miles North West of Leeds. Benton Park School in Rawdon and the primary school in Farnley were also used for filming. Interiors are primarily filmed at Yorkshire Television's Emmerdale Production Centre in Leeds, next to Yorkshire's Leeds Studios.[16] As of 28 March 2011, HD-capable studios in the ITV Studios building were used for most of the interior scenes.\\r\\nFour farms have been featured on Emmerdale:\\r\\nEmmerdale's first sponsor (from 14 December 1999 to 20 February 2002) was Daz detergent, followed by Heinz Tomato Ketchup and Heinz salad cream from May 2003 to May 2005. Reckitt Benckiser took over until 2009, advertising Calgon, Air Wick, Veet, and Lemsip. Tombola Bingo underwrote the show from November 2009 to March 2012, followed by Bet365 Bingo until March 2014. McCain Foods began a two-year, S8 million sponsorship on 7 April 2014.[17]\\r\\nThe 12 actors who have appeared in the series for 20 years or more are listed in the table below. The longest-tenured actor and the longest-serving cast member overall is Chris Chittell who has played Eric Pollard for 32 years. The longest-tenured actresses are Sheila Mercier (Annie Sugden) and Jane Cox (Lisa Dingle) with 22 years.\\r\\nEmmerdale was first broadcast two afternoons a week in 1972, and it later moved to a 19:00 slot. The number of episodes has increased, to its current six half-hour episodes each week. Each episode is filmed two to four weeks before it is broadcast on ITV.\\r\\nEmmerdale reaches viewers in the Republic of Ireland via UTV Ireland, which broadcasts the series simultaneously with ITV in the UK with a live feed from London. Breaking news on ITV would interrupt the broadcast. Emmerdale was broadcast during the day on RT One from 1972 to 2001 before it moved to TV3. RT were several months behind; for many years, they broadcast the show five days a week (instead of ITV's three days a week) and took a break during the summer. As the series began a five-night week, RT fell behind the ITV broadcasts; the gap between RT One's last episode and TV3's first episode was about three months.[18]\\r\\nThe series has appeared in Sweden as Hem till g?rden (\\"Home to the Farm\\") since the 1970s?ÿ originally on TV2 and since 1994 on TV4. Two episodes are broadcast weekdays at 11:35. Emmerdale is the most-watched daytime non-news programme in Sweden, attracting 150,000 to 200,000 viewers daily.[19] Episodes are repeated overnight on TV4 and in prime time on digital channel TV4 Guld.\\r\\nThe programme appears in Finland on MTV3 on primetime, 17:55ÿ18:25 and 18:25ÿ18:55 Monday to Friday, two episodes a day, with repeats of each episode the following weekday morning between 9:00 and 11:00. Episodes originally aired in the UK in June and July 2016 were broadcast in Finland in March 2017. Emmerdale attracts an average of 350,000 to 450,000 viewers per episode, and is the most watched non-Finnish every-weekday program in Finnish television.[20]\\r\\nEmmerdale is broadcast in New Zealand weekdays on ONE, with an hour-long episode Monday to Thursday and a half-hour episode on Friday from 12:30 to 13:00. It is the second-most-watched daytime programme, after the news.[21] Episodes are broadcast a month behind ITV's.\\r\\nEmmerdale was broadcast in Australia for the first time in July 2006, when UKTV began airing the 2006 series with episode 4288.[22][23] As of April 2016, UKTV episodes are from July 2014, twenty one months behind the UK airings.","input":"Who is the longest serving member of emmerdale?"},{"output":"15 September 2017","context":"Sensors:\\r\\nOther:\\r\\nThe Samsung Galaxy Note 8 (marketed as Samsung Galaxy Note8) is an Android phablet smartphone designed, developed and marketed by Samsung Electronics. Unveiled on 23 August 2017, it is the successor to the discontinued Samsung Galaxy Note 7. It was released on 15 September 2017.\\r\\nThe Note 8 improves on the core device specifications and hallmark S Pen features of previous devices. While retaining the overall same look and same approximate size as the Galaxy S8+, it features an upgraded processor and, for the first time in Samsung's smartphone history, a dual-camera system on the rear of the device; one functions as a wide-angle lens and the other as a telephoto lens, with both featuring 12 MP resolution and optical image stabilization. The S Pen has increased pressure sensitivity levels and its software has been upgraded to offer improved notetaking capabilities on the always-on display, as well as animated GIF and improved translation features.\\r\\n\\r\\n\\r\\nOn 20 July 2017, Samsung tweeted a teaser video showing a darkened device with a stylus, stating the date of its next \\"Unpacked\\" event as 23 August 2017.[3][4] The Samsung Galaxy Note 8 was unveiled at that event, with a release date on 15 September 2017.[5]\\r\\nThe Note 8 is powered by an Exynos 8895[6] or Snapdragon 835 processor, depending on geographic region,[5][7] along with 6 GB of RAM.[5][7] It has a 6.3-inch 1440p Super AMOLED display with curved edges similar to the Galaxy S8, but with slightly more flat surface area.[5][7] It is Samsung's first phone to feature a dual-lens camera system, which comes with a 12 MP wide-angle lens with f/1.7 aperture and a 12 MP telephoto lens with f/2.4 aperture, both equipped with optical image stabilization.[5][7] In the U.S., it is sold with 64 GB of internal storage, along with microSD card support, but increases storage to 128 GB and 256 GB internationally.[5][7] The handset features a fingerprint scanner next to the rear camera,[5] and retains facial recognition and iris recognition similar to the S8.[8]\\r\\nThe Note 8 comes bundled with high-end AKG-tuned earbuds[9] as well as the Note series' proprietary S Pen. Compared to the Note 5, the S Pen in the Note 8 has enhanced levels of pressure sensitivity (4,096 distinct levels of pressure),[7] though The Verge noted that those enhancements were featured in the defunct Note 7.[5] Ports include a 3.5 mm headphone jack and a USB-C port for charging and data transfer.[5] It has support for Samsung DeX as well, letting Note 8 users connect their device to a dock and monitor to enable a PC-like computing environment with mouse and keyboard input.[5]\\r\\nBoth the Note 8 and its S Pen are certified with IP68 rating for water and dust resistance, and are available in black, gray, gold and blue,[7] though the latter two colors are exclusive to international markets.[5] Similar to the Galaxy S8, the Note 8 has a dedicated physical key for launching the Bixby virtual assistant.[5][7]\\r\\nSome devices will not be able to have their battery charged if the battery is completely depleted.[10]\\r\\nA limited edition version for the 2018 Winter Olympics was made, with a white back, gold accents, and themed wallpapers.[11]\\r\\nThe Note 8 comes with Android 7.1.1 \\"Nougat\\" with Samsung's own custom user interface pre-installed.[5][7] The S Pen offers expanded software features, including \\"Live Message\\" for the creation of handwritten notes combined with emojis resulting in short animated GIFs. Users can remove the S Pen from the device and immediately write notes on the display through \\"Screen Off Memo\\", which works due to the screen's always-on capabilities. The screen can collect up to 100 notes and allows the user to easily go back to notes pinned directly on the always-on screen. A \\"Translate\\" feature now recognizes punctuation marks, letting users highlight entire sentences rather than single words, and supports 71 different languages.[12][5] The edges of the screen on the Note 8 allow the user to open two apps at once in a multi-window view, dubbed \\"App Pair\\".[12] In the Camera application, a new \\"Live Focus\\" effect lets users adjust the intensity of background blur both before and after capturing photos, while \\"Dual Capture\\" makes both rear cameras take individual photos of the same subject, with one acting as a close-up shot and the other from a distance capturing the whole scene.[12]\\r\\nDxOMark, a camera testing company, gave the Note 8 a rating of 94, being the joint-highest score of any phone, shared with the iPhone 8 Plus.[13][14] Just a few days after the report of the Note 8 was published, the Pixel 2 was tested and given a 98 score, besting them both.[15][16] Although, the Note 8 has the best Optical Image Stabilization of any smartphone.[17]","input":"When was the samsung galaxy note 8 released?"},{"output":"400,000","context":"","input":"How many species have been named in the order coleoptera?"},{"output":"the Indigenous Yucatec Maya language","context":"\\r\\n\\r\\nApocalypto is a 2006 American epic adventure film directed and produced by Mel Gibson and written by Gibson and Farhad Safinia. The film features a cast of Native American and Indigenous Mexican actors consisting of Rudy Youngblood, Raoul Trujillo, Mayra Srbulo, Dalia Hernndez, Ian Uriel, Gerardo Taracena, Rodolfo Palacios, Bernardo Ruiz Juarez, Ammel Rodrigo Mendoza, Ricardo Diaz Mendoza and Israel Contreras. Similar to Gibson's earlier film The Passion of the Christ, all dialogue is in a modern approximation of the ancient language of the setting. Here, the Indigenous Yucatec Maya language is spoken, with subtitles, which sometimes refer to the language as Mayan. This was the last film Gibson directed until 2016's Hacksaw Ridge ten years later.\\r\\n\\r\\nSet in Veracruz, Los Tuxtla around the year 1511, Apocalypto depicts the journey of Jaguar Paw, a Mesoamerican hunter and his fellow tribesmen captured by an invading force after the destruction of their village who must escape human sacrifice at a time when the Maya civilization is about to come to an end. The film was a box office success, grossing over $120 million worldwide, and received mostly positive reviews, with critics praising Gibson's direction, Dean Semler's cinematography, the performances of the cast, and the portrayal of Mayan civilization.\\r\\n\\r\\nWhile hunting in the Mesoamerican rainforest, Jaguar Paw, his father Flint Sky, and their fellow tribesmen encounter a procession of refugees fleeing warfare. The group's leader explains that their lands were ravaged and they seek a new beginning. He asks for permission to pass through the jungle. Flint Sky comments to his son that the visitors were sick with fear, and urges him to never allow fear to infect him.\\r\\n\\r\\nAt sunrise the next morning the tribe's village is raided by a group led by Zero Wolf. Huts are set on fire, many villagers are killed, including Flint Sky, and the rest of the adults are taken captive. Jaguar Paw's heavily pregnant wife Seven and their young son Turtles Run escape by hiding in a small natural pit cave that also serves as a water reservoir, but are left trapped when Jaguar Paw is captured. The raiders then lead the captives on a long forced march through the jungle, having left the children behind to fend for themselves.\\r\\n\\r\\nOn the journey Cocoa Leaf, a badly wounded captive, is killed by the sadistic Middle Eye, eliciting anger from Zero Wolf, who threatens his fellow raider with death if he kills another captive without permission. As the party approaches the Mayan city from which the raiders come, they encounter razed forests and failed maize crops, along with villages decimated by an unknown disease. A little girl infected with the illness prophesies Zero Wolf's death and the end of the Mayan world. Once the raiders and captives reach the city the females are sold as slaves while the males are escorted to the top of a step pyramid to be sacrificed before the Mayan King and Queen.\\r\\n\\r\\nAs a result of a solar eclipse and the superstitions surrounding it, Jaguar Paw and the remaining captives are spared from being sacrificed. Instead, they are ordered to be taken away and \\"disposed of\\". They are offered freedom if they can avoid being killed during target practice by Zero Wolf and his men. After some tribesmen are killed, Jaguar Paw is severely injured but kills Zero Wolf's son Cut Rock and escapes into the jungle. Zero Wolf sets off with eight comrades to chase down and kill Jaguar Paw. Back in his native jungle, Jaguar Paw now has the advantage, although he is badly injured. Most of his pursuersincluding Zero Wolf and Middle Eyeare gradually killed off via clever conceits and traps laid out by Jaguar Paw until there are only two left to hunt him.\\r\\n\\r\\nThe drought breaks and heavy rain begins to fall, threatening to drown Jaguar Paw's family, who are still trapped in the pit cave despite their attempts to escape. Seven gives birth to another son, who is born into the now dangerously rising water. Meanwhile, the two remaining raiders chase Jaguar Paw out of the undergrowth towards the coast. As they reach the beach, all three are stopped in their tracks by the sight of conquistador ships anchored off the coast and Europeans making their way ashore holding up a large cross. Jaguar Paw escapes while the two raiders remain, seemingly mesmerized at the conquistadors' presence. Jaguar Paw returns in time to save his family from the flooded pit. He is overjoyed at the sight of his new baby son.\\r\\n\\r\\nSometime later, as the reunited family look out over the water at the Spanish ships, Jaguar Paw decides not to risk approaching the conquistadors, insisting his family head back into the jungle. They depart in search of a new home and a new beginning.\\r\\n\\r\\nScreenwriter and co-producer Farhad Safinia first met Mel Gibson while working as an assistant during the post-production of The Passion of the Christ. Eventually, Gibson and Safinia found time to discuss \\"their mutual love of movies and what excites them about moviemaking\\".[3]\\r\\n\\r\\nGibson said they wanted to \\"shake up the stale action-adventure genre\\", which he felt was dominated by CGI, stock stories and shallow characters and to create a footchase that would \\"feel like a car chase that just keeps turning the screws.\\"[4]\\r\\n\\r\\nGibson and Safinia were also interested in portraying and exploring an ancient culture as it existed before the arrival of the Europeans. Considering both the Aztecs and the Maya, they eventually chose the Maya for their high sophistication and their eventual decline.\\r\\n\\r\\nThe two researched ancient Maya history, reading both creation and destruction myths, including sacred texts such as the Popul Vuh.[5] In the audio commentary of the film's first DVD release, Safinia states that the old shaman's story (played by Espiridion Acosta Cache, a modern-day Maya storyteller[6]) was modified from an authentic Mesoamerican tale that was re-translated by Hilario Chi Canul, a professor of Maya, into the Yucatec Maya language for the film. He also served as a dialogue coach during production. As they researched the script, Safinia and Gibson traveled to Guatemala, Costa Rica and the Yucatn peninsula to scout filming locations and visit Maya ruins.\\r\\n\\r\\nStriving for a degree of historical accuracy, the filmmakers employed a consultant, Richard D. Hansen, a specialist in the Maya and assistant professor of archaeology at Idaho State University. As director of the Mirador Basin Project, he works to preserve a large swath of the Guatemalan rain forest and its Maya ruins. Gibson has said of Hansen's involvement: \\"Richard's enthusiasm for what he does is infectious. He was able to reassure us and make us feel secure that what we were writing had some authenticity as well as imagination.\\"[5]\\r\\n\\r\\nOther scholars of Mesoamerican history criticized the film for what they said were numerous inaccuracies.[7][8] See further coverage on the film's questionable historical accuracy and representation of the Maya below under \\"Historical accuracy\\". Numerous scholars have also risen to defend the film citing ignorance or political correctness as the primary motivator for the criticisms. A recent essay by Hansen on the film and a critical commentary on the criticisms of the film is now published.[9]\\r\\n\\r\\nGibson decided that all the dialogue would be in the Yucatec Maya language.[10] Gibson explains: \\"I think hearing a different language allows the audience to completely suspend their own reality and get drawn into the world of the film. And more importantly, this also puts the emphasis on the cinematic visuals, which are a kind of universal language of the heart.\\"[5]\\r\\n\\r\\nThe production team consisted of a large group of make-up artists and costume designers who worked to recreate the Maya look for the large cast. Led by Aldo Signoretti, the make-up artists daily applied the required tattoos, scarification, and earlobe extensions to all of the on-screen actors. According to advisor Richard D. Hansen, the choices in body make-up were based on both artistic license and fact: \\"I spent hours and hours going through the pottery and the images looking for tattoos. The scarification and tattooing was all researched, the inlaid jade teeth are in there, the ear spools are in there. There is a little doohickey that comes down from the ear through the nose into the septum ÿ that was entirely their artistic innovation.\\"[11] An example of attention to detail is the left arm tattoo of Seven, Jaguar Paw's wife, which is a horizontal band with two dots above ÿ the Mayan symbol for the number seven.\\r\\n\\r\\nSimon Atherton, an English armorer and weapon-maker who worked with Gibson on Braveheart, was hired to research and provide reconstructions of Maya weapons. Atherton also has a cameo as the cross-bearing Franciscan friar who appears on a Spanish ship at the end of the film.\\r\\n\\r\\nMel Gibson wanted Apocalypto to feature sets with buildings rather than relying on computer-generated images. Most of the step pyramids seen at the Maya city were models designed by Thomas E. Sanders. Sanders explained his approach: \\"We wanted to set up the Mayan world, but we were not trying to do a documentary. Visually, we wanted to go for what would have the most impact. Just as on Braveheart, you are treading the line of history and cinematography. Our job is to do a beautiful movie.\\"[12]\\r\\n\\r\\nHowever, while many of the architectural details of Maya cities are correct,[8] they are blended from different locations and eras,[8] a decision Farhad Safinia said was made for aesthetic reasons.[13] While Apocalypto is set during the terminal post-classic period of Maya civilization, the central pyramid of the film comes from the classic period, which ended in A.D. 900,[13] such as those found in the Postclassic sites of Muyil, Coba, and others in Quintana Roo, Mexico, where later cities are built around earlier pyramids.   The temples are in the shape of those of Tikal in the central lowlands classic style but decorated with the Puuc style elements of the northwest Yucatn centuries later. Richard D. Hansen comments, \\"There was nothing in the post-classic period that would match the size and majesty of that pyramid in the film. But Gibson ... was trying to depict opulence, wealth, consumption of resources.\\"[13] The mural in the arched walkway combined elements from the Maya codices, the Bonampak murals (over 700 years earlier than the film's setting), and the San Bartolo murals (some 1500 years earlier than the film's setting).[citation needed]\\r\\n\\r\\nGibson filmed Apocalypto mainly in Catemaco, San Andrs Tuxtla and Paso de Ovejas in the Mexican state of Veracruz. The waterfall scene was filmed at Eyipantla Falls, located in San Andrs Tuxtla. Other filming by second-unit crews took place in El Petn, Guatemala. The film was originally slated for an August 4, 2006, release, but Touchstone Pictures delayed the release date to December 8, 2006, due to heavy rains and two hurricanes interfering with filming in Mexico. Principal photography ended in July 2006.\\r\\n\\r\\nApocalypto was shot on high-definition digital video, using the Panavision Genesis camera.[14] During filming, Gibson and cinematographer Dean Semler employed Spydercam,[15] a suspended camera system allowing shooting from above. This equipment was used in a scene in which Jaguar Paw leaps off a waterfall.\\r\\n\\r\\nA number of animals are featured in Apocalypto, including a Baird's tapir and a black jaguar. Animatronics or puppets were employed for the scenes injurious to animals.[16]\\r\\n\\r\\nThe soundtrack to Apocalypto was composed by James Horner in his third collaboration with director Mel Gibson. The soundtrack lacks a traditional orchestral score and instead features a large array of exotic instruments and vocals by Pakistani singer Rahat Fateh Ali Khan.\\r\\n\\r\\nWhile Mel Gibson financed the film through his Icon Productions, Disney signed on to distribute Apocalypto for a fee in certain markets under the Touchstone Pictures label. The publicity for the film started with a December 2005 teaser trailer that was filmed before the start of principal photography and before Rudy Youngblood was cast as Jaguar Paw. As a joke, Gibson inserted a subliminal cameo of the bearded director in a plaid shirt with a cigarette hanging from his mouth posing next to a group of dust-covered Maya.[17] A clean-shaven Gibson also filmed a Mayan-language segment for the introduction of the 2006 Academy Awards in which he declined to host the ceremony.\\r\\nOn September 23, 2006, Gibson pre-screened the unfinished film to two predominantly Native American audiences in the US state of Oklahoma, at the Riverwind Casino in Goldsby, owned by the Chickasaw Nation, and at Cameron University in Lawton.[18] He also did a pre-screening in Austin, Texas, on September 24 in conjunction with one of the film's stars, Rudy Youngblood.[19] In Los Angeles, Gibson screened Apocalypto and participated in a Q&A session for Latin Business Association[20] and for members of the Maya community.[21] Due to an enthusiastic response from exhibitors, Disney opened the film on more than 2,500 screens in the United States.[citation needed]\\r\\n\\r\\nAccording to Mel Gibson, the Mayan setting of Apocalypto is \\"merely the backdrop\\" for a more universal story of exploring \\"civilizations and what undermines them\\".[22] The background to the events depicted is the terminal Postclassic period, immediately prior to the arrival of the Spanish, ca. 1511,  which the filmmakers researched before writing. According to archaeologist Michael D. Coe: \\r\\n\\r\\n Coe lists \\"environmental collapse\\" as one of the leading causes of the fall of the great empire, alongside \\"endemic warfare\\", \\"overpopulation\\", and \\"drought\\". \\"There is mounting evidence for massive deforestation and erosion throughout the Central Area. The Maya apocalypse, for such it was, surely had ecological roots,\\" explains Coe.[24]  The plight of the Postclassic Maya is thought to have replicated much of the same situations as existed centuries earlier during the collapse of the Classic period Maya.\\r\\n\\r\\nThe corrosive forces of corruption are illustrated in specific scenes throughout the film. Excessive consumption can be seen in the extravagant lifestyle of the upper-class Maya, their vast wealth contrasted with the sickly, the extremely poor, and the enslaved. Environmental degradation is portrayed both in the exploitation of natural resources, such as the over-mining and farming of the land, but also through the treatment of people, families and entire tribes as resources to be harvested and sold into slavery. Political corruption is seen in the leaders' manipulation, the human sacrifice on a large scale, and the slave trade. The film shows slaves being forced to create the lime stucco cement that covered the temples, an act that some historians consider a major factor in the Maya decline. One calculation estimates that it would take five tons of jungle forestry to make one ton of quicklime. Historical consultant Richard D. Hansen explains, \\"I found one pyramid in El Mirador that would have required nearly 650 hectares (1,600 acres) of every single available tree just to cover one building with lime stucco... Epic construction was happening... creating devastation on a huge scale.\\"[25]\\r\\n\\r\\nThe filmmakers intended this depiction of the Maya collapse to have relevance for contemporary society. The problems \\"faced by the Maya are extraordinarily similar to those faced today by our own civilization,\\" co-writer Safinia stated during production, \\"especially when it comes to widespread environmental degradation, excessive consumption and political corruption\\".[5] Gibson has stated that the film is an attempt at illustrating the parallels between a great fallen empire of the past and the great empires of today, saying \\"People think that modern man is so enlightened, but we're susceptible to the same forces ÿ and we are also capable of the same heroism and transcendence.\\"[5][26] The film serves as a cultural critique ÿ in Hansen's words, a \\"social statement\\" ÿ sending the message that it is never a mistake to question our own assumptions about morality.[27] The main purpose of the movie has a lot to do with a quote from Will Durant at the very beginning of the movie \\"A great civilization is not conquered from without until it has destroyed itself from within\\", the fighting between tribes, and  the arrival of the Conquistadors aboard the ships at the end of the movie.\\r\\n\\r\\nHowever, Gibson has also stated that he wanted the film to be hopeful rather than entirely negative. Gibson has defined the title as \\"a new beginning or an unveiling ÿ a revelation\\"; he says \\"Everything has a beginning and an end, and all civilizations have operated like that\\".[28] The Greek word (?ϫ?, apokalupt) is in fact a verb meaning \\"I uncover\\", \\"disclose\\", or \\"reveal\\".[29] Gibson has also said a theme of the film is the exploration of primal fears.[28]\\r\\n\\r\\nApocalypto received generally positive reviews from critics. On Rotten Tomatoes the film has an approval rating of 65%, based on 196 reviews, with an average rating of 6.3/10. The site's critical consensus reads, \\"Apocalypto is a brilliantly filmed, if mercilessly bloody, examination of a once great civilization.\\"[30] On Metacritic the film has a score of 68 out of 100, based on 37 critics, indicating \\"generally favorable reviews\\".\\r\\n\\r\\nRichard Roeper and guest critic Aisha Tyler on the television show Ebert & Roeper gave it \\"two thumbs up\\" rating.[31] Michael Medved gave Apocalypto four stars (out of four) calling the film \\"an adrenaline-drenched chase movie\\" and \\"a visceral visual experience.\\"[32]\\r\\n\\r\\nThe film was released less than six months after Gibson's 2006 DUI incident, where the director made antisemitic comments to police after being stopped on suspicion of drunk driving;[33] the incident garnered Gibson much negative publicity and magnified concerns some had over alleged antisemitism in his previous film, The Passion of the Christ.[34]  Several key film critics alluded to the incident in their reviews of Apocalypto: In his positive review, The New York Times A. O. Scott commented: \\"say what you will about him ÿ about his problem with booze or his problem with Jews ÿ he is a serious filmmaker.\\"[35]  The Boston Globe review came to a similar conclusion, noting that \\"Gibson may be a lunatic, but he's our lunatic, and while I wouldn't wish him behind the wheel of a car after happy hour or at a B'nai Brith function anytime, behind a camera is another matter.\\"[36]  In a negative review, Salon.com noted \\"People are curious about this movie because of what might be called extra-textual reasons, because its director is an erratic and charismatic Hollywood figure who would have totally marginalized himself by now if he didn't possess a crude gift for crafting violent pop entertainment.\\"[37]\\r\\n\\r\\nApocalypto gained some passionate champions in the Hollywood community. Actor Robert Duvall called it \\"maybe the best movie I've seen in 25 years\\".[38][39] Director Quentin Tarantino said, \\"I think it's a masterpiece. It was perhaps the best film of that year. I think it was the best artistic film of that year.\\"[40] Martin Scorsese, writing about the film, called it \\"a vision,\\" adding, \\"Many pictures today don't go into troubling areas like this, the importance of violence in the perpetuation of what's known as civilization. I admire Apocalypto for its frankness, but also for the power and artistry of the filmmaking.\\"[41] Actor Edward James Olmos said, \\"I was totally caught off guard. It's arguably the best movie I've seen in years. I was blown away.\\"[20] In 2013, director Spike Lee put the film on his list of all-time essential films.[42]\\r\\n\\r\\nOn release in Mexico the film registered a wider number of viewers than Perfume and Rocky Balboa. It even displaced memorable Mexican premieres such as Titanic and Poseidon.[43] According to polls performed by the newspaper Reforma, 80% of polled Mexicans labeled the film as \\"very good\\" or \\"good\\".[43]\\r\\n\\r\\nFor his role as producer and director of the film, Mel Gibson was given the Trustee Award by the First Americans in the Arts organization. Gibson was also awarded the Latino Business Association's Chairman's Visionary Award for his work on Apocalypto on November 2, 2006, at the Beverly Hilton Hotel in Los Angeles. At the ceremony, Gibson said that the film was a \\"badge of honor for the Latino community.\\"[44] Gibson also stated that Apocalypto would help dismiss the notion that \\"history only began with Europeans\\".[45]\\r\\n\\r\\nMany writers felt that Gibson's film was relatively accurate about the Maya, since it depicts the era of decline and division that followed the civilization's peak, collapse, re-settlement, and proto-historic societal conditions. One Mexican reporter, Juan E. Pardinas, wrote that \\"this historical interpretation bears some resemblances with reality [...]. Mel Gibson's characters are more similar to the Mayas of the Bonampak's murals than the ones that appear in the Mexican school textbooks.\\"[47] \\"The first researchers tried to make a distinction between the 'peaceful' Maya and the 'brutal' cultures of central Mexico\\", David Stuart wrote in a 2003 article. \\"They even tried to say human sacrifice was rare among the Maya.\\" But in carvings and mural paintings, Stuart said: \\"we have now found more and greater similarities between the Aztecs and Mayas.\\"[48]\\r\\n\\r\\nRichard D. Hansen, who was a historical consultant on the film, stated that the impact the film will have on Maya archaeology will be beneficial:\\r\\n\\"It is a wonderful opportunity to focus world attention on the ancient Maya and to realize the role they played in world history.\\"[11] However, in an interview with The Washington Post, Hansen stated the film \\"give[s] the feeling they're a sadistic lot\\", and said, \\"I'm a little apprehensive about how the contemporary Maya will take it.\\"[49]\\r\\n\\r\\nSome observers were more cautious. William Booth of The Washington Post wrote that the film depicts the Maya as a \\"super-cruel, psycho-sadistic society on the skids, a ghoulscape engaged in widespread slavery, reckless sewage treatment and bad rave dancing, with a real lust for human blood.\\"[49] Gibson compared the savagery in the film to the Bush administration, telling British film magazine Hotdog, \\"The fear-mongering we depict in the film reminds me of President Bush and his guys.\\"[50] Just prior to its release, Apocalypto was criticized by activists in Guatemala, including Lucio Yaxon, who charged that the trailer depicts Maya as savages.[51] In her review of the film, anthropologist Traci Ardren wrote that Apocalypto was biased because \\"no mention is made of the achievements in science and art, the profound spirituality and connection to agricultural cycles, or the engineering feats of Maya cities\\".[52] Apocalypto also sparked a strong condemnation from art history professor Julia Guernsey, a Mesoamerican specialist, who said, \\"I think it's despicable. It's offensive to Maya people. It's offensive to those of us who try to teach cultural sensitivity and alternative world views that might not match our own 21st century Western ones but are nonetheless valid.\\"[53]\\r\\n\\r\\nApocalypto has been criticized for portraying a type of human sacrifice which was more typical of the Aztecs than of the Maya. Archaeologist Lisa Lucero said, \\"the classic Maya really didn't go in for mass sacrifice. That was the Aztecs.\\"[13] Anthropology professor Karl Taube argued that, \\"We know the Aztecs did that level of killing. Their accounts speak of 20,000.\\"[54] According to the film's technical advisor, the film was meant to describe the post-classic period of the Maya when fiercer influences like the Toltecs and Aztecs arrived. According to Hansen, \\"We know warfare was going on. The Postclassic center of Tulum is a walled city; these sites had to be in defensive positions. There was tremendous Aztec influence by this time. The Aztecs were clearly ruthless in their conquest and pursuit of sacrificial victims, a practice that spilled over into some of the Maya areas.\\"[11] Anthropology professor Stephen Houston made the criticism that sacrifice victims were more likely to be royalty and elites rather than common forest dwellers, as shown in Apocalypto.[54] Anthropology professor Karl Taube criticized the film's apparent depiction of widespread slavery, saying, \\"We have no evidence of large numbers of slaves.\\"[54] Another disputed scene, when Jaguar Paw and the rest of the captives are used as target practice, was acknowledged by the filmmakers to be invented as a plot device for igniting the chase sequence.[13] Some anthropologists objected to the presence of a huge pit filled with rotting corpses near the fields of the Maya.[55] Hansen states that this is \\"conjecture\\", saying that \\"all [Gibson was] trying to do there is express the horror of it\\".[13]\\r\\n\\r\\nThe Washington Post reported that the famous Bonampak murals were digitally altered to show a warrior holding a dripping human heart, which is not present in the original.[56]\\r\\n\\r\\nThe scene where the main character is rescued from sacrifice by a total solar eclipse was seen by some as being reminiscent of a similar incident in the Tintin comic Prisoners of the Sun. In the comic, Tintin and his companions are captured by a tribe of Inca, but are spared by a solar eclipse. However, Mel Gibson and co-writer Farhad Safinia categorically denied the rumor in the DVD commentary, where Gibson pointed to the Mayan's heavy emphasis on solar eclipses and other astronomical events.\\r\\n\\r\\nThe partial eclipse that precedes the total eclipse progresses with unnatural rapidity in the film: a matter of seconds rather than several minutes or more, as would be required in nature.\\r\\n\\r\\nAccording to the DVD commentary track by Mel Gibson and Farhad Safinia, the ending of the film was meant to depict the first contact between the Spaniards and Mayas that took place in 1502 during the fourth voyage of Christopher Columbus.[57]\\r\\n\\r\\nThe thematic meaning of the arrival of the Europeans is a subject of disagreement. Traci Ardren wrote that the Spanish arrivals were Christian missionaries and that the film had a \\"blatantly colonial message that the Mayas needed saving because they were 'rotten at the core'\\". According to Ardren, the Gibson film \\"replays, in glorious big-budget technicolor, an offensive and racist notion that Maya people were brutal to one another long before the arrival of Europeans and thus they deserved, in fact they needed, rescue. This same idea was used for 500 years to justify the subjugation of Maya people\\".[52] On the other hand, David van Biema questions whether the Spaniards are portrayed as saviours of the Mayas, since they are depicted ominously and Jaguar Paw decides to return to the woods.[58] This view is supported by the reference of the Oracle Girl to those who would \\"Scratch out the earth. Scratch you out. And end your world.\\" However, recalling the opening quote to the film (\\"A great civilization is not conquered from without until it has destroyed itself from within\\"), professors David Stuart and Stephen Houston have written the implication is that Postclassic Mayans had become so corrupt that they were \\"a civilization... that deserves to die.\\"[49]","input":"What language is spoken in the movie apocalypto?"},{"output":"United States","context":"","input":"What two countries were involved in the battle of midway?"},{"output":"C. Rajagopalachari","context":"\\r\\n\\r\\nThe Governor-General of India (or, from 1858 to 1947, officially the Viceroy and Governor-General of India, commonly shortened to Viceroy of India) was originally the head of the British administration in India and, later, after Indian independence in 1947, the representative of the Indian head of state. The office was created in 1773, with the title of Governor-General of the Presidency of Fort William. The officer had direct control only over Fort William, but supervised other British East India Company officials in India. Complete authority over all of British India was granted in 1833, and the official came to be known as the \\"Governor-General of India\\".\\r\\n\\r\\nIn 1858, the territories of the East India Company came under the direct control of the British government; see British Raj. The governor-general (now also the viceroy) headed the central government of India, which administered the provinces of British India, including the Punjab, Bengal, Bombay, Madras, the United Provinces, and others.[1] However, much of India was not ruled directly by the British government; outside the provinces of British India, there were hundreds of nominally sovereign princely states or \\"native states\\", whose relationship was not with the British government, but directly with the monarch. To reflect the governor-general's role as the representative of the monarch to the feudal rulers of the princely states, from 1858 the term Viceroy and Governor-General of India (known in short as the Viceroy of India) was applied to him.\\r\\n\\r\\nThe title of viceroy was abandoned when British India split into the two independent dominions of India and Pakistan, but the office of governor-general continued to exist in each country separatelyuntil they adopted republican constitutions in 1950 and 1956, respectively.\\r\\n\\r\\nUntil 1858, the governor-general was selected by the Court of Directors of the East India Company, to whom he was responsible. Thereafter, he was appointed by the sovereign on the advice of the British government; the Secretary of State for India, a member of the UK Cabinet, was responsible for instructing him on the exercise of his powers. After 1947, the sovereign continued to appoint the governor-general, but did so on the advice of the Indian government.\\r\\n\\r\\nGovernors-General served at the pleasure of the sovereign, though the practice was to have them serve five-year terms. Governors-General could have their commission rescinded; and if one was removed, or left, a provisional governor-general was sometimes appointed until a new holder of the office could be chosen. The first Governor-General of British India was Lord William bentinck, and the first Governor-General of independent India was Louis Mountbatten.\\r\\n\\r\\nMany parts of the Indian subcontinent were governed by the East India Company, which nominally acted as the agent of the Mughal Emperor. In 1773, motivated by corruption in the Company, the British government assumed partial control over the governance of India with the passage of the Regulating Act of 1773. A Governor-General and Supreme Council of Bengal were appointed to rule over the Presidency of Fort William in Bengal. The first Governor-General and Council were named in the Act.\\r\\n\\r\\nThe Charter Act 1833 replaced the Governor-General and Council of Fort William with the Governor-General and Council of India. The power to elect the Governor-General was retained by the Court of Directors, but the choice became subject to the Sovereign's approval.\\r\\n\\r\\nAfter the Indian Rebellion of 1857, the East India Company's territories in India were put under the direct control of the Sovereign. The Government of India Act 1858 vested the power to appoint the Governor-General in the Sovereign. The Governor-General, in turn, had the power to appoint all lieutenant governors in India, subject to the Sovereign's approval.\\r\\n\\r\\nIndia and Pakistan acquired independence in 1947, but Governors-General continued to be appointed over each nation until republican constitutions were written. Louis Mountbatten, 1st Earl Mountbatten of Burma remained Governor-General of India for some time after independence, but the two nations were otherwise headed by native Governors-General. India became a secular republic in 1950; Pakistan became an Islamic one in 1956...\\r\\n\\r\\nThe Governor-General originally had power only over the Presidency of Fort William in Bengal. The Regulating Act, however, granted them additional powers relating to foreign affairs and defence. The other Presidencies of the East India Company (Madras, Bombay and Bencoolen) were not allowed to declare war on or make peace with an Indian prince without receiving the prior approval of the Governor-General and Council of Fort William.[citation needed]\\r\\n\\r\\nThe powers of the Governor-General, in respect of foreign affairs, were increased by the India Act 1784. The Act provided that the other Governors under the East India Company could not declare war, make peace or conclude a treaty with an Indian prince unless expressly directed to do so by the Governor-General or by the Company's Court of Directors.\\r\\n\\r\\nWhile the Governor-General thus became the controller of foreign policy in India, he was not the explicit head of British India. That status came only with the Charter Act 1833, which granted him \\"superintendence, direction and control of the whole civil and military Government\\" of all of British India. The Act also granted legislative powers to the Governor-General and Council.\\r\\n\\r\\nAfter 1858, the Governor-General (now usually known as the Viceroy) functioned as the chief administrator of India and as the Sovereign's representative. India was divided into numerous provinces, each under the head of a governor, Lieutenant Governor or Chief Commissioner or Administrator. Governors were appointed by the British Government, to whom they were directly responsible; Lieutenant Governors, Chief Commissioners, and Administrators, however, were appointed by and were subordinate to the Viceroy. The Viceroy also oversaw the most powerful princely rulers: the Nizam of Hyderabad, the Maharaja of Mysore, the Maharaja (Scindia) of Gwalior, the Maharaja of Jammu and Kashmir and the Gaekwad (Gaekwar) Maharaja of Baroda. The remaining princely rulers were overseen either by the Rajputana Agency and Central India Agency, which were headed by representatives of the Viceroy, or by provincial authorities.\\r\\n\\r\\nThe Chamber of Princes was an institution established in 1920 by a Royal Proclamation of King-Emperor George V to provide a forum in which the princely rulers could voice their needs and aspirations to the government. The chamber usually met only once a year, with the Viceroy presiding, but it appointed a Standing Committee, which met more often.\\r\\n\\r\\nUpon independence in August 1947, the title of Viceroy was abolished. The representative of the British Sovereign became known once again as the Governor-General. C. Rajagopalachari became the only Indian Governor-General. However, once India acquired independence, the Governor-General's role became almost entirely ceremonial, with power being exercised on a day-to-day basis by the Indian cabinet. After the nation became a republic in 1950, the President of India continued to perform the same functions.\\r\\n\\r\\nThe Governor-General was always advised by a Council on the exercise of his legislative and executive powers. The Governor-General, while exercising many functions, was referred to as the \\"Governor-General in Council.\\"\\r\\n\\r\\nThe Regulating Act 1773 provided for the election of four counsellors by the East India Company's Court of Directors. The Governor-General had a vote along with the counsellors, but he also had an additional vote to break ties. The decision of the Council was binding on the Governor-General.\\r\\n\\r\\nIn 1784, the Council was reduced to three members; the Governor-General continued to have both an ordinary vote and a casting vote. In 1786, the power of the Governor-General was increased even further, as Council decisions ceased to be binding.\\r\\n\\r\\nThe Charter Act 1833 made further changes to the structure of the Council. The Act was the first law to distinguish between the executive and legislative responsibilities of the Governor-General. As provided under the Act, there were to be four members of the Council elected by the Court of Directors. The first three members were permitted to participate on all occasions, but the fourth member was only allowed to sit and vote when legislation was being debated.\\r\\n\\r\\nIn 1858, the Court of Directors ceased to have the power to elect members of the Council. Instead, the one member who had a vote only on legislative questions came to be appointed by the Sovereign, and the other three members by the Secretary of State for India.\\r\\n\\r\\nThe Indian Councils Act 1861 made several changes to the Council's composition. Three members were to be appointed by the Secretary of State for India, and two by the Sovereign. (The power to appoint all five members passed to the Crown in 1869). The Viceroy was empowered to appoint an additional six to twelve members (changed to ten to sixteen in 1892, and to sixty in 1909). The five individuals appointed by the Sovereign or the Indian Secretary headed the executive departments, while those appointed by the Viceroy debated and voted on legislation.\\r\\n\\r\\nIn 1919, an Indian legislature, consisting of a Council of State and a Legislative Assembly, took over the legislative functions of the Viceroy's Council. The Viceroy nonetheless retained significant power over legislation. He could authorise the expenditure of money without the Legislature's consent for \\"ecclesiastical, political [and] defense\\" purposes, and for any purpose during \\"emergencies.\\" He was permitted to veto, or even stop debate on, any bill. If he recommended the passage of a bill, but only one chamber cooperated, he could declare the bill passed over the objections of the other chamber. The Legislature had no authority over foreign affairs and defence. The President of the Council of State was appointed by the Viceroy; the Legislative Assembly elected its President, but the election required the Viceroy's approval.\\r\\n\\r\\nUntil 1833, the title of the position was \\"Governor-General of Bengal\\". The Government of India Act 1833 converted the title into \\"Governor-General of India.\\" The title \\"Viceroy and Governor-General\\" was first used in the queen's proclamation appointing Viscount Canning in 1858.[3] It was never conferred by an act of parliament, but was used in warrants of precedence and in the statutes of knightly orders. In usage, \\"viceroy\\" is employed where the governor-general's position as the monarch's representative is in view.[4] The viceregal title was not used when the sovereign was present in India. It was meant to indicate new responsibilities, especially ritualistic ones, but it conferred no new statutory authority. The governor-general regularly used the title in communications with the Imperial Legislative Council, but all legislation was made only in the name of the Governor-General-in-Council (or the Government of India).[5]\\r\\n\\r\\nThe Governor-General was styled Excellency and enjoyed precedence over all other government officials in India. He was referred to as 'His Excellency' and addressed as 'Your Excellency'. From 1858 to 1947, the Governor-General was known as the Viceroy of India (from the French roi, meaning 'king'), and wives of Viceroys were known as Vicereines (from the French reine, meaning 'queen'). The Vicereine was referred to as 'Her Excellency' and was also addressed as 'Your Excellency'. Neither title was employed while the Sovereign was in India. However, the only reigning British Sovereign to visit India during the period of British rule was King George V, who accompanied by his consort Queen Mary attended the Delhi Durbar in 1911.[citation needed]\\r\\n\\r\\nWhen the Order of the Star of India was founded in 1861, the Viceroy was made its Grand Master ex officio. The Viceroy was also made the ex officio Grand Master of the Order of the Indian Empire upon its foundation in 1877.\\r\\n\\r\\nMost Governors-General and Viceroys were peers. Frequently, a Viceroy who was already a peer would be granted a peerage of higher rank, as with the granting of a marquessate to Lord Reading and an earldom and later a marquessate to Freeman Freeman-Thomas. Of those Viceroys who were not peers, Sir John Shore was a baronet, and Lord William Bentinck was entitled to the courtesy title 'Lord' because he was the son of a Duke. Only the first and last Governors-General?ÿ Warren Hastings and Chakravarti Rajagopalachari?ÿ as well as some provisional Governors-General, had no honorific titles at all.\\r\\n\\r\\nFrom around 1885, the Viceroy of India was allowed to fly a Union Flag augmented in the centre with the 'Star of India' surmounted by a Crown. This flag was not the Viceroy's personal flag; it was also used by Governors, Lieutenant Governors, Chief Commissioners and other British officers in India. When at sea, only the Viceroy flew the flag from the mainmast, while other officials flew it from the foremast.\\r\\n\\r\\nFrom 1947 to 1950, the Governor-General of India used a dark blue flag bearing the royal crest (a lion standing on the Crown), beneath which was the word 'India' in gold majuscules. The same design is still used by many other Commonwealth Realm Governors-General. This last flag was the personal flag of the Governor-General only.\\r\\n\\r\\nThe Governor-General of Fort William resided in Belvedere House, Calcutta, until the early nineteenth century, when Government House was constructed. In 1854, the Lieutenant Governor of Bengal took up residence there. Now, the Belvedere Estate houses the National Library of India.\\r\\n\\r\\nLord Wellesley, who is reputed to have said that India should be governed from a palace, not from a country house, constructed a grand mansion, known as Government House, between 1799 and 1803. The mansion remained in use until the capital moved from Calcutta to Delhi in 1912. Thereafter, the Lieutenant Governor of Bengal, who had hitherto resided in Belvedere House, was upgraded to a full Governor and transferred to Government House. Now, it serves as the residence of the Governor of the Indian state of West Bengal, and is referred to by its Bengali name Raj Bhavan.\\r\\n\\r\\nAfter the capital moved from Calcutta to Delhi, the Viceroy occupied the newly built Viceroy's House, designed by Sir Edwin Lutyens. Though construction began in 1912, it did not conclude until 1929; the palace was not formally inaugurated until 1931. The final cost exceeded S877,000 (over S35,000,000 in modern terms)?ÿ more than twice the figure originally allocated. Today the residence, now known by the Hindi name of 'Rashtrapati Bhavan', is used by the President of India.\\r\\n\\r\\nThroughout the British administration, Governors-General retreated to the Viceregal Lodge (Rashtrapati Niwas) at Shimla each summer to escape the heat, and the government of India moved with them. The Viceregal Lodge now houses the Indian Institute of Advanced Study.\\r\\n\\r\\nBadge of the Governor-General (1885ÿ1947)\\r\\n\\r\\nStandard of the Governor-General (1885ÿ1947)\\r\\n\\r\\nStandard of the Governor-General (1947ÿ50)","input":"Who was first indian governor general of india?"},{"output":"The Byzantine-Arab Wars","context":"The Byzantine Empire (the Eastern Roman Empire  during the medieval period, after the fall of the Western Roman Empire) following the Sack of Constantinople (1204) slowly collapsed by Ottoman expansion. After crisis of the Gothic Wars it managed to re-establish itself in a golden age under the Justinian dynasty in the 6th century, and during the Early Middle Ages it continued to flourish even after the Muslim conquest of the Levant and the constant threat of Arab invasion.\\r\\n\\r\\nBut in the High Middle Ages, under pressure from the Seljuk Empire, it suffered serious setbacks and fell into decline. After the Battle of Manzikert (1071) it lost control of Anatolia, and while the Komnenos dynasty restored a degree of stability in the 12th century with help from the First Crusade, the empire was captured and partitioned by the Crusaders themselves in the Fourth Crusade in 1204.\\r\\n\\r\\nEven after Byzantine rule was restored in 1261, the empire was now a shadow of its former self, and after the end of the Crusades, the empire had little to set against the rise of the Ottoman Empire during the late medieval period, and was eventually conquered with the Fall of Constantinople in 1453.\\r\\n\\r\\nThe process by which the empire waned, and from when its decline can be traced, is a matter of scholarly debate. In some cases, the entire history of the Byzantine Empire has been portrayed as a protracted period of decline of the Roman Empire. This holds especially for Enlightenment era writers such as Edward Gibbon, author of The History of the Decline and Fall of the Roman Empire, published in six volumes between 1776 and 1789, whose view was coloured by pro-western or anti-clerical biases, and tended to see the whole ten-century empire as merely a sad codicil to the Roman Empire of Antiquity.\\r\\n\\r\\nLate-20th-century and early-21st-century historians have instead emphasized the empire's remarkable resiliency and adaptability to change.[1][2][3][4]\\r\\n\\r\\nThe Byzantine-Arab Wars and the Battle of Manzikert have traditionally been considered the most significant. However, recent books by Paul Magdalino and John Birkenmeier have re-evaluated the position of the empire in the 12th century, citing the collapse under the Angeloi (1185ÿ1204) as the most decisive turning point in the empire's fortunes. Although this view is not universally held, historians generally agree that after the Fourth Crusade in 1204, the empire was only a shadow of its former self. \\r\\nThe death of Michael VIII in 1282 marks the last period of Byzantine success on anything more than a minor scale. From this date onwards, the empire entered its final decline.\\r\\n\\r\\nThe history of the empire includes a number of periods of crisis, interspersed with periods of at least partial recovery:\\r\\n\\r\\nIn the 5th ÿ 7th century, the Eastern Roman or Byzantine Empire was a continuation of the Roman Empire. The loss of the Western territories in the 5th century led to the loss of some important cities such as Rome. The creation of the Germanic states of the Franks, Visigoths, Ostrogoths and later of the Lombards out of the rubble of the Western Roman Empire meant that in time they would seek to challenge the authority of the Eastern Roman Empire. General Flavius Belisarius under Justinian I in the early 6th century made a serious attempt to recover the western half; however his gains were short-lived and poorly planned out ÿ resources and troops that could have been used to defeat the Persians were diverted forcing the Byzantines into tribute and diplomacy to deal with this Eastern threat. The loss of the western territories led to the Patriarch of Rome achieving greater independence from Byzantium, which no longer provided adequate protection to the Pope. Consequently, the Holy See and Byzantium would have disagreements, culminating in the schism of 1054 and the disaster of the Fourth Crusade in the 13th century.\\r\\n\\r\\nIn the 7th ÿ 9th century, Islam gave the Arabs a newfound zeal and desire to conquer. They expanded to the territories in the Levant and Egypt. \\r\\nThe Arab invasions led to the loss of Egypt, Syria, Palestine and for a short period of time, Crete, Sicily, Cyprus and Asia Minor. Though Asia Minor was recaptured and substantial parts of Syria and Mesopotamia either taken back or subjugated, Egypt remained firmly in Arab hands as did the rest of Palestine. The loss of Egypt was a major blow to the Byzantines since the province of Aegyptus had provided much of the Empire's manufactured goods and natural resources, especially grain, ever since the times of Roman Antiquity. Conversely the Arab acquisition of Egypt gave the Ummayad and later Abbasid Caliphates huge resources, meaning that the Byzantines had to direct large amounts of resources to stave off constant Arab incursions into Asia Minor and Syria. When the Fatimid Caliphate broke away from the Abbasids the Byzantines were able to launch successful offensives into Syria and Palestine, due to this division amongst their enemies.\\r\\n\\r\\nAs far back as the invasion of Africa by Belisarius, foreign soldiers were used in war.[6] While foreign military invention was not an all together new occurrence,[7] the reliance on it, and its ability to damage political, social, and economic institutions were dramatically increased in the 11th, 13th, 14th, and 15th centuries. The 11th century saw increasing tensions between Courtly, and Military factions.[8][9] Until the mid 11th century the empire had long been under the control of the Military Factions with leaders such as Basil II, and John I Tzimiskes,[10] however the crisis of Basil II's succession led to increasing uncertainty in the future of politics.[8] The army demanded Basil's daughters remain in power, leading to a number of marriages, and increasing power for the Courtly faction.[8] This culminated after the failed Battle of Manzikert. As civil wars broke out, and tensions between courtly, and military factions reached a zenith, an increasing demand for soldiers led to the hiring of Turkish Mercenaries to fight internal civil war.[11] These mercenaries aided in the Byzantine loss of Anatolia by drawing more Turkish soldiers in to the interior of the empire, and by giving the Turks an increasing presence in Byzantine politics. These interventions also led to further destabilization of the political system.[11][12]\\r\\n\\r\\nReliance on foreign military intervention, and sponsorship for political motives, continued even during the Komnenoi Restoration, Alexius I utilized Turkish mercenaries in the civil wars he participated in with Nikephoros III Botaneiates.[12] In 1204, Alexios IV Angelos relied on Latin soldiers to claim the throne of Byzantium, leading to the sack of Byzantium, and the creation of the successor states.[11] After the resurgence of the Byzantine Empire with the ascension of Michael VIII Palaiologos reliance on foreign sponsorship increased still more. At this time it was common for emperors to seek sponsorship from Venice, Genoa, and the Turks. This led to a series of disastrous trade deals with the Italian states; drying up one of the empire's final sources of revenue.[13] This further led to competition between Venice, and Genoa to get emperors on the throne who supported their respective trade agenda to the detriment of the other, adding another level of instability to the Byzantine political process.[13]\\r\\n\\r\\nIn spite of its grievous losses of territory in Egypt and the Levant, the Byzantine empire successfully re-established itself against the threat of Islam  under the Macedonian dynasty in the 9th and 10th century. \\r\\nBut in the 11th century, a new threat arose as a consequence of the Turkic expansion out of Central Asia. \\r\\nThe Seljuks, a division of the Kankalis branch of Oghuz Turks, profiting from both the breakup of the Abbasid Caliphate and the absorption of the Byzantines with internal crisis and the loss of Italy to the Normans, managed to establish themselves in Asia Minor.\\r\\n\\r\\nFollowing the Battle of Manzikert, the Byzantine Empire lost most of its territory in Asia Minor, and was in immediate danger of complete annihilation. \\r\\nIt recovered following the Komnenian restoration, as Alexios I Komnenos restored relations with the Papacy, and \\r\\nrequested aid against the Turks from  Pope Urban II at the Council of Piacenza in 1095. Later that same year, the pope preached the First Crusade at the Council of Clermont.\\r\\n\\r\\nIn the wake of the success of the First Crusade, the three competent Komnenian emperors, especially Manuel I Komnenos (r. 1143ÿ1180), may have had the power to expel the outnumbered Seljuks, several factors combined to ensure that they never did so. Alexios was unable to derive much of the expected benefit from the  Crusade, though it did at least help him to recover Nicaea and western Asia Minor.  \\r\\nNo emperor after the Komnenian period was in a position to expel the Turks from Asia Minor, while the preoccupation of the Nicaean emperors with the attempt to recover Constantinople meant that resources were diverted away from Asia Minor and towards the west. \\r\\nThe result was a weakening of the Byzantine defenses in the region, which, when combined with insufficient resources and incompetent leadership, led to the complete loss of all the empire's Asian territory to the Turks by 1338.\\r\\n\\r\\nThough the Crusades assisted Byzantium in driving back some of the Turks, they went far beyond the military assistance envisaged by  Alexios I.\\r\\nInstead of following the strategic necessities of the war against the Turks, the Crusaders were focussed on the quest of re-conquering Jerusalem, \\r\\nand instead of returning territory to Byzantium, the Crusaders established their own principalities, becoming a territorial rival to Byzantine interests in their own right.\\r\\n\\r\\nThis was true already during  the Third Crusade, which induced emperor Isaac II Angelos to make a secret alliance with Saladin to impede the progress of Frederick Barbarossa, but open conflict between Crusaders and Byzantium erupted in the Fourth Crusade, resulting in the Sack of Constantinople in 1204. Constantinople was now itself a Crusader state, known as the Latin Empire in historiography, but from the Greek perspective as Frankokratia or \\"rule of the Franks\\". Vestiges of imperial power were preserved in minor principalities,   the Nicaean Empire, Trebizond and Epirus. Much of the Nicaean Emperors' efforts now went into combating the Latins, and even after Constantinople was returned to Greek rule  under the Palaiologoi in 1261, the Empire exerted much of its efforts into defeating its Latin neighbours, contributing to the eventual failure of the Crusades by 1291.\\r\\n\\r\\nThroughout the 14th and 15th centuries the Empire suffered from many natural disasters, invasions and several coups.\\r\\n\\r\\nA series of societal infighting also weakened the Byzantine Empire's military power. There were two major civil wars during the late Byzantine Empire, one in 1321 another in 1341. These civil wars also severely diminished the Byzantines' military capabilities.\\r\\n\\r\\nThe civil war of 1321ÿ1328 was led by a grandson of the Byzantine emperor Andronikos II who was supported by Byzantine Magnates who often clashed with the centralized authority of Byzantium. The Byzantine civil war of 1321ÿ1328 was inconclusive and ended with Andronikos III being made co-emperor with his grandfather. This civil war allowed the Turks to make notable gains in Anatolia and set up their capital in Bursa 100 kilometers from Constantinople the Byzantine's capital. After the initial conflict Andronikos III dethroned his grandfather and became emperor.[14]\\r\\n\\r\\nFollowing the death of Andronikos III in 1341, another civil war broke out which was to continue until 1347. When Andronikos III died he left his six-year-old son under the regency of Anne of Savoy. The de facto leader of the empire, John Cantacuzenus, who was not only a close associate of the deceased emperor but an extremely wealthy landowner, wanted to become regent.[15] However, things did not go his way and he was declared emperor in Thrace.[16] This conflict had elements of class warfare, in that the rich supported Cantacuzenus while the poorer folk supported the empress regent. The civil war of 1341ÿ1347 saw exploitation of the Byzantine Empire by the Serbs, whose ruler took advantage of the chaos to proclaim himself emperor of the Serbs and Greeks. The Serbian king Stefan Uro? IV Du?an made significant territorial gains in Byzantine Macedonia in 1345 and conquered large swathes of Thessaly and Epirus in 1348.[17] Although Dusan would die along with his dream of a Serbian Greek empire in 1355,[18] Byzantium would still face a powerful Turkish state across the Sea of Marmara. Luckily for Cantacuzenus, he conquered Constantinople in 1347 and ended the civil war afterwards.[19] In order to secure his authority during the civil war, Cantacuzenus hired Turkish mercenaries. Although these mercenaries were of some use, in 1352 they seized Gallipoli from the Byzantines.[18] Although in 1354 the rogue mercenaries were defeated by western crusaders,[20] Turkish armies would eventually control many of the Byzantine Empire's once-held territories. These two monumental civil wars severely diminished the Byzantine Empire's military strength and allowed its opportunistic enemies to make substantial gains into Byzantine territory.\\r\\n\\r\\nThe disintegration of the Seljuk Turks led to the rise of the Ottoman Turks. Their first important leader was Osman I Bey, who attracted Ghazi warriors and carved out a domain in north-western Asia Minor.[21] Attempts by the Byzantine Emperors to drive back the Ottomans were unsuccessful, and ceased in 1329 with the Battle of Pelekanon. Following a number of civil disputes in the Byzantine Empire, the Ottomans subjugated the Byzantines as vassals in the late 14th century and attempts to relieve this vassal status culminated in the Fall of Constantinople in 1453.\\r\\n\\r\\nThe Byzantine Empire's survival depended upon its administration and the logistics that enabled it to run the Empire. Though considered complex, its system was one more advanced than those practised by the Frankish Kingdoms in the West and one modelled by the Islamic Powers of the East. As the Empire evolved into an increasingly smaller and defensive state, the governing of the state changed as well. However, by the 14th century the burdens of running an Empire surrounded by many enemies became too much of a strain on Byzantium's increasingly smaller resources. By c. 1350's, the Byzantines lost Thrace to the Ottomans; thereafter Constantinople became the government's primary administrative region.\\r\\n\\r\\nThe Byzantine Empire experienced numerous civil wars. The defeats in the 7th and 12th centuries to the Arabs and Turks respectively speaking were in no small part assisted by numerous internal conflicts. The situation became worse later in the 14th and 15th centuries where Byzantine Emperors were forced to fight their own grandchildren/children, as in the cases of Andronikus II and Andronikus III.\\r\\n\\r\\nThe Military of the Byzantine Empire was often smaller than that of its opponents and thus relied more upon strategy rather than brute strength to achieve success.[22][23] This was in part achieved by the logistics of the Byzantine administration which allowed it to utilize their troops as efficiently as possible. Taxes on the peasantry were collected at times of need so as to raise the supplies needed at the time.[24] However, this bureaucratic system was exploited by the social elite [24] whose increasing power challenged that of the Emperor. Whilst the Theme system worked well to provide efficient military service, it led to the decentralization of power leading to disastrous civil conflicts in the 11th century.\\r\\n\\r\\nFurthermore, as the taxation system became ever more of a burden on the peasantry, the lower classes of the Empire began to resent the state. This contributed to the loss of Asia Minor in the 11th and 14th centuries owing to the arrival of the Turks.\\r\\n\\r\\nAnother major factor in the decline of the Byzantine empire may have been the disintegration of its traditional military system, the 'theme' system. Under this arrangement, the empire was divided into several regions which contributed locally raised troops to the imperial armies. The system provided an effective means of cheaply mobilizing large numbers of men, and the result was a comparatively large and powerful force ÿ the army of the theme of Thrakesion alone had provided about 9,600 men in the period 902ÿ936, for example. However, the demise of the system made the organization of the Byzantine armies less self-sufficient.\\r\\n\\r\\nThe Byzantine military did not immediately collapse following the disappearance of the theme system. In the 12th century, the Komnenian dynasty re-established an effective military force. Manuel I Komnenos, for example, was able to muster an army of over 40,000 men. This was sufficient to ensure the empire's continued status as a great power for the duration of the Komnenian period. However, the Komnenoi never provided for a future that saw their decisive leadership replaced by incompetence. After the deposition of Andronikos I Komnenos in 1185, the dynasty of the Angeloi oversaw a period of military decline. From 1185 onwards, Byzantine emperors found it increasingly difficult to muster and pay for sufficient military forces, while the failure of their efforts to sustain their empire exposed the limitations of the entire Byzantine military system, dependent as it was on competent personal direction from the emperor. The culmination of the empire's military disintegration under the Angeloi was reached on 13 April 1204, when the armies of the Fourth Crusade sacked Constantinople and dismantled the Byzantine Empire.\\r\\n\\r\\nDespite the restoration under the Palaiologoi, Byzantium was never again a great power on the scale of the past. By the 13th century, the imperial army numbered a mere 6,000 men, while the empire's territories had been reduced to little more than the lands immediately surrounding the Aegean sea.\\r\\n\\r\\nThus, it is possible to argue that the demise of the theme system was one of the most significant factors in the decline of the Byzantine empire. As one of the main institutional strengths of the Byzantine state, the theme system was never replaced by a viable long-term alternative. This left the empire lacking in underlying structural strengths. The result was an empire that depended more than ever before on the strengths of each individual emperor or dynasty. The collapse of imperial power and authority after 1185 revealed the inadequacy of this approach.","input":"What was one cause of the decline of the byzantine empire?"},{"output":"31 May 2009","context":"Eliza Gladys \\"Millvina\\" Dean (2 February 1912 ÿ 31 May 2009) was a British civil servant and cartographer. She was the last remaining survivor of the sinking of the RMS Titanic on 15 April 1912.[1] At 2 months old, she was also the youngest passenger aboard.[2]\\r\\n\\r\\n\\r\\nDean was born in Branscombe, England, to Bertram Frank Dean (1886ÿ1912) and Georgette Eva Light (1879ÿ1975). She had an older brother, Bertram Vere Dean, born 21 May 1910. She never married and had no children.[3] Her father died on the Titanic; her mother died on 16 September 1975, aged 96; and her brother died on 14 April 1992, age 81, the 80th anniversary of the iceberg collision.\\r\\nDean's parents decided to leave the United Kingdom and emigrate to Wichita, Kansas, where her father had relatives, and his cousin owned a tobacco shop that he was going to co-own.[4] They were not supposed to be aboard the Titanic, but due to a coal strike, they were transferred onto it and boarded it as third-class passengers at Southampton, England. Dean was barely two months old when she boarded the ship. Her father felt its collision with the iceberg on the night of 14 April 1912, and after investigating, returned to his cabin, telling his wife to dress the children and go up on deck. Dean, her mother, and her brother were placed in Lifeboat 10 and were among the first third-class passengers to escape.[i] Her father, however, did not survive, and his body, if recovered, was never identified.\\r\\nAt first, Dean's mother wanted to continue on to Kansas to fulfil her husband's wish of a new life in the United States. However, after losing him and being left with two small children for whom to care, they returned to Britain aboard the RMS?Adriatic. While aboard the ship, Dean attracted considerable attention. An article in the Daily Mirror dated 12 May 1912 described the ordeal:\\r\\nMillvina and Bertram were raised mostly on pension funds and educated in Southampton schools, including The Gregg School. It was not until she was eight years old, and her mother was planning to remarry, that Dean was told she had been a passenger on the Titanic.\\r\\nDean worked for the British government during the Second World War and later as a purchaser for a local engineering firm. Other careers she held were as a cartographer, a secretary, and an assistant to a tobacconist.\\r\\nIt was not until Dean was in her seventies that she became involved in Titanic-related events. Over the years, she participated in numerous conventions, exhibitions, documentaries, radio and television interviews, and personal correspondence. In 1998, she travelled to the United States to participate in a Titanic convention in Springfield, Massachusetts, and another in 1999 in Montreal. She had also been scheduled to appear at a commemoration of the 94th anniversary of the sinking in 2006, but a broken hip prevented her appearance.[6] She also appeared in the History special Titanic's Final Moments: Missing Pieces.[citation needed]\\r\\nIn October 2007, Dean became the last Titanic survivor following the death of Barbara West Dainton, who died at age 96 in England.[7]\\r\\nIn December 2007, Dean criticised the BBC and its television programme Doctor Who for including an episode with a starship \\"cruise liner\\" called the Titanic, that was similar in appearance to the historical liner. Speaking from her nursing home, she said: \\"The Titanic was a tragedy which tore so many families apart. I lost my father and he lies on that wreck. I think it is disrespectful to make entertainment of such a tragedy.\\"[8] A spokeswoman for the show said: \\"No offence was intended. 'Voyage of the Damned' is set on a spaceship called The Titanic and not a boat.\\"[8]\\r\\nIn April 2008, Dean had accepted an invitation to speak in Southampton at an event commemorating the 96th anniversary of the sinking, but ill health resulting from a respiratory infection forced her to cancel.[9]\\r\\nIn December 2008, at age 96, Dean was forced to sell several of her family's possessions to pay for her private medical care following a broken hip. These included a letter sent to her mother from the Titanic Relief Fund, and a suitcase given to her and her mother in New York following the sinking. Their sale raised approximately S32,000. In February 2009, she announced that she would be selling several more items to pay for her increasing medical costs which she said exceeded S3,000 a month.[10]\\r\\n97-year-old Dean died of pneumonia on the morning of 31 May 2009, 97 years and seven weeks after the Titanic sailed, at a care home in Ashurst, Hampshire.[2][11] She was cremated, and on 24 October 2009, her ashes were scattered from a launch at the docks in Southampton where the Titanic set sail.[12]\\r\\nIn response to the escalating cost of Dean's healthcare, The Millvina Fund was set up in April 2009 by the Belfast, British, and International Titanic Societies with the exclusive aim of taking care of her nursing home bills. It was given a boost by the Irish author and campaigning journalist, Don Mullan, at the opening of his worldwide Nokia photographic exhibition, A Thousand Reasons for Living, (featuring a portrait of Dean) in Dublin on 22 April 2009.[13] Mullan introduced an additional portrait of Dean's hands, as she signed a card for a Titanic autograph collector, which he produced as a limited edition of 100 copies. He made the edition available at ?500 each and then challenged the director and stars of the film Titanic (1997) - James Cameron, Leonardo DiCaprio, and Kate Winslet - singer Celine Dion, and the corporations Sony Music, 20th Century Fox, and Paramount Pictures to match him euro-for-euro to support her with her bills.[14][15] DiCaprio and Winslet led the way with a joint contribution of US$20,000. Cameron and Dion donated US$10,000 each.","input":"When did the last living survivor of the titanic die?"},{"output":"Mao Zedong","context":"","input":"What is the name of china's most famous communist leader?"},{"output":"The Bad Beginning","context":"A Series of Unfortunate Events is a series of thirteen children's novels by Lemony Snicket, the pen name of American author Daniel Handler. The books follow the turbulent lives of Violet, Klaus, and Sunny Baudelaire. After their parents' death in a fire, the children are placed in the custody of a murderous relative, Count Olaf, who attempts to steal their inheritance and, later, orchestrates numerous disasters with the help of his accomplices as the children attempt to flee. As the plot progresses, the Baudelaires gradually confront further mysteries surrounding their family and deep conspiracies involving a secret society known as V.F.D., with connections to both Olaf and their parents. The series is narrated by Snicket, who dedicates each of his works to his deceased love interest, Beatrice, and often attempts to dissuade the reader from reading the Baudelaires' story.\\r\\nCharacterized by Victorian Gothic tones and absurdist textuality,[5][6] the books are noted for their dark humor, sarcastic storytelling, and anachronistic elements, as well as frequent cultural and literary allusions.[3][7] They have been classified as postmodern and metafictional writing, with the plot evolution throughout the later novels being cited as an exploration of the psychological process of transition from the idyllic innocence of childhood to the moral complexity of maturity.[8][9][10] Likewise, the final installments of the series are also acknowledged for their escalatingly intricate ethical ambiguity toward philosophical ambivalence, as the nature of some of the Baudelaires' actions becomes increasingly harder to discern from those of their antagonist counterparts and more characters are revealed to be responsible for permanent wrongdoing, despite their identification with the self-proclaimed good side of the tale.[5][11][12]\\r\\nSince the release of the first novel, The Bad Beginning, in September 1999, the books have gained significant popularity, critical acclaim, and commercial success worldwide, spawning a film, a video game, assorted merchandise and a television series on Netflix. The main thirteen books in the series have collectively sold more than 60 million copies and have been translated into 41 languages.[13][14] Several companion books set in the same universe of the series have also been released, including Lemony Snicket: The Unauthorized Autobiography, The Beatrice Letters and the noir prequel tetralogy All the Wrong Questions, which chronicles Snicket's youth.[15]\\r\\n\\r\\n\\r\\nThe books seem to be set in an alternate, \\"timeless\\"[16] world with stylistic similarities to both the 19th century and the 1930s, though with contemporary, and seemingly anachronistic scientific knowledge. For instance, in The Hostile Hospital, the Baudelaire children send a message via Morse code on a telegraph, yet in the general store they are in, there is fiber-optic cable for sale.[17] An \\"advanced computer\\" appears in The Austere Academy; this computer's exact functions are never stated, as its only use in the book is to show a picture of Count Olaf.[18] In a companion book to the series, The Unauthorized Autobiography, the computer is said to be capable of advanced forgery. The setting of the world has been compared to Edward Scissorhands in that it is \\"suburban gothic\\".[16] Although the film version sets the Baudelaires' mansion in the city of Boston, Massachusetts, real places rarely appear in the books, although some are mentioned. For example, in The Ersatz Elevator, a book in Jerome and Esm Squalor's library was titled Trout, In France They're Out;[19] there are also references to the fictional nobility of North American regions, specifically the Duchess of Winnipeg and the King of Arizona, perhaps allusions to the setting of Kurt Vonnegut's novel Slapstick, which features similar North American fictional nobility. Interestingly, Vonnegut's novel focuses on artificial family as the cure for loneliness and strife, which seems to also be the aim of the \\"artificial family\\" of V.F.D.\\r\\nThe series follows the adventures of three siblings called the Baudelaire orphans. Snicket explains that very few positive things happen to the children. Violet Baudelaire, the eldest, is fourteen when the series begins and is an inventor. Klaus Baudelaire, the middle child, is twelve when the series begins; he loves books and is an extraordinary speed reader with a first-class eidetic memory. Sunny Baudelaire is a baby in the beginning of the series, and enjoys biting things with her abnormally large teeth; she develops a love for cooking later in the series.\\r\\nThe children are orphaned after their parents are killed in a fire at the family mansion. In The Bad Beginning, they are sent to live with a distant relative named Count Olaf after briefly living with Mr. Poe, a banker in charge of the orphans' affairs. The siblings discover that Count Olaf intends to get his hands on the enormous Baudelaire fortune, which Violet is to inherit when she reaches 18 years of age. In the first book, he attempts to marry Violet, pretending it is the storyline for his latest play, but the plan falls through when Violet uses her non-dominant hand to sign the marriage document.[20]\\r\\nIn the following six books, Olaf disguises himself, finds the children and, with help from his many accomplices, tries to steal their fortune, committing arson, murder and other crimes. In the eighth through twelfth books, the orphans adopt disguises while on the run from the police after Count Olaf frames them for one of his murders. The Baudelaires routinely try to get help from Mr. Poe, but he, like many of the adults in the series, is oblivious to the dangerous reality of the children's situation.\\r\\nAs the books continue, the children uncover more of the mystery surrounding their parents' deaths and find that their parents were in a secret organization, V.F.D., along with several other adults they meet. After the acronym first appears at the end of The Austere Academy, the siblings find several red herrings that share the initials. They then start to meet \\"volunteers\\" and gradually learn about the organization, although they discover several mysteries that are never explained. In The End, the children find a diary written by their parents that answers many of their questions but also raises many more. The children leave with another young orphan on a boat from a remote island at the end of the series, their fates left unknown.\\r\\nThe author of the series, Daniel Handler (who uses the pseudonym Lemony Snicket), has said in an interview with The A.V. Club that he decided to write a children's story when he was trying to find a publisher for his first novel, The Basic Eight. One of the publishers, HarperCollins, passed on The Basic Eight, but they were interested in him writing a story for children. Handler thought it was a terrible idea at first, but met with the publishers to discuss the book. They challenged him to write the book he wished he could have read when he was 10.[21] He retooled a manuscript he had for a mock-Gothic book for adults,[22] which became \\"the story of children growing through all these terrible things\\", a concept which the publishers liked, to Handler's surprise.[21]\\r\\nThe first book in the series was The Bad Beginning, released September 30, 1999. When asked in a Moment Magazine interview about the Baudelaire children and Handler's own Jewish heritage he replied, \\"Oh yeah! Yes. The Baudelaires are Jewish! I guess we would not know for sure but we would strongly suspect it, not only from their manner but from the occasional mention of a rabbi or bar mitzvah or synagogue. The careful reader will find quite a few rabbis.\\"[23]\\r\\nThis series is most commonly classified as children's fiction, but it has also been classified in more specific genres such as gothic fiction, or some variety thereof, whether it is mock-gothic,[22][24] a satire of gothic literature,[25] neo-Victorian[26] or \\"suburban gothic\\".[16]\\r\\nAnother genre that the series has been described as is absurdist fiction, because of its strange characters, improbable storylines, and black comedy.[4][27]\\r\\nThe plots of the first seven books follow the same basic pattern: the Baudelaires go to a new guardian in a new location, where Count Olaf appears and attempts to steal their fortune. The books following pick up where the previous book ended.[16] There are thirteen books in the series and each book has thirteen chapters. The last book in the series, The End, contains two stories: The End, which has 13 chapters, and a separate \\"book\\" that is titled Chapter Fourteen.\\r\\nThe location of each book's events is usually identified in the book's title; the first twelve book titles are alliterative. In most books, the children's skills are used to help them defeat Count Olaf's plots; for instance, Violet invents a lockpick in The Reptile Room. Occasionally, the children's roles switch or other characters use their skills to assist the Baudelaires (e.g. Quigley's cartography skills help Violet and Klaus in The Slippery Slope).\\r\\nLemony Snicket frequently explains words and phrases in incongruous detail. When describing a word the reader may not be aware of, he typically says \\"a word which here means?...,\\" sometimes with a humorous definition, or one that is relevant only to the events at hand (for example, he describes \\"adversity\\" as meaning \\"Count Olaf\\").[22]\\r\\nDespite the general absurdity of the books' storyline, Lemony Snicket continuously maintains that the story is true and that it is his \\"solemn duty\\" to record it. Snicket often goes off into humorous or satirical asides, discussing his opinions or personal life. The details of his supposed personal life are largely absurd, incomplete, and not explained in detail. For example, Snicket claims to have been chased by an angry mob for 16 miles. Some details of his life are explained somewhat in a supplement to the series, Lemony Snicket: The Unauthorized Autobiography.\\r\\nLemony Snicket's narration and commentary is characteristically cynical and despondent. In the blurb for each book, Snicket warns of the misery the reader may experience in reading about the Baudelaire orphans and suggests abandoning the books altogether. However, he also provides ample comic relief with wry, dark humor. In the excerpt for The Grim Grotto, he writes: \\"...?the horrors [the Baudelaire children] encounter are too numerous to list, and you wouldn't even want me to describe the worst of it, which includes mushrooms, a desperate search for something lost, a mechanical monster, a distressing message from a lost friend and tap-dancing.\\"[28] Snicket's narration has been described as \\"self-conscious\\" and \\"post-modern\\".[10]\\r\\nSnicket translates for the youngest Baudelaire orphan, Sunny, who in the early books almost solely uses words or phrases that make sense only to her siblings. As the series progresses, her speech often contains disguised meanings. Some words are spelled phonetically: 'surchmi' in The Slippery Slope and 'Kikbucit?' in The End; some are spelled backwards: 'edasurc' in The Carnivorous Carnival, and 'cigam' in The Miserable Mill. Some contain references to culture or people: for instance, when Sunny says \\"Busheney\\" (combining the last names of George W. Bush and Dick Cheney, presumably), it is followed by the definition of \\"you are a vile man who has no regard for anyone else\\". Some words Sunny uses are foreign, such as \\"Shalom\\", \\"Sayonara\\" or \\"Arrete\\". Some are more complex, such as when she says \\"Akrofil, meaning, 'they were not afraid of heights'\\", which phonetically translates to acrophile, meaning one who loves heights. She begins to use standard English words towards the end of the books; one of her longer sentences being \\"I'm not a baby\\" in The Slippery Slope.[29]\\r\\nWhen describing a character whom the Baudelaires have met before, Snicket often describes the character first and does not reveal the name of the character until they have been thoroughly described. Lemony Snicket starts each book with a \\"post-modern dissection of the reading experience\\"[10] before linking it back to how he presents the story of the Baudelaires and what their current situation is. Snicket often uses alliteration to name locations, as well as book titles, throughout the story. Many of the books start with a theme being introduced that is continually referenced throughout the booksuch as the repeated comparisons of the words \\"nervous\\" and \\"anxious\\" in The Ersatz Elevator, the consistent use of the phrase \\"where there's smoke, there's fire\\" in The Slippery Slope and the descriptions of the water cycle in The Grim Grotto.\\r\\nA theme that becomes more prevalent as the series continues is the simultaneous importance and worthlessness of secrets. In the final book, The End, the concept is especially important, as demonstrated by a several-page-long discussion of the phrase \\"in the dark.\\" The children hear of a massive schism within the organization of V.F.D., which was once noble but became filled with corruption and split into two sides, \\"volunteers\\" and \\"villains.\\" While many of the critical plot points are given answers, Snicket explains that no story can be fully devoid of questions as every story is intertwined with numerous others and every character's history is shared in a great web of mysteries and unfortunate events that make up the world's legacy, making it impossible for anyone to know all the answers to every question. The Baudelaire children and Count Olaf's story is said to be merely a fragment of a much bigger story between numerous characters with the central connection being the organization of V.F.D.\\r\\nSocial commentary is a major element in the books, which often comment on the seemingly inescapable follies of human nature. The books consistently present the Baudelaire children as free-thinking and independent, while the adults around them obey authority and succumb to mob psychology, peer pressure, ambition, and other social ills. A high account is given to learning: those who are \\"well read\\" are often sympathetic characters, while those who shun knowledge are villains.\\r\\nThe books have strong themes of moral relativism, as the Baudelaires become more confused during the course of the series about the difference between right and wrong, feeling they have done wicked things themselves and struggling with the question of whether the end justifies the means. In the final book, in an allusion to the Book of Genesis, a snake offers the children a life-giving apple (which the other characters in The End refuse to eat despite the fact that it is a cure for a fatal illness).[8]\\r\\nEvil characters are shown to have sympathetic characteristics and often have led difficult lives. Similarly, good characters' flaws become major problems. Almost every major character in the books has lived a life as difficult as that of the Baudelaires, especially the villains. The books highlight the inevitability of temptation and moral decision-making, regardless of external situation. This indicates that regardless of one's outside influences, one always has the final choice in whether they will be good or bad. Characters that make brave decisions to fight back and take charge are almost always \\"good,\\" and characters that just go along end up as \\"bad.\\" However, some characters suggest that people are neither good nor bad, but a mix of both.[30]\\r\\nThere is a full page picture at the end of each book, showing a hint or clue about the content of the next book. This may be showing a flyer or piece of paper drifting by, though sometimes by a significant object: a snake appears at the end of The Bad Beginning, referring to Montgomery's snake collection in the following book. The same picture is used at the start of the succeeding book. This practice continued at the end of The End which shows a boat sailing off into the sunset and at the start of Chapter Fourteen.[31] The picture at the end of Chapter Fourteen includes a shape of a question mark.[32]\\r\\nFollowing the picture is a letter to the editor, which explains to the editor how to get a manuscript of the next book. Snicket is writing from the location of the next book and usually reveals its title. Snicket notes that the editors will find various objects along with the manuscript, all of them having some impact in the story. Starting with the fourth book (which previews the fifth), each letter has a layout relating to the next book, such as torn edges, fancy stationery, sopping wet paper, or telegram format. The letters change dramatically starting with the letter at the end of The Hostile Hospitalfor this preview letter, the letter is ripped to shreds and only a few scraps remain. The remaining letters are difficult to read, and some do not reveal the title. The final letter appears at the end of The End and simply has \\"The end of THE END can be found at the end of THE END.\\"[33] There is no letter after Chapter Fourteen.\\r\\nEach book begins with a dedication to a woman named Beatrice, and references to her are made by Snicket throughout the series, describing her as the woman he still loves while emphasizing the fact that she apparently died long ago. At the end of the Chapter Fourteen epilogue, it is revealed that Beatrice was the Baudelaires' late mother, who married their father after an unknown event caused her to return Snicket's engagement ring, alongside a two-hundred page book explaining all the reasons she could not marry him.[34]\\r\\nTo see more examples of allusions to literature and the real world in A Series of Unfortunate Events, see the individual article for any book in the series.\\r\\nWhile the books are marketed primarily to children, they are written with adult readers in mind as well; the series features numerous references more likely to make sense to adults,[3] such as allusions to Monty Python (the Baudelaire children's uncle Monty has a large snake collection, including a python, and a reference to the Self Defense Against Fresh Fruit sketch).\\r\\nMany of the characters' names allude to other fictional works or real people with macabre connections; locations may also allude to fiction, or contain foreign or obscure words with negative connotations. Lake Lachrymose appears in The Wide Window; \\"lachrymose\\" means \\"tearful.\\" As the series progresses, more literature appears in the serieseither through quotes, explicit mentions or both. For instance, T. S. Eliot's The Waste Land is important to the plot of The Grim Grotto, the eleventh book. The Baudelaire orphans are named after Charles Baudelaire; Violet's name also comes from the T.S. Eliot's poem The Waste Land, specifically its verses concerning the \\"violet hour,\\"[7] and Sunny and Klaus take their first names from Claus and Sunny von Blow, while Mr. Poe is a reference to Edgar Allan Poe (his sons are named Edgar and Albert).[35][36] In the seventh installment, The Vile Village, Count Olaf's disguise, Detective Dupin, is an allusion to C. Auguste Dupin, a fictional detective created by Edgar Allan Poe.[37]\\r\\nIsadora and Duncan Quagmire are named after Isadora Duncan, a notorious dancer also remembered for her unusual death by strangulation when her scarf entangled around the wheels of the open car in which she was a passenger.[35] In the fourth book, The Miserable Mill, Dr. Georgina Orwell is a reference to British author George Orwell.[7] Orwell finished his famous book 1984 in 1948, and in the sixth book, The Ersatz Elevator, it is not clear if the skyscraper in which Esm and Jerome Squalor live has 48 or 84 stories. The Squalors' names reference Jerome David \\"J. D.\\" Salinger and his short story For Esm?ÿ with Love and Squalor, while in an auction on which the plot hinges, Lot 49 is skipped, i.e. not cried, an allusion to Thomas Pynchon's The Crying of Lot 49. Both Salinger and Pynchon were reputed at one time not to be actual persons. The ninth book in the series, The Carnivorous Carnival, takes place at Caligari Carnival; the carnival's name is a nod to the 1920 silent horror film The Cabinet of Dr. Caligari.[35] Subsequently, many of the inhabitants of the island in which the Baudelaires find themselves on in The End are named after characters from The Tempest, a play by William Shakespeare[3], while some are named after characters from Robinson Crusoe, Moby Dick and others after general nautical or island-based literature.\\r\\nThe name of Beatrice, Snicket's dedicatee, may be an allusion to the poem La Beatrice by Charles Baudelaire. The poem references an \\"actor without a job,\\" like the actor Count Olaf. The poem also begins with the line \\"In a burnt, ash-grey land without vegetation,\\" similar to the Baudelaire mansion burning down at the beginning of the series. The name Beatrice could also be an allusion to Italian poet Dante. Dante dedicated all of his works to \\"Beatrice,\\" with whom he was obsessed, and who was also dead, like Snicket's Beatrice.[35][38]\\r\\nThe series includes thirteen novels as follows:[39]\\r\\nThere are books that accompany the series, such as The Beatrice Letters,[40] Lemony Snicket: The Unauthorized Autobiography,[41] and The Puzzling Puzzles;[42] journals The Blank Book[43] and The Notorious Notations;[44] and short materials such as The Dismal Dinner and 13 Shocking Secrets You'll Wish You Never Knew About Lemony Snicket. The books were at one point published at the rate of three or four books per year.[16] The endpapers were \\"designed in a suitably Victorian style\\", with cloth binding on the spines matching the colors of the cover.\\r\\nA paperback release of the series, featuring restyled covers, new illustrations and a serial supplement entitled The Cornucopian Cavalcade happened with The Bad Beginning: or, Orphans!, The Reptile Room: or, Murder!, and The Wide Window: or, Disappearance!, but stopped after the third.[45]\\r\\nHumorous quotes from the series were used in a book published under the Snicket name, Horseradish: Bitter Truths You Can't Avoid.[46]\\r\\nLemony Snicket's All the Wrong Questions is a four-part young adult series focused on Snicket's childhood working for V.F.D. It is set in the same universe as A Series of Unfortunate Events and features several of the same characters and locations. The first book was titled Who Could That Be at This Hour?, and was released in October 2012. The second, When Did You See Her Last?, was released in October 2013, and the third, Shouldn't You Be in School?, was released in September 2014. The final book, Why Is This Night Different from All Other Nights? was released on September 29, 2015.[47]\\r\\nNetflix, in association with Paramount Television, announced in November 2014 its plans to adapt the books into an original TV series with 26 total episodes spanning 3 seasons, with 2 episodes dedicated to each book, with the exception of the 13th book, The End.[48] Author Daniel Handler will serve as a writer and executive producer.[49]\\r\\nOn September 4, 2015, it was announced that filmmaker Barry Sonnenfeld and True Blood showrunner Mark Hudis had agreed to helm the series. Hudis would serve as showrunner, Sonnenfeld as director, and both as executive producers.[50] Daniel Handler is penning the scripts.[51] On December 3, 2015, an open casting call was announced for the roles of Violet and Klaus Baudelaire, with the casting call confirming that the series would begin production in March 2016.[52]\\r\\nIn January 2016, Netflix announced that Hudis had left the project and they have not yet named a replacement showrunner. However, it was announced that Sonnenfeld and Handler were both still on board, and that Neil Patrick Harris had been cast as Count Olaf and Malina Weissman and Louis Hynes are cast as Violet and Klaus.[48][53][54]\\r\\nIn March 2016, K. Todd Freeman and Patrick Warburton were cast as Mr. Poe and Lemony Snicket respectively.[55][56] The first season, consisting of eight episodes that cover the first four books, was released worldwide on Netflix on January 13, 2017.[57] A Series of Unfortunate Events was renewed for a second season, which was released on March 30, 2018, and consisted of ten episodes that adapt books five through nine of the novel series.[58] The television series was also renewed for a third and final season, which will air in early 2019 and will also adapt the four remaining books(The Slippery Slope, The Grim Grotto,The Penultimate Peril, and The End. The last season will have 7 episodes, The End only being one episode instead of the standard two episodes. (It is assumed Handler will remain on as screenwriter of the new seasons.[59][60]\\r\\nLemony Snicket's A Series of Unfortunate Events is a film adaptation of the first three titles in the series, mixing the various events and characters into one story. It was released on December 17, 2004.[61] Directed by Brad Silberling, it stars Jim Carrey as Count Olaf, Meryl Streep as Aunt Josephine, Billy Connolly as Uncle Monty, Emily Browning as Violet, Liam Aiken as Klaus, Timothy Spall as Mr. Poe, and Jude Law as the voice of Lemony Snicket.[62] The film was successful, however it was also criticized because the tone was comical, when the books were solemn and serious with occasional wry humor.[63]\\r\\nConsidering the success of the movie, the director and some of the lead actors hinted that they were keen on making a sequel, but no script was written.\\r\\nWhen I took the decision to take the movie I said I'd obviously do it with the right to refusal, I'm not going to give in to anything. I asked the studio how they were going to deal with the sequel. But they didn't want to talk about it until the first film was out. It's amazing; a script has not yet been worked on for the sequel, which I find a bit baffling.\\r\\nBrowning has said that further films would have to be produced quickly, as the children do not age much throughout the book series.[65]\\r\\nIn 2008, Daniel Handler stated in a Bookslut Interview that another film was in the works, but had been delayed by corporate shake-ups at Paramount Pictures.[66] In June 2009, Silberling confirmed he still talked about the project with Handler, and suggested the sequel be a stop motion film because the lead actors have grown too old. \\"In an odd way, the best thing you could do is actually have Lemony Snicket say to the audience, 'Okay, we pawned the first film off as a mere dramatization with actors. Now I'm afraid I'm going to have to show you the real thing.'\\"[67]\\r\\nLemony Snicket's A Series of Unfortunate Events is a video game based on the books and film (more so the film, as the name and many plot elements seen in the movie but not the book are seen) that was released in 2004 by Adrenium Games and Activision for PlayStation 2, GameCube, Xbox, Game Boy Advance, and PC. The player plays as all three orphans at points in the game, and encounters characters such as Mr. Poe, Uncle Monty and Aunt Josephine, along with villains such as Count Olaf, the Hook-Handed Man, the White-Faced Women, and the Bald Man.[68] The game, like the movie, follows only the first three books in the series. Although never mentioned in the game there are some references to V.F.D. such as while in the first level a package is delivered from the \\"Very Fast Delivery Service.\\" The note attached to the package also reads at the end \\"P.S. The world is quiet here,\\" which is the motto of V.F.D. and the way to confirm the allegiance of a V.F.D. member.\\r\\nA board game based on the books was distributed by Mattel in 2004, prior to the movie. The Perilous Parlor Game is for 2ÿ4 players, ages 8 and up. One player assumes the role of Count Olaf, and the other players play the Baudelaire children. Count Olaf's objective in the game is to eliminate the guardian, while the children try to keep the guardian alive. The game employs Clever Cards, Tragedy Cards, Secret Passage Tiles, and Disguise Tiles in play.\\r\\nThe Catastrophic Card Game is the second game based on the books. In this card game, players are looking to complete sets of characters. There are 4 different sets: The Baudelaire Orphans, Count Olaf in Disguise, Olaf's Henchmen and the Orphans Confidants. Players take turns drawing a card from either the draw pile or the top card from the discard pile in hopes of completing their sets. For 2ÿ4 players, ages 14 and under.\\r\\nMost of the series of unabridged audio books are read by British actor Tim Curry, though Handler as Lemony Snicket reads books 3 to 5. Of narrating the audio books, Handler has said: \\"It was very, very hard. It was unbelievably arduous. It was the worst kind of arduous.\\"[69] As such, future narrating duties were handed back to Curry, of whom Handler states: \\"he does a splendid job\\".[69] The \\"Dear Reader\\" blurb is usually read by Handler (as Snicket) at the beginning, although it is missing in The Hostile Hospital. Handler usually reads the \\"To my Kind Editor\\" blurb about the next book at the end. Starting at The Carnivorous Carnival there is another actor who replaces Handler in reading the two blurbs, although they are skipped entirely in The Grim Grotto. All of the recordings include a loosely related song by The Gothic Archies, a novelty band of which Handler is a member, featuring lyrics by Handler's Magnetic Fields bandmate Stephin Merritt.[70]\\r\\nIn October 2006, The Tragic Treasury: Songs from A Series of Unfortunate Events by The Gothic Archies was released. The album is a collection of thirteen songs written and performed by Stephin Merritt (of The Magnetic Fields), each one originally appearing on one of the corresponding thirteen audiobooks of the series. Two bonus songs are included.[71]\\r\\nReviews for A Series of Unfortunate Events have generally been positive, with reviewers saying that the series is enjoyable for children and adults alike,[72] and that it brings fresh and adult themes to children's stories.[73] The Times Online refer to the books as \\"a literary phenomenon\\", and discuss how the plight of the Baudelaire orphans helps children cope with lossciting the rise in sales post September 11, 2001 as evidence.[74] Although the series has often been compared to Harry Potter due to the young heroes and the sales of the two series, reviewer Bruce Butt feels that the series' tone is closer to Roald Dahl and Philip Ardagh.[16] Handler acknowledges Edward Gorey and Roald Dahl as influences.[22] Mackey attributes the series' success to the \\"topsy-turvy moral universe\\".[75] Langbauer feels that the series \\"offers a critique of the pieties\\" of earlier generations and \\"imparting its own vision of ethics\\".[76]\\r\\nThe series has come under criticism from some school districts for its dark themes, citing objections to the suggested incest (referring to Olaf's attempt to marry his distant cousin Violet in The Bad Beginning, although his motivation was not sexual in nature, but rather an attempt to gain the Baudelaire fortune)[22] and the words \\"damn\\" and \\"hell\\" being said in The Reptile Room. Handler later commented that the word's use was \\"precipitated by a long discussion of how one should never say this word, since only a villain would do so vile a thing! This is exactly the lily-liveredness of children's books that I can't stand.\\"[77] Access to the books was similarly restricted at Katy ISD Elementary School in Katy, Texas.[78] The series has also been criticized for formulaic and repetitive storytelling.[79]\\r\\nA Series of Unfortunate Events has been printed in 41 different languages,[80] selling at least sixty-five million copies as of 2015.[13]\\r\\nIn addition to its strong reviews, The Bad Beginning won multiple literary awards, including the Colorado Children's Book Award, the Nevada Young Readers Award and the Nene Award.[81] It was also a finalist for the Book Sense Book of the Year.[82] Its sequels have continued this trend, garnering multiple awards and nominations. Among these are three IRA/CBC Children's Choice Awards, which it received for The Wide Window,[83] The Vile Village,[84] and The Hostile Hospital;[85] a best book prize at the Nickelodeon Kids' Choice Awards,[86] and a 2006 Quill Book Award,[87] both for The Penultimate Peril. While not technically awards, The Ersatz Elevator was named a Book Sense 76 Pick,[88] and The Grim Grotto is an Amazon.com Customers' Favorite.[89]","input":"What is the first book in the lemony snicket series?"},{"output":"29 September 1829","context":"The history of the Metropolitan Police Service is long and complex, with many different events taking place between its inception in 1829 to the present day.\\r\\n\\r\\n\\r\\nBefore the passing of the Metropolitan Police Act 1829, law enforcement among the general population in England was carried out by unpaid parish constables who were elected, and later appointed by the local justice of the peace. In certain circumstances, such as serious public disorder, the army would intervene to support the local authorities; yeomanry were extensively used for this purpose before police forces developed. Because this system of policing was largely unorganised and lacked a criminal investigation capability, the novelist Henry Fielding (who had been appointed a Magistrate in 1748) introduced the first detective force, known as the Bow Street Runners, in 1753. Fielding's house at 4 Bow Street had been established as a courtroom by the previous owner, in 1739.\\r\\nFielding's force was made up of eight constables who also investigated crimes handed over to them by the volunteer constables and watchmen. Runners were identified by carrying a tipstaff with the Royal Crown on it, which had a compartment inside to store official identification and documents. In 1805 the Bow Street Horse Patrol, the first form of uniformed policing seen in the capital, was established alongside the Runners, later amalgamating into the Metropolitan Police in 1837.[1] Unofficial \\"thief-takers\\" operated independently from the Bow Street Runners, being employed by fee-paying members of the public to catch criminals and present them before a magistrate.[2]\\r\\nBy 1798, the year the Marine Police Force was established, salaried constables were being paid by local magistrates. The Marine Police was initially made up of 220 Constables assisted by 1,000 registered dock workers, and was responsible for preventing the theft of cargo in and around the River Thames. The London Marine Police Force is widely regarded[3] as being the first modern police force in the world, in the sense that they were not government controlled and were responsible for the prevention of crime. As such it is the oldest police force in continuous operation. In its first year of operation 2,000 offenders were found guilty of theft from the docks. This success led to the enacting of the Marine Police Bill, which made it the first publicly funded preventive police force in the history of English policing. In 1839, the Marine Police amalgamated with the Metropolitan Police to form the Thames Division, being recently renamed to the Marine Policing Unit.[4]\\r\\nThe lack of organisation and efficiency of early law enforcement was often a source of public controversy. Because of this, a parliamentary committee was appointed to investigate the current system of policing. Upon Sir Robert Peel being appointed as Home Secretary in 1822, he established a second and more effective committee, and acted upon its findings. Robert Peel, believing that the way to standardise the police was to make it an official paid profession, to organise it in a civilian fashion, and to make it answerable to the public. After he presented his ideas to Parliament, they were approved and made official with the Metropolitan Police Act of 1829.[5]\\r\\nDuring the early 19th century, the Industrial Revolution witnessed London becoming larger geographically and more significant economically.[6] It became clear that the locally maintained system of volunteer constables and \\"watchmen\\" was ineffective, both in detecting and preventing crime. Due to this, Royal Assent was given to the Metropolitan Police Act on 19 June 1829,[7] placing the policing arrangements for the capital directly under the control of Sir Robert Peel.[7]\\r\\nDue to public fears concerning the deployment of the military in domestic matters, Robert Peel organised the force along civilian lines, rather than paramilitary. To appear neutral, the uniform was deliberately manufactured in blue, rather than red which was then a military colour, along with the officers being armed only with a wooden truncheon and a rattle to signal the need for assistance. Along with this, police ranks did not include military titles, with the exception of Sergeant.[8]\\r\\nThe force did not routinely carry firearms, although Sir Robert Peel authorised the Commissioner to purchase fifty flintlock pocket pistols for use in exceptional circumstances, such as those which involved the use of firearms. Later, the obsolete flintlocks were decommissioned from service, superseded by early revolvers. At the time, burglary (or \\"house breaking\\" as it was then called) was a common problem for police. \\"House breakers\\" were usually armed. It was then also legal (under the Bill of Rights 1689) for members of the public who were Protestants, as most were, to own and use firearms.[9] Following the deaths of officers by firearms on the outer districts of the metropolis, and public debate on arming the force, the Commissioner applied to Robert Peel for authorisation to supply officers on the outer districts with revolvers. The authorisation was issued on the condition that revolvers would only be issued if, in the opinion of the senior officer, the officer could be trusted to use it safely and with discretion. From then, officers could be armed. The practice lasted until 1936, although the vast majority of the system was phased out by the end of the 19th century.\\r\\nDuring the 1860s, the flintlock pistols that had been purchased in 1829 were decommissioned from service, being superseded by 622 BeaumontÿAdams revolvers firing the .450 cartridge which were loaned from the army stores at the Tower of London following the Clerkenwell bombing. In 1883, a ballot was carried out to gather information on officers' views on whether they wished to be armed, and 4,430 out of 6,325 officers serving on outer divisions requested to be issued with revolvers. The now obsolete Adams revolver was returned to stores for emergencies, and the Bulldog 'Metropolitan Police' revolver was issued to officers on the outer districts who felt the need to be armed. On the night of 18 February 1887 PC 52206 Henry Owen became the first officer to fire a revolver while on duty, doing so after he was unable to alert the owners of premises on fire. Following the Siege of Sidney Street, one thousand self-loading Webley & Scott pistols were purchased. In 1914 the Bulldogs were withdrawn from service and returned to stores. Lord Trenchard standardised the issue of pistols among divisions with the division size determining the number of firearms (with thirty-two rounds per pistol) issued: ten pistols with 320 rounds of ammunition were issued to each divisional station; six pistols with 192 rounds to each sub-divisional station; three pistols with 96 rounds to each section station. In 1936 the authorisation to carry revolvers on outer districts was revoked, and at the same time Canadian Ross rifles were purchased in the prelude to the Second World War. In 1952 following the Derek Bentley case, when an officer was shot dead, 15% of firearms in service with the Metropolitan Police were found to be defective, leading to Special Branch and Royalty Protection Officers being armed with an early version of the Beretta automatic pistol.\\r\\nThe original headquarters of the newly formed Metropolitan Police was near Government, at 4 Whitehall Place,[citation needed] with a back entrance on Great Scotland Yard. Scotland Yard soon became established as a name for the force itself.[10] Once formed, the force become the third official non-paramilitary city police force in the world, after the City of Glasgow Police and the Paris Police.\\r\\nThe original standard wage for a Constable was one guinea (S1.05) a week. Recruitment criteria required applicants to be under the age of 35, in good health, and to be at least 5?ft 7?in (1.70?m). Working shifts lasted 12 hours, 6 days a week, with Sunday as a rest day. Until 1897, Metropolitan Police officers did not receive a boot allowance.\\r\\nFrom the Metropolitan Police's foundation, the force had relied on the use of hand rattles for officers to signal the need for assistance. In 1884 the Home Secretary invited competition from many companies to invent a \\"police whistle\\" to replace the rattle. J.Hudson & Company of Birmingham were awarded the contract for 7,175 whistles at the price of 11d each. At the same time, a competition for the contract to supply the Metropolitan Police with new truncheons was under way. This contract was won by Ross & Company, who supplied the Metropolitan Police with Lignum vitae truncheons. In 1886, during a riot between warring working parties in Hyde Park, many truncheons were damaged or broken, samples were sent off to be tested by the Royal Army Clothing Department, at a cost of 16 shillings per day. In October 1886, 900 pounds worth of Lance and Cocuswood were purchased, to use in place of Lignum vitae that was deemed unsuitable after Army testing.\\r\\nSince the MPS's inception, the force has been headed by a Commissioner, rather than a Chief Constable which is the highest rank in police forces outside London. The first Commissioners to hold the post were Lieutenant-Colonel Sir Charles Rowan and Sir Richard Mayne. When Sir Charles Rowan died, leaving Sir Richard Mayne as the surviving Commissioner, Captain William Hay was drafted in to jointly run the force with Mayne. However, because the two Commissioners did not agree on methods of running the force, since 1855 it was decided that only one Commissioner would run the force.\\r\\nMetropolitan Police patrols took to the streets on 29 September 1829, despite resistance from certain elements of the community who saw them to be a threat to civil liberties.[11] The initial force consisted of two Commissioners, eight Superintendents, 20 Inspectors, 88 Sergeants and 895 Constables.[12] Patrolling the streets within a seven-mile (11?km) radius of Charing Cross, in order to prevent crime and pursue offenders.[13] Between 1829 and 1830, 17 local divisions each with its own police station were established, each lettered A to V, allocating each London borough with a designated letter.[14] These divisions were: A (Westminster); B (Chelsea); C (Mayfair and Soho); D (Marylebone); E (Holborn); F (Kensington); G (Kings Cross); H (Stepney); K (West Ham); L (Lambeth); M (Southwark); N (Islington); P (Peckham); R (Greenwich); S (Hampstead); T (Hammersmith) and V (Wandsworth). In 1865 three more divisions were created, W (Clapham); X (Willesden) and Y (Tottenham); J Division (Bethnal Green) was added in 1886.\\r\\nOn 28 June 1830, Constable Joseph Grantham became the first member of the force to be killed in the line of duty, an incident described by the Coroner's Inquest as \\"justifiable homicide\\".[15] Other indications of the Constabulary's unpopularity of the time, were such nicknames as 'Raw Lobsters', 'Blue Devils' and 'Peel's Bloody Gang'. Officers were physically assaulted, others impaled, blinded, and on one occasion held down while a vehicle was driven over them.\\r\\nOne of the priorities of the Metropolitan Police from the beginning was \\"maintaining public order\\", which they were active in doing, against the major Chartist demonstrations (1839ÿ48) and the Bloody Sunday demonstration of the unemployed in Trafalgar Square in 1887.[16]\\r\\nIn 1839, the Bow Street Runners, the Foot and Horse Patrol and the Thames River Police were amalgamated with the Metropolitan Police. However, the City of London Police, created in the same year was an independent force. In 1842 taking over a function formerly the responsibility of the Runners, a new investigative force was formed as the \\"Detective Branch\\". And first consisted of; two Inspectors, six Sergeants and a number of Constables.[17]\\r\\nOne of the first cases investigated by the newly formed Detective Branch was The Bermondsey Horror of 1849, in which a married couple, Frederick and Marie Manning, murdered Patrick O'Connor and buried his body under the kitchen floor. After going on the run they were tracked down by Detective Sergeants Thornton and Langley and publicly hanged outside Horsemonger Gaol in Southwark.[18]\\r\\nAfter Rowan's death in 1852, Mayne presided as sole Commissioner. In 1857 he was paid a salary of S1,883 (a fortune, roughly equivalent to S1.2m in 2009),[19] and his two Assistant Commissioners were paid salaries S800 each,[20] approximately S526,000 in 2009.[19]\\r\\nIt took some time to establish the standards of discipline expected today from a police force. In 1863, 215 officers were arrested for being intoxicated while on duty,[20] In 1872 there was a police strike, and during 1877 three high ranking detectives were tried for corruption at the Old Bailey.[21] Due to this latter scandal the Detective Branch was re-organised in 1878 by C. E. Howard Vincent, and renamed the Criminal Investigation Department (CID). This was separated from the uniformed branch and its head had direct access to the Home Secretary, by-passing the Commissioner.[17]\\r\\nSpecial Constables were first introduced by the Special Constables Act 1831, empowering Magistrates to appoint ordinary citizens as temporary police officers in times of emergency.[22] In 1834, the Act was extended to allow citizens appointed as Specials to act outside of their Parish area.[22] In 1848, 150,000 Specials were sworn in, to assist regular officers in preventing Chartists from reaching Kennington, and then marching to Westminster.[22] In 1912, the Specials were reorganised, scrapping the old system of anyone being liable to be appointed, instead they had to volunteer.[22] In 1934, it was named the Metropolitan Special Constabulary (MSC), a name which it keeps today in its present form.[22] For a short period of time after the MSC was formed, Specials did not receive uniforms like that of a full-time policeman. Instead, they were issued with armbands which identified them as Special Constables, along with being issued a truncheon and a whistle.\\r\\nThe threat of Irish terrorism was combated by the formation of the Special Irish Branch, in March 1883. The \\"Irish\\" sobriquet was dropped in 1888 as the department remit was extended to cover other threats, and became known simply as Special Branch.[23][24]\\r\\nImportant criminal investigations of the period included the Whitechapel murders (1888) and the Cleveland Street scandal (1889).[25]\\r\\nBy 1900, the force had grown to nearly 16,000 officers, organised into 21 divisions, responsible for law enforcement within an area of nearly 700 square miles.[17]\\r\\nDetection of crimes was much improved when Edward Henry, the Commissioner from 1903ÿ18, set up a Fingerprint Bureau at Scotland Yard in 1901.[26] A landmark case for the Met in forensic investigation was the Stratton Brothers case of 1905, concerning a double murder in Deptford, committed by Alfred and Albert Stratton, in which, for the first time, fingerprint evidence secured the conviction.[27] Another important investigation of this period was that into the murderer Hawley Harvey Crippen in 1910.[26]\\r\\nAlong with law enforcement within the Metropolitan Police District, the Metropolitan Police also had responsibility for the policing of the Royal Dockyards and other royal naval bases between 1860 until 1934, including Portsmouth, Chatham, Devonport, Royal Naval Air Station Pembroke and the Royal Woolwich Arsenal. They also policed Rosyth Dockyard from 1914 until 1926.[28]\\r\\nBefore the 1970s, police forces often called for assistance from the Metropolitan Police because of their detective experience. The last case of this was when the now defunct Buckinghamshire Constabulary called upon the MPS to help in the investigation of the Great Train Robbery.[29]\\r\\nIn 1931, Hugh Trenchard was appointed as Police Commissioner[30] Trenchard served as head of the Metropolitan Police until 1935 and during his tenure he instigated several changes. These included limiting membership of the Police Federation, introducing limited terms of employment[31] and the short-lived creation of separate career paths for the lower and higher ranks akin to the military system of officer and non-commissioned career streams. Perhaps Trenchard's most well known achievement during his time as Commissioner was the establishment of the Hendon Police College which originally was the institution from which Trenchard's junior station inspectors graduated before following a career in the higher ranks.[32]\\r\\nWhen Great Britain entered World War II on September 3, 1939, the strength of the Metropolitan Police stood at 18,428, which was 900 officers short of full strength. Due to the increased responsibilities of the police during war-time, three reserve groups were mobilised. The first consisted of 2,737 ex-police pensioners who were re-engaged, a second of 5,380 Special Constables serving on a full-time basis for the duration of the war, and the third being 18,868 War Reserve Constables employed on the same basis as the Special Constables. On the same day as the Battle of Dunkirk, Scotland Yard issued a memorandum detailing the police use of firearms in wartime. The memorandum detailed the planned training for all officers in the use of pistols and revolvers, as despite the police being a non-combatant force, while the war was in progress they would be responsible for providing armed protection at premises deemed at risk from enemy sabotage and would assist the British Armed Forces in the event of an invasion. Due to these added roles, on 1 June 1940, 3,500 Canadian Ross Rifles and 72,384 rounds of .303 ammunition were received from the military and distributed among divisions. Thames Division were allocated the smallest amount of 61 rifles, and \\"S\\" Division the largest with 190. Fifty rifles were also issued to the London Fire Brigade and the Port of London Authority Police.\\r\\nAfter staying stable for decades, crime rates in London soared during and after the Second World War, posing a new challenge to police. The chaotic conditions of the City under aerial attack were followed by crime, such as looting, and theft of goods and foodstuffs for illicit sales as black market rationed goods. This also fuelled the activities of criminal gangs who continued and expanded their activities after the war. By 1948, the number of recorded crimes in London had risen tenfold from the 1920s, to more than 126,000. By 1959 they had reached 160,000.[33]\\r\\nOn the night of 2 November 1952, Derek Bentley and Christopher Craig set out to break into the confectionery manufacturers Barlow & Parker in Croydon. Bentley and Craig were spotted climbing up a drain pipe to gain access to the roof by a member of the public, who called the police. The first officer to arrive on scene was Detective Sergeant Frederick Fairfax; by this time both Bentley and Craig had hidden behind the lift shaft. DS Fairfax gained entry to the roof and apprehended Bentley, but while doing so was shot in the shoulder by Craig. Upon armed uniformed officers arriving, Constable Sidney Miles was shot dead by Craig. After trial, Bentley was sentenced to death and Craig to be remanded at Her Majesty's Pleasure. For DS Fairfax's role in the incident, he received a George Cross, as did Constables Norman Harrison and James McDonald. Constable Robert Jaggs was awarded the British Empire Medal, with Sidney Miles awarded a posthumous Queen's Police Medal for Gallantry.\\r\\nDuring the 1950s and 1960s, London was subject to many protests by organisations. On more than one occasion, police clashed with violent protesters, making newspaper headlines. The need for a public order trained police unit was realised, and in 1965 the Special Patrol Group was formed. The Officers attached to the SPG received higher training in public order policing than divisional counterparts. The group often received controversy and accusations of police brutality. The best known of the police brutality cases was the killing of Blair Peach.[34] (In 1986, the SPG was succeeded by the Territorial Support Group which did much of the same role, but was a modernised form.)\\r\\nIn the late 1970s Operation Countryman investigated allegations of endemic corruption in the 1960s and 1970s. It concluded that there had been corruption at many levels. Only 8 prosecutions were brought but several hundred officers retired or resigned as a result.\\r\\nIn 1981, a report issued by Lord Scarman stated that the Metropolitan Police were having problems regarding racial discrimination.[35] The issue arose again in the 1999 Macpherson Report, which stated that institutional racism existed in the force.[36]\\r\\nThe official title was changed from \\"Metropolitan Police Force\\" to \\"Metropolitan Police Service\\" as part of the \\"PLUS Programme\\" in 1989, under the then Commissioner Sir Peter Imbert, following the presentation of a report entitled \\"A Force for Change: Report on the Corporate Identity of the Metropolitan Police\\" to the force's Policy Committee by Wolff Olins corporate identity consultants in August 1988.[37]\\r\\nThe current uniform for MPS officers is largely the same as forces outside London, apart from insignia differences. Officers on patrol are most likely to carry on the duty belt; extendible/rigid baton, Airwave personal radio, CS/PAVA Incapacitant Spray, Speedcuffs, a very small first aid kit containing a face shield for artificial respiration, protective gloves and sometimes some sticking plasters and a torch at night.\\r\\nThe force continued to be controlled directly by the Home Secretary until 2000, when the newly created Greater London Authority was given responsibility to oversee the force, through the Metropolitan Police Authority. The MPA is made up of members appointed by the Mayor of London and the London Assembly, and several independent members. However, the Metropolitan Police Commissioner is still appointed by the Home Secretary.[38]\\r\\nSince the creation of the MPS in 1829, 2 Albert Medals, 174 King's Police Medals for Gallantry (including 33 for entering waters to save life, 41 for stopping runaway horses, and 26 for saving lives by entering burning buildings) 30 King's Police and Fire Services Medals, 4 Queen's Police Medals for gallantry awarded posthumously, 5 George Crosses, 123 George Medals (including 85 for connection with war activities), 81 British Empire Medals for gallantry, and 49 Queen's Gallantry Medals have been awarded to officers.[39]\\r\\nOfficers (both City and Metropolitan) on duty during various royal jubilees and coronations were eligible for commemorative medals. Since 1951, in common with all members of U.K. police forces, officers can receive the Police Long Service and Good Conduct Medal after 20 (formerly 22) years of duty.[40]\\r\\nWhen a male police officer was asked in 1916 whether women would ever be police constables, he burst out laughing, replying: \\"No, not even if the war lasts 50 years.\\"[41] However, the Womens Police Service, made up of volunteers, had been founded in 1914.[42] Female police officers first joined the Metropolitan Police in 1919, although the then Commissioner, Sir Nevil Macready, insisted he did not want any vinegary spinsters or blighted middle-aged fanatics in its ranks.[42] The female police officers were distinguished from their male counterparts, who had wider authority, by the prefix 'woman' before their rank, such as \\"Woman Police Constable\\" (WPC) and \\"Woman Police Sergeant\\" (WPS).\\r\\nThe first female police officer in the Metropolitan Police was Sofia Stanley in 1919, and she designed the first women's police uniform, known as the Stanley uniform.[43] Initial duties of female police officers included patrolling areas frequented by prostitutes, along with care and observation of female and juvenile detainees, deterring prostitution, helping prevent the deceitful practice of fortune telling (then illegal), and looking after women who attempted to commit suicide (also then illegal).[42] Female officers were allowed to go into brothels, nightclubs and betting houses to observe and gather evidence of untoward behaviour, but at the first sign of crime being committed, they had to call in male colleagues. They were not allowed to carry handcuffs unless instructed to by a senior officer, and were not allowed to make arrests until 1923.[44][45]\\r\\nFemale police officers were initially given six-day, 48-hour work weeks, and were not allowed to work night shifts apart from when on-call duties until 1973. Female officers did in fact do a week of nights at least from 1965. Female officers were usually seconded to the Criminal Investigation Department, but in 1921 Lilian Wyles was appointed the first female Inspector to the Criminal Intelligence Department.[46] A policy in place from 1927 until 1946 forced women to leave the Metropolitan Police if they got married.[43] Female police officers were not authorised to take fingerprints until 1937.[43] The Police Federation, the rank-and-file staff association, let women join in 1948.[42]\\r\\nThe first women police officers to receive George Medals for courage were Sergeant Ethel Bush and Kathleen Parrott, who had been separately attacked by a sex offender they were on a stakeout in pursuit of in 1955.[43] In 1968 Sislin Fay Allen became the Metropolitan Polices first black female officer.[42] On February 1, 1971, Karpal Kaur Sandhu, born in Zanzibar but of Indian heritage, joined the Metropolitan Police and thus became the Metropolitan Police's (and Britain's) first female Asian police officer.[47] This was before India itself had female police officers (the first female police officer in India was Kiran Bedi in 1972).[47]\\r\\nThe first Woman Detective Constable was appointed in 1973. 1973 was also the year that the separate Women's Department was fully integrated into the Metropolitan Police.[42] Female police officers did not get equal pay with male police officers until 1974.[43] In 1976 the first Woman Chief Superintendent was appointed to take charge of a subdivision.[48] In 1977 the first female traffic officer, Dee ODonoghue, was hired.[48] In 1979 the first female dog handler, Nicola Gray, was hired.[48] Prior to this, women were prohibited from being dog handlers since rules stated that an officer should have a wife who could look after a puppy while the officer went to work.[43] In 1995 Pauline Clare became the first female chief constable, for Lancashire.[42] The prefix \\"Woman\\" in front of female officers' ranks has been obsolete since 1999. In 2017 Cressida Dick became the first female Commissioner of the Metropolitan Police Service and therefore the most senior Police Officer in England and Wales.[42]\\r\\nBetween 1990 and 2010, over 50 serving MPS officers died in service, with eight being murdered or fatally injured by an assailant.[49]","input":"When did the metropolitan police force become a service?"},{"output":"Greek","context":"Cynthia is a feminine given name of Greek origin: Ĳ?ϫ, Kyntha, \\"from Mount Cynthus\\" on Delos island. It can be abbreviated as Cindy or as Cyndy. There are various spellings for this name.\\r\\nCynthia was originally an epithet of the Greek goddess Artemis, who according to legend was born on Mount Cynthus. Selene, the Greek personification of the moon, and the Roman Diana were also sometimes called \\"Cynthia.\\"[1]","input":"What is the origin of the name cynthia?"},{"output":"Ren Descartes","context":"A Cartesian coordinate system is a coordinate system that specifies each point uniquely in a plane by a pair of numerical coordinates, which are the signed distances to the point from two fixed perpendicular directed lines, measured in the same unit of length. Each reference line is called a coordinate axis or just axis (plural axes) of the system, and the point where they meet is its origin, at ordered pair (0, 0). The coordinates can also be defined as the positions of the perpendicular projections of the point onto the two axes, expressed as signed distances from the origin.\\r\\nOne can use the same principle to specify the position of any point in three-dimensional space by three Cartesian coordinates, its signed distances to three mutually perpendicular planes (or, equivalently, by its perpendicular projection onto three mutually perpendicular lines). In general, n Cartesian coordinates (an element of real n-space) specify the point in an n-dimensional Euclidean space for any dimension n. These coordinates are equal, up to sign, to distances from the point to n mutually perpendicular hyperplanes.\\r\\nThe invention of Cartesian coordinates in the 17th century by Ren Descartes (Latinized name: Cartesius) revolutionized mathematics by providing the first systematic link between Euclidean geometry and algebra. Using the Cartesian coordinate system, geometric shapes (such as curves) can be described by Cartesian equations: algebraic equations involving the coordinates of the points lying on the shape. For example, a circle of radius 2, centered at the origin of the plane, may be described as the set of all points whose coordinates x and y satisfy the equation x2 + y2 = 4.\\r\\nCartesian coordinates are the foundation of analytic geometry, and provide enlightening geometric interpretations for many other branches of mathematics, such as linear algebra, complex analysis, differential geometry, multivariate calculus, group theory and more. A familiar example is the concept of the graph of a function. Cartesian coordinates are also essential tools for most applied disciplines that deal with geometry, including astronomy, physics, engineering and many more. They are the most common coordinate system used in computer graphics, computer-aided geometric design and other geometry-related data processing.\\r\\n\\r\\n\\r\\nThe adjective Cartesian refers to the French mathematician and philosopher Ren Descartes who published this idea in 1637. It was independently discovered by Pierre de Fermat, who also worked in three dimensions, although Fermat did not publish the discovery.[1] The French cleric Nicole Oresme, used constructions similar to Cartesian coordinates well before the time of Descartes and Fermat.[citation needed].\\r\\nBoth Descartes and Fermat used a single axis in their treatments and have a variable length measured in reference to this axis. The concept of using a pair of axes was introduced later, after Descartes' La Gomtrie was translated into Latin in 1649 by Frans van Schooten and his students. These commentators introduced several concepts while trying to clarify the ideas contained in Descartes' work.[2]\\r\\nThe development of the Cartesian coordinate system would play a fundamental role in the development of the calculus by Isaac Newton and Gottfried Wilhelm Leibniz.[3] The two-coordinate description of the plane was later generalized into the concept of vector spaces.[4]\\r\\nMany other coordinate systems have been developed since Descartes, such as the polar coordinates for the plane, and the spherical and cylindrical coordinates for three-dimensional space.\\r\\nChoosing a Cartesian coordinate system for a one-dimensional space?ÿ  that is, for a straight lineinvolves choosing a point O of the line (the origin), a unit of length, and an orientation for the line. An orientation chooses which of the two half-lines determined by O is the positive, and which is negative; we then say that the line \\"is oriented\\" (or \\"points\\") from the negative half towards the positive half. Then each point P of the line can be specified by its distance from O, taken with a + or ? sign depending on which half-line contains P.\\r\\nA line with a chosen Cartesian system is called a number line. Every real number has a unique location on the line. Conversely, every point on the line can be interpreted as a number in an ordered continuum such as the real numbers.\\r\\nA Cartesian coordinate system in two dimensions (also called a rectangular coordinate system or an orthogonal coordinate system[5]) is defined by an ordered pair of perpendicular lines (axes), a single unit of length for both axes, and an orientation for each axis. The point where the axes meet is taken as the origin for both, thus turning each axis into a number line. For any point P, a line is drawn through P perpendicular to each axis, and the position where it meets the axis is interpreted as a number. The two numbers, in that chosen order, are the Cartesian coordinates of P. The reverse construction allows one to determine the point P given its coordinates.\\r\\nThe first and second coordinates are called the abscissa and the ordinate of P, respectively; and the point where the axes meet is called the origin of the coordinate system. The coordinates are usually written as two numbers in parentheses, in that order, separated by a comma, as in (3, -10.5). Thus the origin has coordinates (0,0), and the points on the positive half-axes, one unit away from the origin, have coordinates (1,0) and (0,1).\\r\\nIn mathematics, physics, and engineering, the first axis is usually defined or depicted as horizontal and oriented to the right, and the second axis is vertical and oriented upwards. (However, in some computer graphics contexts, the ordinate axis may be oriented downwards.) The origin is often labeled O, and the two coordinates are often denoted by the letters X and Y, or x and y. The axes may then be referred to as the X-axis and Y-axis. The choices of letters come from the original convention, which is to use the latter part of the alphabet to indicate unknown values. The first part of the alphabet was used to designate known values.\\r\\nA Euclidean plane with a chosen Cartesian coordinate system Cartesian plane. In a Cartesian plane one can define canonical representatives of certain geometric figures, such as the unit circle (with radius equal to the length unit, and center at the origin), the unit square (whose diagonal has endpoints at (0,0) and (1,1)), the unit hyperbola, and so on.\\r\\nThe two axes divide the plane into four right angles, called quadrant. The quadrants may be named or numbered in various ways, but the quadrant where all coordinates are positive is usually called the first quadrant.\\r\\nIf the coordinates of a point are (x,y), then its distances from the X-axis and from the Y-axis are |y| and |x|. respectively; where |...| denotes the absolute value of a number.\\r\\nA Cartesian coordinate system for a three-dimensional space consists of an ordered triplet of lines (the axes) that go through a common point (the origin), and are pair-wise perpendicular; an orientation for each axis; and a single unit of length for all three axes. As in the two-dimensional case, each axis becomes a number line. For any point P of space, one considers a plane through P perpendicular to each coordinate axis, and interprets the point where that plane cuts the axis as a number. The Cartesian coordinates of P are those three numbers, in the chosen order The reverse construction determines the point P given its three coordinates.\\r\\nAlternatively, each coordinate of a point P can be taken as the distance from P to the plane defined by the other two axes, with the sign determined by the orientation of the corresponding axis.\\r\\nEach pair of axes defines a coordinate plane. These planes divide space into eight trihedra, called octants.\\r\\nThe coordinates are usually written as three numbers (or algebraic formulas) surrounded by parentheses and separated by commas, as in (3,-2.5,1) or (t,u+v,/2). Thus, the origin has coordinates (0,0,0), and the unit points on the three axes are (1,0,0), (0,1,0), and (0,0,1).\\r\\nThere are no standard names for the coordinates in the three axes. The coordinates are often denoted by the letters X, Y, and Z (or x, y, and z), in which case the lines are called the X-, Y-, and Z-axis, respectively. Then the coordinate planes can be referred to as the XY-, YZ-, and XZ-planes.\\r\\nIn mathematics, physics, and engineering contexts, the first two axes are often defined or depicted as horizontal, with the third axis pointing up. In that case the third coordinate may be called height or altitude. The orientations are usually chosen so that the 90 degree angle from the first axis to the second axis looks counter-clockwise when seen from the point (0,0,1); a convention that is commonly called the right hand rule.\\r\\nA Euclidean plane with a chosen Cartesian system is called a Cartesian plane. Since Cartesian coordinates are unique and non-ambiguous, the points of a Cartesian plane can be identified with pairs of real numbers; that is with the Cartesian product \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nR\\r\\n\\r\\n\\r\\n2\\r\\n\\r\\n\\r\\n=\\r\\n\\r\\nR\\r\\n\\r\\nG\\r\\n\\r\\nR\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\mathbb {R} ^{2}=\\\\mathbb {R} \\\\times \\\\mathbb {R} }\\r\\n\\r\\n, where \\r\\n\\r\\n\\r\\n\\r\\n\\r\\nR\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\mathbb {R} }\\r\\n\\r\\n is the set of all reals. In the same way, the points in any Euclidean space of dimension n be identified with the tuples (lists) of n real numbers, that is, with the Cartesian product \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nR\\r\\n\\r\\n\\r\\nn\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\mathbb {R} ^{n}}\\r\\n\\r\\n.\\r\\nThe concept of Cartesian coordinates generalizes to allow axes that are not perpendicular to each other, and/or different units along each axis. In that case, each coordinate is obtained by projecting the point onto one axis along a direction that is parallel to the other axis (or, in general, to the hyperplane defined by all the other axes). In such an oblique coordinate system the computations of distances and angles must be modified from that in standard Cartesian systems, and many standard formulas (such as the Pythagorean formula for the distance) do not hold (see affine plane).\\r\\nThe Cartesian coordinates of a point are usually written in parentheses and separated by commas, as in (10, 5) or (3, 5, 7). The origin is often labelled with the capital letter O. In analytic geometry, unknown or generic coordinates are often denoted by the letters (x, y) in the plane, and (x, y, z) in three-dimensional space. This custom comes from a convention of algebra, which uses letters near the end of the alphabet for unknown values (such as were the coordinates of points in many geometric problems), and letters near the beginning for given quantities.\\r\\nThese conventional names are often used in other domains, such as physics and engineering, although other letters may be used. For example, in a graph showing how a pressure varies with time, the graph coordinates may be denoted p and t. Each axis is usually named after the coordinate which is measured along it; so one says the x-axis, the y-axis, the t-axis, etc.\\r\\nAnother common convention for coordinate naming is to use subscripts, as (x1, x2, ..., xn) for the n coordinates in an n-dimensional space, especially when n is greater than 3 or unspecified. Some authors prefer the numbering (x0, x1, ..., xn?1). These notations are especially advantageous in computer programming: by storing the coordinates of a point as an array, instead of a record, the subscript can serve to index the coordinates.\\r\\nIn mathematical illustrations of two-dimensional Cartesian systems, the first coordinate (traditionally called the abscissa) is measured along a horizontal axis, oriented from left to right. The second coordinate (the ordinate) is then measured along a vertical axis, usually oriented from bottom to top. Young children learning the Cartesian system, commonly learn the order to read the values before cementing the x, y, z axis concepts, by starting with 2D mnemonics (e.g. 'Walk along the hall then up the stairs' akin to straight across the x axis then up vertically along the y axis).[6]\\r\\nComputer graphics and image processing, however, often use a coordinate system with the y-axis oriented downwards on the computer display. This convention developed in the 1960s (or earlier) from the way that images were originally stored in display buffers.\\r\\nFor three-dimensional systems, a convention is to portray the xy-plane horizontally, with the z-axis added to represent height (positive up). Furthermore, there is a convention to orient the x-axis toward the viewer, biased either to the right or left. If a diagram (3D projection or 2D perspective drawing) shows the x- and y-axis horizontally and vertically, respectively, then the z-axis should be shown pointing \\"out of the page\\" towards the viewer or camera. In such a 2D diagram of a 3D coordinate system, the z-axis would appear as a line or ray pointing down and to the left or down and to the right, depending on the presumed viewer or camera perspective. In any diagram or display, the orientation of the three axes, as a whole, is arbitrary. However, the orientation of the axes relative to each other should always comply with the right-hand rule, unless specifically stated otherwise. All laws of physics and math assume this right-handedness, which ensures consistency.\\r\\nFor 3D diagrams, the names \\"abscissa\\" and \\"ordinate\\" are rarely used for x and y, respectively. When they are, the z-coordinate is sometimes called the applicate. The words abscissa, ordinate and applicate are sometimes used to refer to coordinate axes rather than the coordinate values.[5]\\r\\nThe axes of a two-dimensional Cartesian system divide the plane into four infinite regions, called quadrants,[5] each bounded by two half-axes. These are often numbered from 1st to 4th and denoted by Roman numerals: I (where the signs of the two coordinates are I (+,+), II (?,+), III (?,?), and IV (+,?). When the axes are drawn according to the mathematical custom, the numbering goes counter-clockwise starting from the upper right (\\"north-east\\") quadrant.\\r\\nSimilarly, a three-dimensional Cartesian system defines a division of space into eight regions or octants,[5] according to the signs of the coordinates of the points. The convention used for naming a specific octant is to list its signs, e.g. (+ + +) or (? + ?). The generalization of the quadrant and octant to an arbitrary number of dimensions is the orthant, and a similar naming system applies.\\r\\nThe Euclidean distance between two points of the plane with Cartesian coordinates \\r\\n\\r\\n\\r\\n\\r\\n(\\r\\n\\r\\nx\\r\\n\\r\\n1\\r\\n\\r\\n\\r\\n,\\r\\n\\r\\ny\\r\\n\\r\\n1\\r\\n\\r\\n\\r\\n)\\r\\n\\r\\n\\r\\n{\\\\displaystyle (x_{1},y_{1})}\\r\\n\\r\\n and \\r\\n\\r\\n\\r\\n\\r\\n(\\r\\n\\r\\nx\\r\\n\\r\\n2\\r\\n\\r\\n\\r\\n,\\r\\n\\r\\ny\\r\\n\\r\\n2\\r\\n\\r\\n\\r\\n)\\r\\n\\r\\n\\r\\n{\\\\displaystyle (x_{2},y_{2})}\\r\\n\\r\\n is\\r\\nThis is the Cartesian version of Pythagoras's theorem. In three-dimensional space, the distance between points \\r\\n\\r\\n\\r\\n\\r\\n(\\r\\n\\r\\nx\\r\\n\\r\\n1\\r\\n\\r\\n\\r\\n,\\r\\n\\r\\ny\\r\\n\\r\\n1\\r\\n\\r\\n\\r\\n,\\r\\n\\r\\nz\\r\\n\\r\\n1\\r\\n\\r\\n\\r\\n)\\r\\n\\r\\n\\r\\n{\\\\displaystyle (x_{1},y_{1},z_{1})}\\r\\n\\r\\n and \\r\\n\\r\\n\\r\\n\\r\\n(\\r\\n\\r\\nx\\r\\n\\r\\n2\\r\\n\\r\\n\\r\\n,\\r\\n\\r\\ny\\r\\n\\r\\n2\\r\\n\\r\\n\\r\\n,\\r\\n\\r\\nz\\r\\n\\r\\n2\\r\\n\\r\\n\\r\\n)\\r\\n\\r\\n\\r\\n{\\\\displaystyle (x_{2},y_{2},z_{2})}\\r\\n\\r\\n is\\r\\nwhich can be obtained by two consecutive applications of Pythagoras' theorem.[7]\\r\\nThe Euclidean transformations or Euclidean motions are the (bijective) mappings of points of the Euclidean plane to themselves which preserve distances between points. There are four types of these mappings (also called isometries): translations, rotations, reflections and glide reflections.[8]\\r\\nTranslating a set of points of the plane, preserving the distances and directions between them, is equivalent to adding a fixed pair of numbers (a, b) to the Cartesian coordinates of every point in the set. That is, if the original coordinates of a point are (x, y), after the translation they will be\\r\\nTo rotate a figure counterclockwise around the origin by some angle \\r\\n\\r\\n\\r\\n\\r\\nĲ\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\theta }\\r\\n\\r\\n is equivalent to replacing every point with coordinates (x,y) by the point with coordinates (x',y'), where\\r\\nThus:\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n(\\r\\n\\r\\nx\\r\\n\\r\\n\\r\\n,\\r\\n\\r\\ny\\r\\n\\r\\n\\r\\n)\\r\\n=\\r\\n(\\r\\n(\\r\\nx\\r\\ncos\\r\\n?\\r\\nĲ\\r\\n?\\r\\ny\\r\\nsin\\r\\n?\\r\\nĲ\\r\\n\\r\\n)\\r\\n,\\r\\n(\\r\\nx\\r\\nsin\\r\\n?\\r\\nĲ\\r\\n+\\r\\ny\\r\\ncos\\r\\n?\\r\\nĲ\\r\\n\\r\\n)\\r\\n)\\r\\n.\\r\\n\\r\\n\\r\\n{\\\\displaystyle (x',y')=((x\\\\cos \\\\theta -y\\\\sin \\\\theta \\\\,),(x\\\\sin \\\\theta +y\\\\cos \\\\theta \\\\,)).}\\r\\n\\r\\n\\r\\nIf (x, y) are the Cartesian coordinates of a point, then (?x, y) are the coordinates of its reflection across the second coordinate axis (the y-axis), as if that line were a mirror. Likewise, (x, ?y) are the coordinates of its reflection across the first coordinate axis (the x-axis). In more generality, reflection across a line through the origin making an angle \\r\\n\\r\\n\\r\\n\\r\\nĲ\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\theta }\\r\\n\\r\\n with the x-axis, is equivalent to replacing every point with coordinates (x, y) by the point with coordinates (x,y), where\\r\\nThus: \\r\\n\\r\\n\\r\\n\\r\\n(\\r\\n\\r\\nx\\r\\n\\r\\n\\r\\n,\\r\\n\\r\\ny\\r\\n\\r\\n\\r\\n)\\r\\n=\\r\\n(\\r\\n(\\r\\nx\\r\\ncos\\r\\n?\\r\\n2\\r\\nĲ\\r\\n+\\r\\ny\\r\\nsin\\r\\n?\\r\\n2\\r\\nĲ\\r\\n\\r\\n)\\r\\n,\\r\\n(\\r\\nx\\r\\nsin\\r\\n?\\r\\n2\\r\\nĲ\\r\\n?\\r\\ny\\r\\ncos\\r\\n?\\r\\n2\\r\\nĲ\\r\\n\\r\\n)\\r\\n)\\r\\n.\\r\\n\\r\\n\\r\\n{\\\\displaystyle (x',y')=((x\\\\cos 2\\\\theta +y\\\\sin 2\\\\theta \\\\,),(x\\\\sin 2\\\\theta -y\\\\cos 2\\\\theta \\\\,)).}\\r\\n\\r\\n\\r\\nA glide reflection is the composition of a reflection across a line followed by a translation in the direction of that line. It can be seen that the order of these operations does not matter (the translation can come first, followed by the reflection).\\r\\nThese Euclidean transformations of the plane can all be described in a uniform way by using matrices. The result \\r\\n\\r\\n\\r\\n\\r\\n(\\r\\n\\r\\nx\\r\\n\\r\\n\\r\\n,\\r\\n\\r\\ny\\r\\n\\r\\n\\r\\n)\\r\\n\\r\\n\\r\\n{\\\\displaystyle (x',y')}\\r\\n\\r\\n of applying a Euclidean transformation to a point \\r\\n\\r\\n\\r\\n\\r\\n(\\r\\nx\\r\\n,\\r\\ny\\r\\n)\\r\\n\\r\\n\\r\\n{\\\\displaystyle (x,y)}\\r\\n\\r\\n is given by the formula\\r\\nwhere A is a 2G2 orthogonal matrix and b = (b1, b2) is an arbitrary ordered pair of numbers;[9] that is,\\r\\nwhere\\r\\nTo be orthogonal, the matrix A must have orthogonal rows with same Euclidean length of one, that is,\\r\\nand\\r\\nThis is equivalent to saying that A times its transpose must be the identity matrix. If these conditions do not hold, the formula describes a more general affine transformation of the plane provided that the determinant of A is not zero.\\r\\nThe formula defines a translation if and only if A is the identity matrix. The transformation is a rotation around some point if and only if A is a rotation matrix, meaning that\\r\\nA reflection or glide reflection is obtained when,\\r\\nAssuming that translation is not used transformations can be combined by simply multiplying the associated transformation matrices.\\r\\nAnother way to represent coordinate transformations in Cartesian coordinates is through affine transformations. In affine transformations an extra dimension is added and all points are given a value of 1 for this extra dimension. The advantage of doing this is that point translations can be specified in the final column of matrix A. In this way, all of the euclidean transformations become transactable as matrix point multiplications. The affine transformation is given by:\\r\\nUsing affine transformations multiple different euclidean transformations including translation can be combined by simply multiplying the corresponding matrices.\\r\\nAn example of an affine transformation which is not a Euclidean motion is given by scaling. To make a figure larger or smaller is equivalent to multiplying the Cartesian coordinates of every point by the same positive number m. If (x, y) are the coordinates of a point on the original figure, the corresponding point on the scaled figure has coordinates\\r\\nIf m is greater than 1, the figure becomes larger; if m is between 0 and 1, it becomes smaller.\\r\\nA shearing transformation will push the top of a square sideways to form a parallelogram. Horizontal shearing is defined by:\\r\\nShearing can also be applied vertically:\\r\\nFixing or choosing the x-axis determines the y-axis up to direction. Namely, the y-axis is necessarily the perpendicular to the x-axis through the point marked 0 on the x-axis. But there is a choice of which of the two half lines on the perpendicular to designate as positive and which as negative. Each of these two choices determines a different orientation (also called handedness) of the Cartesian plane.\\r\\nThe usual way of orienting the axes, with the positive x-axis pointing right and the positive y-axis pointing up (and the x-axis being the \\"first\\" and the y-axis the \\"second\\" axis) is considered the positive or standard orientation, also called the right-handed orientation.\\r\\nA commonly used mnemonic for defining the positive orientation is the right-hand rule. Placing a somewhat closed right hand on the plane with the thumb pointing up, the fingers point from the x-axis to the y-axis, in a positively oriented coordinate system.\\r\\nThe other way of orienting the axes is following the left hand rule, placing the left hand on the plane with the thumb pointing up.\\r\\nWhen pointing the thumb away from the origin along an axis towards positive, the curvature of the fingers indicates a positive rotation along that axis.\\r\\nRegardless of the rule used to orient the axes, rotating the coordinate system will preserve the orientation. Switching any two axes will reverse the orientation, but switching both will leave the orientation unchanged.\\r\\nOnce the x- and y-axes are specified, they determine the line along which the z-axis should lie, but there are two possible directions on this line. The two possible coordinate systems which result are called 'right-handed' and 'left-handed'. The standard orientation, where the xy-plane is horizontal and the z-axis points up (and the x- and the y-axis form a positively oriented two-dimensional coordinate system in the xy-plane if observed from above the xy-plane) is called right-handed or positive.\\r\\nThe name derives from the right-hand rule. If the index finger of the right hand is pointed forward, the middle finger bent inward at a right angle to it, and the thumb placed at a right angle to both, the three fingers indicate the relative directions of the x-, y-, and z-axes in a right-handed system. The thumb indicates the x-axis, the index finger the y-axis and the middle finger the z-axis. Conversely, if the same is done with the left hand, a left-handed system results.\\r\\nFigure 7 depicts a left and a right-handed coordinate system. Because a three-dimensional object is represented on the two-dimensional screen, distortion and ambiguity result. The axis pointing downward (and to the right) is also meant to point towards the observer, whereas the \\"middle\\"-axis is meant to point away from the observer. The red circle is parallel to the horizontal xy-plane and indicates rotation from the x-axis to the y-axis (in both cases). Hence the red arrow passes in front of the z-axis.\\r\\nFigure 8 is another attempt at depicting a right-handed coordinate system. Again, there is an ambiguity caused by projecting the three-dimensional coordinate system into the plane. Many observers see Figure 8 as \\"flipping in and out\\" between a convex cube and a concave \\"corner\\". This corresponds to the two possible orientations of the coordinate system. Seeing the figure as convex gives a left-handed coordinate system. Thus the \\"correct\\" way to view Figure 8 is to imagine the x-axis as pointing towards the observer and thus seeing a concave corner.\\r\\nA point in space in a Cartesian coordinate system may also be represented by a position vector, which can be thought of as an arrow pointing from the origin of the coordinate system to the point.[10] If the coordinates represent spatial positions (displacements), it is common to represent the vector from the origin to the point of interest as \\r\\n\\r\\n\\r\\n\\r\\n\\r\\nr\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\mathbf {r} }\\r\\n\\r\\n. In two dimensions, the vector from the origin to the point with Cartesian coordinates (x, y) can be written as:\\r\\nwhere \\r\\n\\r\\n\\r\\n\\r\\n\\r\\ni\\r\\n\\r\\n=\\r\\n\\r\\n\\r\\n(\\r\\n\\r\\n\\r\\n\\r\\n1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n0\\r\\n\\r\\n\\r\\n\\r\\n)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\mathbf {i} ={\\\\begin{pmatrix}1\\\\\\\\0\\\\end{pmatrix}}}\\r\\n\\r\\n, and \\r\\n\\r\\n\\r\\n\\r\\n\\r\\nj\\r\\n\\r\\n=\\r\\n\\r\\n\\r\\n(\\r\\n\\r\\n\\r\\n\\r\\n0\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n1\\r\\n\\r\\n\\r\\n\\r\\n)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\mathbf {j} ={\\\\begin{pmatrix}0\\\\\\\\1\\\\end{pmatrix}}}\\r\\n\\r\\n are unit vectors in the direction of the x-axis and y-axis respectively, generally referred to as the standard basis (in some application areas these may also be referred to as versors). Similarly, in three dimensions, the vector from the origin to the point with Cartesian coordinates \\r\\n\\r\\n\\r\\n\\r\\n(\\r\\nx\\r\\n,\\r\\ny\\r\\n,\\r\\nz\\r\\n)\\r\\n\\r\\n\\r\\n{\\\\displaystyle (x,y,z)}\\r\\n\\r\\n can be written as:[11]\\r\\nwhere \\r\\n\\r\\n\\r\\n\\r\\n\\r\\nk\\r\\n\\r\\n=\\r\\n\\r\\n\\r\\n(\\r\\n\\r\\n\\r\\n\\r\\n0\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n0\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n1\\r\\n\\r\\n\\r\\n\\r\\n)\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\mathbf {k} ={\\\\begin{pmatrix}0\\\\\\\\0\\\\\\\\1\\\\end{pmatrix}}}\\r\\n\\r\\n is the unit vector in the direction of the z-axis.\\r\\nThere is no natural interpretation of multiplying vectors to obtain another vector that works in all dimensions, however there is a way to use complex numbers to provide such a multiplication. In a two dimensional cartesian plane, identify the point with coordinates (x, y) with the complex number z = x + iy. Here, i is the imaginary unit and is identified with the point with coordinates (0, 1), so it is not the unit vector in the direction of the x-axis. Since the complex numbers can be multiplied giving another complex number, this identification provides a means to \\"multiply\\" vectors. In a three dimensional cartesian space a similar identification can be made with a subset of the quaternions.\\r\\nCartesian coordinates are an abstraction that have a multitude of possible applications in the real world. However, three constructive steps are involved in superimposing coordinates on a problem application. 1) Units of distance must be decided defining the spatial size represented by the numbers used as coordinates. 2) An origin must be assigned to a specific spatial location or landmark, and 3) the orientation of the axes must be defined using available directional cues for all but one axis.\\r\\nConsider as an example superimposing 3D Cartesian coordinates over all points on the Earth (i.e. geospatial 3D). What units make sense? Kilometers are a good choice, since the original definition of the kilometer was geospatial...10,000?km equalling the surface distance from the Equator to the North Pole. Where to place the origin? Based on symmetry, the gravitational center of the Earth suggests a natural landmark (which can be sensed via satellite orbits). Finally, how to orient X-, Y- and Z-axis directions? The axis of Earth's spin provides a natural direction strongly associated with \\"up vs. down\\", so positive Z can adopt the direction from geocenter to North Pole. A location on the Equator is needed to define the X-axis, and the prime meridian stands out as a reference direction, so the X-axis takes the direction from geocenter out to [ 0 degrees longitude, 0 degrees latitude ]. Note that with 3 dimensions, and two perpendicular axes directions pinned down for X and Z, the Y-axis is determined by the first two choices. In order to obey the right-hand rule, the Y-axis must point out from the geocenter to [ 90 degrees longitude, 0 degrees latitude ]. So what are the geocentric coordinates of the Empire State Building in New York City? Using [ longitude = ?73.985656, latitude = 40.748433 ], Earth radius = 40,000/2, and transforming from spherical --> Cartesian coordinates, you can estimate the geocentric coordinates of the Empire State Building, [ x, y, z ] = [ 1330.53?km, ÿ4635.75?km, 4155.46?km ]. GPS navigation relies on such geocentric coordinates.\\r\\nIn engineering projects, agreement on the definition of coordinates is a crucial foundation. One cannot assume that coordinates come predefined for a novel application, so knowledge of how to erect a coordinate system where there is none is essential to applying Ren Descartes' ingenious thinking.\\r\\nWhile spatial apps employ identical units along all axes, in business and scientific apps, each axis may have different units of measurement associated with it (such as kilograms, seconds, pounds, etc.). Although four- and higher-dimensional spaces are difficult to visualize, the algebra of Cartesian coordinates can be extended relatively easily to four or more variables, so that certain calculations involving many variables can be done. (This sort of algebraic extension is what is used to define the geometry of higher-dimensional spaces.) Conversely, it is often helpful to use the geometry of Cartesian coordinates in two or three dimensions to visualize algebraic relationships between two or three of many non-spatial variables.\\r\\nThe graph of a function or relation is the set of all points satisfying that function or relation. For a function of one variable, f, the set of all points (x, y), where y = f(x) is the graph of the function f. For a function g of two variables, the set of all points (x, y, z), where z = g(x, y) is the graph of the function g. A sketch of the graph of such a function or relation would consist of all the salient parts of the function or relation which would include its relative extrema, its concavity and points of inflection, any points of discontinuity and its end behavior. All of these terms are more fully defined in calculus. Such graphs are useful in calculus to understand the nature and behavior of a function or relation.","input":"Who invented the x and y axis graph?"},{"output":"First grade","context":"First grade (called Year 2 in the UK) is the first grade in elementary school. It is the first school year after kindergarten in Canada and the USA. Children are usually 6ÿ7 years old in this grade level.\\r\\n\\r\\n\\r\\nIn England and Wales, the first year of school is called reception, and the pupils are 4 to 5 years old. However, the first compulsory school year is Year One, when children are five years old. As most primary schools have a reception class which is treated like a compulsory school year, i.e. the children wear uniform and have the same school hours, most children start school in reception. The first grade is the equivalent of Year Two.[3]\\r\\nScottish pupils usually enter the corresponding stage one year younger. In Northern Ireland they are two years younger. In Scotland, first grade is equivalent to Primary 3. Pupils in Primary 3 are 7 to 8 years old. America call their primary education grade school where in Scotland they call it primary school as it would be primary 1 would start after Kindergarten (nursery) and you would be age 4 to 5 years of age. then it would be primary 2 the next year and so on up until primary 7 which would be your last year at primary school then you go off to high school.\\r\\nIn math, students may learn about addition and subtraction of natural numbers, usually with only one digit, and about measurement. Basic geometry and graphing may be introduced. Clock and calendar time as well as money may also be in the curriculum.\\r\\nIn language, first graders are taught the fundamentals of literacy, including reading sentences, writing very simple statements and mastery of the alphabet, building on what the students have learned in kindergarten or other forms of pre-school (although because first grade is the first compulsory level of education in many U.S. states, the level of literacy in incoming students can vary widely). The expectations for first grade have changed due to the Common Core Standards. Curriculum is typically based on state standards developed by educators in each state. Most states use the Common Core Standards, so most schools across the country are using similar curricula. First graders are now expected to read and comprehend stories ranging in lengths and difficulty. They are also expected to show an improving fluency rate during the school year with ability to easily read stories by the end of the year.\\r\\nStudents are also typically introduced to the concept of social studies with an emphasis on establishing ideas of history or civics in either a personal or larger sense. Some states focus on the basics of US history and patriotism is taught, with a focus on the founding fathers and the time period surrounding the American Revolution; other states require a social studies focus on family relationships in first grade, leaving community, state, and nation studies to higher grades. Basic geography is also taught in the first grade. Focus on the school's municipal area and culture, along with basic state geography, may also be focused upon in first grade in some states. First grade science usually involves the discussion of matter, plant and animal science, earth materials, and balance and motion, along with the human body, and basic health and nutrition.\\r\\nScience as inquiry is taught and practiced in first grade. Students are encouraged to observe the world around them and begin asking questions about things they notice. As they become more comfortable investigating the world around them, students will also begin asking better questions and making better, more advanced predictions.\\r\\nIn Australia this year level is called \\"Prep\\" or Foundation. It is the first year of school after kindergarten and children are usually between 5-6 years old.\\r\\n\\"Year 1\\" generally begins if the child is received into a school on their 5th birthday when they turn such in the first few months of the year. If they are already 5 at the beginning of Year 1, they will most likely have had their 5th birthday in the latter part of the previous year, and have come up from \\"New Entrant\\" level.","input":"What grade are you at 6 years old?"},{"output":"fell beasts","context":"The Nazg?l (from Black Speech nazg, \\"ring\\", and g?l, \\"wraith, spirit\\", possibly related to gul, \\"sorcery\\" or a wordplay on \\"ghoul\\" ), also called Ringwraiths, Ring-wraiths, Black Riders, Dark Riders, the Nine Riders, or simply the Nine, are fictional characters in J. R. R. Tolkien's Middle-earth legendarium. They were nine men who succumbed to Sauron's power and attained near-immortality as wraiths, servants bound to the power of the One Ring and completely under the dominion of Sauron. They are first mentioned in The Lord of the Rings, originally published in 1954ÿ1955. The book calls the Nazg?l Sauron's \\"most terrible servants\\".\\r\\n\\r\\n\\r\\nAccording to Tolkien's The Lord of the Rings, the Nazg?l arose as Sauron's most powerful servants in the Second Age of Middle-earth. They were once mortal men; three being \\"great lords\\" of N~menor. Sauron gave each of them one of nine Rings of Power. Ultimately, however, they were bound to the One Ring and completely enslaved by the will of Sauron.\\r\\nThose who used the Nine Rings became mighty in their day, kings, sorcerers, and warriors of old. They obtained glory and great wealth, yet it turned to their undoing. They had, as it seemed, unending life, yet life became unendurable to them. They could walk, if they would, unseen by all eyes in this world beneath the sun, and they could see things in worlds invisible to mortal men; but too often they beheld only the phantoms and delusions of Sauron. And one by one, sooner or later, according to their native strength and to the good or evil of their wills in the beginning, they fell under the thraldom of the ring that they bore and of the domination of the One which was Sauron's. And they became forever invisible save to him that wore the Ruling Ring, and they entered into the realm of shadows. The Nazg?l were they, the Ringwraiths, the ~lairi, the Enemy's most terrible servants; darkness went with them, and they cried with the voices of death.  The Silmarillion, \\"Of the Rings of Power and the Third Age\\", 346\\r\\nThe corrupting effect of the rings extended the bearers' earthly lives far beyond their normal lifespans. Some passages in the novel suggest that the Nazg?l wore their rings, while others suggest that Sauron actually held them.\\r\\nIn a letter from circa 1963 Tolkien says explicitly that Sauron held the rings:\\r\\nThey would have obeyed . . . any minor command of his that did not interfere with their errand  laid upon them by Sauron, who still through their nine rings (which he held) had primary control of their wills . . .  The Letters of J. R. R. Tolkien, Letter 246\\r\\nThey were by far the most powerful of his servants, and the most suitable for such a mission, since they were entirely enslaved to their Nine Rings, which he now himself held . . .  Unfinished Tales, p. 338\\r\\nThe Nazg?l are called ~lairi (a plural) in Tolkien's invented language of Quenya.\\r\\nThey are also called the Fell Riders and the Black Wings (when they ride the fell beasts), as well as the Shadows, the Servants of Sauron, and the Nine Servants of the Lord of the Rings. The Orcs of the Tower of Cirith Ungol call them Shriekers.\\r\\nIn her duel with the Witch-king, owyn calls him a \\"dwimmerlaik\\". This is intended to be a word in Rohirric, the speech of Rohan, translated into Anglo-Saxon, that Tolkien glosses in the index as a \\"work of necromancy\\", a \\"spectre\\". It derives from Anglo-Saxon (ge)dwimor, \\"phantom, illusion\\" and -leikr, the Old Norse ending corresponding to Anglo-Saxon -lac, meaning \\"a state or act\\".[1]\\r\\nOnly two of the Nazg?l are named or identified individually in Tolkien's works. Their leader was the Witch-king of Angmar; he is the only individual Nazg?l identified in The Lord of the Rings. Writings unpublished in Tolkien's lifetime identify his second-in-command as Kham?l, the \\"Black Easterling\\" or the \\"Shadow of the East\\".[2] In the text of The Lord of the Rings, after the fall of the Lord of the Nazg?l, command of Mordor's army in the field falls to Gothmog, the \\"lieutenant of Morgul\\";[3] nothing else is said of Gothmog, but since Minas Morgul was closely associated with the Nazg?l, some have speculated that Gothmog was one of their number. (For example, the 1977 wargame War of the Ring depicts Gothmog as a Nazg?l.)\\r\\nTolkien stated that three of the Nazg?l were great N~men܇rean lords;[4] in his notes for translators, Tolkien speculates that the Witch-king was of N~men܇rean origin.[5]\\r\\nThe Nazg?l wore their rings long enough that their physical forms faded away until they had become entirely invisible to mortal eyes. Their black robes gave them visible form. During the assault on Minas Tirith, the leader of the Nine, the Witch-king of Angmar, cast back his hood to reveal a crown, but the head that wore it was invisible. While wearing the One Ring, Frodo perceived them as pale figures robed in white, with \\"haggard hands\\" and wearing crowns.\\r\\nIn The Fellowship of the Ring they were armed with steel swords, while the Witch-king wielded a Morgul blade that could turn its victim into a wraith. During the Battle of the Pelennor Fields, the Witch-king bore a \\"long pale sword\\", and later used a mace against owyn.\\r\\nThe Witch-king practiced black magic, and used it to break the gates of Minas Tirith. Tolkien said of the Nazg?l \\".?.?.?their chief weapon was terror. This was actually greater when they were unclad and invisible; and it was greater also when they were gathered together.\\"[6] They exuded an aura of fear:\\r\\nThe Nazg?l came again .?.?. like vultures that expect their fill of doomed men's flesh. Out of sight and shot they flew, and yet were ever present, and their deadly voices rent the air. More unbearable they became, not less, at each new cry. At length even the stout-hearted would fling themselves to the ground as the hidden menace passed over them, or they would stand, letting their weapons fall from nerveless hands while into their minds a blackness came, and they thought no more of war, but only of hiding and of crawling, and of death.  The Return of the King, p. 97\\r\\nClose or prolonged encounters with a Nazg?l caused unconsciousness, nightmares, and eventual death: an effect known as \\"the Black Breath\\". Aragorn used the herb athelas to treat victims of the Black Breath, including Frodo, Faramir, owyn, and Merry.\\r\\nThe Appendices of The Return of the King explain that the Nazg?l first appeared around S.A.?2251, some 700 years after the rings were forged, and were soon established as Sauron's principal servants. They were dispersed after the first overthrow of Sauron in S.A.?3441 at the hands of the Last Alliance of Elves and Men, but their survival was assured since the One Ring survived.\\r\\nThey re-emerged around T.A.?1300, when the Witch-king led Sauron's forces against the successor kingdoms of Arnor: Rhudaur, Cardolan and Arthedain. He effectively destroyed all the successor kingdoms, but was defeated in 1975 and returned to Mordor. There he gathered the other Nazg?l in preparation for the return of Sauron to that realm.\\r\\nIn 2000, the Nazg?l besieged Minas Ithil and, after two years, captured it and acquired its palantr for Sauron. The city thereafter became Minas Morgul, the stronghold of the Nazg?l. Sauron returned to Mordor in 2942 and declared himself openly in 2951. Two or three of the Nazg?l were sent to garrison Dol Guldur, his fortress in Mirkwood.\\r\\nBy 3017, near the beginning of the story told in The Lord of the Rings, Sauron had learned from Gollum that Bilbo Baggins of The Shire had the One Ring in his possession. Sauron entrusted its recovery to the Nazg?l. They reappeared \\"west of the River\\", riding black horses that were bred or trained in Mordor to endure their terror. They learned that the Ring had passed to Bilbo's heir, Frodo, and followed him and his companions to Bree. Aragorn arrived ahead of them and hid the Hobbits from their pursuers, but eventually five of the Nazg?l cornered Frodo and his company at Weathertop, where the Witch-king stabbed Frodo in the shoulder with the Morgul blade, breaking off a piece of it in the Hobbit's flesh. When all Nine were swept away by the waters of the river Bruinen, their horses were drowned, and the Ringwraiths were forced to return to Mordor to regroup.\\r\\nIn 3018 the nine companions of the Fellowship of the Ring left Rivendell as the \\"Nine Walkers\\", in opposition to the Nazg?l, the \\"Nine Riders\\". The latter reappeared mounted on hideous flying beasts (reminiscent of  and in part suggested by  pterodactyls).[7][8] They were then called Winged Nazg?l.\\r\\nDuring the Battle of the Pelennor Fields (portrayed in Return of the King), the Witch-king himself was slain by owyn and Merry: Merry's surreptitious stroke with an enchanted Barrow-blade drove the Witch-king to his knees, allowing owyn, the niece of Thoden, to drive her sword between his crown and mantle. Thus was the Witch-king destroyed by a woman and a Hobbit, fulfilling the prophecy that \\"not by the hand of man will he fall\\".[9] Both weapons that pierced him disintegrated, and both assailants were stricken with the Black Breath.\\r\\nThe remaining eight Ringwraiths attacked the Army of the West during the last battle at the Black Gate. When Frodo claimed the Ring for his own near the fires of Mount Doom, Sauron ordered the eight to fly to intercept him. They arrived too late, however: Gollum seized the Ring and fell into the Cracks of Doom, and the Nazg?l perished with its destruction.\\r\\nThe Nazg?l are featured in all adaptations of The Lord of the Rings on radio, film, and stage.\\r\\nIn Ralph Bakshi's 1978 animated film version of The Lord of the Rings, the Nazg?l hack and slash the Hobbits' beds at The Prancing Pony inn themselves. In the book, the assailants are not precisely identified, but Tolkien implies that the attack was carried out by agents of the Nazg?l, possibly including one Bill Ferny, rather than the Nazg?l themselves (though they were present in the town).[10] Another thing to note is that after the beds are destroyed, the Ringwraths remove their hoods, revealing hideous black masks and the armour they wear beneath their cloaks. They remain unhooded, but masked, throughout the remainder of the film.\\r\\nIn the Rankin-Bass adaptation of The Return of the King, the Nazg?l are robed skeletons with white hair.\\r\\nIn the 1981 BBC Radio serial of The Lord of the Rings, the Nazg?l can be heard chanting the Ring-inscription.[11]\\r\\nIn The Lord of the Rings movie trilogy (2001ÿ2003) by Peter Jackson, the Nazg?l are almost always concealed by cloaks; however, they appear as white ghostly corpses to Frodo when he is wearing the Ring. The Nazg?l attack the inn themselves. Their deafeningly loud shrieks and fell beasts are highlighted. During the siege of Minas Tirith, the Witch-king of Angmar wears a distinctive helmet over his hood resembling a mask and a crown, as opposed to the crown worn underneath his hood in the book. Their shrieks are distorted recordings of producer and screenwriter Fran Walsh's scream.[12]\\r\\nIn The Hunt for Gollum (2009) Aragorn fights a Ringwraith on the borders of Mirkwood. The Hunt for Gollum puts more emphasis on the Nazg?l's physical strength: Aragorn is shown physically struggling as he pushes his sword against that of the Nazg?l.\\r\\nIn The Hobbit movie trilogy (2012ÿ2014) by Peter Jackson, a prequel to the Lord of the Rings trilogy, the men who became the Nazg?l are said to have been buried and sealed within the High Fells of Rhudaur. In the first film, Radagast briefly encounters the Witch-king while investigating Dol Guldur, and gives the Nazg?l's sword to Gandalf to present at the White Council. In the second film, at Galadriel's behest, Gandalf heads to the High Fells and finds that all the Nazg?l have left the tomb. This confirms the Necromancer's identity as Sauron, as the Nazg?l appear alongside their master in the third film in spectral forms wearing Morgul armor and fight Elrond and Saruman before being driven away by Galadriel.\\r\\nThe Nazg?l are featured in Middle-earth: Shadow of Mordor and its sequel, Middle-earth: Shadow of War, which are based on Peter Jackson's films. In the latter, Isidur is revealed to be one of the Nazg?l before he was killed by the game's protagonist Talion who used his ring to prolong his life and became Isidur's replacement until his demise in the Return of the King.\\r\\nThe early Middle-earth Role Playing games (and material derived from them) name the eight other than Kham?l; Er-Murazor (the Witch-king, of N~men܇rean race), Dwar of Waw, Ji Ind?r Dawndeath, Akh?rahil (N~men܇rean), Hoarm?rath of Dr, Ad?naphel the Quiet (female N~men܇rean), Ren the Unclean and ?vatha the Horseman,[13] however, none of these names come from Tolkien's writings.\\r\\nIn the Lord of the Rings Trading Card Game, chiefly based on the Jackson films, the Nazg?l are called The Witch King, ~lair? Att?a (The Easterling), ~lair? Nelya, ~lair? Cant?a, ~lair? Lemenya, ~lair? Enqu?a, ~lair? Ots?a, ~lair? Told?a and ~lair? Nert?a. These are not new names: ~lair? is a reconstructed Quenya singular \\"Ringwraith\\" (the singular of Q. pl. Ulairi is not directly attested), and the second word is simply an ordinal number from second to ninth.\\r\\nFor the expansion to its real-time strategy game The Lord of the Rings: The Battle for Middle-earth II, The Rise of the Witch-king  chiefly based on the Jackson films and building much upon the original writings  Electronic Arts invented the name Morgomir for one of the Nazg?l. This appears to be a pastiche of the Sindarin words Morgoth (\\"Dark Enemy\\") and m?r (\\"jewel\\"); it is not attested in Tolkien's Elvish languages.\\r\\nIn The Lord of the Rings Strategy Battle Game, the Witch-king and Kham?l the Easterling retain the titles Tolkien gave them. The other seven are given honorific titles emphasising aspects of how they are used in the game: The Dark Marshal, The Shadow Lord, The Undying, The Tainted, The Betrayer, The Knight of Umbar and The Dwimmerlaik.\\r\\nIn The Heart of the Wild and The Darkening of Mirkwood, supplements for the tabletop role-playing game The One Ring, the three Nazg?l sent to Dol Guldur in T.A. 2951 are named in honorific form as \\"The Lieutenant of Dol Guldur\\", \\"The Ghost of the Forest\\", and \\"The Messenger of Mordor\\".[14][15]\\r\\nAfter losing their horses at the Ford of Bruinen, the Nazg?l returned to Mordor and reappeared mounted on hideous flying beasts; Beregond called them \\"Hell Hawks\\". Tolkien describes them as \\"fell beasts\\", though Tolkien applies the adjective fell (\\"fierce, cruel\\") to a variety of other creatures throughout The Lord of the Rings  even at one point to Gandalf. In a letter, he calls the winged mounts \\"Nazg?l-birds\\".[16] In the absence of a proper name, derivative works sometimes press \\"fellbeast\\" or \\"fell-beast\\" into service.[17]\\r\\nThe flying steeds figure prominently in the Battle of the Pelennor Fields, where the Witch-king of Angmar, the Lord of the Nazg?l, rides one against King Thoden of Rohan. Tolkien describes the Witch-king's mount thus:\\r\\n... it was a winged creature: if bird, then greater than all other birds, and it was naked, and neither quill nor feather did it bear, and its vast pinions were as webs of hide between horned fingers; and it stank. A creature of an older world maybe it was ...[18]\\r\\nA few paragraphs later, it is said to attack with \\"beak and claw\\".[18]\\r\\nTolkien once wrote that he \\"did not intend the steed of the Witch-king to be what is now called a 'pterodactyl'\\", while acknowledging that it was \\"obviously ... pterodactylic and owes much\\" to the \\"new ... mythology of the 'Prehistoric'\\", and might even be \\"a last survivor of older geological eras.\\"[19]\\r\\nIn Ralph Bakshi's 1978 animated version of The Lord of the Rings, one of the Nazg?l is shown riding these creatures. In the Rankin-Bass 1980 animated version of The Return of the King, the Nazg?l ride winged horses, although the Witch-king does ride a creature more in line with the book when he confronts owyn. In Peter Jackson's film trilogy based on The Lord of the Rings, all nine Nazg?l are shown onscreen riding them. Jackson's creatures explicitly differ from Tolkien's description in that they have teeth instead of beaks. The Nazg?l use them in battle more extensively than in the book. In the film the Witch-king's mount is largely responsible for the death of Thoden and his horse Snowmane, while in the book Snowmane is killed by a \\"black dart\\", crushing Thoden as he falls. As confirmed in the films' audio commentary, the design of the creatures was based largely on illustrations by artist John Howe.","input":"What are the dragons that the nazgul ride?"},{"output":"Haider al-Abadi","context":"The Prime Minister of Iraq is Iraq's head of government. The Prime Minister was originally an appointed office, subsidiary to the head of state, and the nominal leader of the Iraqi parliament. Under the newly adopted constitution the Prime Minister is to be the country's active executive authority. Nouri al-Maliki (formerly Jawad al-Maliki) was selected to be Prime Minister on 21 April 2006.[2][3] On 14 August 2014 al-Maliki agreed to step down as prime minister of Iraq to allow Haider al-Abadi to take his place.[4]\\r\\n\\r\\n\\r\\nThe Council of Representatives elected the President of the Republic and his Deputies, including the President of the Council of Ministers.\\r\\nThe Presidency Council then shall name a Prime Minister unanimously. According to this, The Presidency Council must agree on a candidate for the post within two weeks. In the event that it fails to do so, the responsibility of naming the Prime Minister reverts to the National Assembly. In that event, the Council of Representatives must confirm the nomination by an absolute majority. If the Prime Minister is unable to nominate his Council of Ministers within one month, the Presidency Council shall name another Prime Minister.\\r\\nThe Iraqi Counter Terrorism Bureau reports to the Prime Minister directly. The Iraqi CTB oversees the Iraqi Counter Terrorism Command, a formation that includes all Iraqi Special Operations Forces. As of 30 June 2009, there had been legislation in progress for a year to make the Iraqi CTB a separate ministry.[5]","input":"Who is the current prime minister of iraq?"},{"output":"auxiliary forces recruited from the indigenous Soviet populations in the areas acquired by Nazi Germany in Eastern Europe","context":"The term Hiwi ([?hi?vi?]) is a German abbreviation of the word Hilfswilliger, meaning \\"voluntary assistant\\", or more literally, \\"willing helper\\". During World War II, the term Hiwis gained broad popularity in reference to auxiliary forces recruited from the indigenous Soviet populations in the areas acquired by Nazi Germany in Eastern Europe. Hitler reluctantly agreed to allow recruitment of Soviet citizens in the Rear Areas during Operation Barbarossa.[1] In a short period of time, many of them were moved to combat units. In late 1942, Hiwis comprised 50 percent of the 2nd Panzer Army's 134 Infantry Division, while the 6th Army at the Battle of Stalingrad was composed of 25 percent Hiwis.[1] By 1944, their numbers had grown to 600,000. Both men and women of the Soviet Union were recruited. Veteran Hiwis were practically indistinguishable from the regular German troops, and often served in entire company strengths.[1][2]\\r\\n\\r\\nBetween September 1941 and July 1944 the SS employed thousands of collaborationist auxiliary police recruited as Hiwis directly from the Soviet POW camps. After training, they were deployed for service with Nazi Germany in the General Government and the occupied East.[3]\\r\\n\\r\\nIn one instance, the German SS and police inducted, processed, and trained 5,082 Hiwi guards before the end of 1944 at the SS training camp division of the Trawniki concentration camp set up in the village of Trawniki southeast of Lublin. They were known as the \\"Trawniki men\\" (German: Trawnikim?nner) and were former Soviet citizen, mostly Ukrainians. Trawnikis were sent to all major killing sites of the \\"Final Solution\\", which was their primary purpose of training. They took an active role in the executions of Jews at Be??ec, Sobib܇r, Treblinka II, Warsaw (three times), Cz?stochowa, Lublin, Lvov, Radom, Krak܇w, Bia?ystok (twice), Majdanek as well as Auschwitz, and Trawniki itself.[4][5][6]\\r\\n\\r\\nThe term 'Hiwis' acquired a thoroughly negative meaning during World War II when it entered into several other languages in reference to Ostlegionen as well as volunteers enlisted from occupied territories for service in a number of roles including hands-on shooting actions and guard duties at Extermination camps on top of regular military service, drivers, cooks, hospital attendants, ammunition carriers, messengers, sappers, etc.[1][2]\\r\\n\\r\\nIn the context of World War II the term has clear connotations of collaborationism, and in the case of the occupied Soviet territories also of anti-Bolshevism (widely presented as such by the Germans). \\r\\n\\r\\nGerman historian Werner R?hr?(de) wrote that there were many different reasons why Soviet citizens volunteered.[7] He argues that the issue has to be seen first and foremost with the German Vernichtungskrieg (war of annihilation) policy in mind. For example, volunteering allowed Soviet POWs to get out of the barbaric German POW camp system, giving them a much higher chance of survival. During World War II, Nazi Germany engaged in a policy of deliberate maltreatment of Soviet POWs, in contrast to their treatment of British and American POWs. This resulted in some 3.3 to 3.5 million deaths, or 57% of all Soviet POWs.[8][9][10][11] Therefore it becomes very difficult to differentiate between a genuine desire to volunteer, and seeming to volunteer in the hope of a better chance of surviving the war.\\r\\n\\r\\nA captured Hiwi told his People's Commissariat for Internal Affairs (Narodnyy Komissariat Vnutrennikh Del, or NKVD) interrogators:\\r\\n\\r\\nRussians in the German Army can be divided into three categories. Firstly, soldiers mobilized by German troops, so-called Cossack sections, which are attached to German divisions. Secondly, Hilfswillige [Voluntary Assistants] made up of local people or Russian prisoners who volunteer, or those Red Army soldiers who desert to join the Germans. This category wears full German uniform, with their own ranks and badges. They eat like German soldiers and they are attached to German regiments. Thirdly, there are Russian prisoners who do the dirty jobs, kitchens, stables and so on. These three categories are treated in different ways, with the best treatment naturally reserved for the volunteers.[12]\\r\\n\\r\\nSoviet authorities referred to the Hiwis as \\"former Russians\\" regardless of the circumstances of their joining or their fate at the hands of the NKVD secret police.[13] After the war, thousands attempted to return to their homes in the USSR. Hundreds were captured and prosecuted, charged with treason and therefore guilty of enlistment from the start of judicial proceedings.[4] Most were sentenced to the Gulags, and released under the Khrushchev amnesty of 1955.[14]\\r\\n\\r\\nThe reliance upon Hiwis exposed a gap between Nazi ideologues and pragmatic German Army commanders.  Nazi leaders including Adolf Hitler regarded all Slavs as Untermenschen and therefore of limited value as volunteers also. On the other hand, the manpower was needed,[15] and German Intelligence had recognised the need to divide the Soviet nationals. The contradiction was sometimes disguised by reclassification of Slavs as Cossacks.[16] Colonel Helmuth Groscurth (Chief of Staff, XI Corps) wrote to General Beck: \\"It is disturbing that we are forced to strengthen our fighting troops with Russian prisoners of war, who are already being turned into gunners. It's an odd state of affairs that the \\"Beasts\\" we have been fighting against are now living with us in closest harmony.\\"[17] The Hiwis may have constituted one quarter of 6th Army's front-line strength, amounting to over 50,000 Slavic auxiliaries serving with the German troops.[17]","input":"What were the hiwis in world war ii?"},{"output":"Hiram Rhodes Revels","context":"The first African Americans to serve in the United States Congress were Republicans elected during the Reconstruction Era. After slaves were emancipated and granted citizenship rights, freedmen gained political representation in the Southern United States for the first time. White Democrats regained political power in state legislatures across the South and worked to restore white supremacy. By the presidential election of 1876, only three state legislatures were not controlled by white Democrats. The Compromise of 1877 completed the period of Redemption by white Democratic Southerners, with the withdrawal of federal troops from the South. State legislatures began to pass Jim Crow laws to establish racial segregation and restrict labor rights, movement and organizing by blacks. They passed some laws to restrict voter registration, aimed at suppressing the black vote.\\r\\nFrom 1890-1908, Democratic state legislatures in the South essentially disfranchised most blacks and many poor whites from voting by passing new constitutions or amendments, or other laws related to more restrictive electoral and voter registration and electoral rules. The Democratic Party essentially dominated the \\"Solid South\\" until the 1990s. As a result of the Civil Rights Movement, the U.S. Congress passed laws in the mid-1960s to end segregation and enforce constitutional civil rights and voting rights.\\r\\nDuring two waves of massive migration within the United States in the first half of the 20th century, more than 6 million African Americans moved from the South to Northeastern, Midwestern and Western industrial cities, with 5 million migrating from 1940 to 1970. Some were elected to national political office from their new locations. During the Great Depression, many black voters switched allegiances from the Republican Party to the Democratic Party, in support of the New Deal economic, social network, and work policies of Franklin D. Roosevelt's administration. This trend continued in the 1960s when the national Democratic Party supported the civil rights legislation to enforce constitutional rights. At the same time, there was a different movement among whites in the South, who began to vote for Republican candidates for national and then state offices.\\r\\nA total of 139 African Americans have served in the United States Congress, mostly in the United States House of Representatives. This includes five non-voting members of the House of Representatives who have represented the District of Columbia and the U.S. Virgin Islands. An additional House candidate, John Willis Menard, was elected in 1868 but never seated due to an election dispute.\\r\\nTen African Americans have served in the U.S. Senate, four in the Republican Party. Two African Americans served as Senators from Mississippi during the Reconstruction Era and one from Massachusetts during the 1960s and 1970s. The remaining seven served more recently: six Democrats, three from Illinois (including Barack Obama) and one each from Massachusetts, New Jersey and California; and one Republican from South Carolina.\\r\\n\\r\\n\\r\\nThe right of blacks to vote and to serve in the United States Congress was established after the Civil War by amendments to the Constitution. The Thirteenth Amendment (ratified December 6, 1865), abolished slavery. The Fourteenth Amendment (ratified July 9, 1868) made all people born or naturalized in the United States citizens. The Fifteenth Amendment (ratified February 3, 1870) forbade the denial or abridgment of the right to vote on account of race, color, or previous condition of servitude, and gave Congress the power to enforce the law by appropriate legislation.\\r\\nIn 1866, Congress passed the Civil Rights Act and the four Reconstruction Acts, which dissolved all governments in the former Confederate states with the exception of Tennessee. It divided the South into five military districts, where the military through the Freedmen's Bureau helped protect the rights and safety of newly freed blacks. The act required that the former Confederate states ratify their constitutions conferring citizenship rights on blacks or forfeit their representation in Congress.\\r\\nAs a result of these measures, blacks acquired the right to vote across the Southern states. In several states (notably Mississippi and South Carolina), blacks were the majority of the population. By forming coalitions with pro-Union whites, Republicans took control of the state legislatures. At the time, state legislatures elected the members of the US Senate. During Reconstruction, only the state legislature of Mississippi elected any black senators. On February 25, 1870, Hiram Rhodes Revels was seated as the first black member of the Senate, while Blanche Bruce, also of Mississippi, seated in 1875, was the second. Revels was the first black member of the Congress overall.[1]\\r\\nBlacks were a majority of the population in many congressional districts across the South. In 1870, Joseph Rainey of South Carolina was elected to the U.S. House of Representatives, becoming the first directly elected black member of Congress to be seated.[2] Blacks were elected to national office also from Alabama, Florida, Georgia, Louisiana, Mississippi, North Carolina and Virginia.\\r\\nAll of these Reconstruction era black senators and representatives were members of the Republican Party. The Republicans represented the party of Abraham Lincoln and of emancipation. The Southern Democrats represented the party of planters, slavery and secession.\\r\\nFrom 1868, southern elections were accompanied by increasing violence, especially in Louisiana, Mississippi and the Carolinas, in an effort by Democrats to suppress black voting and regain power. In the mid-1870s, paramilitary groups such as the White League and Red Shirts worked openly to turn Republicans out of office and intimidate blacks from voting. This followed the earlier years of secret vigilante action by the Ku Klux Klan against freedmen and allied whites.\\r\\nAfter the disputed Presidential election of 1876 between Democratic Samuel J. Tilden, governor of New York, and Republican Rutherford B. Hayes, governor of Ohio, a national agreement between Democratic and Republican factions was negotiated, resulting in the Compromise of 1877. Under the compromise, Democrats conceded the election to Hayes and promised to acknowledge the political rights of blacks; Republicans agreed to withdraw federal troops from the South and promised to appropriate a portion of federal monies toward Southern projects.\\r\\nWith the Southern states \\"redeemed\\", Democrats gradually regained control of Southern legislatures. They proceeded to restrict the rights of the majority of blacks and many poor whites to vote by imposing new requirements for poll taxes, subjective literacy tests, more strict residency requirements and other elements difficult for laborers to satisfy.\\r\\nBy the 1880s, legislators increased restrictions on black voters through voter registration and election rules. In 1888 John Mercer Langston, president of Virginia State University at Petersburg, was elected to the US Congress as the first African American from Virginia (and the last for nearly a century after the state passed a disenfranchising constitution at the turn of the century, excluding blacks from politics for decades.)[3]\\r\\nStarting with the Florida Constitution of 1885, white Democrats passed new constitutions in ten Southern states with provisions that restricted voter registration and forced hundreds of thousands of people from registration rolls. These changes effectively prevented most blacks and many poor whites from voting. Many whites who were also illiterate were exempted from such requirements as literacy tests by such strategies as the grandfather clause, basing eligibility on an ancestor's voting status as of 1866, for instance.\\r\\nSouthern state and local legislatures also passed Jim Crow laws that segregated transportation, public facilities and daily life. Finally, racial violence in the form of lynchings and race riots increased in frequency, reaching a peak in the last decade of the 19th century.\\r\\nThe last black congressman elected from the South in the 19th century was George Henry White of North Carolina, elected in 1896 and re-elected in 1898. His term expired in 1901, the same year that William McKinley, who was the last president to have fought in the Civil War, died. No blacks served in Congress for the next 28 years, and none represented any Southern state for the next 72 years.\\r\\nFrom 1910 to 1940, the Great Migration of blacks from the rural south to northern cities such as New York, Philadelphia, Chicago, Detroit, and Cleveland began to produce black-majority Congressional districts in the North. Blacks could exercise their right to vote. In the two waves of the Great Migration through 1970, more than six and a half million blacks moved north and west and became highly urbanized.\\r\\nIn 1928, Oscar De Priest won the 1st Congressional District of Illinois (the South Side of Chicago) as a Republican, becoming the first black Congressman of the modern era. DePriest was also the last black Republican elected to the House for 56 years.\\r\\nThe election of President Franklin D. Roosevelt in 1932 led to a shift of black voting loyalties from Republican to Democrat, as Roosevelt's New Deal programs offered economic relief to people suffering from the Great Depression. From 1940 to 1970, nearly five million blacks moved north and also west, especially to California, in the second wave of the Great Migration. By the 1960s, virtually all black voters were Democrats, and most were voting in states outside the former Confederacy.\\r\\nIt was not until after passage by Congress of the Voting Rights Act of 1965, the result of years of effort on the part of African Americans and allies in the Civil Rights Movement, that blacks within the Southern states recovered their ability to exercise their rights to vote and to live with full civil rights. Legal segregation ended. Accomplishing voter registration and redistricting to implement the sense of the law took more time.\\r\\nThe only Southern cities to have black majority districts were Atlanta, Houston, Memphis and New Orleans. By that time, the only Southern rural area to have a black majority district was the Mississippi Delta area in Mississippi.\\r\\nUntil 1992, most black House members were elected from inner-city districts in the North and West: New York City, Newark, New Jersey, Philadelphia, Baltimore, Chicago, Cleveland, Detroit, St. Louis and Los Angeles all elected at least one black member. Following the 1990 census, Congressional districts needed to be redrawn due to the population shifts of the country. Various federal court decisions resulted in states' creating districts to provide for some where the majority of the population were African Americans, rather than gerrymandering to exclude black majorities.[citation needed]\\r\\nHistorically, both parties have used gerrymandering to gain political advantage, by drawing districts to favor their own party. In this case, some districts were created to link widely separated black communities. As a result, several black Democratic members of the House were elected from new districts in Alabama, Florida, rural Georgia, rural Louisiana, North Carolina, South Carolina and Virginia for the first time since Reconstruction. Additional black-majority districts were also created in this way in California, Maryland and Texas, thus increasing the number of black-majority districts.[citation needed]\\r\\nThe creation of black-majority districts was a process supported by both parties. The Democrats saw it as a means of providing social justice, as well as connecting easily to black voters who had been voting Democratic for decades. The Republicans believed they gained by the change, as many of the Democratic voters were moved out of historically Republican-majority districts. By 2000, other demographic and cultural changes resulted in the Republicans' holding a majority of white-majority House districts.\\r\\nSince the 1940s, when decades of the Great Migration resulted in millions of African Americans having migrated from the South, no state has had a majority of African-American residents. Eight African Americans have served in the Senate since the 1940s: Edward W. Brooke, a Republican from Massachusetts; Carol Moseley Braun, Barack Obama, and Roland Burris (appointed to fill a vacancy), all Democrats from Illinois; Tim Scott (initially appointed to fill a vacancy, but later elected), a Republican from South Carolina; Mo Cowan (appointed to fill a vacancy), a Democrat from Massachusetts; Cory Booker, a Democrat from New Jersey, and Kamala Harris, a Democrat from California.\\r\\nIn the last several decades, numerous African Americans have created similar multi-racial coalitions to be elected as mayors of cities (including those without a black majority). (See List of first African-American mayors.)","input":"When was the first african american elected to congress?"},{"output":"wave packet","context":"In physics, the wavelength of a sinusoidal wave is the spatial period of the wavethe distance over which the wave's shape repeats,[1] and thus the inverse of the spatial frequency. It is usually determined by considering the distance between consecutive corresponding points of the same phase, such as crests, troughs, or zero crossings and is a characteristic of both traveling waves and standing waves, as well as other spatial wave patterns.[2][3] Wavelength is commonly designated by the Greek letter lambda (). The concept can also be applied to periodic waves of non-sinusoidal shape.[1][4] The term wavelength is also sometimes applied to modulated waves, and to the sinusoidal envelopes of modulated waves or waves formed by interference of several sinusoids.[5]\\r\\nAssuming a sinusoidal wave moving at a fixed wave speed, wavelength is inversely proportional to frequency of the wave: waves with higher frequencies have shorter wavelengths, and lower frequencies have longer wavelengths.[6]\\r\\nWavelength depends on the medium (for example, vacuum, air, or water) that a wave travels through.\\r\\nExamples of wave-like phenomena are sound waves, light, water waves and periodic electrical signals in a conductor. A sound wave is a variation in air pressure, while in light and other electromagnetic radiation the strength of the electric and the magnetic field vary. Water waves are variations in the height of a body of water. In a crystal lattice vibration, atomic positions vary.\\r\\nWavelength is a measure of the distance between repetitions of a shape feature such as peaks, valleys, or zero-crossings, not a measure of how far any given particle moves. For example, in sinusoidal waves over deep water a particle near the water's surface moves in a circle of the same diameter as the wave height, unrelated to wavelength.[7] The range of wavelengths or frequencies for wave phenomena is called a spectrum. The name originated with the visible light spectrum but now can be applied to the entire electromagnetic spectrum as well as to a sound spectrum or vibration spectrum.\\r\\n\\r\\n\\r\\nIn linear media, any wave pattern can be described in terms of the independent propagation of sinusoidal components. The wavelength  of a sinusoidal waveform traveling at constant speed v is given by[8]\\r\\nwhere v is called the phase speed (magnitude of the phase velocity) of the wave and f is the wave's frequency. In a dispersive medium, the phase speed itself depends upon the frequency of the wave, making the relationship between wavelength and frequency nonlinear.\\r\\nIn the case of electromagnetic radiationsuch as lightin free space, the phase speed is the speed of light, about 3G108?m/s. Thus the wavelength of a 100?MHz electromagnetic (radio) wave is about: 3G108?m/s divided by 108?Hz = 3 metres. The wavelength of visible light ranges from deep red, roughly 700 nm, to violet, roughly 400?nm (for other examples, see electromagnetic spectrum).\\r\\nFor sound waves in air, the speed of sound is 343?m/s (at room temperature and atmospheric pressure). The wavelengths of sound frequencies audible to the human ear (20?Hzÿ20?kHz) are thus between approximately 17?m and 17?mm, respectively. Note that the wavelengths in audible sound are much longer than those in visible light.\\r\\nA standing wave is an undulatory motion that stays in one place. A sinusoidal standing wave includes stationary points of no motion, called nodes, and the wavelength is twice the distance between nodes.\\r\\nThe upper figure shows three standing waves in a box. The walls of the box are considered to require the wave to have nodes at the walls of the box (an example of boundary conditions) determining which wavelengths are allowed. For example, for an electromagnetic wave, if the box has ideal metal walls, the condition for nodes at the walls results because the metal walls cannot support a tangential electric field, forcing the wave to have zero amplitude at the wall.\\r\\nThe stationary wave can be viewed as the sum of two traveling sinusoidal waves of oppositely directed velocities.[9] Consequently, wavelength, period, and wave velocity are related just as for a traveling wave. For example, the speed of light can be determined from observation of standing waves in a metal box containing an ideal vacuum.\\r\\nTraveling sinusoidal waves are often represented mathematically in terms of their velocity v (in the x direction), frequency f and wavelength  as:\\r\\nwhere y is the value of the wave at any position x and time t, and A is the amplitude of the wave. They are also commonly expressed in terms of wavenumber k (2 times the reciprocal of wavelength) and angular frequency  (2 times the frequency) as:\\r\\nin which wavelength and wavenumber are related to velocity and frequency as:\\r\\nor\\r\\nIn the second form given above, the phase (kx ? t) is often generalized to (k?r ? t), by replacing the wavenumber k with a wave vector that specifies the direction and wavenumber of a plane wave in 3-space, parameterized by position vector r. In that case, the wavenumber k, the magnitude of k, is still in the same relationship with wavelength as shown above, with v being interpreted as scalar speed in the direction of the wave vector. The first form, using reciprocal wavelength in the phase, does not generalize as easily to a wave in an arbitrary direction.\\r\\nGeneralizations to sinusoids of other phases, and to complex exponentials, are also common; see plane wave. The typical convention of using the cosine phase instead of the sine phase when describing a wave is based on the fact that the cosine is the real part of the complex exponential in the wave\\r\\nThe speed of a wave depends upon the medium in which it propagates. In particular, the speed of light in a medium is less than in vacuum, which means that the same frequency will correspond to a shorter wavelength in the medium than in vacuum, as shown in the figure at right.\\r\\nThis change in speed upon entering a medium causes refraction, or a change in direction of waves that encounter the interface between media at an angle.[10] For electromagnetic waves, this change in the angle of propagation is governed by Snell's law.\\r\\nThe wave velocity in one medium not only may differ from that in another, but the velocity typically varies with wavelength. As a result, the change in direction upon entering a different medium changes with the wavelength of the wave.\\r\\nFor electromagnetic waves the speed in a medium is governed by its refractive index according to\\r\\nwhere c is the speed of light in vacuum and n(0) is the refractive index of the medium at wavelength 0, where the latter is measured in vacuum rather than in the medium. The corresponding wavelength in the medium is\\r\\nWhen wavelengths of electromagnetic radiation are quoted, the wavelength in vacuum usually is intended unless the wavelength is specifically identified as the wavelength in some other medium. In acoustics, where a medium is essential for the waves to exist, the wavelength value is given for a specified medium.\\r\\nThe variation in speed of light with vacuum wavelength is known as dispersion, and is also responsible for the familiar phenomenon in which light is separated into component colors by a prism. Separation occurs when the refractive index inside the prism varies with wavelength, so different wavelengths propagate at different speeds inside the prism, causing them to refract at different angles. The mathematical relationship that describes how the speed of light within a medium varies with wavelength is known as a dispersion relation.\\r\\nWavelength can be a useful concept even if the wave is not periodic in space. For example, in an ocean wave approaching shore, shown in the figure, the incoming wave undulates with a varying local wavelength that depends in part on the depth of the sea floor compared to the wave height. The analysis of the wave can be based upon comparison of the local wavelength with the local water depth.[11]\\r\\nWaves that are sinusoidal in time but propagate through a medium whose properties vary with position (an inhomogeneous medium) may propagate at a velocity that varies with position, and as a result may not be sinusoidal in space. The figure at right shows an example. As the wave slows down, the wavelength gets shorter and the amplitude increases; after a place of maximum response, the short wavelength is associated with a high loss and the wave dies out.\\r\\nThe analysis of differential equations of such systems is often done approximately, using the WKB method (also known as the LiouvilleÿGreen method). The method integrates phase through space using a local wavenumber, which can be interpreted as indicating a \\"local wavelength\\" of the solution as a function of time and space.[12][13] This method treats the system locally as if it were uniform with the local properties; in particular, the local wave velocity associated with a frequency is the only thing needed to estimate the corresponding local wavenumber or wavelength. In addition, the method computes a slowly changing amplitude to satisfy other constraints of the equations or of the physical system, such as for conservation of energy in the wave.\\r\\nWaves in crystalline solids are not continuous, because they are composed of vibrations of discrete particles arranged in a regular lattice. This produces aliasing because the same vibration can be considered to have a variety of different wavelengths, as shown in the figure.[14] Descriptions using more than one of these wavelengths are redundant; it is conventional to choose the longest wavelength that fits the phenomenon. The range of wavelengths sufficient to provide a description of all possible waves in a crystalline medium corresponds to the wave vectors confined to the Brillouin zone.[15]\\r\\nThis indeterminacy in wavelength in solids is important in the analysis of wave phenomena such as energy bands and lattice vibrations. It is mathematically equivalent to the aliasing of a signal that is sampled at discrete intervals.\\r\\nThe concept of wavelength is most often applied to sinusoidal, or nearly sinusoidal, waves, because in a linear system the sinusoid is the unique shape that propagates with no shape change ÿ just a phase change and potentially an amplitude change.[16] The wavelength (or alternatively wavenumber or wave vector) is a characterization of the wave in space, that is functionally related to its frequency, as constrained by the physics of the system. Sinusoids are the simplest traveling wave solutions, and more complex solutions can be built up by superposition.\\r\\nIn the special case of dispersion-free and uniform media, waves other than sinusoids propagate with unchanging shape and constant velocity. In certain circumstances, waves of unchanging shape also can occur in nonlinear media; for example, the figure shows ocean waves in shallow water that have sharper crests and flatter troughs than those of a sinusoid, typical of a cnoidal wave,[17] a traveling wave so named because it is described by the Jacobi elliptic function of m-th order, usually denoted as cn(x; m).[18] Large-amplitude ocean waves with certain shapes can propagate unchanged, because of properties of the nonlinear surface-wave medium.[19]\\r\\nIf a traveling wave has a fixed shape that repeats in space or in time, it is a periodic wave.[20] Such waves are sometimes regarded as having a wavelength even though they are not sinusoidal.[21] As shown in the figure, wavelength is measured between consecutive corresponding points on the waveform.\\r\\nLocalized wave packets, \\"bursts\\" of wave action where each wave packet travels as a unit, find application in many fields of physics. A wave packet has an envelope that describes the overall amplitude of the wave; within the envelope, the distance between adjacent peaks or troughs is sometimes called a local wavelength.[22][23] An example is shown in the figure. In general, the envelope of the wave packet moves at a different speed than the constituent waves.[24]\\r\\nUsing Fourier analysis, wave packets can be analyzed into infinite sums (or integrals) of sinusoidal waves of different wavenumbers or wavelengths.[25]\\r\\nLouis de Broglie postulated that all particles with a specific value of momentum p have a wavelength  = h/p, where h is Planck's constant. This hypothesis was at the basis of quantum mechanics. Nowadays, this wavelength is called the de Broglie wavelength. For example, the electrons in a CRT display have a De Broglie wavelength of about 10?13 m. To prevent the wave function for such a particle being spread over all space, de Broglie proposed using wave packets to represent particles that are localized in space.[26] The spatial spread of the wave packet, and the spread of the wavenumbers of sinusoids that make up the packet, correspond to the uncertainties in the particle's position and momentum, the product of which is bounded by Heisenberg uncertainty principle.[25]\\r\\nWhen sinusoidal waveforms add, they may reinforce each other (constructive interference) or cancel each other (destructive interference) depending upon their relative phase. This phenomenon is used in the interferometer. A simple example is an experiment due to Young where light is passed through two slits.[27] As shown in the figure, light is passed through two slits and shines on a screen. The path of the light to a position on the screen is different for the two slits, and depends upon the angle Ĳ the path makes with the screen. If we suppose the screen is far enough from the slits (that is, s is large compared to the slit separation d) then the paths are nearly parallel, and the path difference is simply d sin Ĳ. Accordingly, the condition for constructive interference is:[28]\\r\\nwhere m is an integer, and for destructive interference is:\\r\\nThus, if the wavelength of the light is known, the slit separation can be determined from the interference pattern or fringes, and vice versa.\\r\\nFor multiple slits, the pattern is [29]\\r\\nwhere q is the number of slits, and g is the grating constant. The first factor, I1, is the single-slit result, which modulates the more rapidly varying second factor that depends upon the number of slits and their spacing. In the figure I1 has been set to unity, a very rough approximation.\\r\\nThe effect of interference is to redistribute the light, so the energy contained in the light is not altered, just where it shows up.[30]\\r\\nThe notion of path difference and constructive or destructive interference used above for the double-slit experiment applies as well to the display of a single slit of light intercepted on a screen. The main result of this interference is to spread out the light from the narrow slit into a broader image on the screen. This distribution of wave energy is called diffraction.\\r\\nTwo types of diffraction are distinguished, depending upon the separation between the source and the screen: Fraunhofer diffraction or far-field diffraction at large separations and Fresnel diffraction or near-field diffraction at close separations.\\r\\nIn the analysis of the single slit, the non-zero width of the slit is taken into account, and each point in the aperture is taken as the source of one contribution to the beam of light (Huygen's wavelets). On the screen, the light arriving from each position within the slit has a different path length, albeit possibly a very small difference. Consequently, interference occurs.\\r\\nIn the Fraunhofer diffraction pattern sufficiently far from a single slit, within a small-angle approximation, the intensity spread S is related to position x via a squared sinc function:[31]\\r\\nwhere L is the slit width, R is the distance of the pattern (on the screen) from the slit, and  is the wavelength of light used. The function S has zeros where u is a non-zero integer, where are at x values at a separation proportion to wavelength.\\r\\nDiffraction is the fundamental limitation on the resolving power of optical instruments, such as telescopes (including radiotelescopes) and microscopes.[32] For a circular aperture, the diffraction-limited image spot is known as an Airy disk; the distance x in the single-slit diffraction formula is replaced by radial distance r and the sine is replaced by 2J1, where J1 is a first order Bessel function.[33]\\r\\nThe resolvable spatial size of objects viewed through a microscope is limited according to the Rayleigh criterion, the radius to the first null of the Airy disk, to a size proportional to the wavelength of the light used, and depending on the numerical aperture:[34]\\r\\nwhere the numerical aperture is defined as \\r\\n\\r\\n\\r\\n\\r\\n\\r\\nN\\r\\nA\\r\\n\\r\\n=\\r\\nn\\r\\nsin\\r\\n?\\r\\nĲ\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\mathrm {NA} =n\\\\sin \\\\theta \\\\;}\\r\\n\\r\\n for Ĳ being the half-angle of the cone of rays accepted by the microscope objective.\\r\\nThe angular size of the central bright portion (radius to first null of the Airy disk) of the image diffracted by a circular aperture, a measure most commonly used for telescopes and cameras, is:[35]\\r\\nwhere  is the wavelength of the waves that are focused for imaging, D the entrance pupil diameter of the imaging system, in the same units, and the angular resolution Ѵ is in radians.\\r\\nAs with other diffraction patterns, the pattern scales in proportion to wavelength, so shorter wavelengths can lead to higher resolution.\\r\\nThe term subwavelength is used to describe an object having one or more dimensions smaller than the length of the wave with which the object interacts. For example, the term subwavelength-diameter optical fibre means an optical fibre whose diameter is less than the wavelength of light propagating through it.\\r\\nA subwavelength particle is a particle smaller than the wavelength of light with which it interacts (see Rayleigh scattering). Subwavelength apertures are holes smaller than the wavelength of light propagating through them. Such structures have applications in extraordinary optical transmission, and zero-mode waveguides, among other areas of photonics.\\r\\nSubwavelength may also refer to a phenomenon involving subwavelength objects; for example, subwavelength imaging.\\r\\nA quantity related to the wavelength is the angular wavelength (also known as reduced wavelength), usually symbolized by ? (lambda-bar). It is equal to the \\"regular\\" wavelength \\"reduced\\" by a factor of 2 (? = /2). It is usually encountered in quantum mechanics, where it is used in combination with the reduced Planck constant (symbol ?, h-bar) and the angular frequency (symbol ) or angular wavenumber (symbol k).","input":"What is the unit of wavelength of light?"},{"output":"April 30, 1798","context":"The United States Department of the Navy (DoN) was established by an Act of Congress on April 30, 1798 (initiated by the recommendation of James McHenry),[1] to provide a government organizational structure to the United States Navy, the United States Marine Corps (from 1834 onward) and, when directed by the President (or Congress during time of war), the United States Coast Guard, as a service within the Navy,[2] though each remain independent service branches. The Department of the Navy was an Executive Department and the Secretary of the Navy was a member of the President's cabinet until 1949, when amendments to the National Security Act of 1947 changed the name of the National Military Establishment to the Department of Defense and made it an Executive Department. The Department of the Navy then became, along with the Department of the Army and Department of the Air Force, a Military Department within the Department of Defense: subject to the authority, direction and control of the Secretary of Defense.\\r\\n\\r\\n\\r\\nThe Department of the Navy is headed by the Secretary of the Navy, also known as the SECNAV in naval jargon, who has the authority to conduct all of the affairs of the Department, subject to lawful authority, the Secretary of Defense, and the President. The Secretary of the Navy is appointed by the President with the advice and consent of the Senate.[3] The Secretary is assisted by an Under Secretary of the Navy, four Assistant Secretaries of the Navy and a General Counsel of the Department of the Navy, who are also appointed by the President with the advice and consent of the Senate.\\r\\nThe highest ranking military officers in the Department of the Navy are the Chief of Naval Operations and the Commandant of the Marine Corps, who are the principal military advisors to the Secretary of the Navy. They supervise their respective military services of the Department of the Navy, and in a separate capacity serve as members of the Joint Chiefs of Staff. They are assisted by a Vice Chief of Naval Operations and an Assistant Commandant of the Marine Corps.\\r\\nUnlike its U.S. Army and U.S. Air Force counterparts, the Department of the Navy comprises two uniformed services: the United States Navy and the United States Marine Corps (sometimes collectively called the \\"naval services\\" or \\"sea services\\").[4]\\r\\nThe Department of the Navy consists of all elements of the United States Navy and the United States Marine Corps. According to Navy Regulations Section 0204-2, the term \\"Navy Department\\" refers only to the executive offices at the seat of government.\\r\\nThe Department of the Navy is composed of the following:[5]\\r\\nA provision in the initial House of Representatives bill (H.R. 1585) for the fiscal year 2008 national defense authorization would have renamed the Department of the Navy as the \\"Department of the Navy and Marine Corps.\\" The bill passed in the House on 17 May 2007,[6] but encountered opposition among members of the DoD civilian leadership and among senior Navy admirals and Marine Corps generals.\\r\\nIn the Senate, the provision was replaced in S. Amdt. 2011, an amendment in the nature of a substitute proposed by Senator Carl Levin of Michigan on 9 July 2007 and agreed to by unanimous consent on 1 October 2007.[7] The amendment removed the renaming provision and also made other changes. The House version including the provision was withdrawn in conference committee and so was not included in the final National Defense Authorization Act for Fiscal Year 2008.","input":"When was the department of the navy established?"},{"output":"Mauna Kea","context":"\\r\\n\\r\\nMauna Kea (/?m??n? ?ke?.?/ or /?ma?n? ?ke?.?/, Hawaiian:?[?m?wn? ?k?j?]) is a dormant volcano on the island of Hawaii. Its peak is 4,207?m (13,802?ft) above sea level, making it the highest point in the state of Hawaii and the second-highest point above sea level of any island on Earth. Most of the mountain is under water; when measured from its oceanic base, Mauna Kea is over 10,000?m (33,000?ft) tall and is the tallest mountain on Earth, taller than Mount Everest in Nepal and China. Mauna Kea is about a million years old, and has thus passed the most active shield stage of life hundreds of thousands of years ago. In its current post-shield state, its lava is more viscous, resulting in a steeper profile. Late volcanism has also given it a much rougher appearance than its neighboring volcanoes; contributing factors include the construction of cinder cones, the decentralization of its rift zones, the glaciation on its peak, and the weathering effects of the prevailing trade winds. Mauna Kea last erupted 6,000 to 4,000 years ago and is now considered dormant.\\r\\n\\r\\nIn Hawaiian mythology, the peaks of the island of Hawaii are sacred. An ancient law allowed only high-ranking ali?i to visit its peak. Ancient Hawaiians living on the slopes of Mauna Kea relied on its extensive forests for food, and quarried the dense volcano-glacial basalts on its flanks for tool production. When Europeans arrived in the late 18th century, settlers introduced cattle, sheep and game animals, many of which became feral and began to damage the mountain's ecological balance. Mauna Kea can be ecologically divided into three sections: an alpine climate at its summit, a Sophora chrysophyllaÿMyoporum sandwicense (or mmaneÿnaio) forest on its flanks, and an Acacia koaÿMetrosideros polymorpha (or koaÿ?hi?a) forest, now mostly cleared by the former sugar industry, at its base. In recent years, concern over the vulnerability of the native species has led to court cases that have forced the Hawai'i Department of Land and Natural Resources to eradicate all feral species on the mountain.\\r\\n\\r\\nWith its high elevation, dry environment, and stable airflow, Mauna Kea's summit is one of the best sites in the world for astronomical observation. Since the creation of an access road in 1964, thirteen telescopes funded by eleven countries have been constructed at the summit. The Mauna Kea Observatories are used for scientific research across the electromagnetic spectrum and comprise the largest such facility in the world. Their construction on a landscape considered sacred by Native Hawaiians continues to be a topic of debate.\\r\\n\\r\\nMauna Kea is one of five volcanoes  that form the island of Hawaii, the largest and youngest island of the HawaiianÿEmperor seamount chain.[5] Of these five hotspot volcanoes, Mauna Kea is the fourth oldest and fourth most active.[3] It began as a preshield volcano driven by the Hawaii hotspot around one million years ago, and became exceptionally active during its shield stage until 500,000 years ago.[6] Mauna Kea entered its quieter post-shield stage 250,000 to 200,000?years ago,[7] and is currently dormant.[3] Mauna Kea does not have a visible summit caldera, but contains a number of small cinder and pumice cones near its summit. A former summit caldera may have been filled and buried by later summit eruption deposits.\\r\\n\\r\\nMauna Kea is over 32,000?km3 (7,680?cu?mi) in volume, so massive that it and its neighbor, Mauna Loa, depress the ocean crust beneath it by 6?km (4?mi).[8]\\r\\nThe volcano continues to slip and flatten under its own weight at a rate of less than 0.2?mm (0.01?in) per year. Much of its mass lies east of its present summit. Mauna Kea stands 4,205?m (13,800?ft) above sea level, just 35?m (110?ft) higher than its neighbor Mauna Loa,[3] and is the highest point in the state of Hawaii.[9] Measured from its base on the ocean floor, it rises over 10,000?m (33,000?ft), significantly greater than the elevation of Mount Everest above sea level.[10][11]\\r\\n\\r\\nLike all Hawaiian volcanoes, Mauna Kea has been created as the Pacific tectonic plate has moved over the Hawaiian hotspot in the Earth's underlying mantle.[12] The Hawaii island volcanoes are the most recent evidence of this process that, over 70 million years, has created the 6,000?km (3,700?mi)-long Hawaiian RidgeÿEmperor seamount chain.[5] The prevailing, though not completely settled, view is that the hotspot has been largely stationary within the planet's mantle for much, if not all of the Cenozoic Era.[5][13] However, while Hawaiian volcanism is well understood and extensively studied, there remains no definite explanation of the mechanism that causes the hotspot effect.[14]\\r\\n\\r\\nLava flows from Mauna Kea overlapped in complex layers with those of its neighbors during its growth. Most prominently, Mauna Kea is built upon older flows from Kohala to the northwest, and intersects the base of Mauna Loa to the south.[8] The original eruptive fissures (rift zones) in the flanks of Mauna Kea were buried by its post-shield volcanism.[7] Hilo Ridge, a prominent underwater rift zone structure east of Mauna Kea, was once believed to be a part of the volcano; however, it is now understood to be a rift zone of Kohala that has been affected by younger Mauna Kea flows.[12][15]\\r\\n\\r\\nThe shield-stage lavas that built the enormous main mass of the mountain are tholeiitic basalts, like those of Mauna Loa, created through the mixing of primary magma and subducted oceanic crust.[16] They are covered by the oldest exposed rock strata on Mauna Kea, the post-shield alkali basalts of the Hmkua Volcanics, which erupted between 250,000 and 70ÿ65,000 years ago. The most recent volcanic flows are hawaiites and mugearites: they are the post-shield Lauphoehoe Volcanics, erupted between 65,000 and 4,000 years ago.[12][17] These changes in lava composition accompanied the slow reduction of the supply of magma to the summit, which led to weaker eruptions that then gave way to isolated episodes associated with volcanic dormancy. The Lauphoehoe lavas are more viscous and contain more volatiles than the earlier tholeiitic basalts; their thicker flows significantly steepened Mauna Kea's flanks. In addition, explosive eruptions have built cinder cones near the summit.[3] These cones are the most recent eruptive centers of Mauna Kea. Its present summit is dominated by lava domes and cinder cones up to 1.5?km (0.9?mi) in diameter and hundreds of meters tall.[7]\\r\\n\\r\\nMauna Kea is the only Hawaiian volcano with distinct evidence of glaciation.[7] Similar deposits probably existed on Mauna Loa, but have been covered by later lava flows.[3] Despite Hawaii's tropical location, during several past ice ages a drop of only a degree in temperature allowed snow to remain at the mountain's summit through summer, triggering the formation of an ice cap.[18] There are three episodes of glaciation that have been recorded from the last 180,000 years: the Phakuloa (180ÿ130?ka), Wihu (80ÿ60?ka) and Mkanaka (40ÿ13?ka) series. These have extensively sculpted the summit, depositing moraines and a circular ring of till and gravel along the mountain's upper flanks.[12] Subglacial eruptions built cinder cones during the Mkanaka glaciation,[19] most of which were heavily gouged by glacial action. The most recent cones were built between 9000 and 4500 years ago, atop the glacial deposits,[18][20] although one study indicates that the last eruption may have been around 3600 years ago.[21]\\r\\n\\r\\nAt their maximum extent, the glaciers extended from the summit down to between 3,200 and 3,800?m (10,500 and 12,500?ft) of elevation.[22] A small body of permafrost, less than 25?m (80?ft) across, was found at the summit of Mauna Kea prior to 1974, and may still be present.[12] Small gullies etch the summit, formed by rain- and snow-fed streams that flow only during winter melt and rain showers.[23]\\r\\nOn the windward side of the mountain, stream erosion driven by trade winds has accelerated erosion in a manner similar to that on older Kohala.[24]\\r\\n\\r\\nMauna Kea is home to Lake Waiau, the highest lake in the Pacific Basin.[25] At an altitude of 3,969?m (13,022?ft), it lies within the Pu?u Waiau cinder cone and is the only alpine lake in Hawaii. The lake is very small and shallow, with a surface area of 0.73?ha (1.80 acres) and a depth of 3?m (10?ft). Radiocarbon dating of samples at the base of the lake indicates that it was clear of ice 12,600?years ago. Hawaiian lava types are typically permeable, preventing the formation of lakes due to infiltration. Here, either sulfur-bearing steam altered the volcanic ash to low-permeability clays, or explosive interactions between rising magma and groundwater or surface water (phreatic eruptions) formed exceptionally fine ash that also would reduce the permeability of the lake bed.[26]\\r\\n\\r\\nUntil 1993, artesian water was not known to be present in the Island of Hawaii. Drilling by the University of Hawaii at that time encountered an artesian groundwater aquifer at 300 meters below sea level and 100 meters of hole depth within a compacted layer of soil and lava where the flows of both Mauna Loa and Mauna Kea meet (Humuula saddle region). Isotopic composition shows the water present to have been derived from rain coming off Mauna Kea at an elevation higher than 2000 meters above mean sea level. Its presence is attributed to a freshwater head within Mauna Kea's basal lens. Scientists believe there may be more water in Mauna Kea's fresh water lens than current models may indicate.[27] In 2012 two more bore holes were drilled on Mauna Kea and water discovered at much higher elevations than previously believed but shallower than expected. Donald Thomas, director of the University of Hawaii's Center for the Study of Active Volcanoes believes one reason to continue study of the aquifers is due to use and 'occupancy' of the higher elevation areas, stating: \\"Nearly all of these activities depend on the availability of potable water that, in most cases, must be trucked to the Saddle from Waimea or Hilo  an inefficient and expensive process that consumes a substantial quantity of our scarce liquid fuels.\\"[28]\\r\\n\\r\\nThe last eruption of Mauna Kea was about 4,600?years ago (about 2600 BC);[12] because of this inactivity, Mauna Kea is assigned a United States Geological Survey hazard listing of 7 for its summit and 8 for its lower flanks, out of the lowest possible hazard rating of 9 (which is given to the extinct volcano Kohala). Twenty?percent of the volcano's summit has seen lava flows in the past 10,000?years, and its flanks have seen virtually no lava flows during that time.[29]\\r\\n\\r\\nDespite its dormancy, Mauna Kea is expected to erupt again, although there would be sufficient warning to evacuate. The telescopes on Mauna Kea's summit would be the first to detect the minute amounts of deformation resulting from the volcano's swelling, acting like expensive tiltmeters. Based on prior eruptions, such an event could occur anywhere on the volcano's upper flanks and would likely produce extended lava flows, mostly of a'a, of 15ÿ25?km (9ÿ16?mi) in length. Long periods of activity could build a cinder cone at the source. Although not likely in the next few centuries, such an eruption would probably result in little loss of life but significant damage to infrastructure.[30]\\r\\n\\r\\nThe first Ancient Hawaiians to arrive on Hawaii island lived along the shores, where food and water were plentiful. Settlement expanded inland to the Mauna Loa ÿ Mauna Kea region in the 12th and early 13th centuries. Archaeological evidence suggests that these regions were used for hunting, collecting stone material, and possibly for spiritual reasons or for astronomical or navigational observations.[31] The mountain's plentiful forest provided plants and animals for food and raw materials for shelter. Flightless birds that had previously known no predators became a staple food source.[32]\\r\\n\\r\\nEarly settlement of the Hawaiian islands led to major changes to local ecosystems and many extinctions, particularly amongst bird species. Ancient Hawaiians brought foreign plants and animals, and their arrival was associated with increased rates of erosion.[33]\\r\\nThe prevailing lowland forest ecosystem was transformed from forest to grassland; some of this change was caused by the use of fire, but the prevailing cause of forest ecosystem collapse and avian extinction on Hawaii appears to have been the introduction of the Polynesian (or Pacific) rat.[34]\\r\\n\\r\\nThe five volcanoes of Hawaii are revered as sacred mountains; and Mauna Kea's summit, the highest, is the most sacred.[35][36] For this reason, a kapu (ancient Hawaiian law) restricted visitor rights to high-ranking ali?i. Hawaiians associated elements of their natural environment with particular deities. In Hawaiian mythology, the summit of Mauna Kea was seen as the \\"region of the gods\\", a place where benevolent spirits reside. Poli?ahu, deity of snow, also resides there.[32] In Hawaiian, Mauna Kea is a shortened form of Mauna a Wakea which denotes the mountain's connection to the sky father Wakea;[37] however, the English translation of Mauna Kea is \\"white mountain\\" in reference to its seasonally snow-capped summit.[38]\\r\\n\\r\\nAround AD?1100, natives established adze quarries high up on Mauna Kea to extract the uniquely dense basalt (generated by the quick cooling of lava flows meeting glacial ice during subglacial eruptions) to make tools. Volcanic glass and gabbro were collected for blades and fishing gear, and mmane wood was preferred for the handles. At peak quarry activity after AD?1400, there were separate facilities for rough and fine cutting; shelters with food, water, and wood to sustain the workers; and workshops creating the finished product.[32]\\r\\n\\r\\nLake Waiau provided drinking water for the workers. Native chiefs would also dip the umbilical cords of newborn babies in its water, to give them the strength of the mountain. Use of the quarry declined between this period and contact with Americans and Europeans. As part of the ritual associated with quarrying, the workers erected shrines to their gods; these and other quarry artifacts remain at the sites, most of which lie within what is now the Mauna Kea Ice Age Reserve.[32]\\r\\n\\r\\nThis early era was followed by peace and cultural expansion between the 12th and late 18th century. Land was divided into regions designed for both the immediate needs of the populace and the long-term welfare of the environment. These ahupua?a generally took the form of long strips of land oriented from the mountain summits to the coast. Mauna Kea's summit was encompassed in the ahupua?a of Ka?ohe, with part of its eastern slope reaching into the nearby Humu?ula. Principal sources of nutrition for Hawaiians living on the slopes of the volcano came from the mmaneÿnaio forest of its upper slopes, which provided them with vegetation and bird life. Bird species hunted included the ?ua?u (Pterodroma sandwichensis), nn (Branta sandvicensis), and palila (Loxioides bailleui). The lower koaÿ?hi?a forest gave the natives wood for canoes and ornate bird feathers for decoration.[32]\\r\\n\\r\\nThere are three accounts of foreigners visiting Hawaii before the arrival of James Cook, in 1778.[39][40][41] However, the earliest Western depictions of the isle, including Mauna Kea, were created by explorers in the late 18th and early 19th centuries. Contact with Europe and America had major consequences for island residents. Native Hawaiians were devastated by introduced diseases; port cities including Hilo, Kealakekua, and Kailua grew with the establishment of trade; and the adze quarries on Mauna Kea were abandoned after the introduction of metal tools.[32]\\r\\n\\r\\nIn 1793, cattle were brought by George Vancouver as a tribute to King Kamehameha I. By the early 19th century, they had escaped confinement and roamed the island freely, greatly damaging its ecosystem. In 1809 John Palmer Parker arrived and befriended Kamehameha?I, who put him in charge of cattle management on the island. With an additional land grant in 1845, Parker established Parker Ranch on the northern slope of Mauna Kea, a large cattle ranch that is still in operation today.[32] Settlers to the island burned and cut down much of the native forest for the construction of sugarcane plantations and houses.[42]\\r\\n\\r\\nThe Saddle Road, named for its crossing of the saddle-shaped plateau between Mauna Kea and Mauna Loa, was completed in 1943, and eased travel to Mauna Kea considerably.[23]\\r\\n\\r\\nThe Pohakuloa Training Area on the plateau is the largest military training ground in Hawaii. The 108,863-acre (44,055?ha) base extends from the volcano's lower flanks to 2,070?m (6,790?ft) elevation, on state land leased to the US Army since 1956. There are 15?threatened and endangered plants, three endangered birds, and one endangered bat species in the area.[23]\\r\\n\\r\\nMauna Kea has been the site of extensive archaeological research since the 1980s. Approximately 27?percent of the Science Reserve had been surveyed by 2000, identifying 76?shrines, 4?adze manufacturing workshops, 3?other markers, 1?positively identified burial site, and 4?possible burial sites.[32] By 2009, the total number of identified sites had risen to 223, and archaeological research on the volcano's upper flanks is ongoing.[23] It has been suggested that the shrines, which are arranged around the volcano's summit along what may be an ancient snow line, are markers for the transition to the sacred part of Mauna Kea. Despite many references to burial around Mauna Kea in Hawaiian oral history, few sites have been confirmed. The lack of shrines or other artifacts on the many cinder cones dotting the volcano may be because they were reserved for burial.[32]\\r\\n\\r\\nIn pre-contact times, natives traveling up Mauna Kea were probably guided more by landscape than by existing trails, as no evidence of the latter has been found. It is possible that natural ridges and water sources were followed instead. Individuals likely took trips up Mauna Kea's slopes to visit family-maintained shrines near its summit, and traditions related to ascending the mountain exist to this day. However, very few natives actually reached the summit, because of the strict kapu placed on it.[32]\\r\\n\\r\\nIn the early 19th century, the earliest notable recorded ascents of Mauna Kea included the following:\\r\\n\\r\\nIn the late 19th and early 20th centuries trails were formed, often by the movement of game herds, that could be traveled on horseback.[32] However, vehicular access to the summit was practically impossible until the construction of a road in 1964, and it continues to be restricted.[44] Today, multiple trails to the summit exist, in various states of use.[23]\\r\\n\\r\\nHawaii's geographical isolation strongly influences its ecology. Remote islands like Hawaii have a large number of species that are found nowhere else (see Endemism in the Hawaiian Islands).[45] The remoteness resulted in evolutionary lines distinct from those elsewhere and isolated these endemic species from external biotic influence, and also makes them especially vulnerable to extinction and the effects of invasive species. In addition the ecosystems of Hawaii are under threat from human development including the clearing of land for agriculture; an estimated third of the island's endemic species have already been wiped out. Because of its elevation, Mauna Kea has the greatest diversity of biotic ecosystems anywhere in the Hawaiian archipelago. Ecosystems on the mountain form concentric rings along its slopes due to changes in temperature and precipitation with elevation.[42] These ecosystems can be roughly divided into three sections by elevation: alpineÿsubalpine, montane, and basal forest.[4]\\r\\n\\r\\nContact with Americans and Europeans in the early 19th century brought more settlers to the island, and had a lasting negative ecological effect. On lower slopes, vast tracts of koaÿ?hi?a forest were converted to farmland. Higher up, feral animals that escaped from ranches found refuge in, and damaged extensively, Mauna Kea's native mmaneÿnaio forest.[46] Non-native plants are the other serious threat; there are over 4,600 introduced species on the island, whereas the number of native species is estimated at just 1,000.[47]\\r\\n\\r\\nThe summit of Mauna Kea lies above the tree line, and consists of mostly lava rock and alpine tundra. An area of heavy snowfall, it is inhospitable to vegetation, and is known as the Hawaiian tropical high shrublands. Growth is restricted here by extremely cold temperatures, a short growing season, low rainfall, and snow during winter months. A lack of soil also retards root growth, makes it difficult to absorb nutrients from the ground, and gives the area a very low water retention capacity.[4]\\r\\n\\r\\nPlant species found at this elevation include Styphelia tameiameiae, Taraxacum officinale, Tetramolopium humile, Agrostis sandwicensis, Anthoxanthum odoratum, Trisetum glomeratum, Poa annua, Sonchus oleraceus, and Coprosma ernodiodes. One notable species is Mauna Kea silversword (Argyroxiphium sandwicense var. sandwicense), a highly endangered endemic plant species that thrives in Mauna Kea's high elevation cinder deserts. At one stage reduced to a population of just 50 plants,[48] Mauna Kea silversword was thought to be restricted to the alpine zone, but in fact has been driven there by pressure from livestock, and can grow at lower elevations as well.[4]\\r\\n\\r\\nThe Mauna Kea Ice Age Reserve on the southern summit flank of Mauna Kea was established in 1981. The reserve is a region of sparsely vegetated cinder deposits and lava rock, including areas of aeolian desert and Lake Waiau.[49] This ecosystem is a likely haven for the threatened ?ua?u (Pterodroma sandwichensis) and also the center of a study on wkiu bugs (Nysius wekiuicola).[24]\\r\\n\\r\\nWkiu bugs feed on dead insect carcasses that drift up Mauna Kea on the wind and settle on snow banks. This is a highly unusual food source for a species in the genus Nysius, which consists of predominantly seed-eating insects. They can survive at extreme elevations of up to 4,200?m (13,780?ft)[50] because of natural antifreeze in their blood. They also stay under heated surfaces most of the time. Their conservation status is unclear, but the species is no longer a candidate for the Endangered Species List; studies on the welfare of the species began in 1980. The closely related Nysius aa lives on Mauna Loa. Wolf spiders (Lycosidae) and forest tent caterpillar moths have also been observed in the same Mauna Kea ecosystem; the former survive by hiding under heat-absorbing rocks, and the latter through cold-resistant chemicals in their bodies.[51]\\r\\n\\r\\nThe highest forested zone on the volcano, at an elevation of 2,000ÿ3,000?m (6,600ÿ9,800?ft), is dominated by mmane (Sophora chrysophylla) and naio (Myoporum sandwicense), both endemic tree species, and is thus known as mmaneÿnaio forest. Mmane seeds and naio fruit are the chief foods of the birds in this zone, especially the palila (Loxioides bailleui). The palila was formerly found on the slopes of Mauna Kea, Mauna Loa, and Huallai, but is now confined to the slopes of Mauna Keaonly 10% of its former rangeand has been declared critically endangered.[42]\\r\\n\\r\\nThe largest threat to the ecosystem is grazing by feral sheep (Ovis aries), cattle (Bos primigenius),[52] and goats (Capra hircus) introduced to the island in the late 18th century. Feral animal competition with commercial grazing was severe enough that a program to eradicate them existed as far back as the late 1920s,[42] and continued through to 1949. One of the results of this grazing was the increased prevalence of herbaceous and woody plants, both endemic and introduced, that were resistant to browsing.[52] The feral animals were almost eradicated, and numbered a few hundred in the 1950s. However, an influx of local hunters led to the feral species being valued as game animals, and in 1959 the Hawaii Department of Land and Natural Resources, the governing body in charge of conservation and land use management, changed its policy to a sustained-control program designed to facilitate the sport.[42]\\r\\n\\r\\nMouflon (Ovis aries orientalis) was introduced from 1962ÿ1964,[53] and a plan to release axis deer (Axis axis) in 1964 was prevented only by protests from the ranching industry, who said that they would damage crops and spread disease. The hunting industry fought back, and the back-and-forth between the ranchers and hunters eventually gave way to a rise in public environmental concern. With the development of astronomical facilities on Mauna Kea commencing, conservationists demanded protection of Mauna Kea's ecosystem. A plan was proposed to fence 25% of the forests for protection, and manage the remaining 75% for game hunting. Despite opposition from conservationists the plan was put into action. While the land was partitioned no money was allocated for the building of the fence. In the midst of this wrangling the Endangered Species Act was passed; the National Audubon Society and Sierra Club Legal Defense Fund filed a lawsuit against the Hawaii Department of Land and Natural Resources, claiming that they were violating federal law, in the landmark case Palila v. Hawaii Department of Land and Natural Resources (1978).[42][54]\\r\\n\\r\\nThe court ruled in favor of conservationists and upheld the precedence of federal laws before state control of wildlife. Having violated the Endangered Species Act, Hawaii state was required to remove all feral animals from the mountainside.[42] This decision was followed by a second court order in 1981. A public hunting program removed many of the feral animals,[46] at least temporarily. An active control program is in place,[23] though it is not conducted with sufficient rigor to allow significant recovery of the mmane-naio ecosystem.[55] There are many other species and ecosystems on the island, and on Mauna Kea, that remain threatened by human development and invasive species.[42]\\r\\n\\r\\nThe Mauna Kea Forest Reserve protects 52,500 acres (212?km2) of mmane-naio forest under the jurisdiction of the Hawaii Department of Land and Natural Resources. Ungulate hunting is allowed year-round.[23] A small part of the mmaneÿnaio forest is encompassed by the Mauna Kea State Recreation Area.[56]\\r\\n\\r\\nA band of ranch land on Mauna Kea's lower slopes was formerly Acacia koa ÿ Metrosideros polymorpha (koa-?hi?a) forest.[4] Its destruction was driven by an influx of European and American settlers in the early 19th century, as extensive logging during the 1830s provided lumber for new homes. Vast swathes of the forest were burned and cleared for sugarcane plantations. Most of the houses on the island were built of koa, and those parts of the forest that survived became a source for firewood to power boilers on the sugarcane plantations and to heat homes. The once vast forest had almost disappeared by 1880, and by 1900, logging interests had shifted to Kona and the island of Maui.[46] With the collapse of the sugar industry in the 1990s, much of this land lies fallow but portions are used for cattle grazing, small-scale farming and the cultivation of eucalyptus for wood pulp.[23][57]\\r\\n\\r\\nThe Hakalau Forest National Wildlife Refuge is a major koa forest reserve on Mauna Kea's windward slope. It was established in 1985, covering 32,733 acres (13,247?ha) of ecosystem remnant. Eight endangered bird species, twelve endangered plants, and the endangered Hawaiian hoary bat (Lasiurus cinereus semotus) have been observed in the area, in addition to many other rare biota. The reserve has been the site of an extensive replanting campaign since 1989.[58] Parts of the reserve show the effect of agriculture on the native ecosystem,[46] as much of the land in the upper part of the reserve is abandoned farmland.[58]\\r\\n\\r\\nBird species native to the acacia koaÿ?hi?a forest include the Hawaiian crow (Corvus hawaiiensis), the ?akepa (Loxops coccineus), Hawaii creeper (Oreomystis mana), ?akiapl?au (Hemignathus munroi), and Hawaiian hawk (Buteo solitarius), all of which are endangered, threatened, or near threatened; the Hawaiian crow in particular is extinct in the wild, but there are plans to reintroduce the species into the Hakalau reserve.[58]\\r\\n\\r\\nMauna Kea's summit is one of the best sites in the world for astronomical observation due to favorable observing conditions.[23][59][60] The arid conditions are important for submillimeter and infrared astronomy for this region of the electromagnetic spectrum. The summit is above the inversion layer, keeping most cloud cover below the summit and ensuring the air on the summit is dry, and free of atmospheric pollution. The summit atmosphere is exceptionally stable, lacking turbulence for some of the world's best astronomical seeing. The very dark skies resulting from Mauna Kea's distance from city lights are preserved by legislation that minimizes light pollution from the surrounding area; the darkness level allows the observation of faint astronomical objects.[59] These factors historically made Mauna Kea an excellent spot for stargazing.[44]\\r\\n\\r\\nIn the early 1960s, the Hawaii Island Chamber of Commerce encouraged astronomical development of Mauna Kea, as economic stimulus; this coincided with University of Arizona astronomer Gerard Kuiper's search for sites to use newly improved detectors of infrared light. Site testing by Kuiper's assistant Alika Herring in 1964 confirmed the summit's outstanding suitability. An intense three-way competition for NASA funds to construct a large telescope began between Kuiper, Harvard University, and the University of Hawaii (UH), which only had experience in solar astronomy. This culminated in funds being awarded to the \\"upstart\\" UH proposal.[61] UH rebuilt its small astronomy department into a new Institute for Astronomy, and in 1968 the Hawaii Department of Land and Natural Resources gave it a 65-year lease for all land within a 4?km (2.5?mi) radius of its telescope, essentially that above 11,500?ft (3,505?m).[23][59] On its completion in 1970, the UH 88?in (2.2?m) was the seventh largest optical/infrared telescope in the world.[61]\\r\\n\\r\\nBy 1970, two 24?in (0.6?m) telescopes had been constructed by the US Air Force and Lowell Observatory. In 1973, Canada and France agreed to build the 3.6?m CFHT on Mauna Kea.[61] However, local organisations started to raise concerns about the environmental impact of the observatory. This led the Department of Land and Natural Resources to prepare an initial management plan, drafted in 1977 and supplemented in 1980. In January 1982, the UH Board of Regents approved a plan to support the continued development of scientific facilities at the site.[23]  In 1998, 2,033 acres (823?ha) were transferred from the observatory lease to supplement the Mauna Kea Ice Age Reserve. The 1982 plan was replaced in 2000 by an extension designed to serve until 2020: it instituted an Office of Mauna Kea Management,[60] designated 525 acres (212?ha) for astronomy, and shifted the remaining 10,763 acres (4,356?ha) to \\"natural and cultural preservation\\". This plan was further revised to address concern expressed in the Hawaiian community that a lack of respect was being shown toward the cultural values of the mountain.[23]\\r\\n\\r\\nToday the Mauna Kea Science Reserve has 13 observation facilities, each funded by as many as 11 countries.[62] There are nine telescopes working in the visible and infrared spectrum, three in the submillimeter spectrum, and one in the radio spectrum, with mirrors or dishes ranging from 0.9 to 25?m (3 to 82?ft).[63] In comparison, the Hubble Space Telescope has a 2.4?m (7.9?ft) mirror, similar in size to the UH88, now the second smallest telescope on the mountain.[63]\\r\\n\\r\\nA \\"Save Mauna Kea\\" movement, believes development of the mountain to be sacrilegious.[64] Native Hawaiian non-profit groups such as Kahea, concerned with cultural heritage and the environment also oppose development for cultural and religious reasons.[65] The multi-telescope \\"outrigger\\", proposed in 2006 was eventually canceled.[66] A planned new telescope, the Thirty Meter Telescope (TMT), has attracted controversy and protests.[67][68] The TMT was approved in April 2013.[69] In October 2014, the groundbreaking ceremony for the telescope was interrupted by protesters causing the project to temporarily halt.[70] In late March 2015, demonstrators blocked access of the road to the summit again.[71] On April 2, 2015, 300 protestors were gathered near the visitor's center when 12 people were arrested with 11 more arrested at the summit.[72][73] Among the concerns of the protest groups are the land appraisals and Native Hawaiians consultation.[72] Construction was halted on April 7, 2015 after protests expanded over the state.[74] After several halts, the project has been voluntarily postponed. Governor Ige announced substantial changes to the management of Mauna Kea in the future but stated the project can move forward.[75] A pending court case has been accepted by the Supreme Court of Hawaii over the project by a number of groups.[76]\\r\\n\\r\\nThe summit of Mauna Kea has an alpine climate. Due to the influence of its tropical latitude, temperature swings are very low in spite of its high elevation. Frosts are common year round, but in spite of the elevation no month is nearly below freezing in terms of average high, although March almost falls below 32?F (0?C) in average means.\\r\\n\\r\\nFrom December to February, the ground above 12,000?ft (3,700?m) is often covered by snow, ranging from a few inches deep to several feet deep. Snow may fall from 11,000?ft (3,400?m) to 12,000?ft (3,700?m) and higher in any month, but occurs most often between October and April to early May.[77]\\r\\n\\r\\nMauna Kea's coastline is dominated by the Hamakua Coast, an area of rugged terrain created by frequent slumps and landslides on the volcano's flank.[79] The area includes several recreation parks including Kalopa State Recreation Area, Wailuku River State Park and Akaka Falls State Park.[80]\\r\\n\\r\\nThere are over 3,000 registered hunters on Hawaii island, and hunting, for both recreation and sustenance, is a common activity on Mauna Kea. A public hunting program is used to control the numbers of introduced animals including pigs, sheep, goats, turkey, pheasants, and quail.[23][42] The Mauna Kea State Recreation Area functions as a base camp for the sport.[44] Birdwatching is also common at lower levels on the mountain.[59] A popular site is Kؐpuka Pu'u Huluhulu, a kؐpuka on Mauna Kea's flank that formed when lava flows isolated the forest on a hill.[81]\\r\\n\\r\\nMauna Kea's great elevation and the steepness of its flanks provide a better view and a shorter hike than the adjacent Mauna Loa. The high elevation with its risk of altitude sickness, weather concerns, steep road grade, and overall inaccessibility make the volcano dangerous and summit trips difficult. Until the construction of roads in the mid-20th century, only the hardy visited Mauna Kea's upper slopes; hunters tracked game animals, and hikers traveled up the mountain. These travelers used stone cabins constructed by the Civilian Conservation Corps in the 1930s as base camps, and it is from these facilities that the modern mid-level Onizuka Center for International Astronomy telescope support complex is derived. The first Mauna Kea summit road was built in 1964, making the peak itself accessible to larger numbers of people.[44]\\r\\n\\r\\nToday, multiple hiking trails exist, including the Mauna Kea Trail, and by 2007 over 100,000 tourists and 32,000 vehicles were going each year to the Visitor Information Station (VIS) adjacent to the Onizuka Center for International Astronomy. The Mauna Kea Access Road is paved up to the Center at 2,804?m (9,199?ft).[23] One study reported that around a third of visitors and two thirds of professional astronomers working on the mountain have experienced symptoms of acute altitude sickness;[82] visitors traveling up the volcano's flanks are advised to stop for at least half an hour and preferably longer at the visitor center to acclimate to the higher elevation. It is strongly recommended to use a four-wheel drive vehicle to drive all the way to the top. Brakes often overheat on the way down and there is no fuel available on Mauna Kea.[83] A free Star Gazing Program is held at the VIS every night from 6-10 pm.[84] Between 5,000 and 6,000 people visit the summit of Mauna Kea each year, and to help ensure safety, and protect the integrity of the mountain, a ranger program was implemented in 2001.[23]","input":"Is mauna loa the tallest mountain in the world?"},{"output":"Washington, D.C.","context":"\\r\\n\\r\\n\\r\\n\\r\\nThe Texas Rangers are an American professional baseball team based in Arlington, Texas, located in the DallasÿFort Worth metroplex. The Rangers franchise currently competes in Major League Baseball as a member of the American League (AL) West division. Since 1994, the Rangers have played in Globe Life Park in Arlington in Arlington, Texas. The team's name is borrowed from the famous law enforcement agency of the same name.\\r\\n\\r\\nThe franchise was established in 1961 as the Washington Senators, an expansion team awarded to Washington, D.C., after the city's first AL ballclub, the second Washington Senators, moved to Minnesota and became the Twins (the original Washington Senators played primarily in the National League during the 1890s).  After the 1971 season, the new Senators moved to Arlington, Texas, and debuted as the Rangers the following spring.\\r\\n\\r\\nThe Texas Rangers Baseball Club has made eight appearances in the MLB postseason, seven following division championships in 1996, 1998, 1999, 2010, 2011, 2015, and 2016 and as a wild card team in 2012. In 2010, the Rangers advanced past the Division Series for the first time, defeating the Tampa Bay Rays. Texas then brought home their first American League pennant after beating the New York Yankees in six games. In the 2010 World Series, the franchise's first, the Rangers fell to the San Francisco Giants in five games. They repeated as American League champions the following year, then lost the 2011 World Series to the St. Louis Cardinals in seven games.\\r\\n\\r\\nWhen the second Washington Senators moved to Minnesota in 1960 as the Twins, Major League Baseball decided to expand a year earlier than planned to stave off the twin threats of competition from the proposed Continental League and loss of its exemption from the Sherman Antitrust Act. This new team adopted the old Senators name, but was (and still is) considered an expansion team since the Twins retained the old Senators' records and history.  The Senators and their expansion cousins, the Los Angeles Angels began to fill their rosters with American League players in an expansion draft. The team played the 1961 season at old Griffith Stadium before moving to the new District of Columbia Stadium (now the Robert F. Kennedy Memorial Stadium).\\r\\n\\r\\nFor most of their existence, the new Senators were the definition of futility, losing an average of 90 games a season. The team's struggles led to a twist on a joke about the old Senators--\\"Washington: first in war, first in peace and still last in the American League.\\" Frank Howard, known for his towering home runs, was the team's most accomplished player, winning two home run titles.\\r\\n\\r\\nOwnership changed hands several times during the franchise's stay in Washington and was often plagued by poor decision-making and planning. Owner Elwood Richard Quesada once wondered why he should have to pay his players because he believed they didn't belong in the majors. He later agreed to a 10-year lease at D.C. Stadium  a move that would come back to haunt the Senators. In 1963, Quesada sold his stake in the club and resigned. Washington stockbrokers James Johnston and James Lemon owned the team briefly, suffering massive financial losses. Johnston died in 1967 and Lemon sold the team a year later to hotel and trucking executive Bob Short, who outbid a group headed by Bob Hope. Short named himself general manager and hired Hall of Famer Ted Williams as manager. Although Williams had never coached or managed at any level of baseball, he seemed to light a spark under the once-moribund Senators. Williams kept them in contention for most of the season; their 86ÿ76 record in 1969 would be their only winning season in Washington.\\r\\n\\r\\nThe success though was brief, as Short borrowed most of the $9.4 million he had used to pay for the team. As the Senators general manager, Short was forced to make many questionable trades to lower the debt and acquire amounts of the much-needed revenue. As a result, the team rapidly fell back into the American League's cellar position. Fans kept their distance from the Senators while the Baltimore Orioles, 45 miles (72?km) to the northeast, won four American League pennants and two World Series from the 1966 season through the 1971 season. By the end of the 1970 season, Short had issued an ultimatum: unless someone was willing to buy the Senators for $12 million (in comparison, the New York Yankees were sold in 1973 for $8.8 million), he would not proceed to renew the stadium lease and would move the team elsewhere. When that season ended, Short dealt his best starting pitcher and the left side of his infield to the Detroit Tigers for erstwhile 30-game-winner Denny McLain, who had spent most of the 1970 campaign suspended because of gambling allegations.  The dealalleged by onetime Senators broadcaster Shelby Whitfield to have been made in order to secure the Tigers' vote in favor of the Senators' eventual move to Texasturned Detroit back into contenders, while McLain was a monumental bust, losing an embarrassing league-worst of 22 games.\\r\\n\\r\\nShort was especially receptive to an offer brought up by Arlington, Texas mayor Tom Vandergriff, who had been trying to obtain a Major League sports team to play in the Metroplex for over a decade. Years earlier, Charles O. Finley, the owner of the Kansas City Athletics, sought to relocate his baseball team to Dallas, Texas, but the idea was rebuffed and ultimately declined by the other A.L. team owners.\\r\\n\\r\\nArlington's hole card was Turnpike Stadium, a 10,000-seat park which had been built in 1965 to house the AA Dallas-Fort Worth Spurs of the Texas League. However, it had been built to Major League specifications and also located in a natural bowl, meaning only minor excavations would be necessary to expand the park to accommodate Major League crowds.\\r\\n\\r\\nAfter Vandergriff offered a multimillion-dollar down payment, Short decided to make the move to Arlington. On September 21, 1971, by a vote of 10 to 2 (the Orioles' Jerold Hoffberger and John Allyn of the Chicago White Sox registered the dissenting votes), American League owners granted approval to move the franchise to Arlington, Texas for the 1972 season.[5]\\r\\n\\r\\nSenators fans were livid. Enmity came to a head at the club's last game in Washington. Thousands of fans simply walked in without paying after the security guards left early, swelling the paid attendance of 14,460 to around 25,000, while fans unfurled a banner reading \\"SHORT STINKS\\". With the Senators leading 7ÿ5 and two outs in the top of the ninth inning, several hundred youths stormed the field, raiding it for souvenirs.  One man grabbed first base and ran off with it.  With no security in sight and only three bases, umpire crew chief Jim Honochick forfeited the game to the New York Yankees.[6][7]\\r\\n\\r\\nIt would be 33 years before major league baseball returned to the nation's capital, in the form of the National League's Washington Nationals, which had previously been known as the Montreal Expos.\\r\\n\\r\\nDuring the off-season, improvements were made to Turnpike Stadium, which reopened as Arlington Stadium for the  1972 season. Meanwhile, ownership announced that the franchise would be renamed the Texas Rangers. The team played its first game on April 15, 1972, a 1ÿ0 loss at the hands of their 1961 expansion cousins, then known as the California Angels. The next day, the Rangers defeated the Angels 5ÿ1 for the club's first victory. After the season Ted Williams retired as manager; he had made no secret of his displeasure with the franchise's new location.  Whitey Herzog was named the new manager, but he was replaced before the end of the 1973 season by Billy Martin.\\r\\n\\r\\nIn 1974, the Rangers began to come into their own as a team. Under the ownership of Brad Corbett, they finished the season second in the American League West with an 84ÿ76 record, behind the eventual World Series champion Oakland Athletics. The 1974 Rangers are still the only MLB team to finish above .500 after two consecutive 100-loss seasons. Mike Hargrove was awarded A.L. Rookie of the Year, Billy Martin was named Manager of the Year, Jeff Burroughs won American League MVP, and Ferguson Jenkins was named the Comeback Player of the Year after winning 25 games, a club record to this day. However, after a 44ÿ51 start in 1975, Martin was fired as the Rangers' manager and was replaced by Frank Lucchesi.\\r\\n\\r\\nAfter excellent seasons between 1977ÿ79, the Rangers came very close to clinching a playoff spot in the first half of 1981. But when Texas lost its last game before the players' strike began, the Oakland A's won the A.L. West in the first half by a half-game. After 1981, the Rangers would not post a winning record for another five seasons. During this stretch, the Rangers made one of their most unpopular trades ever, sending multi-Gold Glove catcher and fan favorite Jim Sundberg to the Milwaukee Brewers for future Brewers' manager Ned Yost.\\r\\n\\r\\nThe Rangers faced an attendance problem for a few years after moving their team to Texas, in part due to the team's inconsistent performance and in part due to the oppressive heat and humidity that can encompass the area in the summer. Until the Florida Marlins arrived in 1993, Arlington Stadium was often the hottest stadium in the Majors, with temperatures frequently topping around 100 throughout the months of summer. In part because of this, the Rangers began playing most of even their weekend games between May and September at nighta tradition that continues to this day. They usually get a waiver from ESPN to play Sunday night games.\\r\\n\\r\\nBobby Valentine became steward over an influx of splendid talent in the late 1980s and early 1990s. A winning season in 1986 was a shock to pundits and fans alike as the Rangers remained in the race for the American League pennant for the entire season. With a team consisting of many stellar young rookies such as Rubn Sierra, Pete Incaviglia, Mitch Williams, Bobby Witt, and Edwin Correa, the Rangers finished the season 2nd place with an 87ÿ75 record, just five games behind the division champion California Angels. The season marked a dramatic 25-win improvement over the 1985 season, which resulted in yet another last-place finish in the West. The signing of 41-year-old star pitcher Nolan Ryan prior to the 1989 season allowed Ryan to reach his 5,000th strikeout, 300th win, and sixth and seventh no-hitters with the Rangers. Coupled with powerful batters like Juan Gonzlez, Rubn Sierra, Julio Franco, and Rafael Palmeiro and a pitching staff that also included Charlie Hough, Bobby Witt, Kevin Brown, and Kenny Rogers, fans held really high expectations for the Rangers' upcoming season. However, the team never posted a finish higher than second place and Valentine was relieved of his duties during the 1992 season.\\r\\n\\r\\nIn April 1989, Rangers owner and oil tycoon Eddie Chiles, sold the team to an investment group headed by George W. Bush. After hearing that Chiles planned to sell the team, Bush headed a group of investors (including Frank L. Morsani and the Mack family) that bought the team for $89 million.[8] While his own equity in the team was a small one ($500,000), he was named Managing General Partner of the new ownership group. He increased his investment to $600,000 the following year.[9]\\r\\n\\r\\nDuring his tenure, the Rangers and the City of Arlington decided to replace the aging Arlington Stadium with a new publicly funded stadium, at a cost of $193 million, financed by Arlington residents, through a sales tax increase. Ground was broken on October 30, 1991 on what would become The Ballpark in Arlington (now named Globe Life Park in Arlington). The city, through the Arlington Sports Facilities Development Authority, also controversially authorized the seizure of 13 acres (53,000?m2) of land through eminent domain for the Rangers future development. Landowners filed lawsuits over the acquisition and eventually won settlements of $22.2 million which the Rangers failed to pay.\\r\\n\\r\\nBush left his position with the Rangers when he was elected Governor of Texas in 1994. Although Bush no longer has any ownership stake in the Rangers, he remains a fan of the team to this day and regularly attends the team's home games.[10]\\r\\n\\r\\nIn 1993, Kevin Kennedy took over managerial duties, presiding over the team for two seasons, keeping the 1993 Rangers in the hunt for a playoff berth into mid-September. Kennedy was let go in 1994, although the team led the A.L. West prior to the players' strike.  On July 28 Kenny Rogers pitched the 12th perfect game in Major League history.  The strike prematurely ended what could have been the Rangers' first division championship when commissioner Bud Selig canceled the remainder of the season and the playoffs.\\r\\n\\r\\nThe year 1995 saw the beginnings of promise for the Rangers. With a brand new ballpark that hosted its first All-Star Game that year, Johnny Oates was hired as the Rangers' manager.\\r\\n\\r\\nOates and company promptly helped to bring home the 1996 A.L. Western Division Championship, the first division championship in franchise history. The first playoff series, 24 years after the franchise came to Texas, saw the Rangers lose to the New York Yankees, 3 games to 1. Oates was named A.L. Manager of the Year and Juan Gonzlez was named A.L. MVP. The team featured a powerful lineup of hitters with Ivn Rodrguez, Will Clark, Mark McLemore, Dean Palmer, Rusty Greer, Juan Gonzlez, and Mickey Tettleton but continued to struggle with pitching ÿ a common stereotype of Rangers teams ÿ despite having Rick Helling, and Aaron Sele on their roster.\\r\\n\\r\\nOates led the team to consecutive A.L. West championships in 1998 and 1999. Neither of Oates' last two playoff teams could win a single game, losing all 6 in back-to-back sweeps at the hands of the Yankees, a team that won 3 World Championships in the 1990s after defeating Rangers teams in the first round. The 1999 team would be the last playoff-bound team for over a decade, as the Rangers took a step backwards at the beginning of the new millennium. En route to a second straight last-place finish, Oates resigned his position 28 games into the 2001 season.  The Rangers finished the decade going 1ÿ9 in the postseason, without a single home win.\\r\\n\\r\\nIn 1998, venture capital billionaire Tom Hicks bought the team for $250 million. Hicks also agreed to pay the $22.2 million awarded in settlements in relation to the 1991 eminent domain litigation concerning the Ballpark in Arlington.[11]\\r\\n\\r\\nPrior to the 2001 season, star free agent shortstop Alex Rodriguez was signed by the Rangers in the most lucrative deal in baseball history: a 10-year, $252 million contract.\\r\\n\\r\\nThe move was controversial and is frequently maligned by fans and writers who thought that owner Tom Hicks was placing too much emphasis on one player instead of utilizing team resources to acquire several players, especially for a team that lacked pitching talent. Club officials maintained that Rodriguez would be the cornerstone of future postseason success.\\r\\n\\r\\nAlthough Rodriguez's individual performance was outstanding, the Rangers continued to struggle and manager Jerry Narron was fired following the 2002 season, and he was replaced by seasoned manager Buck Showalter.\\r\\n\\r\\nThe 2003 season signified the fourth straight last-place finish, and after a post-season fallout between Rodriguez and club management, the reigning A.L. MVP and newly appointed Rangers captain was traded to the New York Yankees for second baseman Alfonso Soriano and infielder prospect Joaquin Arias.\\r\\n\\r\\nPrior to the 2004 season, little hope was held out for the Rangers to improve on their losing ways. However, the Rangers battled with the Anaheim Angels and Oakland Athletics for first place in the A.L. West for much of the season. Mark Teixeira, Alfonso Soriano, Michael Young, and Hank Blalock became some of the best-hitting infielders in the league, with Young, Blalock, and Soriano being named to the 2004 All-Star Game. Soriano was named the All-Star MVP after going 2 for 3 with a three-run home run. Late in September, the Oakland A's visited Arlington for a 3-game series. After taking the first two games of the series, the Rangers trailed 4ÿ2, in the bottom of the ninth. A loss would have dropped them to four games behind the Athletics. A home run by Hank Blalock and a dramatic two-out, two-run double by David Dellucci (known amongst fans as the \\"Dellucci Double\\") gave the Rangers a 5ÿ4 win, one of the most memorable in club history. It also allowed the Rangers to sweep the first-place Athletics and leave them just two games behind with 10 to play. Unfortunately, the Rangers ended up losing six of the final ten games and another turnaround season came up short. The club finished in third place behind the Angels and A's, a mere 3 games out of first place.\\r\\n\\r\\nIn 2005 the Rangers again struggled to find consistency amid controversy and injuries, notwithstanding that the team swept an entire homestand for the first time in its history. Frank Francisco and Carlos Almanzar, two key members of the bullpen, were sidelined for Tommy John surgery. Kenny Rogers, the team's ace pitcher, received a 20-game suspension from commissioner Bud Selig for attacking a cameraman at Ameriquest Field prior to a game. Management later placed opening day starter Ryan Drese on waivers, where he was claimed by the Washington Nationals. After Drese's release and Rogers' suspension, the Rangers' performance on the mound faltered, and a disastrous 1ÿ12 August road trip all but sealed the squad's fate.\\r\\n\\r\\nOn October 4, 2005, the Rangers announced that John Hart would step down as general manager and that Jon Daniels was being promoted from assistant general manager to replace him. Daniels, at 28 years and one month, would become the youngest general manager in Major League history.\\r\\n\\r\\nDaniels and the Rangers front office were very active in the 2005ÿ2006 offseason. Alfonso Soriano, who had often been mentioned in trade speculation, was finally dealt to the Nationals for outfielders Brad Wilkerson and Terrmel Sledge. The Rangers then began making moves to acquire pitching talent. The Rangers gained enigmatic starter Vicente Padilla from the Philadelphia Phillies in exchange for Ricardo Rodrguez and acquired San Diego Padres pitchers Adam Eaton and Akinori Otsuka in exchange for Chris Young, Adrian Gonzalez, and Sledge. Finally, they signed free agent starter Kevin Millwood to a five-year contract worth $60 million.\\r\\n\\r\\n\\r\\nThe Rangers 2006 season ended with a disappointing 80ÿ82 record and a third-place finish in the West. Though the club showed strength in the early going, the team proved unable to keep pace with the surging Oakland Athletics in the second half and fell out of contention in September. To some extent the Rangers were the victims of bad luck, as their win-loss record was worse than their +51 run differential for the season would normally indicate. However, the element that the club continued to lack was a solid pitching staff, whose combined ERA ranked 9th in the A.L. at the season's end. The Rangers were represented in the 2006 All-Star game by center fielder Gary Matthews, Jr. and shortstop Michael Young, who was named the MVP for his game-winning two-run triple in the ninth inning.\\r\\n\\r\\nSignificant player moves in-season included a July 28 deal acquiring outfielders Carlos Lee and Nelson Cruz from the Milwaukee Brewers in exchange for Kevin Mench, Francisco Cordero, and Laynce Nix.\\r\\n\\r\\nOn October 4 after two attempts to replicate the success of the 2004 team, the Rangers dismissed Buck Showalter as manager with three years left on his contract. A month later, the team announced that Oakland Athletics third base coach Ron Washington had accepted their offer to manage the team.[12] A change at manager would be the first of several moves to strengthen the team in yet another busy off-season.\\r\\n\\r\\nGary Matthews, Jr., Mark DeRosa, Carlos Lee, and Adam Eaton all signed with other clubs as free agents. Vicente Padilla accepted a three-year, $33 million offer with an option for a fourth year at $12 million.[13] The Rangers also signed 1B/OF Frank Catalanotto to a multi-year deal. The Rangers later added reliever ric Gagn and veteran outfielders Kenny Lofton and Sammy Sosa on short-term deals. In a sign that GM Jon Daniels was looking for results in 2007, the Rangers' top pitching prospect John Danks was traded to the Chicago White Sox along with reliever Nick Masset for 23-year-old starter Brandon McCarthy.[14] The Danks trade caused quite a stir amongst fans, many of which had followed the minor league careers of the highly touted \\"DVD\\" trio of pitchers that included Danks, Edinson V܇lquez, and Thomas Diamond. All three pitchers would eventually reach the majors, with varying degrees of success, while McCarthy's career became marred by injury.\\r\\n\\r\\nThe Rangers previously negotiated a 30-year, $75 million stadium naming rights agreement with Ameriquest Mortgage Company in 2004, renaming the Ballpark \\"Ameriquest Field\\". Under the Ameriquest moniker, the Ballpark featured a replica of the Liberty Bell in the stands above the Diamond Club (representing Ameriquest's logo) that rang after home runs. In 2007, though, the Rangers announced the termination of the agreement with Ameriquest, and changed the name to Rangers Ballpark in Arlington. Club president Jeff Cogen cited that the team was more concerned about getting their name back on the ballpark than Ameriquest's public financial troubles. \\"It's all about the brand\\", Cogen said. The Rangers lost a reported $2.5 million per year from the lack of naming rights but regained advertising space given over to Ameriquest in the naming rights deal.[15] Regardless of Cogen's comments, Ameriquest dissolved within months after the naming rights were terminated, and the company ended most business operations in September 2007.\\r\\n\\r\\nThe Rangers struggled offensively early in the season, despite playing in a notoriously hitter-friendly park. On June 20, Sammy Sosa hit his 600th career home run against the Chicago Cubs at the Rangers Ballpark in Arlington. Hank Blalock, the starting 3rd baseman who had been enjoying a good season, was placed on the 60-day disabled list on the May 19 due to thoracic outlet syndrome, and Mark Teixeira followed him onto the disabled list on June 9 (for the first time in his career) with a strained left quadriceps muscle. With a record of 46ÿ59 at the July 31 trade deadline, the team traded Mark Teixeira and Ron Mahay to the Atlanta Braves in a deal that would eventually bring 5 prospects to the Rangers organization, including four of Atlanta's top prospects Jarrod Saltalamacchia, Elvis Andrus, Matt Harrison, and Neftal Feliz. The team also traded closer ric Gagn to the Boston Red Sox for left-hander Kason Gabbard and Minor League outfielders David Murphy and Engel Beltre. These moves were the beginnings of a rebuilding project headed by Jon Daniels with a focus on the acquisition and development of young players. In the coming years, more club resources would be dedicated to improving the quality of the farm system and scouting departments, most notably in Latin America and the Far East. The objective of Daniels' plan was to field a legitimately competitive team by the 2010 season.\\r\\n\\r\\nThe 2007 season remains notable in the minds of baseball fans for two inexplicable oddities. On August 19 at the Metrodome, the Minnesota Twins logged 19 strikeouts against the Rangers, one short of the Major League record. Three days later, the 22nd, in the first game of a doubleheader at Oriole Park at Camden Yards, the Rangers' bats came alive with a modern record for runs by one team, defeating the Baltimore Orioles 30ÿ3. Their 27-run margin of victory is also a modern-day MLB record. Wes Littleton gained probably the easiest save in Major League history: entering the game in the bottom of the seventh, with his team already ahead 14ÿ3, he pitched three innings to finish the game, and gave up just two hits and a walk.\\r\\n\\r\\nThe Rangers began the 2008 season red hot, headlined by newcomer Josh Hamilton who looked to be a threat to win the Triple Crown, before fading off as the season wore on. During the All-Star festivities at Yankee Stadium, Hamilton crushed a first round home run record in the 2008 Home Run Derby with 28. Hamilton hit another four in the second round and three during the final round, for a total of 35 home runs, but lost to the Twins' Justin Morneau. Four Rangers played in the All Star Game: Josh Hamilton, Ian Kinsler, Milton Bradley, and Michael Young, who would repeat his 2006 All-Star Game feat by driving in the winning run via a sac fly.\\r\\n\\r\\nThe Rangers would finish the season with yet another sub-.500 record (79ÿ83), yet ended the season second in the West, the club's best finish since 1999.  The off-season saw perennial All-Star shortstop Michael Young ask for a trade when the team told him he would be moving to third base to make room for rookie Elvis Andrus.[16]  After speaking with club president Nolan Ryan and his agent, Young later rescinded his trade request, and agreed to move to third base.[17]  The offseason also saw the departure of mercurial outfielder/DH Milton Bradley to free agency.\\r\\n\\r\\nThe 2009 season saw the Rangers soar into playoff contention for the first time since 2004. Despite injuries to Josh Hamilton and Ian Kinsler, the Rangers held first place in their division for long stretches of the summer before fading after September 1, losing the division to the Los Angeles Angels. The Rangers finished the season at 87ÿ75, their first winning season since 2004 and good enough for second place in the A.L. West. Michael Young responded to his move to third base by posting one of his best offensive seasons ever while committing just nine errors and earning a sixth straight All-Star appearance.[18] Josh Hamilton and Nelson Cruz were also named 2009 A.L. All-Stars.  Several young stars with the club broke out including the debuts of highly rated rookies Elvis Andrus, Derek Holland, and Neftal Feliz. Second baseman Ian Kinsler hit for the cycle in April, while having a 30ÿ30 season in home runs and stolen bases. Starting pitcher Scott Feldman posted a fantastic season as well in 2009, finishing 3rd in the A.L in wins with 17.\\r\\n\\r\\nWhile the 2009 season was strong on the field, club owner Tom Hicks became the focus of several reports indicating serious financial problems with his holding group, Hicks Sports Group, which also owned the Dallas Stars, the Frisco Roughriders (the Rangers AA-farm club), 1/2 of Liverpool F.C. (sold in mid-October 2010 to New England Sports Ventures, owners of the Boston Red Sox), and the Mesquite Championship Rodeo (later sold by HSG).\\r\\n\\r\\nHSG was reported to have gone into default on a $525 million loan.[19]\\r\\n\\r\\nIn April 2009, Hicks announced he would be willing to sell a minority interest in the team.  Only one month later, Hicks announced he would be willing to sell majority control of the Rangers.[20]\\r\\n\\r\\nIn July 2009, it was reported that Hicks borrowed money from Major League Baseball to meet the team's payroll.[21]\\r\\n\\r\\nAfter the 2009 season, Hicks began scouting prospective buyers and in December entered into exclusive negotiating rights for sale of the Rangers with a consortium headed by Pittsburgh sports lawyer Chuck Greenberg and Rangers team president Nolan Ryan.\\r\\n\\r\\nOn January 22, 2010, Hicks Sports Group officially reached a formal agreement to sell the Texas Rangers to the group headed by Greenberg and Ryan (later called Rangers Baseball Express) for approximately $570 million.[22]  Under the provisions of the deal, former owner Hicks stayed on as a limited minority partner, but was not allowed to retain a seat on the board of governors.  Co-lead investors Dallas businessmen Ray Davis and Bob R. Simpson were named co-chairmen.[22]  Hicks also sold much of the land surrounding Rangers Ballpark to Rangers Baseball Express in a separate deal.\\r\\n\\r\\nThe deal was subject to approval by the other MLB owners (a 3/4 vote is required) and completed by April 1.  However, one of HSG's principal lenders (Monarch Alternative Capital) opposed the sale on grounds that the proceeds would not fully repay the defaulted HSG notes.[23]  On April 21, Major League Baseball issued a statement declaring the Rangers' sale to be under the control of the Commissioner to expedite the process.[24][25]  Because of public comments made by Hicks deemed detrimental to the process, MLB also stripped Hicks of any responsibility regarding the sale of the team.[26]  On May 13, MLB threatened to seize control of the rest of the team's operations if a deal was not completed by the deadline set by the Commissioner.[27]\\r\\n\\r\\nAs the stalemate between HSG and its creditors continued, on May 24, 2010 the Texas Rangers filed for Chapter 11 bankruptcy.[28][29] As of that date, the Rangers and HSG had an estimated debt of $575 million.[29] Much of the unsecured debt was owed in back salary. Officially, New York Yankees third baseman Alex Rodriguez topped the list of unsecured creditors with an estimated $24.9 million owed by the Rangers.[29] Additionally, the Rangers also owed Baltimore Orioles pitcher Kevin Millwood $12.9 million, and then current Rangers third baseman Michael Young $3.9 million.[29]  At a press conference, the Greenberg-Ryan group proposed to buy the team for $575 million.[29] The sale would repay all the team's creditors, including players owed back salary.[29]\\r\\n\\r\\nAfter several attempts to resolve the deal fell through, the bankruptcy court ordered a public auction to be held on August 4.  The Greenberg/Ryan bid would be the opening bid, and other offers (subject to MLB approval) would have to be submitted by the prior day in order to be considered.  At the auction, only one other MLB-approved group submitted an offer?ÿ Radical Baseball LLC, a group formed by Houston businessman Jim Crane (who was previously unsuccessful in buying the Houston Astros, and ultimately bought that team in 2011) and Dallas Mavericks owner Mark Cuban (who was previously unsuccessful in buying the Chicago Cubs). The auction lasted until the early morning of August 5, with the winning bid submitted by Greenberg/Ryan.  The bankruptcy court approved the bid later that morning and the bankruptcy case closed.  The sale to the Greenberg/Ryan was approved by all 30 MLB owners at the owners meeting in Minneapolis on Thursday August 12.\\r\\n\\r\\n\\"I'll be the managing partner and CEO. If you like what's going on or you don't like what's going on, I take responsibility for that. When it comes to baseball, I'm not going to interject my opinions. If Nolan and [Jon Daniels] want to discuss something with me, they can, but I have complete faith and trust in the decisions that they make. I'll be as involved as they wish me to be, but with complete faith and confidence in them.\\r\\nWhat I'm going to focus on, particularly since the baseball side is in great shape, is the business side. How do we connect with the community? How do we create a higher tempo of energy in the front office? How can we do a better job of filling the stands and make an impact on people's lives? If we succeed on the business side and continue on path on the baseball side and combine it with a dynamic market like this is, we can be and should be one of the powerhouse franchises in baseball.[30] \\"\\r\\nThe new ownership group, which included Chuck Greenberg and Nolan Ryan, was called Rangers Baseball Express, LLC and had Greenberg serving as managing general partner and Ryan as club president.[31] Davis and Simpson were named co-chairmen.[32] In March 2011 Greenberg resigned as Chief Executive and Managing General Partner and sold his interest in the Club following a \\"falling out\\" with his partners.[33]  Following Greenberg's resignation, Nolan Ryan was named Chief Executive Officer in addition to his continuing role as Team President.[32]  Ryan was subsequently designated the Controlling Owner of the Club by a unanimous vote of the 30 owners of Major League Baseball on May 12, 2011.[33] Co-Chairmen Simpson and Davis stated they would not be involved in day-to-day operations.[34]\\r\\n\\r\\nOffseason moves made by the Rangers sent Kevin Millwood to the Baltimore Orioles and acquired free agents Rich Harden, Colby Lewis, and Vladimir Guerrero. With the new influx of talent and success in 2009, the Rangers entered the season  expecting to compete for the division and achieve the front office's goals of 2007. During the offseason, Texas Rangers' team President Nolan Ryan spoke about the Rangers' chances in the upcoming season saying, \\"My expectations today are that we're going to be extremely competitive and if we don't win our division, I'll be disappointed.\\"[35]\\r\\n\\r\\nAfter stumbling out of the gates with a sub-.500 start in April, the Rangers took the division lead with a franchise-best month of June, going 21ÿ6. The Rangers would never relinquish first place after an 11-game winning streak. On July 9, the club dealt one of its top prospects, Justin Smoak, with two other minor leaguers to the Seattle Mariners for former Cy Young Award winner Cliff Lee and Mark Lowe. The Rangers also made moves to acquire veterans Bengie Molina, Jorge Cant~, Cristian Guzmn, and Jeff Francoeur. In the 2010 All-Star Game, the team was represented by Cliff Lee, Vladimir Guerrero, Ian Kinsler, Josh Hamilton, Elvis Andrus, and Neftal Feliz. After the All-Star Game, came the debut of the claw and antler hand gestures, which gained much popularity, especially after the release of various apparel and souvenir options for the fans. Foam claws and helmets with deer antlers became quite commonplace in the ballpark as the Rangers played further into the fall. The Rangers won the A.L. West on September 25, advancing to the postseason for the first time since 1999.[36]\\r\\n\\r\\nAfter winning the AL West with a 90ÿ72 record, the Rangers entered the playoffs for the first time since 1999, and faced the Tampa Bay Rays for the first round, which ultimately resulted in a 3ÿ2 series victory and marked the first postseason series victory in the 50-year history of the Rangers/Washington Senators franchise.  Facing the Rangers in the American League Championship Series were the defending World Champion New York Yankees, the team the Rangers failed against three separate times in the 1990s.  In the playoffs, the Rangers record against the Yankees was 1ÿ9. In a 6-game series, the Texas Rangers came out victorious, winning the first pennant in franchise history in front of an ecstatic home crowd.[37] Josh Hamilton was awarded ALCS MVP after setting a series record for intentional walks. The Texas Rangers faced the San Francisco Giants in the 2010 World Series. The Rangers offense struggled against the Giants' young pitching and eventually lost the Series 4ÿ1, the lone win coming in Arlington on October 30. For the first time, the Rangers ended their season in the month of November.\\r\\n\\r\\nThe Rangers successfully defended their AL West Division title in 2011, making the club's second straight division title and postseason appearance and fifth division title and postseason appearance overall.  The Rangers set records for best record (96ÿ66 with a .592 winning percentage) and home attendance (2,946,949). On October 15, they went back to the World Series after beating the Detroit Tigers 15ÿ5 in Game 6 of the ALCS.[38] The series featured Nelson Cruz hitting 6 home runs, the most home runs by one player in a playoff series in MLB history. In game 2 Cruz also became the first player in postseason history to win a game with a walk-off grand slam as the Rangers defeated the Tigers 7ÿ3 in 11 innings. However, they would proceed to lose the World Series to the St. Louis Cardinals in seven games, after twice being one strike away from the championship in game 6. Game 7 is notable for the controversy surrounding the manager Ron Washington's pre-game clubhouse speech.[39]\\r\\n\\r\\nThe Rangers lost C. J. Wilson to the rival Angels in the offseason, but managed to gain the services of Japanese pitcher Yu Darvish. They also added Joe Nathan to fill the closer role vacated by Neftal Feliz, who was shifted to full-time starter. The Rangers dominated the American League standings for much of the season, despite losing Feliz and Colby Lewis to the 60-day DL. Darvish quickly flourished in the starting rotation, finishing with a 16ÿ9 record and a 3.90 ERA. Josh Hamilton also had a spectacular season, finishing with 43 home runs, four of which came in a single game against the Baltimore Orioles on May 8. However the Rangers floundered in September, culminating in a sweep by the Oakland Athletics in the final series. Before the last series of the regular season, the Rangers faced the Los Angeles Angels of Anaheim on September 29, 2012. The game was postponed due to a rain delay and the game was held off until the next day. The Texas Rangers then lost to the Angels 5ÿ4 for the first game and won 8ÿ7 in the second game later that night, which guaranteed them a spot in the first ever American League wild-card playoff game. As a result of their 93ÿ69 record, finishing second in the AL West behind the Oakland A's, the Texas Rangers settled for the Wild Card playoff slot. In the new Wild Card Game, the Rangers' woes continued, as they lost 5ÿ1 to the Orioles.\\r\\n\\r\\nThe Rangers finished the season in second place in the American League West with a 91-72 record, 5 1/2 games behind the Oakland Athletics. On September 30, 2013, the Rangers faced the Tampa Bay Rays in a 163rd play-in 2013 American League Wild Card tie-breaker game, to determine the second participant in the 2013 American League Wild Card Game against the Cleveland Indians. The 163rd game was determined to be an extension of the regular season and not considered to be part of the 2013 Major League Baseball Postseason. The Rangers lost to the Rays 5-2 in the tie-breaker and were eliminated from playoff contention for the first time since 2009.\\r\\n\\r\\nOn October 17, 2013, Nolan Ryan announced that he was stepping down as Rangers CEO effective October 31, 2013.[40]\\r\\n\\r\\nInjuries took a major toll on the Rangers in 2014.[41][42] The lone bright spot was Adrin Beltr, who despite spending some time injured, was the most consistent offensive player on the team.[43][44] On September 4, 2014, the Rangers became the first MLB team officially eliminated from 2014 postseason contention when a 10ÿ2 loss at home to the Seattle Mariners dropped their record to 53ÿ87.[45][46] The following day, manager Ron Washington resigned, citing personal issues.[47]\\r\\n\\r\\nThe 2015 season started off on a bad note with the Ranger's Ace pitcher Yu Darvish having to undergo Tommy John surgery due to a torn UCL before the season began. According to most experts, the Rangers were not expected to place any better than fourth or fifth in the American League West. However, even though the Rangers started the season with an 8-16 record, they were able to surpass the .500 mark on a few different occasions throughout the year. Surprisingly, in 2015 the Rangers were one of the best road teams in all of baseball with a road record of 45-36. Only the Chicago Cubs had a better road record with 48-33.\\r\\n\\r\\nWith the acquisition of Cole Hamels, the Rangers would retain a winning record after August 3. The Rangers would overtake the Houston Astros and clinch the American League West title on the final day of the season with a record of 88-74. It was the Rangers' 6th division title and 7th postseason appearance in franchise history. Hamels' positive effect on the Rangers was compared to that of the Toronto Blue Jays' David Price, another ace starting pitcher who helped spearhead a run to the postseason after blending in seamlessly with a struggling team who acquired him at the trade deadline.[1]\\r\\n\\r\\nThe Rangers lost to the Toronto Blue Jays in five games in the Division Series after squandering a 2-0 series lead.\\r\\n\\r\\nThrough the 2016 season, the Rangers have won 4,277 games and lost 4,649 over their history, equating to a .479 lifetime winning percentage. After the 2011 World Series and the 2012 season, the team is 19ÿ25 in individual playoff games, and 4ÿ6 overall for postseason series.\\r\\n\\r\\nNellie Fox\\r\\n\\r\\nTed Williams\\r\\n\\r\\nBert BlylevenGoose Gossage\\r\\n\\r\\nVladimir GuerreroWhitey Herzog\\r\\n\\r\\nFerguson Jenkins\\r\\n\\r\\nGaylord PerryIvn Rodrguez\\r\\n\\r\\nNolan Ryan\\r\\n\\r\\nChuck Hinton and Frank Howard, who played for the franchise in Washington (although Howard played for the Rangers in 1972), are listed on the Washington Hall of Stars display at Nationals Park in Washington. So are Gil Hodges and Mickey Vernon, who managed the \\"New Senators\\". Vernon also played for the \\"Old Senators\\", who became the Minnesota Twins.\\r\\n\\r\\nJon Miller\\r\\n\\r\\nEric Nadel\\r\\n\\r\\nRangers Captain is the mascot for the Texas Rangers. Introduced in 2002, he is a palomino-style horse, dressed in the team's uniform. He wears the uniform number \\"72\\" in honor of 1972, the year the Rangers relocated to Arlington.\\r\\n\\r\\nThe mascot also has multiple uniforms to match each of the variants the team has. Rangers Captain's chosen uniform for the game matches the uniform choice made by the team for that particular game. Captain's outfits sometimes match a theme the team is promoting; on Apr 24, 2010, he was dressed up like Elvis as part of an Elvis Presley themed night.\\r\\n\\r\\nThe Texas Rangers Hall of Fame was created in 2003 to honor the careers of former Texas Rangers players, managers, executives and broadcasters. There are currently 20 members.\\r\\n\\r\\nThe Hall is located in Globe Life Park in Arlington, behind right field.[48] The Hall's two levels contain 13,000 square feet, a 235-seat theater, and various plaques, photos, and memorabilia. It can accommodate up to 600 people.[49]\\r\\n\\r\\nPitchers\\r\\nStarting rotation\\r\\n\\r\\nBullpen\\r\\n\\r\\nCloser\\r\\n\\r\\nCatchers\\r\\n\\r\\nInfielders\\r\\n\\r\\nOutfielders\\r\\n\\r\\n\\r\\n\\r\\nPitchers\\r\\n\\r\\nCatchers\\r\\n\\r\\nInfielders\\r\\n\\r\\nOutfielders\\r\\n\\r\\n\\r\\n\\r\\nManager\\r\\n\\r\\nCoaches\\r\\n\\r\\n60-day disabled list\\r\\n\\r\\n\\r\\n\\r\\n 7- or 10-day disabled list\\r\\n Suspended list\\r\\n# Personal leave\\r\\nRoster and coaches updated August 12, 2018\\r\\nTransactions\\r\\n? Depth chart\\r\\n\\r\\nIn addition to the flagship stations listed above, Rangers games can be heard on affiliates throughout much of Texas, and also in parts of Oklahoma, Arkansas, Louisiana, and New Mexico.[50] Eric Nadel is the primary play-by-play announcer. He has called games for the club since 1979 beginning on television broadcasts, then moving exclusively to radio beginning in 1985. He became the primary announcer after the late Mark Holtz moved to television. Currently, Nadel provides play-by-play in the 1st, 2nd, 5th, 6th, 8th, and 9th innings, and color commentary for the other innings. On December 11, 2013, he was awarded the 2014 Ford C. Frick Award by the National Baseball Hall of Fame and Museum for excellence in broadcasting.[51] Matt Hicks now shares the broadcast booth with Nadel. He joined the broadcast in 2012 after Steve Busby moved from radio to television to replace Dave Barnett. Hicks provides play-by-play in the 3rd, 4th, and 7th innings, and color commentary for the other innings. Jared Sandler hosts the pre-game and post-game shows, and also fills in whenever Nadel or Hicks have a day off. For the Spanish radio affiliates, Eleno Ornelas is the play-by-play announcer, and former Rangers pitcher Jos Guzmn is the color analyst.\\r\\n\\r\\nTexas Rangers games currently air on regional television network Fox Sports Southwest. During the 2016 season, they had an average 3.96 rating and 105,000 viewers on primetime broadcasts.[52] Due to the Rangers having to play many of their Sunday home games at night, the team has been featured frequently on ESPN's Sunday Night Baseball during the summer months. Rangers games can also be seen on MLB on Fox and TBS. \\r\\n\\r\\nSince 2017, Dave Raymond is the primary television play-by-play announcer and former MLB pitcher C. J. Nitkowski is the primary color commentator.[53] Nitkowski also fills in for Raymond on play-by-play for select games. Raymond replaced Steve Busby, who since 1982 on both TV and radio has had various stints in various positions on Rangers broadcasts from play-by-play to color commentary to pre-game and post-game analysis. In June 2012, Busby moved back to television play-by-play after Dave Barnett left his position as game announcer following an episode in which he experienced speech difficulties.[54] Beginning in 2016, Raymond substituted for Busby on select games. Previously the primary color commentator, Tom Grieve still broadcasts many games. A former Rangers player and general manager, Grieve has been in the TV booth since 1995, following the end of his tenure as GM. Another former Ranger, Mark McLemore, has substituted for Grieve in the past[55] and often joins the booth for an inning during home games. He and former Ranger Ivn Rodrguez are among the pre-game and post-game analysts used on Fox Sports Southwest. FSSW pre-game and post-game shows are hosted by a rotation among Dana Larson, John Rhadigan, Ric Renner, and Erin Hartigan. In-game reporters include Rhadigan, Hartigan, Lesley McCaslin, and Rangers employee Emily Jones (formerly of FSSW).\\r\\n\\r\\nThe Texas Rangers Six Shooters is an interactive dance squad for the Texas Rangers that perform various duties at Rangers home games.[56] The women dance, tumble, and interact with fans.","input":"When were the texas rangers baseball team established?"},{"output":"in the late 1960s","context":"","input":"When did we first see colour television in australia?"},{"output":"Selayang Quarry, Selangor, then in the Federated Malay States (now Malaysia)","context":"The Hash House Harriers (HHH or H3) is an international group of non-competitive running social clubs. An event organized by a club is known as a hash, hash run or simply hashing, with participants calling themselves hashers or hares and hounds.\\r\\n\\r\\n\\r\\nHashing originated in December 1938 in Selayang Quarry, Selangor, then in the Federated Malay States (now Malaysia), when a group of British colonial officers and expatriates began meeting on Monday evenings to run, in a fashion patterned after the traditional British paper chase or \\"hare and hounds\\", to rid themselves of the excesses of the previous weekend.[1] The original members included Albert Stephen (A.S.) Ignatius \\"G\\" Gispert, Cecil Lee, Frederick \\"Horse\\" Thomson, Ronald \\"Torch\\" Bennett, Eric Galvin, H.M. Doig, and John Woodrow.[2] A. S. Gispert suggested the name \\"Hash House Harriers\\" after the Selangor Club Annex, where several of the original hashers lived and dined, known as the \\"Hash House\\".[3][4]\\r\\nHashing died out during World War II shortly after the Invasion of Malaya, but was restarted in 1946 after the war by several of the original group, minus A. S. Gispert, who was killed on 11 February 1942 in the Japanese invasion of Singapore, an event commemorated by many chapters by an annual Gispert Memorial Run.\\r\\nAfter World War II, in an attempt to reorganize in the city of Kuala Lumpur, they were informed by the Registrar of Societies that as a \\"group,\\" they would require a constitution.[5] Apart from the excitement of chasing the hare and finding the trail, harriers reaching the end of the trail would partake of beer, ginger beer and cigarettes.\\r\\nThe objectives of the Hash House Harriers as recorded on the club registration card dated 1950:\\r\\nIn 1962, Ian Cumming founded the second chapter in Singapore. The idea spread through the Far East and the South Pacific, Europe, North America, expanding rapidly during the mid-1970s. Cumming was widely credited with bringing hashing to the United States and lived outside of New York City, where he continued to hash until his death on August 21, 2015.[6]\\r\\nAt present, there are almost two thousand chapters in all parts of the world, with members distributing newsletters, directories, and magazines and organizing regional and world hashing events. As of 2003, there are even two organized chapters operating in Antarctica.[7]\\r\\nMost hashing clubs gather on a weekly or monthly basis, though some events occur sporadically, e.g., February 29th, Friday the 13th, Typhoon 'T8' or a full moon.\\r\\nAt a hash, one or more members (\\"hares\\") lay a trail, which is then followed by the remainder of the group (the \\"pack\\" or \\"hounds\\"). Sawdust, flour or chalk are usually used to mark the trail. The trail periodically ends at a \\"check\\" and the pack must find where it begins again; often the trail includes false trails, short cuts, dead ends, back checks, and splits. These features are designed to keep the pack together despite differences in fitness level or running speed, as front-runners are forced to slow down to find the \\"true\\" trail, allowing stragglers to catch up.\\r\\nMembers sometimes describe their group as \\"a drinking club with a running problem,\\" indicating that the social element of an event is as important, if not more so, than any athleticism involved. Beer remains an integral part of a hash, though the balance between running and drinking differs between chapters, with some groups placing more focus on socialising and others on running.\\r\\nGenerally, hash events are open to the public and require no reservation or membership, but most require a small fee, referred to as \\"hash cash\\", to cover the costs incurred, such as food or drink, and the club treasurer may also be nicknamed \\"Hash Cash\\".\\r\\nThe end of a trail is an opportunity to socialise, have a drink and observe any traditions of the individual chapter (see Traditions). When the hash officially ends, many members may continue socialising at an \\"on-after\\", \\"on-down\\", \\"on-on-on\\", \\"apres\\", or \\"hash bash\\", an event held at a nearby house, pub, or restaurant.\\r\\nIn addition to regularly scheduled hashes, a club or chapter may also organize other events or themed runs. Many also hold special events on their anniversaries or when they reach a milestone in the number of runs e.g. for run number 100, 200, 777, 1000, etc. This may include a special weekend with several runs and evening celebrations\\r\\nAn event held annually by some chapters is the \\"Red Dress Run\\". In 1987, Donna Rhinehart was taken to a hash in Long Beach, California, to be introduced to the sport. She was invited to \\"wait in the truck\\" until her host returned. Instead Rhinehart joined the hash in her red dress. The following year, the San Diego Hash House Harriers sent Rhinehart an airline ticket to attend the inaugural \\"Red Dress Run\\". Hundreds of hashers wore red dresses for the event which was widely covered by local media. In addressing the crowd, Rhinehart suggested that such hashes might be held to raise funds for local charities. The event quickly spread around the globe to places such as Beijing, Montreal, Helsinki, Moscow, Tokyo, Washington DC and Hobart in Australia.[8] Over the years, it has raised millions of dollars for charity. The New Orleans Hash House Harriers attracted 7,000 participants to their Red Dress Run in 2010, raising more than $200,000 for 50 local charities.[9]\\r\\nToday the Red Dress Run is another part of the Hash House Harriers heritage. Rhinehart died in 2013 as some clubs were celebrating the 25th anniversary of their Red Dress Run.[10]\\r\\nHashing has not strayed far from its roots in Kuala Lumpur. The hare(s) mark their trail with paper, chalk, sawdust, or coloured flour, depending on the environment and weather.\\r\\nSpecial marks may be used to indicate a false trail, a backtrack, a shortcut, or a turn. The most commonly used mark is a \\"check\\", indicating that hashers will have to search in any direction to find the continuation of the trail. Trails may contain a \\"beer check\\", where the pack stops to consume beer, water, or snacks, allowing any stragglers to catch up to the group.\\r\\nTrails may pass through any sort of terrain and hashers may run through back alleyways, residential areas, city streets, forests, swamps, deep mud (\\"shiggy\\") or shopping malls and may climb fences, ford streams, explore storm drains or scale cliffs in their pursuit of the hare.\\r\\nHashers often carry horns or whistles to communicate with each other, in addition to verbal communication. Every hash house employs its own set of trail marks and the names for these marks may vary widely, so newcomers or visitors will have the local markings explained to them before the run at a \\"chalk talk\\". The most common term is \\"on-on,\\" shouted by runners to let others know they are on the right trail. A yell of \\"RU\\" (pronounced \\"are you\\") is a question to other hashers if they are on trail ÿ it should be responded with either \\"On-On\\" or \\"Looking\\".\\r\\nSometimes there is a call to \\"circle up\\" ÿ this is a call from a leader for the hashers to form a circle, be quiet, and pay attention. Circles are called for the \\"chalk talk\\", to give news, or for some ceremony such as to thank the hare for the hash.\\r\\nEach group should explain their markings at the start of the trail (see \\"Chalk Talk\\" above\\"). Although not universal, there are several marks that are used on most standard running trails. Marks are most often made with flour (the kind used for baking) but other substances may be used such as chalk or colored powders.\\r\\nRules from group to group may vary.... Some examples of variations: some groups don't use the \\"F\\" mark or only use it after 5 or more spots. Others use the X as a false trail, but always after 2 blobs. The correct trail is recognised when the third blob in a row is reached after a check. For some groups an arrow is always true ÿ other treat it as another spot and therefore may be part of a false trail. These rules should be explained in the \\"chalk talk\\".\\r\\nThere are two types of trails. \\"live trails\\" are laid by hares who are given a head start, while \\"dead trails\\" are pre-laid hours or days before the hash begins. Live trails and dead trails are also known as \\"live hare\\" and \\"dead hare\\" trails, respectively. Live trails are closer to the original \\"hare and hound\\" tradition, with the intent of the pack being to catch the hare rather than making it to the end, and are more common in the United States, while the rest of the world tends toward dead trails.\\r\\nA trail may be \\"A to A,\\" where the trail returns to the start, or \\"A to B,\\" where the beginning and end of the trail are widely separated. Some trails are referred to as \\"A to A1 (prime)\\", denoting an ending point that is close to (usually short walking distance), but not the same as the start. There is also \\"B to A\\" which the participants are ferried to another location for the run back to the gathering point.\\r\\nMost hash events end with a group gathering known as the \\"circle\\", or less commonly as \\"religion\\". Led by chapter leadership, the circle provides a time to socialize, sing drinking songs, recognize individuals, formally name members, or inform the group of pertinent news or upcoming events. Circles may be led by the chapter grandmaster, the group's religious advisor, or by a committee. Impromptu input is welcome and/or solicited.\\r\\nA \\"down-down\\" is a means of punishing, rewarding, or merely recognizing an individual for any action or behaviour according to the customs or whims of the group. Generally, the individual in question is asked to consume without pause the contents of his or her drinking vessel or risk pouring the remaining contents on his or her head. Individuals may be recognized for outstanding service, or for their status as a visitor or newcomer. Down-downs also serve as punishment for misdemeanours real, imagined, or blatantly made up. Such transgressions may include: failing to stop at the beer check, pointing with a finger, or the use of real names. Commonly, hashers who wear new shoes to an event can be required to drink from that shoe.\\r\\nMany chapters include an ice seat or throne as part of the down-down ceremony. Those who are to consume a down-down sit on a large block of ice while they await the completion of the down-down song. If the offence that resulted in the down-down is particularly egregious, the hasher may be subjected to a long song with many verses.\\r\\nIn most chapters, the use of real names during an event is discouraged. Members are typically given a \\"hash name,\\" usually in deference to a particularly notorious escapade, a personality trait, or their physical appearance. In some chapters the name must be earned ÿ that is, hashers are not named until they've done something outstanding, unusual, or stupid enough to warrant a name. In other chapters the process is more mechanical and hashers are named after completing a certain number of events (5ÿ10 being the most common).\\r\\nSome chapters focus on \\"family-friendly\\" names (for example: Lost My Way); others focus on names filled with innuendo (for example: Salt Lick); and some go out of their way to make the name as bawdy, offensive, or politically incorrect as possible.\\r\\nThose hashers who have not been named are generally referred to as \\"Just (Name)\\", \\"No Name (Name)\\" (e.g., \\"No Name John\\") or simply Virgin.\\r\\nHashers are not permitted to give themselves nicknames due to the obvious conflict of interest. Hashers who do so are often renamed by the chapter at the earliest opportunity and with a more offensive name. Similarly, hashers who do get named and don't like their name may end up being renamed by their chapter, the members of whom may strive to give the complaining hasher an even more offensive or inappropriate name.\\r\\nNew hashers verbally in pursuit of an obviously offensive or inappropriate name may intentionally be given a weaker name, such as \\"freckles.\\"\\r\\nThe traditional symbol of hashing is the outline of a human foot, or a pair, often including the phrase \\"On-On\\". T-shirts are a common symbol of various hash clubs, and events. A large sample is available in the Digital Hash T-shirt Museum[11]\\r\\nHashers occasionally wear specialized clothing on trail or to the closing circles. Common items include thick, knee high socks, commonly referred to as \\"Shiggy Socks\\", kilts, or happi coats, with some kennels offered \\"earned\\" clothing such as bibs or sashes. Shiggy socks are worn to protect the shins and knees of the wearer from thorns, mud, branches, or whatever else they run through. The hash has its own tartan[12] for their kilts. Custom happi coats, originating out of Japan, are also commonly seen and made to reflect the local kennel.\\r\\nThere are several international events, where hashers from different groups get together to run and socialize, but the most famous is the biennial Interhash, where hashers from around the world gather. The 2006 InterhashChiang Mai, offered supporting runs in Thailand, Myanmar, Laos, Vietnam, Cambodia, and southwest China. The 2018 event will be in the weekend of 27th May on the Fiji Islands.\\r\\nIn addition to Interhash, there are also many regional and continental hash events, such as the InterAmericas, InterAfrica, InterGulf, InterScandi, EuroHash and PanAsia. National hash events, or \\"nash hashes\\", primarily bring together hashers from one particular nation, although visitors from other countries are actively welcomed.","input":"What are the hash house harriers running club?"},{"output":"Alex O'Loughlin","context":"\\r\\n\\r\\nAlex O'Loughlin /o??l?kl?n/ (born 24 August 1976) is an Australian actor, writer and director, who plays Lieutenant Commander Steve McGarrett on CBS' remake of the TV series Hawaii Five-0. He had starring roles in the films Oyster Farmer (2004) and The Back-up Plan (2010), as well as on such television series as Moonlight (2008) and Three Rivers (2009).\\r\\n\\r\\nO'Loughlin was born on 24 August 1976,[1][2] in Canberra, Australia, of Irish and Scottish descent.[3] His father is a physics and astronomy teacher in Sydney and his mother is a nurse.[4][5]\\r\\n\\r\\nO'Loughlin suffered from obsessive-compulsive disorder when he was a child.[6] He enrolled at the National Institute of Dramatic Art (NIDA) in Sydney in 1999 and graduated in June 2002 after completing a three-year, full-time Bachelor of Dramatic Art program.[7][8]\\r\\n\\r\\nO'Loughlin began working in short films and fringe theatre as a teenager in Sydney. One of his first acting jobs was an extra in a commercial, playing a Marine.[9] After graduating from NIDA (National Institute of Dramatic Arts), he began his career in Australian television and film productions. Some of his TV credits include roles in BlackJack: Sweet Science, Love Bytes and White Collar Blue.[9]\\r\\n\\r\\nIn 2004 he landed his first film role, the lead in Oyster Farmer.[10] He later appeared in Man-Thing, Feed, and in the Australian mini-series The Incredible Journey of Mary Bryant, for which he was nominated as Best Lead Actor in Television from the Australian Film Institute Awards in 2005 and as Most Outstanding Actor in a Drama Series from the Logie Awards in 2006.[11][12] O'Loughlin joined the cast of The Shield in 2007 as Detective Kevin Hiatt, the newest member of the strike team.\\r\\n\\r\\nIn 2005, he screen-tested for the movie role of James Bond. As he told one interviewer: \\"I met with [director] Martin Campbell here in Los Angeles at his office on the Sony [Pictures] lot and he asked me to fly to London and test and we tested at Pinewood [Studios]. It was the biggest screen test I've ever done. It was very comprehensive. I had tuxedos and suits cut for it and hair cuts.\\"[13]\\r\\n\\r\\nHe left The Shield in 2007 after he won the lead role on the CBS series Moonlight, where he played private investigator and vampire Mick St. John.[7][14] The filming of Moonlight was interrupted by the Hollywood writers strike. There was speculation that the series would be dropped but fan pressure prevailed and the show was given four additional episodes to try to regain its audience share. Despite being the highest rated show in its Friday evening slot[citation needed] CBS did not commission a second season. After its cancellation, a fan-based charity campaign to win a second-season renewal for Moonlight by holding blood drives proved unsuccessful.[15]\\r\\n\\r\\nIn August 2008, CBS signed a talent development deal with O'Loughlin as the star of a TV series to be developed by writer Mark Gordon, but it did not materialise.[16] He was cast in the lead role of the CBS hospital drama Three Rivers, developed by producer Carol Barbee, which aired Sunday evenings in the 2009ÿ2010 season.[17]\\r\\n\\r\\nIn April 2009, he guest-starred in an episode of Criminal Minds in the Season Four episode, \\"The Big Wheel\\", as an OCD-ridden serial killer, Vincent Rowlings. In December 2009 CBS pulled Three Rivers from its schedule.[18] O'Loughlin starred alongside Jennifer Lopez in the 2010 romantic comedy film The Back-up Plan.[19]\\r\\n\\r\\nO'Loughlin was cast in the CBS remake of Hawaii Five-0 portraying Lieutenant Commander Steve McGarrett,[20] which premiered on 20 September 2010. The show won \\"Favorite New TV Drama\\" at the 2010ÿ11 People's Choice Awards. Subsequently, BuddyTV ranked him # 2 on its \\"TV's 100 Sexiest Men of 2010\\" list[21] and # 9 in 2011.[22]\\r\\n\\r\\nOn 2 March 2012, CBS announced that O'Loughlin would miss shooting some episodes of Hawaii Five-0 to seek drug treatment related to pain management medication prescribed after a shoulder injury. He was slated to miss at least one episode from the second season.[23][24]\\r\\n\\r\\nIn August 2013, the Los Angeles-based non-profit Australians in Film announced that O'Loughlin would be honored with a Breakthrough Award at a ceremony held on 24 October 2013 in Los Angeles.[25]\\r\\n\\r\\nO'Loughlin's first child, a son, was born in 1997, to a girlfriend from whom he has since separated.[26] He separated from his long-time girlfriend, Australian actress/singer Holly Valance, in February 2009.[7][13][27] He and his then-girlfriend,  Malia Jones have a son born in 2012.[28][29]  O'Loughlin and Jones married in Hawaii on 18 April 2014.[30] They and their  three sons (O'Loughlin's son, O'Loughlin and Jones' son, and Jones' son from a previous relationship) live in Hawaii where he films Hawaii Five-0. He is the current ambassador for Donate Life America.[31]","input":"Who is the actor that plays steve mcgarrett?"},{"output":"after seven to ten days","context":"A kitten, also known as a kitty or kitty cat, is a juvenile cat. After being born, kittens are totally dependent on their mother for survival and they do not normally open their eyes until after seven to ten days. After about two weeks, kittens quickly develop and begin to explore the world outside the nest. After a further three to four weeks, they begin to eat solid food and grow adult teeth. Domestic kittens are highly social animals and enjoy human companionship.\\r\\n\\r\\n\\r\\nThe word \\"kitten\\" derives from the Middle English word kitoun, which in turn came from the Old French chitoun or cheton.[1] Juvenile big cats are called \\"cubs\\" rather than kittens; either term may be used for the young of smaller wild felids, such as ocelots, caracals and lynx, but \\"kitten\\" is usually more common for these species.[2]\\r\\nA feline litter usually consists of two to five kittens[3] born after a gestation lasting between 64 and 67 days, with an average length of 66 days.[3] Kittens emerge in a sac called the amnion, which is bitten off and eaten by the mother cat.[4]\\r\\nFor the first several weeks, kittens are unable to urinate or defecate without being stimulated by their mother.[5] They are also unable to regulate their body temperature for the first three weeks, so kittens born in temperatures less than 27?C (81?F) can die from hypothermia if their mother does not keep them warm.[6] The mother's milk is very important for the kittens' nutrition and proper growth. This milk transfers antibodies to the kittens, which helps protect them against infectious disease.[7] Newborn kittens are unable to produce concentrated urine, and so have a very high requirement for fluids.[8] Kittens open their eyes about seven to ten days after birth. At first, the retina is poorly developed and vision is poor. Kittens are not able to see as well as adult cats until about ten weeks after birth.[9]\\r\\nKittens develop very quickly from about two weeks of age until their seventh week. Their coordination and strength improve. They play-fight with their litter-mates and begin to explore the world outside the nest or den. They learn to wash themselves and others as well as play hunting and stalking games, showing their inborn ability as predators. These innate skills are developed by the kittens' mother or other adult cats, who bring live prey to the nest. Later, the adult cats demonstrate hunting techniques for the kittens to emulate.[10] As they reach three to four weeks old, the kittens are gradually weaned and begin to eat solid food, with weaning usually complete by six to eight weeks.[11] Kittens generally begin to lose their baby teeth around three months of age, and have a complete set of adult teeth by nine months.[12] Kittens live primarily on solid food after weaning, but usually continue to suckle from time to time until separated from their mothers. Some mother cats will scatter their kittens as early as three months of age, while others continue to look after them until they approach sexual maturity.[13]\\r\\nThe sex of kittens is usually easy to determine at birth. By six to eight weeks they are harder to sex because of the growth of fur in the genital region. The male's urethral opening is round, whereas the female's urethral opening is a slit. Another marked difference is the distance between anus and urethral opening, which is greater in males than in females.[14]\\r\\nKittens are highly social animals and spend most of their waking hours interacting with available animals and playing on their own. Play with other kittens peaks in the third or fourth month after birth, with more solitary hunting and stalking play peaking later, at about five months.[15]\\r\\nKittens are vulnerable because they like to find dark places to hide, sometimes with fatal results if they are not watched carefully.[16]\\r\\nAlthough domestic kittens are commonly sent to new homes at six to eight weeks of age, it has been suggested that being with their mother and litter-mates from six to twelve weeks is important for a kitten's social and behavioural development.[15] Usually, breeders and foster/rescue homes will not sell or adopt out a kitten that is younger than twelve weeks. In many jurisdictions, it is illegal to give away kittens younger than eight weeks of age.[17] Kittens generally reach sexual maturity at around seven months old. A cat reaches full \\"adulthood\\" around one year of age.[18]\\r\\nDomestic kittens in developed societies are usually vaccinated against common illnesses from two to three months of age. The usual combination vaccination protects against feline viral rhinotracheitis (FVR), feline calicivirus (C), and feline panleukopenia (P). This FVRCP inoculation is usually given at eight, twelve, and sixteen weeks, and an inoculation against rabies may be given at sixteen weeks. Kittens are usually spayed or neutered at seven months of age, but kittens may be neutered as young as seven weeks (if large enough), especially in animal shelters.[19] Such early neutering does not appear to have any long-term health risks to cats, and may even be beneficial in male cats.[20] Kittens are commonly wormed against roundworms from about four weeks.[21]\\r\\nKittens require a high-calorie diet that contains more protein than the diet of adult cats.[22] Young orphaned kittens require cat milk every two to four hours, and they need physical stimulation to defecate and urinate.[5] Cat milk replacement is manufactured to feed to young kittens, because cow's milk does not provide all the necessary nutrients.[23] Human-reared kittens tend to be very affectionate with humans as adults and sometimes more dependent on them than kittens reared by their mothers, but they can also show volatile mood swings and aggression.[24] Depending on the age at which they were orphaned and how long they were without their mothers, these kittens may be severely underweight and can have health problems later in life, such as heart conditions. The compromised immune system of orphaned kittens (from lack of antibodies found naturally in the mother's milk) can make them especially susceptible to infections, making antibiotics a necessity.[25]","input":"How long does it take for a kitten's eyes to open after birth?"},{"output":"intermediate adult ranking","context":"The Brazilian jiu-jitsu ranking system is a means of signifying a practitioner's increasing levels of technical knowledge and practical skill within the art of Brazilian jiu-jitsu. Colored belts that are worn as part of the uniform are awarded to the practitioner. While the ranking system's structure shares its origins with the judo ranking system and that of all colored martial arts belts, the Brazilian jiu-jitsu ranking system grew to incorporate unique aspects and themes. The system has minor differences from Judo in areas such as a division between youths and adults and the issuance of stripes and degrees. Some distinct differences have become synonymous with the art, such as a marked informality in promotional criteria, a focus on a competitive demonstration of skill, and a conservative approach to promotion.[1][2]\\r\\n\\r\\n\\r\\nIn 1907, Kan Jigor, the founder of judo, introduced the use of belts (obi) and gi (judogi) in the martial arts, replacing the practice of training in formal kimonos.[3] In 1914, Kan's pupil Mitsuyo Maeda arrived in Brazil, a journey which led to the development of Brazilian jiu-jitsu. At the time Kan used only white and black belts, with white representing the beginner, as a color of purity and simplicity, and black being the opposite, representing one who is filled with knowledge.[3]\\r\\nSome believe that Mikonosuke Kawaishi was the first to introduce additional colors in 1935 when he began teaching Judo in Paris, ten years after Carlos Gracie opened his academy in Brazil. Kawaishi thought that a more structured system of colored belts would provide the student with visible rewards to show progress, increasing motivation and retention.[3] Since then, Brazilian jiu-jitsu, judo, and many other martial arts have adopted the use of colored belts to denote students' progression in the arts.[4]\\r\\nThe first official belt ranking system was created in 1967 by the Jiu-Jitsu Federation of Guanabara.[citation needed] Much of the current criteria and modern belt ranks were implemented by the Sport Jiu Jitsu International Federation (SJJIF) and International Brazilian Jiu-Jitsu Federation.\\r\\nWhite belt is the first belt within Brazilian jiu-jitsu. The rank is held by any practitioner new to the art and has no prerequisite.[1] Some instructors and other high-level practitioners think that a white belt's training should emphasize escapes and defensive positioning since a white belt will often fight from inferior positions, especially when training with more experienced practitioners.[5]\\r\\nMost academies will additionally require that a white belt level practitioner works to obtain a well-rounded skills set, with a knowledge of basic offensive moves, such as common submissions and guard passes.[6][7]\\r\\nBlue belt is most often the second adult rank in the Brazilian jiu-jitsu.[1] At the blue belt level, students gain a wide breadth of technical knowledge and undertake hundreds of hours of mat time to learn how to implement these moves efficiently.[6] Blue belt is often the rank at which the student learns a large number of techniques.[9]\\r\\nNot all Brazilian jiu-jitsu schools or regulatory bodies award the blue belt as the second adult belt.[10] Although many Brazilian jiu-jitsu organizations adhere to the IBJJF standard of awarding the yellow, orange, and green belt exclusively as part of a youth belt system (under 16 years of age), some supplement the time between white belt and blue belt with one or more belts of these colors.[11][12]\\r\\nThe IBJJF requires that a practitioner be at least 16 years old to receive a blue belt, thereby officially entering into the adult belt system.\\r\\nPurple belt is the intermediate adult ranking in Brazilian jiu-jitsu.[1] The purple belt level practitioner has gained a large amount of knowledge, and purple belts are generally considered qualified to help instruct lower-ranked students.\\r\\nThe IBJJF requires students to be at least 16 years old and recommends they have spent a minimum of two years ranked as a blue belt to be eligible for a purple belt, with slightly different requirements for those graduating directly from the youth belts.[1]\\r\\nAside from the exceptional belts awarded at the highest levels, brown belt is the highest ranking color belt in Brazilian jiu-jitsu.[1] Brown belt typically requires at least five years of dedicated training to achieve.[2] It is often thought of as a time for refining techniques.[13]\\r\\nThe IBJJF requires that students be at least 18 years old and recommends they have spent a minimum of 18 months as a purple belt to be eligible for a brown belt.[1]\\r\\nAs with many other martial arts, the black belt is the highest common belt within Brazilian jiu-jitsu, denoting an expert level of technical and practical skills.[1] Brazilian jiu-jitsu black belts are often addressed within the art as professor, although some schools and organizations reserve this honorific for more senior black belts holders.[15][16][17]\\r\\nThe IBJJF requires that a student be at least 19 years old and recommends they have spent a minimum of 1 year ranked as a brown belt to be eligible for a black belt. The black belt itself has six different degrees of expertise.[1]\\r\\nWhen a Brazilian jiu-jitsu black belt reaches the seventh degree, he or she is awarded an alternating red-and-black belt similar to the one earned at the sixth degree in Judo.[1][3] This belt is commonly known as a coral belt.[18][19] Coral belts are very experienced practitioners, most of whom have made a large impact on Brazilian jiu-jitsu, and are often addressed within the art by the title master.[15][16][17]\\r\\nThe International Brazilian jiu-jitsu Federation in 2013, amended the graduation guidelines with respect to the transition between seventh degree and eighth degree black belt. In short, a practitioner who has achieved the rank of 8th degree black belt will wear a red and white belt,[20] which is also commonly called a coral belt.[21]\\r\\nAccording to Renzo and Royler Gracie, in Brazilian jiu-jitsu the red belt is reserved \\"for those whose influence and fame takes them to the pinnacle of the art\\".[2] It is awarded in lieu of a ninth and tenth degree black belt. If a practitioner receives his or her black belt at 19 years old, the earliest they could expect to receive a ninth degree red belt would be at the age of 67.[1] Brazilian jiu-jitsu red belt holders are often addressed within the art by the title grandmaster. The 10th degree was given only to the pioneers of Brazilian Jiu-Jitsu, the Gracie brothers: Carlos, Oswaldo, George, Gaston and Helio.[15][16][17][22]\\r\\nFew published guidelines or standards determine when a practitioner is ready for promotion; the criterion is generally determined by individual instructors and/or academies.[23][24] The IBJJF maintains an extensive graduation system that takes into account time-in-grade and membership standing, but makes no mention of specific performance or skill requirements.[1] When instructors or academies comment on the criteria for promotion, the most widely accepted measures are the amount of technical and conceptual knowledge a practitioner can demonstrate, and;[25] performance in grappling (randori) within the academy and/or competition.[26]\\r\\nTechnical and conceptual knowledge are judged by the number of techniques a student can perform, and the level of skill with which they are performed in live grappling, allowing smaller and older practitioners to be recognized for their knowledge, although they may not be the strongest fighters in the school. Brazilian jiu-jitsu is a distinctly individual sport, and practitioners are encouraged to adapt the techniques to their body type, strategic preferences, and level of athleticism. The ultimate criterion for promotion is the ability to execute the techniques successfully, rather than strict stylistic compliance.[2]\\r\\nBrazilian jiu-jitsu has had an informal approach to belt promotions, in which one or more instructors subjectively agree that a given student is ready for the next rank.[2][24] In recent years[when?] some academies have moved toward a more systematic, formal testing approach, especially true for lower ranked students, where the decision to promote is arguably the least contentious.[27] One of the first instructors to publicly publish formal testing criteria was Roy Harris, who has formalized his promotion tests from white belt to black belt.[28] Formal testing is now becoming commonplace in many Gracie Academies and organizations such as Alliance.[7]\\r\\nSome Gracie systems have introduced formal online testing where the student can upload his or her qualification videos to qualify for promotion.[29] Formal tests are generally based around the same elements as a normal promotion, such as the student's technical and conceptual knowledge and the ability to apply those techniques against a resisting opponent. Some tests take other aspects, such as a student's personal character or a basic knowledge of the history of the art, into account.[23] Formal testing may require the payment of testing fees and a require a minimum of pre-testing private lessons with the instructor.[6]\\r\\nStudents are generally encouraged to compete, as this can help them gain experience. Competition allows instructors to gauge students' abilities while grappling with a fully resisting opponent, and it is common for a promotion to follow a good competition performance. In most academies, competing is not essential for promotion, but in a minority of schools, competing is not only endorsed but is required.[25]\\r\\nIn addition to the belt system, many academies award stripes as a form of intra-belt recognition of progress and skill.[30] The cumulative number of stripes earned serves as an indication of the student's skill level relative to others within the same belt rank.[31] Stripes may consist of small pieces of cloth sewn onto the sleeve of the belt, or simple pieces of tape applied to it. Although the exact application, such as the number of stripes allowed for each belt, varies between institutions, the IBJJF sets out a general system under which four stripes can be added before the student may be considered for promotion to the next bjj belt.[1] Stripes are only used for ranks prior to black belt. After black belt is achieved, the markings are known as degrees and are awarded formally. Time-in-grade and skill level are both important factors. Stripes are not used in every academy and, where they are used, they may not be applied consistently.[1]\\r\\nA tradition practiced in some Brazilian jiu-jitsu schools immediately after a promotion, is known as \\"running the gauntlet\\" (\\"passar no corredor\\" in Portuguese).[32] This generally follows one of two basic patterns. The newly promoted student is hit on their back with beltsonce by each of their fellow practitionersas he or she walks or runs past (\\"faixada\\" in Portuguese), or he or she may be thrown by each instructor and sometimes also by each student in the academy of equal or higher grade.[33] Advocates for the custom argue that \\"running the gauntlet\\" serves as a method of team building and reinforces camaraderie between classmates.[32]\\r\\nOther initiation customs may involve being hip-tossed by the instructor in a controlled manner.[citation needed]","input":"What level is purple belt in jiu jitsu?"},{"output":"sixteen","context":"This is a list of the sixteen counties in the U.S. state of Maine. Before statehood, Maine was officially part of the state of Massachusetts and was called the District of Maine. Maine was granted statehood on March 15, 1820 as part of the Missouri Compromise. Nine of the sixteen counties had their borders defined while Maine was still part of Massachusetts, and hence are older than the state itself.[1][page?needed] Even after 1820, the exact location of the northern border of Maine was disputed with Britain, until the question was settled and the northern counties signed their final official form, the Webster-Ashburton Treaty, signed in 1842.[2] Almost all of Aroostook County was disputed land until the treaty was signed.[1][page?needed]\\r\\nThe first county to be created was York County, created as York County, Massachusetts by the government of the Massachusetts Bay Colony in 1652 to govern territories it claimed in southern Maine.[3] No new counties have been created since 1860, when Knox County and Sagadahoc County were created. The most populous counties tend to be located in the southwestern portion of the state, along the Atlantic seaboard. The largest counties in terms of land area are inland and further north. Maine's county names come from a mix of British, American, and Native American sources, reflecting Maine's pre-colonial, colonial, and national heritage.[1][page?needed]\\r\\nThe Federal Information Processing Standard (FIPS) code, which is used by the United States government to uniquely identify states and counties, is provided with each entry. Maine's code is 23, which when combined with any county code would be written as 23XXX. The FIPS code for each county links to census data for that county.[4]\\r\\nA song is taught to many elementary school children across the state, entitled the Maine County Song, to aid in memorizing the names of the state's 16?counties. It is sung to the tune of Yankee Doodle.\\r\\nAn alternate version as put forth by the Maine Secretary of State's Kids' Page:","input":"How many counties are there in the state of maine?"},{"output":"two terms (totaling eight years)","context":"A term of office is the length of time a person serves in a particular elected office. In many jurisdictions there is a defined limit on how long terms of office may be before the officeholder must be subject to re-election. Some jurisdictions exercise term limits, setting a maximum number of terms an individual may hold in a particular office..\\r\\n\\r\\n\\r\\nBeing the origin of the Westminster system, aspects of the United Kingdom's system of government are replicated in many other countries.\\r\\nThe monarch serves as head of state until his or her death or abdication.\\r\\nIn the United Kingdom Members of Parliament (MPs) in the House of Commons are elected for the duration of the parliament. Following dissolution of the Parliament, a general election is held which consists of simultaneous elections for all seats. For most MPs this means that their terms of office are identical to the duration of the Parliament, though an individual's term may be cut short by death or resignation. An MP elected in a by-election mid-way through a Parliament, regardless of how long they have occupied the seat, is not exempt from facing re-election at the next general election.\\r\\nThe Septennial Act 1715 provided that a Parliament expired seven years after it had been summoned; this maximum period was reduced to five years by the Parliament Act 1911. Prior to the Fixed-term Parliaments Act 2011 parliaments had no minimum duration. Parliaments could be dissolved early by the monarch at the Prime Minister's request. Early dissolutions occurred when the make-up of Parliament made forming government impossible (as occurred in 1974), or, more commonly, when the incumbent government reasoned an early general election would improve their re-election chances (e.g. 2001). The Fixed-term Parliaments Act 2011 mandated that Parliaments should last their full five years. Early dissolution is still possible, but under much more limited circumstances.\\r\\nBecause the government and Prime Minister are effectively indirectly elected through the Commons, the terms of Parliaments and MPs do not directly apply to offices of government, though in practice these are affected by changes in Parliament. While, strictly speaking, a Prime Minister whose incumbency spans multiple Parliaments only serves one, unbroken, term of office, some writers may refer to the different Parliaments as separate terms.[1]\\r\\nHereditary peers and life peers retain membership of the House of Lords for life, though members can resign or be expelled. Lords Spiritual hold membership of the House of Lords until the end of their time as bishops, though a senior bishop may be made a life peer upon the end of their bishopric (e.g. George Carey, made Baron Carey of Clifton the day after he ceased being Archbishop of Canterbury).\\r\\nThe devolved administrations in Scotland, Wales and Northern Ireland are variations on the system of government used at Westminster.\\r\\nThe office of the leader of the devolved administrations has no numeric term limit imposed upon it. However, in the case of the Scottish Government and the Welsh Assembly Government there are fixed terms for which the legislatures can sit. This is imposed at four years. Elections may be held before this time but only if no administration can be formed, which has not happened yet.\\r\\nOffices of local government other regional elected officials follow similar rules to the national offices discussed above, with persons elected to fixed terms of a few years.\\r\\nIn the United States, the president of the United States is elected indirectly through the United States Electoral College to a four-year term, with a term limit of two terms (totaling eight years) or a maximum of ten years if the president acted as president for two years or less in a term where another was elected as president, imposed by the Twenty-second Amendment to the United States Constitution, ratified in 1951.\\r\\nU.S. Representatives serve two-year terms. U.S. Senators serve six-years terms.\\r\\nFederal judges have different terms in office. Article I judges; such as those that sit on the United States bankruptcy courts, United States Tax Court, and United States Court of Appeals for the Armed Forces, and certain other federal courts and other forms of adjudicative bodies serve limited terms: The Court of Appeals for the Armed Forces for 15 years, bankruptcy courts for 14. However, the majority of the federal judiciary; Article III judges, such as those of the Supreme Court, courts of appeal, and federal district courts; serve for life.\\r\\nThe terms of office for officials in state governments varies according to the provisions of state constitutions and state law.\\r\\nThe term for state governors is four years in all states but Vermont and New Hampshire; the Vermont and New Hampshire governors serve for two years.\\r\\nThe National Conference of State Legislatures reported in January 2007 that among state legislatures [1]:\\r\\nAmong territories of the United States:\\r\\nMembers of Council of the District of Columbia serve a four-year term.\\r\\nAs a former British territory following the Westminster System, there are many similarities with the United Kingdom, although with some variations based on local customs, the federal system of government and the absentee monarch.\\r\\nBeing a Commonwealth realm, Canada shares a monarch with the United Kingdom and 14 other countries, who serves as head of state of all 16 realms until their death or abdication.\\r\\nThe Governor General is appointed by the monarch as his/her personal representative on the advice of the Prime Minister, and serves for an indefinite term, though the normal convention is 5 years. Similarly, the Lieutenant Governors, who represent the monarch at the provincial level, are appointed by the Governor General on the advice of the Prime Minister (usually also with consultation of the relevant provincial premier), and generally also serve 5 year terms by convention. The territories have Commissioners, who are not representatives of the monarch, but are instead appointed by and represent the Governor-in-Council (i.e. the federal cabinet), and conventionally serve for about 5 years.\\r\\nSimilar to the United Kingdom, MPs serve for the duration of the Parliament. They may resign before the end of a Parliament or be elected in by-elections during the middle of a Parliament.\\r\\nUnder the Constitution Act, 1867, a Parliament may last for a maximum of 5 years from the most recent election before expiring, although all Parliaments to date have been dissolved before they could expire. Bill C-16, introduced in the 39th Parliament, provided for fixed election dates every 4 years on the third Monday in October, beginning in 2009. However, the Prime Minister may still advise the Governor General to dissolve Parliament at any time.\\r\\nAs in the United Kingdom, the cabinet and head of government are indirectly elected based on the composition of the House of Commons, they are not technically affected by the terms of legislators or Parliaments. In practice however, the terms of government office holders are affected by changes in the House of Commons, and those who serve for multiple consecutive Parliaments are generally considered to have served a single term. The term of a government generally ends when it is defeated on a confidence matter or the governing party fails to gain enough seats in a general election.\\r\\nSenators are appointed to the Canadian Senate to represent a province by the Governor General of Canada on the advice of the Prime Minister, and serve until the mandatory retirement age of 75. Senators appointed before the passage of the British North America Act, 1965 served for life. Senators may also resign from office or be expelled from the Senate.\\r\\nProvincial legislatures and the legislature of the Yukon function very similarly to the federal House of Commons. MLAs (called MPPs in Ontario, MNAs in Quebec, and MHAs in Newfoundland and Labrador) serve for the duration of the legislature, though they may resign before the legislature is dissolved or be elected in by-elections between general elections. The legislatures of the Northwest Territories and Nunavut operate using a consensus model, but are similar otherwise. The premiers and their cabinets are selected in the same way as in the House of Commons, and like at the federal level, the term of a provincial government can be ended by defeat in a general election or the loss of the legislature's confidence.\\r\\nAll provincial legislatures except that of Nova Scotia have fixed-term election legislation in place, as does the legislature of the Northwest Territories. Premiers may also advise Lieutenant Governors to dissolve legislatures at any time before the prescribed election date.\\r\\nNumbers in years unless stated otherwise. Note that some countries where fixed-term elections are uncommon, the legislature is almost always dissolved earlier than its expiry date. \\"Until removed from office\\" refers to offices that don't have fixed terms; in these cases, the officeholder(s) may serve indefinitely until death, abdication, resignation, retirement, or forcible removal from office (such as impeachment).\\r\\nIn cases where the head of government is a different person from the head of state, its term of office is identical to the chamber that elected it (the legislature if it is unicameral, or most usually the lower house if it is bicameral), unless it doesn't survive a vote of no confidence.\\r\\n*Excludes senators for life.","input":"How long can an american president stay in office?"},{"output":"Raja Harishchandra","context":"\\r\\n\\r\\nRaja Harishchandra (lit.?King Harishchandra) is a 1913 Indian silent film, directed and produced by Dadasaheb Phalke. It is often considered the first full-length Indian feature film. Raja Harishchandra features Dattatraya Damodar Dabke, Anna Salunke, Bhalchandra Phalke, and Gajanan Vasudev Sane and is based on the legend of Harishchandra, recounted in the Ramayana and Mahabharata. The film, being silent, had English and Hindi language intertitles.\\r\\n\\r\\nPhalke decided to make a feature film after watching The Life of Christ (1906) at a theatre in Mumbai, formerly known as Bombay. He went to London for two weeks to learn filmmaking techniques and founded Phalke Films. He imported the hardware required for the filmmaking and exhibition from England, France, Germany, and the United States. Phalke shot a short film Ankurachi Wadh (Growth of a Pea Plant) to attract investors for his venture. He published advertisements in various newspapers calling for the cast and crew. As no women were available to play female leads, male actors performed the female roles. Phalke was in charge of scriptment, direction, production design, make-up, editing, along with film processing. Trymbak B. Telang handled the camera. Phalke completed filming in six months and 27 days producing a film of 3,700 feet, about four reels.\\r\\n\\r\\nThe film premiered at the Olympia Theatre, Mumbai on 21 April 1913, and had its theatrical release on Saturday, 3 May 1913 at the Coronation Cinema, Girgaon, Mumbai. It was a commercial success and laid the foundation for the film industry in the country. The film is partially lost; only the first and last reels of the film are preserved at the National Film Archive of India. Some film historians believe they belong to a 1917 remake of the film by Phalke titled Satyavadi Raja Harishchandra.\\r\\n\\r\\nThe status of Raja Harischandra as the first full-length Indian feature film has been debated. Some film historians consider Dadasaheb Torne's silent film Shree Pundalik, released on 18 May 1912, the maiden Indian film. The Government of India recognises Raja Harischandra as the first Indian feature film.\\r\\n\\r\\nKing Harishchandra (D. D. Dabke) is shown teaching his son, Rohitashva (Bhalchandra Phalke), how to shoot with a bow and arrow in the presence of Queen Taramati (Anna Salunke). His citizens ask the King to go on a hunting expedition. While on the hunt, he hears the cries of some women. He reaches a place where the sage Vishwamitra (Gajanan Sane) is performing the yajna to get help from Trigunashakti (three powers) against their will. The King unwittingly interrupts Vishvamitra in the midst of his yajna by releasing three powers. To appease his wrath, Harishchandra offers his kingdom. The King visits the royal palace and informs the Queen Taramati of the happenings. Vishwamitra sends Harishchandra, Taramati, and Rohitashva to arrange for the Dakshina. While in exile, Rohitashva dies. Harishchandra sends Taramati to ask the Domb King for a free cremation. On her way to meet the king, Vishwamitra frames Taramati for the murder of the prince of Kashi. Taramati faces trial, pleads guilty and is ordered to be beheaded by Harishchandra. When he raises his sword to complete his task, a pleased Lord Shiva appears. Vishwamitra reveals that he was examining Harishchandra's integrity, returns the crown to the King and rejuvenates Rohitashva.[1][2]\\r\\n\\r\\nOther artists in the film were:[3]\\r\\n\\r\\n?ÿ  Phalke on watching Jesus on the screen[4]\\r\\n\\r\\nOn 14 April 1911, Phalke with his elder son Bhalchandra went to see a film, Amazing Animals, at the America India Picture Palace,[5] Girgaon, Mumbai.[6] Surprised at seeing animals on the screen, Bhalchandra informed his mother, Saraswatibai, about his experience earlier that day. None of the family members believed them, so Phalke took his family to see the film the next day. As it was Easter, the theatre screened a film about Jesus, The Life of Christ (1906) by the French director Alice Guy-Blach instead.[4][6][a] While watching Jesus on the screen, Phalke envisioned Hindu deities Rama and Krishna instead and decided to start in the business of \\"moving pictures\\".[6] After completing his two-week trip to London to learn filmmaking techniques, he founded \\"Phalke Films\\" on 1 April 1912.[7]\\r\\n\\r\\nDuring his London trip, Phalke had placed an order for a Williamson camera and Kodak raw films and a perforator which reached Mumbai in May 1912.[8][9] He set up a processing room and taught his family to perforate and develop the film.[8] Though Phalke was certain of his idea of filmmaking, he could not find any investors. So, he decided to make a short film to demonstrate the techniques. He planted some peas in a pot, placed a camera in front of it, and shot one frame a day for over a month. This resulted in a film, lasting just over a minute, of the seed growing, sprouting, and changing into a climber. Phalke titled this short film Ankurachi Wadh (Growth of a Pea Plant) and showed it to selective individuals. Some of them, including Yashwantrao Nadkarni and Narayanrao Devhare, offered Phalke a loan.[9][10]\\r\\n\\r\\nIn his Marathi language magazine Suvarnamala, Phalke had published a story Surabaichi Kahani (A Tale of Sura). The story, which depicted the ill effects of alcoholism, was the first he considered for filming. After watching several American films screened in Mumbai, he observed they included mystery and romance which the audiences liked. Family members suggested the storyline should appeal to middle class people and women and should also highlight Indian culture.[11]\\r\\n\\r\\nAfter considering various stories depicted in Hindu mythology, Phalke's family shortlisted legends of Krishna, Savitri and Satyavan, and Harishchandra.[8] At the time, a play based on the legends of Harishchandra was popular on Marathi and Urdu stages.[12] Friends and neighbours had often called Phalke \\"Harishchandra\\" for having sold all his belongings, except his wife's Mangala sutra, to fulfil his filmmaking dream.[10] So, Phalke decided on the legends of Harishchandra and wrote the script for his feature film.[12]\\r\\n\\r\\n?ÿ  Casting call published in various newspapers[13]\\r\\n\\r\\nPhalke published advertisements in various newspapers like Induprakash calling for the cast and crew required for the film. It was well-received and huge number of applicants came in for the auditions. Since he was making a silent film, Phalke allowed mute artists to audition. Despite a growing response to the advertisement, he was not satisfied with the performers' skills. He discontinued the advertisements and decided to scout for the artists through theatre companies.[13]\\r\\n\\r\\nPadurang Gadhadhar Sane and Gajanan Vasudev Sane were among the first artists to join Phalke Films.[14] The former was playing female roles in the Natyakala theatre company; the latter was performing in Urdu plays.[13] Both joined for a salary of ? 40 per month. Gajanan Sane introduced his acquaintance Dattatraya Damodar Dabke. Phalke was impressed with his physique and personality and offered him the lead role of Harishchandra.[14]\\r\\n\\r\\nIn response to the advertisement, four prostitutes auditioned for the role of Taramati. Phalke rejected them for not having satisfactory looks and revised the advertisement to read: \\"Only good-looking women should come for interview.\\"[14] Two more prostitutes auditioned but left after two days. A young lady with \\"passable appearance\\", who was a mistress, auditioned and Phalke selected her for the female lead. She rehearsed for four days. However, on the fifth day, her master objected to her working in the film and took her away.[14] In despair, Phalke also visited Mumbai's red-light area on Grant Road in Kamathipura. He was asked either to pay a high salary or to marry the woman.[15] One day, while having tea at a restaurant on Grant Road[15] Phalke noticed Krishna Hari alias Anna Salunke, an effeminate young man with slender features and hands.[16] Salunke was working as a cook or waiter at the restaurant on a monthly salary of ?10.[16][17] He agreed to work in films when Phalke offered him a raise of five rupees.[18][b]\\r\\n\\r\\nPhalke auditioned many boys for the role of Rohitashva, son of Harishchandra and Taramati, but none of the parents would allow their children to work in the film as the character would have to live in the forests and was to die. Finally, Phalke's elder son Bhalchandra was assigned the role.[14] He became the first child actor in Indian cinema.[20]\\r\\n\\r\\nPhalke hired around forty people for his film studio known as a factory in those days.[15][21]Since working in films was a taboo, Phalke advised his artists to tell others they were working in the factory of a man named Harishchandra.[22] Phalke watched several foreign films to learn about screenplay writing and then completed the script for Raja Harishchandra.[23] The film had an all-male cast as no woman was available to play female leads.[24] After coming to the studio, male actors playing female roles were asked to wear saris and do women's chores like sifting rice, and making flour to help Phalke's wife, Saraswatibai.[25] Though some actors were associated with theatres, most of the cast did not have any prior acting experience. Phalke ran several rehearsals with the actors. Often, he had to wear a sari himself and act out the scene.[10] A number of photographs from English periodicals showing various facial expressions where hung up in the rehearsal studio. All the actors had to go through a mandatory exercise where they were asked to make similar faces.[23]\\r\\n\\r\\nAbout the same time, the Rajapurkar Natak Mandali drama company visited Mumbai. Many of the company's shows were based on Hindu mythology. Phalke made the acquaintance of the company's proprietor, Babajirao Rane, and explained his idea of indigenous film production. Rane was impressed by the idea and offered his support by lending his actors and their costumes. Phalke decided to use  material like Harishchandra's crown, wig, swords, shields, and bows and arrows in the film.[26] Phalke's brother-in-law owned two drama companies, Belgaokar Natak Mandali and Saraswati Natak Mandali. He offered similar help, but Phalke politely declined as the majority of the cast and crew were finalised.[25] Phalke designed the costumes and stage scenes based on the paintings of Raja Ravi Varma and M. V. Dhurandhar. He painted the scenes for the palace, jungle, mountains, fields, caves himself on curtains. Painter Rangnekar was hired for the monthly salary of ?60.[27]\\r\\n\\r\\nPhalke imported the hardware required for the filmmaking and exhibition from England, France, Germany, and the United States from manufacturers including Houghton Butcher, Zeiss Tessar, and Path. This included negative and positive film stocks, cameras, lights, Film laboratory equipment, printing and editing machines, negative cutting tools, and film projectors.[28] He decided to take on the responsibility for the scriptment, direction, production design, make-up, editing, and film processing. He asked Trymbak B. Telang, his childhood friend from Nashik, to come to Mumbai. Telang was working as a priest at the Trimbakeshwar Shiva Temple. Phalke had taught him still photography as a childhood hobby.[26] After his arrival, Phalke trained Telang in the operation of the Williamson camera and made him the film's cinematographer.[29]\\r\\n\\r\\nProduction design for the film started after the monsoon season of 1912. While the sets were being erected at Phalke's bungalow at Dadar, an outdoor shooting was scheduled at Vangani, a village outside Mumbai.[27] Some of the male actors playing female roles, including Anna Salunke who was playing the female lead, were not ready to shave off their moustaches because it is considered one of the Hindu rituals to be performed after the death of a father.[10][30] Phalke persuaded the actors and their fathers to do so and the unit left for Vangani.[23][27]\\r\\n\\r\\nThe unit was lodged at the village temple and continued their rehearsals until Phalke arrived from Mumbai. The villagers were frightened to see the troupe of people wearing costumes, wielding swords, shields, and spears and practising the scenes. They informed the Patil (village headman) that dacoits (robbers) had entered the village. He immediately reported to the Faujdar (commander) who visited the temple.[27] The unit explained filmmaking to them, but the Faujdar did not believe their story and arrested everyone. When Phalke reached the village, he immediately met the Patil and the Fauzdar and explained to them again about cinema and filming and showed them the equipment. Without loading film into the camera, he asked his unit to enact one of the film's scenes and went through the motions of filming a scene. After viewing the scene, the Fauzdar understood Phalke's new venture and released everyone.[31]\\r\\n\\r\\nWhile playing with other kids, Bhalchandra fell on a rock and started bleeding from his head. Phalke treated him with a first aid kit, but he remained unconscious. Various unit members suggested that Bhalchandra should be taken to Mumbai for further treatment. Once he was completely recovered, shooting could be resumed. The scene that was to be filmed showed Rohitashva, Bhalchandra's character  dead on a funeral pyre. Resuming the outdoor shoot after Bhalchandra's recovery would have delayed the production and incurred costs. To avoid both, Phalke stoically decided to shoot the scene with the unconscious Bhalchandra.[10][31] As in the legends of Harishchandra, the King along with Taramati and Rohitashva visit Kashi.[32] It was financially challenging for Phalke to go to Kashi and shoot scenes there. So, he took his unit to Trimbakeshwar, where they camped for about a month and filmed the required scenes. Phalke used to develop the film at night for the scenes shot during the day. He would re-shoot the scenes next day if they were not of the desired quality. Filming was completed in six months and 27 days to produce a film of 3,700 feet, about four reels.[31]\\r\\n\\r\\nPhalke used trick photography to shoot one of the scenes. The film negative stocks used were of limited spectral sensitivity with low sensitivity to the red band of the spectrum. Thus, sets, costumes, and artists' make-up artists avoided the colour red.[28] During the early nineteenth century, plays had an introductory episode, a compere or a person who introduces the performers in a show.[33] Unit members suggested the film should also have similar introductory episode with Phalke and his wife playing the roles of Sutradhar and Nati. Phalke agreed to the idea but could not convince Saraswatibai to act in front of a camera. Finally, Padurang Gadhadhar Sane played the role of Nati.[34]\\r\\n\\r\\nPhalke had difficulties getting a theatre for screening as criticism of his work had already started. He decided to show the film to a select audience and arranged for a premiere at the Olympia Theatre, Mumbai on 21 April 1913 at 9:00?PM.[35] The invitees included Bhalchandra Bhatavdekar, R. G. Bhandarkar, Vima Dalal, Justice Donald, newspaper editors and representatives along with some intellectuals and prominent personalities from Mumbai.[36] As Phalke's infant daughter, Mandakini, was ill with pneumonia, his elder brother, Shivrampant, advised him to postpone the premiere to another day.[35] But, the invitations had already been sent and the theatre was available only on 21 April, Phalke could not change his decision.[36]\\r\\n\\r\\nBhatavdekar introduced the premiere acknowledging Phalke for his \\"daring\\". Justice Donald noted the film would help Europeans learn more about Hindu mythology. Anant Narayan Kowlgekar of Kesari in his review mentioned that \\"Phalke has grandly brought his skill to the notice of the world.\\"[37] The Times of India in their review noted the scenes depicted in the film are \\"simply marvellous\\" and \\"[I]t is really a pleasure to see this piece of Indian workmanship\\".[38] With the favourable reviews generated, Nanasaheb Chitre, Manager of Coronation Cinema, then called Coronation Cinematograph and Variety Hall, expressing his desire to screen the film.[39]\\r\\n\\r\\nThe film had its theatrical release on Saturday, 3 May 1913 at the Coronation Cinema, Girgaon. The show included a dance by Irene Delmar, a comic act by McClements, foot-juggling by Alexandroff, and Tip-Top comic items followed by the film.[20] The show's duration was one-and-half hours with four shows scheduled a day at 6:00?PM, 8:00?PM, 10:00?PM, and 11:45?PM. An advertisement for the film published in The Bombay Chronicle had a note added at the end that the ticket rates would be double the usual rates.[39] The film had an overfull run for a week, and it was extended for twelve more days. A special show was scheduled on 17 May for women and children only at half rates. Initially, 18 May was advertised as the last show, but the film continued its run due to popular demand.[40] It ran continuously for twenty-three days until 25 May and was re-run at the Alexandra Theatre on 28 June.[5][20][40]\\r\\n\\r\\nNews of the film's success in Mumbai spread across India and there were demands to screen the film in various locations. As there were no film distributors in those days, Phalke had to move the film, the projector, an operator, and some assistants from place to place. When the film was screened for a week at the Nawabi Theatre in Surat, Phalke signed a temporary agreement for 50% partnership with the theatre owner. Despite advertising the film, at its first showing the film met with a lukewarm response. Disappointed by the earnings of only ? three, the owner asked Phalke either to cancel the show, increase its length, or reduce ticket prices. Phalke politely rejected  these suggestions.[40] He issued an advertisement in the Gujarati language calling on people to see \\"57,000 photographs of three-quarters of an inch width and two miles length\\", for just one Indian anna. He also had his actors enact some of the film's scenes at the town's crossroads. The promotion had the desired effect and earnings increased to ? 300.[21][41] Later, the film was also screened at Pune, Colombo, London, and Rangoon.[39][21][42] The Bombay Chronicle in its issue of 5 May 1913 mentioned that \\"this wonderful drama is splendidly represented by the company of actors\\" and praised the \\"beauty and ingenuity\\" with which Phalke succeeded in presenting the film scenes.[38]\\r\\n\\r\\nFilm historian Firoze Rangoonwalla feels that the film made \\"a wide impression and appealed to a large audience in difference places\\" and its box office success provided \\"the seal of acceptance and laid the foundation of the film industry\\" in the country.[20] Director and cinematographer Govind Nihalani explains that the film was shot partly outdoors in direct sunlight and partly in outdoor studios with sunlight blocked by white muslin producing soft and diffused light. He appreciates the tonal gradation, lighting, and camera movements. He also notes the scene where the God appears and disappears from behind the smoke of sage Vishvamitra's Yajna-kund gives an impression that the scene was filmed in a single shot.[28] Film critic Satish Bahadur points out that though the title cards in the film were in English and Hindi, \\"there was something unmistakably Maharashtrian\\" in the film. He also mentions that the interior architecture and dresses of countries in the film are more of a Deccan Peshwai style than North Indian.[9] Ashish Rajadhyaksha in his The Phalke Era: Conflict of Traditional Form and Modern Technology (1993) mentions that the film's narrative style was borrowed from painting, theatre and traditional arts to attract the audience into cinema.[43] Dilip Rajput of the National Film Archive of India notes that the film's scenes appear to run faster because of the current projector speed of 24-frames-per-second as compared to 16 to 18-frames-per-second speed of the projector that was used for the film.[1]\\r\\n\\r\\nDirected by Paresh Mokashi, the 2009 Marathi film Harishchandrachi Factory (Harishchandra's Factory) depicts the making of Raja Harishchandra. The film won the National Film Award for Best Feature Film in Marathi at the 56th National Film Awards.[44] It was selected as India's official entry to the 82nd Academy Awards in the Best Foreign Language Film category along with the 62nd British Academy Film Awards and the 66th Golden Globe Awards but was not listed among the final five nominations.[45]\\r\\n\\r\\nThe original length of a film was 3,700 feet, about four reels.[31] In 1917, the film's last print caught fire due to the constant friction and the exposure to high temperatures while it was being transported from one theatre to another, by a bullock cart. Phalke readily re-shot the film to produce the version that exists today.[46][47] Only the first and last reels of the original film are preserved at the National Film Archive of India (NFAI) making it a partially lost film. Some film historians believe they actually belong to a 1917 remake of the film, titled Satyavadi Raja Harishchandra.[48][49] NFAI duplicated the film but around twenty percent of the left side of the screen was lost in the transfer.[50] It was believed the film's remaining reels were destroyed along with 1,700 nitrate-based films in the fire at the Film and Television Institute of India on 8 January 2002.[51] The prints were later retrieved from the private collection of Phalke's children.[52][53] The NFAI has restored and digitized the film.[54]\\r\\n\\r\\nThe status of Raja Harischandra as the first full-length Indian feature film has been argued over. Some film historians consider Dadasaheb Torne's silent film Shree Pundalik as the maiden Indian film.[55][56] Torne's film was released on 18 May 1912 at the same theatre as Raja Harischandra.[3][57] An argument has also been made in favour of Raja Harischandra that Shree Pundalik is a cinematographic recording of a play, using a single, fixed camera and it was filmed by a British cameraman with the film stock processed in London.[58][59][60][61] The Government of India recognises Raja Harischandra as the first Indian feature film.[62] In 1969, it introduced the Dadasaheb Phalke Award, the country's highest award in cinema,[63] to commemorate Phalke's contribution to Indian cinema.[64]","input":"What was the first full length indian feature film ever made?"},{"output":"April 1, 1996","context":"First Interstate Bancorp was a bank holding company based in the United States that was taken over in 1996 by Wells Fargo. Headquartered in Los Angeles, it was the nation's eighth largest banking company.[1]\\r\\nThe name (along with the company logo) has continued to be used in the banking world after the merger by First Interstate BancSystem who had been using the name under a franchise agreement since 1984.\\r\\n\\r\\n\\r\\nIn 1928, Amadeo Giannini, born in California to Italian immigrant parents, formed a holding company, the Transamerica Corporation, to consolidate his existing financial ventures, which began business with $1.1 billion in assets and both banking and non-banking activities. From the 1930s through the mid-1950s, Transamerica made a number of acquisitions of banks and other financial corporations throughout the western United States, creating the framework for the later First Interstate system.\\r\\nIn 1953, regulators succeeded in forcing the separation of Transamerica Corporation and Bank of America under the Clayton Antitrust Act. Transamerica Corporation, a Delaware corporation, petitioned this court to review an order of the Board of Governors of the Federal Reserve System entered against it under Section 11 of the Clayton Act, 15 U.S.C.A.  21, to enforce compliance with Section 7 of the Act, 15 U.S.C.A.  18.[2]\\r\\nThe Bank Holding Company Act of 1956 placed new restrictions on companies such as Transamerica. Thus Transamerica's banking operations, which included 23 banks in 11 western states, were spun off as Firstamerica Corporation in 1958.[3] Transamerica continued to pursue its insurance and other operations.\\r\\nFirstamerica changed its name to Western Bancorporation in 1961. Western expanded steadily in the 1960s, both domestically and overseas, ending the decade with assets of more than $10 billion. The bank's financial services network grew through the 1974 founding of the Western Bancorporation Mortgage Company and the 1979 formation of Western Bancorp Venture Capital Company.\\r\\nDuring the 1960s, 1970s, and 1980s the bank operated under the name \\"United California Bank\\" (UCB). In the early 1970s, noticing Bank of America's (BofA) successful credit card BankAmericard, UCB decided to offer its own card which would be issued locally by individual banks under the name \\"Master Charge,\\" Later, when BofA spun off its franchised credit card operations to a separate organization named Visa International and changed the card's name to \\"Visa,\\" UCB did the same thing, spinning off Master Charge to Master Card International and changing the name to MasterCard.\\r\\nIn 1970, their affiliated bank, United California Bank of Basel, Switzerland collapsed after unauthorized trades in cocoa and silver futures. Several of the bank's officers, including President Paul Erdman spent time in jail on fraud charges.[4]\\r\\nIn June 1981 the company changed its name to First Interstate Bancorp.[5] The First Interstate name became a systemwide brand for most of the company's banks, thus promoting greater public recognition of the company and internal consistency. During the 1980s, in addition to acquiring more banks, First Interstate jumped into new areas of financial services as the deregulation of the banking industry progressed. In 1983 the First Interstate Discount Brokerage was set up to provide bank customers with securities and commodities support. In 1984 the bank branched into merchant banking with the purchase of Continental Illinois Ltd. and equipment leasing with the acquisition of the Commercial Alliance Corporation of New York, and broadened its mortgage banking activities by acquiring the Republic Realty Mortgage Corporation. In 1986 and 1987, First Interstate attempted a bold $3.2 billion hostile takeover of the ailing Bank of America, but the bid was successfully defeated.\\r\\nFirst Interstate ran into its own troubles in the late 1980s and early 1990s stemming from bad real estate loans and the severe recession in California. The bank posted losses in the hundreds of millions for 1987, 1989, and 1991. Consequently, First Interstate concentrated on rebuilding and rejuvenating its existing operations rather than acquiring new ones. A number of noncore unprofitable subsidiaries were jettisoned, including the equipment leasing unit, a government securities operation, and most of the wholesale banking unit. Rumors of a takeover of First Interstate were rife in the early 1990s before the bank recovered fully by mid-decade under the leadership of Chairman and CEO Edward M. Carson (1929ÿ2010).[6][7]\\r\\nDespite First Interstate's healthier condition, and with the banking industry consolidation in full swing, Wells Fargo made a hostile bid for First Interstate in October 1995 initially valued at $10.8 billion. Other banks came forward as potential 'white knights,' including Norwest Corporation, Bank One Corporation, and First Bank System. The latter made a serious bid for First Interstate, with the two banks reaching a formal merger agreement in November valued initially at $10.3 billion. But First Bank ran into regulatory difficulties with the way it had structured its offer and was forced to bow out of the takeover battle in mid-January 1996. Talks between Wells Fargo and First Interstate then led within days to a merger agreement for $11.3 billion in stock.[8] Wells Fargo completed the acquisition on April 1, 1996 and announced the elimination of 7,200 jobs.[9]\\r\\nFirst Interstate Bancorp's stock was trade on the New York Stock Exchange under the stock symbol \\"I\\".[10]\\r\\nIn 1984, First Interstate BancSystem of Montana entered into a franchise agreement with First Interstate Bancorp of California to use the First Interstate Bank name and logo. In 1996 when First Interstate Bancorp was split up, the Montana organization successfully negotiated to retain the well known First Interstate name and logo. First Interstate BancSystem continues to offer banking in Idaho, Montana, Oregon, South Dakota, Washington, and Wyoming.[11]\\r\\nIn June 1986, a highly trained group, called the \\"Hole in the Ground\\" crew by the media, tunneled under the First Interstate Bank in Hollywood at Spaulding Avenue and Sunset Boulevard through an extensive network of tunnels over the course of several months and took about US$270,000 (equivalent to $600,000 in 2017) in cash and the contents of 36 safety deposit boxes valued at US$2,500,000 (equivalent to $5,600,000 in 2017). The group rode all-terrain vehicles through the underground storm drain system of Los Angeles, and used gas-powered generators, hammer drills, power saws, and digging equipment used to tunnel 100?ft (30?m) up into the banks vault.[12][13]","input":"When did wells fargo buy first interstate bank?"},{"output":"Gaby Moreno","context":"","input":"Who wrote the parks and recreation theme song?"},{"output":"10 miles","context":"Paradise is an incorporated town in Butte County, California. It is located in the Sierra Nevada foothills above the northeastern Sacramento Valley.\\r\\nThe town is statistically classified within the Chico Metropolitan Area.\\r\\n\\r\\n\\r\\nThe town of Paradise is 10 miles (16?km) east of Chico and 85 miles (137?km) north of Sacramento. The population was 26,283 as of 2013,[8] down from 26,408 at the 2000 census.\\r\\nIt spreads out on a wide ridge between deep canyons formed by the west branch of the Feather River to the east and Butte Creek to the west. The Paradise area extends northward from Paradise to include the unincorporated town of Magalia, and such smaller communities as Stirling City, eleven miles north. Elevation of the town is 1,778 feet (542?m), according to the GNIS.[10] The town is approximately eight miles east of the city of Chico, and ten miles north of the Oroville area.\\r\\nAccording to the United States Census Bureau, the town has a total area of 18.3 square miles (47?km2), over 99% of it land.\\r\\nSoils are mostly well drained reddish brown loam, gravelly in some cases and often grading to clay loam or clay with increasing depth. They have developed on volcanic material. Paradiso is by far the most common soil series in town.[11]\\r\\nThe first post office was established at Paradise in 1877; it closed for a time in 1911, but was re-established later that year, when the post office at Orloff was closed.[12] Paradise incorporated in 1979.[12] For many years, the Butte County Railroad operated trains along the ridge, serving mines and sawmills.\\r\\nA legend persists that the town was named because it was the home of the Pair o' Dice Saloon, an idea supported by the fact than an official 1900 railroad map referred to the town as \\"Paradice\\". However, according to folklorist Barbara Mikkelson of Snopes.com, there is no documentation of such an establishment, nor an explanation of how the map's spelling of the town originated. Gene Sylva, a former mayor of the nearby town of Oroville, has stated that the saloon story is false, and that the true etymology of the town's name is traced to his great-great-grandfather, William Pierce Leonard, who named the town on a summer day in 1864, after a hot and dusty ride from the Sacramento Valley. Arriving at his sawmill while the staff were on break, Leonard \\"took a deep breath of the cool, clean air, and exclaimed, 'Boys, this is paradise.'\\" Mikkelson, however, suggests that Sylva's explanation may also be \\"pleasingly inventive historical fiction\\", but surmises that the town was probably named for it being a pleasant place to live.[13]\\r\\nIn June 2008, a wildfire, named the \\"Humboldt Fire\\" for its point of origin, swept over 22,800 acres (92?km2) of land between Chico and Paradise. As many as 9300 people were forced to evacuate southwestern Paradise until the fire could be brought under control.[14] Also in July 2008 another fire, the Camp Fire, burned on the northern side of Paradise in the canyon where the Feather River is located. Again, thousands were evacuated from their homes, but the fire failed to cross the river.\\r\\nParadise has a hot-summer Mediterranean climate (Csa) according to the K?ppen climate classification system.\\r\\nThe 2010 United States Census[17] reported that Paradise had a population of 26,218. The population density was 1,430.9 people per square mile (552.5/km2). The racial makeup of Paradise was 24,129 (92.0%) White, 112 (0.4%) African American, 301 (1.1%) Native American, 330 (1.3%) Asian, 24 (0.1%) Pacific Islander, 416 (1.6%) from other races, and 906 (3.5%) from two or more races. Hispanic or Latino of any race were 1,836 persons (7.0%).\\r\\nThe Census reported that 25,810 people (98.4% of the population) lived in households, 139 (0.5%) lived in non-institutionalized group quarters, and 269 (1.0%) were institutionalized.\\r\\nThere were 11,893 households, out of which 2,574 (21.6%) had children under the age of 18 living in them, 5,227 (44.0%) were opposite-sex married couples living together, 1,308 (11.0%) had a female householder with no husband present, 511 (4.3%) had a male householder with no wife present. There were 742 (6.2%) unmarried opposite-sex partnerships, and 94 (0.8%) same-sex married couples or partnerships. 4,038 households (34.0%) were made up of individuals and 2,126 (17.9%) had someone living alone who was 65 years of age or older. The average household size was 2.17. There were 7,046 families (59.2% of all households); the average family size was 2.73.\\r\\nThe population was spread out with 4,501 people (17.2%) under the age of 18, 1,858 people (7.1%) aged 18 to 24, 4,822 people (18.4%) aged 25 to 44, 8,466 people (32.3%) aged 45 to 64, and 6,571 people (25.1%) who were 65 years of age or older. The median age was 50.2 years. For every 100 females there were 90.5 males. For every 100 females age 18 and over, there were 88.5 males.\\r\\nThere were 12,981 housing units at an average density of 708.5 per square mile (273.5/km2), of which 7,975 (67.1%) were owner-occupied, and 3,918 (32.9%) were occupied by renters. The homeowner vacancy rate was 2.8%; the rental vacancy rate was 5.9%. 17,381 people (66.3% of the population) lived in owner-occupied housing units and 8,429 people (32.1%) lived in rental housing units.\\r\\nAs of the census[18] of 2000, there were 26,408 people, 11,591 households, and 7,244 families residing in the town. The population density was 1,447.1 people per square mile (558.7/km2). There were 12,374 housing units at an average density of 678.1 per square mile (261.8/km2). The racial makeup of the town was 93.73% White, 0.19% Black or African American, 1.07% Native American, 1.04% Asian, 0.12% Pacific Islander, 1.21% from other races, and 2.64% from two or more races. 4.27% of the population were Hispanic or Latino of any race.\\r\\nThere were 11,591 households out of which 23.0% had children under the age of 18 living with them, 48.7% were married couples living together, 10.3% had a female householder with no husband present, and 37.5% were non-families. 32.0% of all households were made up of individuals and 18.1% had someone living alone who was 65 years of age or older. The average household size was 2.22 and the average family size was 2.77.\\r\\nIn the town the population was spread out with 20.4% under the age of 18, 5.9% from 18 to 24, 21.2% from 25 to 44, 25.3% from 45 to 64, and 27.2% who were 65 years of age or older. The median age was 47 years. For every 100 females there were 87.3 males. For every 100 females age 18 and over, there were 83.5 males.\\r\\nThe median income for a household in the town was $31,863, and the median income for a family was $41,228. Males had a median income of $35,419 versus $25,231 for females. The per capita income for the town was $19,267. About 9.7% of families and 12.4% of the population were below the poverty line, including 17.6% of those under age 18 and 6.7% of those age 65 or over.\\r\\nThe State of California Office of Statewide Health Planning and Development defines Feather River Hospital as a General Acute Care Hospital in Paradise with Basic emergency care as of August 22, 2006. The facility is located at (NAD83) latitude/longitude 394525N 1213416W? / ?39.75694N 121.57111W? / 39.75694; -121.57111.\\r\\nParadise is served by Paradise Unified School District as well as several independent Charter and Private Schools.\\r\\nPUSD schools include:\\r\\nOther Paradise Schools include:\\r\\nThere are not many options for transportation within Paradise other than driving an automobile. The Paradise/Magalia area is served by the \\"B line\\" Butte County Transit. Butte Community College also runs bus service for students.\\r\\nParadise's link with Chico, Skyway Road (referred to locally as simply \\"Skyway\\"), begins in the Sacramento Valley, at the Highway 99 freeway in Chico, and runs up the ridge as a 4-lane divided highway until it reaches Paradise. Through the town it is a four-lane undivided highway, and it becomes a two-lane road as it continues up the Sierra's ridge to Magalia and numerous smaller communities to the north. Paradise is connected to Oroville via Highway 191, otherwise known as Clark Road upon entering the town.\\r\\nThe Paradise Memorial Trail is a paved pedestrian and bicycle path which runs through town on the path of the former railroad tracks leading up the ridge. However, aside from points along this path, the very hilly terrain of the town, coupled with the large spacing of commercial areas and large land area make Paradise difficult to navigate on foot or on a bicycle, in addition to the lack of sidewalks.\\r\\nParadise Skypark (FAA identifier: Q88) is an airport located parallel to State Route 191 and south of the town at 394238N 1213659W? / ?39.71056N 121.61639W? / 39.71056; -121.61639.\\r\\nEclectic Internet radio station Radio Paradise webcasts from Paradise.\\r\\nThe local newspaper is the Paradise Post.[19]\\r\\nScenes from Gone with the Wind were filmed in Paradise off of Stark Lane.[citation needed] Paradise was also used in the comic strip \\"Pickles\\", by Brian Crane, on June 22, 2011.","input":"How far is paradise ca from chico ca?"},{"output":"held in odd-numbered years","context":"The Total Africa Cup of Nations, officially CAN (French: Coupe d'Afrique des Nations), also referred to as African Cup of Nations, or AFCON, is the main international association football competition in Africa. It is sanctioned by the Confederation of African Football (CAF) and was first held in 1957. Since 1968, it has been held every two years. The title holders at the time of a FIFA Confederations Cup qualify for that competition.\\r\\nIn 1957 there were only three participating nations: Egypt, Sudan and Ethiopia. South Africa was originally scheduled to compete, but were disqualified due to the apartheid policies of the government then in power.[1] Since then, the tournament has grown greatly, making it necessary to hold a qualifying tournament. The number of participants in the final tournament reached 16 in 1998 (16 teams were to compete in 1996 but Nigeria withdrew, reducing the field to 15, and the same happened with Togo's withdrawal in 2010), and until 2017, the format had been unchanged, with the sixteen teams being drawn into four groups of four teams each, with the top two teams of each group advancing to a \\"knock-out\\" stage. On 20 July 2017, the Africa Cup of Nations was moved from January to June and expanded from 16 to 24 teams.[2]\\r\\nEgypt is the most successful nation in the cup's history, winning the tournament a record of seven times (including when Egypt was known as the United Arab Republic between 1958 and 1961). Three different trophies have been awarded during the tournament's history, with Ghana and Cameroon winning the first two versions to keep after each of them won a tournament three times. The current trophy was first awarded in 2002 and with Egypt winning it indefinitely after winning their unprecedented third consecutive title in 2010.\\r\\nAs of 2013, the tournament was switched to being held in odd-numbered years so as not to clash with the FIFA World Cup.[3]\\r\\n\\r\\n\\r\\nThe origin of the African Nations Cup dates from June 1956, when the creation of the Confederation of African Football was proposed during the third FIFA congress in Lisbon. There were immediate plans for a continental tournament to be held and, in February 1957, the first African Cup of Nations was held in Khartoum, Sudan. There was no qualification for this tournament, the field being made up of the four founding nations of CAF (Egypt, Sudan, Ethiopia, and South Africa). South Africa's insistence on selecting only white players for their squad due to its apartheid policy led to its disqualification, and as a consequence Ethiopia were handed a bye straight to the final.[4] Hence, only two matches were played, with Egypt being crowned as the first continental champion after defeating hosts Sudan in the semi-final and Ethiopia in the final. Two years later Egypt hosted the second ANC in Cairo with the participation of the same three teams. Host and defending champions Egypt again won, after defeating Sudan.\\r\\nThe field grew to include nine teams for the third ANC in 1962 in Addis Ababa, and for the first time there was a qualification round to determine which four teams would play for the title. Host Ethiopia and reigning champion Egypt received automatic berths, and were joined in the final four by Nigeria and Tunisia. Egypt made its third consecutive final appearance, but it was Ethiopia that emerged as victors, after first beating Tunisia and then downing Egypt in extra time.\\r\\nIn 1963, Ghana made its first appearance as it hosted the event, and won the title after beating Sudan in the final. They repeated that as they became champions two years later in Tunisiaequalling Egypt as two-time winnerswith a squad that included only two returning members from the 1963 team.[5]\\r\\nIn 1965, the CAF introduced a rule that limit the number of overseas players in each team to two. The rule persisted to 1982.[6]\\r\\nThe 1968 competition's final tournament format expanded to include eight of the 22 teams entered in the preliminary rounds. The qualifying teams were distributed in two groups of four to play single round-robin tournaments, with the top two teams of each group advancing to semi-finals, a system that remained in use for the finals until 1992. The Democratic Republic of Congo won its first title, beating Ghana in the final. Starting with the 1968 tournament, the competition has been regularly held every two years in even numbered years. C?te d'Ivoire forward Laurent Pokou led the 1968 and 1970 tournaments in scoring, with six and eight goals respectively, and his total of 14 goals remained the all-time record until 2008. Play was covered for television for the first time during the 1970 tournament in Sudan,[5] as the hosts lifted the trophy after defeating Ghanawho were playing their fourth consecutive final.\\r\\nSix different nations won titles from 1970 to 1980: Sudan, Congo-Brazzaville, Zaire, Morocco, Ghana, and Nigeria. Zaire's second title in the 1974 edition (they won their first as the Democratic Republic of Congo) came after facing Zambia in the final. For the only time to date in the history of the competition, the match had to be replayed as the first contest between the two sides ended in a 2ÿ2 draw after extra time. The final was re-staged two days later with Zaire winning 2ÿ0. Forward Mulamba Ndaye scored all four of Zaire's goals in these two matches: he was also the top scorer of the tournament with nine goals, setting a single-tournament record that remains unmatched. Three months earlier, Zaire had become the first Sub-Saharan African nation to qualify to the FIFA World Cup. Morocco won their first title in the 1976 ANC held in Ethiopia and Ghana took its third championship in 1978, becoming the first nation to win three titles.\\r\\nIn 1980, Nigeria hosted the event and beat Algeria to capture its first championship. Ghana's fourth continental title came in the 1982 cup tournament; they beat Algeria in the semi-finals in extra time, and faced host Libya in the final. The match ended in a 1ÿ1 draw after 120 minutes and Ghana won the penalty shootout to become champions. Cameroon won their first title two years later by beating Nigeria, and in the 1986 cup, they faced Egyptabsent from the final since 1962with Egypt winning the title on penalty kicks. Cameroon reached its third consecutive final in the 1988 tournament and won their second championship by repeating their 1984 victory over Nigeria.\\r\\nIn 1990, Nigeria lost once again as they made their third final appearance in four tournaments, this time falling to host Algeria. The 1992 Cup of Nations expanded the number of final tournament participants to 12; the teams were divided into four groups of three, with the top two teams of each group advancing to quarter-finals. Ghanaian midfielder Abedi \\"Pel\\" Ayew, who scored three goals, was named the best player of the tournament after his contributions helped Ghana reach the final; he was, however, suspended for that match and Ghana lost to C?te d'Ivoire in a penalty shootout that saw each side make 11 attempts to determine the winner. C?te d'Ivoire set a record for the competition by holding each of their opponents scoreless in the six matches of the final tournament.\\r\\nThe 12-team, three-group format was used again two years later, where hosts Tunisia were humiliated by their first round elimination. Nigeria, who had just qualified to the World Cup for the first time in their history, won the tournament, beating Zambia, who a year before had been struck by disaster when most of their national squad died in a plane crash while traveling to play a 1994 World Cup qualification match. Nigerian forward Rashidi Yekini, who had led the 1992 tournament with four goals, repeated as the top scorer with five goals.\\r\\nSouth Africa hosted the 20th ACN competition in 1996, marking their first ever appearance after a decades long ban was lifted with the end of apartheid in the country and a failed attempt to qualify in 1994. The number of final round participants in 1996 was expanded to the current 16, split into four groups. However, the actual number of teams playing in the final was only 15 as Nigeria withdrew from the tournament at the final moment for political reasons.[7] Bafana Bafana won their first title on home soil, defeating Tunisia in the final.[8]\\r\\nThe South Africans would reach the final again two years later in Burkina Faso, but were unable to defend their title, losing to Egypt who claimed their fourth cup.\\r\\nThe 2000 edition was hosted jointly by Ghana and Nigeria, who replaced the originally designated host Zimbabwe. Following a 2ÿ2 draw after extra time in the final, Cameroon defeated Nigeria on penalty kicks. In 2002, Cameroon's Indomitable Lions made the second consecutive titles since Ghana had done it in the 1960s and after Egypt had done it before in 1957 and 1959. Again via penalty kicks, the Cameroonians beat first-time finalists Senegal, who also debuted in the World Cup later that year. Both finalists were eliminated in quarter finals two years later in Tunisia, where the hosts won their first title, beating Morocco 2ÿ1 in the final. The 2006 tournament was also won by the hosts, Egypt, who reached a continental-record fifth title. Ahead of the 2008 Africa Cup of Nations several European clubs called for a rethink of the tournament's schedule. As it takes place during the European season, players who are involved miss several matches for their clubs.[9]\\r\\nIn January 2008, FIFA president Sepp Blatter announced that he wanted the tournament to be held in either June or July by 2016, to fit in the international calendar, although this would preclude many countries in central and west Africa from hosting the competition (as these months occur during their wet season).[10] The 2008 tournament was hosted by Ghana, and saw Egypt retain the trophy, winning their record-extending sixth tournament by defeating Cameroon 1ÿ0 in the final.[11]\\r\\nEgypt set a new record in the 2010 tournament that was hosted by Angola by winning their third consecutive title in an unprecedented achievement on the African level after defeating Ghana 1ÿ0 in the final, retaining the gold-plated cup indefinitely and extending their record to 7 continental titles (including when Egypt was known as UAR between 1958 and 1961).[12] Egypt became the first African nation to win three consecutive cups and joined Mexico, Argentina, and Iran who also won their continent cup 3 times in a row.\\r\\nOn 31 January 2010, Egypt set a new African record, not being defeated for 19 consecutive Cup of Nations matches, since a 2ÿ1 loss against Algeria in Tunisia in 2004,[citation needed] and a record 9 consecutive win streak.[citation needed]\\r\\nIn May 2010, it was announced that the tournament would be moved to odd-numbered years from 2013. This means the tournament will not take place in the same year as the World Cup after 2013. It also meant there were two tournaments within twelve months in January 2012[13] (co-hosted by Gabon and Equatorial Guinea) and January 2013 (hosted by South Africa).[3] The change of FIFA Confederations Cup from a biennial to a quadrennial tournament, and the switching of African cup of nations from even to odd-number years, made some African tournaments winners deprived of participating in the Confederations Cup tournaments like Egypt, Zambia, and Ivory Coast winners of (2010), (2012), and (2015) Africa cup of nations respectively.\\r\\nOn 29 January 2011, Morocco won the bid to host the 2015 edition and Libya won the right to host the 2013 tournament. But due to the 2011 Libyan civil war, Libya and South Africa traded years with South Africa hosting in 2013 and Libya hosting in 2017.[14]\\r\\nIn 2012, Zambia won the final after a penalty shootout against C?te d'Ivoire. This drew increased media attention since the match took place in Gabon, only a few hundred meters from the crash site of the 1993 air disaster of their national team. The 2013 tournament was won by Nigeria, beating first time finalists Burkina Faso.\\r\\nIn 2014-15, the West African Ebola virus epidemic disrupted the tournament.[15] The Antoinette Tubman Stadium in Monrovia, Liberia was converted into an Ebola treatment unit.[16] On 24 July Liberia suspended all football activities.[17] The 2015 Africa Cup of Nations was scheduled to be held in Morocco, but they refused to hold the tournament on the allotted dates due to concerns of the Ebola outbreak. The 2015 tournament was then moved to Equatorial Guinea.[18]\\r\\nWith Ahmad Ahmad being CAF president, there'd been discussions about bringing changes on Africa Cup of Nations. On July 2017, two changes were proposed:[19][20]\\r\\nOn 20 July 2017, the CAF Executive Commission approved the propositions in a reunion in Rabat, Morocco.[2]\\r\\nIn July 2016, Total has secured an eight-year sponsorship package from the Confederation of African Football (CAF) to support 10 of its principal competitions. Total started with the Africa Cup of Nations that was held in Gabon in 2017 therefore renaming it Total Africa cup of Nations.[21]\\r\\nThroughout the history of the Nations Cup, three different trophies have been awarded to the winners of the competition. The original trophy, made of silver, was the Abdelaziz Abdallah Salem Trophy, named after the first CAF president, Egyptian Abdelaziz Abdallah Salem. As the first winner of three Nations Cup tournaments, Ghana obtained the right to permanently hold the trophy in 1978.[22]\\r\\nThe second trophy was awarded from 1980 to 2000, and it was named \\"Trophy of African Unity\\"[23] or \\"African Unity Cup\\".[22] It was given by the Supreme Council for Sports in Africa to the CAF prior to the 1980 tournament and it was a cylindrical piece with the Olympic rings over a map of the continent engraved on it. It sat on a squared base and had stylized triangular handles. Cameroon won the Unity Cup indefinitely after they became three-time champions in 2000.\\r\\nIn 2001, the third trophy was revealed, a gold-plated cup designed and made in Italy.[22] Cameroon, permanent holders of the previous trophy, were the first nation to be awarded the new trophy after they won the 2002 edition. Egypt won the gold-plated cup indefinitely after they became three-time champions in 2010, in an unprecedented achievement by winning three consecutive continental titles. Unlike previous winners who would have then taken the trophy home, Egypt were presented with a special full size replica that they got to keep. First and second time winners usually get a smaller sized replica for their trophy cabinets.\\r\\nAs 2017 (31 edition)\\r\\n* hosts","input":"How often is african cup of nations played?"},{"output":"March 23, 2010","context":"","input":"When was the affordable care act put into effect?"},{"output":"June 22, 1942","context":"\\"The Star-Spangled Banner\\" is the national anthem of the United States. The lyrics come from \\"Defence of Fort M'Henry\\",[2] a poem written on September 14, 1814, by the then 35-year-old lawyer and amateur poet Francis Scott Key after witnessing the bombardment of Fort McHenry by British ships of the Royal Navy in Baltimore Harbor during the Battle of Baltimore in the War of 1812. Key was inspired by the large U.S. flag, with 15 stars and 15 stripes, known as the Star-Spangled Banner, flying triumphantly above the fort during the U.S. victory.\\r\\nThe poem was set to the tune of a popular British song written by John Stafford Smith for the Anacreontic Society, a men's social club in London. \\"To Anacreon in Heaven\\" (or \\"The Anacreontic Song\\"), with various lyrics, was already popular in the United States. Set to Key's poem and renamed \\"The Star-Spangled Banner\\", it soon became a well-known U.S. patriotic song. With a range of 19 semitones, it is known for being very difficult to sing. Although the poem has four stanzas, only the first is commonly sung today.\\r\\n\\"The Star-Spangled Banner\\" was recognized for official use by the United States Navy in 1889, and by U.S. President Woodrow Wilson in 1916, and was made the national anthem by a congressional resolution on March 3, 1931 (46 Stat. 1508, codified at 36 U.S.C.??301), which was signed by President Herbert Hoover.\\r\\nBefore 1931, other songs served as the hymns of U.S. officialdom. \\"Hail, Columbia\\" served this purpose at official functions for most of the 19th century. \\"My Country, 'Tis of Thee\\", whose melody is identical to \\"God Save the Queen\\", the United Kingdom's national anthem,[3] also served as a de facto national anthem.[4] Following the War of 1812 and subsequent U.S. wars, other songs emerged to compete for popularity at public events, among them \\"America the Beautiful\\".\\r\\n\\r\\n\\r\\nOn September 3, 1814, following the Burning of Washington and the Raid on Alexandria, Francis Scott Key and John Stuart Skinner set sail from Baltimore aboard the ship HMS Minden, flying a flag of truce on a mission approved by President James Madison. Their objective was to secure an exchange of prisoners, one of whom was Dr. William Beanes, the elderly and popular town physician of Upper Marlboro and a friend of Key's who had been captured in his home. Beanes was accused of aiding the arrest of British soldiers. Key and Skinner boarded the British flagship HMS Tonnant on September 7 and spoke with Major General Robert Ross and Vice Admiral Alexander Cochrane over dinner while the two officers discussed war plans. At first, Ross and Cochrane refused to release Beanes but relented after Key and Skinner showed them letters written by wounded British prisoners praising Beanes and other Americans for their kind treatment.\\r\\nBecause Key and Skinner had heard details of the plans for the attack on Baltimore, they were held captive until after the battle, first aboard HMS Surprise and later back on HMS Minden. After the bombardment, certain British gunboats attempted to slip past the fort and effect a landing in a cove to the west of it, but they were turned away by fire from nearby Fort Covington, the city's last line of defense.\\r\\nDuring the rainy night, Key had witnessed the bombardment and observed that the fort's smaller \\"storm flag\\" continued to fly, but once the shell and Congreve rocket[5] barrage had stopped, he would not know how the battle had turned out until dawn. On the morning of September 14, the storm flag had been lowered and the larger flag had been raised.\\r\\nDuring the bombardment, HMS Terror and HMS Meteor provided some of the \\"bombs bursting in air\\".\\r\\nKey was inspired by the U.S. victory and the sight of the large U.S. flag flying triumphantly above the fort. This flag, with fifteen stars and fifteen stripes, had been made by Mary Young Pickersgill together with other workers in her home on Baltimore's Pratt Street. The flag later came to be known as the Star-Spangled Banner and is today on display in the National Museum of American History, a treasure of the Smithsonian Institution. It was restored in 1914 by Amelia Fowler, and again in 1998 as part of an ongoing conservation program.\\r\\nAboard the ship the next day, Key wrote a poem on the back of a letter he had kept in his pocket. At twilight on September 16, he and Skinner were released in Baltimore. He completed the poem at the Indian Queen Hotel, where he was staying, and titled it \\"Defence of Fort M'Henry\\". It was first published nationally in The Analectic Magazine.[6][7]\\r\\nMuch of the idea of the poem, including the flag imagery and some of the wording, is derived from an earlier song by Key, also set to the tune of \\"The Anacreontic Song\\". The song, known as \\"When the Warrior Returns\\",[8] was written in honor of Stephen Decatur and Charles Stewart on their return from the First Barbary War.  Absent elaboration by Francis Scott Key prior to his death in 1843, some have speculated in modern times about the meaning of phrases or verses. According to British historian Robin Blackburn, the words \\"the hireling and slave\\" allude to the thousands of ex-slaves in the British ranks organised as the Corps of Colonial Marines, who had been liberated by the British and demanded to be placed in the battle line \\"where they might expect to meet their former masters.\\"[9] Nevertheless, Professor Mark Clague, a professor of musicology at the University of Michigan, argues that the \\"middle two verses of Key's lyric vilify the British enemy in the War of 1812\\" and \\"in no way glorifies or celebrates slavery.\\"[10] Clague writes that \\"For Key ... the British mercenaries were scoundrels and the Colonial Marines were traitors who threatened to spark a national insurrection.\\"[10] This harshly anti-British nature of Verse 3 led to its omission in sheet music in World War I, when the British and the U.S. were allies.[10] Responding to the assertion of writer Jon Schwarz of The Intercept that the song is a \\"celebration of slavery,\\"[11] Clague said that: \\"The reference to slaves is about the use and in some sense the manipulation, of black Americans to fight for the British, with the promise of freedom. The American forces included African-Americans as well as whites. The term 'freemen,' whose heroism is celebrated in the fourth stanza, would have encompassed both.\\"[12]\\r\\nOthers suggest that \\"Key may have intended the phrase as a reference to the British Navy's practice of impressment (kidnapping sailors and forcing them to fight in defense of the crown), or as a semi-metaphorical slap at the British invading force as a whole (which included a large number of mercenaries).\\"[13]\\r\\nKey gave the poem to his brother-in-law Judge Joseph H. Nicholson who saw that the words fit the popular melody \\"The Anacreontic Song\\", by English composer John Stafford Smith. This was the official song of the Anacreontic Society, an 18th-century gentlemen's club of amateur musicians in London. Nicholson took the poem to a printer in Baltimore, who anonymously made the first known broadside printing on September 17; of these, two known copies survive.\\r\\nOn September 20, both the Baltimore Patriot and The American printed the song, with the note \\"Tune: Anacreon in Heaven\\". The song quickly became popular, with seventeen newspapers from Georgia to New Hampshire printing it. Soon after, Thomas Carr of the Carr Music Store in Baltimore published the words and music together under the title \\"The Star Spangled Banner\\", although it was originally called \\"Defence of Fort M'Henry\\". Thomas Carr's arrangement introduced the raised fourth which became the standard deviation from \\"The Anacreontic Song\\".[14] The song's popularity increased and its first public performance took place in October when Baltimore actor Ferdinand Durang sang it at Captain McCauley's tavern. Washington Irving, then editor of the Analectic Magazine in Philadelphia, reprinted the song in November 1814.\\r\\nBy the early 20th century, there were various versions of the song in popular use. Seeking a singular, standard version, President Woodrow Wilson tasked the U.S. Bureau of Education with providing that official version. In response, the Bureau enlisted the help of five musicians to agree upon an arrangement. Those musicians were Walter Damrosch, Will Earhart, Arnold J. Gantvoort, Oscar Sonneck and John Philip Sousa. The standardized version that was voted upon by these five musicians premiered at Carnegie Hall on December 5, 1917, in a program that included Edward Elgar's Carillon and Gabriel Piern's The Children's Crusade. The concert was put on by the Oratorio Society of New York and conducted by Walter Damrosch.[15] An official handwritten version of the final votes of these five men has been found and shows all five men's votes tallied, measure by measure.[16]\\r\\nThe song gained popularity throughout the 19th century and bands played it during public events, such as July 4th celebrations.\\r\\nA plaque displayed at Fort Meade, South Dakota, claims that the idea of making \\"The Star Spangled Banner\\" the national anthem began on their parade ground in 1892. Colonel Caleb Carlton, Post Commander, established the tradition that the song be played \\"at retreat and at the close of parades and concerts.\\" Carlton explained the custom to Governor Sheldon of South Dakota who \\"promised me that he would try to have the custom established among the state militia.\\" Carlton wrote that after a similar discussion, Secretary of War, Daniel E. Lamont issued an order that it \\"be played at every Army post every evening at retreat.\\"[17]\\r\\nIn 1899, the U.S. Navy officially adopted \\"The Star-Spangled Banner\\".[18] In 1916, President Woodrow Wilson ordered that \\"The Star-Spangled Banner\\" be played at military[18] and other appropriate occasions. The playing of the song two years later during the seventh-inning stretch of Game One of the 1918 World Series, and thereafter during each game of the series is often cited as the first instance that the anthem was played at a baseball game,[19] though evidence shows that the \\"Star-Spangled Banner\\" was performed as early as 1897 at opening day ceremonies in Philadelphia and then more regularly at the Polo Grounds in New York City beginning in 1898. In any case, the tradition of performing the national anthem before every baseball game began in World War II.[20]\\r\\nOn April 10, 1918, John Charles Linthicum, U.S. Congressman from Maryland, introduced a bill to officially recognize \\"The Star-Spangled Banner\\" as the national anthem.[21] The bill did not pass.[21] On April 15, 1929, Linthicum introduced the bill again, his sixth time doing so.[21] On November 3, 1929, Robert Ripley drew a panel in his syndicated cartoon, Ripley's Believe it or Not!, saying \\"Believe It or Not, America has no national anthem\\".[22]\\r\\nIn 1930, Veterans of Foreign Wars started a petition for the United States to officially recognize \\"The Star-Spangled Banner\\" as the national anthem.[23] Five million people signed the petition.[23] The petition was presented to the United States House Committee on the Judiciary on January 31, 1930.[24] On the same day, Elsie Jorss-Reilley and Grace Evelyn Boudlin sang the song to the Committee to refute the perception that it was too high pitched for a typical person to sing.[25] The Committee voted in favor of sending the bill to the House floor for a vote.[26] The House of Representatives passed the bill later that year.[27] The Senate passed the bill on March 3, 1931.[27] President Herbert Hoover signed the bill on March 4, 1931, officially adopting \\"The Star-Spangled Banner\\" as the national anthem of the United States of America.[1] As currently codified, the United States Code states that \\"[t]he composition consisting of the words and music known as the Star-Spangled Banner is the national anthem.\\"[28]\\r\\nThe song is notoriously difficult for nonprofessionals to sing because of its wide range?ÿ a 12th. Humorist Richard Armour referred to the song's difficulty in his book It All Started With Columbus.\\r\\nIn an attempt to take Baltimore, the British attacked Fort McHenry, which protected the harbor. Bombs were soon bursting in air, rockets were glaring, and all in all it was a moment of great historical interest. During the bombardment, a young lawyer named Francis Off Key [sic] wrote \\"The Star-Spangled Banner\\", and when, by the dawn's early light, the British heard it sung, they fled in terror.\\r\\nProfessional and amateur singers have been known to forget the words, which is one reason the song is sometimes pre-recorded and lip-synced.[citation needed] Other times the issue is avoided by having the performer(s) play the anthem instrumentally instead of singing it. The pre-recording of the anthem has become standard practice at some ballparks, such as Boston's Fenway Park, according to the SABR publication The Fenway Project.[29]\\r\\n\\"The Star-Spangled Banner\\" is traditionally played at the beginning of public sports events and orchestral concerts in the United States, as well as other public gatherings. The National Hockey League and Major League Soccer both require venues in both the U.S. and Canada to perform both the Canadian and U.S. national anthems at games that involve teams from both countries (with the \\"away\\" anthem being performed first).[30][better?source?needed] It is also usual for both U.S. and Canadian anthems (done in the same way as the NHL and MLS) to be played at Major League Baseball and National Basketball Association games involving the Toronto Blue Jays and the Toronto Raptors (respectively), the only Canadian teams in those two major U.S. sports leagues, and in All Star Games on the MLB, NBA, and NHL. The Buffalo Sabres of the NHL, which play in a city on the CanadaÿUS border and have a substantial Canadian fan base, play both anthems before all home games regardless of where the visiting team is based.[31]\\r\\nTwo especially unusual performances of the song took place in the immediate aftermath of the United States September 11 attacks. On September 12, 2001, the Queen broke with tradition and allowed the Band of the Coldstream Guards to perform the anthem at Buckingham Palace, London, at the ceremonial Changing of the Guard, as a gesture of support for Britain's ally.[32] The following day at a St. Paul's Cathedral memorial service, the Queen joined in the singing of the anthem, an unprecedented occurrence.[33]\\r\\nThe 200th anniversary of the \\"Star-Spangled Banner\\" occurred in 2014 with various special events occurring throughout the United States. A particularly significant celebration occurred during the week of September 10ÿ16 in and around Baltimore, Maryland. Highlights included playing of a new arrangement of the anthem arranged by John Williams and participation of President Obama on Defender's Day, September 12, 2014, at Fort McHenry.[34] In addition, the anthem bicentennial included a youth music celebration[35] including the presentation of the National Anthem Bicentennial Youth Challenge winning composition written by Noah Altshuler.\\r\\nThe first popular music performance of the anthem heard by the mainstream U.S. was by Puerto Rican singer and guitarist Jos Feliciano. He created a nationwide uproar when he strummed a slow, blues-style rendition of the song[36] at Tiger Stadium in Detroit before game five of the 1968 World Series, between Detroit and St. Louis.[37] This rendition started contemporary \\"Star-Spangled Banner\\" controversies. The response from many in the Vietnam War-era U.S. was generally negative. Despite the controversy, Feliciano's performance opened the door for the countless interpretations of the \\"Star-Spangled Banner\\" heard in the years since.[38] One week after Feliciano's performance, the anthem was in the news again when U.S. athletes Tommie Smith and John Carlos lifted controversial raised fists at the 1968 Olympics while the \\"Star-Spangled Banner\\" played at a medal ceremony.\\r\\nMarvin Gaye gave a soul-influenced performance at the 1983 NBA All-Star Game and Whitney Houston gave a soulful rendition before Super Bowl XXV in 1991, which was released as a single that charted at number 20 in 1991 and number 6 in 2001 (along with Jos Feliciano, the only times the national anthem has been on the Billboard Hot 100). In 1993, Kiss did an instrumental rock version as the closing track on their album, Alive III. Another famous instrumental interpretation is Jimi Hendrix's version, which was a set-list staple from autumn 1968 until his death in September 1970, including a famous rendition at the Woodstock music festival in 1969. Incorporating sonic effects to emphasize the \\"rockets' red glare\\", and \\"bombs bursting in air\\", it became a late-1960s emblem. Roseanne Barr gave a controversial performance of the anthem at a San Diego Padres baseball game at Jack Murphy Stadium on July 25, 1990. The comedian belted out a screechy rendition of the song, and afterward, she attempted a gesture of ballplayers by spitting and grabbing her crotch as if adjusting a protective cup. The performance offended some, including the sitting U.S. President, George H. W. Bush.[39] Sufjan Stevens has frequently performed the \\"Star-Spangled Banner\\" in live sets, replacing the optimism in the end of the first verse with a new coda that alludes to the divisive state of the nation today. David Lee Roth both referenced parts of the anthem and played part of a hard rock rendition of the anthem on his song, \\"Yankee Rose\\" on his 1986 solo album, Eat 'Em and Smile. Steven Tyler also caused some controversy in 2001 (at the Indianapolis 500, to which he later issued a public apology) and again in 2012 (at the AFC Championship Game) with a cappella renditions of the song with changed lyrics.[40] A version of Aerosmith's Joe Perry and Brad Whitford playing part of the song can be heard at the end of their version of \\"Train Kept A-Rollin'\\" on the Rockin' the Joint album. The band Boston gave an instrumental rock rendition of the anthem on their Greatest Hits album. The band Crush 40 made a version of the song as opening track from the album Thrill of the Feel (2000).\\r\\nIn March 2005, a government-sponsored program, the National Anthem Project, was launched after a Harris Interactive poll showed many adults knew neither the lyrics nor the history of the anthem.[41]\\r\\nO say can you see, by the dawn's early light,\\r\\nWhat so proudly we hailed at the twilight's last gleaming,\\r\\nWhose broad stripes and bright stars through the perilous fight,\\r\\nO'er the ramparts we watched, were so gallantly streaming?\\r\\nAnd the rockets' red glare, the bombs bursting in air,\\r\\nGave proof through the night that our flag was still there;\\r\\nO say does that star-spangled banner yet wave\\r\\nO'er the land of the free and the home of the brave?\\r\\n\\r\\nOn the shore dimly seen through the mists of the deep,\\r\\nWhere the foe's haughty host in dread silence reposes,\\r\\nWhat is that which the breeze, o'er the towering steep,\\r\\nAs it fitfully blows, half conceals, half discloses?\\r\\nNow it catches the gleam of the morning's first beam,\\r\\nIn full glory reflected now shines in the stream:\\r\\n'Tis the star-spangled banner, O long may it wave\\r\\nO'er the land of the free and the home of the brave.\\r\\n\\r\\nAnd where is that band who so vauntingly swore\\r\\nThat the havoc of war and the battle's confusion,\\r\\nA home and a country, should leave us no more?\\r\\nTheir blood has washed out their foul footsteps' pollution.\\r\\nNo refuge could save the hireling and slave\\r\\nFrom the terror of flight, or the gloom of the grave:\\r\\nAnd the star-spangled banner in triumph doth wave,\\r\\nO'er the land of the free and the home of the brave.\\r\\n\\r\\nO thus be it ever, when freemen shall stand\\r\\nBetween their loved homes and the war's desolation.\\r\\nBlest with vict'ry and peace, may the Heav'n rescued land\\r\\nPraise the Power that hath made and preserved us a nation!\\r\\nThen conquer we must, when our cause it is just,\\r\\nAnd this be our motto: 'In God is our trust.'\\r\\nAnd the star-spangled banner in triumph shall wave\\r\\nO'er the land of the free and the home of the brave![42]\\r\\nIn indignation over the start of the American Civil War, Oliver Wendell Holmes, Sr.[43] added a fifth stanza to the song in 1861, which appeared in songbooks of the era.[44]\\r\\nWhen our land is illumined with Liberty's smile,\\r\\nIf a foe from within strike a blow at her glory,\\r\\nDown, down with the traitor that dares to defile\\r\\nThe flag of her stars and the page of her story!\\r\\nBy the millions unchained who our birthright have gained,\\r\\nWe will keep her bright blazon forever unstained!\\r\\nAnd the Star-Spangled Banner in triumph shall wave\\r\\nWhile the land of the free is the home of the brave.\\r\\nIn a version hand-written by Francis Scott Key in 1840, the third line reads \\"Whose bright stars and broad stripes, through the clouds of the fight\\".[45]\\r\\nSeveral films have their titles taken from the song's lyrics. These include two films titled Dawn's Early Light (2000[46] and 2005);[47] two made-for-TV features titled By Dawn's Early Light (1990[48] and 2000);[49] two films titled So Proudly We Hail (1943[50] and 1990);[51] a feature film (1977)[52] and a short (2005)[53] titled Twilight's Last Gleaming; and four films titled Home of the Brave (1949,[54] 1986,[55] 2004,[56] and 2006).[57] A 1936 short titled \\"The Song of a Nation\\" from Warner Brothers shows a version of the origin of the song.[58]\\r\\nWhen the U.S. national anthem was first recognized by law in 1931, there was no prescription as to behavior during its playing. On June 22, 1942, the law was revised indicating that those in uniform should salute during its playing, while others should simply stand at attention, men removing their hats. The same code also required that women should place their hands over their hearts when the flag is displayed during the playing of the national anthem, but not if the flag was not present. On December 23, 1942, the law was again revised instructing men and women to stand at attention and face in the direction of the music when it was played. That revision also directed men and women to place their hands over their hearts only if the flag was displayed. Those in uniform were required to salute. On July 7, 1976, the law was simplified. Men and women were instructed to stand with their hands over their hearts, men removing their hats, irrespective of whether or not the flag was displayed and those in uniform saluting. On August 12, 1998, the law was rewritten keeping the same instructions, but differentiating between \\"those in uniform\\" and \\"members of the Armed Forces and veterans\\" who were both instructed to salute during the playing whether or not the flag was displayed. Because of the changes in law over the years and confusion between instructions for the Pledge of Allegiance versus the National Anthem, throughout most of the 20th century many people simply stood at attention or with their hands folded in front of them during the playing of the Anthem, and when reciting the Pledge they would hold their hand (or hat) over their heart. After 9/11, the custom of placing the hand over the heart during the playing of the national anthem became nearly universal.[59][60][61]\\r\\nSince 1998, federal law (viz., the United States Code 36 U.S.C.??301) states that during a rendition of the national anthem, when the flag is displayed, all present including those in uniform should stand at attention; Non-military service individuals should face the flag with the right hand over the heart; Members of the Armed Forces and veterans who are present and not in uniform may render the military salute; military service persons not in uniform should remove their headdress with their right hand and hold the headdress at the left shoulder, the hand being over the heart; and Members of the Armed Forces and veterans who are in uniform should give the military salute at the first note of the anthem and maintain that position until the last note. The law further provides that when the flag is not displayed, all present should face toward the music and act in the same manner they would if the flag were displayed. Military law requires all vehicles on the installation to stop when the song is played and all individuals outside to stand at attention and face the direction of the music and either salute, in uniform, or place the right hand over the heart, if out of uniform. The law was amended in 2008, and since allows military veterans to salute out of uniform, as well.[62][63]\\r\\nThe text of 36 U.S.C.??301 is suggestive and not regulatory in nature. Failure to follow the suggestions is not a violation of the law. This behavioral requirement for the national anthem is subject to the same First Amendment controversies that surround the Pledge of Allegiance.[64] For example, Jehovah's Witnesses do not sing the national anthem, though they are taught that standing is an \\"ethical decision\\" that individual believers must make based on their \\"conscience.\\"[65][66][67]\\r\\nThe 1968 Olympics Black Power salute was a political demonstration conducted by African-American athletes Tommie Smith and John Carlos during their medal ceremony at the 1968 Summer Olympics in the Olympic Stadium in Mexico City. After having won gold and bronze medals respectively in the 200-meter running event, they turned on the podium to face their flags, and to hear the American national anthem, \\"The Star-Spangled Banner\\". Each athlete raised a black-gloved fist, and kept them raised until the anthem had finished. In addition, Smith, Carlos, and Australian silver medalist Peter Norman all wore human rights badges on their jackets. In his autobiography, Silent Gesture, Smith stated that the gesture was not a \\"Black Power\\" salute, but a \\"human rights salute\\". The event is regarded as one of the most overtly political statements in the history of the modern Olympic Games.[68]\\r\\nPolitically motivated protests of the national anthem began in the National Football League (NFL) after San Francisco 49ers quarterback (QB) Colin Kaepernick sat during the anthem, as opposed to the tradition of standing, in response to police brutality in America, before his team's third preseason game of 2016. Kaepernick also sat during the first two preseason games, but he went unnoticed.[69]\\r\\nIn November 2017, the California Chapter of the NAACP called on Congress to remove The Star-Spangled Banner as the national anthem. Alice Huffman, California NAACP president said: \\"it's racist; it doesn't represent our community, it's anti-black.\\"[70] The third stanza of the anthem, which is rarely sung and few know, contains the words, \\"No refuge could save the hireling and slave, From the terror of flight, or the gloom of the grave:\\", which some interpret as racist. The organization was still seeking a representative to sponsor the legislation in Congress at the time of their announcement.\\r\\nAs a result of immigration to the United States and the incorporation of non-English speaking people into the country, the lyrics of the song have been translated into other languages. In 1861, it was translated into German.[71] The Library of Congress also has record of a Spanish-language version from 1919.[72] It has since been translated into Hebrew[73] and Yiddish by Jewish immigrants,[74] Latin American Spanish (with one version popularized during immigration reform protests in 2006),[75] French by Acadians of Louisiana,[76] Samoan,[77] and Irish.[78] The third verse of the anthem has also been translated into Latin.[79]\\r\\nWith regard to the indigenous languages of North America, there are versions in Navajo[80][81][82] and Cherokee.[83]","input":"When did standing for the national anthem begin?"},{"output":"February 28, 1909","context":"International Women's Day (IWD) is celebrated on March 8 every year.[3] It is a focal point in the movement for women's rights.\\r\\nAfter the Socialist Party of America organised a Women's Day on February 28, 1909 in New York, the 1910 International Socialist Woman's Conference suggested a Women's Day be held annually. After women gained suffrage in Soviet Russia in 1917, March 8 became a national holiday there. The day was then predominantly celebrated by the socialist movement and communist countries until it was adopted in 1975 by the United Nations.\\r\\nToday, International Women's Day is a public holiday in some countries and largely ignored elsewhere.[4] In some places, it is a day of protest; in others, it is a day that celebrates womanhood.\\r\\n\\r\\n\\r\\nThe earliest Women's Day observance, called \\"National Woman's Day,\\"[5] was held on February 28, 1909 in New York, organized by the Socialist Party of America[6] at the suggestion of Theresa Malkiel.[7] Though there have been claims that the day was commemorating a protest by women garment workers in New York on March 8, 1857, researchers have described this as a myth.[8][9][10]\\r\\nIn August 1910, an International Socialist Women's Conference was organized to precede the general meeting of the Socialist Second International in Copenhagen, Denmark.[11] Inspired in part by the American socialists, German Socialist Luise Zietz proposed the establishment of an annual Women's Day and was seconded by fellow socialist and later communist leader Clara Zetkin, supported by K?te Duncker, although no date was specified at that conference.[12][13] Delegates (100 women from 17 countries) agreed with the idea as a strategy to promote equal rights including suffrage for women.[14] The following year on March 19, 1911, IWD was marked for the first time, by over a million people in Austria, Denmark, Germany and Switzerland.[6] In the Austro-Hungarian Empire alone, there were 300 demonstrations.[12] In Vienna, women paraded on the Ringstrasse and carried banners honouring the martyrs of the Paris Commune.[12] Women demanded that they be given the right to vote and to hold public office. They also protested against employment sex discrimination.[3] The Americans continued to celebrate National Women's Day on the last Sunday in February.[12]\\r\\nIn 1913 Russian women observed their first International Women's Day on the last Saturday in February (by the Julian calendar then used in Russia).[15]\\r\\nIn 1914 International Women's Day was held on March 8, possibly because that day was a Sunday, and now it is always held on March 8 in all countries.[15] The 1914 observance of the Day in Germany was dedicated to women's right to vote, which German women did not win until 1918.[15][16]\\r\\nIn London there was a march from Bow to Trafalgar Square in support of women's suffrage on March 8, 1914. Sylvia Pankhurst was arrested in front of Charing Cross station on her way to speak in Trafalgar Square.[17]\\r\\nOn March 8, 1917, on the Gregorian calendar, in the capital of the Russian Empire, Petrograd, women textile workers began a demonstration, covering the whole city. This marked the beginning of the Russian Revolution.[18][3] Women in Saint Petersburg went on strike that day for \\"Bread and Peace\\"?ÿ demanding the end of World War I, an end to Russian food shortages, and the end of czarism.[15] Leon Trotsky wrote, \\"23 February (8th March) was International Woman's Day and meetings and actions were foreseen. But we did not imagine that this 'Women's Day' would inaugurate the revolution. Revolutionary actions were foreseen but without date. But in the morning, despite the orders to the contrary, textile workers left their work in several factories and sent delegates to ask for support of the strike which led to mass strike... all went out into the streets.\\"[15] Seven days later, the Emperor of Russia, Nicholas II abdicated and the provisional Government granted women the right to vote.[6]\\r\\nFollowing the October Revolution, the Bolshevik Alexandra Kollontai and Vladimir Lenin made it an official holiday in the Soviet Union, but it was a working day until 1965. On May 8, 1965 by the decree of the USSR Presidium of the Supreme Soviet International Women's Day was declared a non-working day in the USSR \\"in commemoration of the outstanding merits of Soviet women in communistic construction, in the defense of their Fatherland during the Great Patriotic War, in their heroism and selflessness at the front and in the rear, and also marking the great contribution of women to strengthening friendship between peoples, and the struggle for peace. But still, women's day must be celebrated as are other holidays.\\"\\r\\nFrom its official adoption in Soviet Russia following the Revolution in 1917, the holiday was predominantly celebrated in communist countries and by the communist movement worldwide. It was celebrated by the communists in China from 1922.[12] After the founding of the People's Republic of China on October 1, 1949 the State Council proclaimed on December 23 that March 8 would be made an official holiday with women in China given a half-day off.[19]\\r\\nCommunist leader Dolores Ibrruri led a women's march in Madrid in 1936 on the eve of the Spanish Civil War.[12]\\r\\nThe United Nations began celebrating International Women's Day in the International Women's Year, 1975. In 1977, the United Nations General Assembly invited member states to proclaim March 8 as the UN Day for women's rights and world peace.[20]\\r\\nOn the occasion of 2010 International Women's Day the International Committee of the Red Cross (ICRC) drew attention to the hardships displaced women endure. The displacement of populations is one of the gravest consequences of today's armed conflicts. It affects women in a host of ways.[21]\\r\\nThough the celebration in the West was low-key, events took place in more than 100 countries[4] on March 8, 2011 to commemorate the 100th anniversary of International Women's Day.[22] In the United States, President Barack Obama proclaimed March 2011 to be \\"Women's History Month\\", calling Americans to mark IWD by reflecting on \\"the extraordinary accomplishments of women\\" in shaping the country's history.[4] Secretary of State Hillary Clinton launched the \\"100 Women Initiative: Empowering Women and Girls through International Exchanges\\", on the eve of IWD.[23] In the run-up to 2011 International Women's Day, the Red Cross called on States and other entities not to relent in their efforts to prevent rape and other forms of sexual violence that harm the lives and dignity of countless women in conflict zones around the world every year.[24]\\r\\nAustralia issued an IWD 100th anniversary commemorative 20-cent coin.\\r\\nIn Egypt, however, the day was a step back for women. In Egypt's Tahrir Square, hundreds of men came out not in support, but to harass the women who came out to stand up for their rights as the police and military stood by watching the events unfold in front of them, doing nothing to stop the crowds of men. \\"The women?ÿ some in headscarves and flowing robes, others in jeans?ÿ had marched to Cairo's central Tahrir Square to celebrate International Women's Day. But crowds of men soon outnumbered them and chased them out\\", wrote Hadeel Al-Shalchi for The Associated Press (AP).[25]\\r\\nThe UN theme for International Women's Day 2012 was Empower Rural Women?ÿ End Hunger and Poverty.[26] In that year, Oxfam America invited people to celebrate inspiring women in their lives by sending a free International Women's Day e-Card or honoring a woman whose efforts had made a difference in the fight against hunger and poverty with Oxfam's International Women's Day award.[27]\\r\\nOn the occasion of International Women's Day 2012, the ICRC called for more action to help the mothers and wives of people who have gone missing during armed conflict. The vast majority of people who go missing in connection with conflict are men. As well as the anguish of not knowing what has happened to the missing husband or son, many of these women face economic and practical difficulties. The ICRC underlined the duty of parties to a conflict to search for the missing and provide information to the families.[28]\\r\\nThe UN theme for International Women's Day 2013 was \\"A promise is a promise: Time for action to end violence against women\\",[29] while International Women's Day 2013 declared the year's theme as The Gender Agenda: Gaining Momentum.[30]\\r\\nThe 2013 International Women's Day, the International Committee of the Red Cross (ICRC) draw attention to the plight of women in prison.[31]\\r\\nThe UN theme for International Women's Day 2014 was \\"Equality for Women is Progress for All\\".[32][33]\\r\\nAmerican singer Beyonc also posted an International Women's Day video to her YouTube account. Throughout the video, her song \\"***Flawless\\" plays, which includes a portion of the \\"We Should All Be Feminists\\" speech given by author Chimamanda Ngozi Adichie. Beyonc is a feminist.[34]\\r\\nThe UN theme for International Women's Day 2015 was \\"Empowering Women, Empowering Humanity: Picture it!\\".[35] Governments and activists around the world will commemorate the 20th anniversary year of the Beijing Declaration and Platform for Action, an historic roadmap that sets the agenda for realizing women's rights.[36]\\r\\nThe International Women's Day theme for 2016 was \\"Planet 50-50 by 2030: Step It Up for Gender Equality\\".[37]\\r\\nThe President of India, Shri Pranab Mukherjee, in his message issued on the eve of International Women's Day said: \\"On the occasion of International Womens Day, I extend warm greetings and good wishes to the women of India and thank them for their contributions over the years in the building of our nation.\\"[38] The ministry of women and child development announced the setting up of four more one-stop crisis centers on March 8, in addition to the eight already functioning across the country.[39] Ahead of Women's Day, the national carrier Air India operated what it claimed to be the world's longest non-stop flight where the entire flight operations were handled by women, as part of International Women's Day celebrations. The flight, from Delhi to San Francisco, covered a distance of around 14,500 kilometers in around 17 hours.[40]\\r\\nThe theme for International Women's Day 2017 was \\"Women in the Changing World of Work: Planet 50-50 by 2030\\".[41]\\r\\nIn a message in support of International Women's Day, the UN Secretary-General Ant܇nio Guterres commented on how women's rights were being \\"reduced, restricted and reversed\\". With men still in leadership positions and a widening economic gender gap, he called for change \\"by empowering women at all levels, enabling their voices to be heard and giving them control over their own lives and over the future of our world\\".[42]\\r\\nThe theme for International Women's Day 2018 was'Time is Now: Rural and urban activists transforming womens lives'.[43]\\r\\nThe day is an official holiday in Afghanistan,[44] Angola, Armenia,[45] Azerbaijan,[46][47] Belarus,[48] Burkina Faso,[49] Cambodia,[50] China (for women only),[51] Cuba,[52] Georgia,[53] Guinea-Bissau, Eritrea, Kazakhstan,[54] Kyrgyzstan,[55] Laos,[56] Madagascar (for women only),[57] Moldova,[58] Mongolia,[59] Nepal, Russia, Tajikistan, Turkmenistan, Uganda, Ukraine,[60] Uzbekistan,[61] Vietnam,[62] and Zambia.[63]\\r\\nIn some countries, such as Cameroon,[64] Croatia,[65] Romania,[66] Bosnia and Herzegovina,[67] Bulgaria[68] and Chile,[69] the day is not a public holiday, but is widely observed nonetheless. On this day it is customary for men to give the women in their lives?ÿ friends, mothers, wives, girlfriends, daughters, colleagues, etc.?ÿ flowers and small gifts. In some countries (such as Bulgaria and Romania) it is also observed as an equivalent of Mother's Day, where children also give small presents to their mothers and grandmothers.[66] In Russia, the day has lost all political context through the time, becoming simply a day to honor women and feminine beauty.[70]\\r\\nIn the Czechoslovak Socialist Republic, huge Soviet-style celebrations were held annually. After the fall of Communism, the holiday, generally considered to be one of the major symbols of the old regime, fell into obscurity. International Women's Day was re-established as an official \\"important day\\" by the Parliament of the Czech Republic in 2004[71] on the proposal of the Social Democrats and Communists. This has provoked some controversy as a large part of the public as well as the political right see the holiday as a relic of the nation's Communist past.[71]\\r\\nInternational Women's Day sparked violence in Tehran, Iran on March 4, 2007, when police beat hundreds of men and women who were planning a rally. (A previous rally for the occasion was held in Tehran in 2003.)[72] Police arrested dozens of women and some were released after several days of solitary confinement and interrogation.[73] Shadi Sadr, Mahbubeh Abbasgholizadeh and several more community activists were released on March 19, 2007, ending a fifteen-day hunger strike.[74]\\r\\nIn Italy, to celebrate the day, men give yellow mimosas to women.[75][76] Communist politician Teresa Mattei chose the mimosa in 1946 as the symbol of IWD in Italy because she felt that the French symbols of the day, violets and lily-of-the-valley, were too scarce and expensive to be used effectively in Italy.[77]\\r\\nIn the United States, actress and human rights activist Beata Pozniak worked with the Mayor of Los Angeles and the Governor of California to lobby members of the U.S. Congress to propose official recognition of the holiday. In February 1994, H. J. Res. 316 was introduced by Rep. Maxine Waters, along with 79 cosponsors, in an attempt to officially recognize March 8 of that year as International Women's Day. The bill was subsequently referred to, and remained in, the House Committee on Post Office and Civil Service. No vote of either house of Congress was achieved on this piece of legislation.[78]","input":"When was the first women's day celebrated?"},{"output":"Immanuel Kant","context":"The nebular hypothesis is the most widely accepted model in the field of cosmogony to explain the formation and evolution of the Solar System (as well as other planetary systems). It suggests that the Solar System formed from nebulous material. The theory was developed by Immanuel Kant and published in his Allgemeine Naturgeschichte und Theorie des Himmels (\\"Universal Natural History and Theory of the Heavens\\"), published in 1755. Originally applied to the Solar System, the process of planetary system formation is now thought to be at work throughout the Universe.[1] The widely accepted modern variant of the nebular hypothesis is the solar nebular disk model (SNDM) or solar nebular model.[2] It offered explanations for a variety of properties of the Solar System, including the nearly circular and coplanar orbits of the planets, and their motion in the same direction as the Sun's rotation. Some elements of the nebular hypothesis are echoed in modern theories of planetary formation, but most elements have been superseded.\\r\\nAccording to the nebular hypothesis, stars form in massive and dense clouds of molecular hydrogengiant molecular clouds (GMC). These clouds are gravitationally unstable, and matter coalesces within them to smaller denser clumps, which then rotate, collapse, and form stars. Star formation is a complex process, which always produces a gaseous protoplanetary disk, proplyd, around the young star. This may give birth to planets in certain circumstances, which are not well known. Thus the formation of planetary systems is thought to be a natural result of star formation. A Sun-like star usually takes approximately 1?million years to form, with the protoplanetary disk evolving into a planetary system over the next?10ÿ100 million years.[1]\\r\\nThe protoplanetary disk is an accretion disk that feeds the central star. Initially very hot, the disk later cools in what is known as the T tauri star stage; here, formation of small dust grains made of rocks and ice is possible. The grains eventually may coagulate into kilometer-sized planetesimals. If the disk is massive enough, the runaway accretions begin, resulting in the rapid100,000 to 300,000?yearsformation of Moon- to Mars-sized planetary embryos. Near the star, the planetary embryos go through a stage of violent mergers, producing a few terrestrial planets. The last stage takes approximately 100?million to a billion years.[1]\\r\\nThe formation of giant planets is a more complicated process. It is thought to occur beyond the frost line, where planetary embryos mainly are made of various types of ice. As a result, they are several times more massive than in the inner part of the protoplanetary disk. What follows after the embryo formation is not completely clear. Some embryos appear to continue to grow and eventually reach 5ÿ10 Earth massesthe threshold value, which is necessary to begin accretion of the hydrogenÿhelium gas from the disk. The accumulation of gas by the core is initially a slow process, which continues for several million years, but after the forming protoplanet reaches about 30 Earth masses (M?) it accelerates and proceeds in a runaway manner. Jupiter- and Saturn-like planets are thought to accumulate the bulk of their mass during only 10,000?years. The accretion stops when the gas is exhausted. The formed planets can migrate over long distances during or after their formation. Ice giants such as Uranus and Neptune are thought to be failed cores, which formed too late when the disk had almost disappeared.[1]\\r\\n\\r\\n\\r\\nThere is evidence that Emanuel Swedenborg first proposed parts of the nebular hypothesis in 1734.[3][4] Immanuel Kant, familiar with Swedenborg's work, developed the theory further in 1755, publishing his own Universal Natural History and Theory of the Heavens, wherein he argued that gaseous clouds (nebulae) slowly rotate, gradually collapse and flatten due to gravity, eventually forming stars and planets.[2]\\r\\nPierre-Simon Laplace independently developed and proposed a similar model in 1796[2] in his Exposition du systeme du monde. He envisioned that the Sun originally had an extended hot atmosphere throughout the volume of the Solar System. His theory featured a contracting and cooling protosolar cloudthe protosolar nebula. As this cooled and contracted, it flattened and spun more rapidly, throwing off (or shedding) a series of gaseous rings of material; and according to him, the planets condensed from this material. His model was similar to Kant's, except more detailed and on a smaller scale.[2] While the Laplacian nebular model dominated in the 19th century, it encountered a number of difficulties. The main problem involved angular momentum distribution between the Sun and planets. The planets have 99% of the angular momentum, and this fact could not be explained by the nebular model.[2] As a result, astronomers largely abandoned this theory of planet formation at the beginning of the 20th century.\\r\\nA major critique came during the 19th century from James Clerk Maxwell (1831-1879), who maintained that different rotation between the inner and outer parts of a ring could not allow condensation of material.[5] Astronomer Sir David Brewster also rejected Laplace, writing in 1876 that \\"those who believe in the Nebular Theory consider it as certain that our Earth derived its solid matter and its atmosphere from a ring thrown from the Solar atmosphere, which afterwards contracted into a solid terraqueous sphere, from which the Moon was thrown off by the same process\\". He argued that under such view, \\"the Moon must necessarily have carried off water and air from the watery and aerial parts of the Earth and must have an atmosphere\\".[6] Brewster claimed that Sir Isaac Newton's religious beliefs had previously considered nebular ideas as tending to atheism, and quoted him as saying that \\"the growth of new systems out of old ones, without the mediation of a Divine power, seemed to him apparently absurd\\".[7]\\r\\nThe perceived deficiencies of the Laplacian model stimulated scientists to find a replacement for it. During the 20th century many theories addressed the issue, including the planetesimal theory of Thomas Chamberlin and Forest Moulton (1901), the tidal model of Jeans (1917), the accretion model of Otto Schmidt (1944), the protoplanet theory of William McCrea (1960) and finally the capture theory of Michael Woolfson.[2] In 1978 Andrew Prentice resurrected the initial Laplacian ideas about planet formation and developed the modern Laplacian theory.[2] None of these attempts proved completely successful, and many of the proposed theories were descriptive.\\r\\nThe birth of the modern widely accepted theory of planetary formationthe solar nebular disk model (SNDM)can be traced to the Soviet astronomer Victor Safronov.[8] His 1969 book Evolution of the protoplanetary cloud and formation of the Earth and the planets,[9] which was translated to English in 1972, had a long-lasting effect on the way scientists think about the formation of the planets.[10] In this book almost all major problems of the planetary formation process were formulated and some of them solved. Safronov's ideas were further developed in the works of George Wetherill, who discovered runaway accretion.[2] While originally applied only to the Solar System, the SNDM was subsequently thought by theorists to be at work throughout the Universe; as of 1 October 2017 astronomers have discovered 3,671 extrasolar planets in our galaxy.[11]\\r\\nThe star formation process naturally results in the appearance of accretion disks around young stellar objects.[12] At the age of about 1?million years, 100% of stars may have such disks.[13] This conclusion is supported by the discovery of the gaseous and dusty disks around protostars and T Tauri stars as well as by theoretical considerations.[14] Observations of these disks show that the dust grains inside them grow in size on short (thousand-year) time scales, producing 1?centimeter sized particles.[15]\\r\\nThe accretion process, by which 1?km planetesimals grow into 1,000?km sized bodies, is well understood now.[16] This process develops inside any disk where the number density of planetesimals is sufficiently high, and proceeds in a runaway manner. Growth later slows and continues as oligarchic accretion. The end result is formation of planetary embryos of varying sizes, which depend on the distance from the star.[16] Various simulations have demonstrated that the merger of embryos in the inner part of the protoplanetary disk leads to the formation of a few Earth-sized bodies. Thus the origin of terrestrial planets is now considered to be an almost solved problem.[17]\\r\\nThe physics of accretion disks encounters some problems.[18] The most important one is how the material, which is accreted by the protostar, loses its angular momentum. One possible explanation suggested by Hannes Alfvn was that angular momentum was shed by the solar wind during its T Tauri phase. The momentum is transported to the outer parts of the disk by viscous stresses.[19] Viscosity is generated by macroscopic turbulence, but the precise mechanism that produces this turbulence is not well understood. Another possible process for shedding angular momentum is magnetic braking, where the spin of the star is transferred into the surrounding disk via that star's magnetic field.[20] The main processes responsible for the disappearance of the gas in disks are viscous diffusion and photo-evaporation.[21][22]\\r\\nThe formation of planetesimals is the biggest unsolved problem in the nebular disk model. How 1?cm sized particles coalesce into 1?km planetesimals is a mystery. This mechanism appears to be the key to the question as to why some stars have planets, while others have nothing around them, not even dust belts.[23]\\r\\nThe formation timescale of giant planets is also an important problem. Old theories were unable to explain how their cores could form fast enough to accumulate significant amounts of gas from the quickly disappearing protoplanetary disk.[16][24] The mean lifetime of the disks, which is less than ten million (107)?years, appeared to be shorter than the time necessary for the core formation.[13] Much progress has been done to solve this problem and current models of giant planet formation are now capable of forming Jupiter (or more massive planets) in about 4 million years or less, well within the average lifetime of gaseous disks.[25][26][27]\\r\\nAnother potential problem of giant planet formation is their orbital migration. Some calculations show that interaction with the disk can cause rapid inward migration, which, if not stopped, results in the planet reaching the \\"central regions still as a sub-Jovian object.\\"[28] More recent calculations indicate that disk evolution during migration can mitigate this problem.[29]\\r\\nStars are thought to form inside giant clouds of cold molecular hydrogengiant molecular clouds roughly 300,000?times the mass of the Sun (M?) and 20?parsecs in diameter.[1][30] Over millions of years, giant molecular clouds are prone to collapse and fragmentation.[31] These fragments then form small, dense cores, which in turn collapse into stars.[30] The cores range in mass from a fraction to several times that of the Sun and are called protostellar (protosolar) nebulae.[1] They possess diameters of 0.01ÿ0.1?pc (2,000ÿ20,000?AU) and a particle number density of roughly 10,000 to 100,000?cm?3.[a][30][32]\\r\\nThe initial collapse of a solar-mass protostellar nebula takes around 100,000?years.[1][30] Every nebula begins with a certain amount of angular momentum. Gas in the central part of the nebula, with relatively low angular momentum, undergoes fast compression and forms a hot hydrostatic (not contracting) core containing a small fraction of the mass of the original nebula.[33] This core forms the seed of what will become a star.[1][33] As the collapse continues, conservation of angular momentum means that the rotation of the infalling envelop accelerates,[34][35] which largely prevents the gas from directly accreting onto the central core. The gas is instead forced to spread outwards near its equatorial plane, forming a disk, which in turn accretes onto the core.[1][34][35] The core gradually grows in mass until it becomes a young hot protostar.[33] At this stage, the protostar and its disk are heavily obscured by the infalling envelope and are not directly observable.[12] In fact the remaining envelope's opacity is so high that even millimeter-wave radiation has trouble escaping from inside it.[1][12] Such objects are observed as very bright condensations, which emit mainly millimeter-wave and submillimeter-wave radiation.[32] They are classified as spectral Class?0 protostars.[12] The collapse is often accompanied by bipolar outflowsjetsthat emanate along the rotational axis of the inferred disk. The jets are frequently observed in star-forming regions (see HerbigÿHaro (HH) objects).[36] The luminosity of the Class?0 protostars is high? a solar-mass protostar may radiate at up to 100 solar luminosities.[12] The source of this energy is gravitational collapse, as their cores are not yet hot enough to begin nuclear fusion.[33][37]\\r\\nAs the infall of its material onto the disk continues, the envelope eventually becomes thin and transparent and the young stellar object (YSO) becomes observable, initially in far-infrared light and later in the visible.[32] Around this time the protostar begins to fuse deuterium. If the protostar is sufficiently massive (above 80 Jupiter masses (MJ)), hydrogen fusion follows. Otherwise, if its mass is too low, the object becomes a brown dwarf.[37] This birth of a new star occurs approximately 100,000?years after the collapse begins.[1] Objects at this stage are known as Class I protostars,[12] which are also called young T Tauri stars, evolved protostars, or young stellar objects.[12] By this time the forming star has already accreted much of its mass: the total mass of the disk and remaining envelope does not exceed 10ÿ20% of the mass of the central YSO.[32]\\r\\nAt the next stage the envelope completely disappears, having been gathered up by the disk, and the protostar becomes a classical T Tauri star.[b] This happens after about 1?million years.[1] The mass of the disk around a classical T Tauri star is about 1ÿ3% of the stellar mass, and it is accreted at a rate of 10?7 to 10?9?M? per year.[40] A pair of bipolar jets is usually present as well.[41] The accretion explains all peculiar properties of classical T Tauri stars: strong flux in the emission lines (up to 100% of the intrinsic luminosity of the star), magnetic activity, photometric variability and jets.[42] The emission lines actually form as the accreted gas hits the \\"surface\\" of the star, which happens around its magnetic poles.[42] The jets are byproducts of accretion: they carry away excessive angular momentum. The classical T Tauri stage lasts about 10?million years.[1] The disk eventually disappears due to accretion onto the central star, planet formation, ejection by jets and photoevaporation by UV-radiation from the central star and nearby stars.[43] As a result, the young star becomes a weakly lined T Tauri star, which slowly, over hundreds of millions of years, evolves into an ordinary Sun-like star.[33]\\r\\nUnder certain circumstances the disk, which can now be called protoplanetary, may give birth to a planetary system.[1] Protoplanetary disks have been observed around a very high fraction of stars in young star clusters.[13][45] They exist from the beginning of a star's formation, but at the earliest stages are unobservable due to the opacity of the surrounding envelope.[12] The disk of a Class?0 protostar is thought to be massive and hot. It is an accretion disk, which feeds the central protostar.[34][35] The temperature can easily exceed 400?K inside 5?AU and 1,000?K inside 1?AU.[46] The heating of the disk is primarily caused by the viscous dissipation of turbulence in it and by the infall of the gas from the nebula.[34][35] The high temperature in the inner disk causes most of the volatile materialwater, organics, and some rocks to evaporate, leaving only the most refractory elements like iron. The ice can survive only in the outer part of the disk.[46]\\r\\nThe main problem in the physics of accretion disks is the generation of turbulence and the mechanism responsible for the high effective viscosity.[1] The turbulent viscosity is thought to be responsible for the transport of the mass to the central protostar and momentum to the periphery of the disk. This is vital for accretion, because the gas can be accreted by the central protostar only if it loses most of its angular momentum, which must be carried away by the small part of the gas drifting outwards.[34][47] The result of this process is the growth of both the protostar and of the disk radius, which can reach 1,000?AU if the initial angular momentum of the nebula is large enough.[35] Large disks are routinely observed in many star-forming regions such as the Orion nebula.[14]\\r\\nThe lifespan of the accretion disks is about 10?million?years.[13] By the time the star reaches the classical T-Tauri stage, the disk becomes thinner and cools.[40] Less volatile materials start to condense close to its center, forming 0.1ÿ1?m dust grains that contain crystalline silicates.[15] The transport of the material from the outer disk can mix these newly formed dust grains with primordial ones, which contain organic matter and other volatiles. This mixing can explain some peculiarities in the composition of Solar System bodies such as the presence of interstellar grains in the primitive meteorites and refractory inclusions in comets.[46]\\r\\nDust particles tend to stick to each other in the dense disk environment, leading to the formation of larger particles up to several centimeters in size.[49] The signatures of the dust processing and coagulation are observed in the infrared spectra of the young disks.[15] Further aggregation can lead to the formation of planetesimals measuring 1?km across or larger, which are the building blocks of planets.[1][49] Planetesimal formation is another unsolved problem of disk physics, as simple sticking becomes ineffective as dust particles grow larger.[23]\\r\\nOne hypothesis is formation by the gravitational instability. Particles several centimeters in size or larger slowly settle near the middle plane of the disk, forming a very thinless than 100?kmand dense layer. This layer is gravitationally unstable and may fragment into numerous clumps, which in turn collapse into planetesimals.[1][23] However, the differing velocities of the gas disk and the solids near the mid-plane can generate turbulence which prevents the layer from becoming thin enough to fragment due to gravitational instability.[50] This may limit the formation of planetesimals via gravitational instabilities to specific locations in the disk where the concentration of solids is enhanced.[51]\\r\\nAnother possible mechanism for the formation of planetesimals is the streaming instability in which the drag felt by particles orbiting through gas creates a feedback effect causing the growth of local concentrations. These local concentration push back on the gas creating a region where the headwind felt by the particles is smaller. The concentration is thus able to orbit faster and undergoes less radial drift. Isolated particles join these concentrations as they are overtaken or as they drift inward causing it to grow in mass. Eventually these concentrations form massive filaments which fragment and undergo gravitational collapse forming planetesimals the size of the larger asteroids.[52]\\r\\nPlanetary formation can also be triggered by gravitational instability within the disk itself, which leads to its fragmentation into clumps. Some of them, if they are dense enough, will collapse,[47] which can lead to rapid formation of gas giant planets and even brown dwarfs on the timescale of 1,000?years.[53] If these clumps migrate inward as the collapse proceeds tidal forces from the star can result in a significant mass loss leaving behind a smaller body.[54] However it is only possible in massive disksmore massive than 0.3?M?. In comparison, typical disk masses are 0.01ÿ0.03?M?. Because the massive disks are rare, this mechanism of the planet formation is thought to be infrequent.[1][18] On the other hand, this mechanism may play a major role in the formation of brown dwarfs.[55]\\r\\nThe ultimate dissipation of protoplanetary disks is triggered by a number of different mechanisms. The inner part of the disk is either accreted by the star or ejected by the bipolar jets,[40][41] whereas the outer part can evaporate under the star's powerful UV radiation during the T Tauri stage[56] or by nearby stars.[43] The gas in the central part can either be accreted or ejected by the growing planets, while the small dust particles are ejected by the radiation pressure of the central star. What is finally left is either a planetary system, a remnant disk of dust without planets, or nothing, if planetesimals failed to form.[1]\\r\\nBecause planetesimals are so numerous, and spread throughout the protoplanetary disk, some survive the formation of a planetary system. Asteroids are understood to be left-over planetesimals, gradually grinding each other down into smaller and smaller bits, while comets are typically planetesimals from the farther reaches of a planetary system. Meteorites are samples of planetesimals that reach a planetary surface, and provide a great deal of information about the formation of the Solar System. Primitive-type meteorites are chunks of shattered low-mass planetesimals, where no thermal differentiation took place, while processed-type meteorites are chunks from shattered massive planetesimals.[57]\\r\\nAccording to the solar nebular disk model, rocky planets form in the inner part of the protoplanetary disk, within the frost line, where the temperature is high enough to prevent condensation of water ice and other substances into grains.[58] This results in coagulation of purely rocky grains and later in the formation of rocky planetesimals.[c][58] Such conditions are thought to exist in the inner 3ÿ4?AU part of the disk of a Sun-like star.[1]\\r\\nAfter small planetesimalsabout 1?km in diameterhave formed by one way or another, runaway accretion begins.[16] It is called runaway because the mass growth rate is proportional to R4~M4/3, where R and M are the radius and mass of the growing body, respectively.[59] It is obvious that the specific (divided by mass) growth accelerates as the mass increases. This leads to the preferential growth of larger bodies at the expense of smaller ones.[16] The runaway accretion lasts between 10,000 and 100,000?years and ends when the largest bodies exceed approximately 1,000?km in diameter.[16] Slowing of the accretion is caused by gravitational perturbations by large bodies on the remaining planetesimals.[16][59] In addition, the influence of larger bodies stops further growth of smaller bodies.[16]\\r\\nThe next stage is called oligarchic accretion.[16] It is characterized by the dominance of several hundred of the largest bodiesoligarchs, which continue to slowly accrete planetesimals.[16] No body other than the oligarchs can grow.[59] At this stage the rate of accretion is proportional to R2, which is derived from the geometrical cross-section of an oligarch.[59] The specific accretion rate is proportional to M?1/3; and it declines with the mass of the body. This allows smaller oligarchs to catch up to larger ones. The oligarchs are kept at the distance of about 10{Hr (Hr=a(1-e)(M/3Ms)1/3 is the Hill radius, where a is the semimajor axis, e is the orbital eccentricity, and Ms is the mass of the central star) from each other by the influence of the remaining planetesimals.[16] Their orbital eccentricities and inclinations remain small. The oligarchs continue to accrete until planetesimals are exhausted in the disk around them.[16] Sometimes nearby oligarchs merge. The final mass of an oligarch depends on the distance from the star and surface density of planetesimals and is called the isolation mass.[59] For the rocky planets it is up to 0.1?M?, or one Mars mass.[1] The final result of the oligarchic stage is the formation of about 100 Moon- to Mars-sized planetary embryos uniformly spaced at about 10{Hr.[17] They are thought to reside inside gaps in the disk and to be separated by rings of remaining planetesimals. This stage is thought to last a few hundred thousand years.[1][16]\\r\\nThe last stage of rocky planet formation is the merger stage.[1] It begins when only a small number of planetesimals remains and embryos become massive enough to perturb each other, which causes their orbits to become chaotic.[17] During this stage embryos expel remaining planetesimals, and collide with each other. The result of this process, which lasts for 10 to 100?million years, is the formation of a limited number of Earth sized bodies. Simulations show that the number of surviving planets is on average from 2 to 5.[1][17][57][60] In the Solar System they may be represented by Earth and Venus.[17] Formation of both planets required merging of approximately 10ÿ20 embryos, while an equal number of them were thrown out of the Solar System.[57] Some of the embryos, which originated in the asteroid belt, are thought to have brought water to Earth.[58] Mars and Mercury may be regarded as remaining embryos that survived that rivalry.[57] Rocky planets, which have managed to coalesce, settle eventually into more or less stable orbits, explaining why planetary systems are generally packed to the limit; or, in other words, why they always appear to be at the brink of instability.[17]\\r\\nThe formation of giant planets is an outstanding problem in the planetary sciences.[18] In the framework of the solar nebular model two theories for their formation exist. The first one is the disk instability model, where giant planets form in the massive protoplanetary disks as a result of its gravitational fragmentation (see above).[53] The second possibility is the core accretion model, which is also known as the nucleated instability model.[18][29] The latter scenario is thought to be the most promising one, because it can explain the formation of the giant planets in relatively low-mass disks (less than 0.1?M?).[29] In this model giant planet formation is divided into two stages: a) accretion of a core of approximately 10?M? and b) accretion of gas from the protoplanetary disk.[1][18] Either method may also lead to the creation of brown dwarfs.[26][61] Searches as of 2011 have found that core accretion is likely the dominant formation mechanism.[61]\\r\\nGiant planet core formation is thought to proceed roughly along the lines of the terrestrial planet formation.[16] It starts with planetesimals that undergo runaway growth, followed by the slower oligarchic stage.[59] Hypotheses do not predict a merger stage, due to the low probability of collisions between planetary embryos in the outer part of planetary systems.[59] An additional difference is the composition of the planetesimals, which in the case of giant planets form beyond the so-called snow line and consist mainly of icethe ice to rock ratio is about 4 to 1.[24] This enhances the mass of planetesimals fourfold. However, the minimum mass nebula capable of terrestrial planet formation can only form 1ÿ2?M? cores at the distance of Jupiter (5?AU) within 10?million years.[59] The latter number represents the average lifetime of gaseous disks around Sun-like stars.[13] The proposed solutions include enhanced mass of the diska tenfold increase would suffice;[59] protoplanet migration, which allows the embryo to accrete more planetesimals;[24] and finally accretion enhancement due to gas drag in the gaseous envelopes of the embryos.[24][27][62] Some combination of the above-mentioned ideas may explain the formation of the cores of gas giant planets such as Jupiter and perhaps even Saturn.[18] The formation of planets like Uranus and Neptune is more problematic, since no theory has been capable of providing for the in situ formation of their cores at the distance of 20ÿ30?AU from the central star.[1] One hypothesis is that they initially accreted in the Jupiter-Saturn region, then were scattered and migrated to their present location.[63] Another possible solution is the growth of the cores of the giant planets via pebble accretion. In pebble accretion objects between a cm and a meter in diameter falling toward a massive body are slowed enough by gas drag for them to spiral toward it and be accreted. Growth via pebble accretion may be as much as 1000 times faster than by the accretion of planesimals.[64]\\r\\nOnce the cores are of sufficient mass (5ÿ10?M?), they begin to gather gas from the surrounding disk.[1] Initially it is a slow process, increasing the core masses up to 30?M? in a few million years.[24][62] After that, the accretion rates increase dramatically and the remaining 90% of the mass is accumulated in approximately 10,000?years.[62] The accretion of gas stops when the supply from the disk is exhausted. This happens gradually, due to the formation of a density gap in the protoplanetary disk and to disk dispersal.[29][65] In this model ice giantsUranus and Neptuneare failed cores that began gas accretion too late, when almost all gas had already disappeared. The post-runaway-gas-accretion stage is characterized by migration of the newly formed giant planets and continued slow gas accretion.[65] Migration is caused by the interaction of the planet sitting in the gap with the remaining disk. It stops when the protoplanetary disk disappears or when the end of the disk is attained. The latter case corresponds to the so-called hot Jupiters, which are likely to have stopped their migration when they reached the inner hole in the protoplanetary disk.[65]\\r\\nGiant planets can significantly influence terrestrial planet formation. The presence of giants tends to increase eccentricities and inclinations (see Kozai mechanism) of planetesimals and embryos in the terrestrial planet region (inside 4?AU in the Solar System).[57][60] If giant planets form too early, they can slow or prevent inner planet accretion. If they form near the end of the oligarchic stage, as is thought to have happened in the Solar System, they will influence the merges of planetary embryos, making them more violent.[57] As a result, the number of terrestrial planets will decrease and they will be more massive.[66] In addition, the size of the system will shrink, because terrestrial planets will form closer to the central star. The influence of giant planets in the Solar System, particularly that of Jupiter, is thought to have been limited because they are relatively remote from the terrestrial planets.[66]\\r\\nThe region of a planetary system adjacent to the giant planets will be influenced in a different way.[60] In such a region, eccentricities of embryos may become so large that the embryos pass close to a giant planet, which may cause them to be ejected from the system.[d][57][60] If all embryos are removed, then no planets will form in this region.[60] An additional consequence is that a huge number of small planetesimals will remain, because giant planets are incapable of clearing them all out without the help of embryos. The total mass of remaining planetesimals will be small, because cumulative action of the embryos before their ejection and giant planets is still strong enough to remove 99% of the small bodies.[57] Such a region will eventually evolve into an asteroid belt, which is a full analog of the asteroid belt in the Solar System, located from 2 to 4?AU from the Sun.[57][60]\\r\\nThousands of exoplanets have been identified in the last twenty years. The orbits of many of these planets and systems of planets differ significantly from the planets in the Solar System. The exoplanets discovered include hot-Jupiters, warm-Jupiters, super-Earths, and systems of tightly packed inner planets.\\r\\nThe hot-Jupiters and warm-Jupiters are thought to have migrated to their current orbits during or following their formation. A number of possible mechanisms for this migration have been proposed. Type I or Type II migration could smoothly decrease the semimajor axis of the planet's orbit resulting in a warm- or hot-Jupiter. Gravitational scattering by other planets onto eccentric orbits with a perihelion near the star followed by the circularization of its orbit due to tidal interactions with the star can leave a planet on a close orbit. If a massive companion planet or star on an inclined orbit was present an exchange of inclination for eccentricity via the Kozai mechanism raising eccentricities and lowering perihelion followed by circularization can also result in a close orbit. Many of the Jupiter sized planets have eccentric orbits which may indicate that gravitational encounters occurred between the planets, although migration while in resonance can also excite eccentricities.[67] The in situ growth of hot Jupiters from closely orbiting super Earths has also been proposed. The cores in this hypothesis could have formed locally or at a greater distance and migrated close to the star.[68]\\r\\nSuper-Earths and other closely orbiting planets are thought to have either formed in situ or to have migrated inward from their initial locations. The in situ formation of closely orbiting super-Earths would require a massive disk, the migration of planetary embryos followed by collisions and mergers, or the radial drift of small solids from farther out in the disk. The migration of the super-Earths, or the embryos that collided to form them, is likely to have been Type I due to their smaller mass. The resonant orbits of some of the exoplanet systems indicates that some migration occurred in these systems, while the spacing of the orbits in many of the other systems not in resonance indicates that an instability likely occurred in those systems after the dissipation of the gas disk. The absence of Super-Earths and closely orbiting planets in the Solar System may be due to the previous formation of Jupiter blocking their inward migration.[69]\\r\\nThe amount of gas a super-Earth that formed in situ acquires may depend on when the planetary embryos merged due to giant impacts relative to the dissipation of the gas disk. If the mergers happen after the gas disk dissipates terrestrial planets can form, if in a transition disk a super-Earth with a gas envelope containing a few percent of its mass may form. If the mergers happen too early runaway gas accretion may occur leading to the formation of a gas giant. The mergers begin when the dynamical friction due to the gas disk becomes insufficient to prevent collisions, a process that will begin earlier in a higher metalicity disk.[70] Alternatively gas accretion may be limited due to the envelopes not being in hydrostatic equilibrium, instead gas may flow through the envelope slowing its growth and delaying the onset of runaway gas accretion until the mass of the core reaches 15 Earth masses.[71]\\r\\nUse of the term \\"accretion disk\\" for the protoplanetary disk leads to confusion over the planetary accretion process. The protoplanetary disk is sometimes referred to as an accretion disk, because while the young T Tauri-like protostar is still contracting, gaseous material may still be falling onto it, accreting on its surface from the disk's inner edge.[35] In an accretion disk, there is a net flux of mass from larger radii toward smaller radii.[19]\\r\\nHowever, that meaning should not be confused with the process of accretion forming the planets. In this context, accretion refers to the process of cooled, solidified grains of dust and ice orbiting the protostar in the protoplanetary disk, colliding and sticking together and gradually growing, up to and including the high-energy collisions between sizable planetesimals.[16]\\r\\nIn addition, the giant planets probably had accretion disks of their own, in the first meaning of the word.[72] The clouds of captured hydrogen and helium gas contracted, spun up, flattened, and deposited gas onto the surface of each giant protoplanet, while solid bodies within that disk accreted into the giant planet's regular moons.[73]","input":"Who came up with the solar nebula theory?"},{"output":"Severino Montano","context":"Severino Montano (1915 in Laoag, Ilocos Norte ÿ 12 December 1980) is considered as one of the Titans of Philippine Theater.[by whom?] He was a playwright, director, actor and theater organizer with an output of one novel, 150 poems and 50 plays in his 65-year lifetime. Through the foundation of the Arena Theater, Montano institutionalized legitimate theater in the Philippines. He also have lifetime achievement award as part of National Artist of the Philippines.\\r\\n\\r\\nAcademically, Montano started his tutelage under a British mentor, Marie Leslie Prising, when he was thirteen. He studied at the University of the Philippines. She took Montano under her wing and endowed him with Western literature, the theater and Shakespeare. He was part of the UP Stage when he studied in the University of the Philippines. Then a scholarship took him to the famous \\"English 47\\" or \\"Workshop 47\\" playwriting workshops of George Pierce Baker at Yale University and guided by Broadway names and international personalities like Komissarjevsky of the famous Moscow Art Theater. He was conferred a Master of Fine Arts degree in playwriting and production by Yale University.\\r\\n\\r\\nIn 1946, he went to London to become a student of economist Harold Laski of the London School of Economics. Not long after, he was offered a teaching job at the American University in Washington, D.C. There, he finished his M.A. in Economic and Ph.D. in management and public administration, at the same time leading a playwriting-drama program as professor, playwright and play director. In 1950, 1952, 1962 and 1963, The Rockefeller Foundation extended him a world travel grant to visit cultural and art centers in 98 cities in Europe, the Middle East, South Asia, India, China and Japan.\\r\\n\\r\\nWhen he reurned to the Philippines, he already had 16 major plays to his credit. As Dean of Instruction of the Philippine Normal College, Montano organized the Arena Theater to bring drama to the masses. He used his own money, about a thousand pesos, to start the Arena Theater, a theater-in-the-round. Due to the PNC being unable to fund the theater, Montano volunteered his services to plan for a self-financing national drama program that would serve the grass-roots, the barrios of the Philippines.\\r\\n\\r\\nIn 1953, the theater opened with three one-act plays and broke all records of all performances in Philippine theater history.[citation needed] The roving troupe took theater to near and far-flung barrios in 47 provinces across the country. Four of his plays became tour staples: the full-length The Love of Leonor Rivera and three one-act plays, Parting at Calamba, The Ladies and the Senador and Sabina. The Arena Theater also began a graduate program for the training of playwrights, directors, technicians, actors and designers. The program was also extended to include a four-year undergraduate curriculum.\\r\\n\\r\\nHe trained and directed a new generation of dramatist including Rolando S. Tinio and Behn Cervantes. The Arena Theater Playwriting Contest also led to the discovery of Wilfrido Nolledo, Jesus T. Peralta and Estrella Alfon.\\r\\n\\r\\nHis awards include the Kalinangan Award from the city of Manila (1968), the Presidential Award for Merit in Drama and Theater (1961), the Citizens Committee for Mass Media Award (1967 and 1968), the Pamulinawen Award (1981), and the National Artist Award (2001). The last two awards were given posthumously.\\r\\n\\r\\nHis published works include The Love of Leonor Rivera (poetic tragedy in two-parts), My Morning Star (poetic historical tragedy in three-parts), But Not My Sons Any Longer (poetic tragedy in two-parts), Gabriela Silang (poetic historical tragedy in three-parts), The Merry Wives of Manila (comedy of manners in three-parts), Sabina (tragedy), The Ladies and the Senador (satirical comedy) and Parting at Calamba (historical drama).\\r\\n\\r\\nHe died on December 12, 1980, at the age of 65.\\r\\n\\r\\n\\"What we want to do at the Arena Theater is not to become professionals overnight, but to develop a professional approach, and grow into professionalism with time, experience and hard work as our legitimate passport towards that goal. In the meanwhile, we want to develop technicians and train playwrights, actors, directors, costume designers, lighting technicians and business-minded artists who will know how to get into production profitably.\\r\\n\\r\\nAt the Arena we also realize the importance that drama plays in personal adjustments. We understand the psychological value of role playing, and the tremendous influence dramatic art may have in developing the character of the individual and his relationship to a social group. We aim to reach other areas of the country through our student teachers who comes from all over the Philippines to work with us.\\"","input":"Who is the proponent of the arena theater?"},{"output":"three","context":"Antigua Guatemala (Spanish pronunciation:?[an?ti?wa ?wate?mala]) (commonly referred to as just Antigua or la Antigua) is a city in the central highlands of Guatemala famous for its well-preserved Spanish Baroque-influenced architecture as well as a number of ruins of colonial churches. It served as the capital of the Kingdom of Guatemala. It has been designated a UNESCO World Heritage Site.\\r\\nAntigua Guatemala serves as the municipal seat for the surrounding municipality of the same name. It also serves as the departmental capital of Sacatepquez Department.\\r\\n\\r\\n\\r\\nThe city had a peak population of some 60,000 in the 1770s; the bulk of the population moved away in the late 18th century. Despite significant population growth in the late 20th century, the city had only reached half that number by the 1990s. According to the 2007 census, the city has some 34,685 inhabitants.[citation needed]\\r\\nAntigua Guatemala means \\"Ancient Guatemala\\" and was the third capital of Guatemala. The first capital of Guatemala was founded on the site of a Kakchikel-Maya city, now called Iximche, on Monday, July 25, 1524the day of Saint Jamesand therefore named Ciudad de Santiago de los Caballeros de Goathemalan (City of Saint James of the Knights of Guatemala). Naturally, St. James became the patron saint of the city.[citation needed]\\r\\nAfter several Kaqchikel uprisings, the capital was moved to a more suitable site in the Valley of Almolonga (place of water) on November 22, 1527, and kept its original name. This new city was located on the site of present-day San Miguel Escobar,[1] which is a neighborhood in the municipality of Ciudad Vieja.[2] This city was destroyed on September 11, 1541 by a devastating lahar from the Volcn de Agua.[3] As a result, the colonial authorities decided to move the capital once more, this time five miles away to the Panchoy Valley. So, on March 10, 1543 the Spanish conquistadors founded present-day Antigua, and again, it was named Santiago de los Caballeros.[citation needed] For more than 200 years, it served as the seat of the military governor of the Spanish colony of Guatemala, a large region that included almost all of present-day Central America and the southernmost State of Mexico: Chiapas.\\r\\nSantiago de los Caballeros was the third seat of the capital called kingdom of Guatemala, which included the current states of Guatemala, Belize, El Salvador, Honduras, Nicaragua and Costa Rica, besides modern state of Chiapas in Mexico. After a flood destroyed the second city, located in the Valley of Almolonga, on the slopes of Volcn de Agua a new city was built in 1543 in the Valley of Panchoy, and it was established as head of the Real Audiencia of Guatemala in 1549.[4] During its development and splendor, it was known as one of the three most beautiful cities of the Spanish Indies.\\r\\nThe city was laid out in a square pattern, with streets running north to south and from east to west, with a central square. Both church and government buildings were designated important places around the central plaza.[5][Note 1] Between 1549 and 1563, property southeast of the square was sold to the crown and occupied by the first president of the Real Audiencia de los Confines: the lawyer Alonso Lopez Cerrato, who also served as governor and captain general.[5][Note 2] The original building was small and paneled with portal, tile roof and adobe walls. The city is surrounded by three enormous volcanoes and mountains, plains and hills. This territory was called \\"Valley of Guatemala\\" and had 73 villages, two towns and the city of Santiago de los Caballeros.[6]\\r\\nDue to constant problems between the conquerors and the representatives of the crown sent by the king of Spain, the Audiencia de los Confines was abolished in 1565.[Note 3] In 1570 the hearing was restored, this time independent of the viceroy of Mexico and the new organization was called Audiencia of Guatemala.[7]\\r\\nThe Franciscan monks were the first to move into the valley Panchoy, the new capital of the Kingdom of Guatemala, and built a chapel on the site where later the Church Escuela de Cristo would be erected. This primitive chapel was destroyed in 1575 by an earthquake and during the next ten years collections were made to build the new complex, two blocks from the previous one.[Note 4] The Franciscan complex became a major cultural and religious center for the entire Captaincy General of Guatemala: Theologians, jurists, philosophers, physicists and mathematicians studied in the school of San Buenaventura, which was located where the monastery ruins are. Notable students included Crist܇bal de Villalpando, Thomas Merlo and Alonso de Paz.\\r\\nThe first building of a cathedral was begun in 1545 with the debris brought from the destroyed settlement in the valley of Almolonga; however, its construction was hampered by frequent earthquakes along the years.[9] The city was the final resting place of the great Spanish chronicler Bernal Daz del Castillo, and his remains were interred in one of the churches that was eventually ruined by earthquakes.\\r\\nThe construction of the royal houses for the residence of the Captain General and the members of the Real Audiencia started in 1558; the complex also included the Royal Treasury, jail, Army quarters, the Hall of Arms and the housing of Audiencia members.[5]\\r\\nIn the sixteenth century, there were several important earthquakes on the following dates:\\r\\nIn 1566 King Felipe II of Spain gave it the title of \\"Muy Noble y Muy Leal\\" (\\"Very Noble and Very Loyal\\").\\r\\nThe Jesuits founded the school of \\"San Lucas of the Society of Jesus\\" in 1608, which became famous and was unrivaled in terms of literature and grammar lessons; it was attended by the elite cassles of the city society, such as Francisco Antonio Fuentes y Guzman, the chronicler Francisco Vzquez and Pedro Betancourt.[11] On 18 July 1626 the Jesuit temple was inaugurated; as the rest of the city, it suffered and was damaged by continuous earthquakes that struck the city between the sixteenth and eighteenth centuries.[12]\\r\\nThe monks of San Juan de Dios founded their hospital and monastery in 1636 and thereafter were in charge of the hospitals in the Kingdom of Guatemala. Their hospitals were:\\r\\nThe temple of the Escuela de Cristo -School of Christ- was founded in the parish of Our Lady of Remedios in 1664 and from 1689 onward it was known as the Congregation of San Felipe de Neri. Meanwhile, around 1690 the Jesuits founded another school: the \\"San Francisco de Borja\\" where eventually would study and serve as principal the poet and priest Rafael Landivar, S.J..\\r\\nIn the seventeenth century there were two types of nuns: discalced and urban.[14]\\r\\nPedro de San Jos Betancourt came to Guatemalan land in 1650 from his native Tenerife. Upon arrival he suffered a serious illness, during which he had the first opportunity to be with the poor and dispossessed. After his recovery he wanted to make ecclesiastical studies but unable to do so, professed as a Franciscan tertiary in the Convent of San Francisco in Santiago de los Caballeros . He founded shelters for the poor, indigenous and homeless and founded the Order of the Brothers of Our Lady of Bethlehem in 1656, to serve the poor.[15] The Santo Hermano Pedro wrote several books, including: Instruction De la Cruz's brother, Crown of the Passion of Jesus Christ our good or Rules Confraternity Betlemitas. It is considered the great evangelist of the West Indies, just as San Francisco Javier what is the East Indies . The Holy attended poor, sick, orphaned, and dying, and was a precursor of Human Rights . On the other hand, was the first literacy of America and the Order of Betlemitas turn was the first religious order born in the Americas. The Santo Hermano Pedro was a man ahead of his time, both in their methods to teach reading and writing to illiterate and in patient treatment.\\r\\nFrancisco Marroqun, first bishop of Guatemala, sent the Spanish King a letter in 1548, asking for a superior education institution for Guatemala, but the letter went unanswered. Towards the end of his life, in 1562, Marroqun left some money in his will to establish a school -which eventually was the \\"Santo Toms de Aquino school\\"- where grammar, arts, philosophy and theology would be taught. Poor Spanish children would be the beneficiaries of this pious work, as they could not travel to those cities where there were universities already, like Mxico City in the New Spain. Historian John Tate Lanning said on this that: \\"Marroqun testament is so famous, that many people that has not even laid eyes on it say that there are things in the document that are really not in it. Marroqun never talks about a University, much less establishing one...\\"[16] On the other hand, there is indeed a document from Mayor Pedro Crespo Suarez, who left twenty thousand pesos after his death to set up classes in the University that \\"was being asked to crown\\".[17]\\r\\nThe Jesuits opposed a university establishment, given that they did not like the idea of having the other regular clergy orders -Mercedarians, Franciscans and Order of Preachers- taking the initiative in religious and educational issues.[17] In August 1655, the Society of Jesus had bought the whole lot from the Daz del Castillo family and by then, their San Lucas School was well known in the region and it even granted two university degrees.[18] In 1653, the San Lucas School had a staff of only thirteen priests, a very small number compared to the size of the building; the Jesuits, however, made a major impact on the cultural and educational life in the Capitana General of Guatemala. The school was the city's most prestigious and from it graduated most of the elite members of society of the time. Most of its students were secular and went on to get the best positions in the country.[18]\\r\\nAfter several decades, petitions and lawsuits, king Carlos II expedited a royal decree, on January 31, 1676, allowing Capitana General of Guatemala to set its university or \\"General Study\\".[Note 6] After a lengthy and cumbersome organization process that lasted five years, the university started classes on January 7, 1681, with more than sixty registered students under President Doctor Jos de Ba?os y Soto Mayor, Cathedral archdeacon, King of Spain preacher and Doctor from University of Osuna.[17] The university began its activities under the protection of Saint Carlos Borromeo, and its norms and regulations were copied from those of the Mxico university which, in turn, were adapted from those of the Universidad de Salamanca in Spain.\\r\\nThe first classes given in the university were:\\r\\nThe Royal University of San Carlos Borromeo became pontifical the Papal Bull of Pope Innocent XI issued dated 18 June 1687.\\r\\nThe strongest earthquakes experienced by the city of Santiago de los Caballeros before its final move in 1776 were the San Miguel earthquakes in 1717. At that time, the power of the Catholic Church over the Spanish Empire citizens was absolute and, therefore, any natural disaster was considered as divine punishment. In the city, people also believed that the proximity of the Volcn de Fuego (English:Volcano of Fire) was the cause of earthquakes; great architect Diego de Porres even said that all the earthquakes were caused volcano explosions.[19]\\r\\nOn August 27 there was a strong eruption of Volcn de Fuego, which lasted until August 30; the residents of the city asked for help to Santo Cristo of the Cathedral and to the Virgen del Socorro who were sworn patrons of the Volcan de Fuego. On August 29 a Virgen del Rosario procession took to the streets after a century without leaving her temple, and there were many more holy processions until 29 September, the day of San Miguel. Early afternoon earthquakes were minor, but at about 7:00 pm there was a strong earthquake that forced residents to leave their homes; tremors and rumblings followed until four o'clock. The neighbors took to the streets and loudly confessed their sins, bracing for the worst.[20]\\r\\nThe San Miguel earthquake damaged the city considerably, to the point that some rooms and walls of the Royal Palace were destroyed. There was also a partial abandonment of the city, food shortages, lack of manpower and extensive damage to the city infrastructure; not to mention numerous dead and injured.[20] These earthquakes made the authorities consider moving to a new city less prone to seismic activity. City residents strongly oppose the move, and even took to the Royal Palace in protest; in the end, the city did not move, but the number of elements in the Army Battalion to safeguard the order was considerable.[5] The damage to the palace were repaired by Diego de Porres, who finished repairs in 1720; although there are indications that there were more jobs done by Porres until 1736.[5]\\r\\nOn March 4, 1751, the San Casimiro earthquake destroyed the city of Santiago de Guatemala once more. The church roof of the Society of Jesus complex fell to the ground, forcing the Jesuits to once again ask for help to the parishioners to rebuild; once again, the building was among the most beautiful of the city when the repairs were completed.[21] In fact, a prosperity period began after the San Casimiro earthquake, as the city saw major improvements, such as street embellishment and tap water system introduction. A new City Hall was built in and on July 17, 1753 work on the Jesuit plaza in front of the church is finished.[21]\\r\\nOn 12 June 1773 Captain General Martn de Mayorga was inaugurated, and alongside Corts y Larrz and the regular clergy vicars, were the top authorities in the Kingdom of Guatemala and would be the main characters in the events that followed the 1773 earthquakes.[22]\\r\\nIn 1773, the Santa Marta earthquakes destroyed much of the town, which led to the third change in location for the city.[23] The Spanish Crown ordered, in 1776, the removal of the capital to a safer location, the Valley of the Shrine, where Guatemala City, the modern capital of Guatemala, now stands. This new city did not retain its old name and was christened Nueva Guatemala de la Asunci܇n (New Guatemala of the Assumption), and its patron saint is Our Lady of the Assumption. The badly damaged city of Santiago de los Caballeros was ordered abandoned, although not everyone left, and was thereafter referred to as la Antigua Guatemala (the Old Guatemala).[23]\\r\\nThe Santa Marta earthquake practically demolished the church and sections of the convent of the Society of Jesus. Its cloisters and towers were in ruins, the walls were at dangerous angles and the \\"Casa de Ejercicios\\" was turned into rubble. By a Royal decree of July 21, 1775, the city move to the \\"Virgin valley\\" was authorized. This was a final order that had to be obeyed by all the people, who started to move slowly, starting on December of that year. In order to build the new city it was necessary to get construction material from the old abandoned churches in Santiago de Guatemala. However, in the case of the Society of Jesus church, there was strong opposition from the neighbors to any possible dismantling of the structure since they considered that it could still be repaired.[24]\\r\\nAfter the independence of Guatemala from Spain in 1821, the Jesuit complex became public property once again and was in several lawsuits that lasted until 1829, when the regular clergy and the conservative Aycinena clan were expelled from Central America after the invasion of liberal general Francisco Morazn and the establishment of a secular government.[25] The new liberal government decreed that all the confiscated Catholic church possessions had to be turned into elementary schools and university classrooms.[24]\\r\\nAs of 1850, Antigua had an estimated population of 9,000.[26] and by 1865, the building was functioning as a vapor activated thread mill, but it was not profitable due to a lack of expert technicians and raw material; and by 1872, the Jesuits were once again expelled from Guatemala by the liberal regime of Justo Rufino Barrios.[27]\\r\\nIn 1884 City Hall made an announcement that it intended to transform the old Society of Jesus buildings into a market, in spite of the strong opposition from the neighbors that already had small shops on the plaza. It was until 1912 that a market was placed in the complex.[24]\\r\\nIn April 1920, during the very last days of Manuel Estrada Cabrera regime, prince Wilhelm of Sweden visited Antigua Guatemala and wrote about his impressions on the city in his book Between two continents.[30] His book is an objective description of the terrible conditions the road and the ruins used to be in: for some little way outside Guatemala City it was a fairly decent car ride, but then the roads began developing sandhills, and later, ravines of tumbled stone as two years earlier, the country had been devastated by the a powerful earthquake and the government corruption made the recovery impossible.[31] The hills grew steeper and steeper, the jolting more pronounced and the stones even sharper; besides, on top of the road was a two-foot layer of dust which hid the pitfalls but did not detract from their effect.[31] Along the way, they passed long lines of Indians on their way to Guatemala City, carrying their heavy burdens with apparent ease; men, women and children carried something in the way of a load, and they all carried it quickly. With respect to traffic, it was almost non-existent, aside from mule pulled wagons.[32]\\r\\nAfter passing Mixco, the road was going more steeply upward, with a precipitous drop on one side and sheer cliff rising on the other; here and there a cross stood by the wayside, marking the spot where some traveler had died. After reaching the highest point, they started down towards Antigua. The city was in sight when a person in uniform planted himself in front of the car and it turned out to be the city commandant, along with six soldiers with wooden guns.[33] Compared to Guatemala City at the time, Antigua was quite nicely kept, although all the churches were equally dilapidated and left entirely to themselves, as rebuilding since 1773 was confined to the strictly necessary. For the most part, only blank walls and shattered domes remained to greet the visitor by 1920.[34] and some of the churches were in pitiful conditions; in Santa Clara, for example, a mule was grazing, and in the Church of Grace a native family had taken up its quarters, along with their varied collection of domestic animals.[35]\\r\\nBut there were other monuments in decent shape:\\r\\nCentral Park -Parque Central- is the heart of the city. The reconstructed fountain there is a popular gathering spot. Off to the side of the Central Park, the Arco de Santa Catalina is among the many notable architectural landmarks of La Antigua.\\r\\nLa Antigua is noted for its very elaborate religious celebrations during Lent (Cuaresma), leading up to Holy Week (Semana Santa) and Easter (Pascua). Each Sunday in Lent, one of the local parishes sponsor a Procession through the streets of Antigua. Elaborate and beautiful artistic carpets predominantly made of dyed sawdust, flowers, pine needles and even fruits and vegetables adorn the processions' path.[citation needed]\\r\\nDue to its popularity amongst tourists and its very well developed tourism infrastructure, Antigua Guatemala is often used as a central location in which many choose to set up base and from here, visit other tourist areas in Guatemala and Central America. Cruise ships that dock at Guatemalan ports offer trips to Antigua from both the Pacific and Atlantic. Antigua also holds a sizeable retirement community from the US as well as Europe.[citation needed]\\r\\nHistorically, the area was considered to be one of the finest agriculturally in Guatemala.[26] Tourism is the main driver of the economy. Antigua is also a coffee-producing region of Anacaf.\\r\\nAntigua is known as a destination for people who want to learn Spanish through immersion. There are many Spanish language schools in Antigua, and it is one of the most popular and best recognized centers for Spanish language study by students from Europe, Asia and North America. Language institutes are one of the primary industries of Antigua, along with tourism.\\r\\nAntigua GFC football club has played in the Guatemala top division for several years but have been playing in the second division lately. Their home stadium is the Estadio Pensativo which has a capacity of 9,000. They are nicknamed Los panzas verdes (\\"Green bellies\\").\\r\\nThere are many restaurants in Antigua. Small eateries can be found at the Antigua marketplace, next to the central bus stop, where you will find traditional Guatemalan dishes, the traditional/Chapin breakfast, for example; refried beans, fried egg, fried plantain, fresh cheese served with handmade tortillas. Throughout Antigua you can find many cuisines from around the world, Mediterranean, Italian, Asian, American, and even British pies and French pastries!\\r\\nThree large volcanoes dominate the horizon around Antigua. The most commanding, to the south of the city, is the Volcn de Agua or \\"Volcano of Water\\", some 3,766 metres (12,356?ft) high. When the Spanish arrived, the inhabitants of the zone, Kakchikel Mayas, called it Hunap~ (and they still do). However, it became known as Volcn de Agua after a lahar from the volcano buried the second site of the capital, which prompted the Spanish authorities to move the capital to present-day Antigua. The original site of the 2nd capital is now the village San Miguel Escobar.\\r\\nTo the west of the city are a pair of peaks, Acatenango, which last erupted in 1972, some 3,976 metres (13,045?ft) high, and the Volcn de Fuego or \\"Volcano of Fire\\", some 3,763 metres (12,346?ft) high. \\"Fuego\\" is famous for being almost constantly active at a low level. Steam and gas issue from its top daily, a larger eruption occurred in September 2012.\\r\\nAntigua is a growing tourist destination in Guatemala as it is close to Guatemala City but is much calmer and safer, with more tourist oriented activities. It is possible to take buses from Antigua to many parts of Guatemala, many travel agencies offer shuttles to the main touristic places: Monterrico beach, Atitlan Lake, Coban, Lanqun (Semuc Champey), or Tikal, though the transportation is more central in Guatemala City. Antigua is also known for its chocolate makers.[36]\\r\\nBefore it was declared a National Monument by president Jorge Ubico on March 30, 1944 the city ruins were practically abandoned. The following galleries show images of the destruction of the structures due to earthquakes and abandonment. There were other churches, like Nuestra Se?ora del Carmen and the Society of Jesus, that endured the 1773 earthquake relatively well, but they were abandoned and the earthquakes from 1917ÿ18 and 1976 destroyed them. In the particular case of de San Francisco El Grande church, this was in good structural shape after the 1773 and 1917 earthquakes and it was rebuilt in 1967 when the Franciscans returned to Guatemala, which eventually protected the stsructure from the 1976 earthquake. Finally, La Merced church was practically new in 1773, and it has withstand time and earthquakes since; the church was not abandoned in 1776, but it was indeed abandoned in 1829 when the Mercedarians were expelled from Central America by general Francisco Morazn, along with the rest of regular clergy and the conservative party members and Aycinena family.[25]\\r\\nThe most traditional processions are:\\r\\nIn 1935, the film The New Adventures of Tarzan, was filmed on location in Guatemala, taking advantage of the help from the United Fruit Company and president Jorge Ubico. The places where the filming was made were:\\r\\nThe initial earthquake sequences from the Jack Nicholson's film The Border were filmed in Antigua Guatemala, specifically in La Recoleccion Architectural Complex.[41]\\r\\nAntigua Guatemala has a subtropical highland climate (K?ppen: Cwb).\\r\\nAntigua Guatemala is surrounded by municipalities of Sacatepquez:\\r\\nCoordinates: 1434N 9044W? / ?14.567N 90.733W? / 14.567; -90.733","input":"How many volcanoes are around the city antigua guatemala?"},{"output":"Mississippi","context":"This article is a list of U.S. states, territories, and the District of Columbia ordered by poverty rate. 2014 statistics are not identical to official poverty rates because they include children not counted in the official numbers (see Revised Tables link below). Supplemental Poverty Measure takes into account differences in cost of living between states (i.e. housing costs appreciably higher/lower than the national average) as well as taxes and the value of government assistance programs. All data are from the United States Census Bureau.\\r\\nThe lowest poverty rate was in New Hampshire, and the highest poverty rate was in American Samoa (the highest poverty rate among the states was in Mississippi).","input":"Which us region has the highest poverty rate?"},{"output":"James Augustus Hicky","context":"The modern newspaper is a European invention.  The oldest direct ancestors of the modern newspaper were the handwritten news sheets that circulated widely in Venice  as early as 1566. These weekly news sheets were filled with information on wars and politics in Italy and Europe.  The first printed newspapers were published weekly in Germany from 1609. Typically they were heavily censored by the government and reported only foreign news, and current prices. After the English government relaxed censorship in 1695, newspapers flourished in London and a few other cities including Boston and Philadelphia. By the 1830s high speed presses could print thousands of papers cheaply, so low cost daily.\\r\\n\\r\\nAvvisi, or Gazzettes (not gazettes), were a mid-16th-century Venice phenomenon. They were issued on single sheets, folded to form four pages, and issued on a weekly schedule. These publications reached a larger audience than handwritten news had in early Rome. Their format and appearance at regular intervals were two major influences on the newspaper as we know it today. The idea of a weekly, handwritten newssheet went from Italy to Germany and then to Holland.[1]\\r\\n\\r\\nThe term newspaper became common in the 17th century. However, in Germany, publications that we would today consider to be newspaper publications, were appearing as early as the 16th century. They were discernibly newspapers for the following reasons: they were printed, dated, appeared at regular and frequent publication intervals, and included a variety of news items (unlike single item news mentioned above).  The emergence of the new media branch was based on the spread of the printing press from which the publishing press derives its name. Historian Johannes Weber says, \\"At the same time, then, as the printing press in the physical, technological sense was invented, 'the press' in the extended sense of the word also entered the historical stage. The phenomenon of publishing was born. The German-language Relation aller Frnemmen und gedenckwrdigen Historien, printed from 1605 onwards by Johann Carolus in Strasbourg, was the first newspaper.[2]\\r\\n\\r\\nOther early papers include the Dutch Courante uyt Italien, Duytslandt, &c. of 1618 which was the first to appear in folio- rather than quarto-size. Amsterdam, a center of world trade, quickly became home to newspapers in many languages, often before they were published in their own country.[3][4]\\r\\n\\r\\nThe first English-language newspaper, Corrant out of Italy, Germany, etc., was published in Amsterdam in 1620. A year and a half later, Corante, or weekely newes from Italy, Germany, Hungary, Poland, Bohemia, France and the Low Countreys. was published in England by an \\"N.B.\\" (generally thought to be either Nathaniel Butter or Nicholas Bourne) and Thomas Archer.[5]\\r\\n\\r\\nThe first newspaper in France was published in 1631, La Gazette (originally published as Gazette de France).[6]\\r\\n\\r\\nThe first newspaper in Portugal, A Gazeta da Restaura??o, was published in 1641 in Lisbon.  The first Spanish newspaper, Gaceta de Madrid, was published in 1661.\\r\\n\\r\\nPost- och Inrikes Tidningar (founded as Ordinari Post Tijdender) was first published in Sweden in 1645, and is the oldest newspaper still in existence, though it now publishes solely online.[7]\\r\\n\\r\\nOpregte Haarlemsche Courant from Haarlem, the Netherlands, first published in 1656, is the oldest paper still printed. It was forced to merge with the newspaper Haarlems Dagblad in 1942 when Germany occupied the Netherlands. Since then the Haarlems Dagblad appears with the subtitle Oprechte Haerlemse Courant 1656 and considers itself to be the oldest newspaper still publishing.\\r\\n\\r\\nMerkuriusz Polski Ordynaryjny was published in Krak܇w, Poland in 1661.\\r\\n\\r\\nThe first successful English daily, The Daily Courant, was published from 1702 to 1735. The first editor, for 10 days in March 1702, was Elizabeth Mallet, who for years had operated her late husband's printing business.[8][9][10]\\r\\n\\r\\nNews was highly selective and often propagandistic. Readers were eager for sensationalism, such as accounts of magic, public executions and disasters; this material did not pose a threat to the state, because it did not pose criticism of the state.\\r\\n\\r\\nNewspaper publications, under the name of corantos, came to the Dutch Republic in the 17th century, first to Amsterdam, which was a center of trade and travelers, an obvious locale for news publication. The term coranto was adopted by other countries for a time as well. The coranto differed from previous German newspapers before it in format. The coranto dropped the highly illustrated German title page, instead including a title on the upper first page of the publication ÿ the masthead common in today's periodicals. Corantos also adopted a two-column format, unlike the previous single-column format, and were issued on halfsheets.[11]\\r\\n\\r\\nOn 7 November 1665, The London Gazette (at first called The Oxford Gazette) began publication.[12] It decisively changed the look of English news printing, echoing the coranto format of two columns, a clear title, and a clear date. It was published twice a week.[13] Other English papers started to publish three times a week, and later the first daily papers emerged.[14]\\r\\n\\r\\nThe newspapers typically  included short articles, ephemeral topics, some illustrations and service articles (classifieds). They were often written by multiple authors, although the authors' identities were often obscured. They began to contain some advertisements, and they did not yet include sections. Mass market papers emerged, including Sunday papers for workers to read in their leisure time The Times adopted new technologies and set the standards for other newspapers. This newspaper covered major wars, among other major events.\\r\\n\\r\\nIn Boston in 1690, Benjamin Harris published Publick Occurrences Both Forreign and Domestick. This is considered the first newspaper in the American colonies even though only one edition was published before the paper was suppressed by the colonial officials, possibly due to censorship and control issues. It followed the two-column format and was a single sheet, printed on both sides.\\r\\n\\r\\nIn 1704, the governor allowed The Boston News-Letter, a weekly, to be published, and it became the first continuously published newspaper in the colonies. Soon after, weekly papers began publishing in New York and Philadelphia. The second English-language newspaper in the Americas was the Weekly Jamaica Courant.[15] These early newspapers followed the British format and were usually four pages long. They mostly carried news from Britain and content depended on the editor's interests. In 1783, the Pennsylvania Evening Post became the first American daily.\\r\\n\\r\\nIn 1751, John Bushell published the Halifax Gazette, the first Canadian newspaper.\\r\\n\\r\\nGermany invented printing, and produced the first newspapers. However, Germany was divided into so many competing  states that before unification in 1871, no newspaper played a dominant role. One example of this type of merchant was the 16th-century German financialist, Fugger. He not only received business news from  his correspondents, but also sensationalist and gossip news as well. It is evident in the correspondence of Fugger with his network that fiction and fact were both significant parts of early news publications.  16th century Germany also saw subscription-based, handwritten news. Those who subscribed to these publications were generally low-level government officials and also merchants. They could not afford other types of news publications, but had enough money to pay for a subscription, which was still expensive for the time.[16]\\r\\n\\r\\nIn the 16th and 17th century, there appeared  numerous printed news sheets summarizing accounts of battles, treaties, king, epidemics, and special events. In 1609 Johann Carolus published the first regular newspaper in Strassburg, comprising brief news bulletins. By the 1620s, numerous major cities had newspapers of 4 to 8 pages appearing at irregular intervals; all were strictly censored. The first daily newspaper appeared in 1660 in Leipzig. Prussia increasingly became the largest and most dominant of the German states, but it had weak newspapers that were kept under very tight control. Advertising was forbidden, and budgets were very small.[17]\\r\\n\\r\\nIn 1766, a Dutch adventurer, William Bolts, proposed starting a newspaper for the English audience in\\r\\nCalcutta. He was deported by the East India Company, before his plans could come to fruition.\\r\\n\\r\\nIn January 1780, James Augustus Hicky published Hicky's Bengal Gazette, the first newspaper in India. The size of that four-page newspaper was 12\\"x8\\". Hicky accused the members of the East India Company, including Governor General Warren Hastings of corruption. In retailiation Hastings prohibited the post office from carrying Hicky's Bengal Gazette, and later sued Hicky for libel. In November 1780, the India Gazette appeared; it supported the Company government.\\r\\n\\r\\nIn 1814, The Times of London acquired a printing press capable of making 1,100 impressions per hour.[18] It was soon adapted to print on both sides of a page at once. This innovation made newspapers cheaper and thus available to a larger part of the population. In 1830, the first penny press newspaper came to the market: Lynde M. Walter's Boston Transcript.[19] Penny press papers cost about one-sixth the price of other newspapers and appealed to a wider audience.[20]  Newspaper editors exchanged copies and freely reprinted material. By the late 1840s telegraph networks linked major and minor cities and permitted overnight news reporting.[21] The invention of wood pulp papermaking in the 1840s significantly reduced the cost of newsprint, having previously been made from rags.  Increasing literacy in the 19th century also increased the size of newspapers' audiences.[22]\\r\\n\\r\\nOnly a few large newspapers could afford bureaus outside their home city.  They relied instead on news agencies, founded around 1859, especially Havas in France and the Associated Press in the U.S. Agenzia Stefani covered Italy.  Former Havas employees founded Reuters in Britain and Wolff in Germany.  Havas is now Agence France-Presse (AFP).[23]  For international news, the agencies pooled their resources, so that Havas, for example, covered the French Empire, South America and the Balkans and shared the news with the other national agencies.  In France the typical contract with Havas provided a provincial newspaper with 1800 lines of telegraphed text daily, for an annual subscription rate of 10,000 francs.  Other agencies provided features and fiction for their subscribers.[24] The major news agencies have always operated on a basic philosophy of providing a single objective news feed to all subscribers. For example, they do not provide separate feeds for conservative or liberal newspapers.  Fenby explains the philosophy:\\r\\n\\r\\nWith literacy rising sharply, the rapidly growing  demand for news, led to changes in the physical size, visual appeal, heavy use of war reporting, brisk writing style, and an omnipresent emphasis on speedy reporting thanks to the telegraph. London set the pace before 1870 but by the 1880s critics noted how London was echoing the emerging New York style of journalism.[26] The new news writing style first spread to the provincial press through the Midland Daily Telegraph around 1900.[27]\\r\\n\\r\\nBy the early 19th century, there were 52 London papers and over 100 other titles. In 1802 and 1815 the tax on newspapers was increased to three pence and then four pence. Unable or unwilling to pay this fee, between 1831 and 1835 hundreds of untaxed newspapers made their appearance. The political tone of most of them was fiercely revolutionary. Their publishers were prosecuted but this failed to get rid of them. It was chiefly Milner Gibson and Richard Cobden who advocated the case in parliament to first reduce in 1836 and in 1855 totally repeal of the tax on newspapers. After the reduction of the stamp tax in 1836 from four pence to one penny, the circulation of English newspapers rose from 39,000,000 to 122,000,000 by 1854; a trend further exacerbated by technological improvements in rail transportation and telegraphic communication combined with growing literacy.\\r\\n\\r\\nThe paper began in 1785 and in 1788 was renamed The Times. In 1817, Thomas Barnes was appointed general editor; he was a political radical, a sharp critic of parliamentary hypocrisy and a champion of freedom of the press. Under Barnes and his successor in 1841, John Thadeus Delane, the influence of The Times rose to great heights, especially in politics and amongst the City of London. It spoke for reform.[28] Peter Fraser and Edward Sterling were two noted journalists, and gained for The Times the pompous/satirical nickname 'The Thunderer' (from \\"We thundered out the other day an article on social and political reform.\\") The paper was the first in the world to reach mass circulation due to its early adoption of the steam-driven rotary printing press. It was also the first properly national newspaper, as it was distributed via the new steam railways to rapidly growing concentrations of urban populations across the country. This helped ensure the profitability of the paper and its growing influence.[29]\\r\\n\\r\\nThe Times was the first newspaper to send war correspondents to cover wars. W. H. Russell, the paper's correspondent with the army in the Crimean War of the mid-1850s, wrote immensely influential dispatches; for the first time the public could read about the reality of warfare. In particular, on September 20, 1854, Russell wrote a missive about one battle that highlighted the surgeons' \\"humane barbarity\\" and the lack of ambulance care for wounded troops. Shocked and outraged, the public's backlash led to major reforms.[30]  The Times became famous for its influential leaders (editorials). For example,  Robert Lowe wrote them between 1851 and 1868 on a wide range of economic topics such as free trade (which he favored).[31]\\r\\n\\r\\nAllan Nevins, the historian of journalism, in 1959 analyzed the importance of The Times in shaping London's elite views of events:\\r\\n\\r\\nThe Manchester Guardian was founded in Manchester in 1821 by a group of non-conformist businessmen. Its most famous editor, Charles Prestwich Scott, made the Guardian into a world-famous newspaper in the 1890s.[33]\\r\\nThe Daily Telegraph was first published on June 29, 1855 and was purchased by Joseph Moses Levy the following year. Levy produced it as the first penny newspaper in London. His son, Edward Lawson soon became editor, a post he held until 1885. The Daily Telegraph became the organ of the middle class and could claim the largest circulation in the world in 1890. It held a consistent Liberal Party allegiance until opposing Gladstone's foreign policy in 1878 when it turned Unionist.[34]\\r\\n\\r\\nThe New Journalism reached out not to the elite but to a popular audience.[35] Especially influential was William Thomas Stead, a controversial journalist and editor who pioneered the art of investigative journalism. Stead's 'new journalism' paved the way for the modern tabloid. He was influential in demonstrating how the press could be used to influence public opinion and government policy, and advocated \\"government by journalism\\". He was also well known for his reportage on child welfare, social legislation and reformation of England's criminal codes.[36]\\r\\n\\r\\nStead became assistant editor of the Liberal Pall Mall Gazette in 1880 where he set about revolutionizing a traditionally conservative newspaper \\"written by gentlemen for gentlemen.\\" Over the next seven years Stead would develop what Matthew Arnold dubbed 'The New Journalism'. His innovations as editor of the Gazette included incorporating maps and diagrams into a newspaper for the first time, breaking up longer articles with eye-catching subheadings and blending his own opinions with those of the people he interviewed. He made a feature of the Pall Mall extras, and his enterprise and originality exercised a potent influence on contemporary journalism and politics. Stead introduced the interview, creating a new dimension in British journalism when he interviewed General Gordon in 1884. He originated the modern journalistic technique of creating a news event rather than just reporting it, with his most famous 'investigation', the Eliza Armstrong case.[37]\\r\\n\\r\\nMatthew Arnold, the leading critic of the day, declared in 1887 that the New Journalism, \\"is full of ability, novelty, variety, sensation, sympathy, generous instincts.\\" However, he added, its \\"one great fault is that it is feather-brained.\\"[38]\\r\\n\\r\\nThe turn of the century saw the rise of popular journalism aimed at the lower middle class and tending to deemphasise hard news, which remain the focus of the party-oriented newspapers. Instead they reached vastly larger audiences by emphasis on sports, crime, sensationalism, gossip about famous personalities.  Alfred Harmsworth, 1st Viscount Northcliffe  (1865ÿ1922)was the chief innovator.  He used his Daily Mail and the Daily Mirror to transform the media along the American model of \\"Yellow Journalism.\\"  Lord Beaverbrook said he was \\"the greatest figure who ever strode down Fleet Street.\\"[39]  P. P. Catterall and Colin Seymour-Ure conclude that: \\r\\n\\r\\nMore than anyone [he] ... shaped the modern press. Developments he introduced or harnessed remain central: broad contents, exploitation of advertising revenue to subsidize prices, aggressive marketing, subordinate regional markets, independence from party control.[40]\\r\\n\\r\\nAfter the war, the major newspapers engaged in a large-scale circulation race. The political parties, which long had sponsored their own papers, could not keep up, and one after another their outlets were sold or closed down.[41]  Sales in the millions depended on popular stories, with a strong human interesting theme, as well as detailed sports reports with the latest scores. Serious news was a niche market and added very little to the circulation base.  The niche was dominated by The Times and, to a lesser extent, The Daily Telegraph.  Consolidation was rampant, as local dailies were bought up and added to chains based in London.   James Curran and Jean Seaton report: \\t\\t\\r\\n\\r\\nThe Times of London was long the most influential prestige newspaper, although far from having the largest circulation. It gave far more attention to serious political and cultural news.[43] In 1922, John Jacob Astor (1886-1971), son of the 1st Viscount Astor (1849-1919), bought The Times from the Northcliffe estate. The paper advocated appeasement of Hitler's demands.  Its editor Geoffrey Dawson was closely allied with Prime Minister  Neville Chamberlain, and pushed hard for the Munich Agreement in 1938.  Candid news reports by  Norman Ebbut from Berlin that warned of warmongering were rewritten in London to support the appeasement policy.  In March 1939, however, it reversed course and called for urgent war preparations.[44][45]\\r\\n\\r\\nDanish news media date back to the 1540s, when handwritten fly sheets reported on the news. In 1666, Anders Bording, the father of Danish journalism, began a state paper. The royal privilege to bring out a newspaper was issued to Joachim Wielandt in 1720. University officials handled the censorship, but in 1770 Denmark became one of the first nations of the world to provide for press freedom; it ended in 1799. In 1834, the first liberal newspaper appeared, one that gave much more emphasis to actual news content rather than opinions. The newspapers championed the Revolution of 1848 in Denmark. The new constitution of 1849 liberated the Danish press.\\r\\n\\r\\nNewspapers flourished in the second half of the 19th century, usually tied to one or another political party or labor union. Modernization, bringing in new features and mechanical techniques, appeared after 1900. The total circulation was 500,000 daily in 1901, more than doubling to 1.2 million in 1925. The German occupation brought informal censorship; some offending newspaper buildings were simply blown up by the Nazis. During the war, the underground produced 550 newspaperssmall, surreptitiously printed sheets that encouraged sabotage and resistance.[46]\\r\\n\\r\\nToday Danish mass media and news programming are dominated by a few large corporations. In printed media JP/Politikens Hus and Berlingske Media, between them, control the largest newspapers Politiken, Berlingske Tidende and Jyllands-Posten and major tabloids B.T. and Ekstra Bladet.\\r\\n\\r\\nIn the early 21st century, the 32 daily newspapers had a combined circulation of over 1 million. The largest was Jyllands-Posten (JP) with a circulation of 120,000. It gained international attention in 2005 by publishing cartoons critical of the Islamic prophet Muhammad. Militant Muslims protested around the world, burning Denmark's embassies in Beirut and Damascus. There have been threats and attempted terrorist plots against the newspaper and its employees ever since.[47]\\r\\n\\r\\nIn the Old regime there were a small number of heavily censored newspapers that needed a royal license to operate.  The first newspaper was the Gazette de France, was established in 1632 by the king's physician Theophrastus Renaudot (1586-1653), with the patronage of Louis XIII.[48] All newspapers were subject to prepublication censorship, and served as instruments of propaganda for the monarchy.  Dissidents used satire and hidden meanings to spread their political criticism.[49][50]\\r\\n\\r\\nNewspapers and pamphlets played role in The Enlightenment \\r\\nin France and they played a central role in stimulating and defining the Revolution.  The meetings of the Estates-General in 1789 created an enormous demand for news, and over 130 newspapers appeared by the end of the year.  The next decade saw 2000 newspapers founded, with 500 in Paris alone.  Most lasted only a matter of weeks.  Together they became the main communication medium, combined with the very large pamphlet literature.[51]  Newspapers were read aloud in taverns and clubs, and circulated hand to hand.  The press saw its lofty role to be the advancement of civic republicanism based on public service, and downplayed the liberal, individualistic goal of making a profit.[52][53][54][55]  In the Revolution the radicals were most active but the royalists flooded the country with their press the \\"Ami du Roi\\" (Friends of the King) until they were suppressed.[56] Napoleon only allowed one newspaper in each department and four in Paris, all under tight control.\\r\\n\\r\\nIn the revolutionary days of 1848 former Saint-Simoniennes founded a Club for the Emancipation of Women; in 1848 it changed its name to La Socit de la Voix des Femmes (Society for Women's Voice) in line with its new newspaper, La Voix des Femmes. It was France's first feminist daily and proclaimed itself \\"a socialist and political journal, the organ of the interests of all women.\\" It lasted for only a few weeks as did two other feminist newspapers; women occasionally contributed articles to the magazines, often under a pseudonym.[57]\\r\\n\\r\\nThe democratic political structure of France, 1870-1914, was supported by the proliferation of newspapers. The circulation of the daily press in Paris went from 1 million in 1870 to 5 million in 1910; it then leveled off and reached 6 million in 1939.  Advertising grew rapidly, providing a steady financial basis.  A new liberal press law of 1881 abandoned the restrictive practices that had been typical for a century.  High-speed rotary Hoe presses, introduced in the 1860s, facilitated quick turnaround time and cheaper publication.  New types of popular newspapers, especially Le Petit Journal reached an audience more interested in diverse entertainment and gossip rather than hard news.  It captured a quarter of the Parisian market, and forced the rest to lower their prices.  The main dailies employed their own journalists who competed for news flashes. All newspapers relied upon the Agence Havas (now Agence France-Presse), a telegraphic news service with a network of reporters and contracts with Reuters to provide world service.  The staid old papers retained their loyal clientele because of their concentration on serious political issues.[58]\\r\\n\\r\\nThe Roman Catholic Assumptionist order revolutionized pressure group media by its national newspaper La Croix.  It vigorously advocated for traditional Catholicism while at the same time innovating with the most modern technology and distribution systems, with regional editions tailored to local taste.  Secularists and Republicans recognize the newspaper as their greatest enemy, especially when it took the lead in attacking Dreyfus as a traitor and stirred up anti-Semitism.  When Dreyfus was pardoned, the Radical government in 1900 closed down the entire Assumptionist order and its newspaper.[59]\\r\\n\\r\\nBusinesses and banks secretly paid certain newspapers to promote particular financial interests, and hide or cover up possible misbehavior.  Publishers took payments for favorable notices in news articles of commercial products. Sometimes, a newspaper would blackmail a business by threatening to publish unfavorable information unless the business immediately started advertising in the paper. Foreign governments, especially Russia and Turkey, secretly paid the press hundreds of thousands of francs a year to guarantee favorable coverage of the bonds it was selling in Paris. When the real news was bad about Russia, as during its 1905 Revolution or during its war with Japan, it raised the bribes it paid to millions of francs.  Each ministry in Paris had a group of journalists whom it secretly paid and fed stories.[60]  During the World War, newspapers became more of a propaganda agency on behalf of the war effort; there was little critical commentary.  The press seldom reported the achievements of the Allies; instead they credited all the good news to the French army.  In a word, the newspapers were not independent champions of the truth, but secretly paid advertisements for special interests and foreign governments.[61]\\r\\n\\r\\nThe World War ended a golden era for the press. Their younger staff members were drafted and male replacements could not be found (women were not considered) available.) Rail transportation was rationed and less paper and ink came in, and fewer copies could be shipped out. Inflation raised the price of newsprint, which was always in short supply. The cover price went up, circulation fell and many of the 242 dailies published outside Paris closed down. The government set up the Interministerial Press Commission to closely supervise the press.  A separate agency imposed tight censorship that led to blank spaces where news reports or editorials were disallowed. The dailies sometimes were limited to only two pages instead of the usual four, leading one satirical paper to try to report the war news in the same spirit: \\r\\n\\r\\nThe Parisian newspapers were largely stagnant after 1914.   The major postwar success story was Paris Soir; which lacked any political agenda and was dedicated to providing a mix of sensational reporting to aid circulation, and serious articles to build prestige.  By 1939 its circulation was over 1.7 million, double that of its nearest rival the tabloid Le Petit Parisien.   In addition to its daily paper Paris Soir sponsored a highly successful women's magazine Marie-Claire. Another magazine Match was modeled after the photojournalism of the American magazine Life.[63]\\r\\n\\r\\nFrance was a democratic society in the 1930s, but the people were kept in the dark about critical issues of foreign policy. The government tightly controlled all of the media to promulgate propaganda to support the government's foreign policy of appeasement to the aggressions of Italy and especially Nazi Germany. There were 253 daily newspapers, all owned separately.  The five major national papers based in Paris were all under the control of special interests, especially right-wing political and business interests that supported appeasement. They were all venal, taking large secret subsidies to promote the policies of various special interests. Many leading journalists were secretly on the government payroll. The regional and local newspapers were heavily dependent on government advertising and published news and editorials to suit Paris. Most of the international news was distributed through the Havas agency, which was largely controlled by the government.  The goal was to tranquilize public opinion, to give it little or nothing to work with, so as not to interfere with the policies of the national government. When serious crises emerged such as the Munich crisis of 1938, people were puzzled and mystified by what was going on. When war came in 1939, Frenchman had little understanding of the issues, and little correct information. They suspiciously distrusted the government, with the result that French morale in the face of the war with Germany was badly prepared.[64]\\r\\n\\r\\nIn 1942, the occupying German forces took control of all of the Parisian newspapers and operated them with collaborators. In 1944, the Free French liberated Paris, and seized control of all of the collaborationist newspapers. They turned the presses and operations over to new teams of editors and publishers, and provided financial support. Thus for example The previously high-prestige Le Temps was replaced by the new daily Le Monde.[65][66]\\r\\n\\r\\nIn the early 21st century, the best-selling daily was the regional Ouest-France in 47 local editions,  followed by Le Progres of Lyon, La Voix du Nord in Lille, and Proven?al in Marseille. In Paris the Communists published l'Humanite while Le Monde Figaro had local rivals in Le Parisien and the leftist Libration.\\r\\n\\r\\nThe Germans read more newspapers than anyone else.[67]  The most dramatic advance in quality came in 1780, with the Neue Zrcher Zeitung in Zrich Switzerland. It set a new standard in objective, in-depth treatment of serious news stories, combined with high-level editorials, and in-depth coverage of music in the theater, as well as an advertising section. Its standards were emulated by the  Norddeutsche Allgemeine Zeitung (1861-1945) and the Frankfurter Zeitung (1856-1943), among others.[68]\\r\\n\\r\\nNapoleon shut down existing German newspapers when he marched through, replacing them with his own, which echoed the official Parisian press. The upsurge of German nationalism after 1809 stimulated underground newspapers, calling for resistance to Napoleon.  Johann Palm took the lead in Augsburg, but he was caught and executed. With the downfall of Napoleon, reactionaries came to power across Germany who had no tolerance for a free press. A repressive police system guaranteed that newspapers would not be criticizing the government.\\r\\n\\r\\nThe revolution of 1848 saw the overnight emergence of a liberal press demanding new freedoms, new constitutions and a free press. Multiple parties formed, and each had its own newspaper network.  Neue Rheinische Zeitung was the first socialist newspaper; it appeared in 1848-49, with Karl Marx as editor. The Revolution of 1848 failed in Germany, the reactionaries returned to power, and many liberal and radical journalists fled the country.[69] The Neue Preussische Zeitung (or Kreuz-Zeitung) became the organ of the Junker East Elbian landowners, the Lutheran clergy, and influential civil and military officials who upheld the King of Prussia. It became the leading Prussian conservative newspaper. Its slogan was \\"With God for king and fatherland.\\"[70]\\r\\n\\r\\nBerlin, the capital of Prussia, had the reputation of being \\"the newspaper city\\" (\\"Zeitungstadt\\"); it published 32 dailies in 1862, along with 58 weekly newspapers.  The main emphasis was not on news are reporting, but among commentary and political analysis.  None of the newspapers however, and none of their editors or journalists was especially influential. However some were using their newspaper experience as a stepping stone to a political career.  The audience was limited to about five percent of the adult men, chiefly from the upper and middle classes, who followed politics. Liberal papers outnumbered conservative ones by a wide margin.[71][72]\\r\\n\\r\\nBismarck's leadership in Prussia in the 1860s, and after 1871 in the German Empire, was highly controversial. His position on domestic policies was conservative or reactionary, and newspapers were mostly liberal; they attacked his defiance of the elected assembly. However, his success in wars against Denmark, Austria, and France made him highly popular, and his establishment of the German Empire was a dream come true for German nationalists.  Bismarck kept a tight rein on the press. Bismarck never listened to public opinion, but he did try to shape it. He secretly subsidized newspapers, and the government gave financial help to small local papers, guaranteeing an overall favorable view. The press law of 1874 guaranteed press freedom, of a sort, but allowed for suppression if an issue contained \\"provocation to treason, incitement to violence, offense to the sovereign, or encouraged assistance of the government.\\" Bismarck often used the code to threaten editors.[73] The press law of  1878 suspended any newspaper advocating socialism ÿ a club Bismarck used to suppress the rapidly growing socialist political movement. He also set up several official propaganda bureaus that distributed foreign and national news to local newspapers.[74]\\r\\n\\r\\nThe newspapers primarily featured lengthy discussions and editorials regarding political conditions. They also included a  \\"Unter dem Strich\\" (\\"Below the line\\") section that featured short stories, poetry, critical reviews of new books, evaluations of art exhibits, and reports on musical concerts and new plays.  An especially popular feature was a novel, serialized with a new chapter every week.[75] in many ways more influential than the newspapers, were the magazines, which proliferated after 1870.  Eminent intellectuals favored this medium.  By 1890 Berlin published  over 600  weeklies, biweeklies, monthlies, and quarterlies, including scholarly journals that were essential reading for scientists everywhere.[76]\\r\\n\\r\\nWhen high-speed rotary presses became available, together with typesetting machinery, it became possible to have press runs in the hundreds of thousands, with frequent updates throughout the day. By 1912 there were 4000 newspapers, printing 5 to 6,000,000,000 copies of the year. New technology made illustrations more feasible, and photographs began appearing. Advertising was now an important feature. Nevertheless, all newspapers focused on their own city, and there was no national newspaper of the sort that flourished in Britain, nor chains owned by one company such as those becoming common in the United States. All the political parties relied heavily on their  own newspapers to inform and rally their supporters. For example, there were 870 papers in 1912 pitched to conservative readers, 580 aimed at liberal elements, 480 aimed at the  Roman Catholics of the Centre party, and 90 affiliated with the Socialist party.[77][78]\\r\\n\\r\\nThe first German newspaper aimed at a mass audience was the Berliner Morgenpost, Founded in 1898 by publisher Hermann Ullstein.  It focused on local news, with very thorough coverage of its home city, ranging from the palaces to the tenements, along with lists of sporting events, streetcar schedules and shopping tips. By 1900 it reached 200,000 subscribers. A rival appeared in 1904, the BZ am Mittag, with a flair for the spectacular and sensational in city life, especially fires, crime and criminals.[79]\\r\\n\\r\\nDuring the First World War (1914-1918), Germany published several newspapers and magazines for the enemy areas it occupied.  The 'Gazette des Ardennes,' was designed for French readers in Belgium and France, Francophone prisoners of war, and generally as a propaganda vehicle in neutral and even enemy countries.  Editor Fritz H. Schnitzer had a relatively free hand, and he tried to enhance his credibility by factual information. He realized until the closing days of the war that it was necessary to produce an increasingly optimistic report to hide the weakening position of the Central Powers in the summer and fall of 1918.[80]\\r\\n\\r\\nThe Nazis, 1933-1945 exercised total control over the press under the direction of Joseph Goebbels. He took control of the wire services and shut down 1000 of the 3000 newspapers, including all those operated by the socialist, communist, and Roman Catholic movements. The survivors received about two dozen press directives every week, which typically were followed very closely.[81][82]\\r\\n\\r\\n\\r\\n\\r\\nIn 1945 the occupying powers took over all newspapers in Germany and purged them of Nazi influence. Each of the four zones had one newspaper; Die Welt in Hamburg the British zone; Die Neue Zeitung in Munich in the American zone; and T?gliche Rundschau (1945ÿ55) in East Berlin in the Soviet zone. By 1949 there were 170 licensed newspapers, but newsprint was strictly rationed, and circulation remains small. The American occupation headquarters,  the Office of Military Government, United States (OMGUS) began its own newspaper based in Munich, Die Neue Zeitung.  It was edited by German and Jewish migrs who fled to the United States before the war, and reached a circulation of 1.6 million in 1946. Its mission was to encourage democracy by exposing Germans to how American culture operated. The paper was filled with details on American sports, politics, business, Hollywood, and fashions, as well as international affairs.[83][84]\\r\\n\\r\\nIn the early 21st century, 78 percent of the population regularly read one of Germany's 1200 newspapers,  most of which are now online.  The heavily illustrated tabloid  Bild had the largest circulation in Europe, at 2.5 million copies a day. It is published by Axel Springer AG, which has a chain of newspapers.  Today, the conservative leaning Frankfurter Allgemeine Zeitung (FAZ) has the highest reputation; its main competitors are the left-wing Sddeutsche Zeitung (Munich) and liberal-conservative Die Welt.  Influential weekly opinion papers include Die Zeit, and until it closed in 2010, Rheinischer Merkur.[85]\\r\\n\\r\\nBetween oppressive rulers, and a low rate of literacy, Italy had little in the way of a serious newspaper press for the 1840s. Gazzetta del Popolo (1848-1983) based in turn was the leading voice for an Italian unification.  La Stampa (1867ÿpresent) in Turin, competes with Corriere della Sera of Milan for primacy in Italian journalism, in terms of circulation numbers and depth of coverage. It was a strong supporter of Prime Minister Giovanni Giolitti, who was denounced daily by Corriere della Sera.\\r\\n\\r\\nThe major newspapers were served by  Agenzia Stefani (1853-1945). It was a News agency that collected news and feature items, and distributed them to subscribing newspapers by telegraph or by mail. It had exchange agreements with Reuters in London and Havas in Paris, and provided a steady flow of domestic and international news and features.[86][87]\\r\\n\\r\\nThe series of crises and confrontations between the papacy, and the kingdom of Italy in the 1870s focused especially on the question of who would control Rome, and what place the pope would have in the new Kingdom. A network of pro-papal newspapers in Italy vigorously supported papal rights and help mobilize the Catholic element.[88]\\r\\n\\r\\nIn 1901 Alberto Bergamini, editor of Rome's Il Giornale d'Italia created the \\"la Terza Pagina\\" (\\"Third Page\\"), featuring essays in literature, philosophy, criticism, the arts, and politics.  It was quickly emulated by the upscale press.[89]  The most important newspaper was the liberal Corriere della Sera, founded in Milan in 1876. It reached a circulation of over 1 million under editor and co-owner Luigi Albertini, 1900-1925. Albertini deliberately modeled his paper after the Times of London, where he had worked briefly. He commissioned leading liberal intellectuals to write essays.Albertini was a strong opponent of Socialism, of clericalism, and of Prime Minister Giovanni Giolitti who was willing to compromise with those forces and corrupt Italian politics.  Albertini's opposition to the Fascist regime forced the other co-owners to oust him in 1925.[90][91]\\r\\n\\r\\nMussolini was a  former editor; his Fascist regime (1922ÿ43) took full control of the media in 1925. Opposition Journalists were physically maltreated; Two thirds of the dailies were shut down. An underground press was developed, using smuggled material.[92]  All the major papers had been mouthpieces for a political party; now all parties save one were abolished, and the newspapers all became its mouthpiece. In 1924 the fascists took control of Agenzia Stefani, and enlarged its scope and mission to make it their tool to control the news content in all of Italy's newspapers. By 1939 it operated 32 bureaus inside Italy and 16 abroad, with 261 correspondents in Italy and 65 abroad. Every day they processed over 1200 dispatches, from which Italian newspapers made up their news pages.[93][94][95]\\r\\n\\r\\nBritish influence extended globally through its colonies and its informal business relationships with merchants in major cities. They needed up-to-date market and political information. El Seminario Republicano was the first non-official newspaper; it appeared in Chile in 1813.  El Mercurio was founded in Valparaiso, Chile, in 1827. The most influential newspaper in Peru, El Comercio, first appeared in 1839. The Jornal do Commercio was established in Rio de Janeiro, Brazil, in 1827. Much later Argentina founded its newspapers in Buenos Aires: La Prensa in 1869 and La Naci܇n in 1870.[96]\\r\\n\\r\\nIn China, early government-produced news sheets, called tipao, were commonly used among court officials during the late Han dynasty (2nd and 3rd centuries AD). Between 713 and 734, the Kaiyuan Za Bao (\\"Bulletin of the Court\\") of the Chinese Tang Dynasty published government news; it was handwritten on silk and read by government officials. In 1582 privately published news sheets appeared in Beijing, during the late Ming Dynasty;[97]\\r\\n\\r\\nFrom the late 19th century until 1949 the international community at Shanghai and Hong Kong sponsored a lively foreign language press that covered business and political news. Leaders included  North China Daily News, Shanghai Evening Post and Mercury,  and for Germans, Der Ostasiatischer Lloyd, and  Deutsche Shanghai Zeitung.\\r\\nBefore 1872 government gazettes printed occasional announcements by officials.  In Shanghai English businessman Ernest Major (1841ÿ1908) established the first Chinese language newspaper in 1872.[98]  His Shen Bao employed Chinese editors and journalists and purchased stories by Chinese writers; it also published letters from readers. Serialized novels were popular with readers and kept them loyal; to the paper.[99] Shanghai's large and powerful International Settlement stimulated the growth of a public sphere of Chinese men of affairs who paid close attention to political and economic developments.  Shanghai became China's media capital. Shen Bao was the most important Chinese-language newspaper until 1905 and was still important until the communists came to power 1949.[100]\\r\\n\\r\\nShen bao and other major newspapers saw public opinion as the driving force of historical change, of the sort that would bring progress reason and modernity to China.  The editors portrayed public opinion as the final arbiter of justice for government officials.  Thereby they broadened the public sphere to include the readership.  The encouragement of the formation of public opinion stimulated activism and form the basis for popular support for the 1911 revolution.[101]\\r\\nChinese newspaper journalism was modernized in the 1920s according to international standards, thanks to the influence of the New Culture Movement. The roles of journalist and editor were professionalized and became prestigious careers. The business side gained importance and with a greater emphasis on advertising and commercial news, the main papers, especially in Shanghai,  moved away from the advocacy journalism that characterized the 1911 revolutionary period.[102]  Outside the main centers the nationalism promoted in metropolitan dailies was not as distinctive as localism and  culturalism.[103]\\r\\n\\r\\nToday China had two news agencies, the Xinhua News Agency and the China News Service (Zhongguo Xinwenshe). Xinhua was the major source of news and photographs for central and local newspapers. In 2002 there were 2100 newspapers, compared to only 400 in 1980. The party's newspapers People's Daily and Guangming Daily, along with the Army's PLA Daily had the largest circulation.  Local papers focused on local news are popular.  In 1981 the English-language China Daily began publication. It printed international news and sports from the major foreign wire services as well as interesting domestic news and feature articles.[104]\\r\\n\\r\\nRobert Knight (1825ÿ90), founded two English language daily papers, The Statesman in Calcutta, and The Times of India in Bombay. In 1860, he bought out the Indian shareholders, merged with rival Bombay Standard, and started India's first news agency. It wired news dispatches to papers across India and became the Indian agent for Reuters news service. In 1861, he changed the name from the Bombay Times and Standard to The Times of India. Knight fought for a press free of prior restraint or intimidation, frequently resisting the attempts by governments, business interests, and cultural spokesmen and led the paper to national prominence. Knight's papers promoted Indian self-rule and often criticized the policies of the British Raj. By 1890 the company employed more than 800 people and had a sizeable circulation in India and the British Empire.[105][106][107] In the 19th century, this newspaper company employed more than 800 people and had a sizeable circulation in India and Europe.\\r\\n\\r\\nJapanese newspapers began in the 17th century as yomiuri (?Citerally \\"to read and sell\\") or kawaraban (\\\\, literally \\"tile-block printing\\" referring to the use of clay printing blocks), which were printed handbills sold in major cities to commemorate major social gatherings or events.\\r\\n\\r\\nThe first modern newspaper was the Japan Herald published  bi-weekly in Yokohama by the Englishman A. W. Hansard from 1861. In 1862, the Tokugawa shogunate began publishing the Kampan batabiya shinbun, a translated edition of a widely distributed Dutch newspaper.  These two papers were published for foreigners, and contained only foreign news.\\r\\n\\r\\nThe first Japanese daily newspaper that covered foreign and domestic news was the Yokohama Mainichi Shinbun (∎?՗Nf?), first published in 1871.  The papers became organs of the political parties.  The early readers of these newspapers mostly came from the ranks of the samurai class.\\r\\n\\r\\nKoshinbun were more plebeian, popular newspapers that contained local news, human interest stories, and light fiction.  Examples of koshinbun were the Tokyo nichinichi shinbun, the predecessor of the present day Mainichi shinbun, which began in 1872; the Yomiuri shinbun, which began in 1874; and the Asahi shinbun, which began in 1879. They soonh became the dominant form.\\r\\n\\r\\nIn the democratic era of the 1910s to the 1920s, the government tried to suppress newspapers such as the Asahi shinbun for their critical stance against government bureaucracy that favored protecting citizens' rights and constitutional democracy.  In the period of growing militarism in the 1930s to 1945, newspapers faced intense government censorship and control. After Japan's defeat, strict censorship of the press continued as the American occupiers used government control in order to inculcate democratic and anti-communist values.  In 1951, the American occupiers finally returned freedom of the press to Japan, which is the situation today.[108]","input":"Who started the first news paper in india?"},{"output":"the episcopal conference of the Catholic Church in the United States","context":"The United States Conference of Catholic Bishops (USCCB) is the episcopal conference of the Catholic Church in the United States.  Founded in 1966 as the joint National Conference of Catholic Bishops (NCCB) and United States Catholic Conference (USCC), it is composed of all active and retired members of the Catholic hierarchy (i.e., diocesan, coadjutor, and auxiliary bishops and the ordinary of the Personal Ordinariate of the Chair of Saint Peter) in the United States and the Territory of the U.S. Virgin Islands. In the Commonwealth of Puerto Rico, the bishops in the six dioceses form their own episcopal conference, the Puerto Rican Episcopal Conference. The bishops in U.S. insular areas in the Pacific Ocean?ÿ  the Commonwealth of the Northern Mariana Islands, the Territory of American Samoa, and the Territory of Guam?ÿ  are members of the Episcopal Conference of the Pacific.\\r\\n\\r\\nThe USCCB adopted its current name in July 2001.  The organization is a registered corporation based in Washington, D.C. As with all bishops' conferences, certain[which?] decisions and acts of the USCCB must receive the recognitio, or approval of the Roman dicasteries, which are subject to the immediate and absolute authority of the Pope. \\r\\n\\r\\nThe current President is GalvestonÿHouston's archbishop, Cardinal Daniel DiNardo.[2]  The current Vice President is Los Angeles's archbishop, Archbishop Jose Gomez.[2]\\r\\n\\r\\nThe United States Conference of Catholic Bishops took its present form in 2010 from the consolidation of the National Conference of Catholic Bishops and the United States Catholic Conference.  The USCCB traces its origins to the National Catholic War Council, which was founded in 1917.[3]\\r\\n\\r\\nThe first national organization of Catholic bishops in the United States was founded in 1917 as the National Catholic War Council (NCWC), formed to enable U.S. Catholics to contribute funds for the spiritual care of Catholic servicemen during World War I.\\r\\n\\r\\nIn 1919 Pope Benedict XV urged the college of bishops around the world to assist him in promoting the labor reforms first articulated by Pope Leo XIII in Rerum novarum. In response, the U.S. Catholic episcopate organized the National Catholic Welfare Council in 1919. They also created the first Administrative Committee of seven members to manage daily affairs between plenary meetings, with archbishop Edward Joseph Hanna of San Francisco as the first chairman. Headquarters were established in Washington, D.C.\\r\\n\\r\\nAfter a threatened suppression of the National Catholic Welfare Council, the administrative board decided to rename the organization to be the National Catholic Welfare Conference, with the purpose of advocating reforms in education, immigration, and social action.\\r\\n\\r\\nEpiscopal conferences were first established as formal bodies by the Second Vatican Council (Christus Dominus, 38), and implemented by Pope Paul VI's 1966 motu proprio Ecclesiae sanctae. In order to fulfill the new requirements for national conferences of bishops, the American bishops established?ÿ  in 1966?ÿ the National Conference of Catholic Bishops (NCCB) and its secular arm, the United States Catholic Conference (USCC).\\r\\n\\r\\nAs separate organizations with distinct responsibilities, the NCCB focused on internal ecclesiastical concerns while the USCC carried forward work in society at large. The NCCB enabled the bishops to deliberate and respond collectively on a broad range of issues, with work being carried out through various secretariats, standing committees, and ad hoc committees.\\r\\n\\r\\nOn July 1, 2001, the NCCB and the USCC were combined to form the United States Conference of Catholic Bishops (USCCB). The merger resulted in the continuation of all of the work formerly done by the NCCB and the USCC, with the same staff.\\r\\n\\r\\nThe operation, authority, and responsibilities of episcopal conferences are currently governed by the 1983 Code of Canon Law (see especially canons 447ÿ459). The nature of episcopal conferences, and their magisterial authority in particular, was subsequently clarified by Pope John Paul II's 1998 motu proprio, Apostolos suos.\\r\\n\\r\\nThe structure of the conference (USCCB) consists of 16 standing committees (whose members are bishops) and the departments, secretariats, and offices that carry out the work of the committees. The leaders of these departments, secretariats, and offices report to the general secretariat of the conference.\\r\\n\\r\\nThe membership of the USCCB consists of all active and retired Latin Church Catholic and Eastern Catholic bishops (i.e., diocesan, coadjutor, and auxiliary bishops) of the United States and the Territory of the Virgin Islands?ÿ  and the ordinary of the Personal Ordinariate of the Chair of Saint Peter?ÿ  but not the bishops of the Commonwealth of Puerto Rico, the Commonwealth of the Northern Mariana Islands, the Territory of American Samoa, and the Territory of Guam. The bishops of the latter four U.S. overseas dependencies belong to other episcopal conferences. In the Commonwealth of Puerto Rico, the bishops in the six dioceses form their own episcopal conference, the Puerto Rican Episcopal Conference. The bishops in U.S. insular areas in the Pacific Ocean?ÿ  the Commonwealth of the Northern Mariana Islands, the Territory of American Samoa, and the Territory of Guam?ÿ  are members of the Episcopal Conference of the Pacific.\\r\\n\\r\\nThe USCCB has two semiannual meetings, in November and June.  Between these meetings, the conference is governed by the Administrative Committee.  There is also an Executive Committee, whose members include the conference president, vice-president, and secretary (all of whom are bishops).  The officers of the conference are elected for three-year terms.  The conference also elects chairmen and chairmen-elect of the standing committees.\\r\\n\\r\\nThe dioceses of the United States are grouped into fifteen regions.  Fourteen of the regions (numbered I through XIV) are geographically based, for the Latin Catholic dioceses.  The Eastern Catholic eparchies (dioceses) constitute Region XV.\\r\\n\\r\\nThe National Conference of Catholic Bishops had appointed James T. McHugh during April 1967 to lead the early formation of what was later to become the National Right to Life Committee. The NRLC was itself formed in 1968 under the auspices of the National Conference of Catholic Bishops to coordinate information and strategy between developing local and state Catholic pro-life groups and is the oldest and the largest national organization against legal abortion in the United States with NRLC affiliates in all 50 states and over 3,000 local chapters nationwide.[4] These NRLC affiliate groups were forming in response to efforts to change abortion laws based on model legislation proposed by the American Law Institute (ALI). New Jersey attorney Juan Ryan served as the organization's first president. NRLC held a nationwide meeting of pro-life leaders in Chicago in 1970 at Barat College. The following year, NRLC held its first convention at Macalestar College in St. Paul, Minnesota.\\r\\n\\r\\nThe USCCB are issuing the \\"Ethical and Religious Directives for Catholic Health Care Services\\"[5][6] that have in some cases caused doctors to refuse treatment of patients although in an emergency situation.[7]\\r\\n\\r\\nIn March 2012, regarding the federal requirement that employers who do not support contraception but are not religious institutions per se must cover contraception via health insurance, USCCB decided to \\"continue its 'vigorous opposition to this unjust and illegal mandate'\\".[8]\\r\\n\\r\\nIn June and July 2012, the USCCB promoted a campaign of events called the Fortnight for Freedom to protest government activities that in their view impinged on their religious liberty.\\r\\n\\r\\nThe USCCB platform on immigration reform includes:[9][10]\\r\\n\\r\\nIn November 2004, the USCCB kicked off the National Pastoral Initiative for Marriage, a multi-year effort to promote traditional marriage values.[citation needed]\\r\\n\\r\\nThe National Pastoral Initiative for Marriage is a multi-year, far-reaching effort to promote traditional marriage values created by the United States Conference of Catholic Bishops in 2004.\\r\\n\\r\\nLed by Archbishop Joseph Edward Kurtz of Louisville, and the USCCB's Office of Laity, Marriage, Family Life and Youth, as well as the Catholic Communication Campaign, the Initiative synthesizes social-science research, Catholic teaching and pastoral practice, and the everyday experiences of married men and women to bolster marriage as a social institution and Christian sacrament.\\r\\n\\r\\nThe NPIM has largely been promoted among the general public in the United States through radio and television public-service announcements.[11]\\r\\n\\r\\nIn the advertising, couples speak candidly about the everyday things done for each other to show love and commitment as they answer the question: What have you done for your marriage today? Viewers are then directed to visit,[12] an online repository of resources, tips and stories that can help strengthen a marriage.\\r\\n\\r\\nThe NPIM is expected to run through 2011, with the next phase including a bishops pastoral letter.\\r\\n\\r\\nOn September 29, 2011, the Archbishop of New York, Timothy M. Dolan, then President of the United States Conference of Catholic Bishops (USCCB) appointed Archibishop William E. Lori the chair of a newly formed Ad Hoc Committee for Religious Liberty to address growing concerns over the erosion of freedom of religion in America.[13] During June 2017, the U.S. Bishops voted to establish a permanent Committee for Religious Liberty.[14] The USCCB Office of Religious Liberty is currently led by Archbishop Joseph Edward Kurtz, who also was President of the USCCB during 2013-2016.  \\r\\n\\r\\nReligious Freedom Week is held annually during early summer to coincide approximately with U.S. Independence Day on July 4 and with the June 22 Feast of Saints Thomas More and John Fisher.[15]\\r\\n\\r\\n? = deceased\\r\\n\\r\\nAt the November 2010 General Meeting in Baltimore, elections were held for President and Vice President. For the first time in the history of the USCCB, and in a break from long-standing tradition, a Vice President standing for the presidency was denied the top post. In those elections, Timothy M. Dolan, Archbishop of New York, was elected President?ÿ  defeating Gerald Kicanas, Bishop of Tucson, 128ÿ111 (54% to 46%)?ÿ  and Joseph Kurtz, Archbishop of Louisville, was elected Vice President in a runoff against Charles Chaput, Archbishop of Denver, 147-91 (62% to 38%).\\r\\n\\r\\nThe budget for 2011 is US$180 million. Money is raised by assessing the individual dioceses.[16]","input":"What is the us conference of catholic bishops?"},{"output":"\\"Trick or treat\\"","context":"Trick-or-treating is a Halloween ritual custom for children and adults in many countries. Children in costumes travel from house-to-house, asking for treats with the phrase \\"Trick or treat\\". The \\"treat\\" is usually some form of candy, although, in some cultures, money is used instead. The \\"trick\\" is a threat, usually idle, to perform mischief on the homeowners or their property if no treat is given. Trick-or-treating usually occurs on the evening of October 31. Some homeowners signal that they are willing to hand out treats by putting up Halloween decorations outside their doors; others simply leave treats available on their porches for the children to take freely.\\r\\nIn North America, trick-or-treating has been a Halloween tradition since the late 1920s. In Britain and Ireland the tradition of going house-to-house collecting food at Halloween goes back at least as far as the 16th century, as had the tradition of people wearing costumes at Halloween. In 19th century Britain and Ireland, there are many accounts of people going house-to-house in costume at Halloween, reciting verses in exchange for food, and sometimes warning of misfortune if they were not welcomed.[1] The Scottish Halloween custom of \\"guising\\" ÿ children disguised in costume going from house to house for food or money ÿ is first recorded in North America in 1911 in Ontario, Canada.[2] While going house-to-house in costume has remained popular among Scots and Irish, the custom of saying \\"trick or treat\\" has only recently become common. The activity is prevalent in the United States, Canada, the United Kingdom, the Republic of Ireland, Puerto Rico, and northwestern and central Mexico. In the latter, this practice is called calaverita (Spanish for \\"sugar skull\\"), and instead of \\"trick or treat\\", the children ask ?me da mi calaverita? (\\"can you give me my sugar skull?\\") where a calaverita is a small skull made of sugar or chocolate.\\r\\n\\r\\n\\r\\nTraditions similar to the modern custom of trick-or-treating extend all the way back to classical antiquity, although it is extremely unlikely that any of them are directly related to the modern custom. The ancient Greek writer Athenaeus of Naucratis records in his book The Deipnosophists that, in ancient times, the Greek island of Rhodes had a custom in which children would go from door-to-door dressed as swallows, singing a song, which demanded the owners of the house to give them food and threatened to cause mischief if the owners of the house refused.[3][4][5] This tradition was claimed to have been started by the Rhodian lawgiver Cleobulus.[6]\\r\\nSince the Middle Ages, a tradition of mumming on a certain holiday has existed in parts of Britain and Ireland. It involved going door-to-door in costume, performing short scenes or parts of plays in exchange for food or drink.\\r\\nThe custom of trick-or-treating on Halloween may come from the belief that supernatural beings, or the souls of the dead, roamed the earth at this time and needed to be appeased.\\r\\nIt may otherwise have originated in a Celtic festival, held on 31 Octoberÿ1 November, to mark the beginning of winter. It was Samhain in Ireland, Scotland and the Isle of Man, and Calan Gaeaf in Wales, Cornwall and Brittany. The festival is believed to have pre-Christian roots. In the 9th century, the Catholic Church made 1 November All Saints' Day. Among Celtic-speaking peoples, it was seen as a liminal time, when the spirits or fairies (the Aos S), and the souls of the dead, came into our world and were appeased with offerings of food and drink. Similar beliefs and customs were found in other parts of Europe. It is suggested that trick-or-treating evolved from a tradition whereby people impersonated the spirits, or the souls of the dead, and received offerings on their behalf. S. V. Peddle suggests they \\"personify the old spirits of the winter, who demanded reward in exchange for good fortune\\".[7] Impersonating these spirits or souls was also believed to protect oneself from them.[8]\\r\\nAt least as far back as the 15th century, among Christians, there had been a custom of sharing soul cakes at Allhallowtide (October 31 through November 2).[10][11] People would visit houses and take soul cakes, either as representatives of the dead, or in return for praying for their souls.[12] Later, people went \\"from parish to parish at Halloween, begging soul-cakes by singing under the windows some such verse as this: 'Soul, souls, for a soul-cake; Pray you good mistress, a soul-cake!'\\"[13] They typically asked for \\"mercy on all Christian souls for a soul cake\\".[14] It was known as 'Souling' and was recorded in parts of Britain, Flanders, southern Germany and Austria.[15] Shakespeare mentions the practice in his comedy The Two Gentlemen of Verona (1593), when Speed accuses his master of \\"puling [whimpering or whining] like a beggar at Hallowmas\\".[16]\\r\\nThe wearing of costumes, or \\"guising\\", at Hallowmas, had been recorded in Scotland in the 16th century[17] and was later recorded in other parts of Britain and Ireland.[18] There are many references to mumming, guising or souling at Halloween in Britain and Ireland during the late 18th century and the 19th century. In parts of southern Ireland, a man dressed as a Lir Bhn (white mare) led youths house-to-house reciting versessome of which had pagan overtonesin exchange for food. If the household donated food it could expect good fortune from the 'Muck Olla', but if they refused to do so, it would bring misfortune.[19] In Scotland, youths went house-to-house in white with masked, painted or blackened faces, reciting rhymes and often threatening to do mischief if they were not welcomed.[18][20][21] In parts of Wales, peasant men went house-to-house dressed as fearsome beings called gwrachod, or presenting themselves as the cenhadon y meirw (representatives of the dead).[18] In western England, mostly in the counties bordering Wales, souling was common.[11] According to one 19th century English writer \\"parties of children, dressed up in fantastic costume [] went round to the farm houses and cottages, signing a song, and begging for cakes (spoken of as \\"soal-cakes\\"), apples, money, or anything that the goodwives would give them\\".[22]\\r\\nGuising at Halloween in Scotland is recorded in 1895, where masqueraders in disguise carrying lanterns made out of scooped out turnips, visit homes to be rewarded with cakes, fruit and money.[23] The practice of guising at Halloween in North America is first recorded in 1911, where a newspaper in Kingston, Ontario, Canada reported children going \\"guising\\" around the neighborhood.[2]\\r\\nAmerican historian and author Ruth Edna Kelley of Massachusetts wrote the first book length history of the holiday in the US; The Book of Hallowe'en (1919), and references souling in the chapter \\"Hallowe'en in America\\"; \\"The taste in Hallowe'en festivities now is to study old traditions, and hold a Scotch party, using Burn's poem Hallowe'en as a guide; or to go a-souling as the English used. In short, no custom that was once honored at Hallowe'en is out of fashion now.\\"[24] Kelley lived in Lynn, Massachusetts, a town with 4,500 Irish immigrants, 1,900 English immigrants, and 700 Scottish immigrants in 1920.[25] In her book, Kelley touches on customs that arrived from across the Atlantic; \\"Americans have fostered them, and are making this an occasion something like what it must have been in its best days overseas. All Hallowe'en customs in the United States are borrowed directly or adapted from those of other countries\\".[26]\\r\\nWhile the first reference to \\"guising\\" in North America occurs in 1911, another reference to ritual begging on Halloween appears, place unknown, in 1915, with a third reference in Chicago in 1920.[27]\\r\\nThe earliest known use in print of the term \\"trick or treat\\" appears in 1927, from Blackie, Alberta:\\r\\nHalloween provided an opportunity for real strenuous fun. No real damage was done except to the temper of some who had to hunt for wagon wheels, gates, wagons, barrels, etc., much of which decorated the front street. The youthful tormentors were at back door and front demanding edible plunder by the word trick or treat to which the inmates gladly responded and sent the robbers away rejoicing.[28]\\r\\nThe thousands of Halloween postcards produced between the start of the 20th century and the 1920s commonly show children but do not depict trick-or-treating.[29] The editor of a collection of over 3,000 vintage Halloween postcards writes, \\"There are cards which mention the custom [of trick-or-treating] or show children in costumes at the doors, but as far as we can tell they were printed later than the 1920s and more than likely even the 1930s. Tricksters of various sorts are shown on the early postcards, but not the means of appeasing them\\".[30]\\r\\nTrick-or-treating does not seem to have become a widespread practice until the 1930s, with the first U.S. appearances of the term in 1934,[31] and the first use in a national publication occurring in 1939.[32]\\r\\nBehavior similar to trick-or-treating was more commonly associated with Thanksgiving from 1870 (shortly after that holiday's formalization) until the 1930s. In New York City, a Thanksgiving ritual known as Ragamuffin Day involved children dressing up as beggars and asking for treats, which later evolved into dressing up in more diverse costumes. [33] [34] Increasing hostility toward the practice in the 1930s eventually led to the begging aspects being dropped, and by the 1950s, the tradition as a whole had ceased.\\r\\nAlmost all pre-1940 uses of the term \\"trick-or-treat\\" are from the United States and Canada. Trick-or-treating spread throughout the United States, stalled only by sugar rationing that began in April 1942 during World War II and did not end until June 1947.[35][36]\\r\\nEarly national attention to trick-or-treating was given in October 1947 issues of the children's magazines Jack and Jill and Children's Activities,[37] and by Halloween episodes of the network radio programs The Baby Snooks Show in 1946 and The Jack Benny Show and The Adventures of Ozzie and Harriet in 1948.[38] Trick-or-treating was depicted in the Peanuts comic strip in 1951.[39] The custom had become firmly established in popular culture by 1952, when Walt Disney portrayed it in the cartoon Trick or Treat, and Ozzie and Harriet were besieged by trick-or-treaters on an episode of their television show.[40] In 1953 UNICEF first conducted a national campaign for children to raise funds for the charity while trick-or-treating.[41]\\r\\nAlthough some popular histories of Halloween have characterized trick-or-treating as an adult invention to re-channel Halloween activities away from Mischief Night vandalism, there are very few records supporting this. Des Moines, Iowa is the only area known to have a record of trick-or-treating being used to deter crime.[42] Elsewhere, adults, as reported in newspapers from the mid-1930s to the mid-1950s, typically saw it as a form of extortion, with reactions ranging from bemused indulgence to anger.[43] Likewise, as portrayed on radio shows, children would have to explain what trick-or-treating was to puzzled adults, and not the other way around. Sometimes even the children protested: for Halloween 1948, members of the Madison Square Boys Club in New York City carried a parade banner that read \\"American Boys Don't Beg.\\"[44] The National Confectioners Association reported in 2005 that 80 percent of adults in the United States planned to give out confectionery to trick-or-treaters,[45] and that 93 percent of children, teenagers, and young adults planned to go trick-or-treating or participating in other Halloween activities.[46]\\r\\nDespite the concept of trick or treating originating in Britain and Ireland in the form of souling and guising, the use of the term 'trick or treat' at the doors of home owners was not common until the 1980s. Guising requires those going door-to-door to perform a song or poem without any jocular threat,[47] and according to one BBC journalist, in the 1980s \\"trick or treat\\" was still often viewed as an exotic and not particularly welcome import, with the BBC referring to it as \\"the Japanese knotweed of festivals\\" and \\"making demands with menaces\\".[48] In Ireland before the phrase \\"trick or treat\\" became common, children would say \\"Help the Halloween Party\\". Very often, the phrase \\"trick or treat\\" is simply said and the revellers are given sweets, with the choice of a trick or a treat having been discarded.\\r\\nTrick-or-treating typically happens between 5:30pm and 9:30pm[49] on October 31,[50] although some municipalities choose other dates.[51] Homeowners wishing to participate in it sometimes decorate their private entrances with artificial spider webs, plastic skeletons and jack-o-lanterns. Some rather reluctant homeowners would simply leave the candy in bowls on the porch, others might be more participative and would even ask an effort from the children in order to provide them with candy. In more recent years, however, the practice has spread to almost any house within a neighbourhood being visited by children, including senior residences and condominiums. Although people decorate their homes for Halloween, when homes would like to participate in handing out candy, a porch light is usually left on to signify that a home is handing out candy.\\r\\nIn Scotland and Ireland, \\"guising\\"? children going from door to door in disguise? is traditional, and a gift in the form of food, coins or \\"apples or nuts for the Halloween party\\" (in more recent times chocolate) is given out to the children.[52][53] The tradition is called \\"guising\\" because of the disguises or costumes worn by the children.[54] In the West Mid Scots dialect, guising is known as \\"galoshans\\".[55] While guising has been recorded in Scotland in the 16th century, a more contemporary record of guising at Halloween in Scotland is in 1895, where masqueraders in disguise carrying lanterns made out of scooped out turnips, visit homes to be rewarded with cakes, fruit, and money.[23] Guising also involved going to wealthy homes, and in the 1920s, boys went guising at Halloween up to the affluent Thorntonhall, South Lanarkshire.[56] An account of guising in the 1950s in Ardrossan, North Ayrshire, records a child receiving 12 shillings and sixpence, having knocked on doors throughout the neighborhood and performed.[47]\\r\\nThere is a significant difference from the way the practice has developed in North America with the associated threat. In Scotland and Ireland, the children are only supposed to receive treats if they perform a party trick for the households they go to. This normally takes the form of singing a song or reciting a joke or a funny poem which the child has memorised before setting out.[47] Occasionally a more talented child may do card tricks, play the mouth organ, or something even more impressive, but most children will earn plenty of treats even with something very simple. Often they won't even need to perform.[52] While going from door to door in disguise has remained popular among Scots and Irish at Halloween, the North American saying \\"trick-or-treat\\" has become common.\\r\\nSome organizations around the United States and Canada sponsor a \\"Trunk-or-Treat\\" on Halloween night (or on occasion, a day immediately preceding Halloween), where trick-or-treating is done from parked car to parked car in a local parking lot, often at a school or church. This annual event began in the mid-1990s as a \\"Fall Festival\\" for an alternative to trick-or-treating, but became \\"Trunk-or-Treat\\" two decades later. The activity involves the open trunk of a car, displaying candy, and often games and decorations. Some parents regard trunk-or-treating as a safer alternative to trick-or-treating;[57] while other parents see it as an easier alternative to walking the neighborhood with their children. Some have called for more city or community group-sponsored Trunk-or-Treats, so they can be more inclusive.[58] These have become increasingly popular in recent years.[59]\\r\\nChildren of the St. Louis, Missouri area are expected to perform a joke, usually a simple Halloween-themed pun or riddle, before receiving any candy; this \\"trick\\" earns the \\"treat\\".[60] Children in Des Moines, Iowa also tell jokes or otherwise perform before receiving their treat.\\r\\nIn most areas of the United States and Canada, trick-or-treating is a practice strictly meant for children. In fact, several US cities have banned trick-or-treaters older than 12 from participating in the event,[61] and most young people stop trick-or-treating by the age of fourteen. In both countries, it is expected that a teenager will transition into more mature expressions of celebrating the holiday, such as fancy dress, games, and diversions like bonfires and bobbing for apples, and sweets like caramel apples, and teenagers will often attend school or community events with a Halloween theme where there will be dancing and music.[62]\\r\\nIn some parts of Canada, children sometimes say \\"Halloween apples\\" instead of \\"trick or treat.\\" This probably originated when the toffee apple was a popular type of candy. Apple-giving in much of Canada, however, has been taboo since the 1960s when stories (of almost certainly questionable authenticity) appeared of razors hidden inside Halloween apples; parents began to check over their children's \\"loot\\" for safety before allowing them to eat it. In Quebec, children also go door to door on Halloween. However, in French speaking neighbourhoods, instead of \\"Trick or treat?\\", they will simply say \\"Halloween\\", though in tradition it used to be La charit s'il-vous-pla?t (\\"Charity, please\\").[63]\\r\\nIn Portugal, children go from house to house in All Saints day and All Souls Day, carrying pumpkin carved lanterns called coca,[64] asking every one they see for P?o-por-Deus singing rhymes where they remind people why they are begging, saying \\"...It is for me and for you, and to give to the deceased who are dead and buried[...]\\"[65] or \\"[...]It is to share with your deceased [...]\\"[66] If a door is not open or the children don't get anything, they end their singing saying \\"[...]In this house smells like lard, here must live someone deceased\\". In the Azores the bread given to the children takes the shape of the top of a skull.[67] The tradition of p?o-por-Deus was already recorded in the 15th century.[68] After this ritual begging, takes place the Magusto and big bonfires are lit with the \\"firewood of the souls\\". The young people play around smothering their faces with the ashes. The ritual begging for the deceased used to take place all over the year as in several regions the dead, those who were dear, were expected to arrive and take part in the major celebrations like Christmas and a plate with food or a seat at the table was always left for them.[69]\\r\\nIn Sweden, children dress up as witches and monsters when they go trick-or-treating on Maundy Thursday (the Thursday before Easter) while Danish children dress up in various attires and go trick-or-treating on Fastelavn (or the next day, Shrove Monday). In Norway, \\"trick-or-treat\\" is called \\"knask eller knep\\", which means almost the same thing, although with the word order reversed, and the practice is quite common among children, who come dressed up to people's doors asking for, mainly, candy. Many Norwegians prepare for the event by consciously buying a small stock of sweets prior to it, to come in handy should any kids come knocking on the door, which is very probable in most areas. The Easter witch tradition is done on Palm Sunday in Finland. In parts of Flanders and some parts of the Netherlands and most areas of Germany, Switzerland, and Austria, children go to houses with homemade beet lanterns or with paper lanterns (which can hold a candle or electronic light), singing songs about St. Martin on St. Martin's Day (the 11th of November), in return for treats.[70] In Northern Germany and Southern Denmark, children dress up in costumes and go trick-or-treating on New Year's Eve in a tradition called \\"Rummelpott\\".[71]\\r\\nUNICEF started a program in 1950 called Trick-or-Treat for UNICEF in which trick-or-treaters ask people to give money for the organization, usually instead of collecting candy. Participating trick-or-treaters say when they knock at doors \\"Trick-or-treat for UNICEF!\\"[72] This program started as an alternative to candy. The organization has long produced disposable collection boxes that state on the back what the money can be used for in developing countries.\\r\\nPretty Boy John Doe rang the door bells and his gang waited his signal. It was his plan to proceed cautiously at first and give a citizen every opportunity to comply with his demands before pulling any rough stuff. \\"Madam, we are here for the usual purpose, 'trick or treat.'\\" This is the old demand of the little people who go out to have some innocent fun. Many women have some apples, cookies or doughnuts for them, but they call rather early and the \\"treat\\" is given out gladly.","input":"What does a child typically say on halloween?"},{"output":"allows consumers to directly buy goods or services from a seller over the Internet using a web browser","context":"\\r\\nOnline shopping is a form of electronic commerce which allows consumers to directly buy goods or services from a seller over the Internet using a web browser. Consumers find a product of interest by visiting the website of the retailer directly or by searching among alternative vendors using a shopping search engine, which displays the same product's availability and pricing at different e-retailers. As of 2016, customers can shop online using a range of different computers and devices, including desktop computers, laptops, tablet computers and smartphones.\\r\\n\\r\\nAn online shop evokes the physical analogy of buying products or services at a regular \\"bricks-and-mortar\\" retailer or shopping center; the process is called business-to-consumer (B2C) online shopping. When an online store is set up to enable businesses to buy from another businesses, the process is called business-to-business (B2B) online shopping. A typical online store enables the customer to browse the firm's range of products and services, view photos or images of the products, along with information about the product specifications, features and prices.\\r\\n\\r\\nOnline stores typically enable shoppers to use \\"search\\" features to find specific models, brands or items. Online customers must have access to the Internet and a valid method of payment in order to complete a transaction, such as a credit card, an Interac-enabled debit card, or a service such as PayPal. For physical products (e.g., paperback books or clothes), the e-tailer ships the products to the customer; for digital products, such as digital audio files of songs or software, the e-tailer typically sends the file to the customer over the Internet. The largest of these online retailing corporations are Alibaba, Amazon.com, and eBay.[1]\\r\\n\\r\\nAlternative names for the activity are \\"e-tailing\\", a shortened  form of \\"electronic retail\\" or \\"e-shopping\\", a shortened form of \\"electronic shopping\\". An online store may also be called an e-web-store, e-shop, e-store, Internet shop, web-shop, web-store, online store, online storefront and virtual store. Mobile commerce (or m-commerce) describes purchasing from an online retailer's mobile device-optimized website or software application (\\"app\\"). These websites or apps are designed to enable customers to browse through a companies' products and services on tablet computers and smartphones.\\r\\n\\r\\nThe growth of the internet as a secure shopping channel has developed since 1994, with the first sales of Sting album 'Ten Summoner's Tales'.[2]  Wine, chocolates and flowers soon followed and were among the pioneering retail categories which fueled the growth of online shopping. Researchers found that having products that are appropriate for e-commerce was a key indicator of Internet success.[3]  Many of these products did well as they are generic products which shoppers didn't need to touch and feel in order to buy.  But also importantly in the early days there were few shoppers online and they were from a narrow segment: affluent, male, 30+.  Online shopping has come along way since these early days and -in the UK- accounts for significant percents (depending on product category as percentages can vary).\\r\\n\\r\\nAs the revenues from online sales continued to grow significantly researchers identified different types of online shoppers, Rohm & Swaninathan[4] identified four categories and named them \\"convenience shoppers, variety seekers, balanced buyers, and store-oriented shoppers\\". They focused on shopping motivations and found that the variety of products available and the perceived convenience of the buying online experience were significant motivating factors.  This was different for offline shoppers, who were more motivated by time saving and recreational motives.\\r\\n\\r\\nDigital High Street 2020[5]\\r\\n\\r\\nEnglish entrepreneur Michael Aldrich was a pioneer of online shopping in 1979. His system connected a modified domestic TV to a real-time transaction processing computer via a domestic telephone line. He believed that videotex, the modified domestic TV technology with a simple menu-driven humanÿcomputer interface, was a 'new, universally applicable, participative communication medium? the first since the invention of the telephone.' This enabled 'closed' corporate information systems to be opened to 'outside' correspondents not just for transaction processing but also for e-messaging and information retrieval and dissemination, later known as e-business.[6] His definition of the new mass communications medium as 'participative' [interactive, many-to-many] was fundamentally different from the traditional definitions of mass communication and mass media and a precursor to the social networking on the Internet 25 years later. In March 1980 he launched Redifon's Office Revolution, which allowed consumers, customers, agents, distributors, suppliers and service companies to be connected on-line to the corporate systems and allow business transactions to be completed electronically in real-time.[7] During the 1980s[8] he designed, manufactured, sold, installed, maintained and supported many online shopping systems, using videotex technology.[9] These systems which also provided voice response and handprint processing pre-date the Internet and the World Wide Web, the IBM PC, and Microsoft MS-DOS, and were installed mainly in the UK by large corporations.\\r\\n\\r\\nThe first World Wide Web server and browser, created by Tim Berners-Lee in 1990, opened for commercial use in 1991.[10] Thereafter, subsequent technological innovations emerged in 1994: online banking, the opening of an online pizza shop by Pizza Hut,[10] Netscape's SSL v2 encryption standard for secure data transfer, and Intershop's first online shopping system. The first secure retail transaction over the Web was either by NetMarket or Internet Shopping Network in 1994.[11] Immediately after, Amazon.com launched its online shopping site in 1995 and eBay was also introduced in 1995.[10] Alibaba's sites Taobao and Tmall were launched in 2003 and 2008, respectively. Retailers are increasingly selling goods and services prior to availability through \\"pretail\\" for testing, building, and managing demand.[citation needed]\\r\\n\\r\\nStatistics show that in 2012, Asia-Pacific increased their international sales over 30% giving them over $433 billion in revenue. That is a $69 billion difference between the U.S. revenue of $364.66 billion. It is estimated that Asia-Pacific will increase by another 30% in the year 2013 putting them ahead by more than one-third of all global ecommerce sales.[needs update] The largest online shopping day in the world is Singles Day, with sales just in Alibaba's sites at US$9.3 billion in 2014.[12][13]\\r\\n\\r\\nOnline customers must have access to the Internet and a valid method of payment in order to complete a transaction. Generally, higher levels of education and personal income correspond to more favorable perceptions of shopping online. Increased exposure to technology also increases the probability of developing favorable attitudes towards new shopping channels.[14]\\r\\n\\r\\nThe marketing around the digital environment, customer's buying behaviour may not be influenced and controlled by the brand and firm, when they make a buying decision that might concern the interactions with search engine, recommendations, online reviews and other information. With the quickly separate of the digital devices environment, people are more likely to use their mobile phones, computers, tablets and other digital devices to gather information. In other words, the digital environment has a growing effect on consumer's mind and buying behaviour. In an online shopping environment, interactive decision may have an influence on aid customer decision making. Each customer is becoming more interactive, and though online reviews customers can influence other potential buyers' behaviors.[15]\\r\\n\\r\\nSubsequently, risk and trust would also are two important factors affecting people's' behavior in digital environments. Customer consider to switch between e-channels, because they are mainly influence by the comparison with offline shopping, involving growth of security, financial and performance-risks In other words, a customer shopping online that they may receive more risk than people shopping in stores. There are three factors may influence people to do the buying decision, firstly, people cannot examine whether the product satisfy their needs and wants before they receive it. Secondly, customer may concern at after-sale services. Finally, customer may afraid that they cannot fully understand the language used in e-sales. Based on those factors customer perceive risk may as a significantly reason influence the online purchasing behaviour.[16]\\r\\n\\r\\nOnline retailers has place much emphasis on customer trust aspect, trust is another way driving customer's behaviour in digital environment, which can depend on customer's attitude and expectation. Indeed, the company's products design or ideas can not met customer's expectations. Customer's purchase intension based on rational expectations, and additionally impacts on emotional trust. Moreover, those expectations can be also establish on the product information and revision from others.[17]\\r\\n\\r\\nConsumers find a product of interest by visiting the website of the retailer directly or by searching among alternative vendors using a shopping search engine. Once a particular product has been found on the website of the seller, most online retailers use shopping cart software to allow the consumer to accumulate multiple items and to adjust quantities, like filling a physical shopping cart or basket in a conventional store. A \\"checkout\\" process follows (continuing the physical-store analogy) in which payment and delivery information is collected, if necessary. Some stores allow consumers to sign up for a permanent online account so that some or all of this information only needs to be entered once. The consumer often receives an e-mail confirmation once the transaction is complete. Less sophisticated stores may rely on consumers to phone or e-mail their orders (although full credit card numbers, expiry date, and Card Security Code,[18] or bank account and routing number should not be accepted by e-mail, for reasons of security).\\r\\n\\r\\nOnline shoppers commonly use a credit card or a PayPal account in order to make payments. However, some systems enable users to create accounts and pay by alternative means, such as:\\r\\n\\r\\nSome online shops will not accept international credit cards. Some require both the purchaser's billing and shipping address to be in the same country as the online shop's base of operation. Other online shops allow customers from any country to send gifts anywhere. The financial part of a transaction may be processed in real time (e.g. letting the consumer know their credit card was declined before they log off), or may be done later as part of the fulfillment process.\\r\\n\\r\\nOnce a payment has been accepted, the goods or services can be delivered in the following ways.  For physical items:\\r\\n\\r\\nFor digital items or tickets:\\r\\n\\r\\nSimple shopping cart systems allow the off-line administration of products and categories. The shop is then generated as HTML files and graphics that can be uploaded to a webspace. The systems do not use an online database.[22] A high-end solution can be bought or rented as a stand-alone program or as an addition to an enterprise resource planning program. It is usually installed on the company's web server and may integrate into the existing supply chain so that ordering, payment, delivery, accounting and warehousing can be automated to a large extent. Other solutions allow the user to register and create an online shop on a portal that hosts multiple shops simultaneously from one back office. Examples are BigCommerce, Shopify and FlickRocket. Open source shopping cart packages include advanced platforms such as Interchange, and off-the-shelf solutions such as Magento, osCommerce, Shopgate, PrestaShop, and Zen Cart. Commercial systems can also be tailored so the shop does not have to be created from scratch. By using an existing framework, software modules for various functionalities required by a web shop can be adapted and combined.[citation needed]\\r\\n\\r\\nCustomers are attracted to online shopping not only because of high levels of convenience, but also because of broader selections, competitive pricing, and greater access to information.[23][24] Business organizations seek to offer online shopping not only because it is of much lower cost compared to bricks and mortar stores, but also because it offers access to a worldwide market, increases customer value, and builds sustainable capabilities.[clarification needed][25]\\r\\n\\r\\nDesigners of online shops are concerned with the effects of information load. Information load is a product of the spatial and temporal arrangements of stimuli in the web store.[26] Compared with conventional retail shopping, the information environment of virtual shopping is enhanced by providing additional product information such as comparative products and services, as well as various alternatives and attributes of each alternative, etc.[27] Two major dimensions of information load are complexity and novelty.[28] Complexity refers to the number of different elements or features of a site, often the result of increased information diversity. Novelty involves the unexpected, suppressed, new, or unfamiliar aspects of the site. The novelty dimension may keep consumers exploring a shopping site, whereas the complexity dimension may induce impulse purchases.[27]\\r\\n\\r\\nAccording to the output of a research report by Western Michigan University published in 2005, an e-commerce website does not have to be good looking with listing on a lot of search engines. It must build relationships with customers to make money. The report also suggests that a website must leave a positive impression on the customers, giving them a reason to come back.[29] However, resent research[30] has proven that sites with higher focus on efficiency, convenience, and personalised services increased the customers motivation to make purchases.\\r\\n\\r\\nDyn, an Internet performance management company conducted a survey on more than 1400 consumers across 11 countries in North America, Europe, Middle-East and Asia and the results of the survey are as follows:\\r\\n\\r\\nThese concerns majorly affect the decisions of almost two thirds of the consumers.[31]\\r\\n\\r\\nThe most important factors determining whether customers return to a website are ease of use and the presence of user-friendly features.[32] Usability testing is important for finding problems and improvements in a web site. Methods for evaluating usability include heuristic evaluation, cognitive walkthrough, and user testing. Each technique has its own characteristics and emphasizes different aspects of the user experience.[32]\\r\\n\\r\\nThe popularity of online shopping continues to erode sales of conventional retailers. For example, Best Buy, the largest retailer of electronics in the U.S. in August 2014 reported its tenth consecutive quarterly dip in sales, citing an increasing shift by consumers to online shopping.[33] Amazon.com has the largest market share in the United States.  As of May 2018, a survey found two-thirds of Americans had bought something from Amazon (92% of those who had bought anything online), with 40% of online shoppers buying something from Amazon at least once a month. The survey found shopping began at amazon.com 44% of the time, compared to a general search engine at 33%.  It estimated 75 million Americans subscribe to Amazon Prime and 35 million more use someone else's account.[34]\\r\\n\\r\\nThere were 242 million people shopping online in China in 2012.[35] For developing countries and low-income households in developed countries, adoption of e-commerce in place of or in addition to conventional methods is limited by a lack of affordable Internet access.\\r\\n\\r\\nOnline stores are usually available 24 hours a day, and many consumers in Western countries have Internet access both at work and at home. Other establishments such as Internet cafes, community centers and schools provide internet access as well. In contrast, visiting a conventional retail store requires travel or commuting and costs such as gas, parking, or bus tickets, and must typically take place during business hours. Delivery was always a problem which affected the convenience of online shopping. However to overcome this many retailers including online retailers in Taiwan brought in a store pick up service. This now meant that customers could purchase goods online and pick them up at a nearby convenience store, making online shopping more advantageous to customers.[36]  In the event of a problem with the item (e.g., the product was not what the consumer ordered or the product was not satisfactory), consumers are concerned with the ease of returning an item in exchange for  the correct product or a refund. Consumers may need to contact the retailer, visit the post office and pay return shipping, and then wait for a replacement or refund. Some online companies have more generous return policies to compensate for the traditional advantage of physical stores. For example, the online shoe retailer Zappos.com includes labels for free return shipping, and does not charge a restocking fee, even for returns which are not the result of merchant error.  (Note: In the United Kingdom, online shops are prohibited from charging a restocking fee if the consumer cancels their order in accordance with the Consumer Protection (Distance Selling) Act 2000).[37] A 2018 survey in the United States found 26% of online shoppers said they never return items, and another 65% said they rarely do so.[38]\\r\\n\\r\\nOnline stores must describe products for sale with text, photos, and multimedia files, whereas in a physical retail store, the actual product and the manufacturer's packaging will be available for direct inspection (which might involve a test drive, fitting, or other experimentation). Some online stores provide or link to supplemental product information, such as instructions, safety procedures, demonstrations, or manufacturer specifications. Some provide background information, advice, or how-to guides designed to help consumers decide which product to buy. Some stores even allow customers to comment or rate their items. There are also dedicated review sites that host user reviews for different products. Reviews and even some blogs give customers the option of shopping for cheaper purchases from all over the world without having to depend on local retailers. In a conventional retail store, clerks are generally available to answer questions. Some online stores have real-time chat features, but most rely on e-mails or phone calls to handle customer questions. Even if an online store is open 24 hours a day, seven days a week, the customer service team may only be available during regular business hours.\\r\\n\\r\\nOne advantage of shopping online is being able to quickly seek out deals for items or services provided by many different vendors (though some local search engines do exist to help consumers locate products for sale in nearby stores). Search engines, online price comparison services and discovery shopping engines can be used to look up sellers of a particular product or service. Shipping costs (if applicable) reduce the price advantage of online merchandise, though depending on the jurisdiction, a lack of sales tax may compensate for this. Shipping a small number of items, especially from another country, is much more expensive than making the larger shipments bricks-and-mortar retailers order. Some retailers (especially those selling small, high-value items like electronics) offer free shipping on sufficiently large orders. Another major advantage for retailers is the ability to rapidly switch suppliers and vendors without disrupting users' shopping experience.\\r\\n\\r\\nGiven the lack of ability to inspect merchandise before purchase, consumers are at higher risk of fraud than face-to-face transactions. When ordering merchandise online, the item may not work properly, it may have defects, or it might not be the same item pictured in the online photo. Merchants also risk fraudulent purchases if customers are using stolen credit cards or fraudulent repudiation of the online purchase. However, merchants face less risk from physical theft by using a warehouse instead of a retail storefront. Secure Sockets Layer (SSL) encryption has generally solved the problem of credit card numbers being intercepted in transit between the consumer and the merchant. However, one must still trust the merchant (and employees) not to use the credit card information subsequently for their own purchases, and not to pass the information to others. Also, hackers might break into a merchant's web site and steal names, addresses and credit card numbers, although the Payment Card Industry Data Security Standard is intended to minimize the impact of such breaches. Identity theft is still a concern for consumers. A number of high-profile break-ins in the 2000s has prompted some U.S. states to require disclosure to consumers when this happens. Computer security has thus become a major concern for merchants and e-commerce service providers, who deploy countermeasures such as firewalls and anti-virus software to protect their networks. Phishing is another danger, where consumers are fooled into thinking they are dealing with a reputable retailer, when they have actually been manipulated into feeding private information to a system operated by a malicious party. Denial of service attacks are a minor risk for merchants, as are server and network outages.\\r\\n\\r\\nQuality seals can be placed on the Shop web page if it has undergone an independent assessment and meets all requirements of the company issuing the seal. The purpose of these seals is to increase the confidence of online shoppers. However, the existence of many different seals, or seals unfamiliar to consumers, may foil this effort to a certain extent.\\r\\n\\r\\nA number of resources offer advice on how consumers can protect themselves when using online retailer services. These include:\\r\\n\\r\\nAlthough the benefits of online shopping are considerable, when the process goes poorly it can create a thorny situation. A few problems that shoppers potentially face include identity theft, faulty products, and the accumulation of spyware. If users are required to put in their credit card information and billing/shipping address and the website is not secure, customer information can be accessible to anyone who knows how to obtain it. Most large online corporations are inventing new ways to make fraud more difficult. However, criminals are constantly responding to these developments with new ways to manipulate the system. Even though online retailers are making efforts to protect consumer information, it is a constant fight to maintain the lead. It is advisable to be aware of the most current technology and scams to protect consumer identity and finances. Product delivery is also a main concern of online shopping. Most companies offer shipping insurance in case the product is lost or damaged. Some shipping companies will offer refunds or compensation for the damage, but this is up to their discretion.\\r\\n\\r\\nThe lack of full cost disclosure may also be problematic. While it may be easy to compare the base price of an item online, it may not be easy to see the total cost up front. Additional fees such as shipping are often not visible until the final step in the checkout process. The problem is especially evident with cross-border purchases, where the cost indicated at the final checkout screen may not include additional fees that must be paid upon delivery such as duties and brokerage. Some services such as the Canadian-based Wishabi attempts to include estimates of these additional cost,[39] but nevertheless, the lack of general full cost disclosure remains a concern.\\r\\n\\r\\nPrivacy of personal information is a significant issue for some consumers. Many consumers wish to avoid spam and telemarketing which could result from supplying contact information to an online merchant. In response, many merchants promise to not use consumer information for these purposes, Many websites keep track of consumer shopping habits in order to suggest items and other websites to view. Brick-and-mortar stores also collect consumer information. Some ask for a shopper's address and phone number at checkout, though consumers may refuse to provide it. Many larger stores use the address information encoded on consumers' credit cards (often without their knowledge) to add them to a catalog mailing list. This information is obviously not accessible to the merchant when paying in cash or through a bank (money transfer, in which case there is also proof of payment).\\r\\n\\r\\nMany successful purely virtual companies deal with digital products, (including information storage, retrieval, and modification), music, movies, office supplies, education, communication, software, photography, and financial transactions. Other successful marketers use drop shipping or affiliate marketing techniques to facilitate transactions of tangible goods without maintaining real inventory. Some non-digital products have been more successful than others for online stores. Profitable items often have a high value-to-weight ratio, they may involve embarrassing purchases, they may typically go to people in remote locations, and they may have shut-ins as their typical purchasers. Items which can fit in a standard mailboxsuch as music CDs, DVDs and booksare particularly suitable for a virtual marketer.\\r\\n\\r\\nProducts such as spare parts, both for consumer items like washing machines and for industrial equipment like centrifugal pumps, also seem good candidates for selling online. Retailers often need to order spare parts specially, since they typically do not stock them at consumer outletsin such cases, e-commerce solutions in spares do not compete with retail stores, only with other ordering systems. A factor for success in this niche can consist of providing customers with exact, reliable information about which part number their particular version of a product needs, for example by providing parts lists keyed by serial number. Products less suitable for e-commerce include products that have a low value-to-weight ratio, products that have a smell, taste, or touch component, products that need trial fittingsmost notably clothingand products where colour integrity appears important. Nonetheless, some web sites have had success delivering groceries and clothing sold through the internet is big business in the U.S.\\r\\n\\r\\nHigh-volume websites, such as Yahoo!, Amazon.com, and eBay, offer hosting services for online stores to all size retailers. These stores are presented within an integrated navigation framework, sometimes known as virtual shopping malls or online marketplaces.\\r\\n\\r\\nOne of the great benefits of online shopping is the ability to read product reviews, written either by experts or fellow online shoppers. The Nielsen Company conducted a survey in March 2010 and polled more than 27,000 Internet users in 55 markets from the Asia-Pacific, Europe, Middle East, North America, and South America to look at questions such as \\"How do consumers shop online?\\", \\"What do they intend to buy?\\", \\"How do they use various online shopping web pages?\\", and the impact of social media and other factors that come into play when consumers are trying to decide how to spend their money on which product or service. According to the research,[40] reviews on electronics (57%) such as DVD players, cellphones, or PlayStations, and so on, reviews on cars (45%), and reviews on software (37%) play an important role in influencing consumers who tend to make purchases online.  Furthermore, 40% of online shoppers indicate that they would not even buy electronics without consulting online reviews first.\\r\\n\\r\\nIn addition to online reviews, peer recommendations on online shopping pages or social media websites play a key role[41] for online shoppers when they are researching future purchases.[42] 90% of all purchases made are influenced by social media.[43]","input":"What is the purpose of online shopping websites?"},{"output":"the authoritative Hebrew and Aramaic text of the Tanakh for Rabbinic Judaism.","context":"Outline of Bible-related topics\\r\\n\\r\\nThe Masoretic[1] Text (MT or ??) is the authoritative Hebrew and Aramaic text of the Tanakh for Rabbinic Judaism. It is not the original text (Urtext) of the Hebrew Bible. It was primarily copied, edited and distributed by a group of Jews known as the Masoretes between the 7th and 10th centuries CE. The oldest extant manuscripts date from around the 9th century.[2]  The Aleppo Codex (once the oldest-known complete copy but now missing the Torah) dates from the 10th century. The Masoretic Text defines the Jewish canon and its precise letter-text, with its vocalization and accentuation known as the Masorah.\\r\\n\\r\\nThe ancient Hebrew word mesorah (?????, alt. ?????) broadly refers to the whole chain of Jewish tradition (see Oral law), which is claimed (by Orthodox Judaism) to be unchanged and infallible. Referring to the Masoretic Text, mesorah specifically means the diacritic markings of the text of the Hebrew Scriptures and the concise marginal notes in manuscripts (and later printings) of the Tanakh which note textual details, usually about the precise spelling of words.\\r\\n\\r\\nModern scholars seeking to understand the history of the Tanakhs text use a range of sources other than the Masoretic Text.[3] These include early Greek (Septuagint) and Syriac (Peshitta) translations, the Samaritan Pentateuch, the Dead Sea Scrolls and quotations from rabbinic manuscripts. Most of these are older than the oldest surviving Masoretic text and often contradict it.[4] Which of the three commonly known versions (Septuagint, Masoretic Text, Samaritan Pentateuch) is closest to the original text (Urtext) is not fully determined.[5]) The Dead Sea Scrolls have shown the Masoretic Text to be nearly identical in consonant text to some texts of the Tanakh dating from 200 BCE but different from others.[6] Though the consonants of the Masoretic Text differ little from the text generally accepted in the early 2nd century (and also differ little from some Qumran texts that are even older), it has many differences of both greater and lesser significance when compared to the manuscripts of the Septuagint, a Greek translation (about 1000 years older than the MT made in the 3rd to 2nd centuries BCE) of the Hebrew Scriptures that was in popular use by Jews in Egypt and the Holy Land (and matches the quotations in the New Testament, especially by Paul the Apostle).[7] A recent finding of a short Leviticus fragment, recovered from the ancient En-Gedi Scroll, carbon-dated to the 3rd or 4th century CE, is completely identical with the Masoretic Text.[8]\\r\\n\\r\\nThe Masoretic Text was used as the basis for translations of the Old Testament in Protestant Bibles such as the King James Version and American Standard Version and (after 1943) for some versions of Catholic Bibles, replacing the Vulgate translation, although the Vulgate had itself already been revised in light of the Masoretic text in the 1500s.\\r\\n\\r\\nThe Talmud and Karaite manuscripts[9] state that a standard copy of the Hebrew Bible was kept in the court of the Temple in Jerusalem for the benefit of copyists; there were paid correctors of Biblical books among the officers of the Temple (Talmud, tractate Ketubot 106a).[10] This copy is mentioned in the Letter of Aristeas ( 30; comp. Blau, Studien zum Althebr. Buchwesen, p.?100), in the statements of Philo (preamble to his \\"Analysis of the Political Constitution of the Jews\\"), and in Josephus (Contra Ap. i. 8).[9][10]\\r\\n\\r\\nA Talmudic story, perhaps referring to an earlier time, relates that three Torah scrolls were found in the Temple court but were at variance with each other. The differences were then resolved by majority decision among the three.[11]\\r\\n\\r\\nThe discovery of the Dead Sea Scrolls at Qumran, dating from c. 150 BCE-75 CE, shows that in this period there was not the scrupulous uniformity of text that was so stressed in later centuries. According to Menachem Cohen, the Dead Sea scrolls decided these issues 'by showing that there was indeed a Hebrew text-type on which the Septuagint-translation was based and which differed substantially from the received MT'.[12] The scrolls show numerous small variations in orthography, both as against the later Masoretic text, and between each other. It is also evident from the notings of corrections and of variant alternatives that scribes felt free to choose according to their personal taste and discretion between different readings.[12]\\r\\n\\r\\nHowever, despite these variations, most of the Qumran fragments can be classified as being closer to the Masoretic text than to any other text group that has survived. According to Lawrence Schiffman, 60% can be classed as being of proto-Masoretic type, and a further 20% Qumran style with bases in proto-Masoretic texts, compared to 5% proto-Samaritan type, 5% Septuagintal type, and 10% non-aligned.[13][page?needed] Joseph Fitzmyer noted the following regarding the findings at Qumran Cave 4 in particular: \\"Such ancient recensional forms of Old Testament books bear witness to an unsuspected textual diversity that once existed; these texts merit far greater study and attention than they have been accorded till now. Thus, the differences in the Septuagint are no longer considered the result of a poor or tendentious attempt to translate the Hebrew into the Greek; rather they testify to a different pre-Christian form of the Hebrew text\\".[14] On the other hand, some of the fragments conforming most accurately to the Masoretic text were found in Cave 4.[15]\\r\\n\\r\\nAn emphasis on minute details of words and spellings, already used among the Pharisees as bases for argumentation, reached its height with the example of Rabbi Akiva (died 135 CE). The idea of a perfect text sanctified in its consonantal base quickly spread throughout the Jewish communities via supportive statements in Halakha, Aggadah, and Jewish thought;[12] and with it increasingly forceful strictures that a deviation in even a single letter would make a Torah scroll invalid.[16]  Very few manuscripts are said to have survived the destruction of Jerusalem in 70 CE.[17] This both drastically reduced the number of variants in circulation, and gave a new urgency that the text must be preserved. New Greek translations were also made. Unlike the Septuagint, large-scale deviations in sense between the Greek of Aquila of Sinope and Theodotion and what we now know as the Masoretic text are minimal. Detailed variations between different Hebrew texts in use still clearly existed though, as witnessed by differences between the present-day Masoretic text and versions mentioned in the Gemara, and often even halachic midrashim based on spelling versions which do not exist in the current Masoretic text.[12]\\r\\n\\r\\nThe current received text finally achieved predominance through the reputation of the Masoretes, schools of scribes and Torah scholars working between the 7th and 11th centuries, based primarily in the Land of Israel in the cities of Tiberias and Jerusalem, and in Babylonia. According to Menachem Cohen these schools developed such prestige for the accuracy and error-control of their copying techniques that their texts established an authority beyond all others.[12]  Differences remained, sometimes bolstered by systematic local differences in pronunciation and cantillation. Every locality, following the tradition of its school, had a standard codex embodying its readings.  In Babylonia the school of Sura differed from that of Nehardea; and similar differences existed in the schools of the Land of Israel as against that at Tiberias, which in later times increasingly became the chief seat of learning. In this period living tradition ceased, and the Masoretes in preparing their codices usually followed the one school or the other, examining, however, standard codices of other schools and noting their differences.[10]\\r\\n\\r\\nIn the first half of the 10th century Aaron ben Moses ben Asher and Ben Naphtali were the leading Masoretes in Tiberias. Their names have come to symbolise the variations among Masoretes, but the differences between ben Asher and ben Naphtali should not be exaggerated.  There are hardly any differences between them regarding the consonants, though they differ more on vocalization and accents. Also, there were other authorities such as Rabbi Pinchas and Moshe Moheh, and ben Asher and ben Naphtali often agree against these others. Further, it is possible that all variations found among manuscripts eventually came to be regarded as disagreements between these figureheads.  Ben Asher wrote a standard codex[10] (the Aleppo Codex) embodying his opinions. Probably ben Naphtali did too, but it has not survived.[citation needed]\\r\\n\\r\\nIt has been suggested that there never was an actual \\"ben Naphtali\\"; rather, the name was chosen (based on the Bible, where Asher and Naphtali are the younger sons of Zilpah and Bilhah) to designate any tradition different from ben Asher's.[citation needed]\\r\\n\\r\\nBen Asher was the last of a distinguished family of Masoretes extending back to the latter half of the 8th century. Despite the rivalry of ben Naphtali and the opposition of Saadia Gaon, the most eminent representative of the Babylonian school of criticism, ben Asher's codex became recognized as the standard text of the Bible.[10] See Aleppo Codex, Codex Cairensis.\\r\\n\\r\\nMost of the secular scholars conclude that Aaron ben Asher was a Karaite, though there is evidence against this view.[18]\\r\\n\\r\\nThe two rival authorities, ben Asher and ben Naphtali, practically brought the Masorah to a close. Very few additions were made by the later Masoretes, styled in the 13th and 14th centuries Na?danim, who revised the works of the copyists, added the vowels and accents (generally in fainter ink and with a finer pen) and frequently the Masorah.[10]\\r\\n\\r\\nConsiderable influence on the development and spread of Masoretic literature was exercised during the eleventh, twelfth, and 13th centuries by the Franco-German school of Tosafists. Rabbi Gershom ben Judah, his brother Machir ben Judah, Joseph ben Samuel Bonfils (Tob 'Elem) of Limoges, Rabbeinu Tam (Jacob ben Me?r), Menahem ben Perez of Joigny, Perez ben Elijah of Corbeil, Marne, Judah ben Isaac Messer Leon, Me?r Spira, and Rabbi Meir of Rothenburg made Masoretic compilations, or additions to the subject, which are all more or less frequently referred to in the marginal glosses of Biblical codices and in the works of Hebrew grammarians.[10]\\r\\n\\r\\nBy long tradition, a ritual Sefer Torah (Torah scroll) could contain only the Hebrew consonantal text ÿ nothing added, nothing taken away. The Masoretic codices however, provide extensive additional material, called masorah, to show correct pronunciation and cantillation, protect against scribal errors, and annotate possible variants. The manuscripts thus include vowel points, pronunciation marks and stress accents in the text, short annotations in the side margins, and longer more extensive notes in the upper and lower margins and collected at the end of each book.\\r\\n\\r\\nThese notes were added because the Masoretes recognized the possibility of human error in copying the Hebrew Bible. The Masoretes were not working with the original Hebrew manuscripts of the Bible and corruptions had already crept into the versions they copied.[19]\\r\\n\\r\\nThe Hebrew word masorah is taken from the Book of Ezekiel 20:37 and means originally \\"legcuffs\\". The fixation of the text was considered to be in the nature of legcuffs upon its exposition. When, in the course of time, the Masorah had become a traditional discipline, the term became connected with the verb ??? \\"to hand down\\" and acquired the general meaning of \\"tradition.\\"[10]\\r\\n\\r\\nThe language of the Masoretic notes is primarily Aramaic but partly Hebrew. The Masoretic annotations are found in various forms: (a) in separate works, e.g., the Oklah we-Oklah; (b) in the form of notes written in the margins and at the end of codices. In rare cases, the notes are written between the lines. The first word of each Biblical book is also as a rule surrounded by notes. The latter are called the Initial Masorah; the notes on the side margins or between the columns are called the Small (Masora parva or Mp) or Inner Masorah (Masora marginalis); and those on the lower and upper margins, the Large or Outer Masorah (Masora magna or Mm[Mas.M]). The name \\"Large Masorah\\" is applied sometimes to the lexically arranged notes at the end of the printed Bible, usually called the Final Masorah,[10] (Masora finalis), or the Masoretic Concordance.[citation needed]\\r\\n\\r\\nThe Small Masorah consists of brief notes with reference to marginal readings, to statistics showing the number of times a particular form is found in Scripture, to full and defective spelling, and to abnormally written letters. The Large Masorah is more copious in its notes. The Final Masorah comprises all the longer rubrics for which space could not be found in the margin of the text, and is arranged alphabetically in the form of a concordance. The quantity of notes the marginal Masorah contains is conditioned by the amount of vacant space on each page. In the manuscripts it varies also with the rate at which the copyist was paid and the fanciful shape he gave to his gloss.[10]\\r\\n\\r\\nThere was accordingly an independent Babylonian Masora which differed from the Palestinian in terminology and to some extent in order. The Masora is concise in style with a profusion of abbreviations, requiring a considerable amount of knowledge for their full understanding. It was quite natural that a later generation of scribes would no longer understand the notes of the Masoretes and consider them unimportant; by the late medieval period they were reduced to mere ornamentation of the manuscripts. It was Jacob ben Chayyim who restored clarity and order to them.[20]\\r\\n\\r\\nIn most manuscripts, there are some discrepancies between the text and the masorah, suggesting that they were copied from different sources or that one of them has copying errors.  The lack of such discrepancies in the Aleppo Codex is one of the reasons for its importance; the scribe who copied the notes, presumably Aaron ben Moses ben Asher, probably wrote them originally.[citation needed]\\r\\n\\r\\nIn classical antiquity, copyists were paid for their work according to the number of stichs (lines of verse). As the prose books of the Bible were hardly ever written in stichs, the copyists, in order to estimate the amount of work, had to count the letters.[10] For the Masoretic Text, such statistical information more importantly also ensured accuracy in the transmission of the text with the production of subsequent copies that were done by hand.[citation needed]\\r\\n\\r\\nHence the Masoretes contributed the Numerical Masorah.[10] These notes are traditionally categorized into two main groups, the marginal Masorah and the final Masorah. The category of marginal Masorah is further divided into the Masorah parva (small Masorah) in the outer side margins and the Masorah magna (large Masorah), traditionally located at the top and bottom margins of the text.[citation needed]\\r\\n\\r\\nThe Masorah parva is a set of statistics in the outer side margins of the text. Beyond simply counting the letters, the Masorah parva consists of word-use statistics, similar documentation for expressions or certain phraseology, observations on full or defective writing, references to the Kethiv-Qere readings and more. These observations are also the result of a passionate zeal to safeguard the accurate transmission of the sacred text.[citation needed]\\r\\n\\r\\nEven though often cited as very exact, the Masoretic \\"frequency notes\\" in the margin of Codex Leningradiensis contain several errors.[21][22][23]\\r\\n\\r\\nThe Masorah magna, in measure, is an expanded Masorah parva. It is not printed in the Biblia Hebraica Stuttgartensia (BHS).[citation needed]\\r\\n\\r\\nThe final Masorah is located at the end of biblical books or after certain sections of the text, such as at the end of the Torah. It contains information and statistics regarding the number of words in a book or section, etc. Thus, Book of Leviticus 8:23 is the middle verse in the Pentateuch. The collation of manuscripts and the noting of their differences furnished material for the Text-Critical Masorah. The close relation which existed in earlier times (from the Soferim to the Amoraim inclusive) between the teacher of tradition and the Masorete, both frequently being united in one person, accounts for the Exegetical Masorah. Finally, the invention and introduction of a graphic system of vocalization and accentuation gave rise to the Grammatical Masorah.[10]\\r\\n\\r\\nThe most important of the Masoretic notes are those that detail the Qere and Ketiv that are located in the Masorah parva in the outside margins of BHS. Given that the Masoretes would not alter the sacred consonantal text, the Kethiv-Qere notes were a way of \\"correcting\\" or commenting on the text for any number of reasons (grammatical, theological, aesthetic, etc.) deemed important by the copyist.[24]\\r\\n\\r\\nThe earliest labors of the Masoretes included standardizing division of the text into books, sections, paragraphs, verses, and clauses (probably in the chronological order here enumerated); the fixing of the orthography, pronunciation, and cantillation; the introduction or final adoption of the square characters with the five final letters; some textual changes to guard against blasphemy and the like (though these changes may pre-date the Masoretes ÿ see Tikkune Soferim below); the enumeration of letters, words, verses, etc., and the substitution of some words for others in public reading.[10]\\r\\n\\r\\nSince no additions were allowed to be made to the official text of the Bible, the early Masoretes adopted other expedients: e.g., they marked the various divisions by spacing, and gave indications of halakic and haggadic teachings by full or defective spelling, abnormal forms of letters, dots, and other signs. Marginal notes were permitted only in private copies, and the first mention of such notes is found in the case of R. Me?r (c. 100ÿ150 CE).[10]\\r\\n\\r\\nEarly rabbinic sources, from around 200 CE, mention several passages of Scripture in which the conclusion is inevitable that the ancient reading must have differed from that of the present text. The explanation of this phenomenon is given in the expression \\"Scripture has used euphemistic language\\" (??? ?????), i.e. to avoid anthropomorphism and anthropopathism.[10]\\r\\n\\r\\nRabbi Simon ben Pazzi (3rd century) calls these readings \\"emendations of the Scribes\\" (tikkune Soferim; Midrash Genesis Rabbah xlix. 7), assuming that the Scribes actually made the changes. This view was adopted by the later Midrash and by the majority of Masoretes. In Masoretic works these changes are ascribed to Ezra; to Ezra and Nehemiah; to Ezra and the Soferim; or to Ezra, Nehemiah, Zechariah, Haggai, and Baruch. All these ascriptions mean one and the same thing: that the changes were assumed to have been made by the Men of the Great Synagogue.[10]\\r\\n\\r\\nThe term tikkun Soferim (???? ??????) has been understood by different scholars in various ways. Some regard it as a correction of Biblical language authorized by the Soferim for homiletical purposes. Others take it to mean a mental change made by the original writers or redactors of Scripture; i.e. the latter shrank from putting in writing a thought which some of the readers might expect them to express.[10]\\r\\n\\r\\nThe assumed emendations are of four general types:\\r\\n\\r\\nAmong the earliest technical terms used in connection with activities of the Scribes are the mikra Soferim and ittur Soferim. In the geonic schools, the first term was taken to signify certain vowel-changes which were made in words in pause or after the article; the second, the cancellation in a few passages of the \\"vav\\" conjunctive, where it had by some been wrongly read. The objection to such an explanation is that the first changes would fall under the general head of fixation of pronunciation, and the second under the head of Qere and Ketiv (i.e. \\"What is read\\" and \\"What is written\\"). Various explanations have, therefore, been offered by ancient as well as modern scholars without, however, succeeding in furnishing a completely satisfactory solution.[10]\\r\\n\\r\\nThere are four words having one of their letters suspended above the line. One of them, ???? (Judges 18:30), is due to an alteration of the original ??? out of reverence for Moses; rather than say that Moses' grandson became an idolatrous priest, a suspended letter nun?(???) was inserted to turn Mosheh into Menasheh (Manasseh). The origin of the other three (Psalms 80:14; Job 38:13, 38:15) is doubtful. According to some, they are due to mistaken majuscular letters; according to others, they are later insertions of originally omitted weak consonants.[10]\\r\\n\\r\\nIn fifteen passages in the Bible, some words are stigmatized; i.e., dots appear above the letters. (Genesis 16:5, 18:9, 19:33, 33:4, 37:12, Numbers 3:39, 9:10, 21:30, 29:15, Deuteronomy 29:28, 2 Samuel 19:20, Isaiah 44:9, Ezekiel 41:20, 46:22, Psalms 27:13) The significance of the dots is disputed. Some hold them to be marks of erasure; others believe them to indicate that in some collated manuscripts the stigmatized words were missing, hence that the reading is doubtful; still others contend that they are merely a mnemonic device to indicate homiletic explanations which the ancients had connected with those words; finally, some maintain that the dots were designed to guard against the omission by copyists of text-elements which, at first glance or after comparison with parallel passages, seemed to be superfluous. Instead of dots some manuscripts exhibit strokes, vertical or else horizontal. The first two explanations are unacceptable for the reason that such faulty readings would belong to Qere and Ketiv, which, in case of doubt, the majority of manuscripts would decide. The last two theories have equal probability.[10]\\r\\n\\r\\nIn nine passages of the Masoretic Text are found signs usually called inverted nuns, because they resemble the Hebrew letter nun?(???)[10] written in some inverted fashion.  The exact shape varies between different manuscripts and printed editions.  In many manuscripts, a reversed nun is foundreferred to as a nun hafucha by the masoretes. In some earlier printed editions, they are shown as the standard nun upside down or rotated, because the printer did not want to bother to design a character to be used only nine times.  The recent scholarly editions of the Masoretic Text show the reversed nun as described by the masoretes. In some manuscripts, however, other symbols are occasionally found instead. These are sometimes referred to in rabbinical literature as simaniyot (markers).[25]\\r\\n\\r\\nThe primary set of inverted nuns is found surrounding the text of Numbers 10:35ÿ36. The Mishna notes that this text is 85 letters long and dotted. This demarcation of this text leads to the later use of the inverted nun markings. Saul Lieberman demonstrated that similar markings can be found in ancient Greek texts where they are also used to denote 'short texts'. During the Medieval period, the inverted nuns were actually inserted into the text of the early Rabbinic Bibles published by Bomberg in the early 16th century. The talmud records that the markings surrounding Numbers 10:35ÿ36 were thought to denote that this 85 letter text was not in its proper place.[26]\\r\\n\\r\\nBar Kappara considered the Torah known to us as composed of seven volumes in the Gemara \\"The seven pillars with which Wisdom built her house (Prov. 9:1) are the seven Books of Moses\\". Genesis, Exodus and Leviticus and Deuteronomy as we know them but Numbers was really three separate volumes Numbers 1:1ÿ10:35 followed by Numbers 10:35ÿ36 and the third text from there to the end of Numbers.[27]\\r\\n\\r\\nThe 85 letter text is also said to be denoted because it is the model for the least number of letters which constitute a 'text' which one would be required to save from fire due to its holiness.[28]\\r\\n\\r\\nThe history of the Masorah may be divided into three periods: (1) creative period, from its beginning to the introduction of vowel-signs; (2) reproductive period, from the introduction of vowel-signs to the printing of the Masorah[10] (1525); (3) critical period, from 1525 to the present time.[citation needed]\\r\\n\\r\\nThe materials for the history of the first period are scattered remarks in Talmudic and Midrashic literature, in the post-Talmudical treatises Masseket Sefer Torah and Masseket Soferim, and in a Masoretic chain of tradition found in ben Asher's Di?du?e ha-?e'amim,  69 and elsewhere.[10]\\r\\n\\r\\nJacob ben Hayyim ibn Adonijah, having collated a vast number of manuscripts, systematized his material and arranged the Masorah in the second Bomberg edition of the Bible (Venice, 1524ÿ25). Besides introducing the Masorah into the margin, he compiled at the close of his Bible a concordance of the Masoretic glosses for which he could not find room in a marginal form, and added an elaborate introduction ÿ the first treatise on the Masorah ever produced. In spite of its numerous errors, this work has been considered by some as the \\"Textus Receptus\\" of the Masorah[10] (Wrthwein 1995:39), and was used for the English translation of the Old Testament for the King James Version.[citation needed]\\r\\n\\r\\nNext to Ibn Adoniyah, the critical study of the Masorah has been most advanced by Elia Levita, who published his famous \\"Massoret ha-Massoret\\" in 1538. The Tiberias of the elder Johannes Buxtorf (1620) made Levita's researches more accessible to a Christian audience. The eighth introduction to Walton's Polyglot Bible is largely a reworking of the Tiberias. Levita compiled likewise a vast Masoretic concordance, Sefer ha-Zikronot, which still lies in the National Library at Paris unpublished. The study is indebted also to R. Me?r b. Todros ha-Levi (RaMaH), who, as early as the 13th century, wrote his Sefer Massoret Seyag la-Torah (correct ed. Florence, 1750); to Menahem Lonzano, who composed a treatise on the Masorah of the Pentateuch entitled \\"Or Torah\\"; and in particular to Jedidiah Norzi, whose \\"Min?at Shai\\" contains valuable Masoretic notes based on a careful study of manuscripts.[10]\\r\\n\\r\\nThe Dead Sea Scrolls have shed new light on the history of the Masoretic Text. Many texts found there, especially those from Masada, are quite similar to the Masoretic Text, suggesting that an ancestor of the Masoretic Text was indeed extant as early as the 2nd century BCE. However, other texts, including many of those from Qumran, differ substantially, indicating that the Masoretic Text was but one of a diverse set of Biblical writings (Lane Fox 1991:99ÿ106; Tov 1992:115). Among the rejected books by both the Judaic and Catholic canons was found the Book of Enoch, the Community Rule (1QS) and War of the Sons of Light Against the Sons of Darkness (1QM).[29]\\r\\n\\r\\nIn a recent finding, the Masoretic Text is discovered to be completely identical with text recovered from an ancient scroll. The approximately 1,700-year-old En-Gedi Scroll was found in 1970 but had not had its content reconstructed until 2016. Researchers were able to recover 35 complete and partial lines of text from the Book of Leviticus and the text deciphered is completely identical with the consonantal framework of the Masoretic Text.[30] The En-Gedi scroll is the first time a biblical scroll has been discovered in an ancient synagogue's holy ark, where it would have been stored for prayers, and not in desert caves like the Dead Sea Scrolls.[31]\\r\\n\\r\\nThere have been very many published editions of the Masoretic Text, some of the most important being:","input":"What is the masoretic text of the bible?"},{"output":"an architectural framework for delivering IP multimedia services","context":"The IP Multimedia Subsystem or IP Multimedia Core Network Subsystem (IMS) is an architectural framework for delivering IP multimedia services. Historically, mobile phones have provided voice call services over a circuit-switched-style network, rather than strictly over an IP packet-switched network.  Alternative methods of delivering voice (VoIP) or other multimedia services have become available on smartphones, but they have not become standardized across the industry.[citation needed] IMS is an architectural framework to provide such standardization.\\r\\n\\r\\nIMS was originally designed by the wireless standards body 3rd Generation Partnership Project (3GPP), as a part of the vision for evolving mobile networks beyond GSM. Its original formulation (3GPP Rel-5) represented an approach for delivering Internet services over GPRS.  This vision was later updated by 3GPP, 3GPP2 and ETSI TISPAN by requiring support of networks other than GPRS, such as Wireless LAN, CDMA2000 and fixed lines.\\r\\n\\r\\nIMS uses IETF protocols wherever possible, e.g., the Session Initiation Protocol (SIP). According to the 3GPP, IMS is not intended to standardize applications, but rather to aid the access of multimedia and voice applications from wireless and wireline terminals, i.e., to create a form of fixed-mobile convergence (FMC).[1] This is done by having a horizontal control layer that isolates the access network from the service layer. From a logical architecture perspective, services need not have their own control functions, as the control layer is a common horizontal layer. However, in implementation this does not necessarily map into greater reduced cost and complexity.\\r\\n\\r\\nAlternative and overlapping technologies for access and provisioning of services across wired and wireless networks include combinations of Generic Access Network, softswitches and \\"naked\\" SIP.\\r\\n\\r\\nSince it is becoming increasingly easier to access content and contacts using mechanisms outside the control of traditional wireless/fixed operators, the interest of IMS is being challenged.[2]\\r\\n\\r\\nExamples of global standards based on IMS are MMTel which is the basis for Voice over LTE (VoLTE) and Rich Communication Services (RCS) which is also known as joyn or Advanced Messaging.\\r\\n\\r\\nEach of the functions in the diagram is explained below.\\r\\n\\r\\nThe IP multimedia core network subsystem is a collection of different functions, linked by standardized interfaces, which grouped form one IMS administrative network.[4] A function is not a node (hardware box): An implementer is free to combine two functions in one node, or to split a single function into two or more nodes. Each node can also be present multiple times in a single network, for dimensioning, load balancing or organizational issues.\\r\\n\\r\\nThe user can connect to IMS in various ways, most of which use the standard IP. IMS terminals (such as mobile phones, personal digital assistants (PDAs) and computers) can register directly on IMS, even when they are roaming in another network or country (the visited network). The only requirement is that they can use IP and run SIP user agents. Fixed access (e.g., Digital Subscriber Line (DSL), cable modems, Ethernet), mobile access (e.g. W-CDMA, CDMA2000, GSM, GPRS) and wireless access (e.g., WLAN, WiMAX) are all supported. Other phone systems like plain old telephone service (POTSthe old analogue telephones), H.323 and non IMS-compatible systems, are supported through gateways.\\r\\n\\r\\nHSS ÿ Home subscriber server:\\r\\nThe home subscriber server (HSS), or user profile server function (UPSF), is a master user database that supports the IMS network entities that actually handle calls. It contains the subscription-related information (subscriber profiles), performs authentication and authorization of the user, and can provide information about the subscriber's location and IP information. It is similar to the GSM home location register (HLR) and Authentication centre (AuC).\\r\\n\\r\\nA subscriber location function (SLF) is needed to map user addresses when multiple HSSs are used.\\r\\n\\r\\nUser identities:\\r\\nVarious identities may be associated with IMS: IP multimedia private identity (IMPI), IP multimedia public identity (IMPU), globally routable user agent URI (GRUU), wildcarded public user identity. Both IMPI and IMPU are not phone numbers or other series of digits, but uniform resource identifier (URIs), that can be digits (a Tel URI, such as tel:+1-555-123-4567) or alphanumeric identifiers (a SIP URI, such as sip:john.doe@example.com\\"  ).\\r\\n\\r\\nIP Multimedia Private Identity:\\r\\nThe IP Multimedia Private Identity (IMPI) is a unique permanently allocated global identity assigned by the home network operator, it has the form of an Network Access Identifier(NAI) i.e. user.name@domain, and is used, for example, for Registration, Authorization, Administration, and Accounting purposes.  Every IMS user shall have one IMPI.\\r\\n\\r\\nIP Multimedia Public Identity:\\r\\nThe IP Multimedia Public Identity (IMPU) is used by any user for requesting communications to other users (e.g. this might be included on a business card). Also known as Address of Record (AOR). There can be multiple IMPU per IMPI. The IMPU can also be shared with another phone, so that both can be reached with the same identity (for example, a single phone-number for an entire family).\\r\\n\\r\\nGlobally Routable User Agent URI:\\r\\nGlobally Routable User Agent URI (GRUU) is an identity that identifies a unique combination of IMPU and UE instance. \\r\\nThere are two types of GRUU: Public-GRUU (P-GRUU) and Temporary GRUU (T-GRUU). \\r\\n\\r\\nWildcarded Public User Identity:\\r\\nA wildcarded Public User Identity expresses a set of IMPU grouped together.\\r\\n\\r\\nThe HSS subscriber database contains the IMPU, IMPI, IMSI, MSISDN, subscriber service profiles, service triggers, and other information.\\r\\n\\r\\nSeveral roles of SIP servers or proxies, collectively called Call Session Control Function (CSCF), are used to process SIP signaling packets in the IMS.\\r\\n\\r\\nSIP Application servers (AS) host and execute services, and interface with the S-CSCF using SIP. An example of an application server that is being developed in 3GPP is the Voice call continuity Function (VCC Server). Depending on the actual service, the AS can operate in SIP proxy mode, SIP UA (user agent) mode or SIP B2BUA mode. An AS can be located in the home network or in an external third-party network. If located in the home network, it can query the HSS with the Diameter Sh or Si interfaces (for a SIP-AS).\\r\\n\\r\\nThe AS-ILCM (Application Server - Incoming Leg Control Model) and AS-OLCM (Application Server - Outgoing Leg Control Model) store transaction state, and may optionally store session state depending on the specific service being executed. \\r\\nThe AS-ILCM interfaces to the S-CSCF (ILCM) for an incoming leg and the AS-OLCM interfaces to the S-CSCF (OLCM) for an outgoing leg.\\r\\nApplication Logic provides the service(s) and interacts between the AS-ILCM and AS-OLCM.\\r\\n\\r\\nPublic Service Identities (PSI) are identities that identify services, which are hosted by application servers. As user identities, PSI takes the form of either a SIP or Tel URI. PSIs are stored in the HSS either as a distinct PSI or as a wildcarded PSI:\\r\\n\\r\\nThe Media Resource Function (MRF) provides media related functions such as media manipulation (e.g. voice stream mixing) and playing of tones and announcements.\\r\\n\\r\\nEach MRF is further divided into a media resource function controller (MRFC) and a media resource function processor (MRFP).\\r\\n\\r\\nThe Media Resource Broker (MRB) is a functional entity that is responsible for both collection of appropriate published MRF information and supplying of appropriate MRF information to consuming entities such as the AS. MRB can be used in two modes:\\r\\n\\r\\nA Breakout Gateway Control Function (BGCF) is a SIP proxy which processes requests for routing from an S-CSCF when  the S-CSCF has determined that the session cannot be routed using DNS or ENUM/DNS. It includes routing functionality based on telephone numbers.\\r\\n\\r\\nA PSTN/CS gateway interfaces with PSTN circuit switched (CS) networks. For signalling, CS networks use ISDN User Part (ISUP) (or BICC) over Message Transfer Part (MTP), while IMS uses SIP over IP. For media, CS networks use Pulse-code modulation (PCM), while IMS uses Real-time Transport Protocol (RTP).\\r\\n\\r\\nMedia Resources are those components that operate on the media plane and are under the control of IMS core functions. Specifically, Media Server (MS) and Media gateway (MGW)\\r\\n\\r\\nThere are two types of next-generation networking interconnection:\\r\\n\\r\\nAn NGN interconnection mode can be direct or indirect. Direct interconnection refers to the interconnection between two network domains without any intermediate network domain. Indirect interconnection at one layer refers to the interconnection between two network domains with one or more  intermediate network domain(s) acting as transit networks. The intermediate network domain(s) provide(s) transit functionality to the two other network domains. Different interconnection modes may be used for carrying service layer signalling and media traffic.\\r\\n\\r\\nOffline charging is applied to users who pay for their services periodically (e.g., at the end of the month). Online charging, also known as credit-based charging, is used for prepaid services, or real-time credit control of postpaid services. Both may be applied to the same session.\\r\\n\\r\\nCharging function addresses are addresses distributed to each IMS entities and provide a common location for each entity to send charging information. charging data function (CDF) addresses are used for offline billing and Online Charging Function (OCF) for online billing.\\r\\n\\r\\nIMS-based PES (PSTN Emulation System) provides IP networks services to analog devices. IMS-based PES allows non-IMS devices to appear to IMS as normal SIP users. Analog terminal using standard analog interfaces can connect to IMS-based PES in two ways:\\r\\n\\r\\nBoth A-MGW and VGW are unaware of the services. They only relay call control signalling to and from the PSTN terminal. Session control and handling is done by IMS components.\\r\\n\\r\\nReplacement for the Gq reference point.\\r\\n\\r\\nOne of the most important features of IMS, that of allowing for a SIP application to be dynamically and differentially (based on the user's profile) triggered, is implemented as a filter-and-redirect signalling mechanism in the S-CSCF.\\r\\n\\r\\nThe S-CSCF might apply filter criteria to determine the need to forward SIP requests to AS. It is important to note that services for the originating party will be applied in the originating network, while the services for the terminating party will be applied in the terminating network, all in the respective S-CSCFs.\\r\\n\\r\\nAn initial filter criteria (iFC) is an XML-based format used for describing control logic. iFCs represent a provisioned subscription of a user to an application. They are stored in the HSS as part of the IMS Subscription Profile and are downloaded to the S-CSCF upon user registration (for registered users) or on processing demand (for services, acting as unregistered users). iFCs are valid throughout the registration lifetime or until the User Profile is changed.[5]\\r\\n\\r\\nThe iFC is composed of:\\r\\n\\r\\nThere are two types of iFCs:\\r\\n\\r\\nIt is envisaged that security defined in TS 33.203 may not be available for a while especially because of the lack of USIM/ISIM interfaces and prevalence of devices that support IPv4. For this situation, to provide some protection against the most significant threats, 3GPP defines some security mechanisms, which are informally known as \\"early IMS security,\\" in TR33.978. This mechanism relies on the authentication performed during the network attachment procedures, which binds between the user's profile and its IP address. This mechanism is also weak because the signaling is not protected on the userÿnetwork interface.\\r\\n\\r\\nCableLabs in PacketCable 2.0, which adopted also the IMS architecture but has no USIM/ISIM capabilities in their terminals, published deltas to the 3GPP specifications where the Digest-MD5 is a valid authentication option. Later on, TISPAN also did a similar effort given their fixed networks scopes, although the procedures are different. To compensate for the lack of IPsec capabilities, TLS has been added as an option for securing the Gm interface. Later 3GPP Releases have included the Digest-MD5 method, towards a Common-IMS platform, yet in its own and again different approach. Although all 3 variants of Digest-MD5 authentication have the same functionality and are the same from the IMS terminal's perspective, the implementations on the Cx interface between the S-CSCF and the HSS are different.","input":"What is ip multimedia subsystem (ims)?"},{"output":"classical music","context":"Classical music is art music produced or rooted in the traditions of Western culture, including both liturgical (religious) and secular music. While a more precise term is also used to refer to the period from 1750 to 1820 (the Classical period), this article is about the broad span of time from before the 6th century AD to the present day, which includes the Classical period and various other periods.[1] The central norms of this tradition became codified between 1550 and 1900, which is known as the common-practice period. The major time divisions of Western art music are as follows:\\r\\nEuropean art music is largely distinguished from many other non-European classical and some popular musical forms by its system of staff notation, in use since about the 11th century.[2] Western staff notation is used by composers to indicate to the performer the pitches (which form the melodies, basslines and chords), tempo, metre and rhythms for a piece of music.[clarification needed] This can leave less room for practices such as improvisation and ad libitum ornamentation, which are frequently heard in non-European art music and in popular-music[3][clarification needed][4][not in citation given][5] styles such as jazz and blues. Another difference is that whereas most popular styles adopt the song (strophic) form or a derivation of this form, classical music has been noted for its development of highly sophisticated forms of instrumental music such as the symphony, concerto, fugue, sonata, and mixed vocal and instrumental styles such as opera, cantata, and mass.[6]\\r\\nThe term \\"classical music\\" did not appear until the early 19th century, in an attempt to distinctly canonize the period from Johann Sebastian Bach to Ludwig van Beethoven as a golden age.[7] The earliest reference to \\"classical music\\" recorded by the Oxford English Dictionary is from about 1829.[1][8]\\r\\n\\r\\n\\r\\nGiven the wide range of styles in European classical music, from Medieval plainchant sung by monks to Classical and Romantic symphonies for orchestra from the 1700s and 1800s to avant-garde atonal compositions for solo piano from the 1900s, it is difficult to list characteristics that can be attributed to all works of that type. However, there are characteristics that classical music contains that few or no other genres of music contain,[9] such as the use of music notation[clarification needed] and the performance of complex forms of solo instrumental works (e.g., the fugue). Furthermore, while the symphony did not exist prior to the late 18th century, the symphony ensembleand the works written for ithave become a defining feature of classical music.[citation needed]\\r\\nThe key characteristic of European classical music that distinguishes it from popular music and folk music is that the repertoire tends to be written down in musical notation, creating a musical part or score. This score typically determines details of rhythm, pitch, and, where two or more musicians (whether singers or instrumentalists) are involved, how the various parts are coordinated. The written quality of the music has enabled a high level of complexity within them: fugues, for instance, achieve a remarkable marriage of boldly distinctive melodic lines weaving in counterpoint yet creating a coherent harmonic logic that would be difficult to achieve in the heat of live improvisation.[10][clarification needed] The use of written notation also preserves a record of the works and enables Classical musicians to perform music from many centuries ago. Musical notation enables 2000s-era performers to sing a choral work from the 1300s Renaissance era or a 1700s Baroque concerto with many of the features of the music (the melodies, lyrics, forms, and rhythms) being reproduced.[citation needed]\\r\\nThat said, the score does allow the interpreter to make choices on how to perform a historical work.[citation needed] For example, if the tempo is written with an Italian instruction (e.g., Allegro), it is not known exactly how fast the piece should be played. As well, in the Baroque era, many works that were designed for basso continuo accompaniment do not specify which instruments should play the accompaniment or exactly how the chordal instrument (harpsichord, lute, etc.) should play the chords, which are not notated in the part[clarification needed] (only a figured bass symbol in the bass part is used to guide the chord-playing performer). The performer and the conductor have a range of options for musical expression and interpretation of a scored piece, including the phrasing of melodies, the time taken during fermatas (held notes) or pauses, and the use (or choice not to use) of effects such as vibrato or glissando (these effects are possible on various stringed, brass and woodwind instruments and with the human voice).\\r\\nAlthough Classical music in the 2000s has lost most of its tradition for musical improvisation, from the Baroque era to the Romantic era, there are examples of performers who could improvise in the style of their era. In the Baroque era, organ performers would improvise preludes, keyboard performers playing harpsichord would improvise chords from the figured bass symbols beneath the bass notes of the basso continuo part and both vocal and instrumental performers would improvise musical ornaments.[11] Johann Sebastian Bach was particularly noted for his complex improvisations.[12] During the Classical era, the composer-performer Mozart was noted for his ability to improvise melodies in different styles.[13] During the Classical era, some virtuoso soloists would improvise the cadenza sections of a concerto. During the Romantic era, Beethoven would improvise at the piano.[14] For more information, see Improvisation.\\r\\nThe instruments currently used in most classical music were largely invented before the mid-19th century (often much earlier) and codified in the 18th and 19th centuries. They consist of the instruments found in an orchestra or in a concert band, together with several other solo instruments (such as the piano, harpsichord, and organ). The symphony orchestra is the most widely known medium for classical music[15] and includes members of the string, woodwind, brass, and percussion families of instruments. The concert band consists of members of the woodwind, brass, and percussion families. It generally has a larger variety and number of woodwind and brass instruments than the orchestra but does not have a string section. However, many concert bands use a double bass. The vocal practices changed over the classical period, from the single line monophonic Gregorian chant done by monks in the Medieval period to the complex, polyphonic choral works of the Renaissance and subsequent periods, which used multiple independent vocal melodies at the same time.\\r\\nMany of the instruments used to perform medieval music still exist, but in different forms. Medieval instruments included the flute, the recorder and plucked string instruments like the lute. As well, early versions of the organ, fiddle (or vielle), and trombone (called the sackbut) existed. Medieval instruments in Europe had most commonly been used singly, often self accompanied with a drone note, or occasionally in parts. From at least as early as the 13th century through the 15th century there was a division of instruments into haut (loud, shrill, outdoor instruments) and bas (quieter, more intimate instruments).[16] During the earlier medieval period, the vocal music from the liturgical genre, predominantly Gregorian chant, was monophonic, using a single, unaccompanied vocal melody line.[17] Polyphonic vocal genres, which used multiple independent vocal melodies, began to develop during the high medieval era, becoming prevalent by the later 13th and early 14th century.\\r\\nMany instruments originated during the Renaissance; others were variations of, or improvements upon, instruments that had existed previously. Some have survived to the present day; others have disappeared, only to be recreated in order to perform music of the period on authentic instruments. As in the modern day, instruments may be classified as brass, strings, percussion, and woodwind. Brass instruments in the Renaissance were traditionally played by professionals who were members of Guilds and they included the slide trumpet, the wooden cornet, the valveless trumpet and the sackbut. Stringed instruments included the viol, the harp-like lyre, the hurdy-gurdy, the cittern and the lute. Keyboard instruments with strings included the harpsichord and the virginals.[clarification needed] Percussion instruments include the triangle, the Jew's harp, the tambourine, the bells, the rumble-pot, and various kinds of drums. Woodwind instruments included the double reed shawm, the reed pipe, the bagpipe, the transverse flute and the recorder. Vocal music in the Renaissance is noted for the flourishing of an increasingly elaborate polyphonic style. The principal liturgical forms which endured throughout the entire Renaissance period were masses and motets, with some other developments towards the end, especially as composers of sacred music began to adopt secular forms (such as the madrigal) for their own designs. Towards the end of the period, the early dramatic precursors of opera such as monody, the madrigal comedy, and the intermedio are seen. Around 1597, Italian composer Jacopo Peri wrote Dafne, the first work to be called an opera today. He also composed Euridice, the first opera to have survived to the present day.\\r\\nBaroque instruments included some instruments from the earlier periods (e.g., the hurdy-gurdy and recorder) and a number of new instruments (e.g, the oboe, bassoon, cello, contrabass and fortepiano). Some instruments from previous eras fell into disuse, such as the shawm and the wooden cornet. The key Baroque instruments for strings included the violin, viol, viola, viola d'amore, cello, contrabass, lute, theorbo (which often played the basso continuo parts), mandolin, cittern, Baroque guitar, harp and hurdy-gurdy. Woodwinds included the Baroque flute, Baroque oboe, rackett, recorder and the bassoon. Brass instruments included the cornett, natural horn, Baroque trumpet, serpent and the trombone. Keyboard instruments included the clavichord, the harpsichord, the pipe organ, and, later in the period, the fortepiano (an early version of the piano). Percussion instruments included the timpani, snare drum, tambourine and the castanets.\\r\\nOne major difference between Baroque music and the classical era that followed it is that the types of instruments used in Baroque ensembles were much less standardized. Whereas a classical era string quartet consists almost exclusively of two violins, a viola and a cello, a Baroque or Classical-era group accompanying a soloist or opera could include one of several different types of keyboard instruments (e.g., pipe organ, harpsichord, or clavichord), additional stringed chordal instruments (e.g., a lute) and an unspecified number of bass instruments performing the basso continuo, including bowed strings, woodwinds and brass instruments (e.g., a cello, contrabass, viol, bassoon, serpent, etc.).\\r\\nVocal developments in the Baroque era included the development of opera types such as opera seria and opra comique, and related forms such as oratorios and cantatas.[18][19]\\r\\nThe term \\"classical music\\" has two meanings: the broader meaning includes all Western art music from the Medieval era to the 2000s, and the specific meaning refers to the art music from the 1750s to the early 1820sthe period of Wolfgang Amadeus Mozart and Joseph Haydn. This section is about the more specific meaning. Classical era musicians continued to use many of instruments from the Baroque era, such as the cello, contrabass, recorder, trombone, timpani, fortepiano (the precursor to the modern piano) and organ. While some Baroque instruments fell into disuse (e.g., the theorbo and rackett), many Baroque instruments were changed into the versions that are still in use today, such as the Baroque violin (which became the violin), the Baroque oboe (which became the oboe) and the Baroque trumpet, which transitioned to the regular valved trumpet. During the Classical era, the stringed instruments used in orchestra and chamber music such as string quartets were standardized as the four instruments which form the string section of the orchestra: the violin, viola, cello and double bass. Baroque-era stringed instruments such as fretted, bowed viols were phased out. Woodwinds included the basset clarinet, basset horn, clarinette d'amour, the Classical clarinet, the chalumeau, the flute, oboe and bassoon. Keyboard instruments included the clavichord and the fortepiano. While the harpsichord was still used in basso continuo accompaniment in the 1750s and 1760s, it fell out of use in the end of the century. Brass instruments included the buccin, the ophicleide (a replacement for the bass serpent, which was the precursor of the tuba) and the natural horn.\\r\\nIn the Romantic era, the modern piano, with a more powerful, sustained tone and a wider range took over from the more delicate-sounding fortepiano. In the orchestra, the existing Classical instruments and sections were retained (string section, woodwinds, brass and percussion), but these sections were typically expanded to make a fuller, bigger sound. For example, while a Baroque orchestra may have had two double bass players, a Romantic orchestra could have as many as ten. \\"As music grew more expressive, the standard orchestral palette just wasn't rich enough for many Romantic composers.\\" [20] New woodwind instruments were added, such as the contrabassoon, bass clarinet and piccolo and new percussion instruments were added, including xylophones, snare drums, celestes (a bell-like keyboard instrument), bells, and triangles,[20] large orchestral harps, and even wind machines for sound effects. Saxophones appear in some scores from the late 19th century onwards. While appearing only as featured solo instruments in some works, for example Maurice Ravel's orchestration of Modest Mussorgsky's Pictures at an Exhibition and Sergei Rachmaninoff's Symphonic Dances, the saxophone is included in other works, such as Ravel's Bolro, Sergei Prokofiev's Romeo and Juliet Suites 1 and 2 and many other works as a member of the orchestral ensemble. The euphonium is featured in a few late Romantic and 20th-century works, usually playing parts marked \\"tenor tuba\\", including Gustav Holst's The Planets, and Richard Strauss's Ein Heldenleben.\\r\\nThe Wagner tuba, a modified member of the horn family, appears in Richard Wagner's cycle Der Ring des Nibelungen and several other works by Strauss, Bla Bart܇k, and others; it has a prominent role in Anton Bruckner's Symphony No. 7 in E Major.[21] Cornets appear in Pyotr Ilyich Tchaikovsky's ballet Swan Lake, Claude Debussy's La mer, and several orchestral works by Hector Berlioz.[clarification needed] Unless these instruments are played by members doubling on another instrument (for example, a trombone player changing to euphonium for a certain passage), orchestras will use freelance musicians to augment their regular rosters.[citation needed]\\r\\nModernism in music is a philosophical and aesthetic stance underlying the period of change and development in musical language that occurred from 1890 to 1930, a period of diverse reactions in challenging and reinterpreting older categories of music, innovations that lead to new ways of organizing and approaching harmonic, melodic, sonic, and rhythmic aspects of music, and changes in aesthetic worldviews in close relation to the larger identifiable period of modernism in the arts of the time. The operative word most associated with it is \\"innovation\\".[22] Its leading feature is a \\"linguistic plurality\\", which is to say that no single music genre ever assumed a dominant position.[23]\\r\\nContemporary classical music is the period that came into prominence in the mid-1970s. It includes different variations of modernist, postmodern, neoromantic, and pluralist music.[24] However, the term may also be employed in a broader sense to refer to all post-1945 musical forms.[25]\\r\\nPostmodern music is a period of music that began around 1930[26][27]. It shares characteristics with postmodernist art ÿ that is, art that comes after and reacts against modernism.\\r\\nMany instruments that in the 2010s are associated with popular music filled important roles in early music, such as bagpipes, theorbos, vihuelas, hurdy-gurdies (hand-cranked string instruments), accordions, alphorns, hydraulises, calliopes, sistrums, and some woodwind instruments such as tin whistles, panpipes, shawms and crumhorns. On the other hand, instruments such as the acoustic guitar, once associated mainly with popular music, gained prominence in classical music in the 19th and 20th centuries in the form of the classical guitar and banjo. While equal temperament gradually accepted as the dominant musical temperament during the 19th century, different historical temperaments are often used for music from earlier periods. For instance, music of the English Renaissance is often performed in meantone temperament. As well, while professional orchestras and pop bands all around the world have tuned to an A fixed at 440?Hz since the late 19th century, there was historically a great variety in the tuning pitch, as attested to in historical pipe organs that still exist.[28][unreliable source?]\\r\\nPerformers who have studied classical music extensively are said to be \\"classically trained\\". This training may come from private lessons from instrument or voice teachers or from completion of a formal program offered by a Conservatory, college or university, such as a Bachelor of Music or Master of Music degree (which includes individual lessons from professors). In classical music, \\"...extensive formal music education and training, often to postgraduate [Master's degree] level\\" is required.[29]\\r\\nPerformance of classical music repertoire requires a proficiency in sight-reading and ensemble playing, harmonic principles, strong ear training (to correct and adjust pitches by ear), knowledge of performance practice (e.g., Baroque ornamentation), and a familiarity with the style/musical idiom expected for a given composer or musical work (e.g., a Brahms symphony or a Mozart concerto).[citation needed]\\r\\nSome \\"popular\\" genre musicians have had significant classical training, such as Billy Joel, Elton John, the Van Halen brothers, Randy Rhoads, Ritchie Blackmore, and Dream Theater members.[citation needed] Moreover, formal training is not unique to the classical genre. Many rock and pop musicians have completed degrees in commercial music programs such as those offered by the Berklee College of Music and many jazz musicians have completed degrees in music from universities with jazz programs, such as the Manhattan School of Music and McGill University.[citation needed]\\r\\nHistorically, major professional orchestras have been mostly or entirely composed of musicians who are men. Some of the earliest cases of women being hired in professional orchestras was in the position of harpist. The Vienna Philharmonic, for example, did not accept women to permanent membership until 1997, far later than the other orchestras ranked among the world's top five by Gramophone in 2008.[30] The last major orchestra to appoint a woman to a permanent position was the Berlin Philharmonic.[31] As late as February 1996, the Vienna Philharmonic's principal flute, Dieter Flury, told Westdeutscher Rundfunk that accepting women would be \\"gambling with the emotional unity (emotionelle Geschlossenheit) that this organism currently has\\".[32] In April 1996, the orchestra's press secretary wrote that \\"compensating for the expected leaves of absence\\" of maternity leave would be a problem.[33]\\r\\nIn 1997, the Vienna Philharmonic was \\"facing protests during a [US] tour\\" by the National Organization for Women and the International Alliance for Women in Music. Finally, \\"after being held up to increasing ridicule even in socially conservative Austria, members of the orchestra gathered [on 28 February 1997] in an extraordinary meeting on the eve of their departure and agreed to admit a woman, Anna Lelkes, as harpist.\\"[34] As of 2013, the orchestra has six female members; one of them, violinist Albena Danailova became one of the orchestra's concertmasters in 2008, the first woman to hold that position.[35] In 2012, women still made up just 6% of the orchestra's membership. VPO president Clemens Hellsberg said the VPO now uses completely screened blind auditions.[36]\\r\\nIn 2013, an article in Mother Jones stated that while \\"[m]any prestigious orchestras have significant female membershipwomen outnumber men in the New York Philharmonic's violin sectionand several renowned ensembles, including the National Symphony Orchestra, the Detroit Symphony, and the Minnesota Symphony, are led by women violinists\\", the double bass, brass, and percussion sections of major orchestras \\"...are still predominantly male.\\"[37] A 2014 BBC article stated that the \\"...introduction of 'blind' auditions, where a prospective instrumentalist performs behind a screen so that the judging panel can exercise no gender or racial prejudice, has seen the gender balance of traditionally male-dominated symphony orchestras gradually shift.\\"[38]\\r\\nWorks of classical repertoire often exhibit complexity in their use of orchestration, counterpoint, harmony, musical development, rhythm, phrasing, texture, and form. Whereas most popular styles are usually written in song form, classical music is noted for its development of highly sophisticated instrumental musical forms,[6] like the concerto, symphony and sonata. Classical music is also noted for its use of sophisticated vocal/instrumental forms, such as opera.[citation needed] In opera, vocal soloists and choirs perform staged dramatic works with an orchestra providing accompaniment. Longer instrumental works are often divided into self-contained pieces, called movements, often with contrasting characters or moods. For instance, symphonies written during the Classical period are usually divided into four movements: (1) an opening Allegro in sonata form, (2) a slow movement, (3) a minuet or scherzo (in a triple metre, such as 3/4), and (4) a final Allegro. These movements can then be further broken down into a hierarchy of smaller units: first sections, then periods, and finally phrases.\\r\\nThe major time divisions of classical music up to 1900 are the early music period, which includes Medieval (500ÿ1400) and Renaissance (1400ÿ1600) eras, and the Common practice period, which includes the Baroque (1600ÿ1750), Classical (1750ÿ1820) and Romantic (1810ÿ1910) eras. Since 1900, classical periods have been reckoned more by calendar century than by particular stylistic movements that have become fragmented and difficult to define.[citation needed] The 20th century calendar period (1901ÿ2000) includes most of the early modern musical era (1890ÿ1930), the entire high modern (mid 20th-century), and the first 25 years of the contemporary (1945 or 1975ÿcurrent) or postmodern musical era (1930ÿcurrent). The 21st century has so far been characterized by a continuation of the contemporary/postmodern musical era.\\r\\nThe dates are generalizations, since the periods and eras overlap and the categories are somewhat arbitrary, to the point that some authorities reverse terminologies and refer to a common practice \\"era\\" comprising baroque, classical, and romantic \\"periods\\".[39] For example, the use of counterpoint and fugue, which is considered characteristic of the Baroque era (or period), was continued by Haydn, who is classified as typical of the Classical era. Beethoven, who is often described as a founder of the Romantic era, and Brahms, who is classified as Romantic, also used counterpoint and fugue, but other characteristics of their music[vague] define their era.\\r\\nThe prefix neo- is used to describe a 19th-, 20th-, or 21st-century composition written in the style of an earlier era, such as Classical or Romantic. Stravinsky's Pulcinella, for example, is a neoclassical composition because it is stylistically similar to works of the Baroque era.[clarification needed]\\r\\nBurgh (2006), suggests that the roots of Western classical music ultimately lie in ancient Egyptian art music via cheironomy and the ancient Egyptian orchestra, which dates to 2695 BC.[40] The development of individual tones and scales was made by ancient Greeks such as Aristoxenus and Pythagoras.[41] Pythagoras created a tuning system and helped to codify musical notation. Ancient Greek instruments such as the aulos (a reed instrument) and the lyre (a stringed instrument similar to a small harp) eventually led to the modern-day instruments of a classical orchestra.[42] The antecedent to the early period was the era of ancient music before the fall of the Roman Empire (476 AD). Very little music survives from this time, most of it from ancient Greece.[citation needed]\\r\\nThe Medieval period includes music from after the fall of Rome to about 1400. Monophonic chant, also called plainsong or Gregorian chant, was the dominant form until about 1100.[43] Polyphonic (multi-voiced) music developed from monophonic chant throughout the late Middle Ages and into the Renaissance, including the more complex voicings of motets. The Renaissance era was from 1400 to 1600. It was characterized by greater use of instrumentation, multiple interweaving melodic lines, and the use of the first bass instruments. Social dancing became more widespread, so musical forms appropriate to accompanying dance began to standardize. It is in this time that the notation of music on a staff and other elements of musical notation began to take shape.[44] This invention made possible the separation of the composition of a piece of music from its transmission; without written music, transmission was oral, and subject to change every time it was transmitted. With a musical score, a work of music could be performed without the composer's presence.[43] The invention of the movable-type printing press in the 15th century had far-reaching consequences on the preservation and transmission of music.[45]\\r\\nTypical stringed instruments of the early period include the harp, lute, vielle, and psaltery, while wind instruments included the flute family (including recorder), shawm (an early member of the oboe family), trumpet, and the bagpipes. Simple pipe organs existed, but were largely confined to churches, although there were portable varieties.[46] Later in the period, early versions of keyboard instruments like the clavichord and harpsichord began to appear. Stringed instruments such as the viol had emerged by the 16th century, as had a wider variety of brass and reed instruments. Printing enabled the standardization of descriptions and specifications of instruments, as well as instruction in their use.[47]\\r\\nNotable Medieval composers include Hildegard of Bingen, Guillaume de Machaut, Lonin, Protin, Philippe de Vitry, Francesco Landini, and Johannes Ciconia.\\r\\nNotable Renaissance composers include Josquin des Prez, Giovanni Pierluigi da Palestrina, John Dunstaple, Johannes Ockeghem, Orlande de Lassus, Guillaume Du Fay, Gilles Binchois, Thomas Tallis, William Byrd, Giovanni Gabrieli, Carlo Gesualdo, John Dowland, Jacob Obrecht, Adrian Willaert, Jacques Arcadelt, and Cipriano de Rore.\\r\\nThe common practice period is usually defined as the era between the formation and the dissolution of the hegemony[citation needed] of common-practice tonality. The term usually spans roughly two-and-a-half centuries, encompassing the Baroque, Classical, and Romantic periods.\\r\\nBaroque music is characterized by the use of complex tonal counterpoint and the use of a basso continuo, a continuous bass line. Music became more complex in comparison with the songs of earlier periods.[15] The beginnings of the sonata form took shape in the canzona, as did a more formalized notion of theme and variations. The tonalities of major and minor as means for managing dissonance and chromaticism in music took full shape.[48]\\r\\nDuring the Baroque era, keyboard music played on the harpsichord and pipe organ became increasingly popular, and the violin family of stringed instruments took the form generally seen today. Opera as a staged musical drama began to differentiate itself from earlier musical and dramatic forms, and vocal forms like the cantata and oratorio became more common.[49] Vocalists began adding embellishments to melodies.[15] Instrumental ensembles began to distinguish and standardize by size[clarification needed], giving rise to the early orchestra for larger ensembles, with chamber music being written for smaller groups of instruments where parts are played by individual (instead of massed) instruments. The concerto as a vehicle for solo performance[clarification needed] accompanied by an orchestra became widespread.\\r\\nThe theories surrounding equal temperament began to be put in wider practice, especially as it enabled a wider range of chromatic possibilities in hard-to-tune keyboard instruments. Although Bach did not use equal temperament, as a modern piano is generally tuned, changes in the temperaments from the meantone system, common at the time, to various temperaments that made modulation between all keys musically acceptable, made possible Bach's Well-Tempered Clavier.[50]\\r\\nImportant composers of this era include Johann Sebastian Bach, Antonio Vivaldi, George Frideric Handel, Henry Purcell, Claudio Monteverdi, Domenico Scarlatti, Georg Philipp Telemann, Arcangelo Corelli, Alessandro Scarlatti, Jean-Philippe Rameau, Jean-Baptiste Lully, and Heinrich Schtz.\\r\\nThe Classical era, from about 1750 to 1820, established many of the norms of composition, presentation, and style, and was also when the piano became the predominant keyboard instrument. The basic forces required for an orchestra became somewhat standardized (although they would grow as the potential of a wider array of instruments was developed in the following centuries). Chamber music grew to include ensembles with as many as 8 to 10 performers for serenades. Opera continued to develop, with regional styles in Italy, France, and German-speaking lands. The opera buffa, a form of comic opera, rose in popularity. The symphony came into its own as a musical form, and the concerto was developed as a vehicle for displays of virtuoso playing skill. Orchestras no longer required a harpsichord (which had been part of the traditional continuo in the Baroque style), and were often led by the lead violinist (now called the concertmaster).[51]\\r\\nWind instruments became more refined in the Classical era. While double reeded instruments like the oboe and bassoon became somewhat standardized in the Baroque, the clarinet family of single reeds was not widely used until Mozart expanded its role in orchestral, chamber, and concerto settings.[52]\\r\\nMajor composers of this period include Wolfgang Amadeus Mozart, Ludwig van Beethoven, Joseph Haydn, Christoph Willibald Gluck, Johann Christian Bach, Luigi Boccherini, Carl Philipp Emanuel Bach, Muzio Clementi, Antonio Salieri, and Johann Nepomuk Hummel.\\r\\nThe music of the Romantic era, from roughly the first decade of the 19th century to the early 20th century, was characterized by increased attention to an extended melodic line, as well as expressive and emotional elements, paralleling romanticism in other art forms. Musical forms began to break from the Classical era forms (even as those were being codified), with free-form pieces like nocturnes, fantasias, and preludes being written where accepted ideas about the exposition and development of themes were ignored or minimized.[53] The music became more chromatic, dissonant, and tonally colorful, with tensions (with respect to accepted norms of the older forms) about key signatures increasing.[54] The art song (or Lied) came to maturity in this era, as did the epic scales of grand opera, ultimately transcended by Richard Wagner's Ring cycle.[55]\\r\\nIn the 19th century, musical institutions emerged from the control of wealthy patrons, as composers and musicians could construct lives independent of the nobility. Increasing interest in music by the growing middle classes throughout western Europe spurred the creation of organizations for the teaching, performance, and preservation of music. The piano, which achieved its modern construction in this era (in part due to industrial advances in metallurgy) became widely popular with the middle class, whose demands for the instrument spurred a large number of piano builders. Many symphony orchestras date their founding to this era.[54] Some musicians and composers were the stars of the day; some, like Franz Liszt and Niccol Paganini, fulfilled both roles.[56]\\r\\nThe family of instruments used, especially in orchestras, grew. A wider array of percussion instruments began to appear. Brass instruments took on larger roles, as the introduction of rotary valves made it possible for them to play a wider range of notes. The size of the orchestra (typically around 40 in the Classical era) grew to be over 100.[54] Gustav Mahler's 1906 Symphony No. 8, for example, has been performed with over 150 instrumentalists and choirs of over 400.[57]\\r\\nEuropean cultural ideas and institutions began to follow colonial expansion into other parts of the world. There was also a rise, especially toward the end of the era, of nationalism in music (echoing, in some cases, political sentiments of the time), as composers such as Edvard Grieg, Nikolai Rimsky-Korsakov, and Antonn Dvo?k echoed traditional music of their homelands in their compositions.[58]\\r\\nProminent composers of this era include Pyotr Ilyich Tchaikovsky, Frdric Chopin, Hector Berlioz, Franz Schubert, Robert Schumann, Felix Mendelssohn, Franz Liszt, Giuseppe Verdi, Richard Wagner, Johannes Brahms, and Johann Strauss II.\\r\\nProminent composers of the early 20th century include Igor Stravinsky, Claude Debussy, Dmitri Shostakovich, Sergei Rachmaninoff, Sergei Prokofiev, Arnold Schoenberg, Anton Webern, Alban Berg, Aram Khachaturian, George Gershwin, Edvard Grieg, and Bla Bart܇k. Although composers of this time can be considered Romantic, usually their works are not in the style of the period.[clarification needed]\\r\\nEncompassing a wide variety of post-Romantic styles composed through the year 2000, 20th-century classical music includes late romantic, impressionist, neoclassical, neoromantic, neomedieval, and postmodern styles of composition. Modernism (1890ÿ1930) marked an era when many composers rejected certain values of the common practice period, such as traditional tonality, melody, instrumentation, and structure. The high-modern era saw the emergence of neo-classical and serial music. A few authorities have claimed high-modernism as the beginning of postmodern music from about 1930.[59][not in citation given][60][not in citation given] Others have more or less equated postmodern music with the \\"contemporary music\\" composed from the late 20th century through to the early 21st century.[61][62]\\r\\nAlmost all of the composers who are described in music textbooks on classical music and whose works are widely performed as part of the standard concert repertoire are male composers, even though there has been a large number of women composers throughout the classical music period. Musicologist Marcia Citron has asked \\"[w]hy is music composed by women so marginal to the standard 'classical' repertoire?\\"[63] Citron \\"examines the practices and attitudes that have led to the exclusion of women composers from the received 'canon' of performed musical works.\\" She argues that in the 1800s, women composers typically wrote art songs for performance in small recitals rather than symphonies intended for performance with an orchestra in a large hall, with the latter works being seen as the most important genre for composers; since women composers did not write many symphonies, they were deemed to be not notable as composers.[63] In the \\"...Concise Oxford History of Music, Clara S[c]humann is one of the only [sic] female composers mentioned.\\"[64] Abbey Philips states that \\"[d]uring the 20th century the women who were composing/playing gained far less attention than their male counterparts.\\"[64]\\r\\nWhile there are differences between particular performances of a classical work, a piece of classical music is generally held to transcend any interpretation of it. The use of musical notation is an effective method for transmitting classical music, since the written music contains the technical instructions for performing the work.[citation needed]\\r\\nThe written score, however, does not usually contain explicit instructions as to how to interpret the piece in terms of production or performance, apart from directions for dynamics, tempo and expression (to a certain extent). This is left to the discretion of the performers, who are guided by their personal experience and musical education, their knowledge of the work's idiom, their personal artistic tastes, and the accumulated body of historic performance practices.[citation needed]\\r\\nAll critics express the opinion that it is only from the mid-19th century, and especially in the 20th century, that the score began to hold such a high significance.[citation needed] Previously, improvisation (in preludes, cadenzas and ornaments), rhythmic flexibility (e.g., tempo rubato), improvisatory deviation from the score and oral tradition of playing was integral to the style.[clarification needed] Classical musicians tend to use scores and the parts extracted from them to play music. Yet, even with notation providing the key elements of the music, there is considerable latitude in the performance of the works. Some of this latitude results from the inherent limitations of musical notation, though attempts to supplement traditional notation with signs and annotations indicating more subtle nuances tend to overwhelm and paralyse the performer.\\r\\nSome quotes that highlight a criticism of overvaluing of the score:\\r\\nImprovisation once played an important role in classical music. A remnant of this improvisatory tradition in classical music can be heard in the cadenza, a passage found mostly in concertos and solo works, designed to allow skilled performers to exhibit their virtuoso skills on the instrument. Traditionally this was improvised by the performer; however, it is often written for (or occasionally by) the performer beforehand. Improvisation is also an important aspect in authentic performances of operas of Baroque era and of bel canto (especially operas of Vincenzo Bellini), and is best exemplified by the da capo aria, a form by which famous singers typically perform variations of the thematic matter of the aria in the recapitulation section ('B section' / the 'da capo' part). An example is Beverly Sills' complex, albeit pre-written, variation of \\"Da tempeste il legno infranto\\" from H?ndel's Giulio Cesare.[citation needed]\\r\\nIts[clarification needed] written transmission, along with the veneration bestowed on certain classical works, has led to the expectation that performers will play a work in a way that realizes in detail the original intentions of the composer. During the 19th century the details that composers put in their scores generally increased. Yet the opposite trendadmiration of performers for new \\"interpretations\\" of the composer's workcan be seen, and it is not unknown for a composer to praise a performer for achieving a better realization of the original intent than the composer was able to imagine. Thus, classical performers often achieve high reputations for their musicianship, even if they do not compose themselves. Generally however, it is the composers who are remembered more than the performers.\\r\\nThe primacy of the composer's written score has also led, today, to a relatively minor role played by improvisation in classical music, in sharp contrast to the practice of musicians who lived during the medieval, renaissance, baroque and early romantic eras. Improvisation in classical music performance was common during both the Baroque and early romantic eras, yet lessened strongly during the second half of the 20th century. During the classical era, Mozart and Beethoven often improvised the cadenzas to their piano concertos (and thereby encouraged others to do so), but for violin concertos they provided written cadenzas for use by other soloists.[citation needed] In opera, the practice of singing strictly by the score, i.e. come scritto, was famously propagated by soprano Maria Callas, who called this practice 'straitjacketing' and implied that it allows the intention of the composer to be understood better, especially during studying the music for the first time.[66]\\r\\nClassical music has often incorporated elements or material from popular music of the composer's time. Examples include occasional music such as Brahms' use of student drinking songs in his Academic Festival Overture, genres exemplified by Kurt Weill's The Threepenny Opera, and the influence of jazz on early and mid-20th-century composers including Maurice Ravel, exemplified by the movement entitled \\"Blues\\" in his sonata for violin and piano.[67] Certain postmodern, minimalist and postminimalist classical composers acknowledge a debt to popular music.[68]\\r\\nNumerous examples show influence in the opposite direction, including popular songs based on classical music, the use to which Pachelbel's Canon has been put since the 1970s, and the musical crossover phenomenon, where classical musicians have achieved success in the popular music arena.[69] In heavy metal, a number of lead guitarists (playing electric guitar) modeled their playing styles on Baroque or Classical era instrumental music, including Ritchie Blackmore and Randy Rhoads.\\r\\nComposers of classical music have often made use of folk music (music created by musicians who are commonly not classically trained, often from a purely oral tradition). Some composers, like Dvo?k and Smetana,[70] have used folk themes to impart a nationalist flavor to their work, while others like Bart܇k have used specific themes lifted whole from their folk-music origins.[71]\\r\\nCertain staples of classical music are often used commercially (either in advertising or in movie soundtracks). In television commercials, several passages have become clichd, particularly the opening of Richard Strauss' Also sprach Zarathustra (made famous in the film 2001: A Space Odyssey) and the opening section \\"O Fortuna\\" of Carl Orff's Carmina Burana; other examples include the \\"Dies irae\\" from the Verdi Requiem, Edvard Grieg's \\"In the Hall of the Mountain King\\" from Peer Gynt, the opening bars of Beethoven's Symphony No. 5, Wagner's \\"Ride of the Valkyries\\" from Die Walkre, Rimsky-Korsakov's \\"Flight of the Bumblebee\\", and excerpts of Aaron Copland's Rodeo.[citation needed] Several works from the Golden Age of Animation matched the action to classical music. Notable examples are Walt Disney's Fantasia, Tom and Jerry's Johann Mouse, and Warner Bros.' Rabbit of Seville and What's Opera, Doc?\\r\\nSimilarly, movies and television often revert to standard, clichd excerpts of classical music to convey refinement or opulence: some of the most-often heard pieces in this category include Bachs Cello Suite No. 1, Mozart's Eine kleine Nachtmusik, Vivaldi's Four Seasons, Mussorgsky's Night on Bald Mountain (as orchestrated by Rimsky-Korsakov), and Rossini's \\"William Tell Overture\\". The same passages are often used by telephone call centres to induce a sense of calm in customers waiting in a queue.[citation needed] Shawn Vancour argues that the commercialization of classical music in the early 20th century may have harmed the music industry through inadequate representation.[72]\\r\\nSince the range of production of classical music is from the 14th century to 21st century, most of this music (14th to early 20th century) belongs to the public domain, mainly sheet music and tablatures. Some projects like Musopen and Open Goldberg Variations were created to produce musical audio files of high quality and release them into the public domain, most of them are available at the Internet Archive website.\\r\\nThe Open Goldberg Variations project released a braille format into the public domain that can be used to produce paper or electronic scores, Braille e-books, for blind people.[73]\\r\\nDuring the 1990s, several research papers and popular books wrote on what came to be called the \\"Mozart effect\\": an observed temporary, small elevation of scores on certain tests as a result of listening to Mozart's works. The approach has been popularized in a book by Don Campbell, and is based on an experiment published in Nature suggesting that listening to Mozart temporarily boosted students' IQ by 8 to 9 points.[74] This popularized version of the theory was expressed succinctly by the New York Times music columnist Alex Ross: \\"researchers... have determined that listening to Mozart actually makes you smarter.\\"[75] Promoters marketed CDs claimed to induce the effect. Florida passed a law requiring toddlers in state-run schools to listen to classical music every day, and in 1998 the governor of Georgia budgeted $105,000 per year to provide every child born in Georgia with a tape or CD of classical music. One of the co-authors of the original studies of the Mozart effect commented \\"I don't think it can hurt. I'm all for exposing children to wonderful cultural experiences. But I do think the money could be better spent on music education programs.\\"[76]\\r\\nIn 1996/97, a research study was conducted on a large population of middle age students in the Cherry Creek School District in Denver, Colorado, US. The study showed that students who actively listen to classical music before studying had higher academic scores. The research further indicated that students who listened to the music prior to an examination also had positively elevated achievement scores. Students who listened to rock-and-roll or Country music had moderately lower scores. The study further indicated that students who used classical music during the course of study had a significant leap in their academic performance; whereas, those who listened to other types of music had significantly lowered academic scores. The research was conducted over several schools within the Cherry Creek School District and was conducted through the University of Colorado. This study is reflective of several recent studies (i.e. Mike Manthei and Steve N. Kelly of the University of Nebraska at Omaha; Donald A. Hodges and Debra S. O'Connell of the University of North Carolina at Greensboro; etc.) and others[full citation needed] who had significant results through the discourse of their work.[77]\\r\\nNation-specific:","input":"What kind of music would most likely be performed by a symphony orchestra?"},{"output":"bicycle shop","context":"","input":"What kind of business did the wright brothers own before their flight?"},{"output":"1 June 1977","context":"\\r\\n\\r\\nThe New South Wales Anti-Discrimination Act 1977 is an Act of the NSW Parliament, relating to discrimination in employment, the public education system, delivery of goods and services, and other services such as banking, health care, property and night clubs. \\r\\n\\r\\nThe Act prohibits unlawful racial, sexual and other types of discrimination in certain circumstances and promotes equality of opportunity for all people\\r\\n\\r\\nThe Act covers the following types of discrimination: \\r\\n\\r\\nThe Act was granted Royal Assent on 28 April 1977 and came into effect on 1 June 1977. It was the 48th Act of 1977. Since then the Act has been amended and reformed about 90 times.[2][3]\\r\\n\\r\\nThe Anti-Discrimination Board of NSW was set up under the NSW Anti-Discrimination Act 1977 to promote anti-discrimination and equal opportunity principles and policies throughout NSW and to administer the Act.\\r\\n\\r\\nThe Anti-Discrimination Board of NSW handles complaints of discrimination made by members of the public, investigating and conciliating complaints when appropriate.\\r\\n\\r\\nIt also works to prevent discrimination by means of consultations, education programs, seminars, presentations, media and community engagement, and informational materials.\\r\\n\\r\\nIn addition, the Board advises the Government on discrimination matters and makes recommendations to the Attorney General on some applications for exemption from the Act.[4]\\r\\n\\r\\nThe Office of Director of Equal Opportunity in Public Employment (ODEOPE) administrates Part 9A of the Act, which pertains to Equal Employment Opportunity (EEO) across the public sector.\\r\\n\\r\\nIn June 2018, both houses of the Parliament of New South Wales unanimously passed and the Governor of New South Wales signed an urgent bill without amendments called the Crimes Amendment (Publicly Threatening and Inciting Violence) Bill 2018[5] to repeal the 1989 vilification laws within the Anti-Discrimination Act 1977 and replace it with criminal legislation with up to an explicit 3 year term of imprisonment within this Act.[6][7] The legislation went into effect on August 13, 2018 - by proclamation on August 10, 2018.[8]","input":"When was the anti discrimination act introduced in australia?"},{"output":"the Molotov Plan","context":"The Molotov Plan was the system created by the Soviet Union in 1947 in order to provide aid to rebuild the countries in Eastern Europe that were politically and economically aligned to the Soviet Union. It can be seen to be the Soviet Union's version of the Marshall Plan, which for political reasons the Eastern European countries would not be able to join without leaving the Soviet sphere of influence. Soviet foreign minister Vyacheslav Molotov rejected the Marshall Plan (1947), proposing instead the Molotov Planthe Soviet-sponsored economic grouping which was eventually expanded to become the Comecon.[1]\\r\\n\\r\\nThe Molotov Plan was symbolic of the Soviet Union's refusal to accept aid from the Marshall Plan, or allow any of their satellite states to do so because of their belief that the Marshall Plan was an attempt to weaken Soviet interest in their satellite states through the conditions imposed and by making beneficiary countries economically dependent on the United States. The plan was a system of bilateral trade agreements which also established Comecon to create an economic alliance of socialist countries.[2] This aid allowed countries in Europe to stop relying on American aid and therefore allowed Molotov Plan states to reorganize their trade to the Soviet Union instead.[3] The plan was in some ways contradictory because while the Soviets were giving aid to Eastern Bloc countries, at the same time they were demanding that countries who were members of the Axis powers pay reparations to the Soviet Union.","input":"What was the soviet union's response to the marshall plan?"},{"output":"Claude Debussy and Maurice Ravel","context":"Impressionism in music was a movement among various composers in Western classical music (mainly during the late 19th and early 20th centuries) whose music focuses on suggestion and atmosphere, \\"conveying the moods and emotions aroused by the subject rather than a detailed tonepicture\\".[1] \\"Impressionism\\" is a philosophical and aesthetic term borrowed from late 19th century French painting after Monet's Impression, Sunrise. Composers were labeled impressionists by analogy to the impressionist painters who use starkly contrasting colors, effect of light on an object, blurry foreground and background, flattening perspective, etc. to make the observer focus his attention on the overall impression.[2]\\r\\nThe most prominent in musical impressionism is the use of \\"color\\", or in musical term, timbre, which can be achieved through orchestration, harmonic usage, texture, etc.[3] Other elements of music impressionism also involve new chord combinations, ambiguous tonality, extended harmonies, use of modes and exotic scales, parallel motions, extra-musicality, and evocative titles such as Reflets dans l'eau (Reflections on the water, 1905), Brouillards (Mists, 1913) etc.[2]\\r\\n\\r\\n\\r\\nClaude Debussy and Maurice Ravel are two leading figures in impressionism, though Debussy rejected this label (he mentioned in his letter that \\"imbeciles call 'impressionism', a term employed with the utmost inaccuracy\\") and Ravel displayed discomfort with it, at one point claiming that it could not be adequately applied to music at all.[4][5] Debussy's impressionist works typically \\"evoke a mood, feeling, atmosphere, or scene\\" by creating musical images through characteristic motifs, harmony, exotic scales (e.g., whole-tone and pentatonic scales), instrumental timbre, large unresolved chords (e.g., 9ths, 11ths, 13ths), parallel motion, ambiguous tonality, extreme chromaticism, heavy use of the piano pedals, and other elements.[2] Some impressionist composers, Debussy and Ravel in particular, are also labeled as symbolist composers. One trait shared with both aesthetic trends is \\"a sense of detached observation: rather than expressing deeply felt emotion or telling a story,\\" as in symbolist poetry, the normal syntax is usually disrupted and individual images that carry the work's meaning are evoked.[2]\\r\\nSome of the key forerunners of and influences on the impressionist style include Modest Mussorgsky, Emmanuel Chabrier, Edvard Grieg, Gabriel Faur, Henri Duparc, and Ernest Chausson. Elements of impressionism can also be traced back to works by Frdric Chopin, Franz Liszt, and Richard Wagner.[6][not in citation given]\\r\\nErnest Fanelli was claimed to have innovated the style in the early 1880s, though his works were unperformed before 1912. The performance of his works in that year led to claims that he was the father of musical Impressionism. Ravel wrote, \\"this impressionism is certainly very different from that of present-day composers...Mr. Fanelli's impressionism derives more directly from Berlioz.\\" He added that Fanelli's alleged priority does not in any way diminish the achievements of later composers: \\"the investigations of the young Fanelli could not have diminished those of his colleagues...It is peculiar that these investigations suddenly assume importance because their embryo is discovered in a work written 30 years ago.\\"[7]\\r\\nOther composers linked to impressionism include Leo? Jan?ek, Isaac Albniz, Frederick Delius, Paul Dukas, Erik Satie, Enrique Granados, Alexander Scriabin, Manuel de Falla, John Alden Carpenter, Joaqun Turina, Ottorino Respighi, Charles Tomlinson Griffes, and Federico Mompou.[8] The Finnish composer Jean Sibelius is also associated with impressionism, and his The Swan of Tuonela (1893) predates Debussy's Prlude  l'aprs-midi d'un faune (regarded as a seminal work of musical impressionism) by a year. The American composer Howard Hanson also borrowed from both Sibelius and impressionism generally in works such as his Second Symphony.[9]\\r\\nIn addition to its enduring influence on the work of later composers in the classical tradition, such as Alan Hovhaness, Gy?rgy Ligeti, and Toru Takemitsu, impressionism has also exerted a notable influence on certain jazz musicians, especially Bix Beiderbecke, Duke Ellington, Claude Thornhill, Dave Brubeck, Gil Evans, Bill Evans, Herbie Hancock, Frank Kimbrough, and others.[citation needed]","input":"Who was the leading composer of impressionist music?"},{"output":"November 21, 1992","context":"\\"The Thanksgiving Song\\" (also known as \\"Happy Thanksgiving\\"[1]) is a song performed by Adam Sandler discussing Thanksgiving.[2] The song was written by Sandler, Ian Maxtone-Graham and Robert Smigel.[3]\\r\\n\\r\\n\\r\\nIt was first performed during the Weekend Update segment of the season 18 episode of Saturday Night Live on November 21, 1992 as a duet between Sandler and Weekend Update anchor Kevin Nealon; it was originally intended to be a recurring Thanksgiving tradition with other cast members debuting their own original songs, but the next year, it was again Sandler, doing another version of the same song in the style of Bruce Springsteen. A live performance of the original song was recorded at The Strand in Redondo Beach, California on July 25, 1993.[3] The version at The Strand was featured on Sandler's debut album They're All Gonna Laugh at You! and was released as a single.\\r\\nThe song, which Sandler sings in a childlike, semi-falsetto voice, primarily revolves around the repetition of the word \\"turkey\\" in various two-line rhymes, many of which are non sequiturs; for instance, \\"Turkey with gravy and cranberries/Can't believe the Mets traded Darryl Strawberry!\\"\\r\\nCelebrity and pop culture references include:\\r\\nThe song is a predecessor to Sandler's more popular holiday song, \\"The Chanukah Song\\" (also co-written by Maxtone-Graham and Sandler). When \\"The Chanukah Song\\" became a hit on the rock charts (and a minor hit on the Hot 100) in 1996, \\"The Thanksgiving Song\\" was released as a follow-up the next year; it too became a hit, charting at #40 on the Adult Top 40 chart and #29 on the Mainstream Rock Tracks chart in 1997.[4] The song continues to receive extensive radio airplay around Thanksgiving, on various formats, both with other holiday-themed songs and without.[5] Various radio cuts remove or edit one verse which states \\"my brother likes to masturbate with baby oil\\" but it is otherwise identical to the live performance at The Strand. The edits also leave in a portion of the performance where Sandler has to stop in the middle of a song due to becoming distracted with the audience's rhythmic clapping.","input":"What year did adam sandler sing the thanksgiving song?"},{"output":"January 14, 1984","context":"Raymond Albert \\"Ray\\" Kroc (October 5, 1902?ÿ January 14, 1984) was an American businessman.[4][5] He joined the Californian chain McDonald's in 1954 and built it into a nationwide and eventually global franchise, making it the most successful fast food corporation in the world. Controversially, Kroc would present himself as the founder of McDonald's during his later life.[6] Kroc was included in Time 100: The Most Important People of the Century, and amassed a fortune during his lifetime. He owned the San Diego Padres baseball team from 1974 until his death in 1984.\\r\\n\\r\\n\\r\\nKroc was born on October 5, 1902 in Oak Park, Illinois, near Chicago, to parents of Czech origin, Rose Mary (Hrach) and Alois \\"Louis\\" Kroc.[7] His father was from the village of B?asy near Plze, Bohemia (now the Czech Republic). The elder Kroc had made a fortune speculating on land during the 1920s, only to lose everything with the stock market crash in 1929.[8]\\r\\nHe grew up and spent most of his life in Oak Park. During World War I, he lied about his age and became a Red Cross ambulance driver at 15. The war, however, ended shortly after. During the Depression, Kroc worked a variety of jobs selling paper cups, as a real estate agent in Florida, and sometimes playing piano in bands.[9]\\r\\nAfter World War II, Kroc found employment as a milk shake mixer salesman for the foodservice equipment manufacturer Prince Castle.[10] When Prince Castle Multi-Mixer sales plummeted because of competition from lower-priced Hamilton Beach products, Kroc was impressed by Richard and Maurice McDonald who had purchased eight of his Multi-Mixers for their San Bernardino, California store, and visited them in 1954. Kroc became convinced that the concept and design of this small chain had the potential to expand across the nation.[citation needed]\\r\\nHaving been in approximately one thousand kitchens, Kroc believed the McDonald brothers had the best-run operation he had ever seen. The restaurant was clean, modern, mechanized, and the staff professional and well-groomed. Roadside hamburger restaurants were more often than not hangouts for motorcycle gangs and rebellious teenagers, and Kroc saw in McDonald's a better vision for a restaurant. He once said \\"In my experience, hamburger joints are nothing but jukeboxes, pay phones, smoking rooms, and guys in leather jackets. I wouldn't take my wife to such a place and you wouldn't take your wife either.\\" Kroc also believed the hamburger was more aesthetic than the hot dog and lent itself better to the mechanized fast food industry he envisioned.[citation needed]\\r\\nKroc opened the first McDonald's franchised under his partnership with the McDonald brothers in Des Plaines, Illinois.\\r\\nAfter finalizing the franchise agreement with the McDonald brothers, Kroc sent a letter to Walt Disney. They had met as ambulance attendant trainees at Sound Beach, Connecticut during World War I. Kroc wrote, \\"I have very recently taken over the national franchise of the McDonald's system. I would like to inquire if there may be an opportunity for a McDonald's in your Disney Development\\". According to one account, Disney agreed under stipulation to increase fries from ten cents to fifteen cents, allowing himself the profit. Kroc refused to gouge his loyal customers, leaving Disneyland to open without a McDonald's restaurant. Journalist Eric Schlosser, writing in his book Fast Food Nation, believes that this is a doctored retelling of the transaction by some McDonald's marketing executives. Most probably, the proposal was returned without approval.[11]\\r\\nKroc has been credited with making a number of innovative changes in the food-service franchise model. Chief among them was the sale of only single-store franchises instead of selling larger, territorial franchises which was common in the industry at the time. Kroc recognized that the sale of exclusive licenses for large markets was the quickest way for a franchisor to make money, but he also saw in the practice a loss in the franchisor's ability to exert control over the course and direction of a chains development. Above all else, Kroc wanted uniformity in service and quality among all of the McDonalds locations. Without the ability to influence franchisees, Kroc knew that it would be difficult to achieve that goal. By granting a franchisee the right to only one store location at a time, Kroc retained for the franchise some measure of control over the franchisee (or at least those desiring to someday own the rights to another store).[12]\\r\\nKroc's policies for McDonald's included establishing locations only in suburban areas, not in downtowns since the seedy underbelly of urban areas might eat in them after the main business hours were over. Restaurants were to be kept properly sanitized at all times, and the staff must be clean, properly groomed and polite to children. The food was to be of a strictly fixed, standardized content and restaurants were not allowed to deviate from specifications in any way. There was to be no waste of anything, Kroc insisted; every condiment container was to be scraped completely clean. No cigarette machines or pinball games were allowed in any McDonald's. Kroc also opposed hiring women as employees because he believed they were unreliable workers and liable to flirt with male customers as they did in carhop restaurants.[citation needed]\\r\\nKroc had difficulty enforcing his strict policies in the beginning as several California franchisees began offering things that were not supposed to be on the menu, altering prices, the recipes, or committing various other offenses. For a time, Kroc held off on licensing more McDonald's in California, preferring to concentrate on the Midwest, where he believed people were more conservative and less likely to challenge authority.[citation needed]\\r\\nKroc had a contemptuous opinion of MBAs and people who attended business school or obtained college degrees in management, believing they lacked competitive drive or market savvy. For a time, McDonald's had a policy of not hiring MBAs. He also forbade McDonald's executives to have secretaries and required them to answer their own phones. They were expected to follow dress and grooming rules similar to those of rank-and-file employees in the restaurants, which included no beards or facial hair, and they received regular company pamphlets extolling thriftiness and financial responsibility both at the company and in their personal lives.[citation needed]\\r\\nDuring the 1960s, a wave of new fast food chains appeared which copied McDonald's model, including Burger King, Burger Chef, Arbys, KFC, and Hardee's. Kroc spoke of the competition with contempt, saying that they did not offer the same quality of food, service, affordable prices, or sanitation as McDonald's. He resisted joining any fast food trade organizations for fear of giving away his business secrets.[citation needed]\\r\\nKroc became frustrated with the McDonald brothers' desire to maintain a small number of restaurants. The brothers also consistently told Kroc he could not make changes to things such as the original blueprint (building codes were different in Illinois than in California), but despite Kroc's pleas, the brothers never sent any formal letters which legally allowed the changes in the chain. In 1961, he bought the company for $2.7 millionenough to pay each brother $1 million after taxes. Obtaining the funds for the buyout was difficult due to existing debt from expansion. However, Harry Sonneborn, whom Kroc referred to as his \\"financial wizard\\", was able to raise the required funds.[citation needed]\\r\\nAt the closing table, Kroc became annoyed that the brothers would not transfer to him the real estate and rights to the original San Bernardino location. The brothers had told Kroc they were giving the operation, property and all, to the founding employees. In his anger, Kroc later opened a new McDonald's restaurant near the original McDonald's, which had been renamed \\"The Big M\\" because the brothers had neglected to retain rights to the name. \\"The Big M\\" closed six years later.[13] It is alleged that as part of the buyout Kroc promised, based on a handshake agreement, to continue the annual 0.5% royalty of the original agreement, but there is no evidence of this beyond a claim by a nephew of the McDonald brothers. Neither of the brothers publicly expressed disappointment over the deal. Speaking to someone about the buyout, Richard McDonald reportedly said that he had no regrets.[14]\\r\\nKroc maintained the assembly line \\"Speedee Service System\\" for hamburger preparation, which was introduced by the McDonald brothers in 1948. He standardized operations, ensuring every burger would taste the same in every restaurant. He set strict rules for franchisees on how the food was to be made, portion sizes, cooking methods and times, and packaging. Kroc also rejected cost-cutting measures like using soybean filler in the hamburger patties. These strict rules also were applied to customer service standards with such mandates that money be refunded to clients whose orders were not correct or to customers who had to wait more than five minutes for their food.\\r\\nBy the time of Kroc's death, the chain had 7,500 outlets in the United States and 31 other countries and territories. The total system-wide sales of its restaurants were more than $8 billion in 1983, and his personal fortune amounted to some $600 million.[4]\\r\\nKroc retired from running McDonald's in 1974. While he was looking for new challenges, he decided to get back into baseball, his lifelong favorite sport, when he learned that the San Diego Padres were for sale. The team had been conditionally sold to Joseph Danzansky, a Washington, D.C. grocery-chain owner, who planned to move the Padres to Washington.[15] However, the sale was tied up in lawsuits when Kroc purchased the team for $12 million, keeping the team in San Diego.[16][17] In Kroc's first year of ownership in 1974, the Padres lost 102 games, yet drew over one million in attendance, the standard of box office success in the major leagues during that era. Their previous top attendance was 644,772 in 1972.[16] The San Diego Union said Kroc was \\"above all, a fan of his team\\".[17] On April 9, 1974, while the Padres were on the brink of losing a 9ÿ5 decision to the Houston Astros in the season opener at San Diego Stadium, Kroc took the public address microphone in front of 39,083 fans. \\"Ive never seen such stupid ballplaying in my life,\\" he said. The crowd cheered in approval.[17][18] In 1979, Kroc's public interest in future free agent players Graig Nettles and Joe Morgan drew a $100,000 fine from Commissioner Bowie Kuhn. Frustrated with the team, he handed over operations of the team to his son-in-law, Ballard Smith. \\"There's more future in hamburgers than baseball,\\" Kroc said.[19]\\r\\nAfter his death, the Padres in 1984 wore a special patch with Kroc's initials, RAK.[20] They would win the NL pennant that year and play in the 1984 World Series. Kroc was inducted posthumously as part of the inaugural class of the San Diego Padres Hall of Fame in 1999.[21]\\r\\nKroc's foundation supported research and treatment of alcoholism, diabetes, and other diseases. He established the Ronald McDonald House foundation. He was a major donor to the Dartmouth Medical School.[4]\\r\\nA lifelong Republican, Kroc believed firmly in self-reliance and staunchly opposed government welfare and the New Deal. He generated significant controversy for donating $255,000 to Richard Nixon's reelection campaign in 1972; many observers, notably Senator Harrison Williams, accused Kroc of making the donation to influence Nixon to veto a minimum wage bill making its way through Congress.[22]\\r\\nIn 1980, Kroc suffered a stroke and entered a rehabilitation facility for his alcoholism.[23] He died of heart failure at a hospital in San Diego, California, on January 14, 1984, at the age of 81,[4] and was buried at the El Camino Memorial Park in Sorrento Valley, San Diego.[8]\\r\\nKroc's third wife, Joan, was a philanthropist. Among many other things, she enabled the founding of The Salvation Army's Ray & Joan Kroc Corps Community Centers. His earlier marriages, to Ethel Fleming (1922ÿ1961) and Jane Dobbins Green (1963ÿ1968), ended in divorce.[2][24] Joan died on October 12, 2003.\\r\\nKroc's acquisition of the McDonald's franchise as well as his \\"Kroc-style\\" business tactics are the subject of Mark Knopfler's 2004 song \\"Boom, Like That.\\"[25][26]\\r\\nKroc co-authored the book Grinding it Out released in 1977. It received positive reviews including from critic Ryan Stewman: \\"Salesman to salesperson, get the book and read it. You should be inspired by it. Ray was one of us who made it!\\"[27]\\r\\nKroc is portrayed by Michael Keaton in the 2016 John Lee Hancock film The Founder. The movie depicts Kroc's franchise development, nationwide expansion, and ultimate acquisition of McDonald's, while being critical of his treatment of the founding McDonald's brothers.[28]","input":"When did the owner of mcdonald's die?"},{"output":"Each figure represents a different character: a queen, which he claims was his first experiment, a king which is madly in love with the Queen and has a resemblance to Henry VIII, an archbishop which disapproves of what the King is doing, the Devil (a man in a black suit), a ballerina dancer, a mermaid (grown from \\"an experiment with seaweed\\") which lives in water","context":"Septimus Pretorius is a fictional character who appears in the Universal film Bride of Frankenstein (1935). He is played by British stage and film actor Ernest Thesiger.[1] Some sources claim he was originally to have been played by Bela Lugosi or Claude Rains.[2] Others indicate that the part was conceived specifically for Thesiger.[3]\\r\\n\\r\\n\\r\\nDoctor Pretorius is a renegade mad scientist who persuades Henry Frankenstein to resume his experiments with bringing dead flesh to life. An amoral egomaniac, he has no regard for human life or ethics and cares only for his own prestige as a scientist.\\r\\nAlong with his sinister qualities, Pretorius is responsible for a large share of the film's dark humor. He eats a picnic dinner in a crypt, trades prissy banter with the Monster and laments that the tiny ballerina he created \\"will only dance to Mendelssohn's 'Spring Song.'\\" He claims that gin is his only weakness. Then later in the film, he claims his cigars are his only weakness when he first meets the Monster. Pretorius also delivers the famous toast \\"To a new world of gods and monsters!\\" midway through the film.\\r\\nSeptimus Pretorius, Frankenstein's former teacher, is a tall, emaciated-looking man with an unusually large nose and devilish pointed ears. A professor of philosophy, Pretorius first points young Henry on the path toward his unwholesome experiments in giving life to the dead. He himself is \\"booted out\\" from his teaching post \\"for knowing too much.\\" Pretorius seeks out his former student after learning that the Monster has survived being trapped in the burning windmill in the climax of the first film. Pretorius himself acknowledges that he may be insane in a conversation with Frankenstein (\\"You think I'm mad? Perhaps I am!\\").\\r\\nPretorius performs experiments creating life similar to Henry's. He unveils to Henry a group of various homunculiminiature living humans which he has kept in bottles, and claims to have grown from \\"seed\\" like cultures. Each figure represents a different character: a queen, which he claims was his first experiment, a king which is madly in love with the Queen and has a resemblance to Henry VIII, an archbishop which disapproves of what the King is doing, the Devil (a man in a black suit), a ballerina dancer, a mermaid (grown from \\"an experiment with seaweed\\") which lives in water. Pretorious gleefully compares his own visage to that of the Devil: \\"There's a certain resemblance to me, don't you think? Or do I flatter myself?\\" He says sometimes he thinks life would be more interesting if they were all devils. He has been unsuccessful, however, in creating a full-sized human. He proposes to Henry that together they create a mate for his monster, with Henry building the body and Pretorius supplying an artificially-grown brain. Henry initially balks at the idea, but Pretorius reminds him that he is capable of exposing him to the authorities as the creator of the Monster who has done so much damage. Later, he meets the Monster in a crypt where he has gone to steal bodies and is dining using the top of a coffin as a picnic table. When the Monster asks him 'Friend?' he gives him the remains of his chicken. He tells the Monster of his plans to create a mate for him.\\r\\nThe Monster, eager for companionship of any kind, considers Pretorius his friend, and from then on is willing to do anything that the impish scientist desires, such as kidnapping Henry's wife, Elizabeth, in order to force him to help Pretorius. Henry agrees, and together the two scientists create the Bride of Frankenstein. Unfortunately, even the Bride finds her would-be husband repulsive, and the heartbroken Monster decides to end his life by blowing up the laboratory. He instructs Henry and Elizabeth to run but barks at Pretorius to stay, saying that they \\"belong dead\\". Before Pretorius can move, the Monster blows up the laboratory and the castle. Frankenstein and Elizabeth escape but Pretorius's fate is not revealed. The Monster, however, returned in the next sequel.\\r\\nDr. Pretorius is frequently identified as homosexual, or as near to homosexual as could be presented on-screen in 1935. (There is, of course, no direct reference to homosexuality in the film.) Bride of Frankenstein director James Whale was openly gay and frequently included camp elements in his films. He directed Thesiger (himself identified in some sources as more or less openly gay)[4] to play the part as an \\"over the top caricature of a bitchy and aging homosexual.\\"[3] Gay film historian Vito Russo stops short of identifying Pretorius as gay, calling him instead \\"sissified\\"[5] (\\"sissy\\" itself being Hollywood code for \\"homosexual\\"). Pretorius serves as a figure of seduction and temptation, pulling Frankenstein away from his bride on their wedding night to engage in the unnatural act of non-procreative life. The Breen Office, responsible for enforcing Hollywood's Production Code, let Pretorius' behavior pass unchallenged. A novelisation of the film published in England made the implication even more clear, having Pretorius say to Frankenstein, \\"'Be fruitful and multiply.' Let us obey the Biblical injunction: you of course, have the choice of natural means; but as for me, I am afraid that there is no course open to me but the scientific way.\\"[6]\\r\\nPretorius appears in Kim Newman's crossover novel Dracula Cha Cha Cha, as a colleague of H. P. Lovecraft's Herbert West. He appears as one of the horror movie spoofs in Jesus Christ Vampire Hunter and also appears in Allan Rune Pettersson's novel Frankenstein's Aunt Returns.\\r\\nIn the Amicus horror film The House That Dripped Blood (1970), Geoffrey Bayldon, playing a mysterious antique shop owner, is made up and costumed to look like Ernest Thesiger as Pretorius.[citation needed]\\r\\nIn the 1973 NBC-TV miniseries Frankenstein: The True Story, James Mason portrays a Pretorius-like figure named Dr. Polidori (named for Lord Byron's real-life physician, John William Polidori, who was part of the 1816 gathering that produced Mary Shelley's novel and also the author of the story \\"The Vampyre\\"). Mason's Dr. Polidori enlists Victor to assist in creating the female creature Prima (Jane Seymour).\\r\\nThe 1986 film From Beyond adds a Dr. Edward Pretorius at Miskatonic University (played by Ted Sorel) as a dark mentor for Crawford Tillinghast (Jeffrey Combs). This Pretorius is an impotent sadist (with a room full of bondage gear) who is assimilated by \\"the Beyond\\" and attempts to drag others into it, boasting that the pineal gland growth brought on by the resonator is like \\"an orgasm of the mind\\".\\r\\nThe 1989 Akif Pirin?ci novel Felidae and its 1994 animated feature film adaptation feature an insane Professor Preterius in the backstory performing increasingly dark experiments on the cat Claudandus.\\r\\nIn the 1998 independent film Gods and Monsters, directed by Bill Condon, Ernest Thesiger as Pretorius is played by Arthur Dignam in a flashback about the filming of Bride of Frankenstein. An earlier scene shows Brendan Fraser as Whale's gardener watching Bride of Frankenstein on television, specifically the climatic scene of Pretorius and Frankenstein unveiling the Bride.\\r\\nIn the 2007 Frankenstein TV movie, the character of Professor Jane Pretorius is based on Septimus Pretorius and served as Victoria Frankenstein's boss.\\r\\nIn the Dark Horse novel Universal's Monsters series \\"The Bride Of Frankenstein: Pandora's Bride,\\" Doctor Septimus Pretorius somehow survives the blowing up of the tower lab along with the Bride and they escape to Germany where he teaches her and she becomes her own woman.\\r\\nIn Mary Shelley's Frankenhole, the character Professor Sanguinaire Polidori (voiced by Scott Adsit) is based on Septimus Pretorius.","input":"What is the significance of pretorius's miniatures?"},{"output":"The Creeper virus","context":"A computer virus is a type of malicious software program (\\"malware\\") that, when executed, replicates itself by modifying other computer programs and inserting its own code.[1] Infected computer programs can include, as well, data files, or the \\"boot\\" sector of the hard drive. When this replication succeeds, the affected areas are then said to be \\"infected\\" with a computer virus.[2][3][4]\\r\\nVirus writers use social engineering deceptions and exploit detailed knowledge of security vulnerabilities to initially infect systems and to spread the virus. The vast majority of viruses target systems running Microsoft Windows,[5][6][7] employing a variety of mechanisms to infect new hosts,[8] and often using complex anti-detection/stealth strategies to evade antivirus software.[9][10][11][12] Motives for creating viruses can include seeking profit (e.g., with ransomware), desire to send a political message, personal amusement, to demonstrate that a vulnerability exists in software, for sabotage and denial of service, or simply because they wish to explore cybersecurity issues, artificial life and evolutionary algorithms.[13]\\r\\nComputer viruses currently cause billions of dollars' worth of economic damage each year,[14] due to causing system failure, wasting computer resources, corrupting data, increasing maintenance costs, etc. In response, free, open-source antivirus tools have been developed, and an industry of antivirus software has cropped up, selling or freely distributing virus protection to users of various operating systems.[15] As of 2005, even though no currently existing antivirus software was able to uncover all computer viruses (especially new ones), computer security researchers are actively searching for new ways to enable antivirus solutions to more effectively detect emerging viruses, before they have already become widely distributed.[16]\\r\\nThe term \\"virus\\" is also commonly, but erroneously, used to refer to other types of malware. \\"Malware\\" encompasses computer viruses along with many other forms of malicious software, such as computer \\"worms\\", ransomware, spyware, adware, trojan horses, keyloggers, rootkits, bootkits, malicious Browser Helper Object (BHOs) and other malicious software. The majority of active malware threats are actually trojan horse programs or computer worms rather than computer viruses. The term computer virus, coined by Fred Cohen in 1985, is a misnomer.[17] Viruses often perform some type of harmful activity on infected host computers, such as acquisition of hard disk space or central processing unit (CPU) time, accessing private information (e.g., credit card numbers), corrupting data, displaying political or humorous messages on the user's screen, spamming their e-mail contacts, logging their keystrokes, or even rendering the computer useless. However, not all viruses carry a destructive \\"payload\\" and attempt to hide themselvesthe defining characteristic of viruses is that they are self-replicating computer programs which modify other software without user consent.\\r\\n\\r\\n\\r\\nThe first academic work on the theory of self-replicating computer programs[18] was done in 1949 by John von Neumann who gave lectures at the University of Illinois about the \\"Theory and Organization of Complicated Automata\\". The work of von Neumann was later published as the \\"Theory of self-reproducing automata\\". In his essay von Neumann described how a computer program could be designed to reproduce itself.[19] Von Neumann's design for a self-reproducing computer program is considered the world's first computer virus, and he is considered to be the theoretical \\"father\\" of computer virology.[20] In 1972, Veith Risak, directly building on von Neumann's work on self-replication, published his article \\"Selbstreproduzierende Automaten mit minimaler Informationsbertragung\\" (Self-reproducing automata with minimal information exchange).[21] The article describes a fully functional virus written in assembler programming language for a SIEMENS 4004/35 computer system. In 1980 Jrgen Kraus wrote his diplom thesis \\"Selbstreproduktion bei Programmen\\" (Self-reproduction of programs) at the University of Dortmund.[22] In his work Kraus postulated that computer programs can behave in a way similar to biological viruses.\\r\\nThe Creeper virus was first detected on ARPANET, the forerunner of the Internet, in the early 1970s.[23] Creeper was an experimental self-replicating program written by Bob Thomas at BBN Technologies in 1971.[24] Creeper used the ARPANET to infect DEC PDP-10 computers running the TENEX operating system.[25] Creeper gained access via the ARPANET and copied itself to the remote system where the message, \\"I'm the creeper, catch me if you can!\\" was displayed. The Reaper program was created to delete Creeper.[26] In fiction, the 1973 Michael Crichton sci-fi movie Westworld made an early mention of the concept of a computer virus, being a central plot theme that causes androids to run amok.[27] Alan Oppenheimer's character summarizes the problem by stating that \\"...there's a clear pattern here which suggests an analogy to an infectious disease process, spreading from one...area to the next.\\" To which the replies are stated: \\"Perhaps there are superficial similarities to disease\\" and, \\"I must confess I find it difficult to believe in a disease of machinery.\\"[28]\\r\\nIn 1982, a program called \\"Elk Cloner\\" was the first personal computer virus to appear \\"in the wild\\"that is, outside the single computer or [computer] lab where it was created.[29] Written in 1981 by Richard Skrenta while in the ninth grade at Mount Lebanon High School near Pittsburgh, it attached itself to the Apple DOS 3.3 operating system and spread via floppy disk.[29][30] This virus, created as a practical joke when Skrenta was still in high school, was injected in a game on a floppy disk. On its 50th use the Elk Cloner virus would be activated, infecting the personal computer and displaying a short poem beginning \\"Elk Cloner: The program with a personality.\\" In 1984 Fred Cohen from the University of Southern California wrote his paper \\"Computer Viruses?ÿ Theory and Experiments\\".[31] It was the first paper to explicitly call a self-reproducing program a \\"virus\\", a term introduced by Cohen's mentor Leonard Adleman. In 1987, Fred Cohen published a demonstration that there is no algorithm that can perfectly detect all possible viruses.[32] Fred Cohen's theoretical compression virus[33] was an example of a virus which was not malicious software (malware), but was putatively benevolent (well-intentioned). However, antivirus professionals do not accept the concept of \\"benevolent viruses\\", as any desired function can be implemented without involving a virus (automatic compression, for instance, is available under the Windows operating system at the choice of the user). Any virus will by definition make unauthorised changes to a computer, which is undesirable even if no damage is done or intended. On page one of Dr Solomon's Virus Encyclopaedia, the undesirability of viruses, even those that do nothing but reproduce, is thoroughly explained.[34]\\r\\nAn article that describes \\"useful virus functionalities\\" was published by J. B. Gunn under the title \\"Use of virus functions to provide a virtual APL interpreter under user control\\" in 1984.[35] The first IBM PC virus in the \\"wild\\" was a boot sector virus dubbed (c)Brain,[36] created in 1986 by the Farooq Alvi Brothers in Lahore, Pakistan, reportedly to deter unauthorized copying of the software they had written.[37] The first virus to specifically target Microsoft Windows, WinVir was discovered in April 1992, two years after the release of Windows 3.0.[38] The virus did not contain any Windows API calls, instead relying on DOS interrupts. A few years later, in February 1996, Australian hackers from the virus-writing crew VLAD created the Bizatch virus (also known as \\"Boza\\" virus), which was the first known virus to target Windows 95. In late 1997 the encrypted, memory-resident stealth virus Win32.Cabanas was releasedthe first known virus that targeted Windows NT (it was also able to infect Windows 3.0 and Windows 9x hosts).[39]\\r\\nEven home computers were affected by viruses. The first one to appear on the Commodore Amiga was a boot sector virus called SCA virus, which was detected in November 1987.[40] The first social networking virus, Win32.5-0-1, was created by Matt Larose on August 15, 2001.[41] The virus specifically targeted users of MSN Messenger and online bulletin boards. Users would be required to click on a link to activate the virus, which would then send an email containing user data to an anonymous email address, which was later found to be owned by Larose. Data sent would contain items such as user IP address and email addresses, contacts, website browsing history, and commonly used phrases. In 2008, larger websites used part of the Win32.5-0-1 code to track web users advertising-related interests.\\r\\nA viable computer virus must contain a search routine, which locates new files or new disks which are worthwhile targets for infection. Secondly, every computer virus must contain a routine to copy itself into the program which the search routine locates.[42] The three main virus parts are:\\r\\nInfection mechanism (also called 'infection vector'), is how the virus spreads or propagates. A virus typically has a search routine, which locates new files or new disks for infection.[43]\\r\\nThe trigger, which is also known as logic bomb, is the compiled version that could be activated any time an executable file with the virus is run that determines the event or condition for the malicious \\"payload\\" to be activated or delivered[44] such as a particular date, a particular time, particular presence of another program, capacity of the disk exceeding some limit,[45] or a double-click that opens a particular file.[46]\\r\\nThe \\"payload\\" is the actual body or data that perform the actual malicious purpose of the virus. Payload activity might be noticeable (e.g., because it causes the system to slow down or \\"freeze\\"), as most of the time the \\"payload\\" itself is the harmful activity,[43] or some times non-destructive but distributive, which is called Virus hoax.[47]\\r\\nVirus phases is the life cycle of the computer virus, described by using an analogy to biology. This life cycle can be divided into four phases:\\r\\nThe virus program is idle during this stage. The virus program has managed to access the target user's computer or software, but during this stage, the virus does not take any action. The virus will eventually be activated by the \\"trigger\\" which states which event will execute the virus, such as a date, the presence of another program or file, the capacity of the disk exceeding some limit or the user taking a certain action (e.g., double-clicking on a certain icon, opening an e-mail, etc.). Not all viruses have this stage.[43]\\r\\nThe virus starts propagating, that is multiplying and replicating itself. The virus places a copy of itself into other programs or into certain system areas on the disk. The copy may not be identical to the propagating version; viruses often \\"morph\\" or change to evade detection by IT professionals and anti-virus software. Each infected program will now contain a clone of the virus, which will itself enter a propagation phase.[43]\\r\\nA dormant virus moves into this phase when it is activated, and will now perform the function for which it was intended. The triggering phase can be caused by a variety of system events, including a count of the number of times that this copy of the virus has made copies of itself.[43]\\r\\nThis is the actual work of the virus, where the \\"payload\\" will be released. It can be destructive such as deleting files on disk, crashing the system, or corrupting files or relatively harmless such as popping up humorous or political messages on screen.[43]\\r\\nComputer viruses infect a variety of different subsystems on their host computers and software.[48] One manner of classifying viruses is to analyze whether they reside in binary executables (such as .EXE or .COM files), data files (such as Microsoft Word documents or PDF files), or in the boot sector of the host's hard drive (or some combination of all of these).[49][50]\\r\\nA memory-resident virus (or simply \\"resident virus\\") installs itself as part of the operating system when executed, after which it remains in RAM from the time the computer is booted up to when it is shut down. Resident viruses overwrite interrupt handling code or other functions, and when the operating system attempts to access the target file or disk sector, the virus code intercepts the request and redirects the control flow to the replication module, infecting the target. In contrast, a non-memory-resident virus (or \\"non-resident virus\\"), when executed, scans the disk for targets, infects them, and then exits (i.e. it does not remain in memory after it is done executing).[51][52][53]\\r\\nMany common applications, such as Microsoft Outlook and Microsoft Word, allow macro programs to be embedded in documents or emails, so that the programs may be run automatically when the document is opened. A macro virus (or \\"document virus\\") is a virus that is written in a macro language, and embedded into these documents so that when users open the file, the virus code is executed, and can infect the user's computer. This is one of the reasons that it is dangerous to open unexpected or suspicious attachments in e-mails.[54][55] While not opening attachments in e-mails from unknown persons or organizations can help to reduce the likelihood of contracting a virus, in some cases, the virus is designed so that the e-mail appears to be from a reputable organization (e.g., a major bank or credit card company).\\r\\nBoot sector viruses specifically target the boot sector and/or the Master Boot Record[56] (MBR) of the host's hard drive or removable storage media (flash drives, floppy disks, etc.).[49][57][58]\\r\\nEmail virus ÿ A virus that intentionally, rather than accidentally, uses the email system to spread. While virus infected files may be accidentally sent as email attachments, email viruses are aware of email system functions. They generally target a specific type of email system (Microsofts Outlook is the most commonly used), harvest email addresses from various sources, and may append copies of themselves to all email sent, or may generate email messages containing copies of themselves as attachments.[59]\\r\\nIn order to avoid detection by users, some viruses employ different kinds of deception. Some old viruses, especially on the MS-DOS platform, make sure that the \\"last modified\\" date of a host file stays the same when the file is infected by the virus. This approach does not fool antivirus software, however, especially those which maintain and date cyclic redundancy checks on file changes.[60] Some viruses can infect files without increasing their sizes or damaging the files. They accomplish this by overwriting unused areas of executable files. These are called cavity viruses. For example, the CIH virus, or Chernobyl Virus, infects Portable Executable files. Because those files have many empty gaps, the virus, which was 1 KB in length, did not add to the size of the file.[61] Some viruses try to avoid detection by killing the tasks associated with antivirus software before it can detect them (for example, Conficker). In the 2010s, as computers and operating systems grow larger and more complex, old hiding techniques need to be updated or replaced. Defending a computer against viruses may demand that a file system migrate towards detailed and explicit permission for every kind of file access.[62]\\r\\nWhile some kinds of antivirus software employ various techniques to counter stealth mechanisms, once the infection occurs any recourse to \\"clean\\" the system is unreliable. In Microsoft Windows operating systems, the NTFS file system is proprietary. This leaves antivirus software little alternative but to send a \\"read\\" request to Windows OS files that handle such requests. Some viruses trick antivirus software by intercepting its requests to the Operating system (OS). A virus can hide by intercepting the request to read the infected file, handling the request itself, and returning an uninfected version of the file to the antivirus software. The interception can occur by code injection of the actual operating system files that would handle the read request. Thus, an antivirus software attempting to detect the virus will either not be given permission to read the infected file, or, the \\"read\\" request will be served with the uninfected version of the same file.[63]\\r\\nThe only reliable method to avoid \\"stealth\\" viruses is to \\"reboot\\" from a medium that is known to be \\"clear\\". Security software can then be used to check the dormant operating system files. Most security software relies on virus signatures, or they employ heuristics.[64][65] Security software may also use a database of file \\"hashes\\" for Windows OS files, so the security software can identify altered files, and request Windows installation media to replace them with authentic versions. In older versions of Windows, file cryptographic hash functions of Windows OS files stored in Windowsto allow file integrity/authenticity to be checkedcould be overwritten so that the System File Checker would report that altered system files are authentic, so using file hashes to scan for altered files would not always guarantee finding an infection.[66]\\r\\nMost modern antivirus programs try to find virus-patterns inside ordinary programs by scanning them for so-called virus signatures.[67] Unfortunately, the term is misleading, in that viruses do not possess unique signatures in the way that human beings do. Such a virus \\"signature\\" is merely a sequence of bytes that an antivirus program looks for because it is known to be part of the virus. A better term would be \\"search strings\\". Different antivirus programs will employ different search strings, and indeed different search methods, when identifying viruses. If a virus scanner finds such a pattern in a file, it will perform other checks to make sure that it has found the virus, and not merely a coincidental sequence in an innocent file, before it notifies the user that the file is infected. The user can then delete, or (in some cases) \\"clean\\" or \\"heal\\" the infected file. Some viruses employ techniques that make detection by means of signatures difficult but probably not impossible. These viruses modify their code on each infection. That is, each infected file contains a different variant of the virus.[citation needed]\\r\\nOne method of evading signature detection is to use simple encryption to encipher (encode) the body of the virus, leaving only the encryption module and a static cryptographic key in cleartext which does not change from one infection to the next.[68] In this case, the virus consists of a small decrypting module and an encrypted copy of the virus code. If the virus is encrypted with a different key for each infected file, the only part of the virus that remains constant is the decrypting module, which would (for example) be appended to the end. In this case, a virus scanner cannot directly detect the virus using signatures, but it can still detect the decrypting module, which still makes indirect detection of the virus possible. Since these would be symmetric keys, stored on the infected host, it is entirely possible to decrypt the final virus, but this is probably not required, since self-modifying code is such a rarity that it may be reason for virus scanners to at least \\"flag\\" the file as suspicious.[69] An old but compact way will be the use of arithmetic operation like addition or subtraction and the use of logical conditions such as XORing,[70] where each byte in a virus is with a constant, so that the exclusive-or operation had only to be repeated for decryption. It is suspicious for a code to modify itself, so the code to do the encryption/decryption may be part of the signature in many virus definitions.[69] A simpler older approach did not use a key, where the encryption consisted only of operations with no parameters, like incrementing and decrementing, bitwise rotation, arithmetic negation, and logical NOT.[70] Some viruses will employ a means of encryption inside an executable in which the virus is encrypted under certain events, such as the virus scanner being disabled for updates or the computer being rebooted. This is called cryptovirology. At said times, the executable will decrypt the virus and execute its hidden runtimes, infecting the computer and sometimes disabling the antivirus software.[citation needed]\\r\\nPolymorphic code was the first technique that posed a serious threat to virus scanners. Just like regular encrypted viruses, a polymorphic virus infects files with an encrypted copy of itself, which is decoded by a decryption module. In the case of polymorphic viruses, however, this decryption module is also modified on each infection. A well-written polymorphic virus therefore has no parts which remain identical between infections, making it very difficult to detect directly using \\"signatures\\".[71][72] Antivirus software can detect it by decrypting the viruses using an emulator, or by statistical pattern analysis of the encrypted virus body. To enable polymorphic code, the virus has to have a polymorphic engine (also called \\"mutating engine\\" or \\"mutation engine\\") somewhere in its encrypted body. See polymorphic code for technical detail on how such engines operate.[73]\\r\\nSome viruses employ polymorphic code in a way that constrains the mutation rate of the virus significantly. For example, a virus can be programmed to mutate only slightly over time, or it can be programmed to refrain from mutating when it infects a file on a computer that already contains copies of the virus. The advantage of using such slow polymorphic code is that it makes it more difficult for antivirus professionals and investigators to obtain representative samples of the virus, because \\"bait\\" files that are infected in one run will typically contain identical or similar samples of the virus. This will make it more likely that the detection by the virus scanner will be unreliable, and that some instances of the virus may be able to avoid detection.\\r\\nTo avoid being detected by emulation, some viruses rewrite themselves completely each time they are to infect new executables. Viruses that utilize this technique are said to be in metamorphic code. To enable metamorphism, a \\"metamorphic engine\\" is needed. A metamorphic virus is usually very large and complex. For example, W32/Simile consisted of over 14,000 lines of assembly language code, 90% of which is part of the metamorphic engine.[74][75]\\r\\nAs software is often designed with security features to prevent unauthorized use of system resources, many viruses must exploit and manipulate security bugs, which are security defects in a system or application software, to spread themselves and infect other computers. Software development strategies that produce large numbers of \\"bugs\\" will generally also produce potential exploitable \\"holes\\" or \\"entrances\\" for the virus.\\r\\nIn order to replicate itself, a virus must be permitted to execute code and write to memory. For this reason, many viruses attach themselves to executable files that may be part of legitimate programs (see code injection). If a user attempts to launch an infected program, the virus' code may be executed simultaneously.[76] In operating systems that use file extensions to determine program associations (such as Microsoft Windows), the extensions may be hidden from the user by default. This makes it possible to create a file that is of a different type than it appears to the user. For example, an executable may be created and named \\"picture.png.exe\\", in which the user sees only \\"picture.png\\" and therefore assumes that this file is a digital image and most likely is safe, yet when opened, it runs the executable on the client machine.[77]\\r\\nThe vast majority of viruses target systems running Microsoft Windows. This is due to Microsoft's large market share of desktop computer users.[78] The diversity of software systems on a network limits the destructive potential of viruses and malware.[79] Open-source operating systems such as Linux allow users to choose from a variety of desktop environments, packaging tools, etc., which means that malicious code targeting any of these systems will only affect a subset of all users. Many Windows users are running the same set of applications, enabling viruses to rapidly spread among Microsoft Windows systems by targeting the same exploits on large numbers of hosts.[5][6][7][80]\\r\\nWhile Linux and Unix in general have always natively prevented normal users from making changes to the operating system environment without permission, Windows users are generally not prevented from making these changes, meaning that viruses can easily gain control of the entire system on Windows hosts. This difference has continued partly due to the widespread use of administrator accounts in contemporary versions like Windows XP. In 1997, researchers created and released a virus for Linuxknown as \\"Bliss\\".[81] Bliss, however, requires that the user run it explicitly, and it can only infect programs that the user has the access to modify. Unlike Windows users, most Unix users do not log in as an administrator, or \\"root user\\", except to install or configure software; as a result, even if a user ran the virus, it could not harm their operating system. The Bliss virus never became widespread, and remains chiefly a research curiosity. Its creator later posted the source code to Usenet, allowing researchers to see how it worked.[82]\\r\\nMany users install antivirus software that can detect and eliminate known viruses when the computer attempts to download or run the executable file (which may be distributed as an email attachment, or on USB flash drives, for example). Some antivirus software blocks known malicious websites that attempt to install malware. Antivirus software does not change the underlying capability of hosts to transmit viruses. Users must update their software regularly to patch security vulnerabilities (\\"holes\\"). Antivirus software also needs to be regularly updated in order to recognize the latest threats. This is because malicious hackers and other individuals are always creating new viruses. The German AV-TEST Institute publishes evaluations of antivirus software for Windows[83] and Android.[84]\\r\\nExamples of Microsoft Windows anti virus and anti-malware software include the optional Microsoft Security Essentials[85] (for Windows XP, Vista and Windows 7) for real-time protection, the Windows Malicious Software Removal Tool[86] (now included with Windows (Security) Updates on \\"Patch Tuesday\\", the second Tuesday of each month), and Windows Defender (an optional download in the case of Windows XP).[87] Additionally, several capable antivirus software programs are available for free download from the Internet (usually restricted to non-commercial use).[88] Some such free programs are almost as good as commercial competitors.[89] Common security vulnerabilities are assigned CVE IDs and listed in the US National Vulnerability Database. Secunia PSI[90] is an example of software, free for personal use, that will check a PC for vulnerable out-of-date software, and attempt to update it. Ransomware and phishing scam alerts appear as press releases on the Internet Crime Complaint Center noticeboard. Ransomware is a virus that posts a message on the user's screen saying that the screen or system will remain locked or unusable until a ransom payment is made. Phishing is a deception in which the malicious individual pretends to be a friend, computer security expert, or other benevolent individual, with the goal of convincing the targeted individual to reveal passwords or other personal information.\\r\\nOther commonly used preventative measures include timely operating system updates, software updates, careful Internet browsing (avoiding shady websites), and installation of only trusted software.[91] Certain browsers flag sites that have been reported to Google and that have been confirmed as hosting malware by Google.[92][93]\\r\\nThere are two common methods that an antivirus software application uses to detect viruses, as described in the antivirus software article. The first, and by far the most common method of virus detection is using a list of virus signature definitions. This works by examining the content of the computer's memory (its Random Access Memory (RAM), and boot sectors) and the files stored on fixed or removable drives (hard drives, floppy drives, or USB flash drives), and comparing those files against a database of known virus \\"signatures\\". Virus signatures are just strings of code that are used to identify individual viruses; for each virus, the antivirus designer tries to choose a unique signature string that will not be found in a legitimate program. Different antivirus programs use different \\"signatures\\" to identify viruses. The disadvantage of this detection method is that users are only protected from viruses that are detected by signatures in their most recent virus definition update, and not protected from new viruses (see \\"zero-day attack\\").[94]\\r\\nA second method to find viruses is to use a heuristic algorithm based on common virus behaviors. This method has the ability to detect new viruses for which antivirus security firms have yet to define a \\"signature\\", but it also gives rise to more false positives than using signatures. False positives can be disruptive, especially in a commercial environment, because it may lead to a company instructing staff not to use the company computer system until IT services has checked the system for viruses. This can slow down productivity for regular workers.\\r\\nOne may reduce the damage done by viruses by making regular backups of data (and the operating systems) on different media, that are either kept unconnected to the system (most of the time, as in a hard drive), read-only or not accessible for other reasons, such as using different file systems. This way, if data is lost through a virus, one can start again using the backup (which will hopefully be recent).[95] If a backup session on optical media like CD and DVD is closed, it becomes read-only and can no longer be affected by a virus (so long as a virus or infected file was not copied onto the CD/DVD). Likewise, an operating system on a bootable CD can be used to start the computer if the installed operating systems become unusable. Backups on removable media must be carefully inspected before restoration. The Gammima virus, for example, propagates via removable flash drives.[96][97]\\r\\nMany websites run by antivirus software companies provide free online virus scanning, with limited \\"cleaning\\" facilities (after all, the purpose of the websites is to sell antivirus products and services). Some websiteslike Google subsidiary VirusTotal.comallow users to upload one or more suspicious files to be scanned and checked by one or more antivirus programs in one operation.[98][99] Additionally, several capable antivirus software programs are available for free download from the Internet (usually restricted to non-commercial use).[100] Microsoft offers an optional free antivirus utility called Microsoft Security Essentials, a Windows Malicious Software Removal Tool that is updated as part of the regular Windows update regime, and an older optional anti-malware (malware removal) tool Windows Defender that has been upgraded to an antivirus product in Windows 8.\\r\\nSome viruses disable System Restore and other important Windows tools such as Task Manager and CMD. An example of a virus that does this is CiaDoor. Many such viruses can be removed by rebooting the computer, entering Windows \\"safe mode\\" with networking, and then using system tools or Microsoft Safety Scanner.[101] System Restore on Windows Me, Windows XP, Windows Vista and Windows 7 can restore the registry and critical system files to a previous checkpoint. Often a virus will cause a system to \\"hang\\" or \\"freeze\\", and a subsequent hard reboot will render a system restore point from the same day corrupted. Restore points from previous days should work, provided the virus is not designed to corrupt the restore files and does not exist in previous restore points.[102][103]\\r\\nMicrosoft's System File Checker (improved in Windows 7 and later) can be used to check for, and repair, corrupted system files.[104] Restoring an earlier \\"clean\\" (virus-free) copy of the entire partition from a cloned disk, a disk image, or a backup copy is one solutionrestoring an earlier backup disk \\"image\\" is relatively simple to do, usually removes any malware, and may be faster than \\"disinfecting\\" the computeror reinstalling and reconfiguring the operating system and programs from scratch, as described below, then restoring user preferences.[95] Reinstalling the operating system is another approach to virus removal. It may be possible to recover copies of essential user data by booting from a live CD, or connecting the hard drive to another computer and booting from the second computer's operating system, taking great care not to infect that computer by executing any infected programs on the original drive. The original hard drive can then be reformatted and the OS and all programs installed from original media. Once the system has been restored, precautions must be taken to avoid reinfection from any restored executable files.[105]\\r\\nBefore computer networks became widespread, most viruses spread on removable media, particularly floppy disks. In the early days of the personal computer, many users regularly exchanged information and programs on floppies. Some viruses spread by infecting programs stored on these disks, while others installed themselves into the disk boot sector, ensuring that they would be run when the user booted the computer from the disk, usually inadvertently. Personal computers of the era would attempt to boot first from a floppy if one had been left in the drive. Until floppy disks fell out of use, this was the most successful infection strategy and boot sector viruses were the most common in the \\"wild\\" for many years. Traditional computer viruses emerged in the 1980s, driven by the spread of personal computers and the resultant increase in bulletin board system (BBS), modem use, and software sharing. Bulletin boardÿdriven software sharing contributed directly to the spread of Trojan horse programs, and viruses were written to infect popularly traded software. Shareware and bootleg software were equally common vectors for viruses on BBSs.[106][107][108] Viruses can increase their chances of spreading to other computers by infecting files on a network file system or a file system that is accessed by other computers.[109]\\r\\nMacro viruses have become common since the mid-1990s. Most of these viruses are written in the scripting languages for Microsoft programs such as Microsoft Word and Microsoft Excel and spread throughout Microsoft Office by infecting documents and spreadsheets. Since Word and Excel were also available for Mac OS, most could also spread to Macintosh computers. Although most of these viruses did not have the ability to send infected email messages, those viruses which did take advantage of the Microsoft Outlook Component Object Model (COM) interface.[110][111] Some old versions of Microsoft Word allow macros to replicate themselves with additional blank lines. If two macro viruses simultaneously infect a document, the combination of the two, if also self-replicating, can appear as a \\"mating\\" of the two and would likely be detected as a virus unique from the \\"parents\\".[112]\\r\\nA virus may also send a web address link as an instant message to all the contacts (e.g., friends and colleagues' e-mail addresses) stored on an infected machine. If the recipient, thinking the link is from a friend (a trusted source) follows the link to the website, the virus hosted at the site may be able to infect this new computer and continue propagating.[113] Viruses that spread using cross-site scripting were first reported in 2002,[114] and were academically demonstrated in 2005.[115] There have been multiple instances of the cross-site scripting viruses in the \\"wild\\", exploiting websites such as MySpace (with the Samy worm) and Yahoo!.","input":"Which was the first virus detected by arpanet?"},{"output":"in the 1920s","context":"Radio broadcasting in the United States is a major mass medium. Unlike radio in most other countries, American radio has historically relied primarily on commercial advertising sponsorship on for-profit stations. The U.S. does not have a national broadcaster that aims its programs at the general public. Nonprofit broadcasting typically comes in three forms: radio evangelism, community radio, and government-subsidized public radio, all of which rely at least to some extent on listener donations. Shortwave broadcasting in the United States includes federal government programs, as well as a limited number of privately managed broadcasts, for overseas audiences. Radio broadcasting became popular in the 1920s, alongside the new film industry. The arrival of television in the 1950s forced radio into a niche role, as leading participants moved into the much larger field of television. In the 21st century, radio has been adjusting to the arrival of the Internet and Internet radio.\\r\\n\\r\\n\\r\\nIn 1912, most amateur-radio transmissions were restricted to wavelengths below 200 meters (i.e., frequencies above 1500?kHz) to prevent interference to future commercial broadcasters.[1] The beginning of regular, commercially licensed radio broadcasting in the United States in 1920, along with the concurrent development of sound and color film in that decade, ended the print monopoly of mass media and opened the doors to the immediate (and pervasive) electronic media. By 1928, the United States had three national radio networks: two owned by NBC (the National Broadcasting Company), and one by CBS (the Columbia Broadcasting System). Until 1943, there were four major national radio networks: two owned by NBC, one owned by CBS and one owned by Mutual Broadcasting System. Stations were connected by broadcast-quality, leased telephone lines. NBC's second network became ABC, the American Broadcasting Company.[2]\\r\\nBy 1919, after the war, radio pioneers across the country resumed transmissions. The early stations gained new call signs. Some early stations were started by newspapers: the first to do this was 8MK (today WWJ) in Detroit, which was owned by the Detroit News. [3] While many sources cite KDKA in Pittsburgh as the first station, and give its first broadcast as November 2, 1920, the question of who came first is still in dispute among historians. [4] For example, there is evidence 8MK was on the air with Michigan state election returns on August 31, 1920 [5] Many other early stations were owned by small companies, such as AMRAD (the American Radio and Research Company, Medford Hillside MA); by some accounts, it broadcast voice and music as early as March 1916 [6], and was on the air sporadically till it began a daily schedule in May 1921, using call letters 1XE and later, WGI. [7] Perhaps the best-known early station began as experimental station 8XK, which became KDKA in 1920. AM radio signals traveled long distances, and KDKA's signal covered much of the country. The existence of early experimental radio stations encouraged young men (and a small number of young women) to build crystal sets (with ear phones) to listen to the new technical marvel. Entrepreneurs set up stations primarily with the goal of selling expensive console radio sets that the whole family could listen to, or which restaurants and shops would buy to attract customers.[8][9]\\r\\nUntil early 1922, there were fewer than forty commercial stations broadcasting. [10] But in mid-1922, the so-called \\"radio craze\\" began, when several hundred new stations took to the airwaves, and thousands of people wanted to buy their own radio set. Even President Warren G. Harding had a radio installed in the White House. [11]\\r\\nWhile \\"direct advertising\\" was not allowed in radio's formative years, per the order of then-head of the Department of Commerce Herbert Hoover [12], station owners wanted this policy changed, since it was very expensive to operate a radio station. By 1926, when the National Broadcasting Company went on the air, advertising was gradually allowed. Madison Avenue recognized the importance of radio as a new advertising medium. Advertising provided the major funding for most stations. The United States never had a license fee for set users.[13]\\r\\nThe National Broadcasting Company began regular broadcasting in 1926, with telephone links between New York and other Eastern cities. NBC became the dominant radio network, splitting into Red and Blue Networks. The Columbia Broadcasting System (CBS) began in 1927 owned and directed by William S. Paley.[14]\\r\\nPrior to 1927, U.S. radio was supervised by the Department of Commerce. Then, the Radio Act of 1927 created the Federal Radio Commission (FRC); in 1934, this agency was renamed the Federal Communications Commission (FCC) and is still in operation.[15]\\r\\nIn the early 1930s, a furious battle broke out as the newspaper industry tried to block the transmission of news by radio. Advertising revenues were plunging during the Great Depression, and newspapers were desperate to protect their main flow of income from the upstart's threat to the status quo. In one particular incident, the newspapers attempted to claim that a broadcast of the H. G. Wells story The War of the Worlds caused a mass hysteria among listeners who thought the broadcast was real news, a myth that continues to the present day. Hollywood briefly became involved, by preventing its stars from appearing on the radio; it soon realized, however, that radio was not a direct competition and the greater visibility for their stars met even larger audiences. The newspaper attack failed, and the networks set up their own newsgathering and reporting system, which played a major role during World War II.[16]\\r\\nA Federal Communications Commission decision in 1939 required NBC to divest itself of its Blue Network. That decision was sustained by the Supreme Court in a 1943 decision, National Broadcasting Co. v. United States, which established the framework that the \\"scarcity\\" of radio-frequency meant that broadcasting was subject to greater regulation than other media. This Blue Network network became the American Broadcasting Company (ABC). Around 1946, ABC, NBC, and CBS began regular television broadcasts. The small DuMont Television Network, was founded earlier; it disbanded in 1956. In 1934, several independent stations formed the Mutual Broadcasting System to exchange syndicated programming, including The Lone Ranger and Amos 'n' Andy.\\r\\nPaley at CBS was the first to fully realize that profitability depended primarily on maximizing the sales of commercials, which in turn meant reaching as large an upscale audience as possible. Surveys and polls could be used to determine not only the size of the audience, but their affluence. Frank Stanton, the president of CBS, worked with Columbia University sociologist Paul Lazarsfeld to develop techniques for measuring audiences.[17]\\r\\nBefore Paley, owners typically viewed their stations as stand-alone outlets or as the broadcast equivalent of local newspapers. It was their job to sell ads to local business. Individual stations bought programming from the network and, thus, were considered the network's clients. Paley changed the business model by looking at national advertisers as the critical element. CBS therefore provided network programming to affiliate stations at a nominal cost, thereby ensuring the widest possible distribution for both the programming and the advertising. The advertisers then became the network's primary clients and, because of the wider distribution brought by the growing network, Paley was able to charge more for the ad time. Affiliates were required to carry programming offered by the network for part of the broadcast day, receiving a portion of the network's fees from advertising revenue. At other times in the broadcast day, affiliates were free to offer local programming and sell advertising time locally.[18]\\r\\nBy 1940, the greatest audience attention was on the networks' evening programs of variety shows, music, and comedy and drama. In the morning and afternoon, smaller audiences (chiefly housewives) listened to 61 soap operas. Phone-in talk shows were rare, but disk jockeys attracted a following through their chatter between records. The most popular radio shows during the Golden Age of Radio. The Jack Benny Program, Fibber McGee and Molly, The Goldbergs and other top-rated American radio shows were heard by 30ÿ35 percent of the radio audience.[19][20]\\r\\nPresident Franklin Roosevelt's opponents had control of most newspapers in the 1930s; press reports were under their control and involved hostile editorial commentary. Roosevelt's solution was a series of 30 evening broadcasts in an informal, almost casual setting, dubbed \\"Fireside chats.\\" Historian Douglas B. Craig says that he, \\"offered voters a chance to receive information unadulterated by newspaper proprietors' bias\\" through the new medium of radio.\\"[21]\\r\\nRoosevelt's radio audiences averaged 18 percent during peacetime, and 58 percent during the war. His address of May 27, 1941, was heard by 70 percent of the radio audience.[22]\\r\\nRadio's instant, on-the-spot reports of dramatic events drew large audiences starting in 1938 in the run-up to World War II.[23][24]\\r\\nAll broadcast stations are licensed locally, in the past with mandated obligations to their \\"city of service,\\" fulfillment of which for license renewal is today largely pass. For many a \\"250-watt station in my hometown,\\" expensive facilities emulated the networks' in the form of multiple, acoustically fine studios in art deco style for local origination of music and variety programs, featuring local, mostly volunteer, talented teens and energetic young adults motivated by the possibility of being discovered. Local programs were \\"sustaining\\" (covered by general ad revenue), or the talent found their own sponsors and bought station time. Often paid just over minimum wage, \\"combo operator-announcers,\\" later called DJs, nevertheless came to consider themselves entertainers, often in fact becoming local celebrities, who cultivated \\"on-air personalities,\\" sometimes pairing one who was straight-laced with one playing the clown. Practical jokes by full-time staff were common, the unseeing audience none the wiser unless someone broke up laughing.\\r\\nContinuous station operations were manual (until automation debuted in the 1970s), and could originate programming only three ways: live; live via remote telephone line (including network feeds as well as store openings and church services around town); or played from \\"electrical transcription\\" (ET) phonograph discs. ETs up to 16in diameter played 15 minute syndicated programs, mailed to stations by the thousands, many for government sales of savings bonds and military recruiting. Magnetic tape arrived after 1947 and the Bing Crosby/Ampex alliance. All sound was monophonic until introduction of the stereo LP in 1958, when sister stations \\"simulcast\\" the left signal on AM, the right on FM, requiring audiofans to set up two receivers. For both practitioners and attentive listeners, local radio during the Golden Age was fun.[25]\\r\\nRadio in education began as early as April 1922, when Medford Hillside's WGI Radio broadcast the first of an ongoing series of educational lectures from Tufts College professors. These lectures were described by the press as a sort of \\"wireless college.\\"[26] Soon, other colleges across the U.S. began adding radio broadcasting courses to their curricula; some, like the University of Iowa, even provided what today would be known as distance-learning credits.[27] Curry College in Massachusetts introduced one of the nation's first broadcasting majors in 1932 when the college teamed up with WLOE in Boston to have students broadcast programs. In the 1950s, television courses became popular.[28] After 2000, Internet-based transmission of college courses came on line. 2012 was the \\"year of the MOOC\\" (massive open online course).[29]\\r\\nAfter 1955 television's visual images replaced the audio-only limitation of radio as the predominant entertainment and news vehicle. Radio adapted by replacing entertainment programs with schedules of music interspersed with news and features, a free-form format adopted by NBC when it launched its popular weekend-long Monitor in 1955. During the 1950s automobile manufacturers began offering car radios as standard accessories, and radio received a boost as Americans listened to their car radios as they drove to and from work. Although this period saw the overall decline of the AM radio, some new stations, like the WGIV station in Charlotte dedicated to African American Music, were still coming up.\\r\\nIn the 1950s, as a result of television's increased popularity coupled with dramatically loosened restrictions on playing recorded music on air, the network model of radio dramatically declined. In its place was the first music radio format: top 40, the forerunner to modern contemporary hit radio. Top 40 stations could be operated locally and gave rise to the disc jockeys, who became prominent local celebrities in their own right. Top 40 became the outlet for the relatively new style of music known as rock and roll.\\r\\nFM radio, which was the successor to the failed Apex band (also known as ultra-shortwave) experiments of the 1930s, arrived in the United States after 1945, but it was not until the 1960s that it became widely popular. In 1945 the Federal Communication Commission shifted FM's allocation of frequencies from 42-50 MHZ to 88-108 MHZ. This allowed FM much more room for growth when listeners arrived after 1960. The FCC later decided to reduce program duplication on AM and FM sister stations, giving the younger medium space to grow. The better sound fidelity of FM, the less static, dated a natural for musical programming. The first FM stations were primarily instrumental, featuring formats that would come to be known as easy listening and beautiful music, and were targeted at shopping centers. Other, later FM sign-ons became known for experimentation; the early freeform stations eventually evolved into progressive rock, the first radio format designed specifically to showcase rock music. From progressive rock came album-oriented rock, which in turn spawned the modern formats of classic rock, active rock and adult album alternative.[30]\\r\\nTop 40 radio would begin arriving on the FM band in the 1970s as FM reached critical mass. As the amount of archival music from the rock and roll era began to expand, oldies radio stations began to appear, later evolving into the modern classic hits and later adult hits formats. FM radio made a major expansion in the late 1980s with the FCC's Docket 80-90, which expanded the number of available FM licenses in the suburban areas of the United States. Country music in particular, previously only heard on rural AM stations particularly in the Southern and Western United States, benefited from this docket, moving en masse to FM; the beautiful music and easy listening formats mostly died out, with adult contemporary music taking its place.\\r\\nMeanwhile, the AM band began declining. All-news radio became popular in the major cities in the late 1960s. As each successive radio format moved to FM, AM radio stations were left with fewer and fewer options. One of the last \\"AM only\\" formats was MOR, or \\"middle-of-the-road\\", the direct forerunner of adult contemporary music and adult standards. Talk radio, although it had a small following in the cities, did not achieve mainstream popularity until the 1980s, when a combination of factors led to the rise of conservative talk radio. The politically charged format swept the country, bringing stardom to one of its pioneers, Rush Limbaugh. Also rising to popularity in the late 1980s was sports radio, which was dedicated to talk about sports as well as the broadcasting of sports events.[31] What few country stations remained on AM typically shifted to classic country and focused primarily on older music.\\r\\nWhile shock jocks had been in existence since at least the 1970s (see, for example, Don Imus) and the morning zoo radio format was popular among local stations beginning in the 1980s, the first shock jock to make a major national impact was Howard Stern, whose New York-based show was syndicated nationwide beginning in the early 1990s. Stern built a multimedia empire that incorporated television, books and feature films, which led to him bestowing upon himself the title of \\"King of All Media.\\" Stern left terrestrial radio in 2005.\\r\\nSatellite networks began replacing landline-based models in the 1980s, making possible wider and quicker national distribution of both talk and music radio. At the same time, the traditional networks started collapsing: NBC Radio and Mutual both were acquired by syndication company Westwood One, while ABC (both radio and television) was acquired by Capital Cities Communications. CBS would later acquire Westwood One, only to spin it off in 2007; as of 2015, Westwood One is now in the hands of Cumulus Media. ABC would end up in the hands of The Walt Disney Company, who broke the radio network up in 2007; Disney owns portions of the old network, while the rest of it is in the hands of Cumulus. Mutual was dissolved in 1999, replaced by CNN Radio, which itself was dissolved in 2012. CBS still owns much of its original network, the last of the four major networks of the Golden Age to do so; its programming is mostly funneled through Cumulus.\\r\\nTwo other major commercial networks have appeared since the 1990s: Premiere Networks, the division of iHeartMedia, and the Salem Radio Network. Premiere owns the radio distribution rights to the current \\"fourth major network\\", Fox (which owns no radio stations), and distributes that company's news and sports radio broadcasts. iHeart's immediate predecessor, Clear Channel Communications, benefited from the Telecommunications Act of 1996, which allowed for greater media consolidation, and built a large empire of both large and small market radio stations; Clear Channel, having overextended itself, jettisoned most of its small-market stations (as well as its now-dissolved television division) in the late 2000s. The Salem Radio Network, a division of Salem Communications (which outside of radio also has a large Internet operation), primarily has a Christian/conservative focus and specializes in Christian music, preaching stations and conservative talk radio, both owning stations and producing original content. Oaktree Capital Management briefly attempted a foray into building a radio network when it purchased the assets of several struggling radio networks in the late 2000s; while it still owns stations through its Townsquare Media holding company, it has since spun off its network holdings (which operated under the Dial Global brand) to Cumulus.\\r\\nDirect-to-consumer subscription satellite radio began appearing in the United States in the early 2000s, but despite heavy investment in programming has continually lost money and has less than a tenth of the reach of terrestrial radio. The two competing satellite radio services, Sirius and XM, merged to form Sirius XM Radio in 2009 and now has a government-regulated monopoly on the format.\\r\\nWhile broadcast radio stations will often brand themselves with plain-text names, identities such as \\"cool FM\\", \\"rock 105\\" or \\"the ABC network\\" are not unique. Another station, in another city or country, may (and often will) have a similar brand; the name of a broadcast station for legal purposes is, therefore, normally its ITU call sign.\\r\\nBroadcast stations in North America generally use call letters in the international series, with common conventions followed in each country. In the United States, the first letter generally is K for stations west of the Mississippi River and W for those east of the Mississippi; all new call signs have had four characters since 1930,[32] although there are historical three-character calls still in use (such as WOR in New York City, WBZ in Boston, WOL in Washington DC, WSB in Atlanta, WMC in Memphis, WGN in Chicago, KOA in Denver, KSL in Salt Lake City, KEX in Portland, Oregon, KUT in Austin, and KFI in Los Angeles).[33]\\r\\nThe expansion and dominance of FM radio, which has better audio quality but a more-limited broadcast range than AM, represented the major technical change in radio during the 1970s and 1980s. FM radio (aided by the development of smaller portable radios and \\"Walkman\\" headsets) dominates music programming, while AM has largely shifted to talk and news formats. Talk radio became more popular during the 1980s as a result of improved satellite communications, the repeal of the Fairness Doctrine and (by the mid-1990s) extensive concentration of media ownership stemming from the Telecommunications Act of 1996. While before the 1980s talk radio was primarily a local phenomenon, the development of national spoken-word programming contributed to the renewed popularity of AM radio. However, this popularity is fading as previously AM-only stations begin moving their operations to FM simulcasts or translators, and as the audience of talk radio ages.\\r\\nBoth FM and AM radio have become increasingly specialized. Music formats (for instance) comprise a variety of specializations, the top five in 1991 being \\"country and western\\", \\"adult contemporary\\", \\"Top 40\\", \\"religion\\" and \\"oldies\\". Radio has been shaped by demographics, although to a lesser degree than television; modern radio formats target groups of people by age, gender, urban (or rural) setting and race. As such, freeform stations with broad-spanning playlists have become uncommon on commercial radio.\\r\\nIn an era in which TV is the predominant medium, the reach of radio is still extensive. Ninety-nine percent of American households in 1999 had at least one radio; the average is five per household. Every day, radio reaches 80 percent of the U.S. population. Revenue more than doubled in a decade, from $8.4 billion in 1990 to more than $17 billion in 2000.[citation needed] Radio continues to prevail in automobiles and offices, where attention can be kept on the road (or the task at hand) while radio is an audio background. The popularity of car radios has led to drive time being the most listened-to dayparts on most radio stations, followed by midday (or the \\"at work\\" shift). Transistor radios, a technology that has been available since the 1950s, were the method of choice for listening to music on the go for most of the late 20th century, before digital media players and later smartphones (many of which have FM radios as part of their hardware) took those roles in the 20th century.\\r\\nThe majority of programming in the United States is in English, with Spanish the second-most popular broadcast language; these are the only two languages with domestically produced, national radio networks. In the largest urban areas of the United States, \\"world ethnic\\" stations may be found with a wide variety of languages (including Russian, Chinese, Korean and the languages of India); relatively widespread languages French and German have comparatively few radio outlets (in the case of German, due to the fact that most of its speakers are Amish or from similar sects and thus shun radio technology). French speakers can generally receive programming direct from Canadian broadcasters, which are receivable across the CanadaÿUS border and a handful of local stations serving the Haitian diaspora and Creole populations also serve areas in the southeast.\\r\\nUntil the 1980s, most commercial radio stations were affiliated with large networks such as ABC, CBS, the Mutual Broadcasting System, NBC, and others (e.g., RKO in the 1980s). The traditional major networks that had dominated the history of American radio up to that point began to be dissolved in the 1980s; RKO was forced to break up in a billing scandal, while NBC Radio and Mutual sold their assets to up-and-coming syndicator Westwood One, which itself would be bought by rival CBS in the 1990s. ABC maintained most of its radio network until 2007, when it sold off most of its stations to Citadel Broadcasting and later Cumulus Media (it maintains two specialty networks, sports-oriented ESPN Radio and youth top 40 Radio Disney, the latter of which has largely shifted to Internet radio; ABC still produces radio programming in addition to its terrestrial networks). CBS sold off Westwood One to private equity interests in the late 2000s as well, but unlike its rivals maintained ownership of its flagship stations. As of 2012, most commercial radio stations are controlled by media conglomerates and private equity firms such as Bain Capital (Clear Channel Communications), Oaktree Capital Management (Townsquare Media) and Cumulus Media.\\r\\nUnlike most countries, the national and state governments in the U.S. do not operate any stations or networks directed at its own domestic citizens. It does operate networks overseas through the Broadcasting Board of Governors; the most widely known of these networks is Voice of America, which serves a general worldwide audience; other networks target specific geographic regions. Broadcasting Board of Governors networks were, until 2013, expressly forbidden from being marketed to American citizens; they still neither own nor affiliate with any AM or FM station. The U.S. also provides the American Forces Network, a service for American armed servicemen stationed overseas that mostly relays commercial programming; AFN broadcasts are restricted to listeners in Japan, Korea and parts of Europe. In lieu of a national broadcaster, the United States government instead subsidizes nonprofit radio programming through the Corporation for Public Broadcasting.[34]\\r\\nIn 1998, the number of U.S. commercial radio stations had grown to 4,793 AM stations and 5,662 FM stations. In addition, there are 1,460 public radio stations. Most of these stations are run by private foundations, universities and public authorities for educational purposes and are financed by donations, foundations, subscriptions and corporate underwriting. Much public-radio broadcasting is supplied by NPR (formerly National Public Radio). NPR was incorporated in February 1970 under the Public Broadcasting Act of 1967; its television counterpart, PBS, was also created by the same legislation. (NPR and PBS are operated separately from each other.)[35]\\r\\nThe BBC World Service has been distributed in the United States since 1986; until July 2012 by PRI, and since then by APM.[36]\\r\\nA new form of radio which is gaining popularity is satellite radio. Sirius XM Radio has a monopoly on the technology after the merger of Sirius Satellite Radio and XM Satellite Radio. Unlike terrestrial-radio broadcasting, most channels feature few (or no) commercials. Satellite-radio content is not regulated by the Federal Communications Commission (FCC).[37]\\r\\nCable radio, a slightly older technology, has also become widespread; Music Choice is the market leader in this field, with its primary competitors being Sirius XM, Muzak, DMX, Sonic Tap and Canada-based Galaxie. CRN Digital Talk Radio Networks specialize in talk radio. Cable radio has the disadvantage that it requires a cable TV hookup, limiting its use outside the home.[38]\\r\\nUnlike the mandated digital television transition, the U.S. government has not mandated a transition to digital radio, although it allows digital radio to be broadcast. The national standard is HD Radio, a proprietary in-band on-channel format that allows digital and analog signals to be broadcast simultaneously. Radio companies aggressively marketed HD Radio throughout the 2000s, touting its clearer signal and capacity for digital subchannels, but the technology never caught on with the general public. The negatives were cost, signal reception problems, a general lack of quality programming on the subchannels, and (especially on AM HD stations) adjacent-channel interference. HD Radio's primary use has been to exploit an FCC loophole to allow low-power broadcast translators to carry HD subchannels in analog, thus giving radio station groups the ability to program more program services in a market than the federally mandated maximum number of stations allowed. FMeXtra is another subchannel service authorized for use in the United States, although that service is generally limited to voice transmissions due to lower bandwidth.[39]\\r\\nInternet radio, digital music players and streaming-capable smartphones are a challenge to terrestrial radio. Unlike satellite radio, most Internet stations do not require a subscription; several of the more popular ones use algorithms which allow listeners to customize the music they want to hear and select new music which may interest them. The proliferation of internet-based stations (which are more numerous and easier to set up than their television counterparts) creates a threat of audience fracturing beyond that experienced by television due to cable and satellite providers.\\r\\nAlthough not nearly to the extent that AM radio has declined in neighboring Canada, AM radio has also begun to decline in the United States. To partially combat this, radio ownership groups have increasingly moved their signals to FM, either through low-power broadcast translators (primarily on small, independent and/or rural stations) or through simulcast on full-market FM stations. The AM-to-FM phenomenon began primarily in mid-sized markets, where there is more bandwidth and less competition, but has since progressed even to New York City, where as of 2012 sports-talk AM stations WEPN and WFAN have both acquired FM stations with the intent to either move or simulcast their AM programming. By 2013 most of the AM/FM simulcasts had been discontinued, in part due to redundancy and the fact that most listeners to AM stations stayed with AM while very few new listeners were picked up on the FM side.\\r\\nAbridged from U.S. State Department IIP publications and other U.S. government materials.\\r\\nAcademy of Radio Arts & Sciences of America","input":"When did radio become popular in the us?"},{"output":"Gone with the Wind","context":"\\r\\n\\r\\nFilms generate income from several revenue streams, including theatrical exhibition, home video, television broadcast rights and merchandising. However, theatrical box office earnings are the primary metric for trade publications in assessing the success of a film, mostly because of the availability of the data compared to sales figures for home video and broadcast rights, but also because of historical practice. Included on the list are charts of the top box office earners (ranked by both the nominal and real value of their revenue), a chart of high-grossing films by calendar year, a timeline showing the transition of the highest-grossing film record, and a chart of the highest-grossing film franchises and series. All charts are ranked by international theatrical box office performance where possible, excluding income derived from home video, broadcasting rights and merchandise.\\r\\n\\r\\nTraditionally, war films, musicals and historical dramas have been the most popular genres, but franchise films have been among the best performers in the 21st century. Six Harry Potter films and five films from Peter Jackson's Middle-earth series are included in the nominal earnings chart, while the Star Wars and Pirates of the Caribbean franchises feature prominently. There is also continued interest in the superhero genre: Batman and Superman from DC Comics and films based on the Marvel Comics brand, such as Spider-Man, X-Men and films in the Marvel Cinematic Universe, have generally done well. Although the nominal earnings chart is dominated by films adapted from pre-existing properties and sequels, it is headed by Avatar and Titanic (both directed by James Cameron), which are original works. Animated family films have performed consistently well, with Disney films enjoying lucrative re-releases prior to the home-video era. Disney also enjoyed later success with films such as Frozen (the highest-grossing animated film), Zootopia and The Lion King, as well as with its Pixar brand, of which Toy Story 3 and the Finding Nemo films have been the best performers. Beyond Disney and Pixar animation, the Shrek, Ice Age and Despicable Me series have met with the most success.\\r\\n\\r\\nWhile inflation has eroded away the achievements of most films from the 1960s and 1970s, there are franchises originating from that period that are still active. Besides the Star Wars and Superman franchises, James Bond and Star Trek films are still being released periodically; all four are among the highest-grossing franchises. Some of the older films that held the record of highest-grossing film still have respectable grosses by today's standards, but no longer compete numerically against today's top-earners in an era of much higher individual ticket prices. When properly adjusted for inflation, however, on that comparative scale Gone with the Windwhich was the highest-grossing film outright for twenty-five yearsis still the highest-grossing film of all time. All grosses on the list are expressed in U.S. dollars at their nominal value, except where stated otherwise.\\r\\n\\r\\nWith a worldwide box-office gross of over $2.7?billion, Avatar is often proclaimed to be the \\"highest-grossing\\" film, but such claims usually refer to theatrical revenues only and do not take into account home video and television income, which can form a significant portion of a film's earnings. Once revenue from home entertainment is factored in it is not immediately clear which film is the most successful. Titanic earned $1.2?billion from video and DVD sales and rentals,[1] in addition to the $2.2?billion it grossed in theaters. While complete sales data are not available for Avatar, it earned $345?million from the sale of sixteen?million DVD and Blu-ray units in North America,[2] and ultimately sold a total of thirty?million DVD and Blu-ray units worldwide.[3] After home video income is accounted for, both films have earned over $3?billion. Television broadcast rights will also substantially add to a film's earnings, with a film often earning as much as 20ÿ25% of its theatrical box-office for a couple of television runs on top of pay-per-view revenues;[4] Titanic earned a further $55?million from the NBC and HBO broadcast rights,[1] equating to about 9% of its North American gross.\\r\\n\\r\\nWhen a film is highly exploitable as a commercial property, its ancillary revenues can dwarf its income from direct film sales.[5] The Lion King earned over $2?billion in box-office and home video sales,[6] but this pales in comparison to the $6?billion earned at box offices around the world by the stage adaptation.[7] Merchandising can be extremely lucrative too: The Lion King also sold $3?billion of merchandise,[8] while Pixar's Carswhich earned $462?million in theatrical revenues and was only a modest hit by comparison to other Pixar films[9]generated global merchandise sales of over $8?billion in the five years after its 2006 release.[10][11] Pixar also had another huge hit with Toy Story 3, which generated almost $10?billion in merchandise retail sales in addition to the $1?billion it earned at the box office.[12]\\r\\n\\r\\nOn this chart, films are ranked by the revenues from theatrical exhibition at their nominal value, along with the highest positions they attained. Thirty-five films in total have grossed in excess of $1?billion worldwide, of which four have grossed over $2?billion, with Avatar ranked in the top position. All of the films have had a theatrical run (including re-releases) in the 21st century, and films that have not played during this period do not appear on the chart because of ticket-price inflation, population size and ticket purchasing trends not being considered.\\r\\n\\r\\nFBox Office Mojo stopped updating its main total for Frozen in August 2014, while it was still in release. The total listed here incorporates subsequent earnings in Japan, Nigeria, Spain, the United Kingdom and Germany up to the end of 2015 but omits earnings in Turkey, Iceland, Brazil, and Australia (2016) which amount to a few hundred thousand dollars. It was re-released in the United Kingdom in December 2017 with Olaf's Frozen Adventure earning an additional $1,655,398. The total is rounded to $1?million to compensate for the numerical inaccuracy. \\r\\nF8In the case of The Fate of the Furious the gross is sourced from BoxOffice rather than the chart's regular source, Box Office Mojo, after irregularities were discovered in the latter's figure. Ongoing weekly drops in the totals for several countriesArgentina being the worst affectedled to a drop in the overall worldwide total.[14] In view of what appears to be an aberration in the source an alternative figure is provided. \\r\\nTS3Box Office Mojo revised the grosses for Pixar films in August 2016, resulting in the gross for Toy Story 3 being corrected from $1.063?billion to $1.067?billion.[15][16] This means that it peaked at #4 rather than #5 at the end of its release, as indicated by the source. \\r\\nDM2Disney issued an erratum to the gross for The Lion King, correcting its gross from $987.5?million to $968.5?million.[17] This means that Despicable Me 2 finished its run ahead of it and would have ranked one place higher at the end of its release. \\r\\nFNFinding Nemo finished one place higher at the end of its original release, after taking corrections into account. Its total now stands at $940.3?million, which would put the first run at $871.0?million after deducting the 3D reissue gross of $69.3?million, and slightly higher than the $864.6?million Box Office Mojo originally had listed. Meanwhile, Box Office Mojo originally had the gross for The Lord of the Rings: The Fellowship of the Ring listed at $871.4?million prior to its 2011 re-release, but this was dropped to $870.8?million by 2009.[18] The slightly higher gross of $871.0?million for Finding Nemo would rank above the slightly lower gross of $870.8?million for The Fellowship of the Ring.\\r\\n\\r\\nBecause of the long-term effects of inflation, notably the significant increase of movie theater ticket prices, the list unadjusted for inflation gives far more weight to later films.[19] The unadjusted list, while commonly found in the press, is therefore largely meaningless for comparing films widely separated in time, as many films from earlier eras will never appear on a modern unadjusted list, despite achieving higher commercial success when adjusted for price increases.[20] To compensate for the devaluation of the currency, some charts make adjustments for inflation, but not even this practice fully addresses the issue since ticket prices and inflation do not necessarily parallel one another. For example, in 1970, tickets cost $1.55 or about $6.68 in inflation-adjusted 2004 dollars; by 1980, prices had risen to about $2.69, a drop to $5.50 in inflation-adjusted 2004 dollars.[21] Ticket prices have also risen at different rates of inflation around the world, further complicating the process of adjusting worldwide grosses.[19]\\r\\n\\r\\nAnother complication is release in multiple formats for which different ticket prices are charged. One notable example of this phenomenon is Avatar, which was also released in 3D and IMAX: almost two-thirds of tickets for that film were for 3D showings with an average price of $10, and about one-sixth were for IMAX showings with an average price over $14.50, compared to a 2010 average price of $7.61 for 2D films.[22] Social and economic factors such as population change[23] and the growth of international markets[24][25][26] also impact on the number of people purchasing theater tickets, along with audience demographics where some films sell a much higher proportion of discounted children's tickets, or perform better in big cities where tickets cost more.[20]\\r\\n\\r\\nThe measuring system for gauging a film's success is based on unadjusted grosses, mainly because historically this is the way it has always been done because of the practices of the film industry: the box office receipts are compiled by theaters and relayed to the distributor, which in turn releases them to the media.[27] Converting to a more representative system that counts ticket sales rather than gross is also fraught with problems because the only data available for older films are the sale totals.[23] As the motion picture industry is highly oriented towards marketing currently released films, unadjusted figures are always used in marketing campaigns so that new blockbuster films can much more easily achieve a high sales ranking, and thus be promoted as a \\"top film of all time\\",[21][28] so there is little incentive to switch to a more robust analysis from a marketing or even newsworthy point of view.[27]\\r\\n\\r\\nDespite the inherent difficulties in accounting for inflation, several attempts have been made. Estimates depend on the price index used to adjust the grosses,[28] and the exchange rates used to convert between currencies can also impact upon the calculations, both of which can have an effect on the ultimate rankings of an inflation adjusted list. Gone with the Windfirst released in 1939is generally considered to be the most successful film, with Guinness World Records in 2014 estimating its adjusted global gross at $3.4?billion. Estimates for Gone with the Wind's adjusted gross have varied substantially: its owner, Turner Entertainment, estimated its adjusted earnings at $3.3?billion in 2007, a few years earlier than the Guinness estimate;[29] other estimates fall either side of this amount, with one putting its gross just under $3?billion in 2010,[30] while another provided an alternative figure of $3.8?billion in 2006.[31] Which film is Gone with the Wind's nearest rival depends on the set of figures used: Guinness had Avatar in second place with $3?billion, while other estimates saw Titanic in the runner-up spot with first-run worldwide earnings of almost $2.9?billion at 2010 prices. The only other film that all sources agreed grossed in excess of $2?billion at recent prices is Star Wars; according to Guinness it has earned $2.8?billion at 2014 price levels, while other sources from 2010/2011 put its adjusted earnings at $2.2ÿ2.6?billion.[30][32]\\r\\n\\r\\nInfInflation adjustment is carried out using the International Monetary Fund's global Consumer price index.[34] The index is uniformly applied to the grosses in the chart published by Guinness World Records in 2014, beginning with the 2014 index. The figures in the above chart take into account inflation that occurred in 2014, and in every available year since then, with 2016 the most recent year available.\\r\\n\\r\\nTGuinness' adjusted total for Titanic only increased by $102,000,000 between the 2012 (published in 2011) and 2015 editions, a rise of 4.2% shared by the other adjusted totals in the chart, and omitted the gross from a 3D re-release in 2012.[35][33] This chart incorporates the gross of $343,550,770 from the reissue and adjusts it from the 2014 index.[36] Titanic was re-released again in 2017 but this figure is not represented in the adjusted total.\\r\\n\\r\\nTFASince Star Wars: The Force Awakens was released in 2015, inflation is only applied to its gross from 2016 onwards.\\r\\n\\r\\nAudience tastes were fairly eclectic during the 20th century, but several trends did emerge. During the silent era, films with war themes were popular with audiences, with The Birth of a Nation (American Civil War), The Four Horsemen of the Apocalypse, The Big Parade and Wings (all World War I) becoming the most successful films in their respective years of release, with the trend coming to an end with All Quiet on the Western Front in 1930. With the advent of sound in 1927, the musicalthe genre best placed to showcase the new technologytook over as the most popular type of film with audiences, with 1928 and 1929 both being topped by musical films. The genre continued to perform strongly in the 1930s, but the outbreak of World War II saw war themed films dominate again during this period, starting with Gone with the Wind (American Civil War) in 1939, and finishing with The Best Years of Our Lives (World War II) in 1946. Samson and Delilah (1949) saw the beginning of a trend of increasingly expensive historical dramas set during Ancient Rome/biblical times throughout the 1950s as cinema competed with television for audiences,[40] with Quo Vadis, The Robe, The Ten Commandments, Ben-Hur and Spartacus all becoming the highest-grossing film of the year during initial release, before the genre started to wane after several high-profile failures.[41] The success of White Christmas and South Pacific in the 1950s foreshadowed the comeback of the musical in the 1960s with West Side Story, Mary Poppins, My Fair Lady, The Sound of Music and Funny Girl all among the top films of the decade. The 1970s saw a shift in audience tastes to high concept films, with six such films made by either George Lucas or Steven Spielberg topping the chart during the 1980s. The 21st century has seen an increasing dependence on franchises and adaptations, with the box office dominance of films based on pre-existing intellectual property at record levels.[42]\\r\\n\\r\\nSteven Spielberg is the most represented director on the chart with six films to his credit, occupying the top spot in 1975, 1981, 1982, 1984, 1989 and 1993. Cecil B. DeMille (1932, 1947, 1949, 1952 and 1956) and William Wyler (1942, 1946, 1959 and 1968) are in second and third place with five and four films respectively, while D. W. Griffith (1915, 1916 and 1920), George Roy Hill (1966, 1969 and 1973) and James Cameron (1991, 1997 and 2009) all feature heavily with three films apiece. George Lucas directed two chart-toppers in 1977 and 1999, but also served in a strong creative capacity as a producer and writer in 1980, 1981, 1983, 1984 and 1989 as well. The following directors have also all directed two films on the chart: Frank Lloyd, King Vidor, Frank Capra, Michael Curtiz, Leo McCarey, Alfred Hitchcock, David Lean, Stanley Kubrick, Guy Hamilton, Mike Nichols, William Friedkin, Peter Jackson, Gore Verbinski and Michael Bay; Mervyn LeRoy, Ken Annakin and Robert Wise are each represented by one solo credit and one shared credit, and John Ford co-directed two films. Disney films are usually co-directed and some directors have served on several winning teams: Wilfred Jackson, Hamilton Luske, Clyde Geronimi, David Hand, Ben Sharpsteen, Wolfgang Reitherman and Bill Roberts have all co-directed at least two films on the list. Only five directors have topped the chart in consecutive years: McCarey (1944 and 1945), Nichols (1966 and 1967), Spielberg (1981 and 1982), Jackson (2002 and 2003) and Verbinski (2006 and 2007).\\r\\n\\r\\nBecause of release schedulesespecially in the case of films released towards the end of the yearand different release patterns across the world, many films can do business in two or more calendar years; therefore the grosses documented here are not confined to just the year of release. Grosses are not limited to original theatrical runs either, with many older films often being re-released periodically so the figures represent all the business a film has done since its original release; a film's first-run gross is included in brackets after the total if known. Because of incomplete data it cannot be known for sure how much money some films have made and when they made it, but generally the chart chronicles the films from each year that went on to earn the most. In the cases where estimates conflict both films are recorded, and in cases where a film has moved into first place because of being re-released the previous record-holder is also retained. At least one film every year has generated $100?million in gross revenue at the box office since 1967,[clarification needed] and from 2008 each year has succeeded in producing a billion-dollar-grossing film.\\r\\n\\r\\n( ... ) Since grosses are not limited to original theatrical runs, a film's first-run gross is included in brackets after the total if known.\\r\\n\\r\\n*Canada and U.S. gross only.\\r\\n\\r\\nRDistributor rental.\\r\\n\\r\\nTBATo be ascertained.\\r\\n\\r\\nINNo contemporary sources provide figures for 20,000 Leagues Under the Sea, although The Numbers provides a figure of $8,000,000 for the North American box office gross.[46] However, it is possible this figure has been mistaken for the gross of the 1954 remake which also earned $8,000,000 in North American rentals.[47]\\r\\n\\r\\nFHSome sources such as The Numbers state that Aloma of the South Seas is the highest grossing film of the year, earning $3?million.[48] However, no contemporary sources provide figures for Aloma of the South Seas, so it is unclear what the $3?million figure relates to. If it were the rental gross then that would have made it not only the highest-grossing film of the year, but one of the highest-grossing films of the silent era, and if that is the case it would be unusual for both International Motion Picture Almanac and Variety to omit it from their lists.\\r\\n\\r\\nSSIt is not clear if the figure for Sunny Side Up is for North America or worldwide. Other sources put its earnings at $2?million,[49] which may suggest the higher figure is the worldwide rental, given the confusion over international figures during this period.[50]\\r\\n\\r\\nONThe figure for It Happened One Night is not truly representative of its success: it was distributed as a package deal along with more than two dozen other Columbia films, and the total earnings were averaged out; the true gross would have been much higher.\\r\\n\\r\\nS7Snow White's $418?million global cume omits earnings outside of North America from 1987 onwards.\\r\\n\\r\\nGWIt is not absolutely clear how much Gone with the Wind earned from its initial release. Contemporary accounts often list it as earning $32?million in North American rentals and retrospective charts have often duplicated this claim; however, it is likely this was the worldwide rental figure. Trade journals would collate the data by either obtaining it from the distributors themselves, who were keen to promote a successful film, or by surveying theaters and constructing an estimate. Distributors would often report the worldwide rental since the higher figure made the film appear more successful, while estimates were limited to performance in North America; therefore it was not unusual for worldwide and North American rentals to be mixed up. Following the outbreak of World War II, many of the foreign markets were unavailable to Hollywood so it became standard practice to just report on North American box-office performance.[50] In keeping with this new approach, the North American rental for Gone with the Wind was revised to $21?million in 1947 ($11?million lower than the previous figure),[51] and as of 1953following the 1947 re-releaseVariety was reporting earnings of $26?million.[52] Through 1956, MGM reported cumulative North American earnings of $30,015,000 and foreign earnings of $18,964,000, from three releases.[53] Worldwide rentals of $32?million from the initial release is consistent with the revised figures and later reported worldwide figures: they indicate that the film earned $21?million in North America and $11?million overseas from the initial release, and added a further $9?million in North America and $8?million overseas from subsequent re-releases up to 1956.\\r\\n\\r\\nMDMom and Dad does not generally feature in 'high-gross' lists such as those published by Variety due to its independent distribution. Essentially belonging to the exploitation genre, it was marketed as an educational sex hygiene film in an effort to circumvent censorship laws. Falling foul of the Motion Picture Production Code, Mom and Dad was prevented from obtaining mainstream distribution and restricted to independent and drive-in theaters. It was the biggest hit of its kind, and remained in continual distribution until the 1970s when hardcore pornography eventually took over. At the end of 1947 it had earned $2?million, and by 1949, $8?million; by 1956 it had earned $22?million in rentals, representing a gross of $80?million, and would have easily placed in the top ten films in the late 1940s and early 1950s. Estimates of its total earnings are as high as $100?million.\\r\\n\\r\\nUNChopra-Gant stipulates that the figure given for Unconquered is for North American box-office, but as was common at the time, the chart confuses worldwide and North American grosses. Other sources state that the takings for Forever Amber ($8?million) and Life with Father ($6.5?million)[54] were in fact worldwide rental grosses, so it is possible this is also true of Unconquered.\\r\\n\\r\\nCIThe Cinerama figures represent gross amounts. Since the Cinerama corporation owned the theaters there were no rental fees for the films, meaning the studio received 100% of the box-office gross, unlike the case with most other films where the distributor typically receives less than half the gross. Since Variety at the time ranked films by their US rental, they constructed a hypothetical rental figure for the Cinerama films to provide a basis for comparison to other films in their chart: in the case of This Is Cinerama, the $50?million worldwide gross was reconfigured as a $12.5?million US rental gross; this is exactly 25% of the amount reported by Cinerama, so Variety's formula seemingly halved the gross to obtain an estimate for the US share, and halved it again to simulate a rental fee. Variety's 'rental' amounts are often repeated, but have no basis in the reality of what the films actually earnedthey are hypothetical figures conceived for comparative analysis.[55] All five Cinerama features collectively generated $120?million in worldwide box office receipts.[56]\\r\\n\\r\\nGSVariety put the worldwide rental for The Greatest Show on Earth at around $18.35?million (with $12.8?million coming from the United States[47]) a year after its release; however, Birchard puts its earnings at just over $15?million up to 1962. It is likely that Birchard's figure is just the North American gross rental, and includes revenue from the 1954 and 1960 reissues.\\r\\n\\r\\nSWThe \\"first run\\" Star Wars grosses do not include revenue from the 1997 special-edition releases; however, the figure does include revenue from the re-releases prior to the special editions.\\r\\n\\r\\nHPProduction costs were shared with Harry Potter and the Deathly Hallows?ÿ Part 1.\\r\\n\\r\\nAt least ten films have held the record of 'highest-grossing film' since The Birth of a Nation assumed the top spot in 1915. Both The Birth of a Nation and Gone with the Wind spent twenty-five consecutive years apiece as the highest-grosser, with films directed by Steven Spielberg holding the record on three occasions and James Cameronthe current holdertwice. Spielberg became the first director to break his own record when Jurassic Park overtook E.T., and Cameron emulated the feat when Avatar broke the record set by Titanic.\\r\\n\\r\\nSome sources claim that The Big Parade superseded The Birth of a Nation as highest-grossing film, eventually being replaced by Snow White and the Seven Dwarfs, which in turn was quickly usurped by Gone with the Wind.[57] Exact figures are not known for The Birth of a Nation, but contemporary records put its worldwide earnings at $5.2?million as of 1919.[58] Its international release was delayed by World War I, and it was not released in many foreign territories until the 1920s; coupled with further re-releases in the United States, its $10?million earnings as reported by Variety in 1932 are consistent with the earlier figure.[59] At this time, Variety still had The Birth of a Nation ahead of The Big Parade ($6,400,000) on distributor rentals andif its estimate is correctSnow White and the Seven Dwarfs ($8,500,000)[60] would not have earned enough on its first theatrical run to take the record;[61] although it would have been the highest-grossing 'talkie',[62] displacing The Singing Fool ($5,900,000).[63] Although received wisdom holds that it is unlikely The Birth of a Nation was ever overtaken by a silent-era film,[64] the record would fall to 1925's Ben-Hur ($9,386,000) if The Birth of a Nation earned significantly less than its estimated gross.[65] In addition to its gross rental earnings through public exhibition, The Birth of a Nation played at a large number of private, club and organizational engagements which figures are unavailable for.[66] It was hugely popular with the Ku Klux Klan who used it to drive recruitment,[67] and at one point Variety estimated its total earnings to stand at around $50?million.[68] Despite later retracting the claim, the sum has been widely reported even though it has never been substantiated.[58] While it is generally accepted that Gone with the Wind took over the record of highest-grossing film on its initial releasewhich is true in terms of public exhibitionit is likely it did not overtake The Birth of a Nation in total revenue until a much later date, with it still being reported as the highest earner up until the 1960s.[66] Gone with the Wind itself may have been briefly overtaken by The Ten Commandments (1956), which closed at the end of 1960 with worldwide rentals of $58ÿ60?million[69][70] compared to Gone with the Wind's $59?million;[71] if it did claim the top spot its tenure there was short-lived, since Gone with the Wind was re-released the following year and increased its earnings to $67?million. Depending on how accurate the estimates are, the 1959 remake of Ben-Hur may also have captured the record from Gone with the Wind: as of the end of 1961 it had earned $47?million worldwide,[72] and by 1963 it was trailing Gone with the Wind by just $2?million with international takings of $65?million,[73] ultimately earning $66?million from its initial release.[74]\\r\\n\\r\\nAnother film purported to have been the highest-grosser is the 1972 pornographic film, Deep Throat. In 1984, Linda Lovelace testified to a United States Senate Judiciary Subcommittee on juvenile justice that the film had earned $600?million;[75] this figure has been the subject of much speculation, since if it is accurate then the film would have made more money than Star Wars, and finished the 1970s as the highest-grossing film. The main argument against this figure is that it simply did not have a wide enough release to sustain the sort of sums that would be required for it to ultimately gross this amount.[76] Exact figures are not known, but testimony in a federal trial in 1976about four years into the film's releaseshowed the film had grossed over $25?million.[77] Roger Ebert has reasoned it possibly did earn as much as $600?million on paper, since mobsters owned most of the adult movie theaters during this period and would launder income from drugs and prostitution through them, so probably inflated the box office receipts for the film.[78]\\r\\n\\r\\nThe Birth of a Nation, Gone with the Wind, The Godfather, Jaws, Star Wars, E.T. and Avatar all increased their record grosses with re-releases. The grosses from their original theatrical runs are included here along with totals from re-releases up to the point that they lost the record; therefore the total for The Birth of a Nation includes income from its reissues up to 1940; the total for Star Wars includes revenue from the late 1970s and early 1980s reissues but not from the 1997 Special Edition; the total for E.T. incorporates its gross from the 1985 reissue but not from 2002; the total for Avataras the current record-holderincludes all its earnings at the present time. Gone with the Wind is represented twice on the chart: the 1940 entry includes earnings from its staggered 1939ÿ1942 release (roadshow/general release/second-run)[79] along with all of its revenue up to the 1961 reissue prior to losing the record to The Sound of Music in 1966; its 1971 entryafter it took back the recordincludes income from the 1967 and 1971 reissues but omitting later releases. The Godfather was re-released in 1973 after its success at the 45th Academy Awards, and Jaws was released again in 1976, and their grosses here most likely include earnings from those releases. The Sound of Music, The Godfather, Jaws, Jurassic Park and Titanic increased their earnings with further releases in 1973, 1997, 1979, 2013 and 2012 respectively, but they are not included in the totals here since they had already conceded the record prior to being re-released.\\r\\n\\r\\nRDistributor rental.\\r\\n\\r\\nIncludes revenue from re-releases. If a film increased its gross through re-releases while holding the record, the year in which it recorded its highest gross is also noted in italics.\\r\\n\\r\\nPrior to 2000, only seven film series had grossed over $1?billion at the box office: James Bond,[89] Star Wars,[90] Indiana Jones,[91] Rocky,[92][93][94] Batman,[95] Jurassic Park[96] and Star Trek.[97] Since the turn of the century that number has increased to over fifty (not including one-off hits such as Avatar, Titanic and Frozen).[98] This is partly due to inflation and market growth, but also to Hollywood's adoption of the franchise model: films that have built-in brand recognition, such as being based on a well known literary source or an established character. The methodology is based on the concept that films associated with things audiences are already familiar with can be more effectively marketed to them, and as such are known as \\"pre-sold\\" films within the industry.[32]\\r\\n\\r\\nThe films in the cross-franchise Marvel Cinematic Universe collectively have grossed the most, amassing over $17?billion at the box office. The Harry Potter films are the highest-grossing series based on a single property, earning nearly $8?billion at the box office (although the Eon James Bond films have earned over $14?billion in total when adjusted to current prices[99]); Harry Potter has also generated at least $3.5?billion in home video revenue,[100] taking total consumer spending on the films to over $11?billion. If ancillary income from merchandise is included, then Star Wars is the most lucrative property;[101] it holds the Guinness world record for the \\"most successful film merchandising franchise\\" and was valued at S19.51?billion in 2012 (approximately $30?billion).[102][103] Only two franchises have had more than three films gross over $1?billion: the Marvel Cinematic Universe with six, and Star Wars with four. The three Avengers films comprise the only franchise where each installment has grossed over $1?billion. Avengers is also the only franchise to have a series average of over $1?billion per film, although the Star Wars, Pirates of the Caribbean, Jurassic Park and Finding Nemo franchises, Harry Potter films, and Peter Jackson's Middle-earth adaptation all average over $1?billion adjusted for inflation.[32][104]\\r\\n\\r\\nThe following list includes the shared universes Marvel Cinematic Universe and DC Extended Universe. Some films in these franchises are grouped both with the MCU or DCEU and with other films featuring the same character(s).[clarification needed]\\r\\n\\r\\n*Canada and U.S. gross only.\\r\\n\\r\\nRDistributor rental.","input":"What is the highest grossing box office film of all time?"},{"output":"January 2015","context":"\\r\\n\\r\\nThe Broads (known for marketing purposes as The Broads National Park) is a network of mostly navigable rivers and lakes in the English counties of Norfolk and Suffolk. The lakes, known as broads, were formed by the flooding of peat workings. The Broads, and some surrounding land, were constituted as a special area with a level of protection similar to a national park by the Norfolk and Suffolk Broads Act 1988. The Broads Authority, a special statutory authority responsible for managing the area, became operational in 1989.[2]\\r\\n\\r\\nThe area is 303 square kilometres (117?sq?mi), most of which is in Norfolk, with over 200 kilometres (120?mi) of navigable waterways. There are seven rivers and 63 broads, mostly less than 4 metres (13?ft) deep. Thirteen broads are generally open to navigation, with a further three having navigable channels. Some broads have navigation restrictions imposed on them in autumn and winter, although the legality of the restrictions is questionable.[3]\\r\\n\\r\\nAlthough the terms Norfolk Broads and Suffolk Broads are used to identify specific areas within the two counties respectively, the whole area is frequently referred to as the \\"Norfolk Broads\\". The Broads has similar status to the national parks in England and Wales; the Broads Authority has powers and duties akin to the national parks, but is also the third-largest inland navigation authority. Because of its navigation role the Broads Authority was established under its own legislation on 1 April 1989. The Broads Authority Act 2009, which was promoted through Parliament by the authority, is intended to improve public safety on the water.\\r\\n\\r\\nIn January 2015 the Broads Authority approved a change in name of the area to the Broads National Park, to recognise that the status of the area is equivalent to the English National Parks, that the Broads Authority shares the same two first purposes (relating to conservation and promoting enjoyment) as the English National Park Authorities, and receives a National park grant.\\r\\n\\r\\nThis followed a three-month consultation which resulted in support from 79% of consultees, including unanimous support from the 14 UK national parks and the Campaign for National Parks. Defra, the Government department responsible for the parks, also expressed it was content that the Authority would make its own decision on the matter.\\r\\n\\r\\nThis is the subject of ongoing controversy among some Broads users who note that the Broads is not named in law as a National Park and claim the branding detracts from the Broads Authority's third purpose which is to protect the interests of navigation. In response to this the Broads Authority has stated that its three purposes will remain in equal balance and that the branding is simply for marketing the National Park qualities of the Broads. [4]\\r\\n\\r\\nThe Broads are administered by the Broads Authority. Special legislation gives the navigation of the waterways equal status with the conservation and public enjoyment of the area.\\r\\n\\r\\nSpecific parts of the Broads have been awarded a variety of conservation designations, for instance:\\r\\n\\r\\nA specific project being considered under the UK Biodiversity Action Plan is re-introduction of the large copper butterfly, whose habitat has been reduced by reduction of fens.\\r\\n\\r\\nThe Broads, although administered by the Broads Authority, give their name to the Broadland local government district, while parts of the Broads also lie within other council areas: North Norfolk, South Norfolk, Norwich and Great Yarmouth (all in Norfolk) and Waveney district in Suffolk.\\r\\n\\r\\nFor many years the lakes known as broads were regarded as natural features of the landscape. It was only in the 1960s that Dr Joyce Lambert proved that they were artificial featuresflooded medieval peat excavations.[5] In the Middle Ages the local monasteries began to excavate the peatlands as a turbary business, selling fuel to Norwich and Great Yarmouth. Norwich Cathedral took 320,000 tonnes of peat a year. Then the sea levels began to rise, and the pits began to flood. Despite the construction of windpumps and dykes, the flooding continued and resulted in the typical Broads landscape of today, with its reedbeds, grazing marshes and wet woodland.\\r\\n\\r\\nVarious attempts were made to extend the navigable rivers. The longest-lasting was on the River Waveney, where an Act of Parliament passed on 17 March 1670 authorised improvements which included three locks, at Geldeston, Ellingham and Wainford. The head of navigation became a new staithe at Bungay. The new section was a private navigation which was not controlled by the Yarmouth Haven and Pier Commissioners, who had responsibility for the rest of the Broadland rivers.[6] It remained in use until 1934 and, although the upper two locks have been replaced by sluices and Geldeston lock is derelict, the Environment Agency have negotiated with local landowners to allow use by canoes and unpowered vessels which can be portaged around the locks.[7]\\r\\n\\r\\nThe next attempt was to extend navigation on the River Bure from Coltishall to Aylsham, which was authorised by an Act of Parliament on 7 April 1773. Five locks were built, to bypass mills, at Coltishall, Oxnead Lamas, Oxnead, Burgh and Aylsham. There were financial difficulties during construction, but the works were eventually completed and opened in October 1779. At Aylsham, a 1-mile (1.6?km) cut was made from the river to a terminal basin, where several warehouses were constructed. Despite the arrival of the railways in 1879, goods continued to be carried to Aylsham by wherries until 1912, when major flooding badly damaged the locks. Unable to fund repairs, the Commissioners closed the 9-mile (14?km) section above Coltishall, although it was not formally abandoned until 1928.[6] All of the locks are derelict, but the course can still be used by canoes and light craft, which can be portaged around the locks.[7]\\r\\n\\r\\nThe third attempt was to make the River Ant navigable from Dilham to Antingham. An Act of Parliament was obtained on 5 May 1812, which authorised the North Walsham & Dilham Canal, but work on its construction did not start until April 1825. The canal was a true canal, as its route did not use the bed of the river, and its construction, including six locks, was completed in 1826. It was about 8?3?4 miles (14.1?km) long, and the locks raised the level by 58 feet (18?m). In 1886 the canal was sold to a miller called Edward Press for S600, but the principal clerk absconded with most of the money and it was never recovered. In 1893 the section from Swafield locks to Antingham was abandoned, and the lower section was damaged by flooding in 1912. Some attempts were made to improve it in the 1920s, but the last commercial traffic used it in 1934, and it gradually became derelict after that.[6] There is still a public right of navigation to Swafield, and there is a campaign to reopen it.[7]\\r\\n\\r\\nIn 1814 the merchants of Norwich first suggested a plan to improve the route between Norwich and the North Sea, as the shallowness of Breydon Water created difficulties for trading vessels, and there was organised theft of cargo during its transshipment at Great Yarmouth, for which 18 men were convicted of taking the goods and one of receiving it in 1820. The initial plan was to dredge a deeper channel along the southern edge of Breydon Water, but the scheme was opposed by the people of Yarmouth. A more expensive scheme, involving the construction of a new cut to link the River Yare to the River Waveney, together with a channel between Oulton Broad and Lake Lothing, where a sea lock was needed, was also opposed by Yarmouth, but formed the basis of a Bill to Parliament. An Act of Parliament was passed on 28 May 1827, creating the Norwich and Lowestoft Navigation Company, and the work of construction and dredging of the River Yare and the Oulton Dyke was completed in 1833. The initial capital of S100,000 was inadequate and a further S50,000 was borrowed from the Exchequer Bill Loan Commission. The venture was not a commercial success, and, with expenditure exceeding income, the Company was unable to repay its loan. The Haddiscoe Cut was taken over by the Commissioners in 1842 and sold to the railway developer Sir Samuel Morton Peto.[6]\\r\\n\\r\\nThe Broads have been a boating holiday destination since the late 19th century. In 1878 small yachts were available to hire from John Loynes, and with easy access to the area by rail from London, Harry Blake created an agency for yachting holidays in 1908. The first boats were owned by the boatbuilder Ernest Collins of Wroxham, but other boatyards were soon added to the business. The range of boats expanded to include powered cruisers in the 1930s, and the Hoseasons agency was founded soon after the Second World War. By the 1980s the number of cruisers available for hire was 2,400, but had decreased to around 1,700 by 2004. For conservation reasons there is a strict speed limit enforced on all vessels, to reduce waves eroding the riverbanks. These speed limits are hardwired onto most rental vessels.\\r\\n\\r\\nThe Broads have also been an important centre for racing yachts since the late 19th century, and the design of the boats have included several innovative features, including short fin keels and a separate rudder[disputed  ÿ discuss]. The design was eventually used on seagoing yachts from the 1960s.[7]\\r\\n\\r\\nThe waterways are lock-free, although there are five bridges under which only small cruisers and smaller boats can pass. The area attracts all kinds of visitors, including ramblers, artists, anglers, and birdwatchers as well as people \\"messing about in boats\\". There are a number of companies hiring boats for leisure use, including both yachts and motor launches. The Norfolk wherry, the traditional cargo craft of the area, can still be seen on the Broads as some specimens have been preserved and restored.\\r\\n\\r\\nTed Ellis, a local naturalist, referred to the Broads as \\"the breathing space for the cure of souls\\".[8]\\r\\n\\r\\nA great variety of boats can be found on the Broads, from Edwardian trading wherries to state-of-the-art electric or solar-powered boats. The Broads Authority is promoting sustainable boating, and the use of electric boats is being encouraged by the provision of charging points at a number of the mooring sites provided by the Authority.[7]\\r\\n\\r\\nThe Broads largely follows the line of the rivers and natural navigations of the area. There are seven navigable rivers, the River Yare and its (direct and indirect) tributaries the Rivers Bure, Thurne, Ant, Waveney, Chet and Wensum. There are no longer any operational locks on any of the rivers (except for Mutford Lock in Oulton Broad that links to the saltwater Lake Lothing in Lowestoft, Suffolk), but all of the waterways are subject to tidal influence. The tidal range decreases with distance from the sea, with highly tidal areas such as Breydon Water contrasted with effectively non-tidal reaches such as the River Ant upstream of Barton Broad.\\r\\n\\r\\nThe broads themselves range in size from small pools to the large expanses of Hickling Broad, Barton Broad and Breydon Water. The broads are unevenly distributed, with far more broads in the northern half of Broadland (the Rivers Bure, Thurne and Ant) than in the central and southern portions (the Rivers Yare, Waveney, Chet and Wensum). Individual broads may lie directly on the river, or are more often situated to one side and connected to the river by an artificial channel or dyke.\\r\\n\\r\\nBesides the natural watercourses of the rivers, and the ancient but artificial broads, there is one more recent navigation canal, the lockless Haddiscoe Cut which connects the Rivers Yare and Waveney whilst permitting boats to bypass Breydon Water.\\r\\n\\r\\nThere is also a second navigable link to the sea, via the River Waveney and its link to Oulton Broad. Oulton Broad is part of the Broads' tidal system, but is immediately adjacent to Lake Lothing which acts as a harbour for Lowestoft and connects to the North Sea. Oulton Broad and Lake Lothing are connected by Mutford Lock, the only lock on the broads and necessary because of the different tidal ranges and cycles in the two lakes.\\r\\n\\r\\nThe River Bure is a tributary of the River Yare which rises near Aylsham in Norfolk and joins the Yare just downstream of Breydon Water. On its way it flows through or passes:\\r\\n\\r\\nThe River Thurne is a tributary of the River Bure. It rises near Martham Broad and flows for about six miles (9.7?km) to Thurne Mouth where it joins the Bure. It is wide open and windswept, and on its way it flows through or passes:\\r\\n\\r\\nThe River Ant is a tributary of the River Bure. It rises at Antingham and joins the Bure at St. Benet's Abbey. It is winding and narrow, and on its way it flows through or passes:\\r\\n\\r\\nThe River Yare rises south of Dereham and flows through the southern fringes of the city of Norwich, passes through Breydon Water and flows into the sea between Great Yarmouth and Gorleston. On its way it passes through:\\r\\n\\r\\nThe River Chet is a tributary of the River Yare. It flows through, or passes by:\\r\\n\\r\\nThe River Waveney is a tributary of the River Yare, joining that river just upstream of Breydon Water. It flows through, or passes by:\\r\\n\\r\\nThe River Wensum rises near Fakenham in northwest Norfolk and flows southeast and through the centre of the city of Norwich before joining the River Yare just to the east of the city. Although the Wensum is the larger of the two rivers at their confluence, it is regarded as a tributary of the River Yare. The navigable section of the river is entirely urban and runs from the centre of Norwich, past Norwich Cathedral to the confluence with the Yare.\\r\\n\\r\\nThe Trinity Broads are an exception to the general rule, in that whilst they are connected to each other they have no navigable connection to the rest of the broads. The broads are:\\r\\n\\r\\nEutrophication is an enormous problem in the Broads. Changes in farming practices and sewage disposal in the 1950s and 1960s released high levels of phosphorus and nitrogen into the Broads, causing eutrophication.[9] Algal blooms can be toxic, posing a health risk to humans and wildlife.[10] Mass decay of plant matter removes oxygen, damaging fish stocks, preventing recreational fishing. The loss of larger plants and reed fringes in eutrophic waters increases erosion of banks and the buildup of sediment on lake floors.[11] This impedes navigation and requires costly dredging to remove. The beauty of the area is damaged by eutrophication, which is detrimental to the tourism industry. The Broads authority and Environment Agency have been working to return the broads to a more natural state since the problem was identified in 1965.[9]\\r\\n\\r\\nThe first stage in reversing eutrophication in the Broads is to reduce phosphate input. Reducing nitrate input would have a similar effect, but due to the relatively higher solubility of nitrates, it is harder to control.[11] The discharge of treated sewage was recognised as the main source of phosphates in the waters of the broads. Iron compounds have been used to precipitate phosphates out of treated sewage in all nine treatment plants upstream of Barton broad, initially cutting phosphorus levels in sewage discharge by 90%.[9] High levels of phosphate can remain present in the sediments at the bottom of waterways, preventing dissolved levels decreasing, even when the source is eliminated. Suction dredging has been used across the Broads to both deepen waterways and remove phosphate-rich sludge. Without stabilising the compacted peat beneath the sludge, the peat loosens and can release phosphorus at a similar rate. The growth of larger water plants, which stabilise the floor, is therefore necessary to complete the transformation.[9]\\r\\n\\r\\nEven with reduced nutrient levels, algae tend to remain dominant, blocking light and preventing plants from growing on the floor of the waterway. By manipulating the food chain, a process called biomanipulation, algae can be removed. To allow zooplankton to thrive, planktivorous fish have been largely removed from some Broads, normally by electrofishing. Around 75% of such fish must be removed for successful treatment.[9] The explosion of zooplankton that results eats almost all algae, creating clear waters. Plants are allowed to naturally recolonise the clearer waterways. The plant growth stabilises the floor, reducing the release of phosphorus. Their own nutrient uptake reduces nutrient available to algae. Larger plants also create a favourable environment for predatory fish such as pike, which eat planktivorous fish, continuing to control their numbers. These effects tend to create a stable ecosystem where low growing underwater plants dominate.[9]\\r\\n\\r\\nThe Broads are Britain's largest protected wetland and are home to a wealth of birdlife. Amongst the species seen are mallard, coot, moorhen, great crested grebe, greylag goose, Canada goose, Egyptian goose, grey heron, marsh harrier, cormorant, kestrel, sparrowhawk and bittern.[citation needed] The scarce Cetti's warbler breeds in the broads and breeding common cranes are found in the area.[citation needed]\\r\\n\\r\\nAmong the rare insects are the Norfolk hawker, a species of dragonfly, and the Old World swallowtail butterfly (Papilio machaon subsp. brittanicus).[12]\\r\\n\\r\\nSome of the broads are surrounded by fens, i.e. reed and sedge beds. Norfolk reed from the broads has been a traditional material for thatching houses.\\r\\n\\r\\nCoordinates: 524327N 13827E? / ?52.72417N 1.64083E? / 52.72417; 1.64083","input":"When did the norfolk broads become a national park?"},{"output":"1 July 1997","context":"The transfer of sovereignty over Hong Kong from the United Kingdom to China, referred to as \\"the Handover\\" internationally or \\"the Return\\" in China, took place on 1 July 1997. The landmark event marked the end of British administration in Hong Kong, and is often regarded as the watershed of the British Empire.\\r\\n\\r\\n\\r\\nHong Kong's territory was acquired from three separate treaties: the Treaty of Nanking in 1842, the Convention of Peking in 1860, and The Convention for the Extension of Hong Kong Territory in 1898, which gave the UK the control of Hong Kong Island, Kowloon (area south of Boundary Street), and the New Territories (area north of Boundary Street and south of the Sham Chun River, and outlying islands), respectively.\\r\\nAlthough Hong Kong Island and Kowloon had been ceded to the United Kingdom in perpetuity, the control on the New Territories was a 99-year lease. The finite nature of the 99-year lease did not hinder Hong Kong's development as the New Territories were combined as a part of Hong Kong.\\r\\nHowever, by 1997, it was impractical to separate the three territories and only return the New Territories. In addition, with the scarcity of land and natural resources in Hong Kong Island and Kowloon, the New Territories were being developed with large-scale infrastructures and other developments, with the break-even day lying well past 30 June 1997. Thus, the status of the New Territories after the expiry of the 99-year lease became important for Hong Kong's economic development.[1]\\r\\nWhen the People's Republic of China obtained its seat in the United Nations as a result of the UN General Assembly Resolution 2758 in 1971, it began to act diplomatically on the sovereignty issues of Hong Kong and Macau. In March 1972, the Chinese UN representative, Huang Hua, wrote to the United Nations Decolonization Committee to state the position of the Chinese government:\\r\\nThe same year, on 8 November, the United Nations General Assembly passed the resolution on removing Hong Kong and Macau from the official list of colonies.[2]\\r\\nIn March 1979, the Governor of Hong Kong Murray MacLehose paid his first official visit to the People's Republic of China (PRC), taking the initiative to raise the question of Hong Kong's sovereignty with Deng Xiaoping.[3] Without clarifying and establishing the official position of the PRC government, the arranging of real estate leases and loans agreements in Hong Kong within the next 18 years would become difficult.[1]\\r\\nIn response to concerns over land leases in the New Territories, MacLehose proposed that British administration of the whole of Hong Kong, as opposed to sovereignty, be allowed to continue after 1997.[4] He also proposed that contracts include the phrase \\"for so long as the Crown administers the territory\\".[5]\\r\\nIn fact, as early as the mid-1970s, Hong Kong had faced additional risks raising loans for large-scale infrastructure projects such as its Mass Transit Railway (MTR) system and a new airport. Caught unprepared, Deng asserted the necessity of Hong Kong's return to China, upon which Hong Kong would be given special status by the PRC government.\\r\\nMacLehose's visit to the PRC raised the curtain on the issue of Hong Kong's sovereignty: Britain was made aware of the PRC's aspiration to resume sovereignty over Hong Kong and began to make arrangements accordingly to ensure the sustenance of her interests within the territory, as well as initiating the creation of a withdrawal plan in case of emergency.\\r\\nThree years later, Deng received the former British Prime Minister Edward Heath, who had been dispatched as the special envoy of Prime Minister Margaret Thatcher to establish an understanding of the PRC's view with regards to the question of Hong Kong; during their meeting, Deng outlined his plans to make the territory a special economic zone, which would retain its capitalist system under Chinese sovereignty.[6]\\r\\nIn the same year, Edward Youde, who succeeded MacLehose as the 26th Governor of Hong Kong, led a delegation of five Executive Councillors to London, including Chung Sze-yuen, Lydia Dunn, and Roger Lobo.[7] Chung presented their position on the sovereignty of Hong Kong to Thatcher, encouraging her to take into consideration the interests of the native Hong Kong population in her upcoming visit to China.[7]\\r\\nIn light of the increasing openness of the PRC government and economic reforms on the mainland, the then British Prime Minister Margaret Thatcher sought the PRC's agreement to a continued British presence in the territory.[8]\\r\\nHowever, the PRC took a contrary position: not only did the PRC wish for the New Territories, on lease until 1997, to be placed under the PRC's jurisdiction, it also refused to recognize the \\"unfair and unequal treaties\\" under which Hong Kong Island and Kowloon had been ceded to Britain in perpetuity.[9] Consequently, the PRC recognized only the British administration in Hong Kong, but not British sovereignty.[9]\\r\\nIn the wake of Governor MacLehose's visit, Britain and the PRC established initial diplomatic contact for further discussions of the Hong Kong question, paving the way for Thatcher's first visit to the PRC in September 1982.[10]\\r\\nMargaret Thatcher, in discussion with Deng Xiaoping, reiterated the validity of an extension of the lease of Hong Kong territory, particularly in light of binding treaties, including the Treaty of Nanking in 1842, the Convention of Peking in 1856, and the Convention for the Extension of Hong Kong Territory signed in 1890.\\r\\nIn response, Deng Xiaoping cited clearly the lack of room for compromise on the question of sovereignty over Hong Kong; the PRC, as the successor of Qing Dynasty and the Republic of China on the mainland, would recover the entirety of the New Territories, Kowloon and Hong Kong Island. China considered treaties about Hong Kong as unequal and ultimately refused to accept any outcome that would indicate permanent loss of sovereignty over Hong Kong's area, whatever wording the former treaties had.[11]\\r\\nDuring talks with Thatcher, China planned to invade and seize Hong Kong if the negotiations set off unrest in the colony. Thatcher later said that Deng told her bluntly that China could easily take Hong Kong by force, stating that \\"I could walk in and take the whole lot this afternoon\\", to which she replied that \\"there is nothing I could do to stop you, but the eyes of the world would now know what China is like\\".[12]\\r\\nAfter her visit with Deng in Beijing, Thatcher was received in Hong Kong as the first British Prime Minister to set foot on the territory whilst in office. At a press conference, Thatcher re-emphasised the validity of the three treaties, asserting the need for countries to respect treaties on universal terms: \\"There are three treaties in existence; we stick by our treaties unless we decide on something else. At the moment, we stick by our treaties.\\".[8]\\r\\nAt the same time, at the 5th session of the 5th National People's Congress, the constitution was amended to include a new Article 31 which stated that the country might establish Special Administrative Regions (SARs) when necessary.[13]\\r\\nThe additional Article would hold tremendous significance in settling the question of Hong Kong and later Macau, putting into social consciousness the concept of \\"One country, two systems\\". The concept would prove useful to deploy until the territories were secured and conditions were ripe for its gradual abrogation.\\r\\nA few months after Thatcher's visit to Beijing, the PRC government had yet to open negotiations with the British government regarding the sovereignty of Hong Kong.\\r\\nShortly before the initiation of sovereignty talks, Governor Youde declared his intention to represent the population of Hong Kong at the negotiations. This statement sparked a strong response from the PRC, prompting Deng Xiaoping to denounce talk of \\"the so-called 'three-legged stool'\\", which implied that Hong Kong was a party to talks on its future, alongside Beijing and London.[14]\\r\\nAt the preliminary stage of the talks, the British government proposed an exchange of sovereignty for administration and the implementation of a British administration post-handover.[8]\\r\\nThe PRC government refused, contending that the notions of sovereignty and administration were inseparable, and although it recognised Macau as a \\"Chinese territory under Portuguese administration\\", this was only temporary.[15]\\r\\nIn fact, during informal exchanges between 1979 and 1981, the PRC had proposed a \\"Macau solution\\" in Hong Kong, under which it would remain under British administration at China's discretion.[3]\\r\\nHowever, this had previously been rejected following the 1967 Leftist riots, with the then Governor, David Trench, claiming the leftists' aim was to leave the UK without effective control, or \\"to Macau us\\".[16]\\r\\nThe conflict that arose at that point of the negotiations ended the possibility of further negotiation. During the reception of former British Prime Minister Edward Heath during his sixth visit to the PRC, Deng Xiaoping commented quite clearly on the impossibility of exchanging sovereignty for administration, declaring an ultimatum: the British government must modify or give up its position or the PRC will announce its resolution of the issue of Hong Kong sovereignty unilaterally.[17]\\r\\nIn 1983, Typhoon Ellen ravaged Hong Kong, causing great amounts of damage to both life and property.[18] The Hong Kong dollar plummeted on Black Saturday, and the Financial Secretary John Bremridge publicly associated the economic uncertainty with the instability of the political climate.[19] In response, the PRC government condemned Britain through the press for \\"playing the economic card\\" in order to achieve their ends: to intimidate the PRC into conceding to British demands.[20]\\r\\nGovernor Youde with nine members of the Hong Kong Executive Council travelled to London to discuss with Prime Minister Thatcher the crisis of confidencethe problem with morale among the people of Hong Kong arising from the ruination of the Sino-British talks. The session concluded with Thatcher's writing of a letter addressed to the PRC Premier Zhao Ziyang.\\r\\nIn the letter, she expressed Britain's willingness to explore arrangements optimising the future prospects of Hong Kong while utilising the PRC's proposals as a foundation. Furthermore, and perhaps most significantly, she expressed Britain's concession on its position of a continued British presence in the form of an administration post-handover.\\r\\nTwo rounds of negotiations were held in October and November. On the sixth round of talks in November, Britain formally conceded its intentions of either maintaining a British administration in Hong Kong or seeking some form of co-administration with the PRC, and showed its sincerity in discussing PRC's proposal on the 1997 issue. Obstacles were cleared.\\r\\nSimon Keswick, chairman of Jardine Matheson & Co., said they were not pulling out of Hong Kong, but a new holding company would be established in Bermuda instead.[21] The PRC took this as yet another plot by the British. The Hong Kong government explained that it had been informed about the move only a few days before the announcement. The government would not and could not stop the company from making a business decision.\\r\\nJust as the atmosphere of the talks was becoming cordial, members of the Legislative Council of Hong Kong felt impatient at the long-running secrecy over the progress of Sino-British talks on the Hong Kong issue. A motion, tabled by legislator Roger Lobo, declared \\"This Council deems it essential that any proposals for the future of Hong Kong should be debated in this Council before agreement is reached\\", was passed unanimously.[22]\\r\\nThe PRC attacked the motion furiously, referring to it as \\"somebody's attempt to play the three-legged stool trick again\\".[23] At length, the PRC and Britain initiated the Joint Declaration on the question of Hong Kong's future in Beijing. Zhou Nan, the then PRC Deputy Foreign Minister and leader of the negotiation team, and Sir Richard Evans, British Ambassador to Beijing and leader of the team, signed respectively on behalf of the two governments.[24]\\r\\nThe Sino-British Joint Declaration was signed by the Prime Ministers of the People's Republic of China and the United Kingdom governments on 19 December 1984 in Beijing. The Declaration entered into force with the exchange of instruments of ratification on 27 May 1985 and was registered by the People's Republic of China and United Kingdom governments at the United Nations on 12 June 1985.\\r\\nIn the Joint Declaration, the People's Republic of China Government stated that it had decided to resume the exercise of sovereignty over Hong Kong (including Hong Kong Island, Kowloon, and the New Territories) with effect from 1 July 1997 and the United Kingdom Government declared that it would restore Hong Kong to the PRC with effect from 1 July 1997. In the document, the People's Republic of China Government also declared its basic policies regarding Hong Kong.\\r\\nIn accordance with the \\"One Country, Two Systems\\" principle agreed between the United Kingdom and the People's Republic of China, the socialist system of the People's Republic of China would not be practised in the Hong Kong Special Administrative Region (HKSAR), and Hong Kong's previous capitalist system and its way of life would remain unchanged for a period of 50 years. This would have left Hong Kong unchanged until 2047.\\r\\nThe Joint Declaration provided that these basic policies should be stipulated in the Hong Kong Basic Law. The ceremony of the signing of the Sino-British Joint Declaration took place at 18:00, 19 December 1984 at the Western Main Chamber of the Great Hall of the People. The Hong Kong and Macao Affairs Office at first proposed a list of 60-80 Hong Kong people to attend the ceremony. The number was finally extended to 101.\\r\\nThe list included Hong Kong government officials, members of the Legislative and Executive Councils, chairmen of the Hongkong and Shanghai Banking Corporation and Standard Chartered Bank, prominent businessmen such as Li Ka-shing, Pao Yue-kong and Fok Ying-tung, and also Martin Lee Chu-ming and Szeto Wah.\\r\\nThe Basic Law was drafted by a Drafting Committee composed of members from both Hong Kong and mainland China. A Basic Law Consultative Committee formed purely by Hong Kong people was established in 1985 to canvas views in Hong Kong on the drafts.\\r\\nThe first draft was published in April 1988, followed by a five-month public consultation exercise. The second draft was published in February 1989, and the subsequent consultation period ended in October 1989.\\r\\nThe Basic Law was formally promulgated on 4 April 1990 by the NPC, together with the designs for the flag and emblem of the HKSAR. Some members of the Basic Law drafting committee were ousted by Beijing following the 4 June 1989 Tiananmen Square protests, after voicing views supporting the students.\\r\\nThe Basic Law was said to be a mini-constitution drafted with the participation of Hong Kong people. The political system had been the most controversial issue in the drafting of the Basic Law. The special issue sub-group adopted the political model put forward by Louis Cha. This \\"mainstream\\" proposal was criticised for being too conservative.[citation needed]\\r\\nAccording to Clauses 158 and 159 of the Basic Law, powers of interpretation and amendment of the Basic Law are vested in the Standing Committee of the National People's Congress and the National People's Congress, respectively. Hong Kong's people have limited influence.\\r\\nAfter the Tiananmen Square protests of 1989, the Executive Councillors and the Legislative Councillors unexpectedly held an urgent meeting, in which they agreed unanimously that the British Government should give the people of Hong Kong the right of abode in the United Kingdom.[25]\\r\\nMore than 10,000 Hong Kong residents rushed to Central in order to get an application form for residency in the United Kingdom. On the eve of the deadline, over 100,000 lined up overnight for a BN(O) application form. While mass migration did begin well before 1989, the event did lead to the peak migration year in 1992 with 66,000 leaving.[26]\\r\\nMany citizens were pessimistic towards the future of Hong Kong and the transfer of the region's sovereignty. A tide of emigration, which was to last for no less than five years, broke out. At its peak, citizenship of small countries, such as Tonga, was also in great demand.[27]\\r\\nSingapore, which also had a predominantly Chinese population, was another popular destination, with the country's Commission (now Consulate-General) being besieged by anxious Hong Kong residents.[28] By September 1989, 6000 applications for residency in Singapore had been approved by the Commission.[29]\\r\\nSome consul staff were suspended or arrested for their corrupt behaviour in granting immigration visas. In April 1997, the acting immigration officer at the US Consulate-General, James DeBates, was suspended after his wife was arrested for smuggling of Chinese migrants into the United States.[30] The previous year, his predecessor, Jerry Stuchiner, had been arrested for smuggling forged Honduran passports into the territory before being sentenced to 40 months in prison.[31]\\r\\nCanada (Vancouver and Toronto), United Kingdom (London, Glasgow, and Manchester), Australia (Sydney and Melbourne), and the United States (San Francisco and New York) were, by and large, the most popular destinations. The United Kingdom devised the British Nationality Selection Scheme, granting 50,000 families British citizenship under the British Nationality Act (Hong Kong) 1990.[32]\\r\\nVancouver was among the most popular destinations, earning the nickname of \\"Hongcouver\\".[33] Richmond, a suburb of Vancouver, was nicknamed \\"Little Hong Kong\\".[34] Other popular settlements are found in Auckland, New Zealand and Dublin, Ireland. All in all, from the start of the settlement of the negotiation in 1984 to 1997, nearly 1 million people emigrated; consequently, Hong Kong suffered serious loss of capital.[35]\\r\\nChris Patten became the last governor of Hong Kong. This was regarded as a turning point in Hong Kong's history. Unlike his predecessors, Patten was not a diplomat, but a career politician and former Member of Parliament. He introduced democratic reforms which pushed PRCÿBritish relations to a standstill and affected the negotiations for a smooth handover.\\r\\nPatten introduced a package of electoral reforms in the Legislative Council. These reforms proposed to enlarge the electorate, thus making voting in the Legislative Council more democratic. This move posed significant changes because Hong Kong citizens would have the power to make decisions regarding their future.\\r\\nThe handover ceremony was held at the new wing of the Hong Kong Convention and Exhibition Centre in Wan Chai on the night of 30 June 1997.\\r\\nThe principal British guest was Prince Charles, who read a farewell speech on behalf of the Queen. The newly elected Prime Minister, Tony Blair, the Foreign Secretary Robin Cook, the departing Governor Chris Patten and General Sir Charles Guthrie, Chief of the Defence Staff, also attended.\\r\\nRepresenting the People's Republic of China were the President, Jiang Zemin, the Premier, Li Peng, and the first Chief Executive Tung Chee-hwa. The event was broadcast around the world.[36][37]\\r\\nAfter the Tiananmen Square protests of 1989, the Hong Kong government proposed a grand \\"Rose Garden Project\\" to restore faith and solidarity among the residents.[124] As the construction of the new Hong Kong International Airport would extend well after the handover, Governor Wilson met PRC Premier Li Peng in Beijing to ease the mind of the PRC government.[125]\\r\\nThe communist press published stories that the project was an evil plan to bleed Hong Kong dry before the handover, leaving the territory in serious debt.[126] After three years of negotiations, Britain and the PRC finally reached an agreement over the construction of the new airport, and signed a Memorandum of Understanding.[127] Removing hills and reclaiming land, it took only a few years to construct the new airport.\\r\\nThe Walled City was originally a single fort built in the mid-19th century on the site of an earlier 17th century watch post on the Kowloon Peninsula of Hong Kong.[128] After the ceding of Hong Kong Island to Britain in 1842 (Treaty of Nanjing), Manchu Qing Dynasty authorities of China felt it necessary for them to establish a military and administrative post to rule the area and to check further British influence in the area.\\r\\nThe 1898 Convention which handed additional parts of Hong Kong (the New Territories) to Britain for 99 years excluded the Walled City, with a population of roughly 700. It stated that China could continue to keep troops there, so long as they did not interfere with Britain's temporary rule.\\r\\nBritain quickly went back on this unofficial part of the agreement, attacking Kowloon Walled City in 1899, only to find it deserted. They did nothing with it, or the outpost, and thus posed the question of Kowloon Walled City's ownership squarely up in the air. The outpost consisted of a yamen, as well as buildings which grew into low-lying, densely packed neighbourhoods from the 1890s to 1940s.\\r\\nThe enclave remained part of Chinese territory despite the turbulent events of the early 20th century that saw the fall of the Qing government, the establishment of the Republic of China and later, a Communist Chinese government (PRC).\\r\\nSquatters began to occupy the Walled City, resisting several attempts by Britain in 1948 to drive them out. The Walled City became a haven for criminals and drug addicts, as the Hong Kong Police had no right to enter the City and China refused maintainability. The 1949 foundation of the People's Republic of China added thousands of refugees to the population, many from Guangdong; by this time, Britain had had enough, and simply adopted a \\"hands-off\\" policy.\\r\\nA murder that occurred in Kowloon Walled City in 1959 set off a small diplomatic crisis, as the two nations each tried to get the other to accept responsibility for a vast tract of land now virtually ruled by anti-Manchurian Triads.\\r\\nAfter the Joint Declaration in 1984, the PRC allowed British authorities to demolish the City and resettle its inhabitants. The mutual decision to tear down the walled city was made in 1987.[129] The government spent up to HK$ 3 billion to resettle the residents and shops.\\r\\nSome residents were not satisfied with the compensation, and some even obstructed the demolition in every possible way.[130] Ultimately, everything was settled, and the Walled City became a park.[131]\\r\\nRennie's Mill got its name from a Canadian businessman named Alfred Herbert Rennie, who established a flour mill at Junk Bay. The business failed, and Rennie hanged himself there in 1908. The incident gave the Chinese name for the site Tiu Keng Leng (?X), meaning \\"Hanging (neck) Ridge\\". The name was later formally changed to similar-sounding Tiu King Leng (?X) because it was regarded as inauspicious.\\r\\nIn the 1950s the (British) Government of Hong Kong settled a considerable number of refugees from Chinaformer Nationalist soldiers and other Kuomintang supportersat Rennie's Mill, following the Chinese civil war. For many years the area was a Kuomintang enclave known as \\"Little Taiwan\\", with the flag of the Republic of China flying, its own school system and practically off-limits to the Royal Hong Kong Police Force.\\r\\nIn 1996 the Hong Kong government finally forcibly evicted Rennie's Mill's residents, ostensibly to make room for new town developments, as part of the Tseung Kwan O New Town, but widely understood to be a move to please the Communist Chinese government before Hong Kong reverted to Communist Chinese rule in 1997.\\r\\nBefore the eviction, Rennie's Mill could be reached by the winding, hilly and narrow Po Lam Road South. At that time, Rennie's Mill's only means of public transport were the routes 90 and 290 of KMB, which were operated by minibuses, and by water transport.\\r\\nThe Republic of China on Taiwan promulgated the Laws and Regulations Regarding Hong Kong & Macao Affairs on 2 April 1997 by Presidential Order, and the Executive Yuan on 19 June 1997 ordered the provisions pertaining to Hong Kong to take effect on 1 July 1997.[132]\\r\\nThe United StatesÿHong Kong Policy Act or more commonly known as the Hong Kong Policy Act (P.L no. 102-383m 106 Stat. 1448) is a 1992 act enacted by the United States Congress. It allows the United States to continue to treat Hong Kong separately from China for matters concerning trade export and economics control after the handover.[133]\\r\\nThe United States was represented by then Secretary of State Madeleine Albright at the Hong Kong handover ceremony.[134] However, she partially boycotted it in protest of China's dissolution of the democratically elected Hong Kong legislature.[135]\\r\\nScholars have begun to study the complexities of the transfer as shown in the popular media, such as films, television and video and online games. For example, Hong Kong director Fruit Chan made a sci-fi thriller The Midnight After (2014) that stressed the sense of loss and alienation represented by survivors in an apocalyptic Hong Kong. Chan infuses a political agenda in the film by playing on Hong Kongers' collective anxiety towards communist China.[136] Yiman Wang has argued that America has viewed China through the prisms of films from Shanghai and Hong Kong, with a recent emphasis on futuristic disaster films set in Hong Kong after the transfer goes awry.[137]","input":"When did hong kong become part of china?"},{"output":"a rivalry between its two title characters, Tom and Jerry","context":"","input":"What is the theme of tom and jerry?"},{"output":"with the stems of the leaves pointing to the wearers right","context":"An oak leaf cluster is a miniature bronze or silver twig of four oak leaves with three acorns on the stem that is authorized by the United States Armed Forces as a ribbon device for a specific set of decorations and awards of the United States Army, Air Force, and Department of Defense to denote subsequent decorations and awards.[2]\\r\\nThe bronze oak leaf cluster represents one additional award, while the silver oak leaf cluster is worn in lieu of five bronze oak leaf clusters.[3]\\r\\n\\r\\n\\r\\nOak leaf clusters are worn with the stems of the leaves pointing to the wearers right. For medals, ?13?32 inch oak leaf clusters are worn on the medal's suspension ribbon. If four oak leaf clusters are worn on the suspension ribbon, the fourth is placed above the middle one in the row of three.[3] For service ribbons, ?5?16 inch oak leaf clusters are worn, with no more than four oak leaf clusters being worn side by side.[3][4] If the number of authorized oak leaf clusters exceeds four, a second ribbon is authorized for wear and is worn after the first ribbon.[3] The second ribbon counts as one additional award, after which more leaf clusters may be added to the second ribbon. If future awards reduce the number of oak leaf clusters worn on the first ribbon due to bronze oak leaf clusters being replaced by a silver oak leaf cluster, the second ribbon is removed and the appropriate number of devices is placed on the first ribbon.[3]\\r\\nThe following are examples of the first through twenty-first awards of an Army Commendation Medal with the bronze and silver oak leaf clusters:\\r\\nOak leaf clusters may be worn on United States Army, Air Force, and Department of Defense decorations and awards presented to members of the seven uniformed services: the Army, Navy, Marine Corps, Air Force, Coast Guard, Public Health Service, and the NOAA Commissioned Corps.\\r\\nExcept for the Air Medal, unique decorations and awards issued by the Army or Air Force, and those decorations and awards issued by the Department of Defense, the other uniformed services use 5/16 inch stars to indicate subsequent personal decorations only; a gold 5/16 inch star is equivalent to a bronze oak leaf cluster, while a silver 5/16 inch star is equivalent to a silver oak leaf cluster.[11] While the Air Force uses oak leaf clusters for the Air Medal, since the Vietnam War, the Army has used 3/16 inch bronze Arabic numerals to denote subsequent awards, in which case the ribbon denotes the first award and numerals starting with the numeral \\"2\\" denote additional awards.[12]\\r\\nIn other nations, oak leaf clusters are also used as symbols for various awards and decorations. In Germany, the German oak is the national tree of Germany, thus oak leaves are a prominent symbol on most German military orders. During World War II, the Knight's Cross of the German Iron Cross could be awarded with the additional distinction of oak leaves (mit Eichenlaub). Of the 7,313 awards of the Knight's Cross, only 882 received oak leaves. After World War II, Iron Crosses awarded previously could be worn by the recipient provided the swastika was replaced by oak leaves. The Bundeswehr awards the Cross of Honour for Bravery for extraordinary bravery. The Cross of Honour for Bravery differs from the Badge of Honour by an adornment in the shape of stylized double oak leaves.[13] Furthermore, it was featured on the Pfennig in Germany and since the introduction of the euro in 2001 it is used on the obverse side of the German euro coinage. In earlier times, the Pour le Mrite, the highest military order in the Kingdom of Prussia, could also be awarded with oak leaves. A civil version of the order, for accomplishments in the arts and sciences, still exists in the Federal Republic of Germany.\\r\\nIn Commonwealth countries, a bronze oak leaf signifies a Mention in Despatches, and is worn as a gallantry award in its own right, rather than to signify multiple instances of campaign service. The Commonwealth equivalent of a United States oak leaf cluster is a medal bar worn with a campaign medal.","input":"Which way does the oak leaf cluster face?"},{"output":"Thousands","context":"8,857 dead in Nepal and 8,964 in total[6][7] 21,952 injured[6]\\r\\nThe April 2015 Nepal earthquake (also known as the Gorkha earthquake)[5][8] killed nearly 9,000 people and injured nearly 22,000. It occurred at 11:56 Nepal Standard Time on 25 April, with a magnitude of 7.8Mw[1] or 8.1Ms[9] and a maximum Mercalli Intensity of IX (Violent). Its epicenter was east of Gorkha District at Barpak, Gorkha, and its hypocenter was at a depth of approximately 8.2?km (5.1?mi).[1] It was the worst natural disaster to strike Nepal since the 1934 NepalÿBihar earthquake.[10][11][12] The ground motion recorded in Kathmandu valley was of low frequency which, along with its occurrence at an hour where many people in rural areas were working outdoors, decreased the loss of property and human life.[13]\\r\\nThe earthquake triggered an avalanche on Mount Everest, killing 21,[14] making April 25, 2015 the deadliest day on the mountain in history.[15] The earthquake triggered another huge avalanche in the Langtang valley, where 250 people were reported missing.[16][17]\\r\\nHundreds of thousands of people were made homeless with entire villages flattened,[16][18][19] across many districts of the country. Centuries-old buildings were destroyed at UNESCO World Heritage Sites in the Kathmandu Valley, including some at the Kathmandu Durbar Square, the Patan Durbar Square, the Bhaktapur Durbar Square, the Changu Narayan Temple, the Boudhanath stupa and the Swayambhunath Stupa. Geophysicists and other experts had warned for decades that Nepal was vulnerable to a deadly earthquake, particularly because of its geology, urbanization, and architecture.[20][21]\\r\\nContinued aftershocks occurred throughout Nepal at the intervals of 15ÿ20 minutes, with one shock reaching a magnitude of 6.7 on 26 April at 12:54:08 NST.[4] The country also had a continued risk of landslides.[22]\\r\\nA major aftershock occurred on 12 May 2015 at 12:50 NST with a moment magnitude (Mw) of 7.3.[23] The epicenter was near the Chinese border between the capital of Kathmandu and Mt. Everest.[24] More than 200 people were killed and over 2,500 were injured by this aftershock.[25]\\r\\nThe earthquake occurred on 25 April 2015 at 11:56?am NST (06:11:26 UTC) at a depth of approximately 15?km (9.3?mi) (which is considered shallow and therefore more damaging than quakes that originate deeper in the ground),[26] with its epicentre approximately 34?km (21?mi) east-southeast of Lamjung, Nepal, lasting approximately fifty seconds.[27] The earthquake was initially reported as 7.5 Mw by the United States Geological Survey (USGS) before it was quickly upgraded to 7.8 Mw. The China Earthquake Networks Center (CENC) reported the earthquake's magnitude to be 8.1 Ms. The India Meteorological Department (IMD) said two powerful quakes were registered in Nepal at 06:11 UTC and 06:45 UTC. The first quake measured 7.8 Mw and its epicenter was identified at a distance of 80?km to the northwest of Kathmandu, the capital of Nepal. Bharatpur was the nearest major city to the main earthquake, 53?km (33?mi) from the epicenter. The second earthquake was somewhat less powerful at 6.6 Mw. It occurred 65?km (40?mi) east of Kathmandu and its seismic focus lay at a depth of 10?km (6.2?mi) below the earth's surface. Over thirty-eight aftershocks of magnitude 4.5 Mw or greater occurred in the day following the initial earthquake, including the one of magnitude 6.8 Mw.[28]\\r\\nAccording to the USGS, the earthquake was caused by a sudden thrust, or release of built-up stress, along the major fault line where the Indian Plate, carrying India, is slowly diving underneath the Eurasian Plate, carrying much of Europe and Asia.[26] Kathmandu, situated on a block of crust approximately 120?km (74 miles) wide and 60?km (37 miles) long, reportedly shifted 3?m (10?ft) to the south in a matter of just 30 seconds.[29]\\r\\nThe risk of a large earthquake was well known beforehand. In 2013, in an interview with seismologist Vinod Kumar Gaur, The Hindu quoted him as saying, \\"Calculations show that there is sufficient accumulated energy [in the Main Frontal Thrust], now to produce an 8 magnitude earthquake. I cannot say when. It may not happen tomorrow, but it could possibly happen sometime this century, or wait longer to produce a much larger one.\\"[30] According to Brian Tucker, founder of a nonprofit organization devoted to reducing casualties from natural disasters, some government officials had expressed confidence that such an earthquake would not occur again. Tucker recounted a conversation he had had with a government official in the 1990s who said, \\"We don't have to worry about earthquakes anymore, because we already had an earthquake\\"; the previous earthquake to which he referred occurred in 1934.[31]\\r\\nNepal lies towards the southern limit of the diffuse collisional boundary where the Indian Plate underthrusts the Eurasian Plate,[32][33] occupying the central sector of the Himalayan arc, nearly one-third of the 2,400?km (1,500?mi) long Himalayas. Geologically, the Nepal Himalayas are sub-divided into five tectonic zones from north to south and, east to west and almost parallel to sub-parallel.[34] These five distinct morpho-geotectonic zones are: (1) Terai Plain, (2) Sub Himalaya (Sivalik Range), (3) Lesser Himalaya (Mahabharat Range and mid valleys), (4) Higher Himalaya, and (5) Inner Himalaya (Tibetan Tethys).[35] Each of these zones is clearly identified by their morphological, geological, and tectonic features.[35]\\r\\nThe convergence rate between the plates in central Nepal is about 45?mm (1.8?in) per year. The location, magnitude, and focal mechanism of the earthquake suggest that it was caused by a slip along the Main Frontal Thrust.[1][36]\\r\\nThe earthquake's effects were amplified in Kathmandu as it sits on the Kathmandu Basin, which contains up to 600?m (2,000?ft) of sedimentary rocks, representing the infilling of a lake.[37]\\r\\nBased on a study published in 2014, of the Main Frontal Thrust, on average a great earthquake occurs every 750?I?140 and 870?I?350?years in the east Nepal region.[38] A study from 2015 found a 700-year delay between earthquakes in the region. The study also suggests that because of tectonic stress buildup, the earthquake from 1934 in Nepal and the 2015 quake are connected, following a historic earthquake pattern.[39] A 2016 study on historical great (M  8) earthquake pairs and cycles found that associated great earthquakes are likely to occur in the West China region through the 2020s.[40]\\r\\nAccording to \\"Did You Feel It?\\" (DYFI?) responses on the USGS website, the intensity in small parts of Kathmandu was IX (Violent).[1] In most of Kathmandu the intensity was VI, as evidenced by the numerous undamaged water towers installed on top of undamaged multi story buildings. Tremors were felt in the neighboring Indian states of Bihar, Uttar Pradesh, Assam, West Bengal, Sikkim, Jharkhand, Uttarakhand, Gujarat, in the National capital region around New Delhi and as far south as Karnataka.[41] Damage was extensive in northern Bihar and minor damage was also reported from parts of Odisha.[41] Shaking was felt in high-rise buildings as far as Kochi in the southern state of Kerala.[41] The intensity in Patna was V (Moderate).[42][not in citation given] The intensity was IV (Light) in Dhaka, Bangladesh.[1] The earthquake was also experienced across southwestern China, ranging from the Tibet Autonomous Region to Chengdu, which is 1,900?km (1,200?mi) away from the epicenter.[43] Tremors were felt in Pakistan[44] and Bhutan.[1][41]\\r\\nA series of aftershocks began immediately after the mainshock, at intervals of 15ÿ30 minutes, with one aftershock reaching 6.6Mw within 34 minutes of the initial quake. A major aftershock of magnitude 6.9 Mw occurred on 26 April 2015 in the same region at 12:54 NST (07:08 UTC), with an epicenter located about 17?km (11?mi) south of Kodari, Nepal.[44][45] The aftershock caused fresh avalanches on Mount Everest and was felt in many places in northern India including Kolkata, Siliguri, Jalpaiguri, and Assam.[46] The aftershock caused a landslide on the Koshi Highway which blocked the section of the road between Bhedetar and Mulghat.[47]\\r\\nA model of GeoGateway, based on a United States Geological Survey mechanism of a near-horizontal fault as well as location of aftershocks showed that the fault had an 11 dip towards the north, striking at 295, 50?km (31?mi) wide, 150?km (93?mi) long, and had a dip slip of 3?m (9.8?ft).[48] The USGS says the aftershock registered at a shallow depth of 10?km (6.2?mi).[46]\\r\\nAssuming that 25 April earthquake was the largest event in this seismic episode, Nepal could expect more than 30 aftershocks greater than magnitude 5 over the following month.[49] As of 24 May 2016, 459 aftershocks had occurred with different epicenters and magnitudes equal to or above 4 Mw (out of which 51 aftershocks are equal to or above 5 Mw and 5 aftershocks above 6 Mw) and more than 20,000 aftershocks less than 4 Mw.[5]\\r\\nA second major earthquake occurred on 12 May 2015 at 12:50 NST with a moment magnitude (Mw) of 7.3Mw 18?km (11?mi) southeast of Kodari. The epicenter was near the Chinese border between the capital of Kathmandu and Mt. Everest. It struck at the depth of 18.5?km (11.5 miles). This earthquake occurred along the same fault as the original magnitude 7.8 earthquake of 25 April but further to the east. As such, it is considered to be an aftershock of the 25 April quake.[50] Tremors were also felt in northern parts of India including Bihar, Uttar Pradesh, West Bengal and other North-Indian States.[51][52][53][54] At least 153 died in Nepal as a result of the aftershock and about 2,500 were injured. 62 others died in India, two in Bangladesh, and one in China.[25][55] In Kathmandu the intensity was VI. People ran out of doors, or dove under a strong table (a behaviour more recommended), but water towers on top of multi story buildings remained intact.\\r\\nDisastrous events in very poor and politically paralyzed nations such as Nepal often become a long drawn out chain of events, in that one disaster feeds into another for years or even decades upon end. The aftereffects from the earthquake have subsequent effects on myriad of seemingly unrelated aspects: human trafficking, labour cost and availability, rental and property cost burdens, urbanization, private and public debt burdens, mental health, politics, tourism, disease, and damage to the healthcare system.[56]\\r\\nSome disasters that came with the monsoon season were suspected to be related to the earthquake. There was a landslip on 11 June that claimed 53 lives.[57] Meanwhile, a glacial lake had burst in particularly hard hit Solukhumbhu district.[58] Whether or not the quake had contributed to such events is often unknown and unresearched, but certainly possible.\\r\\nThe earthquake killed more than 8,800 people in Nepal[6][85][86] and injured nearly three times as many. The rural death toll may have been minimized by the fact that most villagers were outdoors working when the quake hit.[87] As of 15 May, 6,271 people, including 1,700 from the 12 May aftershock, were still receiving treatment for their injuries.[55] Nearly 3.5?million people were left homeless.[59]\\r\\nThe example of this earthquake shows that loss calculations for hypothetical likely future earthquakes can be reasonably reliable. In 2005, the expected numbers of fatalities due to a hypothetical scenario earthquake near Kathmandu for M8.1 was published.[88] The fatalities at that time were estimated between 21,000 and 42,000. A M7.8 earthquake happened on 25 April 2015 near Kathmandu. It killed only about 10,000 people because it was Saturday and the children were not in the collapsing school buildings.[citation needed] The original estimate was correct within a factor of 2.5 and would have been exactly correct, had it not been for the lucky break children got due to Saturday being a holiday.\\r\\nAfter the rupture area of the Kathmandu 2015 earthquake had been derived[89] and the intensities of shaking had been mapped,[90] a line source model for losses could be constructed with energy being radiated along the entire rupture.[91] The fatalities estimated in this way by QLARM agree with those reported in the end. The figure shows reports of fatalities as a function of time. News reports significantly underestimated the actual numbers of fatalities for several days.\\r\\nThe Himalayan Times reported that as many as 20,000 foreign nationals may have been visiting Nepal at the time of the earthquake, although reports of foreign deaths were relatively low.[92]\\r\\nA total of 78 deaths were reported in India - including 58 in Bihar, 16 in Uttar Pradesh, 3 in West Bengal and 1 in Rajasthan.[60]\\r\\n27 dead and 4 missing, all from the Tibet Autonomous Region.[61]\\r\\n4 dead.[62]\\r\\nThis earthquake caused avalanches on Mount Everest. At least 19 died,[93] with at least 120 others injured or missing.[93]\\r\\nIn the Langtang valley located in Langtang National Park, 329 people were reported missing after an avalanche hit the village of Ghodatabela[94][95] and the village of Langtang. The avalanche was estimated to have been two to three kilometres wide. Ghodatabela was an area popular on the Langtang trekking route.[96] The village of Langtang was destroyed by the avalanche. Smaller settlements on the outskirts of Langtang were buried during the earthquake, such as Chyamki, Thangsyap, and Mundu. Twelve locals and two foreigners were believed to have survived. Smaller landslides occurred in the Trishuli River Valley with reports of significant damage at Mailung, Simle, and Archale.[17][97][98] On 4 May it was announced that 52 bodies had been found in the Langtang area, of which seven were of foreigners.[99]\\r\\nAccording to geological models, the frequency and intensity of future landslides in the Langtang Valley is due to increase in the coming decades.[100] This is attributable directly to the effect of the earthquake, which caused widespread fracturing in the grounds of the Langtang area.[101]\\r\\nThousands of houses were destroyed across many districts of the country, with entire villages flattened, especially those near the epicenter.[16][18][19]\\r\\nThe Tribhuvan International Airport serving Kathmandu was closed immediately after the quake, but was re-opened later in the day for relief operations and, later, for some commercial flights.[102] It subsequently shut down operations sporadically due to aftershocks,[103] and on 3 May was closed temporarily to the largest planes for fear of runway damage.[104] During strong aftershocks, the airport opened all boarding-lounge exit doors onto the tarmac, allowing travelers who were waiting post security and immigration to flee to the open spaces of the runway tarmac. Many travelers remained outside as planes were delayed and the airport swelled to capacity. The airport facilities suffered damage and there was no running water or operating toilets for travelers waiting in the airport lounges. Few airport workers were at their posts; most were killed in the earthquake or had to deal with its aftereffects.[105]\\r\\nFlights resumed from Pokhara, to the west of the epicentre, on 27 April.[106]\\r\\nSeveral of the churches in the Kathmandu valley were destroyed. As Saturday is the principal day of Christian worship in Nepal, 500 people were reported to have died in the collapses.[107][108]\\r\\nSeveral temples on Kathmandu Durbar Square, a UNESCO World Heritage Site, collapsed,[27] as did the Dharahara tower, built in 1832; the collapse of the latter structure killed at least 180 people,[109][110][111][112] Manakamana Temple in Gorkha, previously damaged in an earlier quake, tilted several inches further. The northern side of Janaki Mandir in Janakpur was reported to have been damaged.[113] Several temples, including Kasthamandap, Panchtale temple, the top levels of the nine-story Basantapur Durbar, the Dasa Avtar temple and two dewals located behind the Shiva Parvati temple were demolished by the quake. Some other monuments including the Taleju Bhawani Temple partially collapsed.[114][115]\\r\\nThe top of the Jaya Bageshwari Temple in Gaushala and some parts of the Pashupatinath Temple, Swyambhunath, Boudhanath Stupa, Ratna Mandir, inside Rani Pokhari, and Durbar High School have been destroyed.[116]\\r\\nIn Patan, the Char Narayan Mandir, the statue of Yog Narendra Malla, a pati inside Patan Durbar Square, the Taleju Temple, the Hari Shankar, Uma Maheshwar Temple and the Machhindranath Temple in Bungamati were destroyed. In Tripureshwar, the Kal Mochan Ghat, a temple inspired by Mughal architecture, was completely destroyed and the nearby Tripura Sundari also suffered significant damage. In Bhaktapur, several monuments, including the Phasi Deva temple, the Chardham temple and the 17th century Vatsala Durga Temple were fully or partially destroyed.[116]\\r\\nOutside the Valley, the Manakamana Temple in Gorkha, the Gorkha Durbar, the Palanchok Bhagwati, in Kabhrepalanchok District, the Rani Mahal in Palpa District, the Churiyamai in Makwanpur District, the Dolakha Bhimsensthan in Dolakha District, and the Nuwakot Durbar suffered varying degrees of damage. Historian Prushottam Lochan Shrestha stated, \\"We have lost most of the monuments that had been designated as World Heritage Sites in Kathmandu, Bhaktapur and Lalitpur District, Nepal. They cannot be restored to their original states.\\"[116] The northeastern parts of India also received major damage. Heavy shocks were felt in the states of Uttrakhand, Uttar Pradesh, West Bengal and others. Huge damage was caused to the property and the lives of the people.\\r\\nConcern was expressed that harvests could be reduced or lost this season as people affected by the earthquake would have only a short time to plant crops before the onset of the Monsoon rains.[117]\\r\\nNepal, with a total Gross Domestic Product of USD$19.921?billion (according to a 2012 estimate),[118] is one of Asia's poorest countries, and has little ability to fund a major reconstruction effort on its own.[119] Even before the quake, the Asian Development Bank estimated that it would need to spend about four times more than it currently does annually on infrastructure through to 2020 to attract investment.[119] The U.S. Geological Survey initially estimated economic losses from the tremor at 9 percent to 50 percent of gross domestic product, with a best guess of 35 percent. \\"It's too hard for now to tell the extent of the damage and the effect on Nepal's GDP\\", according to Hun Kim, an Asian Development Bank (ADB) official. The ADB said on the 28th that it would provide a USD$3?million grant to Nepal for immediate relief efforts, and up to USD$200?million for the first phase of rehabilitation.[119]\\r\\nRajiv Biswas, an economist at a Colorado-based consultancy, said that rebuilding the economy will need international effort over the next few years as it could \\"easily exceed\\" USD$5?billion, or about 20 percent of Nepal's gross domestic product.[119][120][not in citation given]\\r\\nIt was reported that the survivors were preyed upon by human traffickers involved in the supply of girls and women to the brothels of South Asia. These traffickers took advantage of the chaos that resulted from the aftermath of the earthquake.[121] The most affected were women from poor communities who lost their homes.[122]\\r\\nSingle women have had very little access to relief, according to a report by the Inter-party Women's Alliance (IPWA). The report also found that violence and rapes against women and minors has increased after the earthquake.[123] Additionally, the earthquake has significantly affected certain groups of people. Tibeto-Burman peoples were hardest hit as they tend to inhabit the higher slopes of mountains as opposed to the central valleys, are less educated and connected. All of these factors make them harder to access. According to a government survey, malnutrition in children has worsened considerably some 3 months after the quake, with the most undernourished being Tamang and Chepang peoples.[124] Before the quake, 41 percent of children under five were stunted, 29 percent were underweight and 11 percent were emaciated, according to the World Food Programme.[124]\\r\\nOn 3 May, the hashtag #GoHomeIndianMedia was trending worldwide on Twitter, condemning news covered by the Indian media as insensitive and inhumane to victims of the tragedy. The people of Nepal acknowledged the aid and effort put by the Indian armed forces, yet, at the same time, accused Indian news networks of carrying out \\"a public relations exercise\\" on behalf of the Indian government, of overemphasizing the role of the Indian Army, and of hogging space on relief planes where aid material or rescue or medical personnel could have been sent instead.[125] Indian users responded with the hashtags #SorryNepal and #DontComeBackIndianMedia.[126]\\r\\nThough a feared mass cholera outbreak failed to materialize (there were sporadic reports), other outbreaks have been reported. At least 13 people have died of scrub typhus while 240 people have been taken ill since the disease was first diagnosed in the country in August 2015 until Sept 2016.[127]\\r\\nAbout 90% of soldiers from the Nepalese Army were sent to the stricken areas in the aftermath of the earthquake under Operation Sankat Mochan, with volunteers mobilized from other parts of the country.[128] Rainfall and aftershocks were factors complicating the rescue efforts, with potential secondary effects like additional landslides and further building collapses being concerns. Impassable roads and damaged communications infrastructure posed substantial challenges to rescue efforts.[129] Survivors were found up to a week after the earthquake.[130][131][132]\\r\\nAs of 1 May international aid agencies like Mdecins Sans Frontires and the Red Cross were able to start medically evacuating the critically wounded by helicopter from outlying areas, initially cut-off from the capital city, Kathmandu,[18] and treating others in mobile and makeshift facilities.[133][134] There was concern about epidemics due to the shortage of clean water, the makeshift nature of living conditions and the lack of toilets.[135]\\r\\nEmergency workers were able to identify four men who had been trapped in rubble, and rescue them, using advanced heartbeat detection. The four men were trapped in up to ten feet of rubble in the village of Chautara, north of Kathmandu. An international team of rescuers from several countries using FINDER devices found two sets of men under two different collapsed buildings.[136]\\r\\nVolunteers used crisis mapping to help plan emergency aid work.[137] Local organization Kathmandu Living Labs helped coordinate local knowledge on the ground and collaborated with international crisis mapping and humanitarian organizations. Public volunteers from around the world participated in crowdmapping and added details into online maps.[138][139][140][141][142][143][144] Information was mapped from data input from social media, satellite pictures[145] and drones[137] of passable roads, collapsed houses, stranded, shelterless and starving people, who needed help, and from messages and contact details of people willing to help.[146] On-site volunteers verified these mapping details wherever they could to reduce errors.\\r\\nDigital mappers, through the Kathmandu Living Labs, were already charting the densely populated Kathmandu Valley, and then focused on earthquake relief. \\"They were doing an inventory in the poorer communities where they didn't have a very good sense of the quality of buildings,\\" says Cowan, whose students helped add Kathmandu's buildings and roads to OpenStreetMap. First responders, from Nepalese citizens to the Red Cross, the Nepal army and the United Nations used this data. The Nepal earthquake crisis mapping utilized experience gained and lessons learned about planning emergency aid work from earthquakes in Haiti and Indonesia.[147]\\r\\nIndia decided to donate $1?billion in cash and materials to Nepal. India's External Affairs Minister Sushma Swaraj said \\"I am happy to announce Government of India's pledge for Nepal's post-earthquake reconstruction of Nepali Rupees 10,000 crores, equivalent to one billion US dollars, one fourth of it as Grant.\\" The International Conference on Nepal's Reconstruction has been organised by the Nepalese government to raise funds for rebuilding the country.[148]\\r\\nReports are also coming in of sub-standard relief materials and inedible food being sent to Nepal by many of the foreign aid agencies.[149][150]\\r\\nA United States Marine Corps helicopter crashed on 12 May while involved in delivering relief supplies. The Bell UH-1Y Venom crashed at Charikot, roughly 45 miles (72 kilometers) east of Kathmandu. Two Nepalese soldiers and 6 American Marines died in the crash.[151]\\r\\nNeed-fulfillment application, Getmii, launched a special pilot version in partnership with the Red Cross to double daily blood donors at the Kathmandu donation center using the app.[152]\\r\\nImaging technologies such as satellites and smartphones, were instrumental to relief efforts in Nepal.[153] GLIMS, group of volunteer scientists from nine nations, were able to provide rapid, systematic mapping of the damaged area, allowing the investigation of earthquake-induced geo-hazard processes which provided information to relief and recovery officials on the same timeframe as those operations were occurring.[154]\\r\\nUNESCO and the Ministry of Culture began strengthening damaged monuments in danger of collapsing before the monsoon season. Subsequent restoration of collapsed structures, including historic houses is planned. Architectural drawings exist that provide plans for reconstruction. According to UNESCO, more than 30 monuments in the Kathmandu Valley collapsed in the quakes, and another 120 incurred partial damage.[155] Repair estimates are $160?million to restore 1,000 damaged and destroyed monasteries, temples, historic houses, and shrines across the country. The destruction is concentrated in the Kathmandu Valley. UNESCO designated seven groups of multi-ethnic monuments clustered in the valley as a single World Heritage Site, including Swayambhu, the Durbar squares of Kathmandu, Patan, and Bhaktapur, and the Hindu temples of Pashupatinath and Changu Narayan. Damaged in the quakes were the structures in the three Durbar squares, the temple of Changu Narayan, and the 1655 temple in Sankhu. Drones fly above cultural heritage sites to provide 3D images of the damage to use for planning repairs.[156]\\r\\nUNICEF appealed for donations, as close to 1.7?million children had been driven out into the open, and were in desperate need of drinking water, psychological counsel, temporary shelters, sanitation and protection from disease outbreak. It distributed water, tents, hygiene kits, water purification tablets and buckets.[157] Numerous other organizations provided similar support.[158]\\r\\nIndia was the first to respond within hours, being Nepal's immediate neighbour,[159] with Operation Maitri which provided rescue and relief by its armed forces. It also evacuated its own and other countries' stranded nationals. India has been the largest aid donor to Nepal following the earthquake with a billion dollar support apart from other non-monetary reliefs extended.[160] The United States, China and other nations have provided helicopters as requested by the Nepalese government.[161][162]\\r\\nOn 26 April 2015, international aid agencies and governments mobilized rescue workers and aid for the earthquake. They faced challenges in both getting assistance to Nepal and ferrying people to remote areas as the country had few helicopters.[163][164] Relief efforts were also hampered by Nepalese government insistence on routing aid through the Prime Minister's Disaster Relief Fund and its National Emergency Operation Center. After concerns were raised, it was clarified that \\"Non-profits\\" or NGOs already in the country could continue receiving aid directly and bypass the official fund.[163][165] Aid mismatch and supply of \\"leftovers\\" by donors,[166] aid diversion in Nepal,[167] mistrust over control of the distribution of funds and supplies,[168][169][170] congestion and customs delays at Kathmandu's airport and border check posts were also reported.[171][172] On 3 May 2015, restrictions were placed on heavy aircraft flying in aid supplies after new cracks were noticed on the runway at the Tribhuvan airport (KTM), Nepal's only wide-body jet airport.[173][174][175]\\r\\nThe list below gives a break-up of pledged donations, by each nation, along with aid in kind, delivered immediately.[176]\\r\\n? $3,568,500 to the United Nations relief effort, $793,000 to the World Health Organization, $793,000 to the Australian Red Cross, $396,500 to the RedR Australia relief organisation, $3.172?million for other Australian NGOs.\\r\\n? Two Boeing C-17 aircraft carrying 15 tons of Australian aid and two RAAF aero medical evacuation teams.\\r\\n? The Government of Tasmania donated $7,930 to Rotary Tasmania's Nepal Earthquake Appeal.\\r\\n? 2 humanitarian experts and a crisis-response team initially.\\r\\n? 70 defence personnel, immigration and other federal government officials to distribute aid and help with evacuation efforts.\\r\\n? BAF Lockheed C-130B aircraft with 10 tonnes of relief materials ÿ tents, dry food, water, blankets, etc.\\r\\n? Four cargo trucks carrying approximately 25 tonnes of essential relief materials for earthquake victims in Nepal left Dhaka. The cargoes will travel through Banglabandh-Fulbari-Panitanki-Kakarbhitta land route. The relief materials include 3000 cartons (12 tonne) of dry food and fruit juice donated by PRAN, and 5000 pieces of blankets donated by Brac, according to a press release of the Embassy of Nepal in Bangladesh.\\r\\n? Bangladesh will provide at least one hundred thousand tons of rice and other relief materials including drinking water to help the earthquake victims in Nepal.\\r\\n? 8 tons of baby food\\r\\n? Over 100 tons of medical supplies\\r\\n? 75,000 vials of insulin\\r\\n? Over 200 tons of water\\r\\n? 100,000 bottles of water every day from the Indian Railways\\r\\n? Hundreds of tons of food and dry rations\\r\\n? 43 tons of relief material\\r\\n? 10 tons of blankets\\r\\n? Several tons of stretchers, tents\\r\\n? A reverse osmosis (RO) plant\\r\\n? Oxygen regenerators & cylinders\\r\\n? 345 tons of relief material, dry food and essential medicines from the state governments of Bihar and Uttar Pradesh\\r\\n? 16 National Disaster Response Force teams, over 1,000 personnel, search-&-rescue dogs\\r\\n? Hundreds of retired Indian Gorkha soldiers of the Indian Army\\r\\n? Hundreds of Indian Army and Indian Air Force personnel\\r\\n? Military task forces headquartered in Kathmandu and Barpak\\r\\n? Relief sorties by Ilyushin Il-76, C-130J Hercules, C-17 Globemaster, Antonov An-32 aircraft\\r\\n? Civilian aircraft\\r\\n? Helicopters ÿ Mi-17, Cheetah, HAL Dhruv ALH\\r\\n? Unmanned Aerial Vehicles (UAVs)\\r\\n? 18 member medical team\\r\\n? 3 field hospitals\\r\\n? 2 mobile teams of specialist doctors\\r\\n? 41 member medical team from the state of Rajasthan\\r\\n? Indian Air Force rapid action medical team\\r\\n? 45 bed hospital at Lagankhel\\r\\n? Light vehicles\\r\\n? Earth moving equipment\\r\\n? 18 Indian Army Engineer Task Forces (Indian Army Corps of Engineers)\\r\\n? Indian Oil Corporation team\\r\\n? PowerGrid Corporation of India engineers\\r\\n? 36+ vehicles ÿ ambulances and water tankers ÿ from the Sashastra Seema Bal\\r\\n? 39 member Indian Army team deployed at the Everest Base Camp to search for, rescue and assist climbers\\r\\nEvacuation of over 20,000 Indian citizens and hundreds of foreign nationals by air and road\\r\\n?Four Lockheed C-130 planes\\r\\n?30-bed hospital\\r\\n?2,000 military meals\\r\\n?600 blankets\\r\\n?200 tents\\r\\nother assorted relief items\\r\\n? 1000 tents\\r\\n? Food packages for 230 families (Rice 10?kg, bean 1?kg, salt 1?kg, oil, Nepal noodle 1?kg, 10 vitamin tablets and etc. per a package)\\r\\n? 2.4 tons of rice, 320 bottles of vegetable oil, salts for 740 villagers","input":"How many houses were destroyed in nepal earthquake?"},{"output":"Shinsegae","context":"Shinsegae (Korean: ???, KRX: 004170) is a South Korean department store franchise, along with several other businesses, headquartered in Seoul, South Korea. The name Shinsegae literally means \\"New World\\" in Korean. Its flagship store in Centum City, Busan, is the world's largest department store, surpassing Macy's flagship Herald Square in New York City in 2009.[2]\\r\\nShinsegae was originally part of the Samsung Group chaebol, separated in the 1990s from the Samsung Group along with CJ Group (Food/Chemicals/Entertainment), Saehan Group (Electronic Media/Apparel/Textiles), and the Hansol Group (Paper/Telecom). The chairman Lee Myung-hee is the daughter of Samsung founder Lee Byung-chull and younger sister of the chairman of Samsung Electronics Lee Kun-hee.\\r\\nThe group owns the brands Shinsegae and E-Mart, and is in direct competition with Lotte Shopping and Hyundai Department Store Group. Currently it is the largest retailer in South Korea.[3]\\r\\nShinsegae is also famous for its long history. The main branch of Shinsegae is the oldest department store in Korea. The main building of the store was opened in 1930 as the Gyeongseong branch of Mitsukoshi, a Japanese department store franchise; Korea was occupied by the Japanese Empire at the time. The store was acquired in 1945 by the late founder of Samsung group, Lee Byung-chull, and renamed Donghwa Department Store. After the Korean War (1950ÿ1953) began, it was used for several years as a post exchange by the American army. In 1963, the store was given the name Shinsegae.[4] The old building is currently used as a luxury shopping venue.\\r\\nShinsegae was the first credit card company in South Korea. They issued its own charge card from 1967 to 2000. In 2000, Shinsegae sold its credit card division to KorAm Bank, which was later acquired by Citibank Korea.\\r\\n\\r\\n\\r\\nShinsegae also has a small branch in Incheon International Airport, and a supermarket in Dogok-dong, Gangnam-gu, Seoul.\\r\\nShinsegae launched the Shinsegae Style Market, a smaller shopping mall mainly aimed at young customers, in 2010. Despite its name, the mall is managed by Shinsegae's subsidiary E-Mart.\\r\\nE-Mart (???) is a subsidiary of Shinsegae and a large discount store chain founded in South Korea, having stores in China and Korea. Domestically, E-Mart is the biggest discount store chain followed by Home Plus, owned by Samsung and Tesco, and Lotte Mart.\\r\\nIn late May 2006, Shinsegae revealed plans to buy all 16 of the Wal-Mart stores in Korea.[7] All of the country's Wal-Mart outlets were re-branded as E-Mart on October 2006. Wal-Mart exited the Korean market soon after.\\r\\nShinsegae spun off its E-Mart department into a separate corporation (KRX: 139480) in 2012. The shopping mall was acquired by E-Mart in January 2014.\\r\\nShinsegae banned commercial images of actress Go Hyun-jung (???) from their department stores following her divorce from vice chairman and CEO Chung Yong-jin.[8]","input":"Where is the biggest store in the world?"},{"output":"tactical logistics","context":"Sergeant First Class is a military rank in some militaries and other uniformed organizations around the world, typically that of a senior non-commissioned officer.\\r\\n\\r\\n\\r\\nSergeant First Class (SFC) is the seventh enlisted rank (E-7) in the U.S. Army, ranking above staff sergeant (E-6) and below master sergeant and first sergeant (E-8), and is the first non-commissioned officer rank designated as a senior non-commissioned officer (SNCO).\\r\\nA sergeant first class is typically assigned as a platoon sergeant at the company level or battalion operations non-commissioned officer in charge at the battalion level, but may also hold other positions depending on the type of unit. In a combat arms role, a sergeant first class is typically in charge of from 18 soldiers and four tanks in an armor platoon to 40 soldiers in a rifle platoon. A sergeant first class' primary responsibilities are tactical logistics, tactical casualty evacuations, and serving as the senior tactical adviser to the platoon leader. Sergeant first class replaced the rank of technical sergeant in 1948. (However, the U.S. Air Force, which separated from the Army in 1947, retained the rank of technical sergeant and the U.S. Marine Corps had the rank of technical sergeant until 1959).\\r\\nA sergeant first class is addressed as \\"sergeant\\" except in certain situations, such as field artillery units, in which a sergeant first class serving as platoon sergeant is commonly referred to as \\"Smoke\\". If a sergeant first class is appointed to fill the role of first sergeant, he or she is addressed as \\"First Sergeant\\". Typically a sergeant first class assigned on a manning document to fill a first sergeant role while being promotable to master sergeant can be frocked to first sergeant rank and hold the insignia due its position.[citation needed]\\r\\nSergeant first class is the first enlisted rank in the U.S. Army to be selected by the centralized promotion system. As such, it is considerably more difficult to achieve than the previous ranks. A sergeant first class is the first enlisted rank to be considered a senior non-commissioned officer, and a soldier achieving the rank gains not only prestige, but several benefits due to the position. For example, a sergeant first class cannot be demoted by standard non-judicial punishment. To demote a senior non-commissioned officer requires a court martial, or congressional approval, and normally only for offenses that carry heavy punishments.[citation needed]\\r\\nThe rank title of sergeant first class (SFC) existed in the Army from 1890 [1] until 1920 when it was eliminated in an army-wide simplification of enlisted ranks which had grown into a system containing 128 different rank insignia. The rank of SFC was used in several technical branches such as the Army Medical Department and in the Ordnance, Signal, and Quartermaster Corps and was equivalent to the field service ranks at the company/battery/troop \\"staff\\" NCO level, such as color sergeant, supply sergeant, or radio sergeant. The Army restored the rank of SFC in 1948 when it replaced technical sergeant. (Perrenot, 2009)[2]\\r\\nRav samal (??-???) or rasal (??\\"?), sergeant first class, is the lowest non-commissioned officer (?????) rank in the Israel Defense Forces (IDF), above samal rishon, ??? ????? (samara, ??\\"?), staff sergeant. Because the IDF is an integrated force, it has a unique rank sucture. IDF ranks are the lame in all services (army, navy, air force, etc.). The ranks are derived from those of the paramilitary Haganah developed to protect the Yishuv during the British Mandate of Palestine. This origin is reflected in the slightly-compacted IDF rank structure.[3]","input":"What does a sfc do in the army?"},{"output":"Batian","context":"\\r\\n\\r\\nMount Kenya is the highest mountain in Kenya and the second-highest in Africa, after Kilimanjaro.[4] The highest peaks of the mountain are Batian (5,199 metres (17,057?ft)), Nelion (5,188 metres (17,021?ft)) and Point Lenana (4,985 metres (16,355?ft)). Mount Kenya is located in the former Eastern province of Kenya, now the Eastern region of Kenya, about 16.5 kilometres (10.3?mi) south of the equator, around 150 kilometres (93?mi) north-northeast of the capital Nairobi.[4] Mount Kenya is the source of the name of the Republic of Kenya.\\r\\n\\r\\nMount Kenya is a stratovolcano created approximately 3 million years after the opening of the East African rift.[5] Before glaciation, it was 7,000?m (23,000?ft) high. It was covered by an ice cap for thousands of years. This has resulted in very eroded slopes and numerous valleys radiating from the centre.[6][7] There are currently 11 small glaciers. The forested slopes are an important source of water for much of Kenya.[8]\\r\\n\\r\\nThere are several vegetation bands from the base to the summit.[9] The lower slopes are covered by different types of forest. Many alpine species are endemic to Mount Kenya, such as the giant lobelias and senecios and a local subspecies of rock hyrax.[10] An area of 715?km2 (276?sq?mi) around the centre of the mountain was designated a National Park and listed as a UNESCO World Heritage Site in 1997.[11] The park receives over 16,000 visitors per year.[12][8]\\r\\n\\r\\nMount Kenya National Park, established in 1949, protects the region surrounding the mountain. Currently the national park is within the forest reserve which encircles it.[13] In April 1978 the area was designated a UNESCO Biosphere Reserve.[14] The national park and the forest reserve, combined, became a UNESCO World Heritage Site in 1997.[11]\\r\\n\\r\\nThe Government of Kenya had four reasons for creating a national park on and around Mount Kenya. These were the importance of tourism for the local and national economies, preserve an area of great scenic beauty, conserve the biodiversity within the park and to preserve the water catchment for the surrounding area.[8]\\r\\n\\r\\nKenyas government has announced a project to discourage animals from straying into small holdings surrounding the Park and devastating crops. The project will see the Park enclosed by an electric fence with five electrified strands and is expected to be completed by 2014. The fence will discharge an electric shock, but is not dangerous to humans or animals.[15]\\r\\n\\r\\nThe main ethnic groups living around Mount Kenya are Kikuyu, Ameru, Embu and Maasai. The first three are closely related. They all see the mountain as an important aspect of their cultures. All these cultures arrived in the Mount Kenya area in the last several hundred years.\\r\\n\\r\\nThe Kikuyu live on the southern and western sides of the mountain. They are agriculturalists, and make use of the highly fertile volcanic soil on the lower slopes. They believe that God, Ngai or Mwene Nyaga, lived on Mount Kenya when he came down from the sky.[16] They believe that the mountain is Ngai's throne on earth. It is the place where G?k?y?, the father of the tribe, used to meet with God. Thus according to the Kikuyu records, G?k?y? is the first person on Earth to ascend the mountain. 'Mwene Nyaga' in Kikuyu language can also translate as the \\"Owner of the Striped one\\" where 'Mwene' translates to 'owner', and 'Nyaga' to Stripes. The snow (in Kikuyu: Ira) caps of the mountain symbolically represent a crown on God's habitation .  'Nyaga' can also translate to 'Ostrich'. In this context, God is seen to be the owner of that very rare bird the Ostrich.\\r\\n\\r\\nKikuyu used to build their houses with the doors facing the mountain.[17]  The Kikuyu name for Mount Kenya is K?r? Nyaga (Kirinyaga), which literally translates to that which has the \\"Nyaga\\" ÿ Stripes.The mountain therefore is locally accepted as 'God's Resting Place' or 'Where God Lives'.\\r\\n\\r\\nThe Embu people live to the south-east of Mount Kenya,[10] and believe that the mountain is God's home (the Embu word for God is Ngai or Mwene Njeru). The mountain is sacred, and they build their houses with the doors facing towards it.[17]  The Embu people are closely related to the Ameru and Mbeere people. The Mbeere and Akamba are the settlers of the southeast side of the mountain.\\r\\n\\r\\nThe Ameru occupy the east, north and north-western slopes of the mountain. They are generally agricultural and also keep livestock and occupy what is among the most fertile land in Kenya. The Meru god Murungu was from the skies. Their name for Mt. Kenya is Kirimara, which means 'mountain with white features'.\\r\\n\\r\\nThe Maasai are semi-nomadic people, who use the land to the north of the mountain to graze their cattle. They believe that their ancestors came down from the mountain at the beginning of time.[17]  The Maasai name for Mount Kenya is Ol Donyo Keri, which means 'mountain of stripes', referring to the dark shades as observed from the surrounding plains.[18] At least one Maasai prayer refers to Mount Kenya:\\r\\n\\r\\nGod bless our children, let them be like the olive tree of Morintat, let them grow and expand, let them be like Ngong Hills like Mt. Kenya, like Mt. Kilimanjaro and multiply in number.\\r\\nMount Kenya is a stratovolcano that was active in the Plio-Pleistocene. The original crater was probably over 6,000?m (19,700?ft) high; higher than Kilimanjaro. Since it became extinct there have been two major periods of glaciation, which are shown by two main rings of moraines below the glaciers. The lowest moraine is found at around 3,300?m (10,800?ft).[19] Today the glaciers reach no lower than 4,650?m (15,260?ft).[3] After studying the moraines, Gregory put forward the theory that at one time the whole summit of the mountain was covered with an ice cap, and it was this that eroded the peaks to how they are today.[6]\\r\\n\\r\\nThe lower slopes of the mountain have never been glaciated. They are now mainly cultivated and forested. They are distinguished by steep-sided V-shaped valleys with many tributaries. Higher up the mountain, in the area that is now moorland, the valleys become U-shaped and shallower with flatter bottoms. These were created by glaciation.[19]\\r\\n\\r\\nWhen Mount Kenya was active there was some satellite activity. The north-eastern side of the mountain has many old volcanic plugs and craters. The largest of these, Ithanguni, even had its own ice cap when the main peaks were covered in ice. This can be seen by the smoothed summit of the peak. Circular hills with steep sides are also frequent in this area, which are probably the remains of small plugged vents. However, as the remaining mountain is roughly symmetrical, most of the activity must have occurred at the central plug.[19]\\r\\n\\r\\nThe rocks that form Mount Kenya are mainly basalts, rhomb porphyrites, phonolites, kenytes and trachytes.[19] Kenyte was first reported by Gregory in 1900 following his study of the geology of Mount Kenya.[20]\\r\\n\\r\\nThe geology of the Mount Kenya area was first proposed to the Western Community by Joseph Thomson in 1883. He saw the mountain from the nearby Laikipia Plateau and wrote that it was an extinct volcano with the plug exposed.[21] However, as he had only seen the mountain from a distance his description was not widely believed in Europe, particularly after 1887 when Teleki and von H?hnel ascended the mountain and described what they considered to be the crater.[22] In 1893 Gregory's expedition reached the Lewis Glacier at 5,000?m (16,400?ft). He confirmed that the volcano was extinct and that there were glaciers present.[20][22] The first thorough survey by Europeans was not undertaken until 1966.[19]\\r\\n\\r\\nThe peaks of Mount Kenya are almost all of a volcanic origin. The majority of the peaks are located near the centre of the mountain. These peaks have an Alpine appearance due to their craggy nature. Typically of Alpine terrain, the highest peaks and gendarmes occur at the intersection of ridges.[7] The central peaks only have a few mosses, lichens and small alpine plants growing in rock crevices.[10] Further away from the central peaks, the volcanic plugs are covered in volcanic ash and soils.[23] The vegetation growing on these peaks is typical for their vegetation band.\\r\\n\\r\\nThe highest peaks are Batian (5,199 metres (17,057?ft)), Nelion (5,188?m (17,021?ft)) and Pt Lenana (4,985?m (16,355?ft)). Batian and Nelion are within 250?m (270?yd) of each other, separated by the Gate of the Mists gap (5,144 metres (16,877?ft)).[3][24] Coryndon Peak (4,960?m (16,273?ft)) is the next highest, but unlike the previous peaks it does not form a part of the central plug.[7]\\r\\n\\r\\nOther peaks around the central plug include Pt Piggot (4,957?m (16,263?ft)), Pt Dutton (4,885?m (16,027?ft)), Pt John (4,883?m (16,020?ft)), Pt John Minor (4,875?m (15,994?ft)), Krapf Rognon (4,800?m (15,748?ft)), Pt Peter (4,757?m (15,607?ft)), Pt Slade (4,750?m (15,584?ft)) and Midget Peak (4,700?m (15,420?ft)). All of these have a steep pyramidal form.[3][7]\\r\\n\\r\\nSignificant craggy outlying peaks include Terere (4,714?m (15,466?ft)) and Sendeyo (4,704?m (15,433?ft)) which form a pair of twin peaks to the north of the main plug. Together, they form a large parasitic plug. Other notable peaks include The Hat (4,639?m (15,220?ft)), Delamere Peak, Macmillan Peak and Rotundu.[3]\\r\\n\\r\\nBatian on the left, Nelion on the right, and Slade in the foreground\\r\\n\\r\\nLenana, the third highest peak, is the most ascended\\r\\n\\r\\nMount Kenya, left to right: Point Lenana (4985m), Nelion summit (5188), Batian summit (5199m)\\r\\n\\r\\nKrapf Rognon (4,800?m (15,748?ft)) and Krapf glacier\\r\\n\\r\\nMidget peak can be climbed in a day.[25]\\r\\n\\r\\nTerere and Sendeyo are two craggy outlying peaks\\r\\n\\r\\nMugi hill and the Giant's Billiards Table offers some of the best hillwalking in Kenya.[17]\\r\\n\\r\\nNelion from Batian in Dec 1974\\r\\n\\r\\nLooking down the Diamond Glacier to Pt John\\r\\n\\r\\nPoint Thompson (4955m), Batian (5199m) and Nelion (5188m) on Mt Kenya\\r\\n\\r\\nThe glaciers on Mount Kenya are retreating rapidly. The Mountain Club of Kenya in Nairobi has photographs showing the mountain when it was first climbed in 1899, and again more recently; the retreat of the glaciers is very evident.[26][27]  Descriptions of ascents of several of the peaks advise on the use of crampons, but this is true only in some cases and at higher heights. Every year there is less new snow accumulating in winter than melting on summer, even on the Lewis Glacier (the largest of them) in winter, so there is no formation of new ice. It is predicted to be less than 30?years before there will no longer be ice on Mount Kenya.[17]  Glacial retreat and disappearance can be caused by change in temperature trends, or by change in precipitation trends.[28]\\r\\n\\r\\nThe glacier names are (clockwise from the north):\\r\\n\\r\\nThe area of glaciers on the mountain was measured in the 1980s, and recorded as about 0.7?km2 (0.27?sq?mi).[29]  This is far smaller than the first observations, made in the 1890s.\\r\\n\\r\\nAlthough Mount Kenya is on the equator the freezing nightly temperatures result in periglacial landforms. There is permafrost a few centimetres (inches) below the surface. Patterned ground is present at 3,400?m (11,155?ft) to the west of Mugi Hill.[3][7] These mounds grow because of the repeated freezing and thawing of the ground drawing in more water. There are blockfields present around 4,000?m (13,123?ft) where the ground has cracked to form hexagons. Solifluction occurs when the night temperatures freeze the soil before it thaws again in the morning. This daily expansion and contraction of the soil prevents the establishment of vegetation.[25]\\r\\n\\r\\nMount Kenya is the main water catchment area for two large rivers in Kenya; the Tana, the largest river in Kenya, and the Ewaso Ng'iso North.[8] The Mount Kenya ecosystem provides water directly for over 2 million people.[8] The rivers on Mount Kenya have been named after the villages on the slopes of the mountain that they flow close to. The Thuchi River is the district boundary between Meru and Embu. Mount Kenya is a major water tower for the Tana river which in 1988 supplied 80% of Kenya's electricity using a series of seven hydroelectric powerstations and dams.[30]\\r\\n\\r\\nThe density of streams is very high, especially on the lower slopes which have never been glaciated. The ice cap which used to cover the mountain during the Pliocene eroded large U-shaped valleys which tend to only have one large stream.[7]\\r\\nWhere the original shape of the shield volcano is still preserved, there have been millions of years for streams to erode the hillside. This area is therefore characterised by frequent deep fluvial V-shaped valleys.[31]\\r\\nThe gradual transition from glaciated to fluvial valley can be clearly observed.[32]\\r\\n\\r\\nRivers which start on Mount Kenya are the tributaries of two large Kenyan rivers: the Tana and the Ewaso Ng'iro rivers. A lot of Mount Kenyan rivers flow into the Sagana which itself is a tributary of the Tana, which it joins at the Masinga Reservoir. The rivers in the northern part of the mountain, such as the Burguret, Naru Moru, Nanyuki, Liki, Sirimon flow into the Ewaso Ng'iro. The rivers to the south-west, such as the Keringa and Nairobi flow into the Sagana and then into the Tana. The remaining rivers to the south and east, such as the Mutonga, Nithi, Thuchi and Nyamindi, flow directly into the Tana.[31][32]\\r\\n\\r\\nMount Kenya has several altitudinal ecological zones, between the savanna surrounding the mountain to the nival zone by the glaciers. Each zone has a dominant species of vegetation. Many of the species found higher up the mountain are endemic, either to Mount Kenya or East Africa.[10]\\r\\n\\r\\nThere are also differences within the zones, depending on the side of the mountain and aspect of the slope. The south-east is much wetter than the north,[29] so species more dependent on moisture are able to grow. Some species, such as bamboo, are limited to certain aspects of the mountain because of the amount of moisture.[3]\\r\\n\\r\\nThe climate of Mount Kenya changes considerably with altitude, forming belts of community types.[33]  Around the base of the mountain is fertile farmland. The tribes living around the mountain have cultivated this cool relatively moist area for centuries.[34]\\r\\n\\r\\nMount Kenya is surrounded by forests. The vegetation in the forests depend on rainfall, and the species present differ greatly between the northern and southern slopes.[35]  As time has passed the trees on the edge of the forest have been logged and the farmland has encroached further up the fertile slopes of the mountain.[33][34]\\r\\n\\r\\nAbove the forest is a belt of bamboo. This zone is almost continuous, but is restricted to small isolated bunches in the north because of low rainfall. The bamboo is natural,[25] and does not require forest disturbance. Tracks are common through the bamboo. Bamboo suppresses other vegetation, so it is uncommon to find trees or other plants here.[3]\\r\\n\\r\\nAbove the bamboo is the timberline forest. The trees here are often smaller than the trees in the forests lower down the mountain.[36]  The forest here is more intact, because it is less accessible and better protected.\\r\\n\\r\\nWhen the trees can no longer grow the vegetation changes into heathland and chaparral, at around 3,000?m (9,800?ft). Heathland is found in the wetter areas, on the west side of Mount Kenya, and is dominated by giant heathers. Chaparral is found in the drier areas and grasses are more common.[25] and bush fires still occur.[34]\\r\\n\\r\\nAs the altitude increases the temperature fluctuations become extreme and the air becomes thinner and drier. This region is known as the Afro-alpine zone. The environment here is isolated, with the only similar area nearby being the Aberdares, which are 80?km (50?mi) away.[10]  Many of the species here are endemic, with adaptations to the cold and fluctuating temperatures.[37]  Typical plants here include giant groundsels (senecios) and giant lobelias.[10]\\r\\n\\r\\nThe region where the glaciers have recently retreated from is nival zone. It is the area that plants have not yet been able to colonise.[10]\\r\\n\\r\\nThe flora found on Mount Kenya varies with altitude, aspect and exposure.[38] As the altitude increases, the plants have to be more specialised, with adaptations to strong sunlight with ultraviolet, lower mean temperatures and freezing night temperatures.[25][36]\\r\\n\\r\\nPlants in the Afro-alpine zone have overcome these difficulties in several ways.[37]  One adaptation is known as the giant rosette, which is exhibited by giant senecio, giant lobelia and giant thistle (Carduus), which use bud leaves to protect their buds from freezing. Giant rosette senecios form single-aged stands that drive community structure over decades.[39]\\r\\n\\r\\nMany plant species in the Afro-alpine zone of Mount Kenya are giant versions of lowland (or temperate) relatives. However, nearer the nival zone the plants decrease in size again.[10]\\r\\n\\r\\nThe majority of animals live lower down on the slopes of Mount Kenya. Here there is more vegetation and the climate is less extreme. Various species of monkeys, several antelopes, tree hyrax, porcupines and some larger animals such as elephant and buffalo all live in the forest.[3]  Predators found here include hyena and leopard, and occasionally lion.[3]\\r\\n\\r\\nThere are fewer mammals found at high altitudes on Mount Kenya.[40]  The Mount Kenya hyrax and common duiker are able to live here, and are important to the ecosystem. Some smaller mammals, such as the groove-toothed rat, can live here by burrowing into the giant senecios and using their thick stem of dead leaves as insulation.[10]  The Mount Kenya mole-rat Tachyoryctes rex occurs at high altitudes, living in visible mounds.[41]  Leopards are resident in the alpine zone.\\r\\n\\r\\nOther mammal species are only occasional visitors. Remains of elephants, monkeys and bongo have been found high in the alpine zone,[40] and other sightings are remembered in names such as Simba Tarn (simba means lion in Swahili).[25]\\r\\n\\r\\nSeveral bird species live in the Afro-alpine zone, including sunbirds, alpine chats and starlings and the raptors, augur buzzard, lammergeier and Verreaux's eagle, the latter of which specializes in hunting hyraxes. Birds are important in this ecosystem as pollinators.[38]\\r\\n\\r\\nThe climate of Mount Kenya has played a critical role in the development of the mountain, influencing the topography and ecology amongst other factors. It has a typical equatorial mountain climate which Hedberg described as winter every night and summer every day.[42] Mount Kenya is home to one of the Global Atmosphere Watch's atmospheric monitoring stations.[43]\\r\\n\\r\\nThe year is divided into two distinct wet seasons and two distinct dry seasons which mirror the wet and dry seasons in the Kenyan lowlands.[45] As Mount Kenya ranges in height from 1,374?m (4,508?ft) to 5,199?m (17,057?ft) the climate varies considerably over the mountain and has different zones of influence. The lower, south eastern slopes are the wettest as the predominant weather system comes from the Indian ocean. This rainfall supports dense montane forest on these slopes. High on the mountain most of the precipitation falls as snow.[46] Combined, these water sources feed 11 glaciers.\\r\\n\\r\\nThe current climate on Mount Kenya is wet, but drier than it has been in the past. The temperatures span a wide range, which diminishes with altitude. In the lower alpine zone temperature usually do not go below 12?C (54?F).[47] Snow and rain are common from March to December, but especially in the two wet seasons. The wet seasons combined account for 5/6 of the annual precipitation. The monsoon, which controls the wet and dry seasons, means that most of the year there are south-easterly winds, but during January and February the dominant wind direction is north-easterly.\\r\\n\\r\\nMount Kenya, like most locations in the tropics, has two wet seasons and two dry seasons as a result of the monsoon. From mid-March to June the heavy rain season, known as the long rains, brings approximately half of the annual rainfall on the mountain.[34] This is followed by the wetter of the two dry seasons which lasts until September. October to December are the short rains when the mountain receives approximately a third of its rainfall total. Finally from December to mid-March is the drier dry season when the mountain experiences the least rain.\\r\\n\\r\\nDuring the dry season the mountain almost always follows the same daily weather pattern. Large daily temperature fluctuations occur which led Hedberg to exclaim winter every night and summer every day.[42] There is variation in minimum and maximum temperatures day to day, but the standard deviation of the mean hourly pattern is small.\\r\\n\\r\\nA typical day is clear and cool in the morning with low humidity. The mountain is in direct sunlight which causes the temperatures to rise quickly with the warmest temperatures occurring between 0900 and 1200. This corresponds to a maximum in the pressure, usually around 10 am. Low on the mountain, between 2,400?m (7,874?ft) and 3,900?m (12,795?ft), clouds begin to form over the western forest zone, due to moist air from Lake Victoria.[30] The anabatic winds caused by warm rising air gradually bring these clouds to the summit region in the afternoon. Around 1500 there is a minimum in sunlight and a maximum in humidity causing the actual and perceived temperature to drop. At 1600 there is a minimum in the pressure. This daily cover of cloud protects the glaciers on the south-west of the mountain which would otherwise get direct sun every day, enhancing their melt.[48] The upwelling cloud eventually reaches the dry easterly air streams and dissipates, leading to a clear sky by 5 pm. There is another maximum of temperature associated with this.\\r\\n\\r\\nBeing an equatorial mountain the daylight hours are constant with twelve-hour days. Sunrise is about 0630 with the sun setting at 1830 (both EAT = UTC+3). Over the course of the year there is a one-minute difference between the shortest and longest days.[49] At night, the sky is usually clear with katabatic winds blowing down the valleys. Above the lower alpine zone there is usually frost every night.[47]\\r\\n\\r\\nThe first European to report seeing Mount Kenya was Dr Johann Ludwig Krapf, a German missionary, from Kitui,[50]\\r\\na town 160?km (100?mi)[4] away from the mountain. The sighting was made on 3 December 1849,[35]\\r\\na year after the discovery of Kilimanjaro.[51]\\r\\n\\r\\nKrapf was told by people of the Embu tribe that lived around the mountain that they did not ascend high enough on the mountain because of the intense cold and the white matter that rolled down the mountains with a loud noise. This led him to infer that glaciers existed on the mountain.[50]  It was Krapf who gave the mountain the name \\"Kenya\\", but the derivation of this is not known with certainty (see the various local names below, some of which are similar).\\r\\n\\r\\nKrapf also noted that the rivers flowing from Mount Kenya, and other mountains in the area, were continuously flowing. This was very different from the other rivers in the area, which swelled up in the wet season and completely dried up after the rainy season had ended. As the streams flowed even in the driest seasons he concluded that there must be a source of water up on the mountain, in the form of glaciers.[50] He believed the mountain to be the source of the White Nile.[52]\\r\\n\\r\\nIn 1851 Krapf returned to Kitui. He travelled 65 kilometres (40?mi) closer to the mountain, but did not see it again. In 1877 Hildebrandt was in the Kitui area and heard stories about the mountain, but also did not see it. Since there were no confirmations to back up Krapf's claim people began to be suspicious.[22]\\r\\n\\r\\nEventually, in 1883, Joseph Thomson passed close by the west side of the mountain and confirmed Krapf's claim. He diverted his expedition and reached 1,737?m (5,700?ft) up the slopes of the mountain but had to retreat because of trouble with local people.[21] However, the first European exploration high onto the mountain was achieved in 1887 by Count Samuel Teleki. He managed to reach 4,350?m (14,270?ft) on the south western slopes.[53] On this expedition Teleki mistakenly believed he had found the crater of a volcano.\\r\\n\\r\\nIn 1892, Teleki and von H?hnel returned to the eastern side, but were unable to get through the forest.[10]\\r\\n\\r\\nFinally, in 1893, an expedition managed to ascend Mount Kenya as far as the glaciers. This expedition was traveling from the coast to Lake Baringo in the Rift Valley, and was led by Dr John W Gregory, a British geologist. They managed to ascend the mountain to around 4,730?m (15,520?ft), and spent several hours on the Lewis Glacier with their guide. On his return to Britain, Gregory published papers and a narrative account of his achievements.[25]\\r\\n\\r\\nGeorge Kolb, a German physician, made expeditions in 1894 and 1896[25] and was the first to reach the moorlands on the east side of the mountain. More exploration occurred after 1899 when the Uganda Railway was completed as far as the future site of Nairobi.[25][54]\\r\\n\\r\\nOn 28 July 1899,[54] Sir Halford John Mackinder set out from the site of Nairobi on an expedition to Mount Kenya. The members of the expedition consisted of 6 Europeans, 66 Swahilis, 2 Maasai guides, and 96 Kikuyu. The Europeans were Campbell B. Hausberg, second in command and photographer; Douglas Saunders, botanist; C F Camburn, taxidermist; Cesar Ollier, guide; and Josef Brocherel, guide and porter.[54]\\r\\nThe expedition made it as far as the mountain, but encountered many difficulties on the way. The country they passed through was full of plague and famine. Many Kikuyu porters tried to desert with women from the villages, and others stole from the villages, which made the chiefs very hostile towards the expedition. When they reached the base camp on 18 August,[54] they could not find any food, suffered two of their party killed by the local people, and eventually had to send Saunders to Naivasha to get help from Captain Gorges, the Government Officer there.[54]\\r\\n\\r\\nMackinder pushed on up the mountain, and established a camp at 3,142?m (10,310?ft)[54] in the H?hnel Valley. He made his first attempt on the summit on 30 August with Ollier and Brocherel up the south east face, but they had to retreat when they were within 100?m (330?ft) of the summit of Nelion due to nightfall.\\r\\n\\r\\nOn 5 September, Hausberg, Ollier, and Brocherel made a circuit of the main peaks looking for an easier route to the summit. They could not find one. On 11 September Ollier and Brocherel made an ascent of the Darwin Glacier, but were forced to retreat due to a blizzard.[54]\\r\\n\\r\\nWhen Saunders returned from Naivasha with the relief party, Mackinder had another attempt at the summit with Ollier and Brocherel. They traversed the Lewis Glacier and climbed the south east face of Nelion. They spent the night near the gendarme, and traversed the snowfield at the head of the Darwin Glacier at dawn before cutting steps up the Diamond Glacier. They reached the summit of Batian at noon on 13 September, and descended by the same route.[54]\\r\\n\\r\\nAfter the first ascent of Mount Kenya there were fewer expeditions there for a while. The majority of the exploration until after the First World War was by settlers in Kenya, who were not on scientific expeditions. A Church of Scotland mission was set up in Chogoria, and several Scottish missionaries ascended to the peaks, including Rev Dr. J. W. Arthur, G. Dennis and A. R. Barlow. There were other ascents, but none succeeded in summitting Batian or Nelion.[25]\\r\\n\\r\\nNew approach routes were cleared through the forest, which made access to the peaks area far easier. In 1920, Arthur and Sir Fowell Buxton tried to cut a route in from the south, and other routes came in from Nanyuki in the north, but the most commonly used was the route from the Chogoria mission in the east, built by Ernest Carr. Carr is also credited with building Urumandi and Top Huts.[25]\\r\\n\\r\\nOn 6 January 1929 the first ascent of Nelion was made by Percy Wyn-Harris and Eric Shipton. They climbed the Normal Route, then descended to the Gate of Mists before ascending Batian. On 8 January they reascended, this time with G. A. Sommerfelt, and in December Shipton made another ascent with R. E. G. Russell. They also made the first ascent of Point John. During this year the Mountain Club of East Africa was formed.[25]\\r\\n\\r\\nAt the end of July 1930, Shipton and Bill Tilman made the first traverse of the peaks. They ascended by the West Ridge of Batian, traversed the Gate of Mists to Nelion, and descended the Normal Route. During this trip, Shipton and Tilman made first ascents of several other peaks, including Point Peter, Point Dutton, Midget Peak, Point Pigott and either Terere or Sendeyo.[55]\\r\\n\\r\\nIn the early 1930s there were several visits to the moorlands around Mount Kenya, with fewer as far as the peaks. Raymond Hook and Humphrey Slade ascended to map the mountain, and stocked several of the streams with trout. By 1938 there had been several more ascents of Nelion. In February, Miss C Carroll and Mtu Muthara became the first woman and African respectively to ascend Nelion, in an expedition with Noel Symington, author of The Night Climbers of Cambridge, and on 5 March Miss Una Cameron became the first woman to ascend Batian.[25]\\r\\n\\r\\nDuring the Second World War there was another drop in ascents of the mountain. The most remarkable ascent during this period was by three Italians who were being held in a British POW camp at the base of the mountain in Nanyuki. They escaped from camp to climb the mountain's third peak, Point Lenana, before \\"escaping\\" back into camp. Felice Benuzzi, the team leader, retold his story in the book No Picnic on Mount Kenya (1946).[56][57]\\r\\n\\r\\nIn 1949 the Mountain Club of Kenya split from the Mountain Club of East Africa, and the area above 3,400?m (11,150?ft) was designated a National Park.[25] A road was built from Naro Moru to the moorlands, allowing easier access.\\r\\n\\r\\nMany new routes were climbed on Batian and Nelion in the next three decades, and in October 1959 the Mountain Club of Kenya produced their first guide to Mount Kenya and Kilimanjaro.[55] On Kenyan independence in 1963, Kisoi Munyao raised the Kenyan flag at the top of the mountain. He died in 2007 and was given a heroic funeral attended by Kenyan president Mwai Kibaki.[58]  In the early 1970s the Mount Kenya National Park Mountain Rescue Team was formed, and by the end of the 1970s all major routes on the peaks had been climbed.[55]\\r\\n\\r\\nOn 19 July 2003, a South African registered aircraft, carrying 12 passengers and two crew, crashed into Mount Kenya at Point Lenana; nobody survived.[59][60] This was not the first aircraft lost on the mountain; there is also the wreckage of at least one helicopter that crashed before 1972.[61]\\r\\n\\r\\nIn March 2012 a massive fire raged on Mount Kenya, devouring thousands of hectares of ancient forests and endangered wildlife.[62]\\r\\n\\r\\nMost of the peaks on Mount Kenya have been summited. The majority of these involve rock climbing as the easiest route, although some only require a scramble or a walk. The highest peak that can be ascended without climbing is Point Lenana, 4,985?m (16,355?ft).[25][55] The majority of the 15,000 visitors to the national park each year climb this peak. In contrast, approximately 200 people summit Nelion and 50 summit Batian, the two highest peaks.[17]\\r\\n\\r\\nWhen ascended directly, Batian is usually climbed via the North Face Standard Route, UIAA grade IV+ (or 5.6+ YDS). It was first ascended on 31 July 1944 by Firmin and Hicks.[63][64] The route is usually climbed in two days. The Normal Route is the most climbed route up Nelion, and thence across to Batian. It was first climbed by Shipton and Wyn-Harris on 6 January 1929.[64][65] It is possible to traverse between the two peaks, via the Gates of Mist, but this often involves spending a night in the Howell hut on top of Nelion. There is a bolted abseil descent route off Nelion.[25]\\r\\n\\r\\nMount Kenya's climbing seasons are a result of its location only 20?km (12?mi) from the equator. During the northern summer the rock routes on the north side of the peak are in good summer condition, while at the same time the ice routes on the south side of the peak are prime shape. The situation is reversed during the southern summer. The two seasons are separated by several months of rainy season before and after, during which climbing conditions are generally unfavorable.\\r\\n\\r\\nMount Kenya is home to several good ice routes, the two most famous being the Diamond Couloir and the Ice Window route. Snow and ice levels on the mountain have been retreating at an accelerated rate in recent years, making these climbs increasingly difficult and dangerous. The Diamond Couloir, a steep ice couloir fed by the fusion of the upper Diamond Glacier and pioneered in 1975 by Yvon Chouinard and Michael Covington, was once climbable in summer or winter but now is virtually unclimbable in summer conditions and is seldom deemed in climbable condition even in winter.[66] Last climbing reports describe the route very difficult, especially in the lower section. The route has changed into a modern ice climb with a very difficult 60m first pitch, starting with 8m of overhanging M7 dry tooling, followed by 50m of USA Grade V ice and by others 6 pitches of moderate climbing on good ice and finally one pitch of water ice USA Grade IV+ ice at the headwall before getting to the Upper Diamond Glacier.\\r\\n\\r\\nThe satellite peaks around the mountain also provide good climbs. These can be climbed in Alpine style and vary in difficulty from a scramble to climbing at UIAA grade VI. They are useful for acclimatisation before climbing the higher peaks and as ascents in their own right.[25]\\r\\n\\r\\nThere are eight walking routes up to the main peaks. Starting clockwise from the north these are the: Meru, Chogoria, Kamweti, Naro Moru, Burguret, Sirimon and Timau Routes.[3]\\r\\nOf these Chogoria, Naro Moru and Sirimon are used most frequently and therefore have staffed gates. The other routes require special permission from the Kenya Wildlife Service to use.[17][67]\\r\\n\\r\\nThe Chogoria route leads from Chogoria town up to the peaks circuit path. It heads through the forest to the south-east of the mountain to the moorland, with views over areas such as Ithanguni and the Giant's Billiards Table before following the Gorges Valley past the Temple and up to Simba Col below Point Lenana.[3] The Mountain Club of Kenya claims that Ithanguni and the Giant's Billiards Table offer some of the best hillwalking in Kenya.[25]\\r\\n\\r\\nThe Naro Moru route is taken by many of the trekkers who try to reach Point Lenana. It can be ascended in only 3?days and has bunkhouses at each camp. The route starts at Naro Moru town to the west of the mountain and climbs towards Mackinder's Camp before joining the Peak Circuit Path.[67] The terrain is usually good, although one section is called the Vertical Bog.[25]\\r\\n\\r\\nThe Sirimon route approaches Mount Kenya from the north-west.[3] The path splits on the moorlands, with the more frequently used fork following the Mackinder Valley and the quieter route traversing into the Liki North Valley.[3] The paths rejoin at Shipton's Cave just below Shipton's Camp on the Peak Circuit Path.[25]\\r\\n\\r\\nThe Peak Circuit Path is a path around the main peaks, with a distance of about 10 kilometres (6?mi) and height gain and loss of over 2,000 metres (6,600?ft).[3]  It can be walked in one day, but more commonly takes two or three. It can also be used to join different ascent and descent routes. The route does not require technical climbing.[55][67]\\r\\n\\r\\nThe Gorges Valley is a major feature on the Chogoria Route.\\r\\n\\r\\nVertical bog on Mount Kenya on the Naro Moru Route.\\r\\n\\r\\nLooking towards the peaks up the Mackinder Valley on the Sirimon Route.\\r\\n\\r\\nDevelopment is currently underway for a new route up the mountain starting from the Ragati conservancy and running up the ridge between the Naro Moru route and the old Kamweti trail.\\r\\n\\r\\nAccommodation on Mount Kenya ranges from very basic to luxurious. The more luxurious lodges are found on the lower slopes, in and around the forest.[68][69]\\r\\nThese lodges have hotel-style accommodation, often with log fires and hot running water.[70][71] Many offer guided walks and other activities such as fishing and birdwatching.\\r\\n\\r\\nThe huts higher on the mountain are more basic. Most have several bunkrooms with beds, and also offer somewhere to rest, cook and eat. Some also have running water. A few huts are very basic bothies and offer only a space to sleep that is sheltered from the weather.[25][72] Beds in the huts can be reserved at the park gates.[17]\\r\\nCamping is allowed anywhere in the National Park, but it is most encouraged around the huts to limit environmental impact. It is possible for campers to use the communal spaces in the huts for no extra fee.[17]\\r\\n\\r\\nAustrian Hut is found near the Lewis Glacier on the slopes of Point Lenana. The hut sleeps 30 people, with Top Hut nearby for porters.[25]\\r\\n\\r\\nLiki North Hut is a small bothy in the Liki North Valley.[25] It offers little more than shelter from the weather.\\r\\n\\r\\nShipton's Camp is at the top of the Sirimon Route.[55] It has a large communal area and running cold water.\\r\\n\\r\\nCamping is allowed anywhere within the National Park.[17]\\r\\n\\r\\nFairmont Mount Kenya Safari Club is a resort located in Nanyuki at the base of Mount Kenya. The resort has over 120 rooms and is one of the most exclusive in the region.[69]\\r\\n\\r\\nThe origin of the name Kenya is not clear, but perhaps linked to the Kikuyu, Embu and Kamba words Kirinyaga, Kirenyaa and Kiinyaa which mean \\"God's resting place\\" in all three languages.\\r\\n\\r\\nIn the 19th Century, the German explorer, Ludwig Krapf, recorded the name as both Kenia and Kegnia believed by some to be a corruption of the Kamba version.[50][73][74]\\r\\nOthers however say that this was on the contrary a very precise notation of the correct African pronunciation /?k?nj?/.\\r\\n[75]\\r\\nIn any case, the name was for a long time pronounced by colonial-heritage Europeans as /?ki?nj?/. The European pronunciation has been abandoned in modern times, in favor of the African version.[76]\\r\\n\\r\\nThe peaks of Mount Kenya have been given names from three different sources. Firstly, several Maasai chieftains have been commemorated, with names such as Batian, Nelion and Lenana. They commemorate Mbatian, a Maasai Laibon (Medicine Man), Nelieng, his brother, and Lenana and Sendeyo, his sons.[35]  Terere is named after another Maasai headman.\\r\\n\\r\\nThe second type of names that were given to peaks are after European climbers and explorers. Some examples of this are Shipton, Sommerfelt, Tilman, Dutton and Arthur.[25]\\r\\n\\r\\nThe remaining names are after well-known Kenyan personalities, with the exception of John and Peter, which were named by the missionary Arthur after two disciples. There is a group of four peaks to the east of the main peaks named after European settlers; Coryndon, Grigg, Delamere and McMillan.[25]","input":"What is the highest peak of mountain kenya?"},{"output":"lethal injection","context":"Capital punishment is a legal penalty in the U.S. state of Texas.\\r\\n\\r\\nIn 1982, the state became the first jurisdiction in the world to carry out an execution by lethal injection, when it put to death Charles Brooks Jr.. It was the first execution in the state since 1964.\\r\\n\\r\\nTexas, which is the second most populous state of the Union, has executed 553 offenders from the U.S. capital punishment resumption in 1976 (beginning in 1982 with the Brooks execution) to July 17, 2018 (the Christopher Young execution), more than a third of the national total.[1]\\r\\n\\r\\nThe first recorded execution in Texas occurred in 1819 with the execution of a white male, George Brown, for piracy.[2] In 1840, a free black male, Henry Forbes, was executed for jail-breaking.[3] Prior to Texas statehood in 1846, eight executionsall by hangingwere carried out.[2]\\r\\n\\r\\nUpon statehood, hanging would be the method used for almost all executions until 1924. Hangings were administered by the county where the trial took place. The last hanging in the state was that of Nathan Lee, a man convicted of murder and executed in Angleton, Brazoria County, Texas on August 31, 1923.[4] The only other method used at the time was execution by firing squad, which was used for three Confederate deserters during the American Civil War as well as a man convicted of attempted rape in 1863.[5]\\r\\n\\r\\nTexas changed its execution laws in 1923, requiring the executions be carried out on the electric chair and that they take place at the Texas State Penitentiary at Huntsville (also known as Huntsville Unit). From 1928 until 1965, this was also home to the state's male death row. The first executions on the electric chair were on February 8, 1924, when Charles Reynolds, Ewell Morris, George Washington, Mack Matthews, and Melvin Johnson had their death sentences carried out. The five executions were the most carried out on a single day in the state. The state would conduct multiple executions on a single day on several other occasions, the last being on August 9, 2000. Since then, the state has not executed more than one person on a single day, though there is no law prohibiting it. A total of 361 people were electrocuted in Texas, with the last being Joseph Johnson on July 30, 1964.\\r\\n\\r\\n\\r\\n\\r\\nThe United States Supreme Court decision in Furman v. Georgia (408 U.S. 238 (1972)), which declared Georgia's \\"unitary trial\\" procedure (in which the jury was asked to return a verdict of guilt or innocence and, simultaneously, determine whether the defendant would be punished by death or life imprisonment) to be unconstitutional on the grounds that it was a cruel and unusual punishment in violation of the Eighth Amendment to the United States Constitution, essentially negated all death penalty sentences nationwide.\\r\\n\\r\\nAs result of the Furman decision, the 52 Texas death-row inmates at the time had all their sentences reduced to life imprisonment.[6] Among them was Kenneth McDuff, who was originally condemned for the murder of three teenagers in 1966. He was paroled in 1989 and executed in 1998 for a murder he committed while on parole, and is suspected to have been responsible for many other killings.[7]\\r\\n\\r\\nFurman led to a 1973 revision of the laws, primarily by introducing the bifurcated trial process (where the guilt-innocence and punishment phases are separate) and narrowly limiting the legal definition of capital murder (and, thus, those offenses for which the death penalty could be imposed).  The first person sentenced to death under a new Texas statute was John Devries on February 15, 1974; Devries hanged himself in his cell on July 1, 1974 (using bedsheets from his bunk) before he could be executed.[6][8]\\r\\n\\r\\nThe Supreme Court decision in Gregg v. Georgia in 1976 once again allowed for the death penalty to be imposed.  (A Texas case was a companion case in the Gregg decision and was upheld by the Court; the Court stated that Texas' death penalty scheme could potentially result in fewer death penalty cases, an irony given that post-Gregg Texas has by far executed more inmates than any other state.)  However, the first execution in Texas after this decision would not take place until December 7, 1982 with that of Charles Brooks, Jr.. Brooks was also the first person to be judicially executed by lethal injection in the world, and the first African American to be executed in the United States since 1967.\\r\\n\\r\\nIn the post-Gregg era, Texas has executed 551 people.[9] There are a variety of proposed legal and cultural explanations as to why Texas has more executions than any other state. One possible reason is due to the federal appellate structure ÿ federal appeals from Texas are made to the United States Court of Appeals for the Fifth Circuit. Michael Sharlot, dean of the University of Texas at Austin Law School, found the Fifth Circuit to be a \\"much more conservative circuit\\" than the Ninth Circuit to which federal appeals from California are made.  According to him, the Fifth is \\"more deferential to the popular will\\" that is strongly pro-death penalty and creates few legal obstacles to execution within its jurisdiction.[10][11] As of 2004, however, Texas may have a lower rate of death sentencing than other states, according to a study by Cornell University.[12]\\r\\n\\r\\nTexas has executed nine women in its history, the most recent being Lisa Ann Coleman on September 17, 2014.\\r\\n\\r\\nUnder Texas statutes, a murder is capital if the offender:[13]\\r\\n\\r\\nTexas statute books still provide the death penalty for aggravated sexual assault committed by an offender previously convicted of the same against a child under 14.\\r\\n\\r\\nUnder Texas law, offenders under 17 are not executed[14] but the US Supreme Court in Roper v. Simmons has ruled capital punishment to be unconstitional for those under 18 when the crime was committed.[15] The victim's age under which child murder is punishable by death was raised from six to 10 in 2011 by the Texas legislature.[16]\\r\\n\\r\\nThe prosecution may choose not to seek the death penalty. This can be for various reasons, such as the prosecution believing that they could not show the defendant worthy of death, or the family of the victim has asked that the death penalty not be imposed.\\r\\n\\r\\nWhen the prosecution seeks the death penalty, the sentence is decided by the jury. A death sentence must be unanimous, while a life sentence requires only 10 votes.\\r\\n\\r\\nIn case of a hung jury during the penalty phase of the trial, a life sentence is issued, even if a single juror opposed death (there is no retrial).\\r\\n\\r\\nJurors in the sentencing phase are first asked to determine whether the defendant represents a future danger to society; only after ruling that \\"there is a probability that the defendant would commit criminal acts of violence that would constitute a continuing threat to society\\" will the jury choose the sentence itself, by deciding whether there is \\"sufficient mitigating circumstance or circumstances to warrant that a sentence of life imprisonment without parole rather than a death sentence be imposed\\" or not.\\r\\n\\r\\nThe imposition of a death sentence in Texas results in an automatic direct appeal to the Texas Court of Criminal Appeals, the state's highest criminal tribunal (the intermediate Texas Court of Appeals is bypassed). A person convicted of capital murder may also attack their convictions or sentences via writs of habeas corpus at both the state and federal levels.[17]\\r\\n\\r\\nTexas's appeal process has been criticized as too lengthy compared to other states such as Virginia by death penalty supporters. In 2016, the legal director for the Criminal Justice Legal Foundation Kent Scheidegger said: \\"In Texas, part of the problem is some cases go back for a second review to the trial court and some trial courts just sit on them for years. That simply shouldn't be allowed.\\"[18]\\r\\n\\r\\nIn addition to seeking judicial review of the sentence, a defendant may also appeal to the Texas Board of Pardons and Paroles, a separate agency from TDCJ, for commutation of the sentence to life in prison.\\r\\n\\r\\nThe Board, after hearing testimony, decides whether or not to recommend commutation to the Governor of Texas.  If the Board recommends commutation, the Governor can accept or reject the recommendation.  However, if the board does not recommend commutation, the Governor has no power to override the Board's non-recommendation (the law was changed in 1936 due to concerns that pardons were being sold for cash under the administrations of former Governor James E. Ferguson and later his wife and Texas's first female Governor Miriam A. Ferguson).[19]\\r\\n\\r\\nThe only unilateral action which the Governor can take is to grant a one-time, 30-day reprieve to the defendant, and can do so regardless of what the Board recommends in a particular case.[20][21]\\r\\n\\r\\nSince Texas reinstated the death penalty in 1976, only three defendants sentenced to death have been granted clemency by the Governor after a recommendation from the Board:\\r\\n\\r\\nMale death row inmates are housed at the Polunsky Unit in West Livingston;[24] female death row inmates are housed at the Mountain View Unit in Gatesville. All death row inmates at both units are physically segregated from the general population, are housed in individual cells approximately 60 square feet (5.6?m2) in size, and engage in recreational activities in a cage individually, separate from the general population and other death row inmates. Photographs taken inside death row were provided by the State of Texas in response to a Freedom of Information Act (United States) request filed by attorney Yolanda Torres in 2009.[25] Death row inmates receive special death row ID numbers starting with 999 instead of regular Texas Department of Criminal Justice numbers.[26]\\r\\n\\r\\nDeath row prisoners, along with prisoners in administrative segregation, are seated individually on prison transport vehicles. The TDCJ makes death row prisoners wear various restraints, including belly chains and leg irons, while being transported.[27] Death row offenders and offenders with life imprisonment without parole enter the TDCJ system through two points; men enter through the Byrd Unit in Huntsville, and women enter through the Reception Center in Christina Crain Unit, Gatesville. From there, death row inmates go to their designated death row facilities.[28]\\r\\n\\r\\nPreviously death row inmates were permitted to work. After an escape attempt occurred in 1998, the prison work program was suspended.[29]\\r\\n\\r\\nThe state of Texas began housing death row inmates in the Huntsville Unit in 1928. In 1965 the male death row inmates moved to the Ellis Unit. In 1999 the male death row moved to Polunsky.[30] In the 1923-1973 period Texas state authorities had three female death row inmates;[31] the first, Emma \\"Straight Eight\\" Oliver, was held at Huntsville Unit after her 1949 sentencing, but had her sentence commuted to life imprisonment in 1951.[32] Mary Anderson, sentenced to death in 1978,[33] was held at Goree Unit.[32] Her death sentence was reversed in 1982,[33] and the sentence was changed to murder.[34]\\r\\n\\r\\nThe TDCJ website maintains a list of inmates with scheduled execution dates, which is generally updated within 1ÿ2 days after an execution date is set, an execution takes place, or a stay of execution is granted and the date withdrawn.\\r\\n\\r\\nThe judge presiding over a capital case sets the execution date once it appears that all the offender's appeals have been exhausted.[35] The initial date of execution cannot be prior to the 91st day after the day the order is entered and (if the original order is withdrawn) subsequent execution dates cannot be less than the 31st day after the order is entered, provided that no habeas corpus motion has been filed under Article 11.071; otherwise, the date cannot be set prior to the court either denying relief, or issuing its mandate.[36]  In the event an offender manages to escape confinement, and not be re-arrested until after the set execution date, the revised date of execution shall be not less than 30 days from the date the order is issued.[37]\\r\\n\\r\\nThe law does not prohibit multiple executions in a single day; however, Texas has not executed multiple offenders on a single day since August 9, 2000, on which two offenders were executed.[38]\\r\\n\\r\\nThe law only specifies that \\"[t]he execution shall take place at a location designated by the Texas Department of Corrections in a room arranged for that purpose.\\"[39]  However, since 1923, all executions have been carried out at the Huntsville Unit, the former location of death row.\\r\\n\\r\\nOn the afternoon of a prisoner's scheduled execution,[40] he or she is transported directly from his or her death row unit to the Huntsville Unit.[41] Men leave the Polunsky Unit in a three-vehicle convoy bound for the Huntsville Unit;[40] women leave from the Mountain View Unit. The only individuals who are informed of the transportation arrangements are the wardens of the affected units. The TDCJ does not make an announcement regarding what routes are used.[41]\\r\\n\\r\\nUpon arrival at the Huntsville Unit, the condemned is led through a back gate, submits to a cavity search, then is placed in a holding cell.\\r\\n\\r\\nBefore 2011, the condemned was given an opportunity to have a last meal based on what the unit's cafeteria could prepare from its stock. Robert Perkinson, author of Texas Tough: The Rise of America's Prison Empire, said in 2010 that most condemned prisoners ordered \\"standard American fare in heaping portions, the sorts of meals that recall a childhood Sunday.\\"[40] Many female prisoners under the death sentence did not take a last meal.[42]  However, Lawrence Russel Brewer, a white supremacist gang member convicted for the high-profile hate crime dragging death of James Byrd Jr., ordered a large last meal and did not eat it before his execution. In response, John Whitmire, a member of the Texas legislature, asked the TDCJ to stop special meals. Whitmire stated to the press that Brewer's victim, Mr. Byrd, \\"didn't get to choose his last meal.\\" The TDCJ complied. Brian Price, a former prison chef, offered to personally cook and pay for any subsequent special last meal since the TDCJ is not paying for them anymore.[43] However, Whitmire warned in a letter that he would seek formal state legislation when lawmakers next convened if the \\"last meal\\" tradition wasn't stopped immediately.[44]  Afterwards, the TDCJ stopped serving special last meals, and will only allow execution chamber prisoners to have the same kind of meal served to regular prisoners.[45]  Many prisoners requested cigarettes (which were denied as TDCJ has banned smoking in its facilities).\\r\\n\\r\\nUnder Texas law, executions are carried out at or after 6 p.m. Huntsville (Central) time \\"by intravenous injection of a substance or substances in a lethal quantity sufficient to cause death and until such convict is dead\\".[46]  The law does not specify the substance(s) to be used; previously, according to the Texas Department of Criminal Justice the chemicals used for the lethal injection were the commonly-used three-drug combination of (in order) sodium thiopental (a dose which sedates the offender, but not enough to kill outright), pancuronium bromide (a muscle relaxant which collapses the diaphragm and lungs), and potassium chloride (which stops the heartbeat). The offender is usually pronounced dead approximately seven minutes after start of the injection process; the cost for the three substances is $86.08 per offender.[6] As a result of drug shortages, sodium thiopental was replaced by pentobarbital in 2011.[47] Further shortages of this drug have pushed the cost of the drugs to approximately $1300 per offender.[48] Still further shortages of pancuronium bromide (and the expiration of the existing stock) forced the state into switching to a single-drug protocol, using solely pentobarbital.[49]\\r\\n\\r\\nThe only persons legally allowed to be present (none of whom can be convicts) at the execution are:[50]\\r\\n\\r\\nIn response to victims rights groups, TDCJ adopted a board rule in January 1996 allowing five victim witnesses (six for multiple victims). Initially the witnesses were limited to immediate family and individuals with a close relationship to the victim, but the board rule was modified in 1998 to allow close friends of surviving witnesses, and further modified in May 2008 to allow the victim witnesses to be accompanied by a spiritual advisor who is a bona fide pastor or comparable official of the victim's religion.[51]\\r\\n\\r\\nFive members of the media are also allowed to witness the execution, divided equally as possible between the rooms containing the offender's and victim's witnesses.[51]\\r\\n\\r\\nUnder current TDCJ guidelines, representatives of the Associated Press and The Huntsville Item (the local newspaper for Huntsville, Texas) are guaranteed two of the five slots to witness an execution.[52]  The Associated Press regularly sends a representative to cover executions; Michael Graczyk (from the AP's Houston office) is usually the representative sent, having attended over 300 executions in his career.  The Item also generally covers all executions, regardless of county of conviction (Cody Stark is generally the representative).\\r\\n\\r\\nOther media members must submit their requests at least three days prior to the execution date; priority will be given to media members representing the area in which the capital crime took place.  Generally, other newspapers will only cover executions where the crime was committed within their general circulation area (the Houston Chronicle is often one of them, with Harris County being the state's largest and having the most number of inmates on death row), and frequently even then will rely on the AP report.[53]  College and university media are not permitted to be witnesses.[52]\\r\\n\\r\\nUpon the offender's death the body shall be immediately embalmed, and shall be disposed of as follows:[54]\\r\\n\\r\\nThe TDCJ keeps an online record of all of its executions, including race, age, county of origin, and last words.[55] The TDCJ is the only corrections agency in the US to extensively catalog the last words of executed inmates, and the only one to post the last words other than the California Department of Corrections and Rehabilitation (CDCR).[56]\\r\\n\\r\\nThe main TDCJ prison cemetery for prisoners not picked up by their families after death is the Captain Joe Byrd Cemetery in Huntsville. Headstones of death row prisoners have prison numbers with the beginning 999, a state designation for a death row inmate, or they have the letters \\"EX\\" or \\"X\\".[57]\\r\\n\\r\\nFranklin T. \\"Frank\\" Wilson, an assistant professor of criminology at Indiana State University,[58] and a former PhD student at Sam Houston State University,[59] stated that about 2% of the people buried at the Byrd Cemetery had been executed, but the public believes that all executed prisoners are buried there because the Huntsville Unit, the site of execution in Texas, is in close proximity.[59] Most executed prisoners are claimed by their families.[57] While most prisoner funerals at Byrd Cemetery are held on Thursdays, in order to allow families of executed prisoners to make a single trip to Huntsville instead of two separate trips, the burial of an executed prisoner not claimed by the family is usually done the day after his or her execution.[57]\\r\\n\\r\\nThe Texas Coalition to Abolish the Death Penalty, a 501(c)(3) grassroots membership organization was founded in 1998.  TCADP has members across the state of Texas working to educate their local communities on the problems of the Texas death penalty.  TCADP hosts multiple education and training opportunities each year around the state including releasing an annual report on December 7 and a day-long annual conference which includes workshops, panel discussions, networking and awards.  The conference is held in Austin during legislative years and in other Texas Cities in non-legislative years (2012: San Antonio).  TCADP opened a state office in Austin in 2004 with a paid program coordinator and hired an executive director in 2008.  TCADP is affiliated with the National Coalition to Abolish the Death Penalty.\\r\\n\\r\\n The March to Abolish the Death Penalty is the current name of an event organized each October since 2000 by several Texas anti-death penalty organizations, including Texas Moratorium Network, the Austin chapter of the Campaign to End the Death Penalty, the Texas Death Penalty Abolition Movement and Texas Students Against the Death Penalty.[60] Anti-Death Penalty Alternative Spring Break is an annual event started by Texas Moratorium Network in 2004 and now co-organized by Texas Students Against the Death Penalty. It serves as a training ground for students who oppose the death penalty.[61]\\r\\n\\r\\nThe Death Row Inner-Communalist Vanguard Engagement (D.R.I.V.E.) consists of several male death row inmates from the Polunsky Unit. Through a variety of non-violent strategies, they have begun launching protests against the perceived bad conditions at Polunsky, in particular, and capital punishment, in general. They actively seek to consistently voice complaints to the administration, to organize grievance filing to address problems. They occupy day rooms, non-violently refuse to evacuate their cells or initiate sit-ins in visiting rooms, hallways, pod runs and recreation yards when there is the perception of an act of abuse of authority by guards (verbal abuse; physical abuse; meals/recreations or showers being wrongly denied; unsanitary day rooms and showers being allowed to persist; medical being denied; paper work being denied; refusing to contact higher rank to address the problems and complaints) and when alleged retaliation (thefts, denials, destruction of property; food restrictions; wrongful denials of visits; abuse of inmates) is carried out in response to their grievances.\\r\\n\\r\\nIn 2016, Pfizer and other drug manufacturers banned the use of their products for lethal injections.  Texas and other states were reported to be finding it difficult to obtain supplies of drugs for executions.[62]\\r\\n\\r\\nOne notable case involves Cameron Todd Willingham, who was executed by lethal injection on February 17, 2004 for murdering his three daughters in 1991 by arson, but where a 2009 article in The New Yorker, and subsequent findings, have cast doubt on the evidence used in his conviction.\\r\\n\\r\\nIn 2009, a report conducted by Dr. Craig Beyler, hired by the Texas Forensic Science Commission to review the case, found that \\"a finding of arson could not be sustained\\". Beyler said that key testimony from a fire marshal at Willingham's trial was \\"hardly consistent with a scientific mind-set and is more characteristic of mystics or psychics.[63]\\r\\n\\r\\nGovernor Rick Perry expressed skepticism of Beyler's findings. He stated that court records showed evidence of Willinghams guilt in charges that he intentionally killed his daughters in the fire. Perry is quoted in the report as stating of Willingham, \\"Im familiar with the latter-day supposed experts on the arson side of it,\\" and Perry said that court records provide \\"clear and compelling, overwhelming evidence that he was in fact the murderer of his children.\\"[64]  The Corsicana Fire Department also released a 19-page rebuttal of Beyler's report, stating that the report overlooked several key points that would show Willingham to be guilty.[65]\\r\\n\\r\\nOn July 23, 2010, the Texas Forensic Science Commission released a report saying that the conviction was based on \\"flawed science\\", but that there is no indication that the arson authorities were negligent or committed willful misconduct.[66]\\r\\n\\r\\nCarlos DeLuna was convicted of murder and executed in 1989 for the killing of a 24-year-old gas station attendant on the evening of February 4, 1983.[55]  Since DeLuna's execution by lethal injection, doubts have been raised about the conviction and the question of his guilt. An investigation published by the Columbia Human Rights Law Review in May 2012 has strengthened these claims of innocence by detailing a large amount of evidence suggesting the actual murderer was Carlos Hernandez, a similar-looking man who lived in a nearby neighborhood.[67]\\r\\n\\r\\nFrances Newton was executed in 2005 despite much doubt about her guilt, and much confusion over the actual weapon used in the murder(s) for which she was sentenced to death.[68]\\r\\n\\r\\nJohnny Frank Garrett was executed in 1992 for killing  76-year-old nun Tadea Benz in Amarillo in 1981. In 2004, after DNA-analyses, Leoncio Perez Rueda was found to be the murderer of Narnie Box Bryson, who was killed four months before Sister Benz. After being confronted, the murderer confessed to killing Bryson. Rueda is also believed to have been the real murderer of Sister Benz.\\r\\n\\r\\nFour Mexican nationals have been recently executed in Texas ÿ Jos Medelln in 2008, Humberto Leal Garca in 2011, dgar Tamayo rias in 2014 and Rubn Crdenas Ramrez in 2017 (Prior to Medellin v. Texas, four Mexican nationals were executed by Texas, who were Ram܇n Montoya, Irineo Montoya, Miguel Flores, and Javier Surez Medina, in 1993, 1997, 2000 and 2001 respectively.) At the time of their arrests in the early 1990s, neither had been informed of their rights as Mexican nationals to have the Mexican consulate informed of the charges and provide legal assistance. A 2004 ruling by the International Court of Justice concluded that the U.S. had violated the rights of 51 Mexican nationals, including Medelln and Garca, under the terms of a treaty the U.S. had signed.[69] In response to the ruling, the Bush administration issued an instruction that states comply,[70] but the U.S. Supreme Court ruled that it had exceeded its authority. The Supreme Court also ruled in Medellin v. Texas that the treaty was not binding on states until Congress enacted statutes to implement it, and in Leal Garcia v. Texas declined to place a stay on the executions in order to allow Congress additional time to enact such a statute. A 2008 ruling by the International Court of Justice asked the United States to place a stay on the executions, but Texas officials stated that they were not bound by international law.[71]\\r\\n\\r\\nGarca supporters complained about the use of bite mark analysis and luminol in determining his guilt.[72] However, Garca accepted responsibility for the crimes and apologized before his execution.[73]\\r\\n\\r\\nRegarding the Garca execution, Texas Governor Rick Perry stated that \\"If you commit the most heinous of crimes in Texas, you can expect to face the ultimate penalty under our laws.\\"[74]","input":"What type of execution is used in texas?"},{"output":"similar to a confederation","context":"\\r\\n\\r\\nEuropean Commission\\r\\n\\r\\nCouncil of the EU\\r\\n\\r\\nThe politics of the European Union are different from other organisations and states due to the unique nature of the European Union (EU). The EU is similar to a confederation, where many policy areas are federalised into common institutions capable of making law; however the EU does not, unlike most states, control foreign policy, defence policy or the majority of direct taxation policies (the EU does limit the level of variation allowed for VAT). These areas are primarily under the control of the EU's member states although a certain amount of structured co-operation and coordination takes place in these areas. For the EU to take substantial actions in these areas, all Member States must give their consent. EU laws that override national laws are more numerous than in historical confederations; however the EU is legally restricted from making law outside its remit or where it is no more appropriate to do so at a national or local level (subsidiarity) when acting outside its exclusive competencies. The principle of subsidiarity does not apply to areas of exclusive competence.\\r\\n\\r\\nThe common institutions mix the intergovernmental and supranational (similar to federal) aspects of the EU. The EU treaties declare the EU to be based on representative democracy, and direct elections take place to the European Parliament. The Parliament, together with the Council, form the legislative arm of the EU. The Council is composed of national governments, thus representing the intergovernmental nature of the EU. Laws are proposed by the European Commission which is appointed by and accountable to the Parliament and Council although it has very few executive powers.\\r\\n\\r\\nAlthough direct elections take place every five years, there are no cohesive political parties in the national sense. Instead, there are alliances of ideologically associated parties who sit and vote together in Parliament. The two largest parties are the European People's Party (centre-right) and the Party of European Socialists (centre-left) with the former forming the largest group in Parliament since 1999. As well as there being left and right dividing lines in European politics, there are also divides between those for and against European integration (Pro-Europeanism and Euroscepticism) which shapes the continually changing nature of the EU which adopts successive reforming treaties. The latter is stronger in northern Europe, especially the United Kingdom, and some member states are less integrated than others (Opt-outs).\\r\\n\\r\\n- Article 10 of the Treaty on European Union\\r\\n\\r\\nLegitimation of the European Union rests on the Treaty System. The move toward unification first arose in the Kellogg-Briand Pact in 1928, which gained adherent countries during negotiations and took on a theme of integration for the achievement of peace between the Great Powers.[1] After World War Two, Europe sought to end conflict permanently between France and Germany. In the spirit of the Marshall Plan, those two nations signed the Treaty of Paris in 1951, establishing the European Coal and Steel Community. Since then, the Treaty of Paris, which focused on price setting and competition for purposes of a common market, has been superseded. The legal basis for the European Community now rests on two treaties: The Treaty for the European Union of 1958; and The Treaty of Maastricht of 1992. The various additions and modifications of treaties has led to a patchwork of policy and planning, which contributes to the unwieldiness of the EU. The pastiche of treaties, and not a single actualising charter of government, form the constitutional basis of the European Union. This ambiguity is a primary cause of \\"democratic deficit.\\"\\r\\n\\r\\nThe EU itself is a legal personality and a set of governing institutions empowered by the treaties. However sovereignty is not invested in those institutions, it is pooled with ultimate sovereignty resting with the national governments. Yet in those areas where the EU has been granted competencies, it does have the power to pass binding and direct laws upon its members.\\r\\n\\r\\nThe competencies of the European Union stem from the original Coal and Steel Community, which had as its goal an integrated market. The original competencies were regulatory in nature, restricted to matters of maintaining a healthy business environment. Rulings were confined to laws covering trade, currency, and competition. Increases in the number of EU competencies result from a process known as functional spillover. Functional spillover resulted in, first, the integration of banking and insurance industries to manage finance and investment. The size of the bureaucracies increased, requiring modifications to the treaty system as the scope of competencies integrated more and more functions. While member states hold their sovereignty inviolate, they remain within a system to which they have delegated the tasks of managing the marketplace. These tasks have expanded to include the competencies of free movement of persons, employment, transportation, and environmental regulation.\\r\\n\\r\\nTypes of Legislation\\r\\n\\r\\nThe Ordinary Legislative Procedure (OLP) is the formal, main legislative procedure. In OLP, the European Parliament (EP) and the Council act in much the same way as a bicameral congress, an example being the British Upper and Lower Houses or the American Congress. The co-decision rule in Maastricht, and the subsequent Lisbon Treaty, is ultimately gave the EP and the Council equal weight and formalised OLP as the main legislative procedure. It also gives the EP veto power. The two legislative powers of the OLP are directives and regulations. A directive requires the member states to pass the new law individually, a process called \\"transposition.\\" The difference in the timing of completing transposition is the \\"democratic deficit.\\" A regulation acts on all the member states at once and is effective immediately.\\r\\n\\r\\nThe other legislation option is the Special Legislative Procedure (SLP). The SLP consists of the passing of laws proposed by member states, the Central Bank or Investment Bank, or the Court of Justice. Judicial activismthe interpretation of the spirit of law rather than the letter of lawis handled by the SLP. The procedure is discussed either in the EP with consultation with the Council, or in the Council with participation of the EP. In other words, the consultative role stops short of the equal weight given to both the EP and the Council. While provided for in the treaties, the SLP is not as formal as the OLP, relying less on the hierarchical structure. This lends more credence to Kleine's argument of the contrariness of the EU's agenda-setting procedure.[2]\\r\\n\\r\\nMechanics of Legislation\\r\\n\\r\\nIn the OLP, there are four types of rulings: Regulations, Directives, Decisions, and Recommendations. Decision-making is shared by the European Parliament and the Council, which are equally weighted. \\r\\nRegulations are binding on all member states effective immediately. \\r\\nDirectives are binding on all member states, but the implementation is left up to the national courts in a process called transposition. However, since member states set their own timelines for transposition, there is a democratic deficit between states. \\r\\nDecisions are binding on non-state litigants to whom they are addressed.[3]\\r\\nRecommendations are intended to guide legal judgments and are non-binding, similar to opinions.\\r\\n\\r\\nIn the SLP, the Council is the sole ruling body. The European Parliament is involved strictly in a consultative role and can be ignored.\\r\\n\\r\\nThere are twenty-eight member states who have conferred powers upon the EU institutions (other countries are tied to the EU in other ways). In exchange for conferring competencies, EU states are assigned votes in the Council, seats in Parliament and a European Commissioner among other things. The internal government of member states vary between presidential systems, monarchies, federations and microstates however all members must respect the Copenhagen criteria of being democratic, respecting human rights and having a free market economy. Members joined over time, starting with the original six in 1958 and more members joining in the near future.\\r\\n\\r\\nSome member states are outside certain areas of the EU, for example the eurozone is composed of only 19 of the 28 members and the Schengen Agreement currently includes only 22 of the EU members. However the majority of these are in the process of joining these blocs. A number of countries outside the EU are involved in certain EU activities such as the euro, Schengen, single market or defence.[4][5][6]\\r\\n\\r\\nThe primary institutions of the European Union are the European Commission, the European Council, the Council of the European Union (Council) and the European Parliament.\\r\\n\\r\\nThe ordinary legislative procedure, applies to nearly all EU policy areas. Under the procedure, the Commission presents a proposal to Parliament and the Council. They then send amendments to the Council which can either adopt the text with those amendments or send back a \\"common position\\". That proposal may either be approved or further amendments may be tabled by the Parliament. If the Council does not approve those, then a \\"Conciliation Committee\\" is formed. The Committee is composed of the Council members plus an equal number of MEPs who seek to agree a common position. Once a position is agreed, it has to be approved by Parliament again by an absolute majority.[7][8] There are other special procedures used in sensitive areas which reduce the power of Parliament.\\r\\n\\r\\nThe European Parliament shares the legislative and budgetary authority of the Union with the Council. Its 766 members are elected every five years by universal suffrage and sit according to political allegiance. It represents all European Citizens in the EU's legislative process, in contrast to the Council, which represents the Member States. Despite forming one of the two legislative chambers of the Union, it has weaker powers than the Council in some limited areas, and does not have legislative initiative. It does, however, have powers over the Commission which the Council does not.[9] The powers of the Parliament have increased substantially over the years, and in nearly all areas it now has equal power to the Council.\\r\\n\\r\\nThe European Council is the group of heads of state or government of the EU member states. It meets four times a year to define the Union's policy agenda and give impetus to integration. The President of the European Council, Donald Tusk, is the person responsible for chairing and driving forward the work of the institution, which has been described as the highest political body of the European Union.[10]\\r\\n\\r\\nThe Council of the European Union (informally known as the Council of Ministers or just the Council) is a body holding legislative and some limited executive powers and is thus the main decision making body of the Union. Its Presidency rotates between the states every six months. The Council is composed of twenty-eight national ministers (one per state). However the Council meets in various forms depending upon the topic. For example, if agriculture is being discussed, the Council will be composed of each national minister for agriculture. They represent their governments and are accountable to their national political systems. Votes are taken either by majority or unanimity with votes allocated according to population.[11]\\r\\n\\r\\nThe European Commission is composed of one appointee from each state, currently twenty-eight, but is designed to be independent of national interests. The body is responsible for drafting all law of the European Union and has a monopoly over legislative initiative. It also deals with the day-to-day running of the Union and has a duty to uphold the law and treaties (in this role it is known as the \\"Guardian of the Treaties\\").[12]\\r\\n\\r\\nThe Commission is led by a President who is nominated by the Council (in practice the European Council) and approved by Parliament. The remaining twenty-seven Commissioners are nominated by member-states, in consultation with the President, and has their portfolios assigned by the President. The Council then adopts this list of nominee-Commissioners. The Council's adoption of the Commission is not an area which requires the decision to be unanimous, their acceptance is arrived at according to the rules for qualified majority voting.  The European Parliament then interviews and casts its vote upon the Commissioners.  The interviews of individual nominees are conducted separately, in contrast to Parliament's vote of approval which must be cast on the Commission as a whole without the ability to accept or reject individual Commissioners.  Once approval has been obtained from the Parliament the Commissioners can take office.[13] The current president is Jean-Claude Juncker (EPP); his commission was elected in 2014.[12]\\r\\n\\r\\nDirect elections take place to the European Parliament every five years. The Council and European Council is composed of nationally elected or appointed officials and thus are accountable according to national procedures. The Commission also isn't direct elected although future appointments of the President must take into account of results of Parliament's elections.\\r\\n\\r\\nParliament's elections are held by universal suffrage of EU citizens according to national restrictions (such as age and criminal convictions). Proportional representation is used in all parliamentary constituencies.[14] Members of the European Parliament cannot also be elected nationally and are elected in national or sub-national constituencies. The first such election was of the in 1979. The latest elections were in 2014 The turnout has fallen in every EU election since 1979. In 2014, the overall turnout was 42.6%, down from 43.0% in 2009. In Britain the turnout was 35.6% up from 34.7% in 2009, in France and Germany turnout also increased, whereas in Italy and Poland turnout decreased.\\r\\n\\r\\nPolitical parties in the member states organise themselves with like-minded parties in other states into political parties at European level or Europarties. Most national parties are a member of one of these Europarties and there are currently 11 that are recognised and receive EU funding. Europarties behave and operate to a certain extent like national parties but only the larger ones (EPP, PES, ELDR) put forward comprehensive manifestos during the campaigns for the European elections.\\r\\n\\r\\nThe Europarties are horizontally present in all the main institutions ÿ Council, Commission, Parliament ÿ but are most active through their political groups in Parliament. At the beginning of every parliamentary term, most organise themselves with other parties, non-attached national parties or independents to form a political group. No party has ever held a majority in the Parliament, this does not have a great effect as it does not form a government but there is usual a coalition between the two major parties to elect the President of the European Parliament.[15][16][17]\\r\\n\\r\\nThe EU's foreign affairs are driven by its Common Foreign and Security Policy (CFSP) and also through the Commission-led economic trade negotiations. The EU's chief diplomat, sometimes dubbed its foreign minister, is the High Representative Federica Mogherini.\\r\\n\\r\\nForeign policy is still largely the domain of the member-states however and there has been significant disagreement between members. The failure to present a common voice on the world stage has led to the EU being sidelined in international negotiations.\\r\\n\\r\\nThe Financial Perspective for 2007ÿ2013 was defined in 2005 when EU members agreed to fix the common budget to 1.045% of the European GDP.[18] UK Prime Minister Tony Blair agreed to review the British rebate, negotiated by Margaret Thatcher in 1984. Former French president Jacques Chirac declared this increase in the budget will permit Europe to \\"finance common policies\\" such as the Common Agricultural Policy or the Research and Technological Development Policy. France's demand to lower the VAT in catering was refused.[19] Controversial issues during budget debates include the British rebate, France's benefits from the Common Agricultural Policy, Germany and the Netherlands' large contributions to the EU budget, reform of the European Regional Development Funds, and the question of whether the European Parliament should continue to meet both in Brussels and Strasbourg.\\r\\n\\r\\nThe Treaty establishing a Constitution for Europe (TCE), commonly referred to as the European Constitution, is an international treaty intended to create a constitution for the European Union. The constitution was rejected by France and the Netherlands, where referendums were held[20] causing other countries to postpone or halt their ratification procedures. Late in 2009, a new Reform Treaty was ratified by all member states of the European Union, and took effect on 1 December 2009.\\r\\n\\r\\nEnlargement of the Union's membership is a major political issue, with division over how far the bloc should expand. While some see it as a major policy instrument aiding the Union's development, some fear over-stretch and dilution of the Union.[21][22]\\r\\n\\r\\n\\"Counter-nationalistic shearing stress\\" is the term coined by one commentator for the theoretical tendency of certain regions of larger countries of the EU to wish to become fully independent members within the wider context of the European Union's \\"bigger umbrella\\".  If the Union is to become \\"ever closer\\", it follows that regions with their own distinctive histories and identities within the existing member nations may see little reason to have a layer of \\"insulation\\" between themselves and the EU.  The surprisingly close vote on Scottish Independence in September 2014 may be seen in this context.  Others have suggested that regions of Germany could be candidates for \\"Euro-Balkanisation\\", particularly given Germany's commitment to the EU project and to a more nuanced, mature view of the notion of national allegiance.","input":"What type of government is the european union?"},{"output":"motor control","context":"","input":"What is the major function of the cerebellum?"},{"output":"clay tablet","context":"In the Ancient Near East, clay tablets (Akkadian ?uppu(m) ??)[1] were used as a writing medium, especially for writing in cuneiform, throughout the Bronze Age and well into the Iron Age.\\r\\nCuneiform characters were imprinted on a wet clay tablet with a stylus often made of reed (reed pen). Once written upon, many tablets were dried in the sun or air, remaining fragile. Later, these unfired clay tablets could be soaked in water and recycled into new clean tablets. Other tablets, once written, were fired in hot kilns (or inadvertently, when buildings were burnt down by accident or during conflict) making them hard and durable. Collections of these clay documents made up the very first archives. They were at the root of first libraries. Tens of thousands of written tablets, including many fragments, have been found in the Middle East.[2][3]\\r\\nIn the Minoan/Mycenaean civilizations, writing has not been observed for any use other than accounting. Tablets serving as labels, with the impression of the side of a wicker basket on the back, and tablets showing yearly summaries, suggest a sophisticated accounting system. In this cultural region the tablets were never fired deliberately, as the clay was recycled on an annual basis. However, some of the tablets were \\"fired\\" as a result of uncontrolled fires in the buildings where they were stored. The rest are still tablets of unfired clay, and extremely fragile; some modern scholars are investigating the possibility of firing them now, as an aid to preservation.\\r\\n\\r\\n\\r\\nWriting was not as we see it today. In Mesopotamia, it started out as simple counting marks, alongside which sometimes a non-arbitrary well understood the sign, in the form of a simple picture image, that was cut into wood, stone, pots but more often pressed onto clay tokens. In that way, recorded accounts of amounts of goods involved in a transaction could be made. This convention began when people developed agriculture and settled into permanent communities that were centered on increasingly large and organized trading marketplaces[4] These marketplaces traded sheep, grain, and bread loaves, recording the transactions with clay tokens. These initially very small clay tokens were continually used all the way from the pre-historic Mesopotamia period, 9000 BC, to the start of the historic period around 3000 BC, when the use of writing for recording was widely adopted.[4]\\r\\nThe clay tablet was thus being used by scribes to take down the events of what was happening during his time. Tools that these scribes used were styluses with sharp triangular tips, making it easy to leave markings on the clay,[5] the clay tablets themselves came in a variety of colors such as bone white, chocolate, and charcoal.[6] Pictographs then began to appear on clay tablets around 4000 BC, and after the later development of Sumerian cuneiform writing, a more sophisticated partial syllabic script evolved that by around 2500 BC was capable of recording the vernacular, the everyday speech of the common people.[6]\\r\\nSumerians used what is known as pictograms.[4] Pictograms are symbols that express a pictorial concept, a logogram, as the meaning of the word. Early writing also began in Ancient Egypt using hieroglyphs. Early hieroglyphs and some of the modern Chinese characters are other examples of pictographs. The Sumerians later shifted their writing to Cuneiform, defined as \\"Wedge writing\\" in Latin, which added phonetic symbols, syllabograms.[5]\\r\\nText on clay tablets took the forms of myths, fables, essays, hymns, proverbs, epic poetry, laws, plants, and animals.[6] What these clay tablets allowed was for individuals to record who and what was significant. An example of these great stories was The Story of Gilgamesh. This story would tell of the great flood that destroyed Sumer. Remedies and recipes that would have been unknown were then possible because of the clay tablet. Some of the recipes were stew, which was made with goat, garlic, onions and sour milk.\\r\\nBy the end of the 3rd Millennium BC, (2200-2000 BC), even the \\"short story\\" was first attempted, as independent scribes entered into the philosophical arena, with stories like: The Debate between Bird and Fish, and other topics, (List of Sumerian debates).\\r\\nCommunication grew faster as now there was a way to get messages across just like mail. Important and private clay tablets were coated with an extra layer of clay, that no one else would read it. This means of communicating was used for over[6] 3000 years in fifteen different languages. Sumerians, Babylonians and Eblaites all had their own clay tablet libraries.\\r\\nAs Sumerian writing on clay tablets became mainstream, others began to complain that the Sumerian word-signs, were indeed too complicated to master. Not everyone could interpret the signs and they wanted a writing system that was simple yet got the message across. Elamites, Hurrians and Ugaritans then eliminated this from their style or form of writing. Tablets that were found south of Baghdad were made up of literary works, dictionaries, prayers, omens and astronomical recordings that were still in their own positions. Ugarites took advantage and created the first of the Alphabets.[6] The tablet was said to have 32 cuneiform letters which was discovered in Syria about 1450 B.C. Egyptians took it to the next level by becoming the earliest individuals to have their very own writing system. Mentioned earlier they used hieroglyphics instead of the normal alphabets. Defining hieroglyphics you get the Greek phrase sacred writing because of the early belief, that knowledge was a gift from their gods. The two main types of hieroglyphics were logograms and phonograms. Logograms represented ideas while phonograms focused on sounds that were closely alike to the alphabet.\\r\\nThe T?rt?ria tablets, the Danubian civilization, may be still older, having been dated by indirect method (bones found near the tablet were carbon dated) to before 4000 BC, and possibly dating from as long ago as 5500 BC, but their interpretation remains controversial because the tablets were fired in a furnace and the properties of the carbon changed accordingly.[7]","input":"What tools did the sumerians use for writing?"},{"output":"the late 1600s","context":"The Canadian horse is a horse breed from Canada. It is a strong, well-muscled breed of horse, usually dark in colour. The horses are generally used for riding and driving. Descended from draft and light riding horses imported to Canada in the late 1600s, it was later crossed with other British and American breeds. During the 18th century the Canadian horse spread throughout the northeastern US, where it contributed to the development of several horse breeds. During the peak popularity of the breed, three subtypes could be distinguished, a draft horse type, a trotting type and a pacing type. Thousands of horses were exported in the 19th century, many of whom were subsequently killed while acting as cavalry horses in the American Civil War. These exports decreased the purebred Canadian population almost to the point of extinction, prompting the formation of a studbook and the passage of a law against further export.\\r\\nExperimental breeding programs in the early 20th century succeeded in re-establishing the breed to some extent, but mechanization, combined with two world wars, again resulted in the breed almost becoming extinct. In the 1980s, concerned with the declining population numbers, interested breeders undertook a promotional program, which resulted in renewed interest in the breed. By the 1990s, population numbers were higher, and genetic studies in 1998 and 2012 found relatively high levels of genetic diversity for a small breed. However, livestock conservation organizations still consider the breed to be at risk, due to low population numbers.\\r\\n\\r\\n\\r\\nMost Canadian Horses are dark coloured: black, bay, or brown. A few chestnuts are found, occasionally with flaxen manes and tails, and the cream gene appears in the breed as the result of the genetic influence of one stallion.[1] While some sources state that the gene for gray is no longer found in the breed, after the genetic bottleneck of the late 20th century,[1] the preservation society for the breed states that they can be \\"rarely grey\\".[2] Their height averages 14 to 16?hands (56 to 64?inches, 142 to 163?cm) and stallions average 1,050 to 1,350 pounds (480 to 610?kg) in weight, while mares weigh 1,000 to 1,250 pounds (450 to 570?kg).[3]\\r\\nThe Canadian horse has a rather short, high-set head with a broad forehead. The neck is arched and graceful, and the chest, back and loins broad and strongly muscled. The shoulders and croup are sloping, with a relatively high-set tail. Overall, the breed gives the impression of strength and agility. Their heavy and wavy mane and tail, arched necks and finely boned heads are all reminiscent of Andalusian and Barb ancestry.[4] Their trot is described as flashy. They are hardy horses and easy keepers. Today, most Canadian horses are used as riding and driving horses, and are known for their jumping ability.[1] They are seen in competition in almost every discipline, as well as for leisure riding.[5] They can also be found in light draft work, trail riding, and working as a stock horse.[6]\\r\\nUnlike most breeds, there is a set naming system that is used to identify individuals based on the registration format employed by the Canadian Livestock Records Corporation. First comes the prefix, the farm or breeding establishment of which the foal was born into, followed by the sire's name, and lastly the given name for the foal. Each year a different letter is assigned to begin the given name for the foal, and it is by the year's letter that the foal is named. Some older horses do not fall under this naming strategy, but it is now mandatory in naming registered offspring coming from purebred Canadian lines. Originally, horses were tattooed with identification numbers, but now microchipping is the identification technology chosen by the breed registry.[5]\\r\\nThe Canadian Horse descended from the French stock Louis XIV sent to Canada in the late 17th century.[6] The initial shipment, in 1665, consisted of two stallions and twenty mares from the Royal Stables in Normandy and Brittany, the center of French horse breeding.[7] Only 12 of the 20 mares survived the trip. Two more shipments followed, one in 1667 of 14 horses (mostly mares, but with at least one stallion), and one in 1670 of 11 mares and a stallion. The shipments included a mix of draft horses and light horses, the latter of which included both pacing and trotting horses.[1] The exact origins of all the horses are unknown, although the shipments probably included Bretons, Normans, Arabians, Andalusians and Barbs.[5][7]\\r\\nThe horses were leased to gentleman farmers or religious orders for money or in exchange for a foal, and they remained the property of the king for three years. Despite poor conditions and hard work, the horses thrived in Canada, and were given nicknames that included \\"the little iron horse\\" and \\"the horse of steel\\".[5] Population numbers rose quickly from the early stock, with 148 horses by 1679, 218 horses by 1688, 684 by 1698, and by 1709, enough that the government limited farmers to owning two horses and a foal, with additional horses to be slaughtered, although this law was a failure in terms of enforcement. During the 1700s, the \\"French Canadian Horse\\" spread through what is now eastern Michigan and Illinois in the United States, and lived a generally feral existence, with many escaping human control completely.[1] During the Expulsion of the Acadians in the mid-18th century, the English seized the livestock of the Acadians, including horses. Some of these animals were transported to Sable Island, where their descendents became the Sable Island horse.[8][9] In the late 18th century, imported horses from the US and the British Isles were crossbred with existing Canadian stock. By the 19th century, they were found performing light draft work, as well as riding and driving duties.[1] Cornelius Krieghoff, a 19th-century Canadian painter, was known for his works featuring the Canadian horse, who he usually showed in association with the French habitants, as opposed to the English settlers in the area. His paintings generally portrayed the Canadian horse in a utilitarian, workhorse role, often in winter scenes.[10]\\r\\nIn 1849, there were estimated to be more than 150,000 Canadian horses, and many were exported from Canada annually. Some were shipped to the West Indies, where they possibly contributed to gaited breeds such as the Paso Fino.[7] By the middle of the 19th century, Canadian horses had spread through the northeastern US, where they were used for racing, as roadsters, and, due to their stamina, to pull freight wagons and stagecoaches.[1] Many played a role in the development of other breeds, including the Morgan horse, the American Saddlebred and the Standardbred. Although used extensively in the US, no efforts were made to establish a purebred population, studbook, or breed association in that country.[11] Thousands of horses imported to the US from Canada were used as artillery and cavalry horses in the American Civil War, where many were killed. One equine historian states that \\"The Canadian horse played a major role in the history of that war; it has even been said that the North won simply on the fact that its soldiers had the better horse ÿ the Canadian.\\"[7]\\r\\nBy 1880, through exports and war casualties, Canadian horses had almost become extinct.[11] In 1885, the Canadian Horse Breeders Association was formed to inspect and approve breeding stock with the aim of creating a studbook for the breed, and in 1886, further export from Canada was forbidden by Quebec law. In 1913, an experimental breeding program was begun at Cap-Rouge by the Canadian government. The program's goal was to breed larger horses that retained the endurance and vitality for which the breed was known, and succeeded in increasing the size of stallions to 15.2 to 16?hands (62 to 64?inches, 157 to 163?cm) high and 1,200 to 1,500 pounds (540 to 680?kg) in weight, with mares slightly smaller. However, mechanization, combined with World War I and World War II,[1] ended the federal breeding program, and in 1940 all breeding stock was sold at auction. However, the province of Quebec re-established the program at Deschambault.[7] The program lasted there until 1979, when the herd was again disbanded and sold at auction.[8]\\r\\nBy the 1970s, the popularity of the breed had decreased significantly, and there were approximately 400 Canadian horses worldwide, with only around five annual registrations between 1970 and 1974.[12] Several interested breeders began a campaign of preservation and promotion, which resulted in a Canadian team winning the 1987 North American Driving Championships.[1] Popularity began to increase, and by the mid-1990s population numbers were between 2,500 and 3,000, and The Livestock Conservancy, which had classified the breed as \\"critical\\", changed its designation to \\"rare\\". With the increase in popularity came pressure for the breed standard to change to meet modern show and market trends, by breeding for taller horses with more refinement. In 2002, the Canadian Horse Heritage and Preservation Society was formed in response to these pressures, with a goal of preserving the original Canadian horse type.[12] The Canadian Horse Breeders Association remains the official registering body for the Canadian horse, as governed by the Canadian federal Animal Pedigree Act, with the responsibility to \\"monitor registration, identification, and the keeping of the stud book for Canadian horses\\".[13] It is also responsible for inspecting breeding stock before they are registered with the studbook. The studbook is maintained by the Canadian Livestock Records Corporation.[13] Since the beginning of the studbook, there have been over 13,600 horses registered. In 2012, 208 new horses were registered, mainly in Quebec.[14] The Livestock Conservancy still considers the breed to be threatened, a designation given to breeds with a global population of less than 5,000 and annual US registrations of fewer than 1,000.[15] Rare Breeds Canada considers the breed to be at risk, with fewer than 500 annual registrations of female breeding stock.[16]\\r\\nIn a study of mitochondrial DNA published in 2012, the Canadian horse and the Newfoundland pony were found to be the most genetically diverse of the Canadian breeds studied, which also included the Sable Island horse and the Lac La Croix pony. The Canadian horse showed high haplotype diversity, sharing haplotypes with all Canadian populations, as well as draft breeds, Nordic pony breeds and British mountain and moorland pony breeds also tested in the study. The Canadian horse had been shown to be related to draft horse breeds, including the Percheron, Belgian and Clydesdale, in previous microsatellite loci studies. This relationship was supported by findings in the 2012 study.[17] The high levels of diversity in the Canadian horse supported the conclusions of a 1998 study, which determined that the small population size and historical genetic bottlenecks had not resulted in a significant loss of genetic variation. The 1998 paper also stated that the Canadian horse did not show inbreeding any more significant than other, more popular, breeds.[18]\\r\\nThe Canadian horse is a common animal symbol of Canada.[5] In 1909, the Canadian Parliament declared it the national breed of the country, and in 2002 was made an official animal symbol of Canada by Parliamentary Act.[19] In 2010, the provincial legislature of Quebec named it a heritage breed of the province.[5]\\r\\nDuring the peak popularity of the breed, three main types could be distinguished.[1] All three are now considered extinct,[20] having disappeared or been merged back into the main Canadian horse population. The first, the Canadian Heavy Draft or St. Lawrence, which disappeared by the late 1700s, probably developed from Shire and Clydesdale crosses. They were probably a popular export to New England, which bred large numbers of horses for Caribbean plantations.[1] The second, the Frencher, sometimes also called the St. Lawrence, was a trotting horse known for its power and speed, resulting from crosses with Thoroughbreds. Mixed with French trotting lines, they played a role in the development of the US trotting horses.[1]\\r\\nThe third type was the Canadian Pacer, which was historically better documented than the other two types. Canadian Pacers were likely the result of breeding pacing horses imported from France with Narragansett Pacers from New England. The resulting horses were known for their ability to race on ice.[1] From there, they were exported to the United States, where North Carolina became a breeding center, later exporting them to Tennessee in the late 1700s.[1] Pedigrees were not maintained, so early breeding histories are often impossible to trace.[21] The Canadian Pacer influenced the Tennessee Walker,[1] the American Saddlebred[1] and the Standardbred.[1]\\r\\nCommonly called \\"Canucks\\", the fastest members of the breed came from Quebec near the St. Lawrence River. Racing began in this area during the long, severe winters, when Sunday races after attending church for Mass became common. Eventually these races became large enough to endanger the church-going populace, and races were banned within a certain distance of churches. They instead moved to local rivers, whose smooth, frozen surfaces provided useful raceways, and the resulting contests drew attention to the pacers from Quebec.[21]\\r\\nSeveral horses imported to the United States from Canada had a lasting impact on American horse breeding. In the early 1800s, a roan-coloured stallion named Copperbottom was imported to Lexington, Kentucky from Quebec, through Michigan. He began to be offered for stud service in 1816, and his progeny spread throughout the eastern US. Known mainly as saddle stock, they also included several pacing horses.[21] Another roan stallion, Tom Hal, a successful pacer in his own right, founded an important family of pacers in the US. Appearing in Kentucky in 1824, he was offered for stud, and his offspring (many of whom carried on the family name, being differentiated only by the name of the owner) began the family of Standardbreds that included Little Brown Jug, Brown Hal, Star Pointer, Adios and Good Time, all champion harness racing horses.[21] Another pacing import to the US was a black stallion named Old Pilot, said to have been bred near Montreal, who originated the Pilot family of trotting horses. Old Pilot produced a son, also named Pilot, who was acclaimed as a sire of trotting horses, as well as being a successful harness horse himself.[22][23]","input":"When were the canadian breeds ancestors imported to canada?"},{"output":"Over 60 million people","context":"World War II was the deadliest military conflict in history in absolute terms of total casualties. Over 60 million people were killed, which was about 3% of the 1940 world population (est. 2.3 billion).[1] Statistics of military wounded are available for the major combatants. The tables below give a detailed country-by-country count of human losses. World War II fatality statistics vary, with estimates of total deaths ranging from 50 million to more than 80 million.[2] The higher figure of over 80 million includes deaths from war-related disease and famine. Civilians killed totalled 50 to 55 million, including 19 to 28 million from war-related disease and famine. Military deaths from all causes totalled 21 to 25 million, including deaths in captivity of about 5 million prisoners of war. Statistics on the number of military wounded are included whenever available.\\r\\nRecent historical scholarship has shed new light on the topic of Second World War casualties. Research in Russia since the collapse of the Soviet Union has caused a revision of estimates of Soviet WW2 fatalities.[3] According to Russian government figures, USSR losses within postwar borders now stand at 26.6 million[4][5] including 8.5 million due to war related famine and disease.[6] In August 2009 the Polish Institute of National Remembrance (IPN) researchers estimated Poland's dead at between 5.6 and 5.8 million.[7] Historian Rdiger Overmans of the Military History Research Office (Germany) published a study in 2000 that estimated the German military dead and missing at 5.3 million, including 900,000 men conscripted from outside of Germany's 1937 borders, in Austria, and in east-central Europe.[8][9][10] The People's Republic of China puts its war dead at 20 million,[11] while the Japanese government puts its casualties due to the war at 3.1 million.[12]\\r\\n\\r\\n\\r\\nCompiling or estimating the numbers of deaths and wounded caused during wars and other violent conflicts is a controversial subject. Historians often put forward many different estimates of the numbers killed and wounded during World War II.[13] The authors of the Oxford Companion to World War II maintain that \\"casualty statistics are notoriously unreliable.\\"[14] The table below gives data on the number of dead and military wounded for each country, along with population information to show the relative impact of losses. When scholarly sources differ on the number of deaths in a country, a range of war losses is given, in order to inform readers that the death toll is disputed. Since casualty statistics are sometimes disputed the footnotes to this article present the different estimates by official governmental sources as well as historians. Military figures include battle deaths (KIA) and personnel missing in action (MIA), as well as fatalities due to accidents, disease and deaths of prisoners of war in captivity. Civilian casualties include deaths caused by strategic bombing, Holocaust victims, German war crimes, Japanese war crimes, population transfers in the Soviet Union, Allied war crimes, and deaths due to war related famine and disease.\\r\\nThe sources for the casualties of the individual nations do not use the same methods, and civilian deaths due to starvation and disease make up a large proportion of the civilian deaths in China and the Soviet Union. The losses listed here are actual deaths; hypothetical losses due to a decline in births are not included with the total dead. The distinction between military and civilian casualties caused directly by warfare and collateral damage is not always clear-cut. For nations that suffered huge losses such as the Soviet Union, China, Poland, Germany, and Yugoslavia, sources can give only the total estimated population loss caused by the war and a rough estimate of the breakdown of deaths caused by military activity, crimes against humanity and war-related famine. The casualties listed here include 19 to 25 million war-related famine deaths in the USSR, China, Indonesia, Vietnam, the Philippines, India that are often omitted from other compilations of World War II casualties.[15][16] The footnotes give a detailed breakdown of the casualties and their sources, including data on the number of wounded where reliable sources are available.\\r\\nThe estimated breakdown for each Soviet republic of total war dead[13]^AY4\\r\\nThe source of the figures is Vadim Erlikman. Poteri narodonaseleniia v XX veke: spravochnik. Moscow, 2004. ISBN?5-93165-107-1. pp. 21ÿ35. Erlikman, a Russian historian, notes that these figures are his estimates.\\r\\nIncluded in the figures of total war dead for each nation are victims of the Holocaust.\\r\\nThe Holocaust is the term generally used to describe the genocide of approximately six million European Jews during World War II. Martin Gilbert estimates 5.7 million (78%) of the 7.3 million Jews in German occupied Europe were Holocaust victims.[228] Estimates of Holocaust deaths range between 4.9 and 5.9 million Jews.[229]\\r\\nStatistical breakdown of Jewish dead:\\r\\nThe figures for the pre-war Jewish population and deaths in the table below are from The Columbia Guide to the Holocaust.[229] The low, high and average percentage figures for deaths of the pre-war population have been added.\\r\\nSome scholars maintain that the definition of the Holocaust should also include the other victims persecuted and killed by the Nazis.[243][244]\\r\\nThe following figures are from The Columbia Guide to the Holocaust, the authors maintain that \\"statistics on Gypsy losses are especially unreliable and controversial. These figures (cited below) are based on necessarily rough estimates\\".[255]\\r\\nIncluded with total war dead are victims of Japanese war crimes.\\r\\nThe total war dead in the USSR includes victims of Soviet oppression. The number of deaths in the Gulag labor camps increased as a result of wartime overcrowding and food shortages.[288] The Stalin regime deported the entire populations of ethnic minorities considered to be potentially disloyal.[289] Since 1990 Russian scholars have been given access to the Soviet-era archives and have published data on the numbers of people executed and those who died in Gulag labor camps and prisons.[290] The Russian scholar Viktor Zemskov puts the death toll from 1941ÿ1945 at about 1 million based on data from the Soviet archives.[291] The Soviet-era archive figures on the Gulag labor camps has been the subject of a vigorous academic debate outside Russia since their publication in 1991. J. Arch Getty and Stephen G. Wheatcroft maintain that Soviet-era figures more accurately detail the victims of the Gulag labor camp system in the Stalin era.[292][293] Robert Conquest and Steven Rosefielde have disputed the accuracy of the data from the Soviet archives, maintaining that the demographic data and testimonials by survivors of the Gulag labor camps indicate a higher death toll.[294][295] Rosefielde posits that the release of the Soviet Archive figures is disinformation generated by the modern KGB.[296] Rosefielde maintains that the data from the Soviet archives is incomplete; for example, he pointed out that the figures do not include the 22,000 victims of the Katyn massacre.[297] Rosefielde's demographic analysis puts the number of excess deaths due to Soviet repression at 2,183,000 in 1939ÿ40 and 5,458,000 from 1941ÿ1945.[298] Michael Haynes and Rumy Husun accept the figures from the Soviet archives as being an accurate tally of Stalin's victims, they maintain that the demographic data depicts an underdeveloped Soviet economy and the losses in World War Two rather than indicating a higher death toll in the Gulag labor camps.[299]\\r\\nIn August 2009 the Polish Institute of National Remembrance (IPN) researchers estimated 150,000 Polish citizens were killed due to Soviet repression. Since the collapse of the USSR, Polish scholars have been able to do research in the Soviet archives on Polish losses during the Soviet occupation.[226] Andrzej Paczkowski puts the number of Polish deaths at 90,000ÿ100,000 of the 1.0 million persons deported and 30,000 executed by the Soviets.[300] In 2005 Tadeusz Piotrowski estimated the death toll in Soviet hands at 350,000.[301]\\r\\nThe Estonian State Commission for the Examination of Repressive Policies Carried out During the Occupations put civilian deaths due to the Soviet occupation in 1940ÿ1941 at 33,900 including (7,800 deaths) of arrested people, (6,000) deportee deaths, (5,000) evacuee deaths, (1,100) people gone missing and (14,000) conscripted for forced labor. After the reoccupation by the U.S.S.R., 5,000 Estonians died in Soviet prisons during 1944ÿ45.[302]\\r\\nThe following is a summary of the data from the Soviet archives:\\r\\nReported deaths for the years 1939ÿ1945 1,187,783, including: judicial executions 46,350; deaths in Gulag labor camps 718,804; deaths in labor colonies and prisons 422,629.[303]\\r\\nDeported to special settlements: (figures are for deportations to Special Settlements only, not including those executed, sent to Gulag labor camps or conscripted into the Soviet Army. Nor do the figures include additional deportations after the war).\\r\\nDeported from annexed territories 1940ÿ41 380,000 to 390,000 persons, including: Poland 309ÿ312,000; Lithuania 17,500; Latvia 17,000; Estonia 6,000; Moldova 22,842.[304] In August 1941, 243,106 Poles living in the Special Settlements were amnestied and released by the Soviets.[305]\\r\\nDeported during the War 1941ÿ1945 about 2.3 million persons of Soviet ethnic minorities including: Soviet Germans 1,209,000; Finns 9,000; Karachays 69,000; Kalmyks 92,000;Chechens and Ingush 479,000; Balkars 37,000; Crimean Tatars 191,014; Meskhetian Turks 91,000; Greeks, Bulgarians and Armenians from Crimea 42,000; Ukrainian OUN members 100,000; Poles 30,000.[306]\\r\\nA total of 2,230,500[307] persons were living in the settlements in October 1945 and 309,100 deaths were reported in special settlements for the years 1941ÿ1948.[308]\\r\\nRussian sources list Axis prisoner of war deaths of 580,589 in Soviet captivity based on data in the Soviet archives (Germany 381,067; Hungary 54,755; Romania 54,612; Italy 27,683; Finland 403, and Japan 62,069).[309] However some western scholars estimate the total at between 1.7 and 2.3 million.[310]\\r\\nGermany\\r\\nUSSR\\r\\nBritish Commonwealth\\r\\nU.S.\\r\\nThe Commonwealth War Graves Commission (CWGC) Annual Report 2014ÿ2015[373] is the source of the military dead for the British Empire. The war dead totals listed in the report are based on the research by the CWGC to identify and commemorate Commonwealth war dead. The statistics tabulated by the CWGC are representative of the number of names commemorated for all servicemen/women of the Armed Forces of the Commonwealth and former UK Dependencies, whose death was attributable to their war service. Some auxiliary and civilian organizations are also accorded war grave status if death occurred under certain specified conditions. For the purposes of CWGC the dates of inclusion for Commonwealth War Dead are 3 September 1939 to 31 December 1947.\\r\\n^A xAlbania\\r\\n^B xAustralia\\r\\n^C xAustria\\r\\n^D xBelgium\\r\\n^E xBrazil\\r\\n^F xBulgaria\\r\\n^G xBurma\\r\\n^H xCanada\\r\\n^I xChina Sources for total Chinese war dead are divergent and range from 10 to 20 million as detailed below.\\r\\n^J xCuba\\r\\n^K xCzechoslovakia\\r\\n^L xDenmark\\r\\n^M xDutch East Indies\\r\\n^MA xEgypt\\r\\n^N xEstonia\\r\\n^O xEthiopia\\r\\n^P xFinland\\r\\n^Q xFrance\\r\\n^R xFrench Indochina\\r\\n^S xGermany The following notes summarize German casualties, the details are presented in German casualties in World War II.\\r\\nGerman population\\r\\nTotal German war dead\\r\\nGerman military casualties\\r\\nCivilian Casualties\\r\\nCivilian casualties in air raids\\r\\n????1- The summary report of September 30, 1945 put total casualties for the entire period of the war at 305,000 killed and 780,000 wounded.[446]\\r\\n????2- The section Effects of Strategic Bombing on the German War Economy of October 31, 1945 put the losses at 375,000 killed and 625,000 wounded[447]\\r\\n????3- The section The Effect of Bombing on Health and Medical Care in Germany of January 1947 made a preliminary calculated estimate of air raid dead at 422,000. Regarding overall losses they concluded that \\"It was further estimated that an additional number, approximately 25% of known deaths in 1944ÿ45, were still unrecovered and unrecorded. With an addition of this estimate of 1944ÿ45 unrecorded deaths, the final estimation gave in round numbers a half a million German civilians killed by Allied aerial attacks\\"[448]\\r\\nCivilians killed in 1945 military campaign\\r\\nDeaths due to Nazi political, racial and religious persecution\\r\\nExpulsion and flight of ethnic Germans The following notes summarize German expulsion casualties, the details are presented in the flight and expulsion of Germans (1944ÿ1950), the forced labor of Germans in the Soviet Union' and the Demographic estimates of the flight and expulsion of Germans. The figures for these losses are currently disputed, estimates of the total deaths range from 500,000 to 2,000,000. The death toll attributable to the flight and expulsions was estimated at 2.2 million by the West German government in 1958.[455] German government reports which were released to the public in 1987 and 1989 have caused some historians in Germany to put the actual total at 500,000 to 600,000.[456] English language sources put the death toll at 2 to 3 million based on the West German government statistical analysis of the 1950s.[457][458][459][460][461][462][463][464][465][466]\\r\\nThe German government figures of 2.0 to 2.5 million civilian deaths due to the expulsions have been disputed by scholars since the publication of the results of the German church search service survey and the report by the German Federal Archive.[476][482][483][484][485][486][487][488]\\r\\nPost war increase in natural deaths\\r\\n^T xGreece\\r\\n^TA xGuam\\r\\n^U xHungary\\r\\n^V xIceland\\r\\n^W xIndia\\r\\nBengal famine of 1943\\r\\n^X xIran\\r\\n^Y xIraq\\r\\n^Z xIreland\\r\\n^AA xItaly\\r\\n???????????Military war dead\\r\\n???????????Confirmed dead were 159,957 (92,767 pre-armistice, 67,090 post armistice)[512]\\r\\n???????????Missing and presumed dead(including POWs) were 131,419 (111,579 pre-armistice, 19,840 post armistice)[513]\\r\\n???????????Losses by branch of service: Army 201,405; Navy 22,034; Air Force 9,096; Colonial Forces 354; Chaplains 91; Fascist militia\\r\\n???????????10,066; Paramilitary 3,252; not indicated 45,078.[514]\\r\\n???????????Military Losses by theatre of war: Italy 74,725 (37,573 post armistice); France 2,060 (1,039 post armistice);\\r\\n???????????Germany 25,430 (24,020 post armistice); Greece, Albania, and Yugoslavia 49,459 (10,090 post armistice);\\r\\n???????????USSR 82,079 (3,522 post armistice); Africa 22,341 (1,565 post armistice), at sea 28,438 (5,526 post armistice);\\r\\n???????????other and unknown 6,844 (3,695 post armistice).[515]\\r\\n^AB xJapan\\r\\nMilitary dead\\r\\n?????????????Key: Location, Army dead, Navy dead, (Total dead)\\r\\n?????????????Japan Proper: 58,100, 45,800, (103,900)\\r\\n?????????????Bonin Islands: 2,700, 12,500, (15,200)\\r\\n?????????????Okinawa: 67,900, 21,500, (89,400)\\r\\n?????????????Formosa (Taiwan): 28,500, 10,600, (39,100)\\r\\n?????????????Korea: 19,600, 6,900, (26,500)\\r\\n?????????????Sakhalin, the Aleutian, and Kuril Islands: 8,200, 3,200, (11,400)\\r\\n?????????????Manchuria: 45,900, 800, (46,700)\\r\\n?????????????China (inc. Hong Kong): 435,600, 20,100, (455,700)\\r\\n?????????????Siberia: 52,300, 400, (52,700)\\r\\n?????????????Central Pacific: 95,800, 151,400, (247,200)\\r\\n?????????????Philippines: 377,500, 121,100, (498,600)\\r\\n?????????????French Indochina: 7,900, 4,500, (12,400)\\r\\n?????????????Thailand: 6,900, 100, (7,000)\\r\\n?????????????Burma (inc. India): 163,000, 1,500, (164,500)\\r\\n?????????????Malaya & Singapore: 8,500, 2,900, (11,400)\\r\\n?????????????Andaman & Nicobar Islands: 900, 1,500, (2,400)\\r\\n?????????????Sumatra: 2,700, 500, (3,200)\\r\\n?????????????Java: 2,700, 3,800, (6,500)\\r\\n?????????????Lesser Sundas: 51,800, 1,200, (53,000)\\r\\n?????????????Borneo: 11,300, 6,700, (18,000)\\r\\n?????????????Celebes: 1,500, 4,000, (5,500)\\r\\n?????????????Moluccas: 2,600, 1,800, (4,400)\\r\\n?????????????New Guinea: 112,400, 15,200, (127,600)\\r\\n?????????????Bismarck Archipelago: 19,700, 10,800, (30,500)\\r\\n?????????????Solomon Islands: 63,200, 25,000, (88,200)\\r\\n\\r\\n?????????????Total: 1,647,200, 473,800, (2,121,000)\\r\\n?\\r\\nOverall, perhaps two thirds of all Japanese military dead came not from combat, but from starvation and disease.[519] In some cases this figure was potentially even higher, up to 80% in the Philippines[520] and a staggering 97% in New Guinea.[521]\\r\\n?????????????Army\\r\\n?????????????China after Pearl Harbor 202,958 killed and 88,920 wounded.\\r\\n?????????????vs. United States 485,717 killed and 34,679 wounded.\\r\\n?????????????vs. U.K. and Netherlands 208,026 killed and 139,225 wounded.\\r\\n?????????????vs. Australia 199,511 killed and 15,000 wounded.\\r\\n?????????????French Indochina 2,803 killed and 6,000 wounded.\\r\\n?????????????Manchuria & USSR 7,483 killed and 4,641 wounded.\\r\\n?????????????other overseas 23,388 killed and 0 wounded\\r\\n?????????????Japan proper 10,543 killed and 6,782 wounded\\r\\n?????????????Army total 1,140,429 killed and 295,247 wounded.\\r\\n??????????????Navy\\r\\n??????????????Sailors 300,386 killed and 12,275 wounded and missing.\\r\\n??????????????Civilians in Navy service 114,493 killed and 1,880 wounded and missing.\\r\\n??????????????Navy total 414,879 killed and 14,155 wounded and missing.\\r\\n?\\r\\nCivilian Dead\\r\\n1-Summary Report (July 1946) Total civilian casualties in Japan, as a result of 9 months of air attack, including those from the atomic bombs, were approximately 806,000. Of these, approximately 330,000 were fatalities.[541]\\r\\n2-United States Strategic Bombing Survey, Medical Division (1947) The bombing of Japan killed 333,000 civilians and injured 473,000. Of this total 120,000 died and 160,000 were injured in the atomic bombings, leaving 213,000 dead and 313,000 injured by conventional bombing.[542]\\r\\n3-The effects of air attack on Japanese urban economy. Summary report (1947) Estimated that 252,769 Japanese were killed and 298,650 injured in the air war.[543]\\r\\n4-The Effects of strategic bombing on Japanese morale Based on a survey of Japanese households the death toll was put at 900,000 dead and 1.3 million injured, the SBS noted that this figure was subject to a maximum sampling error of 30%.[544]\\r\\n5-Strategic Bombing Survey The Effects of Atomic Bombs on Hiroshima and Nagasaki The most striking result of the atomic bombs was the great number of casualties. The exact number of dead and injured will never be known because of the confusion after the explosions. Persons unaccounted for might have been burned beyond recognition in the falling buildings, disposed of in one of the mass cremations of the first week of recovery, or driven out of the city to die or recover without any record remaining. No sure count of even the prepaid populations existed. Because of the decline in activity in the two port cities, the constant threat of incendiary raids, and the formal evacuation programs of the Government, an unknown number of the inhabitants had either drifter away from the cities or been removed according to plan. In this uncertain situation, estimates of casualties have generally ranged between 100,000 and 180,000 for Hiroshima, and between 50,000 and 100,000 for Nagasaki. The Survey believes the dead at Hiroshima to have been between 70,000 and 80,000, with an equal number injured; at Nagasaki over 35,000 dead and somewhat more than that injured seems the most plausible estimate. [545]\\r\\n^AC xKorea\\r\\n^AD xLatvia\\r\\n^AE xLithuania\\r\\n^AF xLuxembourg\\r\\n^AG xMalaya and Singapore\\r\\n^AH xMalta 1,493 civilians were killed and 3,734 wounded during the Siege of Malta (World War II)[109] Maltese civilians killed during the siege are also included with U.K. civilian deaths by the Commonwealth War Graves Commission\\r\\n^AI xMexico\\r\\n^AJ xMongolia\\r\\n^AK xNauru\\r\\n^AL xNepal\\r\\n^AM xNetherlands\\r\\n??????Military deaths 6,750 which included 3,900 regular Army, 2,600 Navy forces, and 250 POW in Germany.\\r\\n??????Civilian deaths of 203,250 which included 1,350 Merchant seaman, 2,800 executed, 2,500 dead in Dutch concentration camps,\\r\\n???????20,400 killed by acts of war, 104,000 Jewish Holocaust dead, 18,000 political prisoners in Germany, 27,000 workers in Germany,\\r\\n???????3,700 Dutch nationals in the German armed forces and 7,500 missing and presumed dead in Germany and 16,000 deaths\\r\\n???????in the Dutch famine of 1944. Not Included in the figure of 210,000 war dead are 70,000 \\"indirect war casualties\\",\\r\\n????????which are attributed to an increase in natural deaths from 1940ÿ1945 and 1,650 foreign nationals killed while serving in the\\r\\n????????Dutch Merchant Marine[115]\\r\\n^AN xNewfoundland\\r\\n^AO xNew Zealand\\r\\n^AP xNorway\\r\\n??????????Military(Norwegian & Allied Forces) 2,000 (800 Army, 900 Navy and 100 Air).[123]\\r\\n??????????Civilians 7,500 (3,600 Merchant seaman, 1,500 resistance fighters, 1,800 civilians killed and 600 Jews killed)[123]\\r\\n??????????In German Armed Forces 700[123]\\r\\n^AQ xPapua New Guinea\\r\\n^AR xPhilippines\\r\\n^AS xPoland\\r\\nTotal Polish war dead\\r\\nPolish losses during the Soviet occupation (1939ÿ1941)\\r\\nPolish military casualties\\r\\n^AT xTimor\\r\\n^AU xRomania\\r\\n^AV xRuanda Urundi\\r\\n^AW xSouth Africa\\r\\n^AX xSouth Pacific Mandate\\r\\nThe following notes summarize Soviet casualties, the details are presented in World War II casualties of the Soviet Union\\r\\n^AZ xSpain\\r\\n^BA xSweden\\r\\n^BB xSwitzerland\\r\\n^BC xThailand\\r\\n^BD xTurkey\\r\\n^BE xUnited Kingdom and Colonies\\r\\n?????????Total war dead of 357,116; Navy (50,758); Army (144,079); Air Force (69,606); Women's Auxiliary Territorial Service (624);\\r\\n?????????Merchant Navy (30,248); British Home Guard (1,206) and Civilians (60,595).\\r\\n?????????The total still missing on 2/28/1946 were 6,244; Navy (340); Army (2,267); Air Force (3,089); Women's Auxiliary Territorial Service (18);\\r\\n?????????Merchant Navy (530); British Home Guard (0) and Civilians (0).\\r\\n?????????These figures included the losses of Newfoundland and Southern Rhodesia.\\r\\n??????????Colonial forces are not included in these figures.\\r\\n?????????There were an additional 31,271 military deaths due to \\"natural causes\\" which are not included in these figures.\\r\\n?????????Deaths due to air and V-rocket attacks were 60,595 civilians and 1,206 British Home Guard.\\r\\n^BF xUnited States\\r\\nAmerican military dead#^BF1\\r\\nAmerican civilian dead #^BF2\\r\\n^BG xYugoslavia\\r\\nThe losses of Yugoslav collaborators\\r\\nThe reasons for the high human toll in Yugoslavia were as follows\\r\\nA. Military operations between the occupying military forces and their quisling collaborators against the Yugoslav resistance.[189]\\r\\nB. German forces, under express orders from Hitler, fought with a special vengeance against the Serbs, who were considered Untermensch.[189] One of the worst one-day massacres during the German military occupation of Serbia was the Kragujevac massacre.\\r\\nC. Deliberate acts of reprisal against target populations were perpetrated by all combatants. All sides practiced the shooting of hostages on a large scale. At the end of the war, many Usta?e and Slovene collaborators were killed in or as a result of the Bleiburg repatriations.[189]\\r\\nD. The systematic extermination of large numbers of people for political, religious or racial reasons. The most numerous victims were Serbs.[189] According to Yad Vashem \\"During their four years in power, the Ustasa carried out a Serb genocide, exterminating over 500,000, expelling 250,000 and forcing another 200,000 to convert to Catholicism. The Ustasa also killed most of Croatias Jews, 20,000 Gypsies and many thousands of their political enemies.\\"[662] According to the United States Holocaust Memorial Museum \\"The Croat authorities murdered between 320,000 and 340,000 ethnic Serb residents of Croatia and Bosnia during the period of Usta?a rule; more than 30,000 Croatian Jews were killed either in Croatia or at Auschwitz-Birkenau\\". [663] The USHMM reports between 77,000 and 99,000 persons were killed at the Jasenovac and Stara Gradi?ka concentration camps.[664] The Jasenovac Memorial Site quotes a similar figure of between 80,000 and 100,000 victims. Stara Gradi?ka was a sub-camp of Jasenovac established for women and children.[665] The names and data for 12,790 victims at Stara Gradi?ka have been established[citation needed] Serbian sources currently claim that 700,000 persons were murdered at Jasenovac[665]\\r\\nSome 40,000 Roma were murdered.[666] Jewish victims in Yugoslavia totaled 67,122.[667]\\r\\nE. Reduced food supply caused famine and disease.[189]\\r\\nF. Allied bombing of German supply lines caused civilian casualties. The hardest hit localities were Podgorica, Leskovac, Zadar and Belgrade.[189]\\r\\nG. The demographic losses due to the reduction of 335,000 births and emigration of about 660,000 are not included with war casualties.[189]\\r\\n^BH xOther Nations","input":"What was the total death toll of ww2?"},{"output":"Flora is the plant life occurring in a particular region or time, generally the naturally occurring or indigenousnative plant life. The corresponding term for animal life is fauna.","context":"Flora is the plant life occurring in a particular region or time, generally the naturally occurring or indigenousnative plant life. The corresponding term for animal life is fauna. Flora, fauna and other forms of life such as fungi are collectively referred to as biota. Sometimes bacteria and fungi are also referred to as flora, as in the terms gut flora or skin flora.[1][2][3]\\r\\n\\r\\n\\r\\nThe word \\"flora\\" comes from the Latin name of Flora, the goddess of plants, flowers, and fertility in Roman mythology.[4][citation needed]\\r\\nThe distinction between vegetation (the general appearance of a community) and flora (the taxonomic composition of a community) was first made by Jules Thurmann (1849). Prior to this, the two terms were used indiscriminately.[5][6]\\r\\nPlants are grouped into floras based on region (floristic regions), period, special environment, or climate. Regions can be distinct habitats like mountain vs. flatland. Floras can mean plant life of a historic era as in fossil flora. Lastly, floras may be subdivided by special environments:\\r\\nThe flora of a particular area or time period can be documented in a publication also known as a \\"flora\\" (often capitalized as \\"Flora\\" to distinguish the two meanings when they might be confused). Floras may require specialist botanical knowledge to use with any effectiveness. Traditionally they are books, but some are now published on CD-ROM or websites.\\r\\nIt is said that the Flora Sinensis by the Polish Jesuit Micha? Boym was the first book that used the name \\"Flora\\" in this meaning, a book covering the plant world of a region.[7] However, despite its title it covered not only plants, but also some animals of the region.\\r\\nA published flora often contains diagnostic keys. Often these are dichotomous keys, which require the user to repeatedly examine a plant, and decide which one of two alternatives given best applies to the plant.","input":"What is the meaning of flora and fauna?"},{"output":"George Orwell's novel Nineteen Eighty-Four","context":"Big Brother is a fictional character and symbol in George Orwell's novel Nineteen Eighty-Four. He is ostensibly the leader (most likely a symbolic figurehead) of Oceania, a totalitarian state wherein the ruling Party wields total power \\"for its own sake\\" over the inhabitants.\\r\\nIn the society that Orwell describes, every citizen is under constant surveillance by the authorities, mainly by telescreens (with the exception of the Proles). The people are constantly reminded of this by the slogan \\"Big Brother is watching you\\": a maxim which is ubiquitously on display. In modern culture the term \\"Big Brother\\" has entered the lexicon as a synonym for abuse of government power, particularly in respect to civil liberties, often specifically related to mass surveillance.\\r\\n\\r\\n\\r\\nIn the essay section of his novel 1985, Anthony Burgess states that Orwell got the idea for the name of Big Brother from advertising billboards for educational correspondence courses from a company called Bennett's, current during World War II. The original posters showed J. M. Bennett himself: a kindly-looking old man offering guidance and support to would-be students with the phrase \\"Let me be your father\\" attached. According to Burgess, after Bennett's death, his son took over the company, and the posters were replaced with pictures of the son (who looked imposing and stern in contrast to his father's kindly demeanor) with the text \\"Let me be your big brother.\\"\\r\\nAdditional speculation from Douglas Kellner of UCLA argued that Big Brother represents Joseph Stalin.[1][2] Another theory is that the inspiration for Big Brother was Brendan Bracken, the Minister of Information until 1945. Orwell worked under Bracken on the BBC's Indian Service. Bracken was customarily referred to by MOI employees by his initials, B.B., the same initials as the character Big Brother. Orwell also resented the wartime censorship and need to manipulate information which he felt came from the highest levels of the MOI and from Bracken's office in particular.\\r\\nIn the novel, it is never made clear whether Big Brother is or had been a real person, or is a fictional personification of the Party, similar to Britannia and Uncle Sam. Big Brother is described as appearing on posters and telescreens as a handsome man in his mid-40s.\\r\\nIn Party propaganda, Big Brother is presented as one of the founders of the Party, along with Goldstein. At one point, Winston Smith, the protagonist of Orwell's novel, tries \\"to remember in what year he had first heard mention of Big Brother. He thought it must have been at some time in the sixties, but it was impossible to be certain. In the Party histories, Big Brother figured as the leader and guardian of the Revolution since its very earliest days. His exploits had been gradually pushed backwards in time until already they extended into the fabulous world of the forties and the thirties, when the capitalists in their strange cylindrical hats still rode through the streets of London . . .\\"\\r\\nIn the book The Theory and Practice of Oligarchical Collectivism, read by Winston Smith and purportedly written by Goldstein, Big Brother is referred to as infallible and all-powerful. No-one has ever seen him and there is a reasonable certainty that he will never die. He is simply \\"the guise in which the Party chooses to exhibit itself to the world\\", since the emotions of love, fear and reverence are more easily focused on an individual (if only a face on the hoardings and a voice on the telescreens), than an organisation. When Winston Smith is later arrested, O'Brien repeats that Big Brother will never die. When Smith asks if Big Brother exists, O'Brien describes him as \\"the embodiment of the Party\\" and says that he will exist as long as the Party exists. When Winston asks \\"Does Big Brother exist the same way I do?\\" (meaning is Big Brother an actual human being), O'Brien replies \\"You do not exist\\" (meaning that Smith is now an unperson; an example of doublethink).\\r\\nA spontaneous ritual of devotion to Big Brother (\\"BB\\") is illustrated at the end of the \\"Two Minutes Hate\\":\\r\\nAt this moment the entire group of people broke into a deep, slow, rhythmic chant of 'B-B! . . . B-B! . . . B-B!'over and over again, very slowly, with a long pause between the first 'B' and the seconda heavy murmurous sound, somehow curiously savage, in the background of which one seemed to hear the stamps of naked feet and the throbbing of tom-toms. For perhaps as much as thirty seconds they kept it up. It was a refrain that was often heard in moments of overwhelming emotion. Partly it was a sort of hymn to the wisdom and majesty of Big Brother, but still more it was an act of self-hypnosis, a deliberate drowning of consciousness by means of rhythmic noise.[3]\\r\\nThough Oceania's Ministry of Truth, Ministry of Plenty, and Ministry of Peace each have names with meanings deliberately opposite to their real purpose, the Ministry of Love is perhaps the most straightforward: \\"rehabilitated thought criminals\\" leave the Ministry as loyal subjects who have been brainwashed into adoring (loving) Big Brother, hence its name.\\r\\nLike the Nazi salute, Ingsoc has an own salute to Big Brother, which consists of crossing one's arms above the head during Two Minutes Hates and Hate weeks, showing loyality to Big Brother and Ingsoc.\\r\\nSince the publication of Nineteen Eighty-Four the phrase \\"Big Brother\\" has come into common use to describe any prying or overly-controlling authority figure, and attempts by government to increase surveillance.\\r\\nBig Brother and other Orwellian imagery are often referenced in the type of joke known as the Russian reversal.\\r\\nThe magazine Book ranked Big Brother No. 59 on its 100 Best Characters in Fiction Since 1900[4] list. Wizard magazine rated him the 75th greatest villain of all time.[5]\\r\\nThe worldwide reality television show Big Brother is based on the novel's concept of people being under constant surveillance. In 2000, after the U.S. version of the CBS program \\"Big Brother\\" premiered, the Estate of George Orwell sued CBS and its production company \\"Orwell Productions, Inc.\\" in federal court in Chicago for copyright and trademark infringement. The case was Estate of Orwell v. CBS, 00-c-5034 (ND Ill). On the eve of trial, the case settled worldwide to the parties' \\"mutual satisfaction\\"; the amount that CBS paid to the Orwell Estate was not disclosed. CBS had not asked the Estate for permission. Under current laws the novel will remain under copyright protection until 2020 in the European Union and until 2044 in the United States.\\r\\nThe iconic image of Big Brother (played by David Graham) played a key role in Apple's 1984 television commercial introducing the Macintosh.[6][7] The Orwell Estate viewed the Apple commercial as a copyright infringement, and sent a cease-and-desist letter to Apple and its advertising agency. The commercial was never televised again,[8] though the date mentioned in the ad (January 24) was but two days later, making it unlikely that it would have been re-aired. Subsequent (now posthumous) ads featuring Steve Jobs (for a variety of products including audio books) have mimicked the format and appearance of that original ad campaign, with the appearance of Steve Jobs nearly identical to that of Big Brother.[9][10] In 2008, The Simpsons spoofed Apple's Big Brother commercial in an episode entitled \\"Mypods and Boomsticks.\\"[11]\\r\\nThe December 2002 issue of Gear magazine featured a story about technologies and trends that could violate personal privacy moving society closer to a \\"Big Brother\\" state and utilised a recreation of the movie poster[12] from the film version of 1984 created by Dallmeierart.com.[13]\\r\\nComputer company Microsoft patented in 2011 a product distribution system with a camera or capture device that monitors the viewers that consume the product, allowing the provider to take \\"remedial action\\" if the actual viewers do not match the distribution license.[14] The system has been compared with 1984's telescreen surveillance system.[15]\\r\\nA series of laws intended to implement the EU Data Retention Directive in Romania were nicknamed \\"the Big Brother laws\\" by Romanian media and civil society, as they would have led to blanket storage of citizens' telecommunications data for 6 months.[16] All of these laws were struck down as unconstitutional by the Constitutional Court of Romania, and the Directive itself was ultimately invalidated by the Court of Justice of the European Union.","input":"When was the term big brother first used?"},{"output":"In the 1670s","context":"","input":"When did the first immigrants from germany arrived in america?"},{"output":"the post does not have a fixed term","context":"Provincial and territorial executive councils\\r\\nConstitution\\r\\nThe Prime Minister of Canada (French: Premier ministre du Canada) is the primary minister of the Crown, chairman of the Cabinet, and thus Canada's head of government, charged with advising the Canadian monarch or federal viceroy on the exercise of the executive powers vested in them by the constitution.[2] Not outlined in any constitutional document, the office exists only as per long-established convention (originating in Canada's former colonial power, the United Kingdom) that stipulates the monarch's representative, the governor general, must select as prime minister the person most likely to command the confidence of the elected House of Commons; this individual is typically the leader of the political party that holds the largest number of seats in that chamber.[n 1][3] Canadian prime ministers are styled as The Right Honourable (French: Le Trs Honorable), a privilege maintained for life.\\r\\nThe current, and 23rd, Prime Minister of Canada is the Liberal Party's Justin Trudeau, who was appointed on November 4, 2015, by Governor General David Johnston, following the general election that took place that year.\\r\\n\\r\\n\\r\\nThe position of prime minister is not outlined in any Canadian constitutional document and is mentioned only in passing in the Constitution Act, 1982,[4][5] and the Letters Patent, 1947 issued by King George VI.[6] The office and its functions are instead governed by constitutional conventions and modelled on the same office in the United Kingdom.\\r\\nThe prime minister, along with the other ministers in cabinet, is appointed by the governor general on behalf of the monarch.[7] However, by the conventions of responsible government, designed to maintain administrative stability, the viceroy will call to form a government the individual most likely to receive the support, or confidence, of a majority of the directly elected members of the House of Commons;[8] as a practical matter, this is often the leader of a party whose members form a majority, or a very large plurality, of Members of Parliament (MPs).[9]\\r\\nWhile there is no legal requirement for the prime minister to be a member of parliament, for practical and political reasons the prime minister is expected to win a seat very promptly.[10] However, in rare circumstances individuals who are not sitting members of the House of Commons have been appointed to the position of prime minister. Two former prime ministersSir John Joseph Caldwell Abbott and Sir Mackenzie Bowellserved in the 1890s while members of the Senate.[11] Both, in their roles as Government Leader in the Senate, succeeded prime ministers who had died in officeJohn A. Macdonald in 1891 and John Sparrow David Thompson in 1894. That convention has since evolved toward the appointment of an interim leader from the commons in such a scenario.\\r\\nPrime ministers who are not Members of Parliament upon their appointment (or who lose their seats while in office) have since been expected to seek election to the commons as soon as possible. For example, William Lyon Mackenzie King, after losing his seat in the 1925 federal election (that his party won), briefly \\"governed from the hallway\\" before winning a by-election a few weeks later. Similarly, John Turner replaced Pierre Trudeau as leader of the Liberal Party in 1984 and subsequently was appointed prime minister while not holding a seat in the House of Commons; Turner won a riding in the next election but the Liberal Party was swept from power. Turner was the last serving prime minister to not hold a commons seat.\\r\\nShould a serving prime minister today lose his or her seat in the legislature, or should a new prime minister be appointed without holding a seat, the typical process that follows is that a junior member in the governing political party will resign to allow the prime minister to run in the resulting by-election.[11] A safe seat is usually chosen; while the Liberal and Conservative parties traditionally observed a convention of not running a candidate against another party's new leader in the by-election, the New Democrats and smaller political parties typically do not follow the same convention. However, if the governing party selects a new leader shortly before an election is due, and that new leader is not a member of the legislature, he or she will normally await the upcoming election before running for a seat in parliament.\\r\\nIn a poll conducted by Ipsos-Reid following the first prorogation of the 40th parliament on December 4, 2008, it was found that 51% of the sample group thought the prime minister was directly elected by Canadians.[12][13]\\r\\nThe Canadian prime minister serves at Her Majesty's pleasure, meaning the post does not have a fixed term. Once appointed and sworn in by the governor general, the prime minister remains in office until he or she resigns, is dismissed, or dies.[14] The lifespan of parliament was limited by the constitution to five years and, though the governor general may still, on the advice of the prime minister, dissolve parliament and issue the writs of election prior to the date mandated by the Canada Elections Act; the KingÿByng Affair was the only time since Confederation that the viceroy deemed it necessary to refuse his prime minister's request for a general vote. As of 2007, with an amendment to the Elections Act, Section 56.1(2) was changed to limit the term of a majority government to four (4) years, with election day being set as the third Monday in October of the 4th calendar year after the previous polling date. [15]\\r\\nFollowing parliamentary dissolution, the prime minister must run in the resulting general election if he or she wishes to maintain a seat in the House of Commons. Should the prime minister's party subsequently win a majority of seats in the House of Commons, it is unnecessary to re-appoint the prime minister or again swear him or her into office.[14] If, however, an opposition party wins a majority of seats, the prime minister may resign or be dismissed by the governor general. Should the prime minister's party achieve a minority while an opposition party wins a plurality (i.e., more seats than any other party but less than a majority), the prime minister can attempt to maintain the confidence of the House by forming a coalition with other minority parties. This option was last entertained in 1925.\\r\\nBecause the prime minister is, in practice, the most politically powerful member of the Canadian government, he or she is sometimes erroneously referred to as Canada's head of state,[n 2] when, in fact, that post is held by the Canadian monarch, represented by the governor general.[16] The prime minister is, instead, the head of government and is responsible for advising the Crown on how to exercise the Royal Prerogative and its executive powers,[3] which are governed by the constitution and its conventions. However, the function of the prime minister has evolved with increasing power. Today, as per the doctrines of constitutional monarchy, the advice given by the prime minister is ordinarily binding, meaning the prime minister effectively carries out those duties ascribed to the sovereign or governor general, leaving the latter to act in predominantly ceremonial fashions.[17] As such, the prime minister, supported by the Office of the Prime Minister (PMO), controls the appointments of many key figures in Canada's system of governance, including the governor general, the Cabinet, justices of the Supreme Court, senators, heads of crown corporations, ambassadors to foreign countries, the provincial lieutenant governors, and approximately 3,100 other positions. Further, the prime minister plays a prominent role in the legislative processwith the majority of bills put before parliament originating in the Cabinetand the leadership of the Canadian Armed Forces.\\r\\nPierre Trudeau is credited with, throughout his tenure as prime minister between 1968 and 1984, consolidating power in the PMO,[18] which is itself filled by political and administrative staff selected at the prime minister's discretion and unaccountable to parliament. At the end of the 20th century and into the 21st, analystssuch as Jeffrey Simpson,[19] Donald Savoie, Andrew Coyne,[20] and John Gomeryargued that both parliament and the Cabinet had become eclipsed by prime ministerial power;[n 3][21] Savoie wrote: \\"The Canadian prime minister has little in the way of institutional check, at least inside government, to inhibit his ability to have his way.\\"[22] Indeed, the position has been described as undergoing a \\"presidentialisation\\",[18][23] to the point that its incumbents publicly outshine the actual head of state (and prime minister's spouses are sometimes called the \\"First Lady of Canada\\"[24][25]).[26][27] Former governor general Adrienne Clarkson alluded to what she saw as \\"an unspoken rivalry\\" that had developed between the prime minister and the Crown.[28] It has been theorized that such is the case in Canada as its parliament is less influential on the executive than in other countries with Westminster parliamentary systems; particularly, Canada has fewer MPs, a higher turnover rate of MPs after each election, and an Americanised system for selecting political party leaders, leaving them accountable to the party membership rather than caucus, as is the case in the United Kingdom.[29]\\r\\nThere do exist checks on the prime minister's power: the commons may revoke its confidence in an incumbent prime minister and Cabinet or caucus revolts can quickly bring down a serving premier and even mere threats of such action can persuade or compel a prime minister to resign his post, as happened with Jean Chrtien. The Reform Act, 2014,[30] codifies the process by which a caucus may trigger a party leadership review and, if necessary, chose an interim leader, thereby making a prime minister more accountable to the MPs in his or her party. Caucuses may choose to follow these rules, though the decision would be made by recorded vote, thereby subjecting the party's choice to public scrutiny.[31]\\r\\nThe Senate may delay or impede legislation put forward by the Cabinet, such as when Brian Mulroney's bill creating the Goods and Services Tax (GST) came before the upper chamber and, given Canada's federal nature, the jurisdiction of the federal government is limited to areas prescribed by the constitution. Further, as executive power is constitutionally vested in the monarch, meaning the Royal Prerogative belongs to the Crown and not to any of its ministers,[32][33][34] the sovereign's supremacy over the prime minister in the constitutional order is thus seen as a \\"rebuff to the pretensions of the elected: As it has been said, when the Prime Minister bows before the Queen, he bows before us [the Canadian people].\\"[35][36] Either the sovereign or his or her viceroy may therefore oppose the prime minister's will in extreme, crisis situations.[n 4] Near the end of her time as governor general, Adrienne Clarkson stated: \\"My constitutional role has lain in what are called 'reserve powers': making sure that there is a prime minister and a government in place, and exercising the right 'to encourage, to advise, and to warn'[...] Without really revealing any secrets, I can tell you that I have done all three.\\"[37]\\r\\nTwo official residences are provided to the prime minister24 Sussex Drive in Ottawa and Harrington Lake, a country retreat in Gatineau Parkas well an office in the Langevin Block, across from Parliament Hill.[38] For transportation, the prime minister is granted an armoured car and shared use of two official aircrafta CC-150 Polaris for international flights and a Challenger 601 for domestic trips. The Royal Canadian Mounted Police also furnish constant personal security for the prime minister and his or her family. All of the aforementioned is supplied by the Queen-in-Council through budgets approved by parliament, as is the prime minister's annual salary of CAD$170,400.[1] (A prime minister additionally earns the normal salary of a Member of Parliament: $170,400.[1])\\r\\nShould a serving or former prime minister die, he or she is accorded a state funeral, wherein their casket lies in state in the Centre Block of Parliament Hill.[39] Only Bowell and the Viscount Bennett were given private funerals, Bennett also being the only former Prime Minister of Canada to die and be buried outside the country and Bowell the only whose funeral was not attended by politicians. John Thompson also died outside Canada, at Windsor Castle, where Queen Victoria permitted his lying-in-state before his body was returned to Canada for a state funeral in Halifax.[40]\\r\\nIn earlier years, it was traditional for the monarch to bestow a knighthood on newly appointed Canadian prime ministers. Accordingly, several carried the prefix Sir before their name; of the first eight premiers of Canada, only Alexander Mackenzie refused the honour of a knighthood from Queen Victoria. Following the 1919 Nickle Resolution, however, it was against non-binding policy for the sovereign to grant such honorific titles to Canadians; the last prime minister to be knighted was Sir Robert Borden, who was premier at the time the Nickle Resolution was debated in the House of Commons. Still, Bennett was in 1941, six years after he stepped down as prime minister, elevated to the peerage by King George VI as Viscount Bennett, of Mickleham in the County of Surrey and of Calgary and Hopewell in the Dominion of Canada.[41][42]\\r\\nThe Canadian Heraldic Authority (CHA) has granted former prime ministers an augmentation of honour on the personal coat of arms of those who pursued them. The heraldic badge, referred to by the CHA as the mark of the Prime Ministership of Canada,[43] consists of four red maple leaves joined at the stem on a white field (\\"Argent four maple leaves conjoined in cross at the stem Gules\\"); the augmentation has, so far, been granted either as a canton sinister or centred in the chief.[43][44][45][46][47] To date, former prime ministers Joe Clark,[43] Pierre Trudeau,[44] John Turner,[45] Brian Mulroney,[46] and Kim Campbell were granted arms with the augmentation.[47]\\r\\nCanada continues the Westminster tradition of using the title Prime Minister when one is speaking to the federal head of government directly; this is in contrast to the United States protocol of addressing the federal head of government as mister (as in, Mister President); the Department of Canadian Heritage advises that it is incorrect to use the term Mr Prime Minister.[48] The written form of address for the prime minister should use his or her full parliamentary title: The Right Honourable [name], [post-nominal letters], Prime Minister of Canada. However, while in the House of Commons during Question Period, other members of parliament may address the prime minister as The Right Honourable, Member for [prime minister's riding] or simply The Right Honourable Prime Minister.[49] Former prime ministers retain the prefix The Right Honourable for the remainder of their lives; should they remain sitting MPs, they may be referred as The Right Honourable Member for [member's riding] or by their portfolio title (if appointed to one), as in The Right Honourable Minister of National Defence.\\r\\nIn the decades following Confederation, it was common practice to refer to the prime minister as Premier of Canada,[50][51][52] a custom that continued until the First World War, around the time of Robert Borden's premiership.[53][54][55] While contemporary sources will still speak of early prime ministers of Canada as premier,[56][57][58] the modern practice is such that the federal head of government is known almost exclusively as the prime minister, while the provincial heads of government are termed premiers (save for within Quebec and New Brunswick, where the premiers are addressed in French as Premier ministre du [province], literally translated as Prime Minister of [province]).\\r\\nAfter exiting office, former prime ministers of Canada have engaged in various pursuits. Some remained in politics: Bowell continued to serve as a senator, Stephen Harper returned to the House of Commons as a backbench Member of Parliament, and Bennett moved to the United Kingdom after being elevated to the House of Lords.[59] A number led Her Majesty's Loyal Opposition in the Canadian parliament: John A. Macdonald, Arthur Meighen, Mackenzie King,[60] and Pierre Trudeau, all before being re-appointed as prime minister (Mackenzie King twice); Alexander Mackenzie and John Diefenbaker, both prior to sitting as regular Members of Parliament until their deaths;[61] Wilfrid Laurier dying while still in the post;[62] and Charles Tupper,[63] Louis St. Laurent,[64] and John Turner, each before they returned to private business. Meighen was also appointed to the Senate following his second period as prime minister, but resigned his seat to seek re-election and moved to private enterprise after failing to win a riding.[65] Following Meighen into civilian life were: Robert Borden, who served as Chancellor of Queen's and McGill Universities, as well as working in the financial sector; Lester B. Pearson, who acted as Chancellor of Carleton University;[66] Joe Clark and Kim Campbell, who became university professors, Clark also consultant and Campbell working in international diplomacy and as the director of private companies and chairperson of interest groups; while Pierre Trudeau and Jean Chrtien returned to legal practice.[67] Former prime ministers also commonly penned autobiographiesTupper,[63] for exampleor published their memoirssuch as Diefenbaker and Paul Martin.[61]","input":"How long does the canadian prime minister serve?"},{"output":"1 October 1964","context":"The Shinkansen (??, new trunk line), colloquially known in English as the bullet train, is a network of high-speed railway lines in Japan operated by five Japan Railways Group companies. Starting with the Tkaid Shinkansen (515.4?km, 320.3?mi) in 1964,[1] the network has expanded to currently consist of 2,764.6?km (1,717.8?mi) of lines with maximum speeds of 240ÿ320?km/h (150ÿ200?mph), 283.5?km (176.2?mi) of Mini-shinkansen lines with a maximum speed of 130?km/h (80?mph), and 10.3?km (6.4?mi) of spur lines with Shinkansen services.[2] The network presently links most major cities on the islands of Honshu and Kyushu, and Hakodate on northern island of Hokkaido, with an extension to Sapporo under construction and scheduled to commence in March 2031.[3]\\r\\nThe maximum operating speed is 320?km/h (200?mph) (on a 387.5?km section of the Thoku Shinkansen).[4] Test runs have reached 443?km/h (275?mph) for conventional rail in 1996, and up to a world record 603?km/h (375?mph) for maglev trains in April 2015.[5]\\r\\nShinkansen literally means new trunk line,[6] referring to the high-speed rail line network. The name Superexpress (]ġ, ch-tokky), initially used for Hikari trains, was retired in 1972 but is still used in English-language announcements and signage.\\r\\nThe original Tkaid Shinkansen, connecting the largest cities of Tokyo and Osaka, is the world's busiest high-speed rail line. Carrying 151 million passengers per year (March 2008),[7] and at over 5?billion total passengers it has transported more passengers[8] than any other high-speed line in the world. The service on the line operates much larger trains and at higher frequency than most other high speed lines in the world. At peak times, the line carries up to thirteen trains per hour in each direction with sixteen cars each (1,323-seat capacity and occasionally additional standing passengers) with a minimum headway of three minutes between trains.\\r\\nJapan's Shinkansen network had the highest annual passenger ridership (a maximum of 353 million in 2007) of any high-speed rail network until 2011, when the Chinese high-speed railway network surpassed it at 370 million passengers annually, reaching over 1.7 billion annual passengers in 2017,[9] though the total cumulative passengers, at over 10 billion, is still larger.[10] While the Shinkansen network has been expanding, Japan's declining population is expected to cause ridership to decline over time. The recent expansion in tourism has boosted ridership marginally.\\r\\nThough largely a long-distance transport system, the Shinkansen also serves commuters who travel to work in metropolitan areas from outlying cities one or two stops removed from the main cities, and there are some services dedicated to this market.\\r\\n\\r\\n\\r\\nJapan was the first country to build dedicated railway lines for high-speed travel. Because of the mountainous terrain, the existing network consisted of 1,067?mm (3?ft?6?in) narrow-gauge lines, which generally took indirect routes and could not be adapted to higher speeds. Consequently, Japan had a greater need for new high-speed lines than countries where the existing standard gauge or broad gauge rail system had more upgrade potential.\\r\\nAmong the key people credited with the construction of the first Shinkansen are Hideo Shima, the Chief Engineer, and Shinji Sog, the first President of Japanese National Railways (JNR) who managed to persuade politicians to back the plan. Other significant people responsible for its technical development were Tadanao Miki, Tadashi Matsudaira, and Hajime Kawanabe based at the Railway Technology Research Institute (RTRI), part of JNR. They were responsible for much of the technical development of the first line, the Tkaid Shinkansen. All three had worked on aircraft design during World War II.[11]\\r\\nThe popular English name bullet train is a literal translation of the Japanese term dangan ressha (??F?), a nickname given to the project while it was initially being discussed in the 1930s. The name stuck because of the original 0 Series Shinkansen's resemblance to a bullet and its high speed.\\r\\nThe Shinkansen name was first formally used in 1940 for a proposed standard gauge passenger and freight line between Tokyo and Shimonoseki that would have used steam and electric locomotives with a top speed of 200?km/h (120?mph). Over the next three years, the Ministry of Railways drew up more ambitious plans to extend the line to Beijing (through a tunnel to Korea) and even Singapore, and build connections to the Trans-Siberian Railway and other trunk lines in Asia. These plans were abandoned in 1943 as Japan's position in World War II worsened. However, some construction did commence on the line; several tunnels on the present-day Shinkansen date to the war-era project.\\r\\nFollowing the end of World War II, high-speed rail was forgotten for several years while traffic of passengers and freight steadily increased on the conventional Tkaid Main Line along with the reconstruction of Japanese industry and economy. By the mid-1950s the Tkaid Line was operating at full capacity, and the Ministry of Railways decided to revisit the Shinkansen project. In 1957, Odakyu Electric Railway introduced its 3000 series SE Romancecar train, setting a world speed record of 145?km/h (90?mph) for a narrow gauge train. This train gave designers the confidence that they could safely build an even faster standard gauge train. Thus the first Shinkansen, the 0 series, was built on the success of the Romancecar.\\r\\nIn the 1950s, the Japanese national attitude was that railways would soon be outdated and replaced by air travel and highways as in America and many countries in Europe. However, Shinji Sog, President of Japanese National Railways, insisted strongly on the possibility of high-speed rail, and the Shinkansen project was implemented.\\r\\nGovernment approval came in December 1958, and construction of the first segment of the Tkaid Shinkansen between Tokyo and Osaka started in April 1959. The cost of constructing the Shinkansen was at first estimated at nearly 200 billion yen, which was raised in the form of a government loan, railway bonds and a low-interest loan of US$80 million from the World Bank. Initial cost estimates, however, had been deliberately understated and the actual figures were nearly double at about 400 billion yen. As the budget shortfall became clear in 1963, Sogo resigned to take responsibility.[12]\\r\\nA test facility for rolling stock, now part of the line, opened in Odawara in 1962.\\r\\nThe Tkaid Shinkansen began service on 1 October 1964, in time for the first Tokyo Olympics.[13] The conventional Limited Express service took six hours and 40 minutes from Tokyo to Osaka, but the Shinkansen made the trip in just four hours, shortened to three hours and ten minutes by 1965. It enabled day trips between Tokyo and Osaka, the two largest metropolises in Japan, changed the style of business and life of the Japanese people significantly, and increased new traffic demand. The service was an immediate success, reaching the 100?million passenger mark in less than three years on 13 July 1967, and one billion passengers in 1976. Sixteen-car trains were introduced for Expo '70 in Osaka. With an average of 23,000 passengers per hour in each direction in 1992, the Tkaid Shinkansen is the world's busiest high-speed rail line.[14]\\r\\nThe first Shinkansen trains, the 0 series, ran at speeds of up to 210?km/h (130?mph), later increased to 220?km/h (137?mph). The last of these trains, with their classic bullet-nosed appearance, were retired on 30 November 2008. A driving car from one of the 0 series trains was donated by JR West to the National Railway Museum in York, England in 2001.[15]\\r\\nThe Tkaid Shinkansen's rapid success prompted an extension westward to Okayama, Hiroshima and Fukuoka (the Sany Shinkansen), which was completed in 1975. Prime Minister Kakuei Tanaka was an ardent supporter of the Shinkansen, and his government proposed an extensive network paralleling most existing trunk lines. Two new lines, the Thoku Shinkansen and Jetsu Shinkansen, were built following this plan. Many other planned lines were delayed or scrapped entirely as JNR slid into debt throughout the late 1970s, largely because of the high cost of building the Shinkansen network. By the early 1980s, the company was practically insolvent, leading to its privatization in 1987.\\r\\nDevelopment of the Shinkansen by the privatised regional JR companies has continued, with new train models developed, each generally with its own distinctive appearance (such as the 500 series introduced by JR West). Since 2014, shinkansen trains run regularly at speeds up to 320?km/h (200?mph), placing them alongside the French TGV and German ICE as the fastest trains in the world.\\r\\nSince 1970, development has also been underway for the Ch Shinkansen, a planned maglev line from Tokyo to Osaka. On 21 April 2015, a seven-car L0 series maglev trainset set a world speed record of 603?km/h (375?mph).[5]\\r\\nTo enable high-speed operation Shinkansen uses a range of advanced technology compared with conventional rail, achieving not only high speed but also a high standard of safety and comfort. Its success has influenced other railways in the world and the importance and advantage of high-speed rail has consequently been reevaluated.\\r\\nShinkansen routes are completely separate from conventional rail lines (except Mini-shinkansen which goes through to conventional lines). Consequently, the shinkansen is not affected by slower local or freight trains (except for Hokkaido Shinkansen while traveling through the Seikan Tunnel), and has the capacity to operate many high-speed trains punctually. The lines have been built without road crossings at grade. Tracks are strictly off-limits with penalties against trespassing strictly regulated by law. It uses tunnels and viaducts to go through and over obstacles rather than around them, with a minimum curve radius of 4,000?meters (2,500?meters on the oldest Tkaid Shinkansen).[16]\\r\\nThe Shinkansen uses 1,435?mm (4?ft?8?1?2?in) standard gauge in contrast to the 1,067?mm (3?ft?6?in) narrow gauge of older lines. Continuous welded rail and swingnose crossing points are employed, eliminating gaps at turnouts and crossings. Long rails are used, joined by expansion joints to minimize gauge fluctuation due to thermal elongation and shrinkage.\\r\\nA combination of ballasted and slab track is used, with slab track exclusively employed on concrete bed sections such as viaducts and tunnels. Slab track is significantly more cost-effective in tunnel sections, since the lower track height reduces the cross-sectional area of the tunnel, thereby reducing construction costs by up to 30%.[17] However, the smaller diameter of Shinkansen tunnels compared to some other high-speed lines has resulted in the issue of tunnel boom becoming a concern for residents living close to tunnel portals.\\r\\nThe slab track consists of rails, fasteners and track slabs with a cement asphalt mortar. On the roadbed and in tunnels, circular upstands (measuring 400ÿ520?mm in diameter and 200?mm high) are located at 5m intervals. The prefabricated upstands are made of either reinforced concrete or pre-stressed reinforced concrete; they prevent the track slab from moving along either the latitudinal or the longitudinal directions. One track slab weighs approximately 5 tons, measuring 2220ÿ2340?mm wide, 4900ÿ4950?mm long and 160ÿ200?mm thick.[18]\\r\\nThe Shinkansen employs an ATC (Automatic Train Control) system, eliminating the need for trackside signals. It uses a comprehensive system of Automatic Train Protection.[12] Centralized traffic control manages all train operations, and all tasks relating to train movement, track, station and schedule are networked and computerized.\\r\\nShinkansen uses a 25kV AC overhead power supply (20 kV AC on Mini-shinkansen lines), to overcome the limitations of the 1,500 V direct current used on the existing electrified narrow-gauge system. Power is distributed along the axles of the train to reduce the heavy axle loads under single power cars.[12] The power supply for the Tokaido Shinkansen is 60?Hz.\\r\\nShinkansen trains are electric multiple units, offering fast acceleration, deceleration and reduced damage to the track because of the use of lighter vehicles compared to locomotives or power cars. The coaches are air-sealed to ensure stable air pressure when entering tunnels at high speed.\\r\\nThe Shinkansen is very reliable thanks to several factors, including its near-total separation from slower traffic. In 2014, JR Central reported that the Shinkansen's average delay from schedule per train was 54 seconds. This includes delays due to uncontrollable causes, such as natural disasters.[19] The record, in 1997, was 18 seconds.\\r\\nThe Shinkansen has used the electric multiple unit configuration from the outset, with the 0 Series Shinkansen having all axles powered. Other railway manufacturers were traditionally reluctant, or unable to use distributed traction configurations (e.g. Talgo used the locomotive configuration with the AVE Class 102 and continues with it for the Talgo AVRIL on account of the fact that it is not possible to use powered bogies as part of the Talgo Pendular system). In Japan, significant engineering desirability exists for the electric multiple unit configuration. A greater proportion of motored axles results in higher acceleration, meaning that the Shinkansen does not lose as much time if stopping frequently. Shinkansen lines have more stops in proportion to their lengths than high-speed lines elsewhere in the world.\\r\\nOver the Shinkansen's 50-plus year history, carrying over 10?billion passengers, there have been no passenger fatalities due to derailments or collisions,[20] despite frequent earthquakes and typhoons. Injuries and a single fatality have been caused by doors closing on passengers or their belongings; attendants are employed at platforms to prevent such accidents. There have, however, been suicides by passengers jumping both from and in front of moving trains.[21] On 30 June 2015, a passenger committed suicide on board a Shinkansen train by setting himself on fire, killing another passenger and seriously injuring seven other people.[22]\\r\\nThere have been two derailments of Shinkansen trains in passenger service. The first one occurred during the Chetsu earthquake on 23 October 2004. Eight of ten cars of the Toki No. 325 train on the Jetsu Shinkansen derailed near Nagaoka Station in Nagaoka, Niigata. There were no casualties among the 154 passengers.[23]\\r\\nAnother derailment happened on 2 March 2013 on the Akita Shinkansen when the Komachi No. 25 train derailed in blizzard conditions in Daisen, Akita. No passengers were injured.[24]\\r\\nIn the event of an earthquake, an earthquake detection system can bring the train to a stop very quickly, newer trainsets are lighter and have stronger braking systems, allowing for quicker stopping. A new anti-derailment device was installed after detailed analysis of the Jetsu derailment.\\r\\nSeveral months after the expose of the Kobe Steel falsification scandal, which is among the suppliers of high-strength steel for shinkansen trainsets, cracks were found upon inspection of a single bogie, and removed from service on 11 December 2017.[25]\\r\\nThe Shinkansen has had a significant beneficial effect on Japan's business, economy, society, environment and culture in ways beyond mere construction and operation contributions.[14] The results were as follows: time savings alone from switching from a conventional to a high-speed network have been estimated at 400 million hours, an economic impact of 500 billion per year.[14] That does not include the savings from reduced reliance on imported fuel, which also has national security benefits. Shinkansen lines, particularly in the very crowded coastal Taiheiy Belt megalopolis, met two primary goals:\\r\\nHowever, the initial Shinkansen prudence gave way to political considerations to extend the mode to far less populated regions of the country, partly to spread these benefits beyond the key centres of Kanto and Kinki. In some areas regional extension was frustrated by protracted land acquisition issues, sometimes influenced by fierce protests from locals against expanding Narita airport's runways to handle more traffic that extended well into the 2000s. Tokyo's airports were already at or near capacity and there was no room for another civilian airport given the geography and required US military presence. Shinkansen lines were extended to sparsely populated areas with the intent the network would disperse the population away from the capital.\\r\\nSuch expansion had a significant cost. JNR, the national railway company, was already burdened with subsidizing unprofitable rural and regional railways. Additionally it assumed Shinkansen construction debt to the point the government corporation eventually owed some 28 trillion, contributing to it being regionalised and privatized.[26] The privatized JRs eventually paid a total of 9.2 trillion to acquire JNR's Shinkansen network.[14]\\r\\nAfter privatization, the Shinkansen network continues to see significant expansion to less populated areas, but with far more flexibility to spin off unprofitable railways or cut costs than in JNR days. Currently an important factor is the post bubble zero interest-rate policy that allows JR to borrow huge sums of capital without significant concern regarding repayment timing.\\r\\nTraveling by the Tokaido Shinkansen from Tokyo to Osaka produces only around 16% of the carbon dioxide of the equivalent journey by car, a saving of 15,000 tons of CO2 per year.[14]\\r\\nNoise pollution concerns mean that increasing speed is becoming more difficult. In Japan, the population density is high and there have been severe protests against the Shinkansen's noise pollution, meaning that its noise is now limited to less than 70 dB in residential areas.[27] Hence, improvement and reduction of pantograph, weight saving of cars, and construction of noise barriers and other measures have been implemented. Current research is primarily aimed at reducing operational noise, particularly the tunnel boom phenomenon caused when trains transit tunnels at high speed.\\r\\nBecause of the risk of earthquakes, the Urgent Earthquake Detection and Alarm System (UrEDAS) (earthquake warning system) was introduced in 1992. It enables automatic braking of bullet trains in the case of large earthquakes.\\r\\nThe Tkaid Shinkansen often experiences heavy snow in the area around Maibara Station in winter. Trains have to reduce speed, which can disrupt the timetable. Sprinkler systems were later installed, but delays of 10 to 20 minutes still occur during snowy weather. Additionally, treefalls related to excess snow have caused service interruptions. Along the route of the Jetsu Shinkansen, winter snow can be very heavy, with snow depths of two to three metres, so the line is equipped with stronger sprinklers and slab track, to mitigate the effects of deep snow.\\r\\nNotes:\\r\\nCumulative ridership since October 1964 is over 5 billion passengers for the Tokaido Shinkansen Line alone and 10 billion passengers for Japan's entire shinkansen network.[8]\\r\\n* The sum of the ridership of individual lines does not equal the ridership of the system because a single rider may be counted multiple times when using multiple lines, to get proper ridership figures for a system, in the above case, is only counted once.\\r\\n** Only refers to 6 days of operation: 26 March 2016 (opening date) to 31 March 2016 (end of FY2015).\\r\\nAnnual ridership peaked in 2007 until 2010, then began rising again since 2011, although the Kyushu line (connected to the Sanyo Shinkansen in March 2011) has seen significant gains while patronage on other lines has fallen. Until 2011, Japan's high-speed rail system had the highest annual patronage of any system worldwide, China's HSR network's patronage reached 440 million and is now the highest.[8] 2012 ridership reached a new record of 355.66 million.[34]\\r\\nE5 series trains, capable of up to 320?km/h (200?mph) (initially limited to 300?km/h), were introduced on the Thoku Shinkansen in March 2011. Operation at the maximum speed of 320?km/h between Utsunomiya and Morioka on this route commenced on 16 March 2013, and reduced the journey time to around 3?hours for trains from Tokyo to Shin-Aomori (a distance of 674?km (419?mi)).\\r\\nExtensive trials using the Fastech 360 test trains have shown that operation at 360?km/h (224?mph) is not currently feasible because of problems of noise pollution (particularly tunnel boom), overhead wire wear, and braking distances. On 30 October 2012, JR East announced that it is pursuing research and development to increase speeds to 360?km/h on the Tohoku Shinkansen by 2020.[35]\\r\\nAs of 2016[update], the maximum speed on the approximately 82?km dual gauge section of the Hokkaido Shinkansen (including through the Seikan Tunnel) is 140?km/h (85?mph).[3] There are approximately 50 freight trains using the dual gauge section each day, so limiting the travel of such trains to times outside of Shinkansen services is not an option. Because of this and other weather-related factors cited by JR East and JR Hokkaido, the fastest journey time between Tokyo and Shin-Hakodate-Hokuto is currently 4 hours, 2 minutes.[36] The new section takes 61 minutes from Shin-Aomori to Shin-Hakodate-Hokuto on the fastest services.[36]\\r\\nBy 2018, it is proposed to allow one Shinkansen service each day to travel at 260?km/h (160?mph) (the maximum speed proposed for the tunnel) by ensuring no freight trains are scheduled to travel on the dual gauge section at that time. To achieve the full benefit of Shinkansen trains travelling on the dual gauge section at 260?km/h (160?mph), other alternatives are being considered, such as a system to automatically slow Shinkansen trains down to 200?km/h (125?mph) when passing narrow-gauge trains, and loading freight trains onto special \\"Train on Train\\" standard-gauge trains (akin to a covered piggyback flatcar train) built to withstand the shock wave of oncoming Shinkansen trains traveling at full speed. This would enable a travel time from Tokyo to Shin-Hakodate-Hokuto of 3 hours and 45 minutes, a saving of 17 minutes on the current timetable.\\r\\nThe Hokuriku Shinkansen is being extended from Kanazawa to Tsuruga (proposed for completion by 2023) at an estimated cost of 3.04 trillion yen (in 2012 currency).[37]\\r\\nThere are further plans to extend the line from Tsuruga to Osaka, with the 'Obama-Kyoto' route chosen by the government on 20 December 2016,[38] after a government committee investigated the five nominated routes.[39] The five routes that were under investigation are detailed in the Hokuriku Shinkansen page.\\r\\nConstruction of the extension beyond Tsuruga is not expected to commence before 2030, with a projected 15 year construction period. On 6 March 2017 the government committee announced the chosen route from Kyoto to Shin-Osaka is to be via Kyotanabe, with a station at Matsuiyamate on the Katamachi Line.[40][41]\\r\\nIn order to extend the benefits of the Hokuriku Shinkansen to stations west of Tsuruga before the line to Osaka is completed, JR West is working in partnership with Talgo on the development of a Gauge Change Train (CGT), which will be capable of operating under both the 25?kV AC electrification used on the Shinkansen and the 1.5?kV DC system employed on conventional lines. The six-car train is due to start trials on the Hokuriku Shinkansen and the 1067?mm-gauge Hokuriku and Kosei lines in 2017. As part of the project JR West has already begun trials with a purpose-built 180-metre-long gauge-changer at Tsuruga.[42]\\r\\nThe Hokkaido Shinkansen forms an extension of the Tohoku Shinkansen north of Shin-Aomori to Shin-Hakodate-Hokuto Station (north of the Hokkaido city of Hakodate) through the Seikan Tunnel, which was converted to dual gauge as part of the project, opening in March 2016.\\r\\nJR Hokkaido is extending the Hokkaido Shinkansen from Shin-Hakodate-Hokuto to Sapporo to open by March 2031,[3] with tunnelling work on the 5,265 m Murayama tunnel, situated about 1?km north of Shin-Hakodate-Hokuto Station, commencing in March 2015, and due to be completed by March 2021. The 211.3?km extension will be approximately 76% in tunnels, including major tunnels such as Oshima (~26.5?km), Teine (~18.8?km) and Shiribeshi (~18?km).[43]\\r\\nAlthough an extension from Sapporo to Asahikawa was included in the 1973 list of planned lines, at this time it is unknown whether the Hokkaido Shinkansen will be extended beyond Sapporo.\\r\\nJR Kyushu is constructing an extension (to be known as the West Kyushu Shinkansen) line of the Kyushu Shinkansen to Nagasaki, partly to full Shinkansen standard gauge construction standards (Takeo Onsen ÿ Nagasaki) with the existing narrow gauge section between Shin-Tosu and Takeo Onsen to be upgraded as part of this project.\\r\\nThis proposal initially involved introducing Gauge Change Trains (GCT) travelling from Hakata to Shin-Tosu (26.3?km) on the existing Kyushu Shinkansen line, then passing through a specific gauge changing (standard to narrow) section of track linking to the existing Nagasaki Main Line, along which it would travel to Hizen Yamaguchi (37.6?km), then onto the Sasebo Line to Takeo Onsen (13.7?km), where another gauge changing section (narrow to standard) would lead onto the final Shinkansen line to Nagasaki (66.7?km). However, technical issues with the development of the GCT bogies means the train will not be available for service until at least 2025, requiring consideration of other options such as 'relay' service to ensure the line can open on schedule in 2023.[44]\\r\\nThe proposal shortens the distance between Hakata and Nagasaki by 6.2% (9.6?km), and while only 64% of the route will be built to full Shinkansen standards, it will eliminate the slowest sections of the existing narrow gauge route. Use of the GCT was estimated to result in a time saving of 28.5% (32 minutes) on the current timetable. The proposed time saving if a 'relay' service is introduced is unknown at this time.\\r\\nAs part of this proposal, the current 12.8?km section of single track between Hizen Yamaguchi and Takeo Onsen is to be duplicated, with work on that component scheduled to commence in April 2016.\\r\\nWith the completion of excavation of the 1351m Enogushi tunnel, being the sixth tunnel completed in this section, approximately 25% of the 40.7?km of tunnel excavation work on the Takeo Onsen ÿ Nagasaki section has been finished. The entire project is scheduled for completion by March 2023.\\r\\nMaglev trains have been undertaking test runs on the Yamanashi test track since 1997, running at speeds of over 500?km/h (310?mph). As a result of this extensive testing, maglev technology is almost ready for public usage.[45] An extension of this test track from 18.4?km to 42.8?km was completed in June 2013, enabling extended high-speed running trials to commence in August 2013. This section will be incorporated into the Ch Shinkansen which will eventually link Tokyo to Osaka. Construction of the Shinagawa to Nagoya section began in 2014, with 86% of the 286?km route to be in tunnels.\\r\\nThe CEO of JR Central announced plans to have the maglev Ch Shinkansen operating from Tokyo to Nagoya by 2027.[45] Following the shortest route (through the Japanese Alps), JR Central estimates that it will take 40 minutes to run from Shinagawa to Nagoya. A subsequent extension to Osaka is planned to be completed by 2045. The planned travel time from Shinagawa to Shin-Osaka is 1 hour 7 minutes. Currently the Tokaido Shinkansen has a minimum connection time of 2 hours 19 minutes.[46]\\r\\nWhile the government has granted approval[47] for the shortest route between Tokyo and Nagoya, some prefectural governments, particularly Nagano, lobbied to have the line routed farther north to serve the city of Chino and either Ina or Kiso-Fukushima. However, that would increase both the travel time (from Tokyo to Nagoya) and the cost of construction.[48] JR Central has confirmed it will construct the line through Kanagawa Prefecture, and terminate at Shinagawa Station.\\r\\nThe route for the Nagoya to Osaka section is also contested. It is planned to go via Nara, about 40?km south of Kyoto. Kyoto is lobbying to have the route moved north and be largely aligned with the existing Tokaido Shinkansen, which services Kyoto and not Nara.[49]\\r\\nMini-shinkansen (??) is the name given to the routes where former narrow gauge lines have been converted to standard gauge to allow Shinkansen trains to travel to cities without the expense of constructing full Shinkansen standard lines.\\r\\nTwo mini-shinkansen routes have been constructed: the Yamagata Shinkansen and Akita Shinkansen. Shinkansen services to these lines traverse the Tohoku Shinkansen line from Tokyo before branching off to traditional main lines. On both the Yamagata/Shinjo and Akita lines, the narrow gauge lines were regauged, resulting in the local services being operated by standard gauge versions of 1,067?mm gauge suburban/interurban rolling stock. On the Akita line between Omagari and Akita, one of the two narrow gauge lines was regauged, and a section of the remaining narrow gauge line is dual gauge, providing the opportunity for Shinkansen services to pass each other without stopping.\\r\\nThe maximum speed on these lines is 130?km/h, however the overall travel time to/from Tokyo is improved due to the elimination of the need for passengers to change trains at Fukushima and Morioka respectively.\\r\\nAs the Loading gauge (size of the train that can travel on a line) was not altered when the rail gauge was widened, only Shinkansen trains specially built for these routes can travel on the lines. At present they are the E3 and E6 series trains.\\r\\nWhilst no further Mini-shinkansen routes have been proposed to date, it remains an option for providing Shinkansen services to cities on the narrow gauge network.\\r\\nThis is the name for the concept of using a single train that is specially designed to travel on both 1,067?mm (3?ft?6?in) narrow gauge railway lines and the 1,435?mm (4?ft?8?1?2?in) standard gauge used by Shinkansen train services in Japan. The trucks/bogies of the Gauge Change Train (GCT) allow the wheels to be unlocked from the axles, narrowed or widened as necessary, and then relocked. This allows a GCT to traverse both standard gauge and narrow gauge tracks without the expense of regauging lines.\\r\\nThree test trains have been constructed, with the second set having completed reliability trials on the Yosan Line east of Matsuyama (in Shikoku) in September 2013. The third set was undertaking gauge changing trials at Shin-Yatsushiro Station (on Kyushu), commencing in 2014 for a proposed three-year period, however testing was suspended in December 2014 after accumulating approximating 33,000?km, following the discovery of defective thrust bearing oil seals on the bogies.[50] The train was being trialled between Kumamoto, travelling on the narrow gauge line to Shin-Yatsushiro, where a gauge changer has been installed, so the GCT could then be trialled on the Shinkansen line to Kagoshima. It was anticipated the train would travel approximately 600,000?km over the three-year trial.\\r\\nA new \\"full standard\\" Shinkansen line is under construction from Takeo Onsen to Nagasaki, with the Shin-Tosu ÿ Takeo Onsen section of the Kyushu Shinkansen branch to remain narrow gauge. GCTs were proposed to provide the Shinkansen service from the line's scheduled opening in March 2023, however with the GCT now being unavailable for service until at least 2025, other options are being considered, such as a 'relay' service.[44]\\r\\nThe main Shinkansen lines are:\\r\\nIn practice, the Tokaido, Sanyo and Kyushu lines form a contiguous west/southbound line from Tokyo, as train services run between the Tokaido and Sanyo lines and between the Sanyo and Kyushu lines, though the lines are operated by different companies.\\r\\nThe Tokaido Shinkansen is not physically connected to the lines of the Tohoku Shinkansen at Tokyo Station. Therefore, there is no through service between those lines. All northbound services from Tokyo travel along the Tohoku Shinkansen until at least miya.\\r\\nTwo further lines, known as Mini-shinkansen, have also been constructed by re-gauging and upgrading existing sections of line:\\r\\nThere are two standard-gauge lines not technically classified as Shinkansen lines but with Shinkansen services:\\r\\nMany Shinkansen lines were proposed during the boom of the early 1970s but have yet to be constructed. These are called Seibi Shinkansen (G???) or planned Shinkansen. One of these lines, the Narita Shinkansen to Narita Airport, was officially cancelled, but a few remain under development.\\r\\nThe following lines were also proposed in the 1973 plan, but have subsequently been shelved indefinitely.\\r\\nIn addition, the Basic Plan specified that the Jetsu Shinkansen should start from Shinjuku, not Tokyo Station, which would have required building an additional 30?km of track between Shinjuku and miya. While no construction work was ever started, land along the proposed track, including an underground section leading to Shinjuku Station, remains reserved. If capacity on the current Tokyoÿmiya section proves insufficient, at some point, construction of the Shinjukuÿmiya link may be reconsidered.\\r\\nThe Narita Shinkansen project to connect Tokyo to Narita International Airport, initiated in the 1970s but halted in 1983 after landowner protests, has been officially cancelled and removed from the Basic Plan governing Shinkansen construction. Parts of its planned right-of-way were used by the Narita Sky Access Line which opened in 2010. Although the Sky Access Line uses standard-gauge track, it was not built to Shinkansen specifications and there are no plans to convert it into a full Shinkansen line.\\r\\nIn December 2009, then transport minister Seiji Maehara proposed a bullet train link to Haneda Airport, using an existing spur that connects the Tkaid Shinkansen to a train depot. JR Central called the plan \\"unrealistic\\" due to tight train schedules on the existing line, but reports said that Maehara wished to continue discussions on the idea.[52] The current minister has not indicated whether this proposal remains supported. While the plan may become more feasible after the opening the Chuo Shinkansen (sometimes referred to as a bypass to the Tokaido Shinkansen) frees up capacity, construction is already underway for other rail improvements between Haneda and Tokyo station expected to be completed prior to the opening of the 2020 Tokyo Olympics, so any potential Shinkansen service would likely offer only marginal benefit beyond that.\\r\\nTrains are up to sixteen cars long. With each car measuring 25?m (82?ft) in length, the longest trains are 400?m (1/4?mile) end to end. Stations are similarly long to accommodate these trains. Some of Japan's high-speed maglev trains are considered Shinkansen,[53] while other slower maglev trains (such as the Linimo maglev train line serving local community near the city of Nagoya in Aichi, Japan) are intended as alternatives to conventional urban rapid transit systems.\\r\\nNote that these trains were and currently are used only for experimental runs\\r\\nOriginally intended to carry passenger and freight trains by day and night, the Shinkansen lines carry only passenger trains. The system shuts down between midnight and 06:00 every day for maintenance. The few overnight trains that still run in Japan run on the older narrow gauge network that the Shinkansen parallels.\\r\\nCompared with air transport, the Shinkansen has several advantages, including scheduling frequency and flexibility, punctual operation, comfortable seats, and convenient city-center terminals.\\r\\nShinkansen fares are generally competitive with domestic air fares. From a speed and convenience perspective, the Shinkansen's market share has surpassed that of air travel for journeys of less than 750?km, while air and rail remain highly competitive with each other in the 800ÿ900?km range and air has a higher market share for journeys of more than 1,000?km.[59]\\r\\nRailways using Shinkansen technology are not limited to those in Japan.\\r\\nTaiwan High Speed Rail operates 700T Series sets built by Kawasaki Heavy Industries.\\r\\nThe China Railways CRH2, built by CSR Sifang Loco & Rolling stocks corporation, with the license purchased from a consortium formed of Kawasaki Heavy Industries, Mitsubishi Electric Corporation, and Hitachi, is based on the E2-1000 series design.\\r\\nClass 395 EMUs were built by Hitachi based on Shinkansen technology for use on high-speed commuter services in Britain on the High Speed 1 line.\\r\\nIn 2014, it was announced[61] that Texas Central Railway would build a 300 mile long line using the N700 series rolling stock. The trains will run at over 320?km/h (200?mph).[62]\\r\\nIn December 2015, India and Japan signed an agreement for the construction of India's first high speed rail link connecting Mumbai to Ahmedabad. Funded primarily through Japanese soft loans, the link is expected to cost up to $18.6b and should be operational in about 7 years.[63][64]\\r\\nThis followed India and Japan conducting feasibility studies on high-speed rail and dedicated freight corridors.\\r\\nThe Indian Ministry of Railways' white-paper Vision 2020[65] submitted to Indian Parliament by Railway Minister Mamata Banerjee on 18 December 2009[66] envisages the implementation of regional high-speed rail projects to provide services at 250ÿ350?km/h.\\r\\nDuring Indian Prime Minister Manmohan Singh's visit to Tokyo in December 2006, Japan assured cooperation with India in creating a high-speed link between New Delhi and Mumbai.[67] In January 2009, the then Railway Minister Lalu Prasad rode a bullet train travelling from Tokyo to Kyoto.[68]\\r\\nIn December 2013 a Japanese consortium was appointed to undertake a feasibility study of a ~500?km high-speed line between Mumbai and Ahmedabad by July 2015.[69] A total of 7 high-speed lines are in planning stages in India, and Japanese firms have now succeeded in winning contracts to prepare feasibility studies for three of the lines.\\r\\nThe National High Speed Rail Corporation (NHSRC) was incorporated in 2017 to manage all HSR related activities in India. Under its management, a High Speed Rail Training Institute is being developed with Japanese assistance in Vadodara, Gujarat. After the laying of the foundation stone for the Mumbai and Ahmedabad by the Prime Ministers of India and Japan in September 2017, work began on preparatory surveys along the 508?km (316?mi) route. The route consists of approximately 477?km (296?mi) elevated viaduct through 11 districts of Gujarat and four districts of Maharashtra, a 21?km (13?mi) deep-sea tunnel starting from BKC in Mumbai, and approximately 10?km (6.2?mi) of at-grade alignment near the other terminus at Sabarmati, near Ahmedabad. Most of the civil works for the elevated viaduct shall be handled by Indian companies, while the deep-sea tunnel at Mumbai will be handled by a Japanese consortium (along with other technical aspects, such as safety, electricals, communication systems, signaling, and rolling stock). BHEL of India and Kawasaki Heavy Industries of Japan have entered into a technology collaboration agreement to build and assemble the rolling stock (of E5 series vintage) in India. Other potential joint ventures are being explored under the patronage of NHSRC. The line is expected to operational by 2023.[citation needed]\\r\\nJapan will provide Shinkansen technology for a high-speed rail link between Bangkok and the northern city of Chiang Mai under an agreement reached with Thailand on 27 May 2015. Total project costs are estimated in excess of 1 trillion yen ($8.1 billion). Several hurdles remain, however, including securing the funding. If the project is realized, it would mark the fifth time Shinkansen technology has been exported.[70]\\r\\nThe U.S. Federal Railroad Administration is in talks with a number of countries with high-speed rail, notably Japan, France and Spain. On 16 May 2009, FRA Deputy Chief Karen Rae expressed hope that Japan would offer its technical expertise to Canada and the United States. Transportation Secretary Ray LaHood indicated interest in test riding the Japanese Shinkansen in 2009.[71][72]\\r\\nOn 1 June 2009, JR Central Chairman, Yoshiyuki Kasai, announced plans to export both the N700 Series Shinkansen high-speed train system and the SCMaglev to international export markets, including the United States and Canada.[73]\\r\\nJapan is promoting its Shinkansen technology to the Government of Brazil for use on the planned high-speed rail set to link Rio de Janeiro, S?o Paulo and Campinas.[74] On 14 November 2008, Japanese Prime Minister Tar As and Brazilian President Luiz Incio Lula da Silva talked about this rail project. President Lula asked a consortium of Japanese companies to participate in the bidding process. Prime Minister Aso concurred on the bilateral cooperation to improve rail infrastructure in Brazil, including the RioÿS?o PauloÿCampinas high-speed rail line.[75] The Japanese consortium includes the Ministry of Land, Infrastructure, Transport and Tourism, Mitsui & Co., Mitsubishi Heavy Industries, Kawasaki Heavy Industries and Toshiba.[76][77]\\r\\nVietnam Railways was considering the use of Shinkansen technology for high-speed rail between the capital Hanoi and the southern commercial hub of Ho Chi Minh City, according to the Nihon Keizai Shimbun, citing an interview with Chief Executive Officer Nguyen Huu Bang. The Vietnamese government had already given basic approval for the Shinkansen system, although it still requires financing and formal consent from the prime minister. Vietnam rejected a funding proposal in 2010, so funding for the $56 billion project is uncertain. Hanoi was exploring additional Japanese funding Official Development Assistance as well as funds from the World Bank and Asian Development Bank. The 1,560-kilometre (970?mi) line would replace the current colonial-era rail line. Vietnam hopes to launch high-speed trains by 2020 and plans to start by building three sections, including a 90-kilometre stretch between the central coastal cities of Da Nang and Hu?, seen as potentially most profitable. Vietnam Railways had sent engineers to Central Japan Railway Company for technical training.[78][79]","input":"When was the first bullet train in japan?"},{"output":"Albert Henry Woolson","context":"Albert Henry Woolson (February 11, 1850 ÿ August 2, 1956) was the last surviving member of the Union Army who served in the American Civil War. He was also the last surviving Civil War veteran on either side whose status is undisputed. At least three men who followed him in death claimed to be Confederate veterans, but one has been debunked and the other two are unverified. The last surviving Union soldier to see combat was James Hard (1843ÿ1953).[1]\\r\\n\\r\\n\\r\\nWoolson was born in Antwerp, New York, to Willard P. Woolson (1811ÿ1862) and Caroline Baldwin (ca. 1822ÿunknown).[2] He claimed to be born on February 11, 1847, but his entry in the 1850 United States Census lists him as born in 1850.[3][4] Entries in the later census records and in the 1905 Minnesota State Census support the conclusion that he was born in 1850.[5]\\r\\nHis father, Willard Woolson, enlisted in the Union Army. Willard was wounded at the Battle of Shiloh and was transported to an Army hospital in Windom, Minnesota, where he would die of his wounds. Albert and his mother moved to Windom to accompany Willard. Albert enlisted as a drummer boy in Company C, 1st Minnesota Heavy Artillery Regiment on October 10, 1864, becoming the company's drummer. However, the company never saw action, and Albert Woolson was discharged on September 7, 1865.\\r\\nWoolson returned to Minnesota, where he lived the rest of his life. He was a carpenter and later a member of the Grand Army of the Republic (G.A.R.), a powerful political organization made up of Civil War veterans where he became senior vice commander in chief in 1953.\\r\\nIn his final days, he lived at 215 East Fifth Street in Duluth, Minnesota. Woolson died at St. Luke's Hospital in Duluth on August 2, 1956, at what was then thought to be the age of 109, of a \\"recurring lung congestion condition\\". He was twice widowed and was survived by six daughters and two sons. Woolson was buried with full military honors by the National Guard at Park Hill Cemetery.[6]\\r\\nFollowing his death, President Dwight D. Eisenhower said:\\r\\n\\"The American people have lost the last personal link with the Union Army ... His passing brings sorrow to the hearts of all of us who cherished the memory of the brave men on both sides of the War Between the States.\\"[7]\\r\\nWoolson and fellow drummer-boy Frank Mayer marched together, both aged 99, in the Memorial Day Parade in May, 1949, to lay a wreath at the tomb of General Grant in New York City.\\r\\nLife magazine ran a seven-page article upon the death of Albert Woolson, in the August 20, 1956 issue.[8] The article included much information about the G.A.R., with pictures or drawings of several encampments (conventions). Also included are photos of the last three living Confederate soldiers (status and age disputed): William Lundy, 108; Walter Williams, 113; and John Salling, 110.\\r\\nIn mid-2006, new census research indicated that Albert Woolson was actually only 106 years old, being listed as less than one year old in the 1850 census. Previous research in 1991 had suggested he was only a year younger than claimed (108 instead of 109), although this does not affect his veteran status.\\r\\nAfter his death, the Grand Army of the Republic was dissolved because Woolson was its last surviving member.\\r\\nThe 2011ÿ12 Minnesota Legislative Manual was dedicated to him.[9]\\r\\nIn 1956 a monument of Woolson was erected in Gettysburg as a memorial to the Grand Army of the Republic.[10]\\r\\nWoolson as \\"Henry Albert Woolson\\" in the 1850 census as a newborn","input":"When did the last civil war person died?"},{"output":"Six pack rings or six pack yokes","context":"Six pack rings or six pack yokes are a set of connected plastic rings that are used in multi-packs of beverage, particularly six packs of beverage cans.\\r\\nThe six pack rings in most common use today are the descendants of an original design by ITW Hi-Cone, which first introduced them in St. Louis, Missouri in the summer of 1960.[1] Within 10 years, plastic rings had completely replaced the paper and metal based holders then common in the market.[1] Today several other manufacturers continue to produce six pack rings. Though interest in multi-packs has continued to grow, other variations, including paperboard baskets and HDPE plastic can carriers have grown in popularity, providing an alternative to conventional six pack rings.[2]\\r\\nSince the late 1970s, six pack rings were cited as a particularly dangerous form of marine litter as marine wildlife were found entangled in the rings, sometimes strangling to death. But since 1989, six-pack rings in the US have been manufactured to be 100 percent photo-degradable, so the plastic will begin to disintegrate in just a few weeks, allowing animals to easily free themselves from the brittle and crumbling rings.[3] This is in accordance with the US Federal regulation for testing plastic photo-degradation, which is 40 CFR Ch. I (7ÿ1ÿ03 Edition) PART 238.[4] In 2016, SaltWater Brewery developed edible rings that sea-creatures could consume safely.[5][6]\\r\\nSix-pack rings are now a relatively minor contributor to marine litter and wildlife fatalities. Fishing gear and other plastic wastes are a larger problem.[7][8]","input":"What is the plastic that holds cans together?"},{"output":"the development of space travel techniques to support the Apollo mission to land astronauts on the Moon","context":"\\r\\n\\r\\n2:\\r\\n\\r\\nProject Gemini was NASA's second human spaceflight program. Conducted between projects Mercury and Apollo, Gemini started in 1961 and concluded in 1966. The Gemini spacecraft carried a two-astronaut crew. Ten Gemini crews flew low Earth orbit (LEO) missions during 1965 and 1966, putting the United States in the lead during the Cold War Space Race against the Soviet Union.\\r\\n\\r\\nGemini's objective was the development of space travel techniques to support the Apollo mission to land astronauts on the Moon. It performed missions long enough for a trip to the Moon and back, perfected working outside the spacecraft with extra-vehicular activity (EVA), and pioneered the orbital maneuvers necessary to achieve space rendezvous and docking. With these new techniques proven by Gemini, Apollo could pursue its prime mission without doing these fundamental exploratory operations.\\r\\n\\r\\nAll Gemini flights were launched from Launch Complex 19 (LC-19) at Cape Kennedy Air Force Station in Florida. Their launch vehicle was the GeminiÿTitan II, a modified Intercontinental Ballistic Missile (ICBM).[Note 1] Gemini was the first program to use the newly built Mission Control Center at the Houston Manned Spacecraft Center for flight control.[Note 2]\\r\\n\\r\\nThe astronaut corps that supported Project Gemini included the \\"Mercury Seven\\", \\"The New Nine\\", and the 1963 astronaut class. During the program, three astronauts died in air crashes during training, including the prime crew for Gemini 9. This mission was flown by the backup crew, the only time that has happened in NASA's history to date.\\r\\n\\r\\nGemini was robust enough that the United States Air Force planned to use it for the Manned Orbital Laboratory (MOL) program, which was later canceled. Gemini's chief designer, Jim Chamberlin, also made detailed plans for cislunar and lunar landing missions in late 1961. He believed that Gemini spacecraft could fly in lunar operations before Project Apollo, and cost less. NASA's administration did not approve those plans. In 1969, McDonnell-Douglas proposed a \\"Big Gemini\\" that could have been used to shuttle up to 12 astronauts to the planned space stations in the Apollo Applications Project (AAP). The only AAP project funded was Skylab ÿ which used existing spacecraft and hardware ÿ thereby eliminating the need for Big Gemini.\\r\\n\\r\\nAfter the existing Apollo program was chartered by President John F. Kennedy on May 25, 1961, to land men on the Moon, it became evident to NASA officials that a follow-on to the Mercury program was required to develop certain spaceflight capabilities in support of Apollo.\\r\\n\\r\\nSpecifically, Jim Chamberlin, the head of engineering at the Space Task Group (STG), was already assigned to start working on a bridge program between Mercury and Apollo in February 1961.[2] He presented two initial versions of Gemini at a NASA retreat at Wallops Island in March 1961.[2] Scale models of Mercury Mark II were shown in July 1961 at McDonnell Aircraft Corporation's offices in St. Louis.[2]  NASA approved Project Gemini on December 7, 1961.[2] The McDonnell Aircraft Corporation was contracted to build it on December 22, 1961.[3]\\r\\n\\r\\nWhen it was publicly announced on January 3, 1962, it was formally re-christened Project Gemini. Gemini in Latin means \\"twins\\" or \\"double\\", which reflected that the spacecraft would hold two astronauts. Gemini is also the name of the third constellation of the Zodiac and its twin stars, Castor and Pollux.\\r\\n\\r\\nThe major objectives were:[4]\\r\\n\\r\\nThe Canadian Jim Chamberlin designed the Gemini capsule, which carried a crew of two. He was previously the chief aerodynamicist on Avro Canada's Avro Arrow fighter interceptor program.[5] Chamberlin joined NASA along with 25 senior Avro engineers after cancellation of the Arrow program, and became head of the U.S. Space Task Group's engineering division in charge of Gemini.[5][6] The prime contractor was McDonnell Aircraft Corporation, which was also the prime contractor for the Project Mercury capsule.[7]\\r\\n\\r\\nAstronaut Gus Grissom was heavily involved in the development and design of the Gemini spacecraft (the other Mercury astronauts dubbed the Gemini spacecraft the \\"Gusmobile\\").[8] Grissom wrote in his posthumous 1968 book Gemini! that the realization of Project Mercury's end and the unlikelihood of his having another flight in that program prompted him to focus all of his efforts on the upcoming Gemini program.\\r\\n\\r\\nThe Gemini program was managed by the Manned Spacecraft Center, located in Houston, Texas, under direction of the Office of Manned Space Flight, NASA Headquarters, Washington, D.C. Dr. George E. Mueller, Associate Administrator of NASA for Manned Space Flight, served as acting director of the Gemini program. William C. Schneider, Deputy Director of Manned Space Flight for Mission Operations, served as mission director on all Gemini flights beginning with Gemini 6A.\\r\\n\\r\\nGuenter Wendt was a McDonnell engineer who supervised launch preparations for both the Mercury and Gemini programs and would go on to do the same when the Apollo program launched crews. His team was responsible for completion of the complex pad close-out procedures just prior to spacecraft launch, and he was the last person the astronauts would see prior to closing the hatch. The astronauts appreciated his taking absolute authority over, and responsibility for, the condition of the spacecraft and developed a good-humored rapport with him.[9]\\r\\n\\r\\nNASA selected McDonnell Aircraft, which had been the prime contractor for the Project Mercury capsule, in 1961 to build the Gemini capsule, the first of which was delivered in 1963. The spacecraft was 18?feet 5?inches (5.61?m) long and 10 feet (3.0?m) wide, with a launch weight varying from 7,100 to 8,350 pounds (3,220 to 3,790?kg).[10]\\r\\n\\r\\nThe Gemini crew capsule (referred to as the Reentry Module) was essentially an enlarged version of the Mercury capsule. Unlike Mercury, the retrorockets, electrical power, propulsion systems, oxygen, and water were located in a detachable Adapter Module behind the Reentry Module. A major design improvement in Gemini was to locate all internal spacecraft systems in modular components, which could be independently tested and replaced when necessary, without removing or disturbing other already tested components.\\r\\n\\r\\nMany components in the capsule itself were reachable through their own small access doors. Unlike Mercury, Gemini used completely solid-state electronics, and its modular design made it easy to repair.[11]\\r\\n\\r\\nGemini's emergency launch escape system did not use an escape tower powered by a solid-fuel rocket, but instead used aircraft-style ejection seats. The tower was heavy and complicated, and NASA engineers reasoned that they could do away with it as the Titan II's hypergolic propellants would burn immediately on contact. A Titan II booster explosion had a smaller blast effect and flame than on the cryogenically fueled Atlas and Saturn. Ejection seats were sufficient to separate the astronauts from a malfunctioning launch vehicle. At higher altitudes, where the ejection seats could not be used, the astronauts would return inside the spacecraft, which would separate from the launch vehicle.[12]\\r\\n\\r\\nThe main proponent of using ejection seats was James Chamberlin, head of the engineering division of NASA's Space Force Task Group. Chamberlin had never liked the Mercury escape tower and wished to use a simpler alternative that would also reduce weight. He reviewed several films of Atlas and Titan II ICBM failures, which he used to estimate the approximate size of a fireball produced by an exploding launch vehicle and from this he gauged that the Titan II would produce a much smaller explosion, thus the spacecraft could get away with ejection seats.\\r\\n\\r\\nMaxime Faget, the designer of the Mercury LES, was on the other hand less-than-enthusiastic about this setup. Aside from the possibility of the ejection seats seriously injuring the astronauts, they would also only be usable for about 40 seconds after liftoff, by which point the booster would be attaining Mach 1 speed and ejection would no longer be possible. He was also concerned about the astronauts being launched through the Titan's exhaust plume if they ejected in-flight and later added that \\"The best thing about Gemini was that they never had to make an escape.\\"[13]\\r\\n\\r\\nGemini was the first astronaut-carrying spacecraft to include an onboard computer, the Gemini Guidance Computer, to facilitate management and control of mission maneuvers. This computer, sometimes called the Gemini Spacecraft On-Board Computer (OBC), was very similar to the Saturn Launch Vehicle Digital Computer. The Gemini Guidance Computer weighed 58.98 pounds (26.75?kg). Its core memory had 4096 addresses, each containing a 39-bit word composed of three 13-bit \\"syllables\\". All numeric data was 26-bit two's-complement integers (sometimes used as fixed-point numbers), either stored in the first two syllables of a word or in the accumulator. Instructions (always with a 4-bit opcode and 9 bits of operand) could go in any syllable.[14][15][16][17]\\r\\n\\r\\nUnlike Mercury, the Gemini used in-flight radar and an artificial horizondevices similar to those used in the aviation industry.[14]\\r\\n\\r\\nThe original intention for Gemini was to land on solid ground instead of at sea, using a Rogallo wing rather than a parachute, with the crew seated upright controlling the forward motion of the craft. To facilitate this, the airfoil did not attach just to the nose of the craft, but to an additional attachment point for balance near the heat shield. This cord was covered by a strip of metal which ran between the twin hatches.[18] This design was ultimately dropped, and parachutes were used to make a sea landing as in Mercury. The capsule was suspended at an angle closer to horizontal, so that a side of the heat shield contacted the water first. This eliminated the need for the landing bag cushion used in the Mercury capsule.\\r\\n\\r\\nThe adapter module in turn was separated into a Retro module and an Equipment module.\\r\\n\\r\\nThe Retro module contained four solid-fuel TE-M-385 Star-13E retrorockets, each spherical in shape except for its rocket nozzle, which were structurally attached to two beams that reached across the diameter of the retro module, crossing at right angles in the center.[19] Re-entry began with the retrorockets firing one at a time. Abort procedures at certain periods during lift-off would cause them to fire at the same time, thrusting the Descent module away from the Titan rocket.\\r\\n\\r\\nGemini was equipped with an Orbit Attitude and Maneuvering System (OAMS), containing sixteen thrusters for translation control in all three perpendicular axes (forward/backward, left/right, up/down), in addition to attitude control (pitch, yaw, and roll angle orientation) as in Mercury. Translation control allowed changing orbital inclination and altitude, necessary to perform space rendezvous with other craft, and docking with the Agena Target Vehicle (ATV), with its own rocket engine which could be used to perform greater orbit changes.\\r\\n\\r\\nEarly short-duration missions had their electrical power supplied by batteries; later endurance missions used the first fuel cells in manned spacecraft.\\r\\n\\r\\nGemini was in some regards more advanced than Apollo because the latter program began almost a year earlier. It became known as a \\"pilot's spacecraft\\" due to its assortment of jet fighter-like features, in no small part due to Gus Grissom's influence over the design, and it was at this point where the American manned space program clearly began showing its superiority over that of the Soviet Union with long duration flight, rendezvous, and extravehicular capability.[Note 4] The Soviet Union during this period was developing the Soyuz spacecraft intended to take cosmonauts to the Moon, but political and technical problems began to get in the way, leading to the ultimate end of their manned lunar program.\\r\\n\\r\\nThe Titan II had debuted in 1962 as the Air Force's second-generation ICBM to replace the Atlas. By using hypergolic fuels, it could be stored for long periods of time and be easily readied for launch in addition to being a simpler design with fewer components, the only caveat being that the propellant mix (nitrogen tetroxide and hydrazine) was extremely toxic compared to the Atlas's liquid oxygen/RP-1. However, the Titan had considerable difficulty being man-rated due to early problems with pogo oscillation. The launch vehicle used a radio guidance system that was unique to launches from Cape Kennedy.\\r\\n\\r\\nDeke Slayton, as director of flight crew operations, had primary responsibility for assigning crews for the Gemini program. Each flight had a primary crew and backup crew, and the backup crew would rotate to primary crew status three flights later. Slayton intended for first choice of mission commands to be given to the four remaining active astronauts of the Mercury Seven: Alan Shepard, Grissom, Cooper, and Schirra. (John Glenn had retired from NASA in January 1964 and Scott Carpenter, who was blamed by some in NASA management for the problematic reentry of Aurora 7, was on leave to participate in the Navy's SEALAB project and was grounded from flight in July 1964 due to an arm injury sustained in a motorbike accident. Slayton himself continued to be grounded due to a heart problem.)\\r\\n\\r\\nTitles used for the left-hand (command) and right-hand seat crew positions were taken from the U.S. Air Force pilot ratings, Command Pilot and Pilot. Sixteen astronauts flew on 10 manned Gemini missions:\\r\\n\\r\\nIn late 1963, Slayton selected Shepard and Stafford for Gemini 3, McDivitt and White for Gemini 4, and Schirra and Young for Gemini 5 (which was to be the first Agena rendezvous mission). The backup crew for Gemini 3 was Grissom and Borman, who were also slated for Gemini 6, to be the first long-duration mission. Finally Conrad and Lovell were assigned as the backup crew for Gemini 4.\\r\\n\\r\\nDelays in the production of the Agena Target Vehicle caused the first rearrangement of the crew rotation. The Schirra and Young mission was bumped to Gemini 6 and they became the backup crew for Shepard and Stafford. Grissom and Borman then had their long-duration mission assigned to Gemini 5.\\r\\n\\r\\nThe second rearrangement occurred when Shepard developed Mnire's disease, an inner ear problem. Grissom was then moved to command Gemini 3. Slayton felt that Young was a better personality match with Grissom and switched Stafford and Young. Finally, Slayton tapped Cooper to command the long-duration Gemini 5. Again for reasons of compatibility, he moved Conrad from backup commander of Gemini 4 to pilot of Gemini 5, and Borman to backup command of Gemini 4. Finally he assigned Armstrong and Elliot See to be the backup crew for Gemini 5.\\r\\nThe third rearrangement of crew assignment occurred when Slayton felt that See wasn't up to the physical demands of EVA on Gemini 8. He reassigned See to be the prime commander of Gemini 9 and put Scott as pilot of Gemini 8 and Charles Bassett as the pilot of Gemini 9.\\r\\n\\r\\nThe fourth and final rearrangement of the Gemini crew assignment occurred after the deaths of See and Bassett when their trainer jet crashed, coincidentally into a McDonnell building which held their Gemini 9 capsule in St. Louis. The backup crew of Stafford and Cernan was then moved up to the new prime crew of the re-designated Gemini 9A. Lovell and Aldrin were moved from being the backup crew of Gemini 10 to be the backup crew of Gemini 9. This cleared the way through the crew rotation for Lovell and Aldrin to become the prime crew of Gemini 12.\\r\\n\\r\\nAlong with the deaths of Grissom, White, and Roger Chaffee in the fire of Apollo 1, this final arrangement helped determine the makeup of the first seven Apollo crews, and who would be in position for a chance to be the first to walk on the Moon.\\r\\n\\r\\nIn 1964 and 1965 two Gemini missions were flown without crews to test out systems and the heat shield. These were followed by ten flights with crews in 1965 and 1966. All were launched by Titan II launch vehicles. Some highlights from the Gemini program:\\r\\n\\r\\nRendezvous in orbit is not a straightforward maneuver. Should a spacecraft increase its speed to catch up with another, the result is that it goes into a higher and slower orbit and the distance thereby increases. The right procedure is to slow down and go to a lower orbit first, and then later to increase speed and go to the same orbit as the other.[21] To practice these maneuvers special rendezvous and docking simulators were built for the astronauts.[22]\\r\\n\\r\\nEdward White during spacewalk, Gemini 4, June 1965\\r\\n\\r\\nRendezvous of Gemini 6 and 7, December 1965\\r\\n\\r\\nFirst docking; Agena target is seen from Gemini 8, March 1966\\r\\n\\r\\nThe Gemini-Titan II launch vehicle was adapted by NASA from the U.S. Air Force Titan II ICBM. (Similarly, the Mercury-Atlas launch vehicle had been adapted from the USAF Atlas missile.) The Gemini-Titan II rockets were assigned Air Force serial numbers, which were painted in four places on each Titan II (on opposite sides on each of the first and second stages). USAF crews maintained Launch Complex 19 and prepared and launched all of the Gemini-Titan II launch vehicles. Data and experience operating the Titans was of value to both the U.S. Air Force and NASA.\\r\\n\\r\\nThe USAF serial numbers assigned to the Gemini-Titan launch vehicles are given in the tables above. Fifteen Titan IIs were ordered in 1962 so the serial is \\"62-12XXX\\", but only \\"12XXX\\" is painted on the Titan II. The order for the last three of the 15 launch vehicles was canceled on July 30, 1964, and they were never built. Serial numbers were, however, assigned to them prospectively: 12568 - GLV-13; 12569 - GLV-14; and 12570 - GLV-15.\\r\\n\\r\\nFrom 1962 to 1967, Gemini cost $1.3 billion in 1967 dollars ($7.32?billion in 2016[23]).[1] In January 1969, a NASA report to the US Congress estimating the costs for Mercury, Gemini, and Apollo (through the first manned Moon landing) included $1.2834 billion for Gemini: $797.4 million for spacecraft, $409.8 million for launch vehicles, and $76.2 million for support.[24]\\r\\n\\r\\nMcDonnell Aircraft, the main contractor for Mercury and Gemini, was also one of the original bidders on the prime contract for Apollo, but lost out to North American Aviation. McDonnell later sought to extend the Gemini program by proposing a derivative which could be used to fly a cislunar mission and even achieve a manned lunar landing earlier and at less cost than Apollo, but these proposals were rejected by NASA.\\r\\n\\r\\nA range of applications were considered for Advanced Gemini missions, including military flights, space station crew and logistics delivery, and lunar flights. The Lunar proposals ranged from reusing the docking systems developed for the Agena Target Vehicle on more powerful upper stages such as the Centaur, which could propel the spacecraft to the Moon, to complete modifications of the Gemini to enable it to land on the lunar surface. Its applications would have ranged from manned lunar flybys before Apollo was ready, to providing emergency shelters or rescue for stranded Apollo crews, or even replacing the Apollo program.\\r\\n\\r\\nSome of the Advanced Gemini proposals used \\"off-the-shelf\\" Gemini spacecraft, unmodified from the original program, while others featured modifications to allow the spacecraft to carry more crew, dock with space stations, visit the Moon, and perform other mission objectives. Other modifications considered included the addition of wings or a parasail to the spacecraft, in order to enable it to make a horizontal landing.\\r\\n\\r\\nBig Gemini (or \\"Big G\\") was another proposal by McDonnell Douglas made in August 1969. It was intended to provide large-capacity, all-purpose access to space, including missions that ultimately used Apollo or the Space Shuttle.\\r\\n\\r\\nThe study was performed to generate a preliminary definition of a logistic spacecraft derived from Gemini that would be used to resupply an orbiting space station. Land-landing at a preselected site and refurbishment and reuse were design requirements. Two baseline spacecraft were defined: a nine-man minimum modification version of the Gemini B called Min-Mod Big G and a 12-man advanced concept, having the same exterior geometry but with new, state-of-the-art subsystems, called Advanced Big G.[citation needed] Three launch vehicles-Saturn IB, Titan IIIM, and Saturn INT-20 (S-IC/S-IVB) were investigated for use with the spacecraft.\\r\\n\\r\\nThe Air Force had an interest in the Gemini system, and decided to use its own modification of the spacecraft as the crew vehicle for the Manned Orbital Laboratory. To this end, the Gemini 2 spacecraft was refurbished and flown again atop a mockup of the MOL, sent into space by a Titan IIIC. This was the first time a spacecraft went into space twice.\\r\\n\\r\\nThe USAF also had the notion of adapting the Gemini spacecraft for military applications, such as crude observation of the ground (no specialized reconnaissance camera could be carried) and practicing making rendezvous with suspicious satellites. This project was called Blue Gemini. The USAF did not like the fact that Gemini would have to be recovered by the US Navy, so they intended for Blue Gemini eventually to use the airfoil and land on three skids, carried over from the original design of Gemini.\\r\\n\\r\\nAt first some within NASA welcomed sharing of the cost with the USAF, but it was later agreed that NASA was better off operating Gemini by itself. Blue Gemini was canceled in 1963 by Secretary of Defense Robert McNamara, who decided that the NASA Gemini flights could conduct necessary military experiments. MOL was canceled by Secretary of Defense Melvin Laird in 1969, when it was determined that unmanned spy satellites could perform the same functions much more cost-effectively.\\r\\n\\r\\n?This article incorporates?public domain material from websites or documents of the National Aeronautics and Space Administration.","input":"What was the purpose of the gemini mission?"},{"output":"Justice R. Subhash Reddy","context":"The Chief Justice of Gujarat presides over the Gujarat High Court. They are appointed by the President of India under warrant. The President is required to consult the Governor of Gujarat and the Chief Justice of India before making such appointment. The Governor of Gujarat administers the oath of office at time of appointment.\\r\\nThe Chief Justice of the High Court of Gujarat is Mr. Justice R. Subhash Reddy.","input":"Who is chief justice of gujarat high court?"},{"output":"Amphiboles","context":"Amphibole ( /??mf?bo?l/) is the name of an important group of generally dark-colored, inosilicate minerals, forming prism or needlelike crystals,[1] composed of double chain SiO\\r\\n4 tetrahedra, linked at the vertices and generally containing ions of iron and/or magnesium in their structures. Amphiboles can be green, black, colorless, white, yellow, blue, or brown. The International Mineralogical Association currently classifies amphiboles as a mineral supergroup, within which are two groups and several subgroups.[2]\\r\\n\\r\\n\\r\\nAmphiboles crystallize into two crystal systems, monoclinic and orthorhombic. In chemical composition and general characteristics they are similar to the pyroxenes. The chief differences from pyroxenes are that (i) amphiboles contain essential hydroxyl (OH) or halogen (F, Cl) and (ii) the basic structure is a double chain of tetrahedra (as opposed to the single chain structure of pyroxene). Most apparent, in hand specimens, is that amphiboles form oblique cleavage planes (at around 120 degrees), whereas pyroxenes have cleavage angles of approximately 90 degrees. Amphiboles are also specifically less dense than the corresponding pyroxenes. In optical characteristics, many amphiboles are distinguished by their stronger pleochroism and by the smaller angle of extinction (Z angle c) on the plane of symmetry. Amphiboles are the primary constituent of amphibolites.\\r\\nAmphiboles are minerals of either igneous or metamorphic origin; in the former case occurring as constituents (hornblende) of igneous rocks, such as granite, diorite, andesite and others. Calcium is sometimes a constituent of naturally occurring amphiboles. Those of metamorphic origin include examples such as those developed in limestones by contact metamorphism (tremolite) and those formed by the alteration of other ferromagnesian minerals (hornblende). Pseudomorphs of amphibole after pyroxene are known as uralite.\\r\\nThe name amphibole (Ancient Greek ?׶?? - amphbolos meaning 'ambiguous') was used by Ren Just Hay to include tremolite, actinolite, tourmaline and hornblende. The group was so named by Hay in allusion to the protean variety, in composition and appearance, assumed by its minerals. This term has since been applied to the whole group. Numerous sub-species and varieties are distinguished, the more important of which are tabulated below in two series. The formulae of each will be seen to be built on the general double-chain silicate formula RSi4O11.[3]\\r\\nFour of the amphibole minerals are among the minerals commonly called asbestos. These are: anthophyllite, riebeckite, cummingtonite/grunerite series, and actinolite/tremolite series. The cummingtonite/grunerite series is often termed amosite or brown asbestos; riebeckite is known as crocidolite or blue asbestos. These are generally called amphibole asbestos.[4]\\r\\nOrthorhombic series\\r\\nMonoclinic series\\r\\nOn account of the wide variations in chemical composition, the different members vary considerably in properties and general appearance.\\r\\nAnthophyllite occurs as brownish, fibrous or lamellar masses with hornblende in mica-schist at Kongsberg in Norway and some other localities. An aluminous related species is known as gedrite and a deep green Russian variety containing little iron as kupfferite.[3]\\r\\nHornblende is an important constituent of many igneous rocks. It is also an important constituent of amphibolites formed by metamorphism of basalt.\\r\\nActinolite is an important and common member of the monoclinic series, forming radiating groups of acicular crystals of a bright green or greyish-green color. It occurs frequently as a constituent of greenschists. The name (from Greek ?ٶ??, ?ٶ??/akts, akt?nos, a 'ray' and ?Ĳ?/lthos, a 'stone') is a translation of the old German word Strahlstein (radiated stone).[3]\\r\\nGlaucophane, crocidolite, riebeckite and arfvedsonite form a somewhat special group of alkali-amphiboles. The first two are blue fibrous minerals, with glaucophane occurring in blueschists and crocidolite (blue asbestos) in ironstone formations, both resulting from dynamo-metamorphic processes. The latter two are dark green minerals, which occur as original constituents of igneous rocks rich in sodium, such as nepheline-syenite and phonolite.[3]\\r\\nPargasite is a rare magnesium-rich amphibole with essential sodium, usually found in ultramafic rocks. For instance, it occurs in uncommon mantle xenoliths, carried up by kimberlite. It is hard, dense, black and usually idiomorphic, with a red-brown pleochroism in petrographic thin section.","input":"The most common member of the amphibole group?"},{"output":"Quebec City","context":"The Catholic Church in Canada is part of the worldwide Catholic Church, under the spiritual leadership of the Pope. As of 2011[update], it has the largest number of adherents to a Christian denomination and a religion in Canada, with 38.7% of Canadians (12.73 million) baptized as Catholics. There are 52 dioceses and about 8,000[citation needed] priests in Canada.\\r\\n\\r\\n\\r\\nCatholicism arrived in Canada in 1497, when John Cabot landed on Newfoundland, raised the Venetian and Papal banners and claimed the land for his sponsor King Henry VII of England, while recognizing the religious authority of the Roman Catholic Church.[1] A letter of John Day states that Cabot landed on 24 June 1497 and \\"went ashore with a crucifix and raised banners bearing the arms of the Holy Father\\". In 1608, Samuel de Champlain founded the first Catholic colony in Quebec City. In 1611, he established a fur trading post on the Island of Montreal, which later became a Catholic colony for trade and missionary activity.[citation needed]\\r\\nIn 1620, George Calvert, 1st Baron Baltimore purchased a tract of land in Newfoundland from Sir William Vaughan and established a colony, calling it Avalon, after the legendary spot where Christianity was introduced to Britain.[2] In 1627 Calvert brought two Roman Catholic priests to Avalon. This was the first continuous Roman Catholic ministry in British North America. Despite the severe religious conflicts of the period, Calvert secured the right of Catholics to practice their religion unimpeded in Newfoundland, and embraced the novel principle of religious tolerance, which he wrote into the Charter of Avalon and the later Charter of Maryland. The Colony of Avalon was thus the first North American jurisdiction to practice religious tolerance.[3]\\r\\nFears of the Catholic Church were quite strong in the 19th century, especially among Presbyterian and other Protestant Irish immigrants across Canada.[4] In 1853, the Gavazzi Riots left 10 dead in Quebec in the wake of Catholic Irish protest against anti-Catholic speeches by ex-monk Alessandro Gavazzi.[5][6]\\r\\nThe major flashpoint was public support for Catholic French language schools. Although the Confederation Agreement of 1867 guaranteed the status of Catholic schools where they had been legalized, disputes erupted in numerous provinces, especially in the Manitoba Schools Question in the 1890s and Ontario in the 1910s.[7] In Ontario, Regulation 17 was a regulation by the Ontario Ministry of Education, that restricted the use of French as a language of instruction to the first two years of schooling. French Canada reacted vehemently, and lost, dooming its French language Catholic schools. This became a central reason why French Canada distanced itself from the war effort, as its young men refused to enlist.[8]\\r\\nProtestant elements succeeded in blocking the growth of French-language Catholic public schools. However, the Irish Catholics generally supported the English language position advocated by the Protestants.[9]\\r\\nThe central theme of Catholic history from the 1840s through the 1920s was the contest for control of the church between the French, based in Quebec, and the English-speaking Irish (along with smaller numbers of Catholic Scots, English, and others) based in Ontario.[10] The French Catholics saw Catholics in general as God's chosen people (versus Protestants) and the French as more truly Catholic than any other ethnic group. The fact that the Irish Catholics formed coalition with the anti-French Protestants further infuriated the French.\\r\\nThe Irish Catholics collaborated with Protestants inside Canada, on the school issue: they opposed French language Catholic schools. The Irish had a significant advantage since they were favoured by the Vatican. Irish Catholicism was \\"ultramontane\\", which meant its adherents professed total obedience to the Pope. By contrast, the French bishops in Canada kept their distance from the Vatican. In the form of Regulation 17 this became the central issue that finally alienated the French in Quebec from the Canadian Anglophone establishment during the First World War.[11][12] Ontario's Catholics were led by the Irish Bishop Fallon, who united with the Protestants in opposing French schools.[13] Regulation 17 was repealed in 1927.[14][15] The French-speakers remain more liberal than the English-speakers to this day, and in addition are also leaving the faith much more quickly.\\r\\nOne by one, the Irish took control of the church in each province except for Quebec. Tensions were especially high in Manitoba at the end of the 19th century. In Alberta in the 1920s, a new Irish bishop undermined French language Catholic schooling, and removed the Francophile order of teaching sisters.[16]\\r\\nIn the Dominion of Newfoundland (which was an independent dominion before joining Canada in 1949), politics was polarized around religious lines, with the Protestants confronting the Irish Catholics.[17]\\r\\nIn 1861, the Protestant governor dismissed the Catholic Liberals from office and the ensuing election was marked by riot and disorder with both the Anglican bishop Edward Feild and Catholic bishop John Thomas Mullock taking partisan stances. The Protestants narrowly elected Hugh Hoyles as the Conservative Prime Minister. Hoyles suddenly reversed his long record of militant Protestant activism and worked to defuse tensions. He shared patronage and power with the Catholics; all jobs and patronage were split between the various religious bodies on a per capita basis. This 'denominational compromise' was further extended to education when all religious schools were put on the basis which the Catholics had enjoyed since the 1840s. Alone in North America, Newfoundland had a state funded system of denominational schools. The compromise worked and politics ceased to be about religion and became concerned with purely political and economic issues.[18]\\r\\nThe Roman Catholic population in Canada in 2001[19] and 2011.[20]\\r\\nThe Catholic population underwent its first recorded drop between 2001 and 2011. Notable trends include the de-Catholicization of Quebec, a drop in the Catholic population in small provinces with stagnant populations, and a rise in Catholics in the large English-speaking provinces of Ontario, British Columbia, and Alberta. Immigration has not helped prevent the decline in the Catholic population; the only major source of Catholic immigrants to Canada is the Philippines. There are also adherents of Eastern Catholic Churches who had already migrated to Canada, most notably the Ukrainians.\\r\\nWithin Canada, the Latin hierarchy consists of:\\r\\nThere is a Military Ordinariate of Canada for Canadian military personnel.\\r\\nThe Anglican use of the Latin Church is served from the United States, based in Houston, Texas, by the Personal Ordinariate of the Chair of Saint Peter.\\r\\nOne former Canadian bishopric, the francophone Roman Catholic Diocese of Gravelbourg in Saskatchewan, has since its suppression in 1998 become a titular episcopal see, which may be bestowed on any Latin bishop without proper diocese, working in the Roman Curia or anywhere in the world.\\r\\nThere is a Ukrainian Greek Catholic (Byzantine Rite) province, headed by the Metropolitan Archeparchy of Winnipeg, which has four suffragan eparchies (dioceses):\\r\\nThere are 4 other eparchies and 2 exarchates in Canada:\\r\\nA few Eastern particular church communities are pastorally served from the United States:","input":"Where was the first church in canada created?"},{"output":"in the 5th and 6th centuries","context":"Anglo-Saxon England was early medieval England, existing from the 5th to the 11th century from the end of Roman Britain until the Norman conquest in 1066. It consisted of various Anglo-Saxon kingdoms until 927 when it was united as the Kingdom of England by King ?thelstan (r. 927ÿ939). It became part of the North Sea Empire of Cnut the Great, a personal union between England, Denmark and Norway in the 11th century.\\r\\nThe Anglo-Saxons were the members of Germanic-speaking groups who migrated to the southern half of the island from continental Europe, and their cultural descendants. Anglo-Saxon history thus begins during the period of Sub-Roman Britain following the end of Roman control, and traces the establishment of Anglo-Saxon kingdoms in the 5th and 6th centuries (conventionally identified as seven main kingdoms: Northumbria, Mercia, East Anglia, Essex, Kent, Sussex, and Wessex), their Christianisation during the 7th century, the threat of Viking invasions and Danish settlers, the gradual unification of England under Wessex hegemony during the 9th and 10th centuries, and ending with the Norman conquest of England by William the Conqueror in 1066.\\r\\nAnglo-Saxon identity survived beyond the Norman conquest,[1] came to be known as Englishry under Norman rule and ultimately developed into the modern English people.\\r\\n\\r\\n\\r\\nBede completed his book Historia ecclesiastica gentis Anglorum (Ecclesiastical History of the English People) in around 731. Thus the term for English people (Latin: gens Anglorum; Anglo-Saxon: Anglecynn) was in use by then to distinguish Germanic groups in Britain from those on the continent (Old Saxony in Northern Germany).[1][a] The term 'Anglo-Saxon' came in practice in the 8th century (probably by Paul the Deacon) to distinguish English Saxons from continental Saxons (Ealdseaxe, 'old' Saxons).\\r\\nThe historian James Campbell suggested that it was not until the late Anglo-Saxon period that England could be described as a nation state.[2] It is certain that the concept of \\"Englishness\\" only developed very slowly.[3][4]\\r\\nAs the Roman occupation of Britain was coming to an end, Constantine III withdrew the remains of the army, in reaction to the barbarian invasion of Europe.[5][6] The Romano-British leaders were faced with an increasing security problem from seaborne raids, particularly by Picts on the East coast of England.[7] The expedient adopted by the Romano-British leaders was to enlist the help of Anglo-Saxon mercenaries (known as foederati), to whom they ceded territory.[7][8] In about 442 the Anglo-Saxons mutinied, apparently because they had not been paid.[9] The Romano-British responded by appealing to the Roman commander of the Western empire, A?tius for help (a document known as the Groans of the Britons), even though Honorius, the Western Roman Emperor, had written to the British civitas in or about 410 telling them to look to their own defence.[10][11][12][13] There then followed several years of fighting between the British and the Anglo-Saxons.[14] The fighting continued until around 500, when, at the Battle of Mount Badon, the Britons inflicted a severe defeat on the Anglo-Saxons.[15]\\r\\nThere are records of Germanic infiltration into Britain that date before the collapse of the Roman Empire.[16] It is believed that the earliest Germanic visitors were eight cohorts of Batavians attached to the 14th Legion in the original invasion force under Aulus Plautius in AD?43.[16][17][18] There is a recent hypothesis that some of the native tribes, identified as Britons by the Romans, may have been Germanic-language speakers, but most scholars disagree with this due to a lack of evidence in Roman-period evidence for local languages.[19][20][21]\\r\\nIt was quite common for Rome to swell its legions with foederati recruited from the German homelands.[22] This practice also extended to the army serving in Britain, and graves of these mercenaries, along with their families, can be identified in the Roman cemeteries of the period.[23] The migration continued with the departure of the Roman army, when Anglo-Saxons were recruited to defend Britain; and also during the period of the Anglo-Saxon first rebellion of 442.[24]\\r\\nIf the Anglo-Saxon Chronicle is to be believed, the various Anglo-Saxon kingdoms which eventually merged to become England were founded when small fleets of three or five ships of invaders arrived at various points around the coast of England to fight the Sub-Roman British, and conquered their lands.[25] The language of the migrants, Old English, came over the next few centuries to predominate throughout what is now England, at the expense of British Celtic and British Latin.\\r\\nThe arrival of the Anglo-Saxons into Britain can be seen in the context of a general movement of Germanic peoples around Europe between the years 300 and 700, known as the Migration period (also called the Barbarian Invasions or V?lkerwanderung). In the same period there were migrations of Britons to the Armorican peninsula (Brittany and Normandy in modern-day France): initially around 383 during Roman rule, but also c.?460 and in the 540s and 550s; the 460s migration is thought to be a reaction to the fighting during the Anglo-Saxon mutiny between about 450 to 500, as was the migration to Britonia (modern day Galicia, in northwest Spain) at about the same time.[26] The historian Peter Hunter-Blair expounded what is now regarded as the traditional view of the Anglo-Saxon arrival in Britain.[27] He suggested a mass immigration, fighting and driving the Sub-Roman Britons off their land and into the western extremities of the islands, and into the Breton and Iberian peninsulas.[28] This view was probably influenced by sources such as Bede, where he talks about the Britons being slaughtered or going into \\"perpetual servitude\\".[29] According to H?rke the more modern view is of co-existence between the British and the Anglo-Saxons.[30][31][32] He suggests that several modern archaeologists have now re-assessed the invasion model, they have developed a co-existence model largely based on the Laws of Ine. The laws include several clauses that provide six different wergild levels for the Britons, of which four are below that of freeman.[33] Although it was possible for the Britons to be rich freemen, in Anglo-Saxon society, generally it seems that they had a lower status than that of the Anglo-Saxons.[32][33]\\r\\nDiscussions and analysis still continue on the size of the migration, and whether it was a small elite band of Anglo-Saxons who came in and took over the running of the country, or a mass migration of peoples who overwhelmed the Britons.[34][35][36][37]\\r\\nAccording to Gildas, initial vigorous British resistance was led by a man called Ambrosius Aurelianus,[38] from which time victory fluctuated between the two nations. Gildas records a \\"final\\" victory of the Britons at the Battle of Mount Badon in c.?500, and this might mark a point at which Anglo-Saxon migration was temporarily stemmed.[15] Gildas said that this battle was \\"forty-four years and one month\\" after the arrival of the Saxons, and was also the year of his birth.[15] He said that a time of great prosperity followed.[15] But, despite the lull, the Anglo-Saxons took control of Sussex, Kent, East Anglia and part of Yorkshire; while the West Saxons founded a kingdom in Hampshire under the leadership of Cerdic, around 520.[39] However, it was to be 50 years before the Anglo-Saxons began further major advances.[39] In the intervening years the Britons exhausted themselves with civil war, internal disputes, and general unrest: which was the inspiration behind Gildas's book De Excidio Britanniae (The Ruin of Britain).[40]\\r\\nThe next major campaign against the Britons was in 577, led by Cealin, king of Wessex, whose campaigns succeeded in taking Cirencester, Gloucester and Bath (known as the Battle of Dyrham).[39][41][42] This expansion of Wessex ended abruptly when the Anglo-Saxons started fighting among themselves, and resulted in Cealin eventually having to retreat to his original territory. He was then replaced by Ceol (who was possibly his nephew): Cealin was killed the following year, but the annals do not specify by whom.[43][44] Cirencester subsequently became an Anglo-Saxon kingdom under the overlordship of the Mercians, rather than Wessex.[45]\\r\\nBy 600, a new order was developing, of kingdoms and sub-Kingdoms. Henry of Huntingdon (a medieval historian) conceived the idea of the Heptarchy, which consisted of the seven principal Anglo-Saxon kingdoms (Heptarchy literal translation from the Greek: hept ÿ seven; archy ÿ rule).[46]\\r\\nThe four main kingdoms in Anglo-Saxon England were:\\r\\nMinor kingdoms:\\r\\nAt the end of the 6th century the most powerful ruler in England was ?thelberht of Kent, whose lands extended north to the Humber River.[47] In the early years of the 7th century, Kent and East Anglia were the leading English kingdoms.[48] After the death of ?thelberht in 616, R?dwald of East Anglia became the most powerful leader south of the Humber.[48]\\r\\nFollowing the death of ?thelfrith of Northumbria, R?dwald provided military assistance to the Deiran Edwin in his struggle to take over the two dynasties of Deira and Bernicia in the unified kingdom of Northumbria.[48] Upon the death of R?dwald, Edwin was able to pursue a grand plan to expand Northumbrian power.[48]\\r\\nThe growing strength of Edwin of Northumbria forced the Anglo-Saxon Mercians under Penda into an alliance with the Welsh King Cadwallon ap Cadfan of Gwynedd, and together they invaded Edwin's lands and defeated and killed him at the Battle of Hatfield Chase in 633.[49][50] Their success was short-lived, as Oswald (one of the sons of the late King of Northumbria, ?thelfrith) defeated and killed Cadwallon at Heavenfield near Hexham.[51] In less than a decade Penda again waged war against Northumbria, and killed Oswald in the Battle of Maserfield in 642.[52]\\r\\nHis brother Oswiu was chased to the northern extremes of his kingdom.[52][53] However, Oswiu killed Penda shortly after, and Mercia spent the rest of the 7th and all of the 8th century fighting the kingdom of Powys.[52] The war reached its climax during the reign of Offa of Mercia,[52] who is remembered for the construction of a 150-mile-long dyke which formed the Wales/England border.[54] It is not clear whether this was a boundary line or a defensive position.[54] The ascendency of the Mercians came to an end in 825, when they were soundly beaten under Beornwulf at the Battle of Ellendun by Egbert of Wessex.[55]\\r\\nChristianity had been introduced into the British Isles during the Roman occupation.[56] The early Christian Berber author, Tertullian, writing in the third century, said that \\"Christianity could even be found in Britain.\\"[57] The Roman Emperor Constantine (306ÿ337), granted official tolerance to Christianity with the Edict of Milan in 313.[58] Then, in the reign of Emperor Theodosius \\"the Great\\" (378ÿ395), Christianity was made the official religion of the Roman Empire.[59]\\r\\nIt is not entirely clear how many Britons would have been Christian when the pagan Anglo-Saxons arrived.[60][61] There had been attempts to evangelise the Irish by Pope Celestine I in 431.[62] However, it was Saint Patrick who is credited with converting the Irish en-masse.[62] A Christian Ireland then set about evangelising the rest of the British Isles, and Columba was sent to found a religious community in Iona, off the west coast of Scotland.[63] Then Aidan was sent from Iona to set up his see in Northumbria, at Lindisfarne, between 635ÿ651.[64] Hence Northumbria was converted by the Celtic (Irish) church.[64]\\r\\nBede is very uncomplimentary about the indigenous British clergy: in his Historia ecclesiastica he complains of their unspeakable crimes, and that they did not preach the faith to the Angles or Saxons.[65] Pope Gregory I sent Augustine in 597 to convert the Anglo-Saxons, but Bede says the British clergy refused to help Augustine in his mission.[66][67] Despite Bede's complaints, it is now believed that the Britons played an important role in the conversion of the Anglo-Saxons.[68] On arrival in the south east of England in 597, Augustine was given land by King ?thelberht of Kent to build a church; so in 597 Augustine built the church and founded the See at Canterbury.[69] He baptised ?thelberht in 601, then continued with his mission to convert the English.[70] Most of the north and east of England had already been evangelised by the Irish Church. However, Sussex and the Isle of Wight remained mainly pagan until the arrival of Saint Wilfrid, the exiled Archbishop of York, who converted Sussex around 681 and the Isle of Wight in 683.[71][72][73]\\r\\nIt remains unclear what \\"conversion\\" actually meant. The ecclesiastical writers tended to declare a territory as \\"converted\\" merely because the local king had agreed to be baptised, regardless of whether, in reality, he actually adopted Christian practices; and regardless, too, of whether the general population of his kingdom did.[74] When churches were built, they tended to include pagan as well as Christian symbols, evidencing an attempt to reach out to the pagan Anglo-Saxons, rather than demonstrating that they were already converted.[75][76]\\r\\nEven after Christianity had been set up in all of the Anglo-Saxon kingdoms, there was friction between the followers of the Roman rites and the Irish rites, particularly over the date on which Easter fell and the way monks cut their hair.[77] In 664 a conference was held at Whitby Abbey (known as the Whitby Synod) to decide the matter; Saint Wilfrid was an advocate for the Roman rites and Bishop Colmn for the Irish rites.[78] Wilfrid's argument won the day and Colmn and his party returned to Ireland in their bitter disappointment.[78] The Roman rites were adopted by the English church, although they were not universally accepted by the Irish Church.[78]\\r\\nBetween the 8th and 11th centuries, raiders and colonists from Scandinavia, mainly Danish and Norwegian, plundered western Europe, including the British Isles.[79] These raiders came to be known as the Vikings; the name is believed to derive from Scandinavia, where the Vikings originated.[80][81] The first raids in the British Isles were in the late 8th century, mainly on churches and monasteries (which were seen as centres of wealth).[80][82] The Anglo-Saxon Chronicle reports that the holy island of Lindisfarne was sacked in 793.[83] The raiding then virtually stopped for around forty years; but in about 835 it started becoming more regular.[84]\\r\\nIn the 860s, instead of raids, the Danes mounted a full-scale invasion. In 865 an enlarged army arrived that the Anglo-Saxons described as the Great Heathen Army. This was reinforced in 871 by the Great Summer Army.[84] Within ten years nearly all of the Anglo-Saxon kingdoms fell to the invaders: Northumbria in 867, East Anglia in 869, and nearly all of Mercia in 874ÿ77.[84] Kingdoms, centres of learning, archives, and churches all fell before the onslaught from the invading Danes. Only the Kingdom of Wessex was able to survive.[84] In March 878, the Anglo-Saxon King of Wessex, Alfred, with a few men, built a fortress at Athelney, hidden deep in the marshes of Somerset.[86] He used this as a base from which to harry the Vikings. In May 878 he put together an army formed from the populations of Somerset, Wiltshire, and Hampshire, which defeated the Viking army in the Battle of Edington.[86] The Vikings retreated to their stronghold, and Alfred laid siege to it.[86] Ultimately the Danes capitulated, and their leader Guthrum agreed to withdraw from Wessex and to be baptised. The formal ceremony was completed a few days later at Wedmore.[86][87] There followed a peace treaty between Alfred and Guthrum, which had a variety of provisions, including defining the boundaries of the area to be ruled by the Danes (which became known as the Danelaw) and those of Wessex.[88] The Kingdom of Wessex controlled part of the Midlands and the whole of the South (apart from Cornwall, which was still held by the Britons), while the Danes held East Anglia and the North.[89]\\r\\nAfter the victory at Edington and resultant peace treaty, Alfred set about transforming his Kingdom of Wessex into a society on a full-time war footing.[90] He built a navy, reorganised the army, and set up a system of fortified towns known as burhs. He mainly used old Roman cities for his burhs, as he was able to rebuild and reinforce their existing fortifications.[90] To maintain the burhs, and the standing army, he set up a taxation system known as the Burghal Hidage.[91] These burhs (or burghs) operated as defensive structures. The Vikings were thereafter unable to cross large sections of Wessex: the Anglo-Saxon Chronicle reports that a Danish raiding party was defeated when it tried to attack the burh of Chichester.[92][93] The burhs, although primarily designed as defensive structures, were also commercial centres, attracting traders and markets to a safe haven, and they provided a safe place for the king's moneyers and mints.[94] A new wave of Danish invasions commenced in the year 891,[95] beginning a war that lasted over three years.[96][97] Alfred's new system of defence worked, however, and ultimately it wore the Danes down: they gave up and dispersed in the summer of 896.[97]\\r\\nAlfred is also remembered as a literate king. He or his court commissioned the Anglo-Saxon Chronicle, which was written in Old English (rather than in Latin, which was the language of the European annals).[98] Alfred's own literary output was mainly of translations, though he wrote introductions and amended manuscripts as well.[98][99]\\r\\nOn Alfred's death in 899, his son Edward the Elder succeeded him.[100] Alfred's son Edward, and his grandsons ?thelstan, Edmund I, and Eadred, continued the policy of resistance against the Vikings.[101] In Mercia, from 874ÿ879 the western half was ruled by Ceowulf II, who was succeeded by ?thelred.[102] In 886/887 ?thelred married Alfred's daughter ?thelfl?d.[102] When ?thelred died in 911, his widow administered the Mercian province with the title \\"Lady of the Mercians\\".[102] As commander of the Mercian army she worked with her brother, Edward the Elder, to win back the Mercian lands that were under Danish control.[102] Edward and his successors expanded Alfred's network of fortified burhs, a key element of their strategy, enabling them to go on the offensive.[101][103] Edward recaptured Essex in 913. Edward's son, ?thelstan, annexed Northumbria and forced the kings of Wales to submit; at the Battle of Brunanburh in 937, he defeated an alliance of the Scots, Danes, and Vikings to become King of all England.[101][104]\\r\\nAlong with the Britons and the settled Danes, some of the other Anglo-Saxon kingdoms disliked being ruled by Wessex. Consequently, the death of a Wessex king would be followed by rebellion, particularly in Northumbria.[101] In 973, Alfred's great-grandson, Edgar, was crowned King of England and Emperor of Britain at Bath.[105] On his coinage he had inscribed EADGAR REX ANGLORUM (\\"Edgar, King of the English\\"). Edgar's coronation was a magnificent affair, and many of its rituals and words could still be seen in the coronation of Elizabeth II in 1953, though in English rather than Latin.[106]\\r\\nThe presence of Danish and Norse settlers in the Danelaw had a lasting impact; the people there saw themselves as \\"armies\\" a hundred years after settlement:[107] King Edgar issued a law code in 962 that was to include the people of Northumbria, so he addressed it to Earl Olac \\"and all the army that live in that earldom\\".[107] There are over 3,000 words in modern English that have Scandinavian roots,[108][109] Additionally, more than 1,500 place-names in England are Scandinavian in origin; for example, topographic names such as Howe, Norfolk and Howe, North Yorkshire are derived from the Old Norse word haugr meaning hill, knoll, or mound.[109][110] In archeology and other academic contexts the term \\"Anglo-Scandinavian\\" is often used for Scandinavian culture in England.\\r\\nTwo years after his coronation at Bath, Edgar died while still only in his early thirties.[111] He left two surviving sons, Edward (the eldest) and his half-brother ?thelred.[111] Edward was crowned king, at Kingston, but three years later he was assassinated by one of his half-brother's retainers, with the assistance of ?thelred's stepmother.[111] Hence ?thelred II was crowned in 978, and although he reigned for thirty-eight years, one of the longest reigns in English history, he earned the name \\"?thelred the Unready\\", as he proved to be one of England's most disastrous kings.[112] William of Malmesbury, writing in his Chronicle of the kings of England about one hundred years later, was scathing in his criticism of ?thelred, saying that he occupied the kingdom, rather than governed it.[113]\\r\\nJust as ?thelred was being crowned, the Danish King Gormsson was trying to force Christianity onto his domain.[114] Many of his subjects did not like this idea, and shortly before 988, Swein, his son, drove his father from the kingdom.[114] The rebels, dispossessed at home, probably formed the first waves of raids on the English coast.[114] The rebels did so well in their raiding that the Danish kings decided to take over the campaign themselves.[115]\\r\\nIn 991 the Vikings sacked Ipswich, and their fleet made landfall near Maldon in Essex.[115] The Danes demanded that the English pay a ransom, but the English commander Byrhtnoth refused; he was killed in the ensuing Battle of Maldon, and the English were easily defeated.[115] From then on the Vikings seem to have raided anywhere at will; they were contemptuous of the lack of resistance from the English. Even the Alfredian systems of burhs failed.[116] ?thelred seems to have just hidden, out of range of the raiders.[116]\\r\\nBy the 980s the kings of Wessex had a powerful grip on the coinage of the realm. It is reckoned there were about 300 moneyers, and 60 mints, around the country.[117] Every five or six years the coinage in circulation would cease to be legal tender and new coins were issued.[117] The system controlling the currency around the country was extremely sophisticated; this enabled the king to raise large sums of money if needed.[118][119] The ability to raise large sums of money was needed after the battle of Maldon, as ?thelred decided that, rather than fight, he would pay ransom to the Danes in a system known as Danegeld.[120] As part of the ransom, a peace treaty was drawn up that was intended to stop the raids. However, rather than buying the Vikings off, payment of Danegeld only encouraged them to come back for more.[121]\\r\\nThe Dukes of Normandy were quite happy to allow these Danish adventurers to use their ports for raids on the English coast. The result was that the courts of England and Normandy became increasingly hostile to each other.[114] Eventually, ?thelred sought a treaty with the Normans, and ended up marrying Emma, daughter of Richard I, Duke of Normandy in the Spring of 1002, which was seen as an attempt to break the link between the raiders and Normandy.[116][122]\\r\\nThen, on St.?Brice's day in November 1002, Danes living in England were slaughtered on the orders of ?thelred.[123]\\r\\nIn the summer of 1013, Sven Forkbeard, King of Denmark, brought the Danish fleet to Sandwich, Kent.[124] From there he went north to the Danelaw, where the locals immediately agreed to support him.[124] He then struck south, forcing ?thelred into exile in Normandy (1013ÿ1014). However, on 3 February 1014 Sven died suddenly.[124] Capitalising on his death, ?thelred returned to England and drove Sven's son, Cnut, back to Denmark, forcing him to abandon his allies in the process.[124] In 1015, Cnut launched a new campaign against England.[124] Edmund fell out with his father, ?thelred, and struck out on his own.[125] Some English leaders decided to support Cnut, so ?thelred ultimately retreated to London.[125] Before engagement with the Danish army, ?thelred died and was replaced by Edmund.[125] The Danish army encircled and besieged London, but Edmund was able to escape and raised an army of loyalists.[125] Edmund's army routed the Danes, but the success was short-lived: at the battle of Ashingdon the Danes were victorious and many of the English leaders were killed.[125] Cnut and Edmund agreed to split the kingdom in two, with Edmund ruling Wessex and Cnut the rest.[125][126]\\r\\nIn 1017, Edmund died in mysterious circumstances, probably murdered by Cnut or his supporters, and the English council (the witan) confirmed Cnut as king of all England.[125] Cnut divided England into earldoms: most of these were allocated to nobles of Danish descent, but he made an Englishman earl of Wessex. The man he appointed was Godwin, who eventually became part of the extended royal family when he married the king's sister-in-law.[127] In the summer of 1017, Cnut sent for ?thelred's widow, Emma, with the intention of marrying her.[128] It seems that Emma agreed to marry the king on condition that he would limit the English succession to the children born of their union.[129] Cnut already had a wife known as ?lfgifu of Northampton who bore him two sons, Svein and Harold Harefoot.[129] However it seems that the church regarded ?lfgifu as Cnut's concubine rather than his wife.[129] In addition to the two sons he had with ?lfgifu, he had a further son with Emma, who was named Harthacnut.[129][130]\\r\\nWhen Cnut's brother, Harald?II, King of Denmark, died in 1018 Cnut went to Denmark to secure that realm. Two years later, Cnut brought Norway under his control, and he gave ?lfgifu and their son Svein the job of governing it.[130]\\r\\nOne result of Cnut's marriage to Emma was to precipitate a succession crisis after his death in 1035,[130] as the throne was disputed between ?lfgifu's son, Harald Harefoot, and Emma's son, Harthacnut.[131] Emma supported her son by Cnut, Harthacnut, rather than a son by ?thelred.[132] Her son by ?thelred, Edward, made an unsuccessful raid on Southampton, and his brother Alfred was murdered on an expedition to England in 1036.[132] Emma fled to Bruges when Harald Harefoot became king of England, but when he died in 1040 Harthacnut was able to take over as king.[131] Harthacnut quickly developed a reputation for imposing high taxes on England.[131] He became so unpopular that Edward was invited to return from exile in Normandy to be recognised as Harthacnut's heir,[132][133] and when Harthacnut died suddenly in 1042 (probably murdered), Edward (known to posterity as Edward the Confessor) became king.[132]\\r\\nEdward was supported by Earl Godwin of Wessex and married the earl's daughter. This arrangement was seen as expedient, however, as Godwin had been implicated in the murder of Alfred, the king's brother. In 1051 one of Edward's in-laws, Eustace, arrived to take up residence in Dover; the men of Dover objected and killed some of Eustace's men.[132] When Godwin refused to punish them, the king, who had been unhappy with the Godwins for some time, summoned them to trial. Stigand, the Archbishop of Canterbury, was chosen to deliver the news to Godwin and his family.[134] The Godwins fled rather than face trial.[134] Norman accounts suggest that at this time Edward offered the succession to his cousin, William (duke) of Normandy (also known as William the Conqueror, William the Bastard, or William?I), though this is unlikely given that accession to the Anglo-Saxon kingship was by election, not heredity ÿ a fact which Edward would surely have known, having been elected himself by the Witenagemot.\\r\\nThe Godwins, having previously fled, threatened to invade England. Edward is said to have wanted to fight, but at a Great Council meeting in Westminster, Earl Godwin laid down all his weapons and asked the king to allow him to purge himself of all crimes.[135] The king and Godwin were reconciled,[135] and the Godwins thus became the most powerful family in England after the king.[136][137] On Godwin's death in 1053, his son Harold succeeded to the earldom of Wessex; Harold's brothers Gyrth, Leofwine, and Tostig were given East Anglia, Mercia, and Northumbria.[136] The Northumbrians disliked Tostig for his harsh behaviour, and he was expelled to an exile in Flanders, in the process falling out with his brother Harold, who supported the king's line in backing the Northumbrians.[138][139]\\r\\nOn 26 December 1065, Edward was taken ill[139] He took to his bed and fell into a coma; at one point he woke and turned to Harold Godwinson and asked him to protect the Queen and the kingdom.[140][141] On 5 January 1066 Edward the Confessor died, and Harold was declared king.[139] The following day, 6 January 1066, Edward was buried and Harold crowned.[141][142]\\r\\nAlthough Harold Godwinson had grabbed the crown of England, others laid claim to it, primarily William, Duke of Normandy, who was cousin to Edward the Confessor through his aunt, Emma of Normandy.[143] It is believed that Edward had promised the crown to William.[132] Harold Godwinson had agreed to support William's claim after being imprisoned in Normandy, by Guy of Ponthieu. William had demanded and received Harold's release, then during his stay under William's protection it is claimed, by the Normans, that Harold swore a solemn oath of loyalty to William.[144]\\r\\nHarald Hardrada (\\"The Ruthless\\") of Norway also had a claim on England, through Cnut and his successors.[143] He had, too, a further claim based on a pact between Harthacnut, King of Denmark (Cnut's son) and Magnus, King of Norway.[143]\\r\\nTostig, Harold's estranged brother, was the first to move; according to the medieval historian Orderic Vitalis, he travelled to Normandy to enlist the help of William, Duke of Normandy, later to be known as William the Conqueror.[143][144][145] William was not ready to get involved so Tostig sailed from the Cotentin Peninsula, but because of storms ended up in Norway, where he successfully enlisted the help of Harold Hardrada.[145][146] The Anglo Saxon Chronicle has a different version of the story, having Tostig land in the Isle of Wight in May 1066, then ravaging the English coast, before arriving at Sandwich, Kent.[142][146] At Sandwich Tostig is said to have enlisted and press ganged sailors before sailing north where, after battling some of the northern earls and also visiting Scotland, he eventually joined Hardrada (possibly in Scotland or at the mouth of the river Tyne).[142][146]\\r\\nAccording to the Anglo Saxon Chronicle (Manuscripts D and E) Tostig became Hadrada's vassal, and then with 300 or so longships sailed up the Humber estuary bottling the English fleet in the river Swale and then landed at Riccall on the Ouse on 24 September.[146][147] They marched towards York, where they were confronted, at Fulford Gate, by the English forces that were under the command of the northern earls, Edwin and Morcar; the battle of Fulford Gate followed, on 20 September, which was one of the bloodiest battles of mediaeval times.[148] The English forces were routed, though Edwin and Morcar escaped. The victors entered the city of York, exchanged hostages and were provisioned.[149] Hearing the news whilst in London, Harold Godwinson force-marched a second English army to Tadcaster by the night of the 24th, and after catching Harald Hardrada by surprise, on the morning of the 25 September, Harold achieved a total victory over the Scandinavian horde after a two-day-long engagement at the Battle of Stamford Bridge.[150] Harold gave quarter to the survivors allowing them to leave in 20 ships.[150]\\r\\nHarold would have been celebrating his victory at Stamford Bridge on the night of 26/27 September 1066, while William of Normandy's invasion fleet set sail for England on the morning of 27 September 1066.[151] Harold marched his army back down to the south coast, where he met William's army, at a place now called Battle just outside Hastings.[152] Harold was killed when he fought and lost the Battle of Hastings on 14 October 1066.[153]\\r\\nThe Battle of Hastings virtually destroyed the Godwin dynasty. Harold and his brothers Gyrth and Leofwine were dead on the battlefield, as was their uncle ?lfwig, Abbot of Newminster. Tostig had been killed at Stamford Bridge. Wulfnoth was a hostage of William the Conqueror. The Godwin women who remained were either dead or childless.[154]\\r\\nWilliam marched on London. The city leaders surrendered the kingdom to him, and he was crowned at Westminster Abbey, Edward the Confessor's new church, on Christmas Day 1066.[155] It took William a further ten years to consolidate his kingdom, during which any opposition was suppressed ruthlessly; in a particularly brutal process known as the Harrying of the North, William issued orders to lay waste the north and burn all the cattle, crops and farming equipment and to poison the earth.[156] According to Orderic Vitalis, the Anglo-Norman chronicler, over one hundred thousand people died of starvation.[157] Figures based on the returns for the Domesday Book estimate that the overall population of England in 1086 was about 2.25?million, so the figure of one hundred thousand deaths, due to starvation, would have been a huge proportion (about one in 20) of the population.[158]\\r\\nBy the time of William's death in 1087, those who had been England's Anglo-Saxon rulers were dead, exiled, or had joined the ranks of the peasantry.[159] It was estimated that only about 8 percent of the land was under Anglo-Saxon control.[155] Nearly all the Anglo-Saxon cathedrals and abbeys of any note had been demolished and replaced with Norman-style architecture by 1200.[160]","input":"When did the anglo-saxon kingdoms establish in britain?"},{"output":"The Angel of Christian Charity","context":"Piccadilly Circus is a road junction and public space of London's West End in the City of Westminster, built in 1819 to connect Regent Street with Piccadilly. In this context, a circus, from the Latin word meaning \\"circle\\", is a round open space at a street junction.[1]\\r\\nPiccadilly now links directly to the theatres on Shaftesbury Avenue, as well as the Haymarket, Coventry Street (onwards to Leicester Square) and Glasshouse Street. The Circus is close to major shopping and entertainment areas in the West End. Its status as a major traffic junction has made Piccadilly Circus a busy meeting place and a tourist attraction in its own right. The Circus is particularly known for its video display and neon signs mounted on the corner building on the northern side, as well as the Shaftesbury memorial fountain and statue, which is popularly, though mistakenly, believed to be of Eros. It is surrounded by several notable buildings, including the London Pavilion and Criterion Theatre. Directly underneath the plaza is Piccadilly Circus tube station, part of the London Underground system.\\r\\n\\r\\n\\r\\nPiccadilly Circus connects to Piccadilly, a thoroughfare whose name first appeared in 1626 as Piccadilly Hall, named after a house belonging to one Robert Baker, a tailor famous for selling piccadills, or piccadillies, a term used for various kinds of collars. The street was known as Portugal Street in 1692 in honour of Catherine of Braganza, the queen consort of King Charles II but was known as Piccadilly by 1743. Piccadilly Circus was created in 1819, at the junction with Regent Street, which was then being built under the planning of John Nash on the site of a house and garden belonging to a Lady Hutton. Around 1858 it was briefly known as Regent's Circus.[2] The circus lost its circular form in 1886 with the construction of Shaftesbury Avenue.\\r\\nThe junction has been a very busy traffic interchange since construction, as it lies at the centre of Theatreland and handles exit traffic from Piccadilly, which Charles Dickens, Jr. described in 1879: \\"Piccadilly, the great thoroughfare leading from the Haymarket and Regent-street westward to Hyde Park-corner, is the nearest approach to the Parisian boulevard of which London can boast.\\"\\r\\nThe Piccadilly Circus tube station was opened 10 March 1906, on the Bakerloo line, and on the Piccadilly line in December of that year. In 1928, the station was extensively rebuilt to handle an increase in traffic. The junction's first electric advertisements appeared in 1910, and, from 1923, electric billboards were set up on the fa?ade of the London Pavilion. Traffic lights were first installed on 3 August 1926.\\r\\nDuring World War II many servicemen's clubs in the West End served American soldiers based in Britain. So many prostitutes roamed the area approaching the soldiers that they received the nickname \\"Piccadilly Commandos\\", and both Scotland Yard and the Foreign Office discussed possible damage to Anglo-American relations.[3]\\r\\nAt the start of the 1960s, it was determined that the Circus needed to be redeveloped to allow for greater traffic flow. In 1962, Lord Holford presented a plan which would have created a \\"double-decker\\" Piccadilly Circus; the upper deck would have been an elevated pedestrian concourse linking the buildings around the perimeter of the Circus, with the lower deck being solely for traffic, most of the ground-level pedestrian areas having been removed to allow for greater vehicle flow. This concept was kept alive throughout the rest of the 1960s. A final scheme in 1972 proposed three octagonal towers (the highest 240 feet (73?m) tall) to replace the Trocadero, the Criterion and the \\"Monico\\" buildings.[4] The plans were permanently rejected by Sir Keith Joseph and Ernest Marples; the key reason given was that Holford's scheme only allowed for a 20% increase in traffic, and the Government required 50%.\\r\\nThe Holford plan is referenced in the short-form documentary film \\"Goodbye, Piccadilly\\", produced by the Rank Organisation in 1967 as part of their Look at Life series when it was still seriously expected that Holford's recommendations would be acted upon. Piccadilly Circus has since escaped major redevelopment, apart from extensive ground-level pedestrianisation around its south side in the 1980s.\\r\\nThe Shaftesbury Memorial Fountain in Piccadilly Circus was erected in 1893 to commemorate the philanthropic works of Anthony Ashley Cooper, 7th Earl of Shaftesbury. During the Second World War, the statue atop the Shaftesbury Memorial Fountain was removed and was replaced by advertising hoardings. It was returned in 1948. When the Circus underwent reconstruction work in the late 1980s, the entire fountain was moved from the centre of the junction at the beginning of Shaftesbury Avenue to its present position at the southwestern corner.\\r\\nPiccadilly Circus is surrounded by several major tourist attractions, including the Shaftesbury Memorial, Criterion Theatre, London Pavilion and several major retail stores. Numerous nightclubs, restaurants and bars are located in the area and neighbouring Soho, including the former Chinawhite club.\\r\\nPiccadilly Circus was surrounded by illuminated advertising hoardings on buildings, starting in 1908 with a Perrier sign,[5] but only one building now carries them, the one in the northwestern corner between Shaftesbury Avenue and Glasshouse Street. The site is unnamed (usually referred to as \\"Monico\\" after the Caf Monico, which used to be on the site); its addresses are 44/48 Regent Street, 1/6 Sherwood Street, 17/22 Denman Street and 1/17 Shaftesbury Avenue, and it has been owned by property investor Land Securities Group since the 1970s.\\r\\nThe earliest signs used incandescent light bulbs; these were replaced with neon lights and with moving signs (there was a large Guinness clock at one time). The first Neon sign was for the British meat extract Bovril.[6] From December 1998, digital projectors were used for the Coke sign, the square's first digital billboard,[7] while in the 2000s there was a gradual move to LED displays, which completely replaced neon lamps by 2011. The number of signs has reduced over the years as the rental costs have increased.\\r\\nDuring 2017, the current six advertising screens will be combined into one large ultra-high definition curved Daktronics display, turning the signs off during renovation for the longest time since the 1940s. The current signs were switched off on 16 January 2017, with the new screen expected to take their place in the autumn.[8]\\r\\nAs of 2016, the site has six LED advertising screens above three large retail units facing Piccadilly Circus on the north side, occupied by Boots, Gap and a mix of smaller retail, restaurant and office premises fronting the other streets. A Burger King located under the Samsung advert, which had been a Wimpy Bar until 1989, closed in early 2008 and was converted into a Barclays Bank.\\r\\nOn special occasions the lights are switched off, such as the deaths of Winston Churchill in 1965 and Diana, Princess of Wales in 1997. On 21 June 2007, they were switched off for one hour as part of the Lights Out London campaign.[17]\\r\\nOther companies and brands that have had signs on the site were Bovril, Volkswagen, Max Factor, Wrigley's Spearmint, Skol, Air India and Gold Flake (as Will's Gold Flake Cigarettes).[18]\\r\\nAt the southeastern side of the Circus, moved after World War II from its original position in the centre, stands the Shaftesbury Memorial Fountain, erected in 1892ÿ1893 to commemorate the philanthropic works of Lord Shaftesbury, a Victorian politician, philanthropist and social reformer. The subject of the Memorial is the Greek god Anteros and was given the name The Angel of Christian Charity but is generally mistaken for his brother Eros.[19]\\r\\nThe Criterion Theatre, a Grade II* listed building, stands on the south side of Piccadilly Circus. Apart from the box office area, the entire theatre, with nearly 600 seats, is underground and is reached by descending a tiled stairway. Columns are used to support both the dress circle and the upper circle, restricting the views of many of the seats inside.\\r\\nThe theatre was designed by Thomas Verity and opened as a theatre on 21 March 1874, although original plans were for it to become a concert hall. In 1883, it was forced to close to improve ventilation and to replace gaslights with electric lights and was reopened the following year. The theatre closed in 1989 and was extensively renovated, reopening in October 1992.\\r\\nOn the northeastern side of Piccadilly Circus, on the corner between Shaftesbury Avenue and Coventry Street, is the London Pavilion. The first building bearing the name was built in 1859 and was a music hall. In 1885, Shaftesbury Avenue was built through the former site of the Pavilion, and a new London Pavilion was constructed, which also served as a music hall. In 1924 electric billboards were erected on the side of the building.\\r\\nIn 1934, the building underwent significant structural alteration and was converted into a cinema. In 1986, the building was rebuilt, preserving the 1885 facade, and converted into a shopping arcade. In 2000, the building was connected to the neighbouring Trocadero Centre, and signage on the building was altered in 2003 to read \\"London Trocadero\\". The basement of the building connects with Piccadilly Circus tube station.\\r\\nThe former Swan & Edgar department store on the west side of the circus between Piccadilly and Regent Street was built in 1928ÿ29 to a design by Reginald Blomfield.[20] Since the closure of the department store in the early 1980s, the building has been successively the flagship London store of music chains Tower Records, Virgin Megastore and Zavvi. The current occupier is clothing brand The Sting.\\r\\nLillywhites is a major retailer of sporting goods located on the corner of the circus and Lower Regent Street, next to the Shaftesbury fountain. It moved to its present site in 1925. Lillywhites is popular with tourists, and they regularly offer sale items, including international football jerseys up to 90% off. Nearby Fortnum & Mason is often considered to be part of the Piccadilly Circus shopping area and is known for its expansive food hall.[21]\\r\\nThe Piccadilly Circus station on the London Underground is located directly beneath Piccadilly Circus itself, with entrances at every corner. It is one of the few stations which have no associated buildings above ground and is fully underground. The below ground concourse and subway entrances are Grade II listed.[22]\\r\\nThe station is on the Piccadilly line between Green Park and Leicester Square, and the Bakerloo line between Charing Cross and Oxford Circus.\\r\\nThe Circus' status as a high-profile public space has made it the destination for numerous political demonstrations, including the February 15, 2003 anti-war protest[23] and the \\"Carnival Against Capitalism\\" protest against the 39th G8 summit in 2013.[24]\\r\\nThe phrase it's like Piccadilly Circus is commonly used in the UK to refer to a place or situation which is extremely busy with people. It has been said that a person who stays long enough at Piccadilly Circus will eventually bump into everyone they know. Probably because of this connection, during World War II, \\"Piccadilly Circus\\" was the code name given to the Allies' D-Day invasion fleet's assembly location in the English Channel.[25]\\r\\nPiccadilly Circus has inspired artists and musicians. Piccadilly Circus (1912) is the name and subject of a painting by British artist Charles Ginner, part of the Tate Britain collection. Sculptor Paul McCarthy also has a 320-page two-volume edition of video stills by the name of Piccadilly Circus. Bob Marley mentioned Piccadilly Circus in his song \\"Kinky Reggae\\", on the Catch a Fire album from 1973.\\r\\nL. S. Lowry R.A painting 'Piccadilly Circus, London' (1960), part of Lord Charles Forte's collection for almost three decades,[26] sold for S5,641,250 when auctioned for the first time at Christie's 20th Century British & Irish Art sale on 16 November 2011.[27]\\r\\nBooks\\r\\nArticles and websites","input":"What is the statue in piccadilly circus called?"},{"output":"approximately 30 miles","context":"\\r\\n\\r\\nAmes is a city in central Iowa approximately 30 miles (48?km) north of Des Moines. It is best known as the home of Iowa State University (ISU), with leading Agriculture, Design, Engineering, and Veterinary Medicine colleges. An United States Department of Energy national laboratory, Ames Laboratory, is located on the ISU campus.\\r\\n\\r\\nIn 2017, Ames had a population of 66,498.[6] Iowa State University is home to 36,321 students (Fall 2017),[7] which make up approximately one half of the city's population.\\r\\n\\r\\nAmes also hosts United States Department of Agriculture (USDA) sites: the largest federal animal disease center in the United States, USDA's Agricultural Research Service's National Animal Disease Center (NADC).[8], as well as, one of two national USDA sites for the Animal and Plant Health Inspection Service (APHIS), which comprises the National Veterinary Services Laboratory and the Center for Veterinary Biologics.[9] Ames has the headquarters for the Iowa Department of Transportation.\\r\\n\\r\\nIn 2010, Ames was ranked ninth on CNNMoney \\"Best Places to Live\\" list.[10]\\r\\n\\r\\nThe city was founded in 1864 as a station stop on the Cedar Rapids and Missouri Railroad and was named after 19th century U.S. Congressman Oakes Ames of Massachusetts, who was influential in the building of the transcontinental railroad.[11] Ames was founded by local resident Cynthia Olive Duff (ne Kellogg) and railroad magnate John Insley Blair,[12] near a location that was deemed favorable for a railroad crossing of the Skunk River.\\r\\n\\r\\nAmes is located along the western edge of Story County, Iowa, United States. It is located roughly 30 miles (48?km) north of the state capital Des Moines, near the intersection of Interstate 35 and U.S. Route 30. A smaller highway, U.S. Route 69, runs through the town. Also passing through Ames is the cross country line of the Union Pacific Railroad & two small streams (the South Skunk River and Squaw Creek). \\r\\n\\r\\nAccording to the United States Census Bureau, the city has a total area of 24.27 square miles (62.86?km2), of which 24.21 square miles (62.70?km2) is land and 0.06 square miles (0.16?km2) is water.[1]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nCampustown is the neighborhood directly south of Iowa State University Central Campus bordered by Lincoln Way on the north. Campustown is a high-density mixed-use neighborhood that is home to many student apartments, nightlife venues, restaurants, and numerous other establishments, most of which are unique to Ames.\\r\\nAmes has a humid continental climate (K?ppen climate classification Dfa). On average, the warmest month is July and the coldest is January. The highest recorded temperature was 102?F (39?C) in 1988 and the lowest was ?28?F in 1996.[13]\\r\\n\\r\\nAs of the census[3] of 2010, there were 58,965 people, 22,759 households, and 9,959 families residing in the city. The population density was 2,435.6 inhabitants per square mile (940.4/km2). There were 23,876 housing units at an average density of 986.2 per square mile (380.8/km2). The racial makeup of the city was 84.5% White, 3.4% African American, 0.2% Native American, 8.8% Asian, 1.1% from other races, and 2.0% from two or more races. Hispanic or Latino of any race were 3.4% of the population.\\r\\n\\r\\nThere were 22,759 households of which 19.1% had children under the age of 18 living with them, 35.6% were married couples living together, 5.4% had a female householder with no husband present, 2.7% had a male householder with no wife present, and 56.2% were non-families. 30.5% of all households were made up of individuals and 6.2% had someone living alone who was 65 years of age or older. The average household size was 2.25 and the average family size was 2.82.\\r\\n\\r\\nThe median age in the city was 23.8 years. 13.4% of residents were under the age of 18; 40.5% were between the ages of 18 and 24; 22.9% were from 25 to 44; 15% were from 45 to 64; and 8.1% were 65 years of age or older. The gender makeup of the city was 53.0% male and 47.0% female.\\r\\n\\r\\nAs of the census of 2000,[16] there were 50,731 people, 18,085 households, and 8,970 families residing in the city. The population density was 2,352.3 people per square mile (908.1/km2). There were 18,757 housing units at an average density of 869.7 per square mile (335.7/km2). The racial makeup of the city was 87.34% White, 7.70% Asian, 2.65% African American, 0.04% Native American, 0.76% Pacific Islander and other races, and 1.36% from two or more races. Hispanic or Latino of any race were 1.98% of the population.\\r\\n\\r\\nThere were 18,085 households out of which 22.3% had children under the age of 18 living with them, 42.0% were married couples living together, 5.3% had a female householder with no husband present, and 50.4% were non-families. 28.5% of all households were made up of individuals and 5.9% had someone living alone who was 65 years of age or older. The average household size was 2.30 and the average family size was 2.85.\\r\\n\\r\\nAge spread: 14.6% under the age of 18, 40.0% from 18 to 24, 23.7% from 25 to 44, 13.9% from 45 to 64, and 7.7% who were 65 years of age or older. The median age was 24 years. For every 100 females, there were 109.3 males. For every 100 females age 18 and over, there were 109.9 males.\\r\\n\\r\\nThe median income for a household in the city was $36,042, and the median income for a family was $56,439. Males had a median income of $37,877 versus $28,198 for females. The per capita income for the city was $18,881. About 7.6% of families and 20.4% of the population were below the poverty line, including 9.2% of those under age 18 and 4.1% of those age 65 or over.\\r\\n\\r\\nThe U.S. Census Bureau designates the Ames metropolitan statistical area as encompassing all of Story County. While Ames is the largest city in Story County, the county seat is in the nearby city of Nevada 8 miles (13?km) east of Ames.\\r\\n\\r\\nAmes metropolitan statistical area combined with the Boone, Iowa micropolitan statistical area (Boone County, Iowa) make up the larger Ames-Boone combined statistical area. Ames is the larger principal city of the Combined Statistical Area that includes all of Story County, Iowa and Boone County, Iowa.[17][18][19] which had a combined population of 106,205 at the 2000 census.[16]\\r\\n\\r\\nAmes is home of Iowa State University of Science and Technology, a public land-grant and space-grant research university, and member of the prestigious Association of American Universities. At its founding in 1858, Iowa State was formerly known as the Iowa State College of Agriculture and Mechanic Arts. Ames is the home of the closely allied U.S. Department of Agriculture's National Animal Disease Center (See Ames strain), the U.S. Department of Energy's Ames Laboratory (a major materials research and development facility), and the main offices of the Iowa Department of Transportation. State and Federal institutions are the largest employers in Ames.\\r\\n\\r\\nOther area employers include a 3M manufacturing plant; Danfoss Power Solutions, a hydraulics manufacturer; Barilla, a pasta manufacturer; Ball, a manufacturer of canning jars and plastic bottles; Renewable Energy Group, America's largest producer of biomass-based diesel; and the National Farmers Organization.\\r\\n\\r\\nThe Iowa State University Research Park is a not-for-profit, business development incubator located in Ames, and affiliated with Iowa State University.[20]\\r\\n\\r\\nIn 2015, Ames was ranked in the top 15 \\"Cities That Have Done the Best Since the Recession\\" by Bloomberg Businessweek.[21]\\r\\n\\r\\nThe Bureau of Labor Statistics ranked Ames and Boulder, CO as having the lowest unemployment rate (2.5%) of any metropolitan area in the US in 2016.[22] By June 2018, unemployment in Ames had fallen even further, to 1.5%, and wage increases for workers were not keeping pace with rising rents.[23]\\r\\n\\r\\nAccording to Ames's 2015 Comprehensive Annual Financial Report, the top employers in the city are:\\r\\n\\r\\nVelma Wallace Rayness\\r\\nAmes, Iowa was home to Gerard M. and Velma Wallace Rayness.\\r\\nBoth artists taught art and were nationally recognized artists.\\r\\nTheir art was exhibited nationally as well as abroad.\\r\\nGerard died in the 1940s. Velma Wallace Rayness died in 1977.\\r\\nVelma Wallace Rayness usually signed her paintings \\"V.W. Rayness\\"\\r\\n\\r\\nThe Iowa State University Cyclones play a variety of sports in the Ames area. The Cyclones' football team plays at Jack Trice Stadium near Ames. Also, the Cyclones' Men's and Women's Basketball teams and Volleyball team play at Hilton Coliseum just across the street from Jack Trice Stadium. The Iowa State Cyclones are a charter member of the Big 12 Conference in all sports and compete in NCAA Division I-A.\\r\\n\\r\\nThe Ames Figure Skating Club provides recreational to professional level skating opportunities.  The club sponsors the Learn to Skate Program.  Coaches provide on and off ice lessons or workshops. The club hosts the figure skating portion of the Iowa Games competition every summer.  In the fall the club hosts Cyclone Country Championships.  Every year the club puts on the Winter Gala.  The big event is the annual Spring Ice Show where young to adult skaters can perform their best moves.\\r\\n\\r\\nThe Ames area has a large number of parks and arboretums.\\r\\n\\r\\nSpecialized Parks:\\r\\n\\r\\nCommunity Parks:\\r\\n\\r\\nNeighborhood Parks:\\r\\n\\r\\nAmes High School: Grades 9ÿ12\\r\\n\\r\\nIowa State University of Science and Technology, more commonly known as Iowa State University (ISU), is a public land-grant and space-grant research university located in Ames. Iowa State University is the birthplace of the AtanasoffÿBerry Computer, the world's first electronic digital computer.[30] Iowa State has produced a number of astronauts, scientists, Nobel laureates,[31] Pulitzer Prize winners, and a variety of other notable individuals in their respective fields.[citation needed] Until 1945 it was known as the Iowa State College of Agriculture and Mechanic Arts. The university is a member of the American Association of Universities and the Big 12 Conference.\\r\\n\\r\\nISU is the nation's first designated land-grant university[32] In 1856, the Iowa General Assembly enacted legislation to establish the State Agricultural College and Model Farm. Story County was chosen as the location on June 21, 1859, from proposals by Johnson, Kossuth, Marshall, Polk, and Story counties. When Iowa accepted the provisions of the Morrill Act of 1862, Iowa State became the first institution in nation designated as a land-grant college. The institution was coeducational from the first preparatory class admitted in 1868. The formal admitting of students began the following year, and the first graduating class of 1872 consisted of 24 men and 2 women.[32]\\r\\n\\r\\nThe first building on the Iowa State campus was Farm House. Built in the 1860s, it currently serves as a museum and National Historic Landmark. Today, Iowa State has over 60 notable buildings, including Beardshear Hall, Morrill Hall, Memorial Union, Catt Hall, Curtiss Hall, Carver Hall, Parks Library, the Campanile, Hilton Coliseum, C.Y. Stephens Auditorium, Fisher Theater, Jack Trice Stadium, Lied Recreation Center, numerous residence halls, and many buildings specific to ISU's many different majors and colleges. \\r\\n\\r\\nThe official mascot for ISU is Cy the Cardinal. The official school colors are cardinal and gold. The Iowa State Cyclones play in the NCAA's Division I-A as a member of the Big 12 Conference.\\r\\n\\r\\nAmes is also served by stations in the Des Moines media market, which includes Clear Channel's 50,000-watt talk station WHO, music stations KAZR, KDRB, KGGO, KKDM, KDXA, KHKI, KIOA, KJJY, KRNT, KSPZ and KSTZ, talk station KWQW, and sports station KXNO, \\r\\n\\r\\nLike radio, Ames is served by the Des Moines media market. WOI-DT, the ABC affiliate in central Iowa, was originally owned and operated by Iowa State University until the 1990s. The station is still licensed to Ames, but studio's are located in West Des Moines. Other stations serving Ames include KCCI, KDIN-TV, WHO-DT, KCWI-TV, KDMI, KDSM-TV and KFPX.\\r\\n\\r\\nThe town is served by U.S. Highways 30 and 69 and Interstate 35. Ames is the only town in Iowa with a population of greater than 50,000 that does not have a state highway serving it.  As of  2015[update], Ames does not have any roundabouts, though a project to create three roundabouts is planned.[33]\\r\\n\\r\\nAmes was serviced by the Fort Dodge, Des Moines and Southern Railroad via a branch from Kelley to Iowa State University and to downtown Ames. The tracks were removed in the 1960s. The Chicago and North Western Transportation Company twin mainline runs east and west bisecting the town and running just south of the downtown business district. The C&NW used to operate a branch to Des Moines. This line was removed in the 1980s when the Spine Line through Nevada was purchased from the Rock Island Railroad after its bankruptcy. The Union Pacific, successor to the C&NW, still runs 60ÿ70 trains a day through Ames on twin mainlines, which leads to some traffic delays. There is also a branch to Eagle Grove that leaves Ames to the north. The Union Pacific maintains a small yard called Ames Yard east of Ames between Ames and Nevada. Ames has been testing automatic train horns at several of its crossings. These directional horns which are focused down the streets are activated when the crossing signals turn on and are shut off after the train crosses the crossing. This system cancels out the need for the trains to blow their horns. Train noise had been a problem in the residential areas to the west and northwest of downtown.\\r\\n\\r\\nAmes Municipal Airport is located 1-mile (1.6?km) southeast of the city. The current (and only) FBO is Hap's Air Service, a company which has been based at the airport since 1975. The airport has two runways ÿ 01/19, which is 5,700 by 100 feet (1,737?m G?30?m), and 13/31, which is 3,492 by 100 feet (1,064?m G?30?m).\\r\\n\\r\\nThe City of Ames offers a transit system throughout town, called CyRide, that is funded jointly by Iowa State University, the ISU Government of the Student Body, and the City of Ames. Rider fares are subsidized through this funding, and are free for children under five. Students pay a set cost as part of their tuition.\\r\\n\\r\\nIn 2009, the Ames metropolitan statistical area (MSA) ranked as the third highest in the United States for percentage of commuters who walked to work (10.4 percent).[34]\\r\\n\\r\\nAmes has the headquarters of the Iowa Department of Transportation.[35]\\r\\n\\r\\nAmes is served by Mary Greeley Medical Center, a 220-bed regional referral hospital which is adjacent to McFarland Clinic PC, central Iowa's largest physician-owned multi-specialty clinic, and also Iowa Heart Center.\\r\\n\\r\\nThis is a list of notable people associated with Ames, Iowa arranged by career and in alphabetical order.\\r\\n\\r\\nIowa is a political \\"battleground state\\" that has trended slightly Democratic in recent years, and Ames, like Iowa City, also trends Democratic. Because Iowa is the first caucus state and Ames is a college town, it is the site of many political appearances, debates and events, especially during election years.\\r\\n\\r\\nFrom 1979 through 2011, Ames was the location of the Ames Straw Poll, which was held every August prior to a presidential election year in which the Republican presidential nomination was undecided (meaning there was no Republican president running for re-electionas in 2011, 2007, 1999, 1995, 1987, and 1979). The poll would gauge support for the various Republican candidates amongst attendees of a fundraising dinner benefiting the Iowa Republican Party. The straw poll was frequently seen by national media and party insiders as a first test of organizational strength in Iowa.[40] In 2015, the straw poll was to be moved to nearby Boone before the Iowa Republican Party eventually decided to cancel it altogether.[41]","input":"How far is ames iowa from des moines iowa?"},{"output":"14 seasons","context":"Bonanza is an NBC television western series that ran from 1959 to 1973. Lasting 14 seasons and 431 episodes, Bonanza is NBC's longest-running western, and ranks overall as the second-longest-running western series on U.S. network television (behind CBS's Gunsmoke), and within the top 10 longest-running, live-action American series. The show continues to air in syndication. The show is set in the 1860s and it centers on the wealthy Cartwright family that live in the vicinity of Virginia City, Nevada, bordering Lake Tahoe. The series initially starred Lorne Greene, Pernell Roberts, Dan Blocker, and Michael Landon and later featured (at various times) Guy Williams, David Canary, Mitch Vogel, and Tim Matheson. The show is known for presenting pressing moral dilemmas.[4]\\r\\nThe title \\"Bonanza\\" is a term used by miners in regard to a large vein or deposit of silver ore,[5] from Spanish bonanza (prosperity) and commonly refers to the 1859 revelation of the Comstock Lode of rich silver ore mines under the town of Virginia City, not far from the fictional Ponderosa Ranch that the Cartwright family operated. The show's theme song, also titled \\"Bonanza,\\" became a hit song in its own right. Only instrumental renditions, absent Ray Evans' lyrics, were ever used during the series's long run.[6]\\r\\nIn 2002, Bonanza was ranked No. 43 on TV Guide's 50 Greatest TV Shows of All Time,[7] and in 2013 TV Guide included it in its list of The 60 Greatest Dramas of All Time.[8] The time period for the television series is roughly between 1861 (Season 1) and 1867 (Season 13) during and shortly after the American Civil War.\\r\\nDuring the summer of 1972, NBC aired reruns of episodes from the 1967ÿ1970 period in prime time on Tuesday evening under the title Ponderosa.[9]\\r\\n\\r\\n\\r\\nThe show chronicles the weekly adventures of the Cartwright family, headed by the thrice-widowed patriarch Ben Cartwright (Lorne Greene). He had three sons, each by a different wife: the eldest was the urbane architect Adam Cartwright (Pernell Roberts) who built the ranch house; the second was the warm and lovable giant Eric \\"Hoss\\" Cartwright (Dan Blocker); and the youngest was the hotheaded and impetuous Joseph or \\"Little Joe\\" (Michael Landon). Via exposition (S01:E01 ÿ \\"Rose for Lotta\\") and flashback episodes, each wife was accorded a different ancestry: English (S02:E65 ÿ \\"Elizabeth My Love\\"), Swedish (S03:E95 ÿ \\"Inger My Love\\") and French Creole (S04:E120 ÿ \\"Marie My Love\\") respectively. The family's cook was the Chinese immigrant Hop Sing (Victor Sen Yung). Greene, Roberts, Blocker, and Landon were billed equally. The opening credits would alternate the order among the four stars.\\r\\nThe family lived on a thousand square-mile (2,600 km2) ranch called the Ponderosa on the eastern shore of Lake Tahoe in Nevada opposite California on the edge of the Sierra Nevada range of the western Rocky Mountains.[10] The vast size of the Cartwrights' land was quietly revised to \\"half a million acres\\" (2,000 km2) on Lorne Greene's 1964 song, \\"Saga of the Ponderosa.\\" The ranch name refers to the Ponderosa Pine, common in the West. The nearest town to the Ponderosa was Virginia City, where the Cartwrights would go to converse with Sheriff Roy Coffee (played by veteran actor Ray Teal), or his deputy Clem Foster (Bing Russell).\\r\\nBonanza was considered an atypical western for its time, as the core of the storylines dealt less about the range but more with Ben and his three dissimilar sons, how they cared for one another, their neighbors, and just causes. \\"You always saw stories about family on comedies or on an anthology, but Bonanza was the first series that was week-to-week about a family and the troubles it went through. Bonanza was a period drama that attempted to confront contemporary social issues. That was very difficult to do on television. Most shows that tried to do it failed because the sponsors didn't like it, and the networks were nervous about getting letters\\", explains Stephen Battaglio, a senior editor for TV Guide magazine.[11]\\r\\nEpisodes ranged from high drama (\\"Bushwhacked\\", episode #392, 1971; \\"Shanklin\\", episode #409, 1972), to broad comedy (\\"Hoss and the Leprechauns\\", episode #146, 1964; \\"Mrs. Wharton and the Lesser Breeds\\", episode #318, 1969; \\"Caution, Bunny Crossing\\", episode #358, 1969), and addressed issues such as the environment (\\"Different Pines, Same Wind\\", episode #304, 1968), substance abuse (\\"The Hidden Enemy\\", episode #424, 1972), domestic violence (\\"First Love\\", episode #427, 1972), anti-war sentiment (\\"The Weary Willies\\", episode #364, 1970), and illegitimate births (\\"Love Child\\", episode #370, 1970; \\"Rock-A-Bye Hoss\\", episode #393, 1971). The series sought to illustrate the cruelty of bigotry against: Asians (\\"The Fear Merchants\\", episode #27, 1960; \\"The Lonely Man\\", episode #404, 1971), African-Americans (\\"Enter Thomas Bowers\\", episode #164, 1964; \\"The Wish\\", episode #326, 1968; \\"Child\\", episode #305, 1969), Native Americans (\\"The Underdog\\", episode #180, 1964; \\"Terror at 2:00\\", episode #384, 1970), Jews, (\\"Look to the Stars\\", episode #90, 1962); Mormons (\\"The Pursued\\", episodes #239-40, 1966), the disabled (\\"Tommy\\", episode #249, 1966) and \\"little people\\" (\\"It's A Small World\\", episode #347, 1968).\\r\\nOriginally, the Cartwrights tended to be depicted as put-off by outsiders. Lorne Greene objected to this, pointing out that as the area's largest timber and livestock producer, the family should be less clannish. The producers agreed with this observation and changed the Cartwrights to be more amiable.\\r\\nThough not familiar stars in 1959, the cast quickly became favorites of the first television generation. The order of billing at the beginning of the broadcast appeared to be shuffled randomly each week, with no relation whatsoever to the current episode featured that week. The main cast of actors portraying Cartwrights is listed here in the order of their characters' ages, followed by an array of recurring supporting players:\\r\\nBorn in Ottawa, Ontario, Canada, to Russian-Jewish parents,[12][13] Lorne Greene was chosen to play widowed patriarch Ben Cartwright. Early in the show's history, he recalls each of his late wives in flashback episodes. A standard practice with most westerns was to introduce some romance but avoid matrimony. Few media cowboys had on-screen wives. Any time one of the Cartwrights seriously courted a woman, she died from a malady, was abruptly slain, or left with someone else.\\r\\nGreene appeared in all but fourteen Bonanza episodes. Greene was 44 years old at the beginning of the series while Pernell Roberts and Dan Blocker, who portrayed two of his sons, were both 31, only thirteen years younger.\\r\\nIn 2007, a TV Guide survey listed Ben Cartwright as television's #2 favorite dad.[14]\\r\\nPernell Roberts played eldest son Adam, an architectural engineer with a university education. Adam built the impressive ranch house.[15][16] Roberts disdained the assembly-line mindset of serial television[citation needed] (a rigid 34 episode season), and fought with series writers regarding Adam's lack of independence, noting that his 30-plus year old character was dependent on his \\"Pa's\\" approval. Despite the show's success, Roberts departed the series after the 1964ÿ65 season (202 episodes) and returned to stage productions.\\r\\nAttempts to replace Adam with Little Joe's maternal half-brother Clay (Barry Coe) and Cartwright cousin Will (Guy \\"Zorro\\" Williams), were unsuccessful.[17] Creator David Dortort introduced a storyline that would keep the character of Adam in the mix, but with a lighter schedule. During season five Adam falls for a widow with a young daughter, while making Will Cartwright a central figure. Roberts decided to stay an additional season, so the scripts were quickly revised by having Adam's fiance and her daughter depart the series prematurely with Guy Williams' Will, with whom she'd fallen in love. It was Landon, not Roberts, who objected to the infusion of any new Cartwrights.[13][17] After Roberts did leave the following year, it was eventually mentioned that Adam had gone \\"to sea\\", and in the later movies he had emigrated to Australia. In mid 1972, the series producers considered inviting Roberts back in the wake of Dan Blocker's death: \\"One suggestion was to return Pernell Roberts, who had played another Cartwright son when Bonanza first premiered on NBC fourteen years ago. We only considered that briefly, [producer Richard] Collins says... Some people felt it was a logical stepthe oldest son returning at a time of family needbut most of us didn't think it would work.'\\"[18]\\r\\nDan Blocker was 6-foot-4, 320-pounds[19] when chosen to play the gentle middle son Eric, better known as \\"Hoss\\". The nickname was used as a nod to the character's ample girth,[20] an endearing term for \\"big and friendly\\", used by his Swedish mother (and Uncle Gunnar).[21] In the Bonanza flashback,[22] his mother Inger names him Eric after her father. To satisfy young Adam, Inger and Ben agree to try the nickname Hoss and \\"see which one sticks.\\" Inger says of \\"Hoss\\", \\"In the mountain country, that is the name for a big, friendly man.\\" According to a biography,[13] the show's crew found Blocker to be the \\"least actor-ish as well as the most likeable\\" cast member. According to producer David Dortort: \\"Over the years he gave me the least amount of trouble.\\"[13]\\r\\nIn May 1972, Blocker died suddenly from a post-operative pulmonary embolism following surgery to remove his gall bladder. The producers felt nobody else could continue the role. It was the first time a TV show's producers chose to kill off a young major male character (though it was done twice previously with young female leadsin 1956 on Make Room For Daddy, and again in 1963 with The Real McCoys). Not until the TV movie Bonanza: The Next Generation was it explained that Hoss had drowned attempting to save a woman's life.[citation needed]\\r\\nAlthough \\"big and lovable\\", Blocker was also tough. Several years after his death, Landon appeared on The Tonight Show Starring Johnny Carson and related the following anecdote. During the shooting of one episode, Blocker's horse stumbled and fell, throwing Blocker and breaking his collarbone. Blocker got up and the bone was actually protruding from his skin. The crew wanted to call an ambulance but Blocker refused and stuck the bone back in place himself and resumed filming. At the end of the day he was convinced to go to the hospital where they set the broken bone and gave him strict instructions, no riding for six weeks. According to Landon, evidently Blocker's horse forgot what it was like to carry the big man during his convalescence because the first time that Blocker swung up into the saddle on his return, the horse collapsed under his weight and the cast and crew collapsed in fits of laughter.[citation needed]\\r\\nThe role of \\"Little Joe\\" was given to Michael Landon, who had earlier played the title role in I Was a Teenage Werewolf. He portrayed the youngest Cartwright son, whose mother (Felicia in the pilot, and later changed to Marie) was of French Creole descent. Landon began to develop his skills in writing and directing Bonanza episodes, starting with \\"The Gamble.\\" Most of the episodes Landon wrote and directed were dramas, including the two-hour, \\"Forever\\" (1972), which was recognized by TV Guide as being one of television's best specials (November 1993).[vague] Landon's development was a bit stormy according to David Dortort, who felt that the actor grew more difficult during the last five seasons the show ran.[23] Landon appeared in all but fourteen Bonanza episodes for its run, a total of 416 episodes.\\r\\nBeginning in 1962, a foundation was being laid to include another \\"son\\", as Pernell Roberts was displeased with his character. In the episode \\"First Born\\" (1962), viewers learn of Little Joe's older, maternal half-brother Clay Stafford. The character departed in that same episode, but left an opportunity for a return if needed. This character's paternity is open to debate. In the 1963 flashback episode \\"Marie, My Love\\", his father was Jean De'Marigny. Then in 1964, Lorne Greene released the song \\"Saga of the Ponderosa\\",[23] wherein Marie's previous husband was \\"Big Joe\\" Collins, who dies saving Ben's life. After Ben consoles Marie, the two bond and marry. They choose to honor \\"Big Joe\\" by calling their son \\"Little Joe\\". So, whether to Stafford, De'Marigny or Collins, Marie Cartwright was previously married. In the last of the three Bonanza TV movies, it is revealed that \\"Little Joe\\" had died in the SpanishÿAmerican War?ÿ a member of the \\"Rough Riders\\".\\r\\nVeteran character actor Ray Teal essayed the role of Sheriff Roy Coffee on 98 episodes from 1960 to 1972.[24] He appeared in more than 250 movies and some 90 television programs during his 37-year career. His longest-running role was as Sheriff Roy Coffee. He had also played a sheriff in the Billy Wilder film Ace in the Hole (1951). Teal co-starred in numerous TV westerns throughout his career: he appeared five times on Cheyenne, twice on The Lone Ranger, on The Alaskans, a short-lived series starring Roger Moore, three times in different roles on another long-running western series, Wagon Train, on NBC's Tales of Wells Fargo with Dale Robertson, on the ABC western series Broken Arrow, five times on the ABC western comedy Maverick starring James Garner and Jack Kelly, on the CBS western series The Texan with Rory Calhoun, the NBC western series The Californians, twice on Colt .45 with Wayde Preston, once on Wanted: Dead or Alive with Steve McQueen and as \\"Sheriff Clay\\" for a single 1960 episode of the NBC western series Riverboat with Darren McGavin, and four times on a western series about the rodeo entitled Wide Country.\\r\\nTeal was a bit-part player in western films for several years before landing a substantial role in Northwest Passage (1940) starring Spencer Tracy. Another of his roles was as Little John in The Bandit of Sherwood Forest (1946). Notable film roles include playing one of the judges in Judgment at Nuremberg (1961) with Spencer Tracy, and an indulgent bar owner to Marlon Brando's motorcycle gang in The Wild One (1953), which was the second of three times that Teal appeared with Brando, having done so already as a drunk in Brando's debut in The Men (1950) and later in Brando's only directorial effort, One-Eyed Jacks (1961), as a bartender.\\r\\nSheriff Coffee was occasionally the focus of a plot as in the episode \\"No Less a Man\\" (broadcast March 15, 1964). A gang of thieves has been terrorizing towns around Virginia City and the town council wants to replace Coffee, whom they consider over-the-hill, with a younger sheriff before the gang hits town, not realizing that they'd been spared earlier because the gang's leader was wary of Coffee's longevity and only acquiesced to rob the Virginia City bank after extreme pressure from other gang members. Coffee ends up showing the town that youth and a fast gun don't replace experience.\\r\\nGuy Williams was slated in 1964, the year that Bonanza hit #1 in the ratings, to replace Pernell Roberts upon Roberts' departure, enabling the series to preserve the four-Cartwright format for the run of the series. His character, Ben's nephew Will Cartwright, was introduced and was the lead character in five episodes, receiving \\"Starring\\" billing after the four original rotating Cartwrights during his second appearance going forward, but Roberts changed his mind later and decided to stay for one more season, whereupon Williams found himself pushed out of the part; it was rumored that Landon and Greene felt threatened by the studio initiating a precedent of successfully replacing one heroic leading man Cartwright with a new one, particularly in view of Williams' popularity with viewers. Williams had previously portrayed the titular character in Walt Disney's Zorro television series, and went on to play the lead in Lost in Space, a science fiction television series, after the role in Bonanza ended.\\r\\nAfter graduating from the University of Cincinnati, David Canary was offered a left-end position with the Denver Broncos,[13] but pursued acting and singing. In 1967, he joined the cast as \\"Candy\\" Canaday, a plucky Army brat turned cowboy,[25] who became the Cartwrights' confidant, ranch foreman, and timber vessel captain. Dortort was impressed by Canary's talent, but the character vanished in September 1970, after Canary had a contract dispute. He returned two seasons later after co-star Dan Blocker's death, reportedly having been approached by Landon. Canary played the character on a total of 91 episodes.[24] Canary joined the cast in Season 9.\\r\\nChinese American actor Victor Sen Yung played the Cartwrights' happy-go-lucky cook, whose blood pressure rose when the family came late for dinner. Cast here as the faithful domestic, the comedy relief character had little to do beyond chores. He once used martial arts to assail a towering family foe.[26] Though often referenced, Hop Sing only appeared in an average of eight to nine shows each season. As a semi-regular cast member, Sen Yung was only paid per episode. After 14 years, he was widely known, but making far less than his Ponderosa peers. The Hop Sing character was central in only two episodes: \\"Mark Of Guilt\\" (#316) and \\"The Lonely Man\\" (#404).\\r\\nAfter Canary's departure in mid-1970, and aware of the show's aging demographic, the writers sought a fresh outlet for Ben's fatherly advice. Fourteen-year-old Mitch Vogel was introduced as Jamie Hunter in \\"A Matter of Faith\\" (season 12, episode 363). Vogel played the red-haired orphan of a roving rainmaker, whom Ben takes in and adopts later in a 1971 episode, called \\"A Home For Jamie.\\"\\r\\nDuring the final season, in 1972ÿ1973, Tim Matheson portrayed Griff King, a parolee who tries to reform his life as a worker at the Ponderosa Ranch under Ben Cartwright's tutelage.\\r\\nFollowing Canary's departure, Frizzell's character accompanied Jamie Hunter to the Ponderosa and became the Cartwright's foreman.\\r\\nLove story's\\r\\nInitially, Bonanza aired on Saturday evenings opposite \\"Dick Clark's Saturday Night Beech-Nut Show\\" and \\"John Gunther's High Road\\" on ABC, and Perry Mason on CBS. Bonanza's initial ratings were respectable, often coming in behind Mason but ahead of the ABC line-up. Ironically, executives considered canceling the show before its premiere because of its high cost. NBC kept it because Bonanza was one of the first series to be filmed and broadcast in color, including scenes of picturesque Lake Tahoe, Nevada, although most of the episodes were obviously filmed on soundstages until the seasons near the end of the series. NBC's corporate parent, Radio Corporation of America (RCA), used the show to spur sales of RCA-manufactured color television sets (RCA was also the primary sponsor of the series during its first two seasons).\\r\\nNBC moved Bonanza to Sundays at 9:00?pm Eastern with new sponsor Chevrolet (replacing The Dinah Shore Chevy Show). The new time slot caused Bonanza to soar in the ratings, and it eventually reached number one by 1964, an honor it would keep until 1967 when it was seriously challenged by the socially daring variety show, The Smothers Brothers Comedy Hour on CBS. By 1970, Bonanza was the first series to appear in the Top Five list for nine consecutive seasons (a record that would stand for many years) and thus established itself as the most consistent strong-performing hit television series of the 1960s. Bonanza remained high on the Nielsen ratings until 1971, when it finally fell out of the Top Ten.\\r\\nDuring the summer of 1972, NBC broadcast reruns of episodes of the show from the 1967ÿ1970 era on Tuesday evenings at 7:30?p.m. under the title Ponderosa while also rerunning more recent episodes on Sunday evenings in the show's normal time slot as Bonanza.[9] In the fall of 1972, off-network episodes were released in broadcast syndication to local stations by NBC under the Ponderosa name. After the series was canceled, the syndicated reruns reverted to the Bonanza name.\\r\\nFrom the fourth season on, the Cartwrights and nearly every other recurring character on the show wore the same clothing in almost every episode. The reason for this is twofold: it made duplication of wardrobe easier for stunt doubles (Hal Burton, Bob Miles, Bill Clark, Lyle Heisler, Ray Mazy) and it cut the cost of refilming action shots (such as riding clips in-between scenes), as previously shot stock footage could be reused. Below is a survey of costumes employed:\\r\\nIt was not unusual for Little Joe Cartwright and Candy Canaday to appear shirtless in various scenes involving manual labor.\\r\\nIn 1968, Blocker began wearing a toupee on the series, as he was approaching age 40 and his hair loss was becoming more evident. He joined the ranks of his fellow co-stars Roberts and Greene, both of whom had begun the series with hairpieces (Greene wore his modest frontal piece in private life too, whereas Roberts preferred not wearing his, even to rehearsals/blocking). Landon was the only original cast member who was wig-free throughout the series, as even Sen Yung wore an attached queue.[28]\\r\\nBonanza features a memorable theme song by Jay Livingston and Ray Evans that was orchestrated by David Rose and arranged by Billy May for the television series. Members of the Western Writers of America chose it as one of the Top 100 Western songs of all time.[29]\\r\\nThe Bonanza theme song famously opens with a blazing Ponderosa map and saddlebound Cartwrights. The melodic intro, emulating galloping horses, is one of the most recognized television scores. Variations of the theme were used for 12 seasons on the series. Although there were two official sets of lyrics (some country-western singers, avoiding royalties, substituted the copyright renditions with their own words), the series simply used an instrumental theme. Three of the cast members bellowed-out the original lyrics, unaccompanied, at the close of the pilot (Pernell Roberts, the sole professional singer of the quartet, abstained and untethered the horse reins). Before the pilot aired (on September 12, 1959), the song sequence, deemed too campy, was edited out of the scene and instead the Cartwrights headed back to the ranch whooping and howling. In a 1964 song, the Livingston-Evans lyrics were revised by Lorne Greene with a more familial emphasis, \\"on this land we put our brand, Cartwright is the name, fortune smiled the day we filed the Ponderosa claim\\" (\\"Bonanza\\", Bear Family Box set, Disc #2). In 1968, a slightly revamped horn and percussion-heavy arrangement of the original score introduced the series- which was used until 1970. A new theme song, called \\"The Big Bonanza\\" was written in 1970 by episode scorer David Rose, and was used from 1970ÿ1972. Action-shot pictorials of the cast replaced the galloping trio. Finally, a faster rendition of the original music returned for the 14th and final season, along with action shots of the cast.\\r\\nThe theme song has been recorded by numerous artists in a diverse variety of styles. The first recorded and released version was an instrumental by Marty Gold, on his 1960 album Swingin' West. This was followed by the February 1960 single by Buddy Morrow and his Orchestra, which included vocals. Morrow's version also appeared on his 1960 album Double Impact which featured several other then-recent television themes. In December 1960, another vocal version was issued only in the United Kingdom by Johnny Gregory (bandleader) and his Orchestra and Chorus released on the Fontana label. All aforementioned vocal versions, including the television pilot, used lyrics written by Livingston and Evans contained in the first published sheet music for the song, though not all the lyrics were sung. A Bonanza soundtrack album released in late 1961 included a version by David Rose; Rose also had a 1960 single and included the theme on his 1961 album Exodus in a different mix. The biggest hit version is a guitar instrumental by Al Caiola, which reached number 19 on Billboard in 1961. Other versions were released by Billy Vaughn, Valjean, Lorne Greene, Johnny Cash and Nelson Riddle.\\r\\nCountry singer Johnny Cash was first to record a full length vocal version of the theme song. He and Johnny Western discarded the original Livingston and Evans lyrics, and wrote new ones, though the revised lyrics still make direct reference to the Cartwrights and the Ponderosa. The song first saw release by September 1962 as a single. Sometime after June 1963, it was released as a track on his sixteenth album: Ring of Fire: The Best of Johnny Cash. This version was later covered by Faron Young for his 1963 album Aims at the West. Singer Ralf Paulsen recorded a German-language version of the song in 1963, released in mid-June 1963 on Capitol Records in the United States. His German version (lyrics attributed to \\"Nicolas\\") was sung in the same style and mood in which Cash had recorded it, and was fairly close in translation.\\r\\nCarlos Malcolm & His Afro-Jamaican Rhythms released a ska version of the song as \\"Bonanza Ska\\" on Trojan Records in 1964. This version was later covered by Bad Manners (1989) and the Hurtin' Buckaroos (1997). Michael Richards, as Stanley Spadowski, sang a bit of the theme song while being held hostage by Channel 8's news goons in UHF (he did not know the words to the song he was originally supposed to sing, \\"Helter Skelter\\"). Michael Feinstein was the last to record the song in 2002 on his Songs of Evans and Livingston tribute CD. The Little House on the Prairie theme (also by Rose), was heard first in a 1971 episode of Bonanza. The overture for The High Chaparral composed by Harry Sukman can be heard briefly at the start of the 1966 episode \\"Four Sisters from Boston\\". On January 29, 2011, Marty Stuart and the Fabulous Superlatives performed the song on episode 56 of The Marty Stuart Show. The band often includes the song in their live shows.[30]\\r\\nThe opening scene for the first season was shot at Lake Hemet, a reservoir in the San Jacinto Mountains, Riverside County, California, and later moved to Lake Tahoe. During the first season extra horses were rented from the Idyllwild Stables in Idyllwild, also in the San Jacinto Mountains. The first Virginia City set was used on the show until 1970 and was located on a backlot at Paramount and featured in episodes of Have Gun ÿ Will Travel, Mannix, and The Brady Bunch. In the 1970 premiere episode of the 12th season entitled \\"The Night Virginia City Died\\", Deputy Clem Foster's pyromaniac fiance levels the town in a series of fires (reflecting a real 1875 fire that destroyed three-quarters of Virginia City). This allowed for a switch to the less expensive Warner studios from September 1970 through January 1973. The script was initially written for the departing David Canary's Candy, but was rewritten for actors Ray Teal (Sheriff Roy Coffee) and Bing Russell (Deputy Clem Foster), who rarely appeared together on the show.\\r\\nThe program's Nevada set, the Ponderosa Ranch house, was recreated in Incline Village, Nevada, in 1967, and remained a tourist attraction until its sale thirty-seven years later in September 2004.\\r\\nBonanza is uniquely known for having addressed racism, not typically covered on American television during the time period, from a compassionate, humanitarian point-of-view.\\r\\nBigotry, and specifically anti-semitism,[31] was the subject of the episode \\"Look to the Stars\\" (Season 3, Episode 26; original air date March 18, 1962). A bigoted school teacher Mr. Norton (oblivious of his prejudice) routinely expels minority students. When he expels the brilliant Jewish student Albert Michelson, a scientific genius whose experiments on the streets of Virginia City often cause commotion, Ben Cartwright steps in and confronts Norton on his bigotry. Ashamed, the school teacher vows to reform.[32] A coda to the episode reveals that Michaelson went on to win the Nobel Prize for Physics.\\r\\nIn the episode \\"Enter Thomas Bowers\\" (Season 5, Episode 30; original air date April 26, 1964), the Cartwright family helps the opera singer Bowers, an African American freedman, after he encounters prejudice while in Virginia City to perform. Bowers winds up arrested as a fugitive slave. At the beginning of the episode, Adam is shown to be outraged at the Supreme Court's Dred Scott v. Sandford decision (placing the time as 1857), which he discusses with his father. According to David Dortort, sponsor General Motors was anxious about the episode. As producer, Dortort ensured that the episode re-aired during the summer rerun seasons, though two TV stations in the South refused to air it.[33]\\r\\nIn the episode \\"The Wish,\\" directed by Michael Landon, Hoss protects an African American former slave's family when confronted with racism after the American Civil War. In \\"The Fear Merchants,\\" discrimination against Chinese immigrants who attempt to assimilate in American society is addressed.[34][35] \\"The Lonely Man\\" presents the controversial interracial marriage between the Cartwrights longtime Chinese chef (Hop Sing) and a white woman (Missy).\\r\\nBonanza has had a highly profitable merchandising history. Currently, Bonanza Ventures, Inc. grants merchandising and licensing rights worldwide. The original series has spawned: several successful novelty western/folk albums from 1962ÿ1965; three dozen Dell and Gold Key comic books from 1962 through 1970; a short-lived comic book adaptation by Dutch comics artist Hans G. Kresse between 1965-1966,[36] Jim Beam Whiskey Ponderosa Ranch decanters 1964ÿ1966; a series of \\"Big-Little\\" books from 1966ÿ1969; Revel Bonanza model character sets from 1966ÿ1968; a chain of Bonanza and Ponderosa steakhouses from 1963ÿpresent; the Lake Tahoe-based \\"Ponderosa\\" theme park from 1967ÿ2004; a line of American Character action figures in 1966ÿ1967; Aladdin lunch buckets and thermos bottles in 1966ÿ1968; View Master slide sets in 1964, 1971; Ponderosa tin cups from 1967ÿ2004; a series of Hamilton collector plates in 1989ÿ1990; and most recently, Breyer Fiftieth Anniversary Ponderosa Stable sets, with horses and Cartwright figures in 2009ÿ2011. Fourteen Bonanza novels have been published: Bonanza: A Novel by Noel Loomis (1960); Bonanza: One Man With Courage by Thomas Thompson (1966); Bonanza: Killer Lion by Steve Frazee (1966); Bonanza: Treachery Trail by Harry Whittington (1968); Winter Grass by Dean Owen (1968); Ponderosa Kill by Dean Owen (1968); The Pioneer Spirit by Stephen Calder (1988); The Ponderosa Empire by Stephen Calder (1991); Bonanza: The High Steel Hazard by Stephen Calder (1993); Journey of the Horse by Stephen Calder (1993); The Money Hole by Stephen Calder (1993); The Trail to Timberline by Stephen Calder (1994); Bonanza: Felling of the Sons by Monette L. Bebow-Reinhard (2005), and Bonanza: Mystic Fire by Monette L. Bebow-Reinhard (2009). There is also a collection of Bonanza stories: The Best of Bonanza World: A Book of Favorite Stories, published by CreateSpace Independent Publishing Platform (2012). Bonanza Gold (2003ÿ2009), a quarterly magazine, featured detailed information about the show, including interviews with guest actors and other production personnel, articles about historical events and people depicted in the series, fan club information, and fan fiction. Seasons 1-8 (as of 06/2015) are available on DVD, as well as several non-successive public-domain episodes (sans original theme music). The prequel series, The Ponderosa, as well as the three sequel movies (see below), are all available on DVD09\\r\\nIn the fall of 1972, NBC moved Bonanza to Tuesday nights?ÿ where reruns from the 1967ÿ1970 period had aired the previous summer under the title Ponderosa[9]?ÿ opposite the All In The Family spinoff show, Maude, a virtual death sentence for the show. The scheduling change, as well as Dan Blocker's death several months earlier, resulted in plunging ratings for the show. David Canary returned to his former role of Candy (to offset Hoss' absence), and a new character named Griff King (played by Tim Matheson) was added to lure younger viewers. Griff, in prison for nearly killing his abusive stepfather, was paroled into Ben's custody and got a job as a ranch hand. Several episodes were built around his character, one that Matheson never had a chance to fully develop before the show's abrupt cancellation in November 1972 (with last episode airing January 16, 1973). Many fans, as well as both Landon and Greene, felt that the Hoss character was essential, as he was a nurturing, empathetic soul who rounded out the all-male cast.\\r\\nFor 14 years, the Cartwrights were the premier western family on American television and have been immensely popular on cable networks such as TV Land, INSP, Family Channel, and the Hallmark Channel. The series currently airs on Me-TV, TV Land, INSP, and Encore Westerns. TV Land airs Bonanza from only the first season to the 1969-1970 season. INSP first aired only selected first and second-season episodes of Bonanza and began to air the Bonanza Lost Episodes packages which contains episodes that originally aired from 1965-1973. Family Channel and the Hallmark Channel were two other channels that have also only aired the Bonanza Lost Episodes package. In October 2015, Me-TV additionally began airing the Bonanza Lost Episodes package.[37] As of December 2016, Me-TV returned to airing the original episodes package. Ever since March 2018 Me-TV has been airing the Lost Episodes, wanting to play the entire series from beginning to end.\\r\\nBonanza was revived for three made-for-television movies featuring the Cartwrights' children: Bonanza: The Next Generation (1988), Bonanza: The Return (1993), and Bonanza: Under Attack (1995). Michael Landon Jr., played Little Joe's son Benji while Gillian Greene, Lorne Greene's daughter, played a love interest. In the second movie, airing on NBC, a one-hour retrospective was done to introduce the drama. It was hosted by both Michael Landon Jr., and Dirk Blocker, who looks and sounds much like his father Dan Blocker. According to the magazine TV Guide, producer Dortort told Blocker he was too old to play the Hoss scion, but gave him the role of an unrelated newspaper reporter. Clips of his appearance were heavily used in advertisements promoting the \\"second generation\\" theme, perhaps misleading audiences to believe that Blocker was playing Hoss' heir. Hoss' son Josh was born out of wedlock, as it is explained that Hoss drowned without knowing his fianc?e was pregnant. Such a storyline might have been problematic in the original series. (The Big Valley, however, had a major character in Heath, who was presented as illegitimate. The Gunsmoke movies of the early 1990s employed a similar theme when Marshal Matt Dillon learned he had sired Michael Learned's character's daughter in a short-lived romance. The initial story was first introduced in 1973, when depiction of fornication courted protests, so CBS insisted their hero Matt have the encounter when he had amnesia.)\\r\\nIn 2001, there was an attempt to revive the Bonanza concept with a prequel, Ponderosa?ÿ not to be confused with the 1972 summer reruns under the same title[9]?ÿ with a pilot directed by Simon Wincer and filmed in Australia. Covering the time when the Cartwrights first arrived at the Ponderosa, when Adam was a teenager and Joe a little boy, the series lasted 20 episodes and featured less gunfire and brawling than the original. Bonanza creator David Dortort approved PAX TV (now Ion TV)'s decision to hire Beth Sullivan, formerly of Dr. Quinn, Medicine Woman, which some believe gave the series more depth as well as a softer edge. The Hop Sing character is depicted not only as a cook but also a family counselor and herbal healer. The series takes place in Nevada Territory in 1849, which is actually an anachronism. The Nevada Territory did not split from the Utah Territory until 1861, meaning that until at least the 5th season (the episode \\"Enter Thomas Bowers\\" establishes that year as 1857), Bonanza is also set in what in real life would have been Utah Territory.\\r\\n Several early episodes have fallen into the public domain. These episodes have been released by several companies in different configurations, with substandard picture and sound quality, edited, and by legal necessity with the copyright-protected Evans-Livingston theme song replaced with generic western music.\\r\\nIn 1973, NBC licensed the distribution rights to the series, along with the rest of its pre-1973 library, to National Telefilm Associates, which changed its name to Republic Pictures in 1986. Republic would become part of the Spelling Entertainment organization in 1994 through Worldvision Enterprises. Select episodes (\\"The Best of Bonanza\\") were officially released in North America in 2003 on DVD through then-Republic video licensee Artisan Entertainment (which was later purchased by Lionsgate Home Entertainment). Republic (through CBS Television Distribution, which holds the television side of Republic's holdings) still retains the syndication distribution rights to the series. CBS Home Entertainment (under Paramount Home Media Distribution) is the official home video rights distributor at present.\\r\\nStarting in September 2009, CBS Home Entertainment (distributed by Paramount) has to date released the first eight seasons on DVD in Region 1. All episodes have been digitally remastered from original 35mm film elements to yield the best picture and sound quality possible with current technology. CBSHE has released each season in two-volume sets (available together and separately). Each and every set contains exclusive multiple and rare bonus features, more than any other vintage long-running television series released on DVD. Classic series collections usually have bonus features included with the first season release only, if at all.\\r\\nIn Region 2, AL!VE AG released the first seven seasons on DVD in Germany between 2008ÿ2010. These releases are now out of print as AL!VE has lost the rights. In 2011, StudioCanal acquired the rights to the series and have begun re-releasing it on DVD, and all seasons have now been released but have not been remastered.\\r\\nEpisodes of the series have also been officially released as part-works on DVD in France and the United Kingdom.\\r\\nBonanza \\"the official first season\\" was released in Scandinavia during 2010. The first season is released in 4 volumes. The first two volumes were released on October 20, 2010, and the second two volumes on April 27, 2011.\\r\\nRegion 1","input":"How long did the tv series bonanza run?"},{"output":"Himadri Station","context":"Himadri Station is India's first Arctic research station[1] located at Spitsbergen, Svalbard, Norway. It is located at the International Arctic Research base, Ny-?lesund. It was inaugurated on the 1st of July, 2008 by the Minister of Earth Sciences.[2] It was set up during India's second Arctic expedition in June 2008.[3] It is located at a distance of 1,200 kilometres (750?mi) from the North Pole.[4]\\r\\n\\r\\n\\r\\nThe Himadri's functions include long term monitoring of the fjord (Kongsfjorden)dynamics, and atmospheric research.[5] The primary goals of India's research includes research on aerosol radiation, space weather, food-web dynamics, microbial communities, glaciers, sedimentology, and carbon recycling.[6] The research base has devoted time for the research of governance and policy of the Arctic. India has prioritised research and study in the fields of genetics, glaciology, geology, pollution in the atmosphere, and space weather among other fields.[7]\\r\\nIn 2012-2013, a total of 25 scientists visited the base which was staffed for 185 days to carry out studies under ten distinct projects.[8]\\r\\nThe United States Geological Survey estimates that 22 percent of the world's oil and natural gas could be located beneath the Arctic.[9] India's ONGC Videsh is reported to be interested in joint venture with Russia for oil exploration[10] and has reportedly requested Rosneft for stake in a project.[11] In addition, using Arctic sea lanes for shipping would reduce voyage times by 40% compared to Indian, Pacific or Atlantic ocean routes.[10] On May 15, 2013, India was made a permanent observer at the Arctic Council.[12]\\r\\nIndia is the 11th country after Britain, Germany, France, Italy, China, Japan, South Korea, The Netherlands, Sweden and Norway to set up a permanent research station in Ny-?lesund.[13]\\r\\nThe station was set up in a refurbished two floored building with four bedrooms. The building has an area of 220?m2 (2,400?sq?ft)[14] and has other facilities including a computer room, store room, drawing room and internet. It can host 8 scientists at normal conditions.[13] The crew of the station are given training in shooting with rifles to protect themselves from polar bears.[13]\\r\\nEstablished in August 2014. Designed and developed by scientists from the National Centre for Antarctic and Ocean Research (NCAOR) and National Institute of Ocean Technology (NIOT), the observatory has been deployed in the Kongsfjorden fjord of the Arctic, roughly halfway between Norway and the North Pole is named \\"IndARC\\" [15]","input":"What is the name of india's first arctic research station setup?"},{"output":"April 1991","context":"","input":"When did the republic of georgia become independent?"},{"output":"25 July 1978","context":"Louise Joy Brown (born 25 July 1978) is an English woman known for being the first human to have been born after conception by in vitro fertilisation, or IVF.\\r\\n\\r\\n\\r\\nLouise Joy Brown was born at Oldham General Hospital, Oldham, by planned Caesarean section delivered by registrar John Webster.[1] She weighed 5 pounds, 12 ounces (2.608?kg) at birth.[2] Her parents, Lesley and John Brown, had been trying to conceive for nine years. Lesley faced complications of blocked fallopian tubes.[2]\\r\\nOn 10 November 1977, Lesley Brown underwent a procedure, later to become known as IVF (in vitro fertilisation), developed by Patrick Steptoe and Robert Edwards. Edwards was awarded the 2010 Nobel Prize in Medicine for this work.[3] Although the media referred to Brown as a \\"test tube baby\\",[4] her conception actually took place in a Petri dish. Her younger sister, Natalie Brown, was also conceived through IVF four years later, and became the world's fortieth child after conception by IVF. In May 1999, Natalie was the first human born after conception by IVF to give birth herselfwithout IVFto daughter Casey.[2] Natalie has subsequently had three additional children; sons Christopher, Daniel, and Aeron, the last of whom was born in August 2013. And after four years her second child died due to medical issues.\\r\\nIn 2004, Brown married nightclub doorman (bouncer) Wesley Mullinder. Dr. Edwards attended their wedding.[2] Their son Cameron, conceived naturally,[4] was born on 20 December 2006.[5] Brown's second son, Aiden Patrick Robert, was born in August 2013.\\r\\nBrown's father died in 2007.[6] Her mother died on 6 June 2012 in Bristol Royal Infirmary at the age of 64[7] due to complications from a gallbladder infection.[6]\\r\\nAlthough the Browns knew the procedure was experimental, the doctors did not tell them that no case had yet resulted in a baby. This has raised questions of informed consent.[8]\\r\\nShortly before the death of Pope Paul VI, when asked for his reaction to Brown's birth, the patriarch of Venice, Cardinal Albino Luciani (later Pope John Paul I), expressed concerns about the possibility that artificial insemination could lead to women being used as \\"baby factories\\", but also refused to condemn the parents of the child,[9] noting they simply wanted to have a baby.[10]","input":"When was the first test tube baby born?"},{"output":"English","context":"","input":"What language is spoken the most in the united states?"},{"output":"September 27, 1996","context":"Sabrina the Teenage Witch is an American sitcom based on the Archie Comics series of the same name. The show premiered on September 27, 1996, on ABC to over 17 million viewers in its \\"T.G.I.F.\\" line-up.[3]\\r\\n\\r\\nThe show stars Melissa Joan Hart as Sabrina Spellman, an American teenager who, on her sixteenth birthday, discovers she has magical powers (a departure from the Archie Comics, where she has known of her magic powers since an early age as revealed in the \\"Sabrina -- That Cute Little Witch\\" storylines in many Little Archie comic books). She lives with her 500-year-old aunts, witches Hilda (played by Caroline Rhea) and Zelda (played by Beth Broderick), and their magical talking cat Salem (voiced by Nick Bakay) at 133 Collins Road[4] in the fictional Boston suburb of Westbridge, Massachusetts through most of the series.\\r\\n\\r\\nThe series' first four seasons aired on ABC from September 27, 1996, to May 5, 2000; the final three seasons ran on The WB from September 22, 2000, to April 24, 2003.\\r\\n\\r\\nThe unofficial pilot of the series was the 1996 TV movie Sabrina the Teenage Witch in April.[5] The movie, produced by Viacom and Hartbreak Films and aired on Showtime, starred Melissa Joan Hart as the title character, Sabrina Sawyer, and Charlene Fernetz and Sherry Miller as Sabrina's aunts Zelda and Hilda, respectively. When the television series debuted on ABC later that year, Hart became Sabrina Spellman, and Caroline Rhea and Beth Broderick replaced Fernetz and Miller. In 2000, the show was dropped by ABC and picked up by The WB. When viewership began to wane, the show was canceled after seven seasons.[6]\\r\\n\\r\\nThe television series was produced by Hartbreak Films and Viacom Productions with Finishing the Hat Productions being there for season 1 only. Later it was syndicated through Paramount Domestic Television on reruns. Paramount Network Television absorbed Viacom Productions in 2004, and two years later, Paramount's TV operations were renamed to CBS Paramount Television (now CBS Television Studios), whose name is seen on the season 1, 2, and 3 DVDs. Then in 2007, the syndication arm was renamed from CBS Paramount Domestic Television to CBS Television Distribution, whose name is on the season 4, 5, 6, and 7 DVDs. The official copyright holder for the series (as with all series originally produced by Viacom Productions) is CBS Studios Productions, LLC.\\r\\n\\r\\nThe opening titles of the first three seasons shows Sabrina in front of a mirror posing with four different costumes and outfits as the cast members' names quickly flash on the bottom of the screen. The first three outfits are always the same, but the fourth one changes from episode to episode. At the end, Sabrina would say something that is related to the last costume (often a pun or a joke related to the costume or the content of the episode), and then magically disappear from head on down.\\r\\n\\r\\nThe opening sequence for the fourth season is completely redone, featuring a completely new theme and the show's main characters, starting with Sabrina, floating in bubbles while their names are displayed in gold letters and a voice chants \\"Secret\\" in the background.\\r\\n\\r\\nThe opening credits for the final three seasons are accompanied by a new vocal theme song and feature Sabrina at various locations around Boston: Harvard Bridge, Boston Common, Union Oyster House, Massachusetts State House, Quincy Market, Newbury Street, Harvard University, Tufts University, and Beacon Hill. In the credits of seasons five and six, after leaving Newbury Comics on Newbury Street, Sabrina walks down a flight of stairs and computer graphics morph Sabrina into her room, lying on her bed next to Salem. In the final season, however, the computer graphics morph Sabrina arriving at Scorch. Upon pushing the door open, she is revealed to be walking into her house to greet Roxie, Morgan, and Salem.\\r\\n\\r\\nThe actual house photographed and filmed to depict Spellman Manor is a Victorian mansion located at 64 E. Main St. in Freehold, New Jersey.[7] The exteriors for Westbridge High School, which Sabrina attended, were Dwight Morrow High School in Englewood, New Jersey.\\r\\n\\r\\nThe show went through many cast changes, the first major change being the unexplained departure of Sabrina's best friend Jenny Kelly (Michelle Beaudoin) at the end of the first season.\\r\\n\\r\\nAt the beginning of the fourth season, Valerie departs the show permanently along with Libby. Valerie's character moves away to Alaska with her family, while Libby transfers to a boarding school.\\r\\n\\r\\nWhen the series finished its fourth season, several secondary characters left the show, including Martin Mull and Nate Richert. Richert, who played Sabrina's boyfriend Harvey since the first season, was cut in order to give the show a more \\"grown-up look\\" as Sabrina was about to attend college. The decision was later rescinded, and Richert appeared in three episodes of season five and then returned as a series regular in season six and seven.\\r\\n\\r\\nAfter the series' sixth season, Caroline Rhea and Beth Broderick, who portrayed Sabrina's aunts from the show's premiere, decided to leave the show. When the character of Sabrina started to attend college, the role of her aunts became less important. Broderick felt that the role of Zelda had nothing more to offer, while Rhea landed her own syndicated talk show, The Caroline Rhea Show.\\r\\n\\r\\nTrevor Lissauer, who played Sabrina's housemate Miles, left the show after appearing in seasons five and six. Producers felt that his character wasn't well received by fans and also had to make some budget cuts for the show's seventh and final season. Miles was never properly written out, leaving open what really happened to him.\\r\\n\\r\\nSabrina's love interest Josh, played by David Lascher, left for Prague after appearing from season four to six. Lascher reportedly wanted to pursue other projects. In order to fill the void, producers brought in Aaron, played by Dylan Neal, as Sabrina's love interest in the show's final season.\\r\\n\\r\\nThe show chronicles the adventures of Sabrina Spellman (played by Melissa Joan Hart), a girl who discovers on her sixteenth birthday that she is a witch. As a novice witch, her spells often go awry. Her aunts Hilda and Zelda Spellman (played by Caroline Rhea and Beth Broderick, respectively, until 2002) counsel her on the proper use of her magic and give her moral advice. Additionally, Hilda and Zelda must take care of Salem Saberhagen (voiced by Nick Bakay), a witch turned into a cat for trying to take over the world. Sabrina's basic premise and \\"genial loopiness\\" earned the show comparisons to the 1960s television series Bewitched.[8][9][10]\\r\\n\\r\\nThe show included contemporary pop cultural references, with Sabrina often saying she liked Britney Spears and No Doubt. It also mentioned human history alongside witches, such as the Salem witch hunt, which Zelda tells Sabrina was not a hunt for real witches. Hilda mentions to Sabrina in Episode 1 that \\"For two months a bunny ruled all of England\\" causing the witches council to turn back time, as if to say mortals are ignorant at the most. One episode also suggests that Jerry Springer is a witch when he hosts The Jerry Springer Show in the other realm.\\r\\n\\r\\nWhereas the comic book series was set in the fictional Greendale, the TV show was set in fictional Boston suburb of Westbridge, Massachusetts. The Spellmans' house is located at 133 Collins Road.[11]\\r\\n\\r\\nUltimately, the show covered a span of seven years over seven seasons, though each season was not a year.[12]\\r\\n\\r\\nThe pilot episode opens with Sabrina asleep on her 16th birthday, levitating above her bed. In the morning, her aunts reveal to her that she is a witch, but Sabrina does not believe them until she has a magical talk with her father from inside a book, where her father reveals that he is a witch and her mother is mortal. It is also revealed that Sabrina cannot see her mother, who is in Peru for two years, or her mother will be transformed into a ball of wax. After a rough day at school, Sabrina accidentally turns Libby Chesler, the most popular girl in school, into a pineapple. Fearing that she will appear \\"weird\\" to her crush, Harvey Kinkle, Sabrina asks the Witches Council to let her relive that day. The first season follows Sabrina as she tries to keep the balance between being a teenager and having magical powers. Sabrina's friend Jenny Kelley and her teacher, Mr. Pool, both exit the series without explanation after this season.\\r\\n\\r\\nAt the beginning of the second season, Sabrina turns seventeen and learns that she must earn her witch's license or otherwise lose her magic. However, she neglects her aunts' warnings to study for the test to obtain the license and consequently fails it. She then has to attend witch boot camp to earn the chance to take a makeup test. She passes the makeup test, but only receives a learner's permit. Her aunts explain that she will be able to earn her license when she turns eighteen (\\"when she can pay for the insurance\\") and that she will be tested throughout the year by a Quizmaster, a witch whose job is to instruct witches earning their licenses. Also introduced during the second season are Sabrina's neurotic friend Valerie and the new school vice principal Mr. Kraft, who finds Sabrina to be very odd and has a crush on Hilda.\\r\\n\\r\\nAt the beginning of the third season, Sabrina earns her license, but then learns that in order to use her license, she must solve her family's secret. Throughout the season, family members visit her and provide clues. At the end of the season, she solves the family secret (\\"Every member of the Spellman family is born with a twin\\"). Both Valerie and Libby exit the series after the end of the third season.\\r\\n\\r\\nAt the beginning of the fourth season, Sabrina is assigned to be a mentor, which is like a Quizmaster, except \\"Quizmasters get paid\\".[13] Sabrina's charge is Dreama, a witch newly immigrated from the Other Realm. A new student, Brad Alcero, transfers to Sabrina's school. Because Brad has a witch-hunter gene (which allows him to turn a witch into a mouse if the witch reveals his/her magic), Sabrina must keep herself and Dreama from using magic in front of Brad. Also, Sabrina begins working at Bean There, Brewed That, a coffee shop, where she meets and is attracted to Josh (played by David Lascher), a college student who is the manager of the shop, which leads to her kissing him and thus cheating on Harvey, who ends their relationship, but they soon get back together. Both Dreama and Brad are written out of the series without explanation over the course of the season. At the end of the season, Harvey reaches his \\"spell quota\\" (meaning that no spells can be used on him anymore) and discovers that Sabrina is a witch, and later breaks up with Sabrina off screen and is written out of the show prior to the start of the next season.\\r\\n\\r\\nAt the beginning of the fifth season, Sabrina starts college at Adams College and moves out of her aunts' house and into a house with other students at Adams. Her roommates are Morgan Cavanaugh, a shallow girl (played by Elisa Donovan); Roxie King (played by Soleil Moon Frye), a social feminist; and Miles Goodman (played by Trevor Lissauer), a geek who is obsessed with science fiction and the paranormal. Hilda and Zelda, feeling lonely since Sabrina moved out, find ways to stay close to her. Hilda buys the coffee shop where Sabrina works and Zelda becomes a professor at Adams and starts dating Sabrina's English professor. The season ends with Sabrina and Josh giving into their feelings and sharing a passionate kiss.\\r\\n\\r\\nAt the beginning of the sixth season Josh and Sabrina get together but soon face a tough decision when Josh is tempted to move abroad to work as a photographer. Things are made even more complicated between them when Sabrina's ex-boyfriend from high school, Harvey, reappears, this time dating Sabrina's roommate Morgan. At the end of the sixth season, Sabrina agrees to sacrifice true love in order to save Hilda after the latter literally falls to pieces after Sabrina sabotages her relationship. Hilda recovers and is married, but Sabrina then falls to pieces when Josh, Harvey, and an attractive waiter announce they are all moving away and will never see her again.\\r\\n\\r\\nAt the beginning of the seventh and final season, Sabrina is put back together after her Aunt Zelda sacrifices her adult years to save Sabrina. At this point, Zelda, Hilda, Miles, and Josh are written out of the show and Morgan and Roxie move into Hilda and Zelda's old house. Sabrina gets a job as a writer for the entertainment magazine Scorch, but this storyline and all the associated characters are dropped midway through the season. Sabrina then meets Aaron, the man to whom she becomes engaged. In the series finale Sabrina calls off her wedding with Aaron and runs off with Harvey, her soul mate, at 12:36, which is the time of day they first met and is a plot point in the season one episode As Westbridge Turns. In the last scene of the series, Sabrina and Harvey ride off on his motorcycle to the song Running by No Doubt, Sabrinas favorite band.\\r\\n\\r\\nDuring its four-year run on ABC, Sabrina was the highest-rated series among the network's TGIF line-up. In the 2000ÿ2001 season, the show moved to The WB after a negotiation dispute with ABC. While ABC was willing to renew the show for a fifth season, the network was not willing to pay the reported $1.5 million per episode that Viacom Productions, which produced the show, wanted. The WB then picked up the show for a mere $675,000 per episode, but agreed to commit to 44 episodes.\\r\\n\\r\\nWhen The WB network picked up the series after ABC canceled the series, local stations (usually WB or UPN affiliates, with a couple of \\"Big 4\\" network affiliates as well) also picked up the rights to air reruns of the series from September 2000 on a weekday basis after the broadcast syndication ended in summer 2006. For example, WPIX reran weeknights at 5:30pm EST, after reruns of The Fresh Prince of Bel-Air. ABC Family, a United States cable network, had exclusive United States rights to air the show at 8:00am on weekdays, until March 25, 2011, at which point they dropped the series from the broadcast roster due to a contract expiration.[21] The series aired on The N during 2004ÿ2009, until the channel became TeenNick in September 2009, where it continued to air intermittently for years. From late 2015 to early 2016, TeenNick aired the series with two back-to-back episodes at 6:00 pm on Saturdays and Sundays.\\r\\n\\r\\nThe series aired on Hub Network from 2012 to 2014.[22] MTV2 also aired the show at one point. In 2015, the series aired on Logo TV on Thursday afternoons and Thursday late nights, in addition to POP from 2016-2017. The series began airing on Antenna TV in January 2016. As of July 21, 2018, the series is on Antenna TV Weekends at 1:00 pm ET.\\r\\n\\r\\nIn the United Kingdom, Sabrina previously aired on ITV and Nickelodeon. It began airing on Pop Girl, a free-to-air children's channel, in July 2012, which currently broadcasts the first two seasons and the two subsequent movie. It was also broadcast on The Vault in 2014, during weekdays. The show can be seen in Ireland on RT Two each weekday, 5:10ÿ5:35 pm, as part of the youth oriented show TRT. The entire series is currently (as of December 2016) available for streaming on hulu. The entire series is available on Amazon Prime.\\r\\n\\r\\nIn Australia, the show could be seen on Eleven, the Network Ten owned free-to-air channel at 6:00 pm and until 6 December 2013, with repeats at 12.30 am, seven days a week.\\r\\n\\r\\nOn June 11, 1999, Knowledge Adventure through Simon & Schuster Interactive and Havas Interactive officially announced the video games Sabrina the Teenage Witch: Spellbound, Sabrina the Teenage Witch: Brat Attack and Sabrina the Teenage Witch: Bundle of Magic for Macintosh and Microsoft Windows operating systems.\\r\\n\\r\\nOn March 29, 2001, Knowledge Adventure through Simon & Schuster Interactive and Havas Interactive officially announced the video game Sabrina the Teenage Witch: A Twitch in Time! for the PlayStation game system.[23]\\r\\n\\r\\nAn animated spin-off of the show, Sabrina: The Animated Series, started airing during the live action show's 4th season. The role of Sabrina was voiced by Hart's younger sister Emily Hart. Melissa Joan Hart voiced both aunts. This series was followed by a television film, Sabrina: Friends Forever, which in turn was followed by another series titled Sabrina's Secret Life. Neither Emily Hart nor Melissa Joan Hart returned for the television film or the follow up series.\\r\\n\\r\\nA new animated spin-off was produced by Hub Network in 2013 called Sabrina: Secrets of a Teenage Witch. In this version, Sabrina (voiced by Ashley Tisdale) is a witch princess in training so that she can one day rule the other realm.\\r\\n\\r\\nT?G?I?f","input":"When did sabrina the teenage witch first air?"},{"output":"through evolution","context":"Flightless birds are birds that through evolution lost the ability to fly.[1] There are over 60 extant species[2] including the well known ratites (ostrich, emu, cassowary, rhea and kiwi) and penguins. The smallest flightless bird is the Inaccessible Island rail (length 12.5?cm, weight 34.7 g). The largest (both heaviest and tallest) flightless bird, which is also the largest living bird, is the ostrich (2.7 m, 156?kg). Ostriches are farmed for their decorative feathers, meat and their skins, which are used to make leather.\\r\\nMany domesticated birds, such as the domestic chicken and domestic duck, have lost the ability to fly for extended periods, although their ancestral species, the red junglefowl and mallard, respectively, are capable of extended flight. A few particularly bred birds, such as the Broad Breasted White turkey, have become totally flightless as a result of selective breeding; the birds were bred to grow massive breast meat that weighs too much for the bird's wings to support in flight.\\r\\nFlightlessness has evolved in many different birds independently. There were also other families of flightless birds, such as the now extinct Phorusrhacidae, that evolved to be powerful terrestrial predators. Taking this to a greater extreme, the terror birds (and their relatives the bathornithids), eogruids, gastornithiforms, and dromornithids (all extinct) all evolved similar body shapes ÿ long legs, long necks and big heads ÿ but none of them were closely related. Furthermore, they also share traits of being giant, flightless birds with vestigial wings, long legs, and long necks with some of the ratites, although they are not related.[3][4]\\r\\n\\r\\n\\r\\nDivergences and losses of flight within ratite lineage occurred right after the K-Pg extinction event wiped out all non-avian dinosaurs and large vertebrates 66 million years ago.[5] The immediate evacuation of niches following the mass extinction provided opportunities for Palaeognathes to distribute and occupy novel environments. New ecological influences selectively pressured different taxon to converge on flightless modes of existence by altering them morphologically and behaviorally. The successful acquisition and protection of a claimed territory selected for large size and cursoriality in Tertiary ancestors of ratites[6] Temperate rainforests dried out throughout the Miocene and transformed into semiarid deserts causing habitats to be widely spread across the growingly disparate landmasses. Cursoriality was an economic means of traveling long distances to acquire food that was usually low lying vegetation, more easily accessed by walking[6] Traces of these events are reflected in ratite distribution throughout semiarid grasslands and deserts today[7]\\r\\nGigantism and flightlessness are almost exclusively correlated.[clarification needed] This is mostly observed in islands lacking predators and competition. However, ratites occupy environments that are mostly occupied by a diverse number of mammals.[8] It is thought that they first originated through allopatric speciation caused by breakup of the supercontinent Gondwana.[9] However recent evidence suggests this hypothesis first proposed by Joel Cracraft in 1974 is incorrect.[10] Rather ratites arrived in their respective locations via a flighted ancestor and lost the ability to fly multiple times within the lineage.\\r\\nGigantism is not a requirement for flightlessness. The kiwi does not exhibit gigantism, along with tinamous, even though they coexisted with the moa and rhea that both exhibit gigantism. This could be the result of different ancestral flighted birds arrival or because of competitive exclusion.[11] The first flightless bird to arrive in each environment utilized the large flightless herbivore or omnivore niche, forcing the later arrivals to remain smaller. In environments where flightless birds are not present, it is possible that after the K/T Boundary there were no niches for them to fill. They were pushed out by other herbivorous mammals.[8]\\r\\nNew Zealand had more species of flightless birds (including the kiwi, several species of penguins, the takahe, the moa, and several other extinct species) than any other such location. One reason is that until the arrival of humans roughly a thousand years ago, there were no large land predators in New Zealand; the main predators of flightless birds were larger birds.[12]\\r\\nRatites belong to the superorder Palaeognathae birds, which include the volant tinamou, and are believed to have evolved flightlessness independently multiple times within their own group.[3][5][6][8] Some birds evolved flightlessness in response to the absence of predators, for example on oceanic islands. Incongruences between ratite phylogeny and Gondwana geological history indicate the presence of ratites in their current locations is the result of a secondary invasion by flying birds.[13] It remains possible that the most recent common ancestor of ratites was flightless and the tinamou regained the ability to fly[14] However, it is believed that the loss of flight is an easier transition for birds rather than the loss and regain of flight, which has never been documented in avian history.[6] Moreover, tinamou nesting within flightless ratites indicates ancestral ratites were volant and multiple losses of flight occurred independently throughout the lineage. This indicates that the distinctive flightless nature of ratites is the result of convergent evolution.[15]\\r\\nTwo key differences between flying and flightless birds are the smaller wing bones of flightless birds[16] and the absent (or greatly reduced) keel on their breastbone. (The keel anchors muscles needed for wing movement.[2]).\\r\\nAdapting to a cursorial lifestyle causes two inverse morphological changes to occur in the skeleto-muscular system: the pectoral apparatus used to power flight is paedorphically reduced while peramorphosis leads to enlargement of the pelvic girdle for running[17] Repeated selection for cursorial traits across ratites suggests these adaptions comprise a more efficient use of energy in adulthood[6] The name ratite refers to their flat sternum that is distinct from the typical sternum of flighted birds because it lacks the keel. This structure is the place where flight muscles attach and thus allow for powered flight[15] Though, ratite anatomy presents other primitive characters meant for flight such as the fusion of wing elements, a cerebellar structure, the presence of a pygostyle for tail feathers and an alula on the wing[10] These morphological traits suggest some affinities to volant groups. Palaeognathes were one of the first colonizers of novel niches and were free to increase in abundance until the population was limited by food and territory. A study looking at energy conservation and the evolution of flightlessness hypothesized intraspecific competition selected for a reduced individual energy expenditure, which is achieved by the loss of flight[18]\\r\\nSome flightless varieties of island birds are closely related to flying varieties, implying flight is a significant biological cost.[citation needed][18] Flight is the most costly type of locomotion exemplified in the natural world. The energy expenditure required for flight increases proportionally with body size, which is often why flightlessness coincides with body mass[7] By reducing large pectoral muscles that require a significant amount of overall metabolic energy, ratites decrease their basal metabolic rate and conserve energy.[18][19] A study looking at the basal rates of birds found a significant correlation between low basal rate and pectoral muscle mass in kiwis. On the contrary, flightless penguins exude an intermediate basal rate. This is likely because penguins have well-developed pectoral muscles for hunting and diving in the water[18] For ground feeding birds, a cursorial lifestyle is more economical and allows for easier access to dietary requirements[6] Flying birds have different wing and feather structures that make flying easier, while flightless birds' wing structures are well adapted to their environment and activities, such as diving in the ocean.[20]\\r\\nA number of bird species appear to be in the process of losing their powers of flight to various extents. These include the Zapata rail of Cuba, the Okinawa rail of Japan, and the Laysan duck of Hawaii. All of these birds show adaptations common to flightlessness, and evolved recently from flying ancestors, but have not yet fully given up the use of their wings. They are, however, weak flyers and are incapable of traveling long distances by air.[21]\\r\\nAlthough selection pressure for flight was largely absent, the wing structure has not been lost except in the New Zealand moas.[17] Ostriches are the fastest running birds in the world and emus have been documented running 50?km/hr[7] At these high speeds, wings are necessary for balance and serving as a parachute apparatus to help the bird slow down. Wings are hypothesized to have played a role in sexual selection in early ancestral ratites and were thus maintained. This can be seen today in both the rheas and ostriches. These ratites utilize their wings extensively for courtship and displays to other males.[10] Sexual selection also influences the maintenance of large body size, which discourages flight. The large size of ratites leads to greater access to mates and higher reproductive success. Ratites and tinamous are monogamous and mate only a limited number of times per year.[22] High parental involvement denotes the necessity for choosing a reliable mate. In a climactically stable habitat providing year round food supply, a males claimed territory signals to females the abundance of resources readily available to her and her offspring.[19] Male size also indicates his protective abilities. Similar to the emperor penguin, male ratites incubate and protect their offspring anywhere between 85ÿ92 days while females feed. They can go up to a week without eating and survive only off fat stores. The emu has been documented fasting as long as 56 days.[7] If no continued pressures warrant the energy expenditure to maintain the structures of flight, selection will tend towards these other traits.\\r\\nMany flightless birds are extinct; this list shows species that are either still extant, or became extinct in the Holocene (no more than 11,000 years ago). Extinct species are indicated with a dagger (?). A number of species that are suspected, but not confirmed to be flightless, are also included here.","input":"Why are birds such as kiwi and penguin unable to fly?"},{"output":"the GMC Canyon","context":"The Chevrolet Colorado and its counterpart, the GMC Canyon, are mid-size pickup trucks marketed by American automaker General Motors. They were introduced in 2004 to replace the Chevrolet S-10 compact pickups. It is named for the U.S. state of Colorado.\\r\\n\\r\\n\\r\\nThe Chevrolet Colorado and its twin, the GMC Canyon were jointly designed by GM's North American operations, GM's Brazil operations, and Isuzu. Isuzu, which participated in the design process, began selling its own version worldwide in 2002.[2] In late 2005, Isuzu offered a version in North America called the Isuzu i-Series. This North American model Isuzu shared North American powertrains, styling, and equipment with the Colorado/Canyon twins and differed from Isuzu's worldwide offering. All Chevrolet, GMC, and Isuzu versions worldwide are based on the GMT355, itself the basis for the GMT 345-based Hummer H3. Most vehicles for markets outside North America are manufactured at a GM plant in Rayong, Thailand, as well as at a GM plant in S?o Jos dos Campos, Brazil. All North American-market vehicles were manufactured by Shreveport Operations.\\r\\nThe cooperation between GM and Isuzu to build a light-duty truck and offer it in North America returns to an original arrangement the two companies had in the 1970s with the Chevrolet LUV, a rebadged Isuzu Faster.\\r\\nThe Colorado/Canyon offer both manual and automatic transmissions. GM also offers either a rear-wheel drive or four-wheel drive drivetrain with standard, extended, and four-door crew cab body styles. Most models come with the 2.8?L (171?cu?in) LK5 I4 engine as standard, but a more powerful 3.5 I5 comes with the Z71 package on four-door versions and is optional on all others. The 4-door Z71s also get the 4-speed automatic transmission standard. This package was later dropped in favor of LT2 and LT3.\\r\\nA ZQ8 edition is available with a lower and more road-tuned \\"sport\\" suspension than the standard Z85 and comes with 17?inch wheels, color-matched bumper and grille, and low profile fender flares. There is also a Xtreme edition of the ZQ8 which has a different front bumper, rear bumper, side skirts, fender flares, grille, headlights, and 18?inch wheels. The Xtreme edition is basically a continuation of a trim package from its predecessor, the Chevrolet S-10.\\r\\nFor 2007, Colorado/Canyon was facelifted and offered new engines, which include the 2.9?L (177?cu?in) LLV I4 and 3.7?L (226?cu?in) LLR I5 which were both introduced due to numerous head problems, new colors? Deep Ruby (Sonoma Red for the Canyon), Pace Blue (Sport Blue for the Canyon) and Imperial Blue (Midnight Blue for the Canyon), and new tires and wheels. Minor changes to the grille and interior for the LT and LTZ models; the LS models kept the same pre-facelift look, similar to the facelift of the TrailBlazer in 2005 & 2006. In addition, the \\"Colorado\\" & \\"Canyon\\" badges were phased out from the doors in favour of GM's corporate logo. For model year 2009, the Colorado was facelifted again and a 5.3?L (323?cu?in) LH8 V8 is offered, producing 300?hp (224?kW) and 320?lb?ft (434?N?m). For the 2010 model year, the GM badges were phased out from those trucks,[3] although fewer 2010 models had the GM logo on the doors.\\r\\nColorado/Canyon U.S. sales peaked in 2005 at 163,204 units, surpassing the perennial segment leader, the Ford Ranger, by almost 35% and just 3.3% behind the new best-seller, the Toyota Tacoma. In 2006, however, while still leading the Ford pickup by 27.5%, Colorado/Canyon's sales lagged their Toyota competitor's by almost 34%. One 2005 Canyon owned by successful U.S. Senate candidate Scott Brown became famous as it was widely featured in his TV advertisements.[4]\\r\\nSG Automotive manufactures a clone of the Colorado called the Huanghai Plutus in China\\r\\n[5]\\r\\nIn the Insurance Institute for Highway Safety's frontal offset test the Colorado (extended version) is given a \\"Good\\" overall score,[22] however the crew cab is rated \\"Acceptable\\".[23] In the side impact test, the Colorado crew cab is rated \\"Poor\\" with or without side curtain airbags.[24][25] Side curtain airbags were made standard on all 2010 models; side torso airbags are not offered.\\r\\nThe Isuzu i-Series mid-size pickup truck line was manufactured from 2005 to 2008. Launched at the 2005 New York International Auto Show, for the 2006 model year, the i-Series replaced the Isuzu Hombre, which had been out of production since 2000. Like the Hombre, which was based on the compact Chevrolet S-10 / GMC Sonoma, the i-Series was based on the Chevrolet Colorado / GMC Canyon. The sales for the i-Series were poor, with just 1,377 sold from the start of production through February 2006 according to Automotive News. As part of Isuzu's withdrawal from the United States market after the 2008 model year, the i-Series was discontinued.\\r\\nThe second-generation Chevrolet Colorado was shown at the 2011 Bangkok Motor Show.[28] It is available in three different cab styles: regular cab (single cab), extended cab (space cab), and crew cab (double cab) and may either be rear-wheel drive or four-wheel drive.\\r\\nThe Holden Colorado was revealed at the 2011 Australian International Motor Show and went on sale in June 2012 in both Australia and New Zealand, sourced from the Rayong factory in Thailand. Only one engine is offered, the 2.8L turbo diesel, built by GM in the Rayong Factory. It is available in four trim levels: DX (single cab chassis only), LX (chassis only), LT and LTZ. Single cab and crew cab models are available as either two-wheel drive or four-wheel drive.[29]\\r\\nA Thunder Edition is available on 4x4 LX, LT and LTZ models. Among the features included with the pack are a nudge bar and towing kit, front carpet floor mats and a one-piece rear carpet floor mat, a bonnet protector, slimline weather shields and \\"Thunder\\" badging.[30]\\r\\n[31]\\r\\nPre-facelift Holden Colorado\\r\\nFacelift Holden Colorado\\r\\n[33]\\r\\nThe US-spec model was revealed on November 20, 2013 at the Los Angeles Auto Show.[34] Production began at Wentzville Assembly in 2014, and vehicles started arriving at dealerships nationwide by mid-late 2014, as an early 2015 model year vehicle. It competes with other midsized pickup truck offerings, such as the Nissan Frontier and Toyota Tacoma. The US spec model features a different front fascia, with its design being similar to the 2014 Chevrolet Silverado, and different engines. General Motors' Wentzville assembly plant received a $380 million expansion of 500,000 square feet that was constructed to support the new Colorado's assembly.[35] On September 18, 2014, GM announced that it would add more workers to the Wentzville factory after it received early advanced orders of 30,000 Colorado/Canyons from its dealerships ahead of its release.[36]\\r\\nFor the initial launch, the Colorado was available with either a 2.5 L Ecotec I4 engine or the 3.6 L LFX V6 engine.[37] The 2.8 L Duramax LWN turbodiesel engine was added in 2016,[37] and is a first for its class ever in the US market. It is also available in three cab configurations: extended cab with a 6.2?ft (1.9?m) bed, crew cab with a 5.2?ft (1.6?m) bed, or crew cab with a 6.2?ft (1.9?m) bed, with four-wheel drive being optional.[37]\\r\\nSixteen-inch or seventeen-inch steel wheels or alloy wheels are available, as well as a decor package for the W/T model that adds a chrome front grille and body-colored door handles and side view mirrors.\\r\\nThe base Colorado trim level includes the following standard equipment: sixteen-inch steel wheels, vinyl seating surfaces, an A/M-F/M stereo with auxiliary audio input and a six-speaker audio system, power windows and door locks (NO keyless entry), a 2.5L EcoTec Inline Four-Cylinder (I4) gasoline engine, a six-speed manual transmission, carpeted flooring, and a chrome front grille. This model is available exclusively as an Extended Cab.\\r\\nThe W/T includes the same standard equipment as the base Colorado trim level, but has more optional equipment, such as a seven-inch color touch-screen MyLink radio, cloth seating surfaces, keyless entry, sixteen-inch aluminum-alloy wheels, color-keyed door mirrors, an automatic transmission, and a 3.6L V6 gasoline engine. It is available as either an Extended Cab, or as a Crew Cab with either a Short Box or a Long Box.\\r\\nThe LT adds the following equipment to the base Colorado and W/T trim levels: seventeen-inch aluminum-alloy wheels, a 3.6L V6 gasoline engine, an automatic transmission, an eight-inch color touch-screen MyLink radio, SiriusXM Satellite Radio, OnStar with 4G LTE Wi-Fi Connectivity (later models only), cloth seating surfaces, and keyless entry. Optional equipment includes a 2.8L Duramax Inline Four-Cylinder (I4) Turbo Diesel engine, remote start, a rear-view backup camera system, a premium Bose seven-speaker audio system, a GPS navigational radio with MyLink, and chrome exterior accessories. It is available as either an Extended Cab, or as a Crew Cab with either a Short Box or a Long Box.\\r\\nThe Z/71 is the top-of-the-line trim level, and adds the following equipment to the LT trim level: unique aluminum-alloy wheels, vinyl and cloth combination seating surfaces, and a black front grille. Optional equipment is the same as the LT. It is available as either an Extended Cab, or as a Crew Cab with either a Short Box or a Long Box.\\r\\nThe ZR-2 is the \\"Off-Road\\" trim level, and adds the following equipment to the Z/71 trim level: 2\\" taller suspension, 3.5\\" wider track, 31\\" off-road tires, unique aluminum-alloy wheels, and leather-trimmed seating surfaces, functional rock sliders, Multimatic DSSV position sensitive shock absorbers, full set of skid plates, locking front and rear differential, high approach angle front bumper. Optional equipment is the same as the LT and Z/71 trim levels, except for the availability of a rear bed-mounted light bar. It is available as either an Extended Cab with a long box, or as a Crew Cab with a Short Box.\\r\\nThe all-new GMC Canyon was introduced on January 12, 2014. GMC introduced in an official press and video release and made its public debut the following day (January 13, 2014) at the North American International Auto Show in Detroit. Sales started at dealerships in the second quarter of 2014 as a 2015 model.\\r\\nThe Canyon offers the same features as its cousin, but will have a higher MSRP and more options. It also sports a front grille design that closely resembles its full-size sibling, the GMC Sierra. The 2015 Canyon comes in three trims and either two-wheel drive or four-wheel drive. The base model is only available with the 2.5 L Ecotec I4 engine, a 6-speed manual transmission, and two-wheel drive.\\r\\nOptional features (based on trim) will include active aero grille shutters, available OnStar 4G LTE connectivity with on-board wifi, forward collision warning, lane departure warning, and an available Teen Driver feature that limits speed and audio volume.[38]\\r\\nOn February 16, 2015, GMC posted spyshots of a Canyon with Denali styling, indicating plans to add the mid-size truck to the Denali lineup as a 2016 model.[39]\\r\\nThe ZR2 concept was shown at the 2014 LA Auto Show, and due to positive feedback a production ready version was announced 2 years later at the 2016 LA Auto Show to be a part of the 2017 lineup.\\r\\nThe Colorado ZR2 offers the same amenities as its brother the Z71, but is heavily tailored towards off-road performance. The ZR2 model will be available in 2 body configurations; crew cab with 5?ft bed or extended cab with 6?ft bed. Engine options are the 3.6 L LGZ V6 engine and the 2.8 L Duramax LWN turbodiesel engine.\\r\\nChevrolet designed many new parts specifically for the ZR2 to improve off-road performance. New features and parts standard on the ZR2 include:\\r\\nThe 2018 Colorado ZR2 was featured in Season 9 Episode 99 of Motor Trend Head 2 Head, where it faced off against a 2017 Ram Power Wagon.","input":"What is the gmc version of the chevy colorado?"},{"output":"eight wins","context":"The Carnival Road March is the musical composition played most often at the \\"judging points\\" along the parade route during Carnival. Originating as part of Trinidad and Tobago Carnival, the term has been applied to other Caribbean carnivals.\\r\\nIn Trinidad and Tobago the Road March title has been given out in every year since 1932 (with the exception of years affected by World War II when Carnival did not take place). Scoring is based upon a rank-and-points system devised by a Carnival committee before the start of the parade. The Road March title is among the most prestigious in Trinidad Carnival. The most such titles have gone to the Mighty Sparrow and Machel Montano with eight wins, Super Blue, with nine wins, and the late Lord Kitchener, with ten. In the mid-1970s, women entered the calypso men' s oriented arena. Calypso Rose was the first female to win the Trinidad Road March competition in 1977 with her song \\"Gimme More Tempo\\". The following year with \\"Come Leh We Jam\\" she won the \\"Calypso King \\" competition, the first time a woman had received the award. The competition's title was changed to Calypso Monarch in her honour.\\r\\n\\r\\n\\r\\nInvader has won the Road March in St. Lucia seven times. In a period from 1970-1972 and again from 1974-1976, Road March winners were imported from Trinidad and Tobago.\\r\\nThe Jam Band, formerly Eddie and the Movements has won the Road March Title a record 21 times.","input":"How many road march titles machel montano won?"},{"output":"June 18, 2012","context":"Love & Hip Hop: Atlanta is the second installment of the Love & Hip Hop reality television franchise.[1] It premiered June 18, 2012 on VH1, and chronicles the lives of several people in Atlanta involved with hip hop music. Love & Hip Hop: Atlanta is the highest-rated installment of the Love & Hip Hop franchise.[2] On January 30, 2017, VH1 announced the show's return for the sixth season, which premiered on March 6, 2017.\\r\\n\\r\\n\\r\\nNote:\\r\\nThe series has a large ensemble cast. The lead cast members appear in the opening credits, while other cast members are credited as \\"additional cast\\" or \\"featured\\" in the show's credits. These cast members appear in green screen confessional interview segments and (for the most part) have the same amount of screen time and storyline focus as the show's main cast members. Some major supporting cast members have been eventually upgraded to main cast members.\\r\\nNote:","input":"When did love and hip hop atlanta start?"},{"output":"world's second largest","context":"","input":"What is the size of the chinese economy?"},{"output":"around 1280 CE","context":"","input":"When did the first settlers come to new zealand?"},{"output":"Dan Patrick","context":"The Lieutenant Governor of Texas is the second-highest executive office in the government of Texas, a state in the U.S. It is the second most powerful post in Texas government because its occupant controls the work of the Texas Senate and controls the budgeting process as a leader of the Legislative Budget Board.\\r\\nUnder the provisions of the Texas Constitution, the Lieutenant Governor is President of the Texas Senate. Unlike with most other states' senates and the U.S. Senate, the Lieutenant Governor regularly exercises this function rather than delegating it to the president pro tempore or a Majority Leader. By the rules of the Senate, the Lieutenant Governor establishes all special and standing committees, appoints all chairpersons and members, and assigns all Senate legislation to the committee of his choice. The Lieutenant Governor decides all questions of parliamentary procedure in the Senate. He or she also has broad discretion in following Senate procedural rules.\\r\\nThe Lieutenant Governor is an ex officio member of several statutory bodies. These include the Legislative Budget Board, the Legislative Council, the Legislative Audit Committee, the Legislative Board and Legislative Council, which have considerable sway over state programs, the budget and policy. The Lieutenant Governor is also a member of the Legislative Redistricting Board (together with the Speaker of the House, Attorney General, Comptroller, and Land Commissioner), which is charged with adopting a redistricting plan for the Texas House of Representatives, Texas Senate, or U.S. House of Representatives after the decennial census if the Legislature fails to do so.\\r\\nIn the case of a vacancy in the Lieutenant Governor's office, the Senate elects one of its members to act as President of the Senate until the next statewide office election, in effect becoming the Lieutenant Governor. A Senator elected as presiding officer in this way retains his district seat and the voting privileges entailed with his Senate election. The Lieutenant Governor is sworn-in on the third Tuesday every four years, the same as the Governor.\\r\\nDan Patrick has been the Lieutenant Governor of Texas since January 20, 2015.\\r\\nThe term of office was two years from 1846 to 1972. Voters then increased it to four years, effective for the 1974 election. [1]\\r\\nThe Lieutenant Governor assumes the powers of the Governor of Texas when the governor is out of the state or otherwise unable to discharge the office. The Lieutenant Governor is elected separately from the Governor, rather than on the same ticket; it is therefore possible for the Governor and Lieutenant Governor to be from different political parties (which was the case during Governor George W. Bush's first term and also during Bill Clements's two non-consecutive terms). The Lieutenant Governor becomes Governor if the elected Governor resigns, dies or is removed from office via impeachment and conviction. Former Governor Rick Perry took office upon George W. Bush's resignation on December 21, 2000. Bush became US President on January 20, 2001. When Perry became lieutenant governor on 19 January 1999, he became the first Republican since Albert Jennings Fountain in 1873 to serve as lieutenant governor, and the first Republican to be elected as Lieutenant Governor since James W. Flanagan in 1869.\\r\\n\\r\\n\\r\\nTexas is one of the few states that vests significant power in the office of lieutenant governor, making it among the most influential. By contrast, the lieutenant governor position in other states has few (if any) legislative responsibilities, akin to the Vice President of the United States. The consequence is that the Governor of Texas is weaker than other states' governors.\\r\\nPolitical party affiliation:\\r\\nIn the Texas Senate, there are no majority or minority leaders.\\r\\nIn the Texas House, there are no majority or minority leaders.","input":"Who is the presiding officer of the texas senate?"},{"output":"Robert Pershing Wadlow","context":"Robert Pershing Wadlow (February 22, 1918 ÿ July 15, 1940), also known as the Alton Giant and the Giant of Illinois, was an American who became famous as the tallest person ever in human recorded history for whom there is irrefutable evidence.[3] The Alton and Illinois monikers reflect that he was born and raised in Alton, Illinois.[1]\\r\\nWadlow reached 8?ft 11.1?in (2.72?m)[2][4][5] in height and weighed 490?lb (220?kg) at his death at age 22. His great size and his continued growth in adulthood were due to hyperplasia of his pituitary gland, which results in an abnormally high level of human growth hormone. He showed no indication of an end to his growth even at the time of his death.\\r\\n\\r\\n\\r\\nWadlow was born to Addie Johnson and Harold Wadlow in Alton, Illinois, on February 22, 1918, and was the oldest of five children. He was taller than his father by the age of 8, and in elementary school they had to make a special desk for him due to his size. By the time he had graduated from Alton High School in 1936, he was 8?ft 4?in (2.54?m).[1] After graduating he enrolled in Shurtleff College with the intention of studying law.\\r\\nWadlow's size began to take its toll: he required leg braces to walk and had little feeling in his legs and feet. Despite these difficulties, he never used a wheelchair.\\r\\nWadlow became a celebrity after his 1936 U.S. tour with the Ringling Brothers Circus. In 1938, he did a promotional tour with the International Shoe Company (since 1966 called INTERCO). They provided him his shoes free of charge. Examples still exist in several locations throughout the U.S., including Snyder's Shoe Store of Ludington and Manistee, Michigan, and the Alton Museum of History and Art. He continued participating in tours and public appearances, though only in his normal street clothes.[6] He possessed great physical strength until the last year of his life, when his strength and his health in general began to deteriorate rapidly.[citation needed]\\r\\nWadlow was a member of the Order of DeMolay, the Masonic-sponsored organization for young men. He was also a Freemason. In 1939, he petitioned Franklin Lodge #25 in Alton, Illinois, and by late November of that year[7] was raised to the degree of Master Mason under the jurisdiction of the Grand Lodge of Illinois A.F and A.M.. His Freemason ring was the largest ever made.[citation needed]\\r\\nOne year before his death, Wadlow passed John Rogan as the tallest person ever. On June 27, 1940 (eighteen days before his death), he was measured at 8?ft 11.1?in (2.72?m) by doctors C. M. Charles and Cyril MacBryde of Washington University in St. Louis, Missouri.[citation needed]\\r\\nOn July 4, 1940, during a professional appearance at the Manistee National Forest Festival, a faulty brace irritated his ankle, causing a blister and subsequent infection. Doctors treated him with a blood transfusion and emergency surgery, but his condition worsened due to an autoimmune disorder, and on July 15, 1940, 11 days after contracting the infection, he died in his sleep at the age of 22.[citation needed] His coffin measured 10?feet 9?inches (3.28?m) long by 32 inches (81?cm) wide by 30 inches (76?cm) deep, weighed 1,000 pounds (450?kg) and was carried by twelve pallbearers and eight assistants.[1][8][9] He was buried at the Oakwood Cemetery in Upper Alton, Madison County, Illinois.\\r\\nA life-size statue of Wadlow stands on College Avenue in Alton, opposite the Alton Museum of History and Art. It was erected in 1986, in honor of the well-known native.[1][10] Others stand in the Guinness Museums in Niagara Falls and Gatlinburg, as well as several of the Ripley's Believe It or Not Museums. A group of six life-size models of him, made by artist James Butler, exist, and are shipped and displayed in replica caskets.[11]\\r\\nAnother life-size statue of Wadlow may be viewed at Marvin's Marvelous Mechanical Museum in Farmington Hills, Michigan. In front of it is a small, quarter-operated \\"TV-box\\", which plays a short, documentary movie about his extraordinary short life.\\r\\nWadlow is still affectionately known as the \\"Gentle Giant\\".[12]\\r\\nThe 1998 song \\"The Giant of Illinois\\", by The Handsome Family (and later covered by Andrew Bird) honors Wadlow. In 2005, Sufjan Stevens recorded \\"The Tallest Man, the Broadest Shoulders\\" about Wadlow for the Illinois album. A picture of Wadlow with his family is featured on the back cover of the VHS version of the Talking Heads music video compilation, Storytelling Giant.","input":"Who was the tallest person that ever lived?"},{"output":"20 April 1964","context":"Nutella (/nu??t?l?/; Italian pronunciation:?[nu?-?t?lla]) is a brand of sweetened hazelnut cocoa spread[1] manufactured by the Italian company Ferrero that was first introduced in 1965, although its first iteration dates to 1963.[2]\\r\\n\\r\\n\\r\\nPietro Ferrero, who owned a bakery in Alba, Piedmont, an area known for the production of hazelnuts, sold an initial batch of 300 kilograms (660?lb) of \\"Pasta Gianduja\\" in 1946. At the time, there was very little chocolate because cocoa was in short supply due to World War II rationing.[3] Ferrero instead used hazelnuts, which are plentiful in the Piedmont region of Italy (northwest), to extend the limited chocolate supply. This product, called \\"Pasta Gianduja\\" was originally sold as a solid block, but Ferrero started to sell a creamy version in 1951 as \\"Supercrema\\".[4]\\r\\nIn 1963, Ferrero's son Michele Ferrero revamped Supercrema with the intention of marketing it throughout Europe. Its composition was modified and it was renamed \\"Nutella\\". The first jar of Nutella left the factory in Alba on 20 April 1964. The product was an instant success and remains widely popular.[5]\\r\\nIn 2012, French senator Yves Daudigny proposed a tax increase on palm oil from ?100 to ?400 per metric tonne. At 20 percent, palm oil is one of Nutella's main ingredients and the tax was dubbed \\"the Nutella tax\\" in the media.[6]\\r\\nWorld Nutella Day is February 5.[7]\\r\\nOn 14 May 2014, Poste italiane issued a 50th anniversary Nutella commemorative stamp.[8][9] The 70 Euro cent stamp was designed by Istituto Poligrafico e Zecca dello Stato and features a jar of Nutella on a golden background.[8] Ferrero held a Nutella Day on 17 and 18 May to celebrate the anniversary.[10]\\r\\nIt is currently available in more than 75 countries and since 2005 in Brazil.[11]\\r\\nThe main ingredients of Nutella consist of sugar and palm oil (greater than 50%). It also includes hazelnut at 13%,[12], cocoa solids and skimmed milk.[13] In the United States and the UK, Nutella contains soy products.[14] Nutella is marketed as \\"hazelnut cream\\" in many countries. Under Italian law, it cannot be labeled as a \\"chocolate cream\\", as it does not meet minimum cocoa solids concentration criteria. Ferrero uses 25 percent of the global supply of hazelnuts.[15]\\r\\nIn November 2017, the company modified the recipe slightly, increasing the sugar and skimmed milk powder content.[16] Since the colour of the product is lighter in tone, the Hamburg Consumer Protection Center estimated that the cocoa content was also reduced.[17] Some news outlets reported that the modification of the recipe led to consumers being \\"outraged\\" or \\"going nuts\\".[18]\\r\\nThe traditional Piedmont recipe, Gianduja, was a mixture containing approximately 71.5% hazelnut paste and 19.5% chocolate. It was developed in Piedmont, Italy due to a lack of cocoa beans due to post war rationing led to a lack of availability of the raw material.[19]\\r\\nNutella is produced in various facilities. In the North American market, it is produced at a plant in Brantford, Ontario in Canada [20] and more recently in San Jos Iturbide, Guanajuato, Mexico.[21]\\r\\nFor Australia and New Zealand, Nutella has been manufactured in Lithgow, New South Wales since the late 1970s.[22]\\r\\nTwo of the four Ferrero plants in Italy produce Nutella, in Alba, Piedmont, and in Sant'Angelo dei Lombardi in Campania.[23] In France, a production facility is located in Villers-calles.[24] For Eastern Europe (including Southeast Europe, Poland, Turkey, Czech Republic and Slovakia) and South Africa, it is produced in Warsaw and Manisa. For Germany and northern Europe, Nutella is produced at the Ferrero plant in Stadtallendorf, which has been in existence since 1956.[25]\\r\\nFerrero also has a plant in Brazil, which supplies the Brazilian market, with part of the production being exported overseas.[26]\\r\\nGlobal production in 2013 was about 350,000 tonnes.[27]\\r\\nWhile resembling a Chocolate spread, Nutella is a hazelnut spread that contains some cocoa.[28] The manufacturing process for this food item is very similar to a generic production of chocolate spread. Nutella is made from sugar, modified palm oil, hazelnuts, cocoa powder, skimmed milk powder, whey powder, lecithin, and vanillin.\\r\\nThe process of making this spread begins with the extraction of cocoa powder from the cocoa bean. These cocoa beans are harvested from cocoa trees and are left to dry for about ten days before being shipped for processing.[29] Typically cocoa beans contain approximately 50 percent of cocoa butter; therefore, they must be roasted to reduce the cocoa bean into a liquid form.[29] This step is not sufficient for turning cocoa bean into a chocolate paste because it solidifies at room temperature, and would not be spreadable. After the initial roast, the liquid paste is sent to presses, which are used to squeeze the butter out of the cocoa bean. The final products are round discs of chocolate made of pure compressed cocoa. The cocoa butter is transferred elsewhere so it can be used in other products.\\r\\nThe second process involves the hazelnuts. Once the hazelnuts have arrived at the processing plant, a quality control is issued to inspect the nuts so they are suitable for processing. A guillotine is used to chop the nuts to inspect the interior.[30] After this process, the hazelnuts are cleaned and roasted. A second quality control is issued by a computer-controlled blast of air, which removes the bad nuts from the batch.[30] This ensures that each jar of Nutella is uniform in its look and taste. Approximately 50 hazelnuts can be found in each jar of Nutella, as claimed by the company.[31]\\r\\nThe cocoa powder is then mixed with the hazelnuts along with sugar, vanillin and skim milk in a large tank until it becomes a paste-like spread.[32] Modified palm oil is then added to help retain the solid phase of the Nutella at room temperature, which substitutes for the butter found in the cocoa bean. Whey powder is then added to the mix to act as a binder for the paste. Whey powder is an additive commonly used in spreads to prevent the coagulation of the product because it stabilizes the fat emulsions.[33] Similarly, lecithin, a form of a fatty substance found in animal and plant tissues, is added to help emulsify the paste as it promotes homogenized mixing of the different ingredients, allowing the paste to become spreadable. It also aids the lipophilic properties of the cocoa powder which, again, keeps the product from separating.[31] Vanillin is added to enhance the sweetness of the chocolate. The finished product is then packaged.\\r\\nNutella contains 10.4 percent of saturated fat and 58% of processed sugar by weight. A two-tablespoon (37 gram) serving of Nutella contains 200 calories including 99 calories from 11 grams of fat (3.5g of which are saturated) and 80 calories from 21 grams of sugar. The spread also contains 15?mg of sodium and 2g of protein per serving (for reference a Canadian serving size is 1 tablespoon or 19 grams).[34][35]\\r\\nThe label states that Nutella does not need to be refrigerated. This is because the large quantity of sugar in the product acts as a preservative to prevent the growth of microorganisms. More specifically, the sugar acts as a preservative by binding the water in the product, which prevents the microorganisms from growing.[36] In fact, refrigeration causes Nutella to harden because it contains fats from the hazelnuts. When nut fats are placed in cold temperatures they become too hard to spread. Hazelnuts contain almost 91 percent monounsaturated fat, which are known to be liquid at room temperature and solidify at refrigerator temperatures.[37][38] Room temperature allow the product to have a smooth and spreadable consistency because the monounsaturated oils from the hazelnut are liquid at this state.[39] In addition, the palm oil used in Nutella does not require refrigeration because it contains high amounts of saturated fat and resists becoming rancid.[40] Therefore, Nutella can be stored at room temperature in the cabinet without going rancid before the Best Before date. The remaining ingredients in Nutella, such as cocoa, skimmed milk powder, soy lecithin, and vanillin also do not require refrigeration.[41][42] Hence, Nutella does not need to be put in the refrigerator after opening.\\r\\nIn the United States, Ferrero was sued in a class action for false advertising leading to consumer inferences that Nutella has nutritional and health benefits (from advertising claims that Nutella was \\"part of a nutritious breakfast\\").\\r\\nIn April 2012, Ferrero agreed to pay a $3 million settlement (up to $4 per jar for up to five jars in returns by customers). The settlement also required Ferrero to make changes to Nutella's labeling and marketing, including television commercials and their website.[43]","input":"When was nutella first sold in the uk?"},{"output":"4 years of full-time study","context":"The Doctor of Business Administration (abbreviated DBA, D.B.A., DrBA, or Dr.B.A.) is a research doctorate awarded on the basis of advanced study and research in the field of business administration. Along with research skills the doctorate focuses on business intelligence and original theoretical study.[1] The D.B.A. is a terminal degree in business administration.[2] Along with the Ph.D, it represents the highest academic qualification in business administration. Successful completion of a D.B.A. or Ph.D in business administration is required to gain employment as a full-time, tenure-track university professor or postdoctoral researcher in the field. As with other earned research doctorates, individuals with the degree are awarded the academic title doctor, which is often represented via the English honorific \\"Dr.\\" or the post-nominal letters \\"D.B.A.\\", \\"DBA\\", \\"Dr.B.A.\\", or \\"DrBA\\".\\r\\nD.B.A. candidates submit a significant project, typically referred to as a thesis or dissertation, consisting of a body of original academic research that is in principle worthy of publication in a peer-reviewed journal.[3] Candidates must defend this work before a panel of expert examiners called a thesis, dissertation, or doctoral committee.[4]\\r\\n\\r\\n\\r\\nDoctor of Business Administration programs have a dual purpose: contribute to business theory and further develop the professional practice (e.g. contribute to professional knowledge in business). Universities generally require candidates to have significant experience in business, particularly in roles with leadership or other strategic responsibilities. D.B.A. candidates specialize in areas such as management science, information technology management, organizational behaviour, economics, finance and the like. As with other doctorate programs, curricula may be offered on a full-time or part-time basis. According to the European higher education standards set by the Bologna Process, the normal duration of doctorate programs like the D.B.A. and Ph.D is 4 years of full-time study.\\r\\nThe responsibility for the structure of doctoral programs resides within the graduate research committees or their equivalent within the university. As such, D.B.A programs have a specific set of university regulations and are subject to quality approval processes. Regulations include references to protocols for treating ethical issues in research. These regulations are widely used in Australian Universities. For instance, a D.B.A student cannot embark on the research phase before passing all his or her coursework. Furthermore, upon passing the proposal stage, he or she must clear ethics-related issues with an Ethics Committee. The D.B.A candidate must go through numerous internal moderations of the dissertation before submitting to external examinations (usually at least two). Successful candidates usually revise their dissertations numerous times before final approval is granted from the doctoral committee.\\r\\nThe Doctor of Business Administration and the Ph.D in Business Administration are equivalent degrees.[2] Both doctorates are research doctorates representing the highest academic qualification in business in the United States. As such, both D.B.A. and Ph.D programs require students to develop original research leading to a dissertation defense.[4] Furthermore, both doctorates enable holders to become faculty members at academic institutions. The D.B.A. and Ph.D in Business Administration are terminal degrees, allowing the recipient to obtain a tenure-track position.\\r\\nIn some cases, as in that of Harvard University, the distinction is solely administrative (Harvard Business School is not authorized to issue Ph.Ds; only the Faculty of Arts and Sciences may do so).[5] In other cases,the distinction is one of orientation and intended outcomes. The Ph.D is highly focused on developing theoretical knowledge, while the D.B.A. emphasizes applied research.[4][6][7] Upon completion, graduates of Ph.D programs generally migrate to full-time faculty positions in academia, while those of D.B.A. programs re-emerge in industry as applied researchers or executives. If working full-time in industry, graduates of D.B.A. and Ph.D programs often become adjunct professors in top undergraduate and graduate degree programs.\\r\\nD.B.A. programs are offered worldwide. The majority, however, are offered in Europe (42 percent), followed by North America (28 percent) and the Asia-Pacific region (22 percent). 6 percent of the programs are offered in Africa and 2 percent in Latin America and the Caribbean.","input":"How many years is a doctorate degree in business administration?"},{"output":"Chynna Phillips","context":"Wilson Phillips is an American vocal group consisting of Carnie Wilson, Wendy Wilson, and Chynna Phillips, the daughters, respectively, of Brian Wilson of The Beach Boys and of John and Michelle Phillips of The Mamas & the Papas.\\r\\n\\r\\nTheir 1990 eponymous debut album sold over 10 million copies worldwide and placed three number-one singles on the Billboard Hot 100, making the trio the best-selling female group at the time. In 1990, the group won the Billboard Music Award for Hot 100 Single of the Year for \\"Hold On,\\" and in addition was nominated for five Grammy Awards and two American Music Awards.[1]\\r\\n\\r\\nThe Wilson sisters and Phillips grew up together in Southern California in the 1970s and 1980s. The three shared a love of music and songwriting, and developed their own style of vocal harmonies. In 1989, the trio signed with SBK Records.\\r\\nThe group is also known for being the offspring of prominent musicians; the Wilsons are the daughters of Brian Wilson of The Beach Boys and Marilyn Rovell of The Honeys, while Phillips is the daughter of John and Michelle Phillips of The Mamas & the Papas. Wilson Phillips released their debut album, Wilson Phillips, in 1990. Their debut single, \\"Hold On,\\" hit number one on the U.S. Billboard Hot 100 chart on June 9, 1990, bumping Madonna's \\"Vogue\\" off the top spot.[2] The single was also number one  on the US Billboard Adult Contemporary and became a worldwide hit, peaking at number two  in Australia, number six in the UK, number seven in Ireland, number 10 in Sweden, and  number 15 in the Netherlands, Germany, and Switzerland.[3] The single also won Wilson Phillips the Billboard Music Award for Hot 100 Single of the Year for 1990.[4] The second and fourth singles from the album also became number-one singles on the Billboard Hot 100: \\"Release Me\\" (two weeks) and \\"You're in Love\\" (one week). The singles \\"Impulsive\\" and \\"The Dream Is Still Alive\\", peaked at number four and number 12, respectively, on the Hot 100.[5]\\r\\n\\r\\nIn 1991, they contributed a recording of \\"Daniel\\" to the tribute album Two Rooms: Celebrating the Songs of Elton John & Bernie Taupin.  While not released as a single, it peaked at number seven in the US Adult Contemporary chart due to strong radio airplay.[citation needed] They also released the song \\"You're in Love\\" the same year, which scored as a number one (and to date, their last top-10) pop single.[6] In 1992, Wilson Phillips also made history as Billboard declared their debut album the best-selling album of all time by an all female group, peaking at number two on the Billboard 200 album chart, and selling over 5 million copies in the US and over 10 million copies worldwide, which also made Wilson Phillips, at the time, the best-selling female group of all time for a single album, breaking the previous record set by The Supremes.[7]\\r\\n\\r\\nIn June 1992, Wilson Phillips released their second album, Shadows and Light. The album was deeply personal and adopted a more serious tone, with tracks exploring issues such as child abuse (\\"Where Are You\\") and their estrangement from their fathers (\\"Flesh and Blood\\", \\"All the Way From New York\\"). The first single, \\"You Won't See Me Cry\\" peaked at number 20 in the US and number 18 in the UK, the first time they had a higher-ranking single in the UK than in the US. The album peaked at number four on the Billboard 200 and was certified platinum in the US, and double platinum in Canada.[citation needed] Shortly after the release of Shadows and Light in 1992, Chynna Phillips announced plans for a solo career, and the group decided to disband.[8]\\r\\n\\r\\nDuring the next decade, the group went their separate ways. In 1993, Wendy and Carnie Wilson released the Christmas holiday-themed album, Hey Santa!, a collection of classic holiday themes with an original song (the title track). In 1995, Chynna Phillips released a solo album, Naked and Sacred. The same year, Carnie went on to host her own short-lived syndicated television talk show titled, Carnie!.[9] In 1997, Carnie and Wendy, along with their father Brian Wilson, released The Wilsons and the single \\"Monday Without You.\\"[10] Capitol Records released the compilation Greatest Hits in 2000, although the group did not officially reunite for the release. On March 29, 2001, they reunited to perform The Beach Boys' song \\"You're So Good to Me\\" at a tribute show to Brian Wilson held at New York's Radio City Music Hall. Chynna Phillips dedicated the performance to her own father John Phillips, who had died a few days earlier.[citation needed]\\r\\n\\r\\nWilson Phillips reunited in 2004 to release California, an album of cover songs. A single, \\"Go Your Own Way,\\" a song originally recorded by Fleetwood Mac, peaked at number 13 on the Billboard Adult Contemporary chart. The album debuted at number 35 on the Billboard 200 chart with 31,000 copies sold in its first week of release. In New Zealand, the album was a surprise hit, reaching the top 10 and amassing gold sales after \\"Go Your Own Way\\" topped the country's adult contemporary radio chart for several weeks.[11]\\r\\n\\r\\nOn October 12, 2010, Wilson Phillips released a new Christmas album, Christmas in Harmony. The album yielded a single, \\"I Wish It Could Be Christmas Everyday,\\" a cover of the popular seasonal tune that first was a hit in 1973 for English glam-rock band Wizzard. The band appeared in a cameo and performed \\"Hold On\\" in the film Bridesmaids (2011).[12][13][14] Bandmember Chynna Phillips was a contestant on season 13 of ABC's Dancing With the Stars and explained that \\"Hold On\\" was written by  bandmates Carnie and Wendy Wilson and her about Phillips' recovery from drug and alcohol addiction. Phillips was eliminated after week four.[15]\\r\\n\\r\\nOn March 20, 2012, the group appeared on QVC to promote their release Dedicated (2012), a new studio album composed of covers of songs by both The Beach Boys and The Mamas and the Papas. On April 3, 2012, the group released Dedicated, which peaked at number 29 on the Billboard 200 chart.[16] The trio appeared in their own reality show, Wilson Phillips: Still Holding On, which debuted on TV Guide Network in April 2012.[17] In 2015, Wilson Phillips contributed backup vocals to the song FourFiveSeconds by Rihanna, Kanye West and Paul McCartney for Rihanna's eighth studio album.[18]\\r\\n\\r\\nIn July 2016, Wilson Phillips reunited and performed on ABC's Greatest Hits.[19] In 2017, the group performed on the season finale of NBC's The New Celebrity Apprentice.[20]","input":"Who are the band members of wilson phillips?"},{"output":"named after their oxides","context":"The alkaline earth metals are six chemical elements in group 2 of the periodic table. They are beryllium (Be), magnesium (Mg), calcium (Ca), strontium (Sr), barium (Ba), and radium (Ra).[1] The elements have very similar properties: they are all shiny, silvery-white, somewhat reactive metals at standard temperature and pressure.[2]\\r\\nStructurally, they have in common an outer s- electron shell which is full;[2][3][4] that is, this orbital contains its full complement of two electrons, which these elements readily lose to form cations with charge +2, and an oxidation state of +2.[5]\\r\\nAll the discovered alkaline earth metals occur in nature.[6] Experiments have been conducted to attempt the synthesis of element 120, the next potential member of the group, but they have all met with failure.\\r\\n\\r\\n\\r\\nAs with other groups, the members of this family show patterns in their electronic configuration, especially the outermost shells, resulting in trends in chemical behavior:\\r\\nMost of the chemistry has been observed only for the first five members of the group. The chemistry of radium is not well-established due to its radioactivity;[2] thus, the presentation of its properties here is limited.\\r\\nThe alkaline earth metals are all silver-colored and soft, and have relatively low densities, melting points, and boiling points. In chemical terms, all of the alkaline metals react with the halogens to form the alkaline earth metal halides, all of which are ionic crystalline compounds (except for beryllium chloride, which is covalent). All the alkaline earth metals except beryllium also react with water to form strongly alkaline hydroxides and, thus, should be handled with great care. The heavier alkaline earth metals react more vigorously than the lighter ones.[2] The alkaline metals have the second-lowest first ionization energies in their respective periods of the periodic table[4] because of their somewhat low effective nuclear charges and the ability to attain a full outer shell configuration by losing just two electrons. The second ionization energy of all of the alkaline metals is also somewhat low.[2][4]\\r\\nBeryllium is an exception: It does not react with water or steam, and its halides are covalent. If beryllium did form compounds with an ionization state of +2, it would polarize electron clouds that are near it very strongly and would cause extensive orbital overlap, since beryllium has a high charge density. All compounds that include beryllium have a covalent bond.[7] Even the compound beryllium fluoride, which is the most ionic beryllium compound, has a low melting point and a low electrical conductivity when melted.[8][9][10]\\r\\nAll the alkaline earth metals have two electrons in their valence shell, so the energetically preferred state of achieving a filled electron shell is to lose two electrons to form doubly charged positive ions.\\r\\nThe alkaline earth metals all react with the halogens to form ionic halides, such as calcium chloride (CaCl\\r\\n2), as well as reacting with oxygen to form oxides such as strontium oxide (SrO). Calcium, strontium, and barium react with water to produce hydrogen gas and their respective hydroxides, and also undergo transmetalation reactions to exchange ligands.\\r\\nThe table below is a summary of the key physical and atomic properties of the alkaline earth metals.\\r\\nOf the six alkaline earth metals, beryllium, calcium, barium, and radium have at least one naturally occurring radioisotope; magnesium and strontium do not. Beryllium-7, beryllium-10, and calcium-41 are trace radioisotopes; calcium-48 and barium-130 have very long half-lives and thus occur naturally on earth; and all isotopes of radium are radioactive. Calcium-48 is the lightest nuclide to undergo double beta decay.[21] Calcium and barium are weakly radioactive: calcium contains about 0.1874% cacium-41,[22] and barium contains about 0.1062% barium-130.[23]\\r\\nThe alkaline earth metals are named after their oxides, the alkaline earths, whose old-fashioned names were beryllia, magnesia, lime, strontia, and baryta. These oxides are basic (alkaline) when combined with water. \\"Earth\\" is an old term applied by early chemists to nonmetallic substances that are insoluble in water and resistant to heatingproperties shared by these oxides. The realization that these earths were not elements but compounds is attributed to the chemist Antoine Lavoisier. In his Trait lmentaire de Chimie (Elements of Chemistry) of 1789 he called them salt-forming earth elements. Later, he suggested that the alkaline earths might be metal oxides, but admitted that this was mere conjecture. In 1808, acting on Lavoisier's idea, Humphry Davy became the first to obtain samples of the metals by electrolysis of their molten earths,[24] thus supporting Lavoisier's hypothesis and causing the group to be named the alkaline earth metals.\\r\\nThe calcium compounds calcite and lime have been known and used since prehistoric times.[25] The same is true for the beryllium compounds beryl and emerald.[26] The other compounds of the alkaline earth metals were discovered starting in the early 15th century. The magnesium compound magnesium sulfate was first discovered in 1618 by a farmer at Epsom in England. Strontium carbonate was discovered in minerals in the Scottish village of Strontian in 1790. The last element is the least abundant: radioactive radium, which was extracted from uraninite in 1898.[27][28][29]\\r\\nAll elements except beryllium were isolated by electrolysis of molten compounds. Magnesium, calcium, and strontium were first produced by Humphry Davy in 1808, whereas beryllium was independently isolated by Friedrich W?hler and Antoine Bussy in 1828 by reacting beryllium compounds with potassium. In 1910, radium was isolated as a pure metal by Curie and Andr-Louis Debierne also by electrolysis.[27][28][29]\\r\\nBeryl, a mineral that contains beryllium, has been known since the time of the Ptolemaic Kingdom in Egypt.[26] Although it was originally thought that beryl was an aluminium silicate,[30] beryl was later found to contain a then-unknown element when, in 1797, Louis-Nicolas Vauquelin dissolved aluminium hydroxide from beryl in an alkali.[31] In 1828, Friedrich W?hler[32] and Antoine Bussy[33] independently isolated this new element, beryllium, by the same method, which involved a reaction of beryllium chloride with metallic potassium; this reaction was not able to produce large ingots of beryllium.[34] It was not until 1898, when Paul Lebeau performed an electrolysis of a mixture of beryllium fluoride and sodium fluoride, that large pure samples of beryllium were produced.[34]\\r\\nMagnesium was first produced by Sir Humphry Davy in England in 1808 using electrolysis of a mixture of magnesia and mercuric oxide.[35] Antoine Bussy prepared it in coherent form in 1831. Davys first suggestion for a name was magnium,[35] but the name magnesium is now used.\\r\\nLime has been used as a material for building since 7000 to 14,000?BCE,[25] and kilns used for lime have been dated to 2,500?BCE in Khafaja, Mesopotamia.[36][37] Calcium as a material has been known since at least the first century, as the ancient Romans were known to have used calcium oxide by preparing it from lime. Calcium sulfate has been known to be able to set broken bones since the tenth century. Calcium itself, however, was not isolated until 1808, when Humphry Davy, in England, used electrolysis on a mixture of lime and mercuric oxide,[38] after hearing that J?ns Jakob Berzelius had prepared a calcium amalgam from the electrolysis of lime in mercury.\\r\\nIn 1790, physician Adair Crawford, who had been working with barium, realized that Strontian ores showed different properties than other supposed ores of barium.[39] Therefore, he concluded that these ores contained new minerals, which were named strontites in 1793 by Thomas Charles Hope, a chemistry professor at the University of Glasgow,[40] who confirmed Crawford's discovery. Strontium was eventually isolated in 1808 by Sir Humphry Davy by electrolysis of a mixture of strontium chloride and mercuric oxide. The discovery was announced by Davy on 30 June 1808 at a lecture to the Royal Society.[41]\\r\\nBarite, a mineral containing barium, was first recognized as containing a new element in 1774 by Carl Scheele, although he was able to isolate only barium oxide. Barium oxide was isolated again two years later by Johan Gottlieb Gahn. Later in the 18th century, William Withering noticed a heavy mineral in the Cumberland lead mines, which are now known to contain barium. Barium itself was finally isolated in 1808 when Sir Humphry Davy used electrolysis with molten salts, and Davy named the element barium, after baryta. Later, Robert Bunsen and Augustus Matthiessen isolated pure barium by electrolysis of a mixture of barium chloride and ammonium chloride.[42][43]\\r\\nWhile studying uraninite, on 21 December 1898, Marie and Pierre Curie discovered that, even after uranium had decayed, the material created was still radioactive. The material behaved somewhat similarly to barium compounds, although some properties, such as the color of the flame test and spectral lines, were much different. They announced the discovery of a new element on 26 December 1898 to the French Academy of Sciences.[44] Radium was named in 1899 from the word radius, meaning ray, as radium emitted power in the form of rays.[45]\\r\\nBeryllium occurs in the earth's crust at a concentration of two to six parts per million (ppm),[46] much of which is in soils, where it has a concentration of six ppm. Beryllium is one of the rarest elements in seawater, even rarer than elements such as scandium, with a concentration of 0.2 parts per trillion.[47][48] However, in freshwater, beryllium is somewhat more common, with a concentration of 0.1 parts per billion.[49]\\r\\nMagnesium and calcium are very common in the earth's crust, with calcium the fifth-most-abundant element, and magnesium the eighth. None of the alkaline earth metals are found in their elemental state, but magnesium and calcium are found in many rocks and minerals: magnesium in carnallite, magnesite, and dolomite; and calcium in chalk, limestone, gypsum, and anhydrite.[2]\\r\\nStrontium is the fifteenth-most-abundant element in the Earth's crust. Most strontium is found in the minerals celestite and strontianite.[50] Barium is slightly less common, much of it in the mineral barite.[51]\\r\\nRadium, being a decay product of uranium, is found in all uranium-bearing ores.[52] Due to its relatively short half-life,[53] radium from the Earth's early history has decayed, and present-day samples have all come from the much slower decay of uranium.[52]\\r\\nMost beryllium is extracted from beryllium hydroxide. One production method is sintering, done by mixing beryl, sodium fluorosilicate, and soda at high temperatures to form sodium fluoroberyllate, aluminium oxide, and silicon dioxide. A solution of sodium fluoroberyllate and sodium hydroxide in water is then used to form beryllium hydroxide by precipitation. Alternatively, in the melt method, powdered beryl is heated to high temperature, cooled with water, then heated again slightly in sulfuric acid, eventually yielding beryllium hydroxide. The beryllium hydroxide from either method then produces beryllium fluoride and beryllium chloride through a somewhat long process. Electrolysis or heating of these compounds can then produce beryllium.[7]\\r\\nIn general, strontium carbonate is extracted from the mineral celestite through two methods: by leaching the celestite with sodium carbonate, or in a more complicated way involving coal.[54]\\r\\nTo produce barium, barite ore is separated from quartz, sometimes by froth flotation methods, resulting in relatively pure barite. Carbon is then used to reduce the baryte into barium sulfide, which is dissolved with other elements to form other compounds, such as barium nitrate. These in turn are thermally decompressed into barium oxide, which eventually yields pure barium after a reaction with aluminium.[51] The most important supplier of barium is China, which produces more than 50% of world supply.[55]\\r\\nBeryllium is used mostly for military applications,[56] but there are other uses of beryllium, as well. In electronics, beryllium is used as a p-type dopant in some semiconductors,[57] and beryllium oxide is used as a high-strength electrical insulator and heat conductor.[58] Due to its light weight and other properties, beryllium is also used in mechanics when stiffness, light weight, and dimensional stability are required at wide temperature ranges.[59][60]\\r\\nMagnesium has many different uses. One of its most common uses was in industry, where it has many structural advantages over other materials such as aluminium, although this usage has fallen out of favor recently due to magnesium's flammability.[61] Magnesium is also often alloyed with aluminium or zinc to form materials with more desirable properties than any pure metal.[62] Magnesium has many other uses in industrial applications, such as having a role in the production of iron and steel, and the production of titanium.[63]\\r\\nCalcium also has many uses. One of its uses is as a reducing agent in the separation of other metals from ore, such as uranium. It is also used in the production of the alloys of many metals, such as aluminium and copper alloys, and is also used to deoxidize alloys as well. Calcium also has a role in the making of cheese, mortars, and cement.[64]\\r\\nStrontium and barium do not have as many applications as the lighter alkaline earth metals, but still have uses. Strontium carbonate is often used in the manufacturing of red fireworks,[65] and pure strontium is used in the study of neurotransmitter release in neurons.[66][67] Barium has some use in vacuum tubes to remove gases,[51] and barium sulfate has many uses in the petroleum industry,[4] as well as other industries.[4][51][68]\\r\\nDue to its radioactivity, radium no longer has many applications, but it used to have many. Radium used to be used often in luminous paints,[69] although this use was stopped after workers got sick.[70] As people used to think that radioactivity was a good thing, radium used to be added to drinking water, toothpaste, and many other products, although they are also not used anymore due to their health effects.[61] Radium is no longer even used for its radioactive properties, as there are more powerful and safer emitters than radium.[71][72]\\r\\nMagnesium and calcium are ubiquitous and essential to all known living organisms. They are involved in more than one role, with, for example, magnesium or calcium ion pumps playing a role in some cellular processes, magnesium functioning as the active center in some enzymes, and calcium salts taking a structural role, most notably in bones.\\r\\nStrontium plays an important role in marine aquatic life, especially hard corals, which use strontium to build their exoskeletons. It and barium have some uses in medicine, for example \\"barium meals\\" in radiographic imaging, whilst strontium compounds are employed in some toothpastes. Excessive amounts of strontium-90 are toxic due to its radioactivity and strontium-90 mimics calcium and then can kill.\\r\\nBeryllium and radium, however, are toxic. Beryllium's low aqueous solubility means it is rarely available to biological systems; it has no known role in living organisms and, when encountered by them, is usually highly toxic.[7] Radium has a low availability and is highly radioactive, making it toxic to life.\\r\\nThe next alkaline earth metal after radium is thought to be element 120, although this may not be true due to relativistic effects.[73] The synthesis of element 120 was first attempted in March 2007, when a team at the Flerov Laboratory of Nuclear Reactions in Dubna bombarded plutonium-244 with iron-58 ions; however, no atoms were produced, leading to a limit of 400 fb for the cross-section at the energy studied.[74] In April 2007, a team at the GSI attempted to create element 120 by bombarding uranium-238 with nickel-64, although no atoms were detected, leading to a limit of 1.6 pb for the reaction. Synthesis was again attempted at higher sensitivities, although no atoms were detected. Other reactions have been tried, although all have been met with failure.[75]\\r\\nThe chemistry of element 120 is predicted to be closer to that of calcium or strontium[76] instead of barium or radium. This is unusual as periodic trends would predict element 120 to be more reactive than barium and radium. This lowered reactivity is due to the expected energies of element 120's valence electrons, increasing element 120's ionization energy and decreasing the metallic and ionic radii.[76]\\r\\nAlthough simple extrapolation would put element 170 as the next member of this series, calculations suggest that the next element of this series might actually be element 166.\\r\\nBeryllium\\r\\nBe\\r\\nAtomic Number: 4\\r\\nAtomic Weight: 9.012182\\r\\nMelting Point: 1560.15 K\\r\\nBoiling Point: 2742 K\\r\\nSpecific mass: 1.85 g/cm3\\r\\nElectronegativity: 1.57\\r\\nMagnesium\\r\\nMg\\r\\nAtomic Number: 12\\r\\nAtomic Weight: 24.3050\\r\\nMelting Point: 923.15 K\\r\\nBoiling Point: 1363 K\\r\\nSpecific mass: 1.738 g/cm3\\r\\nElectronegativity: 1.31\\r\\nCalcium\\r\\nCa\\r\\nAtomic Number: 20\\r\\nAtomic Weight: 40.078\\r\\nMelting Point: 1112.15 K\\r\\nBoiling Point: 1757 K\\r\\nSpecific mass: 1.54 g/cm3\\r\\nElectronegativity: 1\\r\\nStrontium\\r\\nSr\\r\\nAtomic Number: 38\\r\\nAtomic Weight: 87.62\\r\\nMelting Point: 1042.15 K\\r\\nBoiling Point: 1655 K\\r\\nSpecific mass: 2.64 g/cm3\\r\\nElectronegativity: 0.95\\r\\nBarium\\r\\nBa\\r\\nAtomic Number: 56\\r\\nAtomic Weight: 137.327\\r\\nMelting Point: 1002.15 K\\r\\nBoiling Point: 2170 K\\r\\nSpecific mass: 3.594 g/cm3\\r\\nElectronegativity: 0.89\\r\\nRadium\\r\\nRa\\r\\nAtomic Number: 88\\r\\nAtomic Weight: [226]\\r\\nMelting Point: 973.15 K\\r\\nBoiling Point: 2010 K\\r\\nSpecific mass: 5.5 g/cm3\\r\\nElectronegativity: 0.9","input":"Why group 2 elements called alkaline earth metals?"},{"output":"1920s","context":"Plasma (from Ancient Greek ?ϫ?, meaning 'moldable substance'[1]) is one of the four fundamental states of matter, and was first described by chemist Irving Langmuir[2] in the 1920s.[3] Unlike the other three states, solid, liquid, and gas, plasma does not exist freely on the Earth's surface under normal conditions. Plasma can only be artificially generated by heating or subjecting a neutral gas to a strong electromagnetic field to the point an ionised gaseous substance becomes increasingly electrically conductive, and long-range electromagnetic fields dominate the behavior of the matter.[4]\\r\\nPlasma and ionised gases have unique properties and display behaviors unlike those of the other states, and the transition between them is mostly a matter of nomenclature[2] and subject to interpretation.[5] Based on the surrounding environmental temperature and density, partially ionised or fully ionised forms of plasma may be produced. Neon signs or lightning storms are examples of partially ionised plasma,[6] while the interior of the Sun is an example of fully ionised plasma,[7] along with the solar corona[8] and stars.[9]\\r\\nPositive charges in ions is achieved by stripping away electrons orbiting the atomic nuclei, where the total number of electrons removed is only related to either increasing temperature or the local density of other ionised matter. This also can be accompanied by the dissociation of molecular bonds[10], though this process is distinctly different from chemical processes of ion interactions in liquids or the behavior of shared ions in metals. The response of plasma to electromagnetic fields can be usefully employed in many modern technological devices, such as plasma televisions or plasma etching.[11]\\r\\nPlasma may be the most abundant form of ordinary matter in the universe,[12] although this hypothesis is currently tentative based on the existence and unknown properties of dark matter. Plasma is mostly associated with stars, extending to the rarefied intracluster medium and possibly the intergalactic regions.[13]\\r\\n\\r\\n\\r\\nThe word plasma comes from the Ancient Greek ?ϫ, meaning \\"moldable substance\\"[1] or \\"jelly\\",[2] whose usage describes the behaviour of the ionised atomic nuclei and the electrons within the surrounding region of the plasma. Very simply, each of these nuclei are suspended in a movable sea of electrons. Plasma was first identified in a Crookes tube, and so described by Sir William Crookes in 1879 (he called it \\"radiant matter\\").[14] The nature of this \\"cathode ray\\" matter was subsequently identified by British physicist Sir J.J. Thomson in 1897.[15]\\r\\nThe term \\"plasma\\" was coined by Irving Langmuir in 1928.[16] Lewi Tonks and Harold Mott-Smith, both of whom worked with Irving Langmuir in the 1920s, recall that Langmuir first used the word \\"plasma\\" in analogy with blood.[17][18] Mott-Smith recalls, in particular, that the transport of electrons from thermionic filaments reminded Langmuir of the way blood plasma carries red and white corpuscles and germs.[19]\\r\\nLangmuir described the plasma he observed as follows:\\r\\nPlasma is a state of matter in which an ionised gaseous substance becomes highly electrically conductive to the point that long-range electric and magnetic fields dominate the behavior of the matter.[21][22] This state can be contrasted with the other states: solid, liquid, and gas. Unlike these other states of matter, plasma mostly does not naturally exist on the Earth's surface under normal conditions, and must be artificially generated from neutral gases.[4]\\r\\nPlasma is an electrically neutral medium of unbound positive and negative particles (i.e. the overall charge of a plasma is roughly zero). Although these particles are unbound, they are not free in the sense of not experiencing forces. Moving charged particles generate an electric current within a magnetic field, and any movement of a charged plasma particle affects and is affected by the fields created by the other charges. In turn this governs collective behavior with many degrees of variation.[10][23] Three factors define a plasma:[24][25]\\r\\nPlasma temperature is commonly measured in kelvins or electronvolts and is, informally, a measure of the thermal kinetic energy per particle. High temperatures are usually needed to sustain ionisation, which is a defining feature of a plasma. The degree of plasma ionisation is determined by the electron temperature relative to the ionization energy (and more weakly by the density), in a relationship called the Saha equation. At low temperatures, ions and electrons tend to recombine into bound statesatoms[29]and the plasma will eventually become a gas.\\r\\nIn most cases the electrons are close enough to thermal equilibrium that their temperature is relatively well-defined, even when there is a significant deviation from a Maxwellian energy distribution function, for example, due to UV radiation, energetic particles, or strong electric fields. Because of the large difference in mass, the electrons come to thermodynamic equilibrium amongst themselves much faster than they come into equilibrium with the ions or neutral atoms. For this reason, the ion temperature may be very different from (usually lower than) the electron temperature. This is especially common in weakly ionised technological plasmas, where the ions are often near the ambient temperature.\\r\\nFor plasma to exist, ionisation is necessary. The term \\"plasma density\\" by itself usually refers to the \\"electron density\\", that is, the number of free electrons per unit volume. The degree of ionisation of a plasma is the proportion of atoms that have lost or gained electrons, and is controlled by the electron and ion temperatures and electron-ion vs electron-neutral collision frequencies. The degree of ionisation, \\r\\n\\r\\n\\r\\n\\r\\nϫ\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\alpha }\\r\\n\\r\\n, is defined as \\r\\n\\r\\n\\r\\n\\r\\nϫ\\r\\n=\\r\\n\\r\\n\\r\\n\\r\\nn\\r\\n\\r\\ni\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nn\\r\\n\\r\\ni\\r\\n\\r\\n\\r\\n+\\r\\n\\r\\nn\\r\\n\\r\\nn\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\alpha ={\\\\frac {n_{i}}{n_{i}+n_{n}}}}\\r\\n\\r\\n, where \\r\\n\\r\\n\\r\\n\\r\\n\\r\\nn\\r\\n\\r\\ni\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle n_{i}}\\r\\n\\r\\n is the number density of ions and \\r\\n\\r\\n\\r\\n\\r\\n\\r\\nn\\r\\n\\r\\nn\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle n_{n}}\\r\\n\\r\\n is the number density of neutral atoms. The electron density is related to this by the average charge state \\r\\n\\r\\n\\r\\n\\r\\n?\\r\\nZ\\r\\n?\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\langle Z\\\\rangle }\\r\\n\\r\\n of the ions through \\r\\n\\r\\n\\r\\n\\r\\n\\r\\nn\\r\\n\\r\\ne\\r\\n\\r\\n\\r\\n=\\r\\n?\\r\\nZ\\r\\n?\\r\\n\\r\\nn\\r\\n\\r\\ni\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle n_{e}=\\\\langle Z\\\\rangle n_{i}}\\r\\n\\r\\n, where \\r\\n\\r\\n\\r\\n\\r\\n\\r\\nn\\r\\n\\r\\ne\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle n_{e}}\\r\\n\\r\\n is the number density of electrons.\\r\\nIn a plasma, the electron-ion collision frequency \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\ne\\r\\ni\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\nu _{ei}}\\r\\n\\r\\n is much greater than the electron-neutral collision frequency \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\ne\\r\\nn\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\nu _{en}}\\r\\n\\r\\n. Therefore, with a weak degree of ionization \\r\\n\\r\\n\\r\\n\\r\\nϫ\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\alpha }\\r\\n\\r\\n, the electron-ion collision frequency can equal the electron-neutral collision frequency: \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\ne\\r\\ni\\r\\n\\r\\n\\r\\n=\\r\\n\\r\\n\\r\\n\\r\\ne\\r\\nn\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\nu _{ei}=\\\\nu _{en}}\\r\\n\\r\\n is the limit separating a plasma from being partially or fully ionized.\\r\\nMost of \\"technological\\" (engineered) plasmas are weakly ionized gases.\\r\\nBased on the relative temperatures of the electrons, ions and neutrals, plasmas are classified as \\"thermal\\" or \\"non-thermal\\" (also referred to as \\"cold plasmas\\").\\r\\nA particular and unusual case of \\"inverse\\" nonthermal plasma is the very high temperature plasma produced by the Z machine, where ions are much hotter than electrons.[32][33]\\r\\nSince plasmas are very good electrical conductors, electric potentials play an important role.[clarification needed] The potential as it exists on average in the space between charged particles, independent of the question of how it can be measured, is called the \\"plasma potential\\", or the \\"space potential\\". If an electrode is inserted into a plasma, its potential will generally lie considerably below the plasma potential due to what is termed a Debye sheath. The good electrical conductivity of plasmas makes their electric fields very small. This results in the important concept of \\"quasineutrality\\", which says the density of negative charges is approximately equal to the density of positive charges over large volumes of the plasma (\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nn\\r\\n\\r\\ne\\r\\n\\r\\n\\r\\n=\\r\\n?\\r\\nZ\\r\\n?\\r\\n\\r\\nn\\r\\n\\r\\ni\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle n_{e}=\\\\langle Z\\\\rangle n_{i}}\\r\\n\\r\\n), but on the scale of the Debye length there can be charge imbalance. In the special case that double layers are formed, the charge separation can extend some tens of Debye lengths.[citation needed]\\r\\nThe magnitude of the potentials and electric fields must be determined by means other than simply finding the net charge density. A common example is to assume that the electrons satisfy the Boltzmann relation:\\r\\nDifferentiating this relation provides a means to calculate the electric field from the density:\\r\\nIt is possible to produce a plasma that is not quasineutral. An electron beam, for example, has only negative charges. The density of a non-neutral plasma must generally be very low, or it must be very small, otherwise it will be dissipated by the repulsive electrostatic force.[35]\\r\\nIn astrophysical plasmas, Debye screening prevents electric fields from directly affecting the plasma over large distances, i.e., greater than the Debye length. However, the existence of charged particles causes the plasma to generate, and be affected by, magnetic fields. This can and does cause extremely complex behavior, such as the generation of plasma double layers, an object that separates charge over a few tens of Debye lengths. The dynamics of plasmas interacting with external and self-generated magnetic fields are studied in the academic discipline of magnetohydrodynamics.[36]\\r\\nPlasma with a magnetic field strong enough to influence the motion of the charged particles is said to be magnetized. A common quantitative criterion is that a particle on average completes at least one gyration around the magnetic field before making a collision, i.e., \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nc\\r\\ne\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n/\\r\\n\\r\\n\\r\\nv\\r\\n\\r\\n\\r\\nc\\r\\no\\r\\nl\\r\\nl\\r\\n\\r\\n\\r\\n\\r\\n>\\r\\n1\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\omega _{\\\\mathrm {ce} }/v_{\\\\mathrm {coll} }>1}\\r\\n\\r\\n, where \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nc\\r\\ne\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\omega _{\\\\mathrm {ce} }}\\r\\n\\r\\n is the \\"electron gyrofrequency\\" and \\r\\n\\r\\n\\r\\n\\r\\n\\r\\nv\\r\\n\\r\\n\\r\\nc\\r\\no\\r\\nl\\r\\nl\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle v_{\\\\mathrm {coll} }}\\r\\n\\r\\n is the \\"electron collision rate\\". It is often the case that the electrons are magnetized while the ions are not. Magnetized plasmas are anisotropic, meaning that their properties in the direction parallel to the magnetic field are different from those perpendicular to it. While electric fields in plasmas are usually small due to the high conductivity, the electric field associated with a plasma moving in a magnetic field is given by \\r\\n\\r\\n\\r\\n\\r\\n\\r\\nE\\r\\n\\r\\n=\\r\\n?\\r\\nv\\r\\nG\\r\\n\\r\\nB\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\mathbf {E} =-v\\\\times \\\\mathbf {B} }\\r\\n\\r\\n (where \\r\\n\\r\\n\\r\\n\\r\\n\\r\\nE\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\mathbf {E} }\\r\\n\\r\\n is the electric field, \\r\\n\\r\\n\\r\\n\\r\\n\\r\\nv\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\mathbf {v} }\\r\\n\\r\\n is the velocity, and \\r\\n\\r\\n\\r\\n\\r\\n\\r\\nB\\r\\n\\r\\n\\r\\n\\r\\n{\\\\displaystyle \\\\mathbf {B} }\\r\\n\\r\\n is the magnetic field), and is not affected by Debye shielding.[37]\\r\\nPlasma is often called the fourth state of matter after solid, liquids and gases, despite plasma typically being an ionised gas.[38][39][40] It is distinct from these and other lower-energy states of matter. Although it is closely related to the gas phase in that it also has no definite form or volume, it differs in a number of ways, including the following:\\r\\nPlasmas are by far the most common phase of ordinary matter in the universe, both by mass and by volume.[42]\\r\\nWithin our Solar System, interplanetary space is filled with the plasma expelled via the solar wind, extending from the Sun's surface out to the heliopause. Furthermore, all the distant stars, and much of interstellar space or intergalactic space is also likely filled with plasma, albeit at very low densities. Astrophysical plasmas are also observed in Accretion disks around stars or compact objects like white dwarfs, neutron stars, or black holes in close binary star systems.[43] Plasma is associated with ejection of material in astrophysical jets, which have been observed with accreting black holes[44] or in active galaxies like M87's jet that possibly extends out to 5,000 light-years.[45]\\r\\nPlasmas can appear in nature in various forms and locations, which can be usefully broadly summarised in the following Table:\\r\\nAlthough the underlying equations governing plasmas are relatively simple, plasma behavior is extraordinarily varied and subtle: the emergence of unexpected behavior from a simple model is a typical feature of a complex system. Such systems lie in some sense on the boundary between ordered and disordered behavior and cannot typically be described either by simple, smooth, mathematical functions, or by pure randomness. The spontaneous formation of interesting spatial features on a wide range of length scales is one manifestation of plasma complexity. The features are interesting, for example, because they are very sharp, spatially intermittent (the distance between features is much larger than the features themselves), or have a fractal form. Many of these features were first studied in the laboratory, and have subsequently been recognized throughout the universe. Examples of complexity and complex structures in plasmas include:\\r\\nStriations or string-like structures,[47] also known as Birkeland currents, are seen in many plasmas, like the plasma ball, the aurora,[48] lightning,[49] electric arcs, solar flares,[50] and supernova remnants.[51] They are sometimes associated with larger current densities, and the interaction with the magnetic field can form a magnetic rope structure.[52] High power microwave breakdown at atmospheric pressure also leads to the formation of filamentary structures.[53] (See also Plasma pinch)\\r\\nFilamentation also refers to the self-focusing of a high power laser pulse. At high powers, the nonlinear part of the index of refraction becomes important and causes a higher index of refraction in the center of the laser beam, where the laser is brighter than at the edges, causing a feedback that focuses the laser even more. The tighter focused laser has a higher peak brightness (irradiance) that forms a plasma. The plasma has an index of refraction lower than one, and causes a defocusing of the laser beam. The interplay of the focusing index of refraction, and the defocusing plasma makes the formation of a long filament of plasma that can be micrometers to kilometers in length.[54] One interesting aspect of the filamentation generated plasma is the relatively low ion density due to defocusing effects of the ionised electrons.[55] (See also Filament propagation)\\r\\nThe strength and range of the electric force and the good conductivity of plasmas usually ensure that the densities of positive and negative charges in any sizeable region are equal (\\"quasineutrality\\"). A plasma with a significant excess of charge density, or, in the extreme case, is composed of a single species, is called a non-neutral plasma. In such a plasma, electric fields play a dominant role. Examples are charged particle beams, an electron cloud in a Penning trap and positron plasmas.[56]\\r\\nA dusty plasma contains tiny charged particles of dust (typically found in space). The dust particles acquire high charges and interact with each other. A plasma that contains larger particles is called grain plasma. Under laboratory conditions, dusty plasmas are also called complex plasmas.[57]\\r\\nImpermeable plasma is a type of thermal plasma which acts like an impermeable solid with respect to gas or cold plasma and can be physically pushed. Interaction of cold gas and thermal plasma was briefly studied by a group led by Hannes Alfvn in 1960s and 1970s for its possible applications in insulation of fusion plasma from the reactor walls.[58] However, later it was found that the external magnetic fields in this configuration could induce kink instabilities in the plasma and subsequently lead to an unexpectedly high heat loss to the walls.[59] In 2013, a group of materials scientists reported that they have successfully generated stable impermeable plasma with no magnetic confinement using only an ultrahigh-pressure blanket of cold gas. While spectroscopic data on the characteristics of plasma were claimed to be difficult to obtain due to the high pressure, the passive effect of plasma on synthesis of different nanostructures clearly suggested the effective confinement. They also showed that upon maintaining the impermeability for a few tens of seconds, screening of ions at the plasma-gas interface could give rise to a strong secondary mode of heating (known as viscous heating) leading to different kinetics of reactions and formation of complex nanomaterials.[60]\\r\\nTo completely describe the state of a plasma, all of the particle locations and velocities and describe the electromagnetic field in the plasma region would need to be written down. However, it is generally not practical or necessary to keep track of all the particles in a plasma. Therefore, plasma physicists commonly use less detailed descriptions, of which there are two main types:\\r\\nFluid models describe plasmas in terms of smoothed quantities, like density and averaged velocity around each position (see Plasma parameters). One simple fluid model, magnetohydrodynamics, treats the plasma as a single fluid governed by a combination of Maxwell's equations and the NavierÿStokes equations. A more general description is the two-fluid plasma picture, where the ions and electrons are described separately. Fluid models are often accurate when collisionality is sufficiently high to keep the plasma velocity distribution close to a MaxwellÿBoltzmann distribution. Because fluid models usually describe the plasma in terms of a single flow at a certain temperature at each spatial location, they can neither capture velocity space structures like beams or double layers, nor resolve wave-particle effects.\\r\\nKinetic models describe the particle velocity distribution function at each point in the plasma and therefore do not need to assume a MaxwellÿBoltzmann distribution. A kinetic description is often necessary for collisionless plasmas. There are two common approaches to kinetic description of a plasma. One is based on representing the smoothed distribution function on a grid in velocity and position. The other, known as the particle-in-cell (PIC) technique, includes kinetic information by following the trajectories of a large number of individual particles. Kinetic models are generally more computationally intensive than fluid models. The Vlasov equation may be used to describe the dynamics of a system of charged particles interacting with an electromagnetic field. In magnetized plasmas, a gyrokinetic approach can substantially reduce the computational expense of a fully kinetic simulation.\\r\\nMost artificial plasmas are generated by the application of electric and/or magnetic fields through a gas. Plasma generated in a laboratory setting and for industrial use can be generally categorized by:\\r\\nJust like the many uses of plasma, there are several means for its generation, however, one principle is common to all of them: there must be energy input to produce and sustain it.[62] For this case, plasma is generated when an electric current is applied across a dielectric gas or fluid (an electrically non-conducting material) as can be seen in the adjacent image, which shows a discharge tube as a simple example (DC used for simplicity).\\r\\nThe potential difference and subsequent electric field pull the bound electrons (negative) toward the anode (positive electrode) while the cathode (negative electrode) pulls the nucleus.[63] As the voltage increases, the current stresses the material (by electric polarization) beyond its dielectric limit (termed strength) into a stage of electrical breakdown, marked by an electric spark, where the material transforms from being an insulator into a conductor (as it becomes increasingly ionised). The underlying process is the Townsend avalanche, where collisions between electrons and neutral gas atoms create more ions and electrons (as can be seen in the figure on the right). The first impact of an electron on an atom results in one ion and two electrons. Therefore, the number of charged particles increases rapidly (in the millions) only \\"after about 20 successive sets of collisions\\",[64] mainly due to a small mean free path (average distance travelled between collisions).\\r\\nWith ample current density and ionisation, this forms a luminous electric arc (a continuous electric discharge similar to lightning) between the electrodes.[Note 1] Electrical resistance along the continuous electric arc creates heat, which dissociates more gas molecules and ionises the resulting atoms (where degree of ionisation is determined by temperature), and as per the sequence: solid-liquid-gas-plasma, the gas is gradually turned into a thermal plasma.[Note 2] A thermal plasma is in thermal equilibrium, which is to say that the temperature is relatively homogeneous throughout the heavy particles (i.e. atoms, molecules and ions) and electrons. This is so because when thermal plasmas are generated, electrical energy is given to electrons, which, due to their great mobility and large numbers, are able to disperse it rapidly and by elastic collision (without energy loss) to the heavy particles.[65][Note 3]\\r\\nBecause of their sizable temperature and density ranges, plasmas find applications in many fields of research, technology and industry. For example, in: industrial and extractive metallurgy,[65][66] surface treatments such as plasma spraying (coating), etching in microelectronics,[67] metal cutting[68] and welding; as well as in everyday vehicle exhaust cleanup and fluorescent/luminescent lamps,[62], fuel ignition, while even playing a part in supersonic combustion engines for aerospace engineering.[69]\\r\\nA world effort was triggered in the 1960s to study magnetohydrodynamic converters in order to bring MHD power conversion to market with commercial power plants of a new kind, converting the kinetic energy of a high velocity plasma into electricity with no moving parts at a high efficiency. Research was also conducted in the field of supersonic and hypersonic aerodynamics to study plasma interaction with magnetic fields to eventually achieve passive and even active flow control around vehicles or projectiles, in order to soften and mitigate shock waves, lower thermal transfer and reduce drag.\\r\\nSuch ionized gases used in \\"plasma technology\\" (\\"technological\\" or \\"engineered\\" plasmas) are usually weakly ionized gases in the sense that only a tiny fraction of the gas molecules are ionized.[79] These kinds of weakly ionized gases are also nonthermal \\"cold\\" plasmas. In the presence of magnetics fields, the study of such magnetized nonthermal weakly ionized gases involves resistive magnetohydrodynamics with low magnetic Reynolds number, a challenging field of plasma physics where calculations require dyadic tensors in a 7-dimensional phase space. When used in combination with a high Hall parameter, a critical value triggers the problematic electrothermal instability which limited these technological developments.\\r\\n\\r\\nPlasmas are the object of study of the academic field of plasma science or plasma physics, including sub-disciplines such as space plasma physics. It currently involves the following fields of active research and features across many journals, whose interest includes:\\r\\nHall effect thruster. The electric field in a plasma double layer is so effective at accelerating ions that electric fields are used in ion drives.\\r\\nSolar plasma\\r\\nPlasma spraying","input":"When did plasma become a state of matter?"},{"output":"a prime New Deal agency established by U.S. president Franklin D. Roosevelt (FDR) in 1933","context":"The National Recovery Administration was a prime New Deal agency established by U.S. president Franklin D. Roosevelt (FDR) in 1933. The goal was to eliminate \\"cut-throat competition\\" by bringing industry, labor, and government together to create codes of \\"fair practices\\" and set prices. The NRA was created by the National Industrial Recovery Act (NIRA) and allowed industries to get together and write \\"codes of fair competition.\\" The codes were intended to reduce \\"destructive competition\\" and to help workers by setting minimum wages and maximum weekly hours, as well as minimum prices at which products could be sold. The NRA also had a two-year renewal charter and was set to expire in June 1935 if not renewed.[1]\\r\\nIn 1935, the U.S. Supreme Court unanimously declared that the NRA law was unconstitutional, ruling that it infringed the separation of powers under the United States Constitution. The NRA quickly stopped operations, but many of its labor provisions reappeared in the National Labor Relations Act (Wagner Act), passed later the same year. The long-term result was a surge in the growth and power of unions, which became a core of the New Deal Coalition that dominated national politics for the next three decades.\\r\\nThe NRA, symbolized by the Blue Eagle, was popular with workers. Businesses that supported the NRA put the symbol in their shop windows and on their packages, though they did not always go along with the regulations entailed. Though membership to the NRA was voluntary, businesses that did not display the eagle were very often boycotted, making it seem mandatory for survival to many.\\r\\n\\r\\n\\r\\nAs part of the \\"First New Deal,\\" the NRA was based on the premise that the Great Depression was caused by market instability and that government intervention was necessary to balance the interests of farmers, business and labor. The NIRA, which created the NRA, declared that codes of fair competition should be developed through public hearings, and gave the Administration the power to develop voluntary agreements with industries regarding work hours, pay rates, and price fixing.[2] The NRA was put into operation by an executive order, signed the same day as the passage of the NIRA.\\r\\nNew Dealers who were part of the administration of President Franklin D. Roosevelt saw the close analogy with the earlier crisis handling the economics of World War I. They brought ideas and experience from the government controls and spending of 1917ÿ18.\\r\\nIn his June 13, 1933 \\"Statement on the National Industrial Recovery Act,\\" President Roosevelt described the spirit of the NRA: \\"On this idea, the first part of the NIRA proposes to our industry a great spontaneous cooperation to put millions of men back in their regular jobs this summer.\\"[3][4] He further stated, \\"But if all employers in each trade now band themselves faithfully in these modern guildswithout exceptionand agree to act together and at once, none will be hurt and millions of workers, so long deprived of the right to earn their bread in the sweat of their labor, can raise their heads again. The challenge of this law is whether we can sink selfish interest and present a solid front against a common peril.\\"[3][4]\\r\\nThe first director of the NRA was Hugh S. Johnson, a retired United States Army general and a successful businessman. He was named Time magazine's \\"Man of the Year\\" in 1933. Johnson saw the NRA as a national crusade designed to restore employment and regenerate industry.\\r\\nJohnson called on every business establishment in the nation to accept a stopgap \\"blanket code\\": a minimum wage of between 20 and 45 cents per hour, a maximum workweek of 35 to 45 hours, and the abolition of child labor. Johnson and Roosevelt contended that the \\"blanket code\\" would raise consumer purchasing power and increase employment.\\r\\nTo mobilize political support for the NRA, Johnson launched the \\"NRA Blue Eagle\\" publicity campaign to boost his bargaining strength to negotiate the codes with business and labor.\\r\\nHistorian Clarence B. Carson noted:\\r\\nAt this moment in time from the early days of the New Deal, it is difficult to recapture, even in imagination, the heady enthusiasm among a goodly number of intellectuals for a government planned economy. So far as can now be told, they believed that a bright new day was dawning, that national planning would result in an organically integrated economy in which everyone would joyfully work for the common good, and that American society would be freed at last from those antagonisms arising, as General Hugh Johnson put it, from \\"the murderous doctrine of savage and wolfish individualism, looking to dog-eat-dog and devil take the hindmost.\\"[5]\\r\\nThe negotiations of a code for the bituminous coal industry came against the background of a rapidly swelling union, the United Mine Workers headed by John L. Lewis and an unstable truce in the Pennsylvania coal fields. The NRA tried to get the principals to compromise with a national code for a decentralized industry in which many companies were anti-union, sought to keep wage differentials, and tried to escape the collective bargaining provisions of section 7A. Agreement among the parties was finally reached only after the NRA threatened that it would impose a code. The code did not establish price stabilization, nor did it resolve questions of industrial self-government versus governmental supervision or of centralization versus local autonomy, but it made dramatic changes in abolishing child labor, eliminating the compulsory scrip wages and company store, and establishing fair trade practices. It paved the way for an important wage settlement.[6]\\r\\nIn early 1935 the new chairman, Samuel Williams, announced that the NRA would stop setting prices, but businessmen complained. Chairman Williams told them plainly that, unless they could prove it would damage business, NRA was going to put an end to price control. Williams said, \\"Greater productivity and employment would result if greater price flexibility were attained.\\"[7] Of the 2,000 businessmen on hand probably 90% opposed Mr. Williams' aim, reported Time magazine: \\"To them a guaranteed price for their products looks like a royal road to profits. A fixed price above cost has proved a lifesaver to more than one inefficient producer.\\"[7] However, it was also argued NRA's price control method promoted monopolies.[8]\\r\\nThe business position was summarized by George A. Sloan, head of the Cotton Textile Code Authority:\\r\\n\\"Maximum hours and minimum wage provisions, useful and necessary as they are in themselves, do not prevent price demoralization. While putting the units of an industry on a fair competitive level insofar as labor costs are concerned, they do not prevent destructive price cutting in the sale of commodities produced, any more than a fixed price of material or other element of cost would prevent it. Destructive competition at the expense of employees is lessened, but it is left in full swing against the employer himself and the economic soundness of his enterprise....But if the partnership of industry with Government which was invoked by the President were terminated (as we believe it will not be), then the spirit of cooperation, which is one of the best fruits of the NRA equipment, could not survive.[7]\\r\\nMost businesses adopted the NRA without complaint, but Henry Ford was reluctant to join.[9]\\r\\nThe National Recovery Review Board, headed by noted criminal lawyer Clarence Darrow, a prominent liberal, was set up by President Roosevelt in March 1934 and abolished by him that same June. The board issued three reports highly critical of the NRA from the perspective of small business, charging the NRA with fostering cartels. The Darrow board, influenced by Justice Louis D. Brandeis, wanted instead to promote competitive capitalism.[10]\\r\\nRepresenting big business, the American Liberty League, 1934ÿ40, was run by leading industrialists who opposed the liberalism of the New Deal. Regarding the controversial NRA, the League was ambivalent. Jouett Shouse, the League president, commented that \\"the NRA has indulged in unwarranted excesses of attempted regulation\\"; on the other, he added that \\"in many regards [the NRA] has served a useful purpose.\\"[11] Shouse said that he had \\"deep sympathy\\" with the goals of the NRA, explaining, \\"While I feel very strongly that the prohibition of child labor, the maintenance of a minimum wage and the limitation of the hours of work belong under our form of government in the realm of the affairs of the different states, yet I am entirely willing to agree that in the case of an overwhelming national emergency the Federal Government for a limited period should be permitted to assume jurisdiction of them.\\"[12]\\r\\nBritish journalist Alistair Cooke described Franklin Delano Roosevelt's presidency of the United States of America in the two years from his inauguration to the Supreme Court's declaration that the National Recovery Administration was unconstitutional, as a benevolent dictatorship.[13]\\r\\nThe NRA negotiated specific sets of codes with leaders of the nation's major industries; the most important provisions were anti-deflationary floors below which no company would lower prices or wages, and agreements on maintaining employment and production. In a remarkably short time, the NRA won agreements from almost every major industry in the nation. According to some conservative economists, the NRA increased the cost of doing business by forty percent.[14] Donald Richberg, who soon replaced Johnson as the head of the NRA said:\\r\\nThere is no choice presented to American business between intelligently planned and uncontrolled industrial operations and a return to the gold-plated anarchy that masqueraded as \\"rugged individualism.\\"...Unless industry is sufficiently socialized by its private owners and managers so that great essential industries are operated under public obligation appropriate to the public interest in them, the advance of political control over private industry is inevitable.[15]\\r\\nBy the time it ended in May 1935, industrial production was 22% higher than in May 1933.[citation needed]\\r\\nPennock (1997) shows that the rubber tire industry faced debilitating challenges, mostly brought about by changes in the industry's retail structure and exacerbated by the Depression. Segments of the industry attempted to use the NRA codes to solve these new problems and stabilize the tire market, but the tire manufacturing and tire retailing codes were patent failures. Instead of leading to cartelization and higher prices, which is what most scholars assume the NRA codes did, the tire industry codes led to even more fragmentation and price cutting.[16]\\r\\nAlexander (1997) examines the macaroni industry and concludes that cost heterogeneity was a major source of the \\"compliance crisis\\" affecting a number of NRA \\"codes of fair competition\\" that were negotiated by industries and submitted for government approval under the National Industry Recovery Act of 1933. The argument boils down to assumptions that progressives at the NRA allowed majority coalitions of small, high-cost firms to impose codes in heterogeneous industries, and that these codes were designed by the high-cost firms under an ultimately erroneous belief that they would be enforced by the NRA.[17]\\r\\nStorrs (2000) says the National Consumers' League (NCL) had been instrumental in the passage and legal defense of labor legislation in many states since 1899. Women activists used the New Deal opportunity to gain a national forum. General Secretary Lucy Randolph Mason and her league relentlessly lobbied the NRA to make its regulatory codes just and fair for all workers and to eliminate explicit and de facto discrimination in pay, working conditions, and opportunities for reasons of sex, race, or union status. Even after the demise of the NRA, the league continued campaigning for collective bargaining rights and fair labor standards at both federal and state levels.[18]\\r\\nAbout 23 million people were employed under the NRA codes. However, violations of codes became common and attempts were made to use the courts to enforce the NRA. The NRA included a multitude of regulations imposing the pricing and production standards for all sorts of goods and services. Individuals were arrested for not complying with these codes. For example, one small businessman was fined for violating the \\"Tailor's Code\\" by pressing a suit for 35 rather than NRA required 40 cents. Roosevelt critic John T. Flynn, in The Roosevelt Myth (1944), wrote:\\r\\nThe NRA was famous for its bureaucracy. Journalist Raymond Clapper reported that between 4,000 and 5,000 business practices were prohibited by NRA orders that carried the force of law, which were contained in some 3,000 administrative orders running to over 10 million pages, and supplemented by what Clapper said were \\"innumerable opinions and directions from national, regional and code boards interpreting and enforcing provisions of the act.\\" There were also \\"the rules of the code authorities, themselves, each having the force of law and affecting the lives and conduct of millions of persons.\\" Clapper concluded: \\"It requires no imagination to appreciate the difficulty the business man has in keeping informed of these codes, supplemental codes, code amendments, executive orders, administrative orders, office orders, interpretations, rules, regulations and obiter dicta.\\"[19]\\r\\nOn 27 May 1935, in the court case of Schechter Poultry Corp. v. United States, the Supreme Court held the mandatory codes section of NIRA unconstitutional,[20] because it attempted to regulate commerce that was not interstate in character, and that the codes represented an unacceptable delegation of power from the legislature to the executive. Chief Justice Charles Evans Hughes wrote for a unanimous Court in invalidating the industrial \\"codes of fair competition\\" which the NIRA enabled the President to issue. The Court held that the codes violated the United States Constitution's separation of powers as an impermissible delegation of legislative power to the executive branch. The Court also held that the NIRA provisions were in excess of congressional power under the Commerce Clause.[21]\\r\\nThe Court distinguished between direct effects on interstate commerce, which Congress could lawfully regulate, and indirect, which were purely matters of state law. Though the raising and sale of poultry was an interstate industry, the Court found that the \\"stream of interstate commerce\\" had stopped in this case: Schechter's slaughterhouses bought chickens only from intrastate wholesalers and sold to intrastate buyers. Any interstate effect of Schechter was indirect, and therefore beyond federal reach.[22]\\r\\nSpecifically, the Court invalidated regulations of the poultry industry promulgated under the authority of the National Industrial Recovery Act of 1933, including price and wage fixing, as well as requirements regarding a whole shipment of chickens, including unhealthy ones, which has led to the case becoming known as \\"the sick chicken case.\\" The ruling was one of a series which overturned some New Deal legislation between January 1935 and January 1936, and which ultimately caused Roosevelt to attempt to pack the Court with judges that were in favor of the New Deal.\\r\\nSubsequent to the decision, the remainder of Title I was extended until April 1, 1936, by joint resolution of Congress (49 Stat. 375), June 14, 1935, and NRA was reorganized by E.O. 7075, June 15, 1935, to facilitate its new role as a promoter of industrial cooperation and to enable it to produce a series of economic studies,[20] which the National Recovery Review Board was already doing.[23] Many of the labor provisions reappeared in the Wagner Act of 1935.\\r\\nIn Philadelphia, Pennsylvania in 1933, DeBenneville \\"Bert\\" Bell formed a new National Football League franchise to replace the defunct Frankford Yellow Jackets, naming this team the Eagles in recognition of the NRA (a name the team retains to the present).[24][25]\\r\\nIn 1934, at the request of the Interior Secretary Ickes, who wished to use the statute criminalizing making false statements to enforce Section 9(c) of the NIRA against producers of \\"hot oil\\", oil produced in violation of production restrictions established pursuant to the NIRA, Congress passed Pub.L. 73ÿ394, 48?Stat.?996, enacted June?18, 1934, which amended the False Claims Act of 1863 to read:[26]\\r\\n or whoever, for the purpose of obtaining or aiding to obtain the payment or approval of such claim, or for the purpose and with the intent of cheating and swindling or defrauding the Government of the United States, or any department thereof, or any corporation in which the United States of America is a stockholder, shall knowingly and willfully falsify or conceal or cover up by any trick, scheme, or device a material fact, or make or cause to be made any false or fraudulent statements or representations, or make or use or cause to be made or used any false bill, receipt, voucher, account, roll, claim, certificate, affidavit, or deposition, knowing the same to contain any fraudulent or fictitious statement or entry, in any matter within the jurisdiction of any department or agency of the United States or of any corporation in which the United States of America is a stockholder \\r\\nThis form of the statute, in slightly modified form, still exists today at 18 U.S.C.??1001.","input":"What is the nra during the great depression?"},{"output":"June 1944","context":"Spaceflight (also written space flight) is ballistic flight into or through outer space. Spaceflight can occur with spacecraft with or without humans on board. Examples of human spaceflight include the U.S. Apollo Moon landing and Space Shuttle programs and the Russian Soyuz program, as well as the ongoing International Space Station. Examples of unmanned spaceflight include space probes that leave Earth orbit, as well as satellites in orbit around Earth, such as communications satellites. These operate either by telerobotic control or are fully autonomous.\\r\\nSpaceflight is used in space exploration, and also in commercial activities like space tourism and satellite telecommunications. Additional non-commercial uses of spaceflight include space observatories, reconnaissance satellites and other Earth observation satellites.\\r\\nA spaceflight typically begins with a rocket launch, which provides the initial thrust to overcome the force of gravity and propels the spacecraft from the surface of the Earth. Once in space, the motion of a spacecraftboth when unpropelled and when under propulsionis covered by the area of study called astrodynamics. Some spacecraft remain in space indefinitely, some disintegrate during atmospheric reentry, and others reach a planetary or lunar surface for landing or impact.\\r\\nThe first theoretical proposal of space travel using rockets was published by Scottish astronomer and mathematician William Leitch, in an 1861 essay \\"A Journey Through Space\\".[1] More well-known (though not widely outside Russia) is Konstantin Tsiolkovsky's work, \\"ʶ󶶿־ҿ ҰѾ־ ŤѾҰ־ ѿ־ּ ŤҼɾ\\" (The Exploration of Cosmic Space by Means of Reaction Devices), published in 1903.\\r\\nSpaceflight became an engineering possibility with the work of Robert H. Goddard's publication in 1919 of his paper A Method of Reaching Extreme Altitudes. His application of the de Laval nozzle to liquid fuel rockets improved efficiency enough for interplanetary travel to become possible. He also proved in the laboratory that rockets would work in the vacuum of space;[specify] nonetheless, his work was not taken seriously by the public. His attempt to secure an Army contract for a rocket-propelled weapon in the first World War was defeated by the November 11, 1918 armistice with Germany.\\r\\nNonetheless, Goddard's paper was highly influential on Hermann Oberth, who in turn influenced Wernher von Braun. Von Braun became the first to produce modern rockets as guided weapons, employed by Adolf Hitler. Von Braun's V-2 was the first rocket to reach space, at an altitude of 189 kilometers (102 nautical miles) on a June 1944 test flight.[2]\\r\\nTsiolkovsky's rocketry work was not fully appreciated in his lifetime, but he influenced Sergey Korolev, who became the Soviet Union's chief rocket designer under Joseph Stalin, to develop intercontinental ballistic missiles to carry nuclear weapons as a counter measure to United States bomber planes. Derivatives of Korolev's R-7 Semyorka missiles were used to launch the world's first artificial Earth satellite, Sputnik 1, on October 4, 1957, and later the first human to orbit the Earth, Yuri Gagarin in Vostok 1, on April 12, 1961.[3]\\r\\nAt the end of World War II, von Braun and most of his rocket team surrendered to the United States, and were expatriated to work on American missiles at what became the Army Ballistic Missile Agency. This work on missiles such as Juno I and Atlas enabled launch of the first US satellite Explorer 1 on February 1, 1958, and the first American in orbit, John Glenn in Friendship 7 on February 20, 1962. As director of the Marshall Space Flight Center, Von Braun oversaw development of a larger class of rocket called Saturn, which allowed the US to send the first two humans, Neil Armstrong and Buzz Aldrin, to the Moon and back on Apollo 11 in July 1969. Over the same period, the Soviet Union secretly tried but failed to develop the N1 rocket to give them the capability to land one person on the Moon.\\r\\nRockets are the only means currently capable of reaching orbit or beyond. Other non-rocket spacelaunch technologies have yet to be built, or remain short of orbital speeds. A rocket launch for a spaceflight usually starts from a spaceport (cosmodrome), which may be equipped with launch complexes and launch pads for vertical rocket launches, and runways for takeoff and landing of carrier airplanes and winged spacecraft. Spaceports are situated well away from human habitation for noise and safety reasons. ICBMs have various special launching facilities.\\r\\nA launch is often restricted to certain launch windows. These windows depend upon the position of celestial bodies and orbits relative to the launch site. The biggest influence is often the rotation of the Earth itself. Once launched, orbits are normally located within relatively constant flat planes at a fixed angle to the axis of the Earth, and the Earth rotates within this orbit.\\r\\nA launch pad is a fixed structure designed to dispatch airborne vehicles. It generally consists of a launch tower and flame trench. It is surrounded by equipment used to erect, fuel, and maintain launch vehicles.\\r\\nThe most commonly used definition of outer space is everything beyond the Krmn line, which is 100 kilometers (62?mi) above the Earth's surface. The United States sometimes defines outer space as everything beyond 50 miles (80?km) in altitude.\\r\\nRockets are the only currently practical means of reaching space. Conventional airplane engines cannot reach space due to the lack of oxygen. Rocket engines expel propellant to provide forward thrust that generates enough delta-v (change in velocity) to reach orbit.\\r\\nFor manned launch systems launch escape systems are frequently fitted to allow astronauts to escape in the case of emergency.\\r\\nMany ways to reach space other than rockets have been proposed. Ideas such as the space elevator, and momentum exchange tethers like rotovators or skyhooks require new materials much stronger than any currently known. Electromagnetic launchers such as launch loops might be feasible with current technology. Other ideas include rocket assisted aircraft/spaceplanes such as Reaction Engines Skylon (currently in early stage development), scramjet powered spaceplanes, and RBCC powered spaceplanes. Gun launch has been proposed for cargo.\\r\\nAchieving a closed orbit is not essential to lunar and interplanetary voyages. Early Russian space vehicles successfully achieved very high altitudes without going into orbit. NASA considered launching Apollo missions directly into lunar trajectories but adopted the strategy of first entering a temporary parking orbit and then performing a separate burn several orbits later onto a lunar trajectory. This costs additional propellant because the parking orbit perigee must be high enough to prevent reentry while direct injection can have an arbitrarily low perigee because it will never be reached.\\r\\nHowever, the parking orbit approach greatly simplified Apollo mission planning in several important ways. It substantially widened the allowable launch windows, increasing the chance of a successful launch despite minor technical problems during the countdown. The parking orbit was a stable \\"mission plateau\\" that gave the crew and controllers several hours to thoroughly check out the spacecraft after the stresses of launch before committing it to a long lunar flight; the crew could quickly return to Earth, if necessary, or an alternate Earth-orbital mission could be conducted. The parking orbit also enabled translunar trajectories that avoided the densest parts of the Van Allen radiation belts.\\r\\nApollo missions minimized the performance penalty of the parking orbit by keeping its altitude as low as possible. For example, Apollo 15 used an unusually low parking orbit (even for Apollo) of 92.5 nmi by 91.5 nmi (171?km by 169?km) where there was significant atmospheric drag. But it was partially overcome by continuous venting of hydrogen from the third stage of the Saturn V, and was in any event tolerable for the short stay.\\r\\nRobotic missions do not require an abort capability or radiation minimization, and because modern launchers routinely meet \\"instantaneous\\" launch windows, space probes to the Moon and other planets generally use direct injection to maximize performance. Although some might coast briefly during the launch sequence, they do not complete one or more full parking orbits before the burn that injects them onto an Earth escape trajectory.\\r\\nNote that the escape velocity from a celestial body decreases with altitude above that body. However, it is more fuel-efficient for a craft to burn its fuel as close to the ground as possible; see Oberth effect and reference.[5] This is another way to explain the performance penalty associated with establishing the safe perigee of a parking orbit.\\r\\nPlans for future crewed interplanetary spaceflight missions often include final vehicle assembly in Earth orbit, such as NASA's Project Orion and Russia's Kliper/Parom tandem.\\r\\nAstrodynamics is the study of spacecraft trajectories, particularly as they relate to gravitational and propulsion effects. Astrodynamics allows for a spacecraft to arrive at its destination at the correct time without excessive propellant use. An orbital maneuvering system may be needed to maintain or change orbits.\\r\\nNon-rocket orbital propulsion methods include solar sails, magnetic sails, plasma-bubble magnetic systems, and using gravitational slingshot effects.\\r\\nThe term \\"transfer energy\\" means the total amount of energy imparted by a rocket stage to its payload. This can be the energy imparted by a first stage of a launch vehicle to an upper stage plus payload, or by an upper stage or spacecraft kick motor to a spacecraft.[6][7]\\r\\nVehicles in orbit have large amounts of kinetic energy. This energy must be discarded if the vehicle is to land safely without vaporizing in the atmosphere. Typically this process requires special methods to protect against aerodynamic heating. The theory behind reentry was developed by Harry Julian Allen. Based on this theory, reentry vehicles present blunt shapes to the atmosphere for reentry. Blunt shapes mean that less than 1% of the kinetic energy ends up as heat that reaches the vehicle and the heat energy instead ends up in the atmosphere.\\r\\nThe Mercury, Gemini, and Apollo capsules all splashed down in the sea. These capsules were designed to land at relatively low speeds with the help of a parachute. Russian capsules for Soyuz make use of a big parachute and braking rockets to touch down on land. The Space Shuttle glided to a touchdown like a plane.\\r\\nAfter a successful landing the spacecraft, its occupants and cargo can be recovered. In some cases, recovery has occurred before landing: while a spacecraft is still descending on its parachute, it can be snagged by a specially designed aircraft. This mid-air retrieval technique was used to recover the film canisters from the Corona spy satellites.\\r\\nUnmanned spaceflight is all spaceflight activity without a necessary human presence in space. This includes all space probes, satellites and robotic spacecraft and missions. Unmanned spaceflight is the opposite of manned spaceflight, which is usually called human spaceflight. Subcategories of unmanned spaceflight are robotic spacecraft (objects) and robotic space missions (activities). A robotic spacecraft is an unmanned spacecraft with no humans on board, that is usually under telerobotic control. A robotic spacecraft designed to make scientific research measurements is often called a space probe.\\r\\nUnmanned space missions use remote-controlled spacecraft. The first unmanned space mission was Sputnik I, launched October 4, 1957 to orbit the Earth. Space missions where animals but no humans are on-board are considered unmanned missions.\\r\\nMany space missions are more suited to telerobotic rather than crewed operation, due to lower cost and lower risk factors. In addition, some planetary destinations such as Venus or the vicinity of Jupiter are too hostile for human survival, given current technology. Outer planets such as Saturn, Uranus, and Neptune are too distant to reach with current crewed spaceflight technology, so telerobotic probes are the only way to explore them. Telerobotics also allows exploration of regions that are vulnerable to contamination by Earth micro-organisms since spacecraft can be sterilized. Humans can not be sterilized in the same way as a spaceship, as they coexist with numerous micro-organisms, and these micro-organisms are also hard to contain within a spaceship or spacesuit.\\r\\nTelerobotics becomes telepresence when the time delay is short enough to permit control of the spacecraft in close to real time by humans. Even the two seconds light speed delay for the Moon is too far away for telepresence exploration from Earth. The L1 and L2 positions permit 400-millisecond round trip delays, which is just close enough for telepresence operation. Telepresence has also been suggested as a way to repair satellites in Earth orbit from Earth. The Exploration Telerobotics Symposium in 2012 explored this and other topics.[8]\\r\\nThe first human spaceflight was Vostok 1 on April 12, 1961, on which cosmonaut Yuri Gagarin of the USSR made one orbit around the Earth. In official Soviet documents, there is no mention of the fact that Gagarin parachuted the final seven miles.[9] Currently, the only spacecraft regularly used for human spaceflight are the Russian Soyuz spacecraft and the Chinese Shenzhou spacecraft. The U.S. Space Shuttle fleet operated from April 1981 until July 2011. SpaceShipOne has conducted two human suborbital spaceflights.\\r\\nOn a sub-orbital spaceflight the spacecraft reaches space and then returns to the atmosphere after following a (primarily) ballistic trajectory. This is usually because of insufficient specific orbital energy, in which case a suborbital flight will last only a few minutes, but it is also possible for an object with enough energy for an orbit to have a trajectory that intersects the Earth's atmosphere, sometimes after many hours. Pioneer 1 was NASA's first space probe intended to reach the Moon. A partial failure caused it to instead follow a suborbital trajectory to an altitude of 113,854 kilometers (70,746?mi) before reentering the Earth's atmosphere 43 hours after launch.\\r\\nThe most generally recognized boundary of space is the Krmn line 100?km above sea level. (NASA alternatively defines an astronaut as someone who has flown more than 50 miles (80?km) above sea level.) It is not generally recognized by the public that the increase in potential energy required to pass the Krmn line is only about 3% of the orbital energy (potential plus kinetic energy) required by the lowest possible Earth orbit (a circular orbit just above the Krmn line.) In other words, it is far easier to reach space than to stay there. On May 17, 2004, Civilian Space eXploration Team launched the GoFast Rocket on a suborbital flight, the first amateur spaceflight. On June 21, 2004, SpaceShipOne was used for the first privately funded human spaceflight.\\r\\nPoint-to-point sub-orbital spaceflight is a category of spaceflight in which a spacecraft uses a sub-orbital flight for transportation. This can provide a two-hour trip from London to Sydney, which would be much faster than what is currently over a twenty-hour flight. Today, no company offers this type of spaceflight for transportation. However, Virgin Galactic has plans for a spaceplane called SpaceShipThree, which could offer this service in the future.[10] Suborbital spaceflight over an intercontinental distance requires a vehicle velocity that is only a little lower than the velocity required to reach low Earth orbit.[11] If rockets are used, the size of the rocket relative to the payload is similar to an Intercontinental Ballistic Missile (ICBM). Any intercontinental spaceflight has to surmount problems of heating during atmosphere re-entry that are nearly as large as those faced by orbital spaceflight.\\r\\nA minimal orbital spaceflight requires much higher velocities than a minimal sub-orbital flight, and so it is technologically much more challenging to achieve. To achieve orbital spaceflight, the tangential velocity around the Earth is as important as altitude. In order to perform a stable and lasting flight in space, the spacecraft must reach the minimal orbital speed required for a closed orbit.\\r\\nInterplanetary travel is travel between planets within a single planetary system. In practice, the use of the term is confined to travel between the planets of our Solar System.\\r\\nFive spacecraft are currently leaving the Solar System on escape trajectories, Voyager 1, Voyager 2, Pioneer 10, Pioneer 11, and New Horizons. The one farthest from the Sun is Voyager 1, which is more than 100 AU distant and is moving at 3.6 AU per year.[12] In comparison, Proxima Centauri, the closest star other than the Sun, is 267,000 AU distant. It will take Voyager 1 over 74,000 years to reach this distance. Vehicle designs using other techniques, such as nuclear pulse propulsion are likely to be able to reach the nearest star significantly faster. Another possibility that could allow for human interstellar spaceflight is to make use of time dilation, as this would make it possible for passengers in a fast-moving vehicle to travel further into the future while aging very little, in that their great speed slows down the rate of passage of on-board time. However, attaining such high speeds would still require the use of some new, advanced method of propulsion.\\r\\nIntergalactic travel involves spaceflight between galaxies, and is considered much more technologically demanding than even interstellar travel and, by current engineering terms, is considered science fiction.\\r\\nSpacecraft are vehicles capable of controlling their trajectory through space.\\r\\nThe first 'true spacecraft' is sometimes said to be Apollo Lunar Module,[13] since this was the only manned vehicle to have been designed for, and operated only in space; and is notable for its non aerodynamic shape.\\r\\nSpacecraft today predominantly use rockets for propulsion, but other propulsion techniques such as ion drives are becoming more common, particularly for unmanned vehicles, and this can significantly reduce the vehicle's mass and increase its delta-v.\\r\\nLaunch systems are used to carry a payload from Earth's surface into outer space.\\r\\nAll launch vehicles contain a huge amount of energy that is needed for some part of it to reach orbit. There is therefore some risk that this energy can be released prematurely and suddenly, with significant effects. When a Delta II rocket exploded 13 seconds after launch on January 17, 1997, there were reports of store windows 10 miles (16?km) away being broken by the blast.[15]\\r\\nSpace is a fairly predictable environment, but there are still risks of accidental depressurization and the potential failure of equipment, some of which may be very newly developed.\\r\\nIn 2004 the International Association for the Advancement of Space Safety was established in the Netherlands to further international cooperation and scientific advancement in space systems safety.[16]\\r\\nIn a microgravity environment such as that provided by a spacecraft in orbit around the Earth, humans experience a sense of \\"weightlessness.\\" Short-term exposure to microgravity causes space adaptation syndrome, a self-limiting nausea caused by derangement of the vestibular system. Long-term exposure causes multiple health issues. The most significant is bone loss, some of which is permanent, but microgravity also leads to significant deconditioning of muscular and cardiovascular tissues.\\r\\nOnce above the atmosphere, radiation due to the Van Allen belts, solar radiation and cosmic radiation issues occur and increase. Further away from the Earth, solar flares can give a fatal radiation dose in minutes, and the health threat from cosmic radiation significantly increases the chances of cancer over a decade exposure or more.[17]\\r\\nIn human spaceflight, the life support system is a group of devices that allow a human being to survive in outer space. NASA often uses the phrase Environmental Control and Life Support System or the acronym ECLSS when describing these systems for its human spaceflight missions.[18] The life support system may supply: air, water and food. It must also maintain the correct body temperature, an acceptable pressure on the body and deal with the body's waste products. Shielding against harmful external influences such as radiation and micro-meteorites may also be necessary. Components of the life support system are life-critical, and are designed and constructed using safety engineering techniques.\\r\\nSpace weather is the concept of changing environmental conditions in outer space. It is distinct from the concept of weather within a planetary atmosphere, and deals with phenomena involving ambient plasma, magnetic fields, radiation and other matter in space (generally close to Earth but also in interplanetary, and occasionally interstellar medium). \\"Space weather describes the conditions in space that affect Earth and its technological systems. Our space weather is a consequence of the behavior of the Sun, the nature of Earth's magnetic field, and our location in the Solar System.\\"[19]\\r\\nSpace weather exerts a profound influence in several areas related to space exploration and development. Changing geomagnetic conditions can induce changes in atmospheric density causing the rapid degradation of spacecraft altitude in Low Earth orbit. Geomagnetic storms due to increased solar activity can potentially blind sensors aboard spacecraft, or interfere with on-board electronics. An understanding of space environmental conditions is also important in designing shielding and life support systems for manned spacecraft.\\r\\nRockets as a class are not inherently grossly polluting. However, some rockets use toxic propellants, and most vehicles use propellants that are not carbon neutral. Many solid rockets have chlorine in the form of perchlorate or other chemicals, and this can cause temporary local holes in the ozone layer. Re-entering spacecraft generate nitrates which also can temporarily impact the ozone layer. Most rockets are made of metals that can have an environmental impact during their construction.\\r\\nIn addition to the atmospheric effects there are effects on the near-Earth space environment. There is the possibility that orbit could become inaccessible for generations due to exponentially increasing space debris caused by spalling of satellites and vehicles (Kessler syndrome). Many launched vehicles today are therefore designed to be re-entered after use.\\r\\nCurrent and proposed applications for spaceflight include:\\r\\nMost early spaceflight development was paid for by governments. However, today major launch markets such as Communication satellites and Satellite television are purely commercial, though many of the launchers were originally funded by governments.\\r\\nPrivate spaceflight is a rapidly developing area: space flight that is not only paid for by corporations or even private individuals, but often provided by private spaceflight companies. These companies often assert that much of the previous high cost of access to space was caused by governmental inefficiencies they can avoid. This assertion can be supported by much lower published launch costs for private space launch vehicles such as Falcon 9 developed with private financing. Lower launch costs and excellent safety will be required for the applications such as Space tourism and especially Space colonization to become successful.\\r\\n Media related to Spaceflight at Wikimedia Commons\\r\\nSolar System?L Local Interstellar Cloud?L Local Bubble?L Gould Belt?L Orion Arm?L Milky Way?L Milky Way subgroup?L Local Group?L Virgo Supercluster?L Laniakea Supercluster?L PiscesÿCetus Supercluster Complex?L Observable universe?L Universe\\r\\nEach arrow (L) may be read as \\"within\\" or \\"part of\\".","input":"When did the first rocket go into space?"},{"output":"ten provinces","context":"The provinces and territories of Canada are the administrative divisions that are responsible for the delivery of sub-national governance within the geographical areas of Canada under the authority of the Canadian Constitution. In the 1867 Canadian Confederation, three provinces of British North AmericaNew Brunswick, Nova Scotia and the Province of Canada (which, upon Confederation, was divided into Ontario and Quebec)were united to form a federated colony, which eventually became a sovereign nation in the next century. Over its history, Canada's international borders have changed several times, and the country has grown from the original four provinces to the current ten provinces and three territories. The ten provinces are Alberta, British Columbia, Manitoba, New Brunswick, Newfoundland and Labrador, Nova Scotia, Ontario, Prince Edward Island, Quebec, and Saskatchewan. Several of the provinces were former British colonies, and Quebec was originally a French colony, while others were added as Canada grew. The three territories are Northwest Territories, Nunavut, and Yukon, which govern the rest of the area of the former British North America. Together, the provinces and territories make up the world's second-largest country by area.\\r\\nThe major difference between a Canadian province and a territory is that provinces receive their power and authority from the Constitution Act, 1867 (formerly called the British North America Act, 1867), whereas territorial governments have powers delegated to them by the Parliament of Canada. The powers flowing from the Constitution Act are divided between the Government of Canada (the federal government) and the provincial governments to exercise exclusively. A change to the division of powers between the federal government and the provinces requires a constitutional amendment, whereas a similar change affecting the territories can be performed unilaterally by the Parliament of Canada or government.\\r\\nIn modern Canadian constitutional theory, the provinces are considered to be sovereign within certain areas based on the divisions of responsibility between the provincial and federal government within the Constitution Act 1867, and each province thus has its own representative of the Canadian \\"Crown\\", the lieutenant governor. The territories are not sovereign, but instead their authorities and responsibilities come directly from the federal level, and as a result have a commissioner instead of a lieutenant governor.\\r\\n\\r\\n\\r\\n\\r\\nNotes:\\r\\nBritish Columbia Parliament Buildings\\r\\nAlberta Legislative Building\\r\\nSaskatchewan Legislative Building\\r\\nManitoba Legislative Building\\r\\nOntario Legislative Building\\r\\nParliament Building (Quebec)\\r\\nConfederation Building (Newfoundland and Labrador)\\r\\nNew Brunswick Legislative Building\\r\\nProvince House (Nova Scotia)\\r\\nProvince House (Prince Edward Island)\\r\\nThere are three territories in Canada. Unlike the provinces, the territories of Canada have no inherent sovereignty and have only those powers delegated to them by the federal government.[8][9][10] They include all of mainland Canada north of latitude 60 north and west of Hudson Bay, as well as most islands north of the Canadian mainland (from those in James Bay to the Canadian Arctic islands). The following table lists the territories in order of precedence (each province has precedence over all the territories, regardless of the date each territory was created).\\r\\nYukon Legislative Building\\r\\nNorthwest Territories Legislative Building\\r\\nLegislative Building of Nunavut\\r\\nOntario, Quebec, New Brunswick, and Nova Scotia were the original provinces, formed when several British North American colonies federated on July 1, 1867, into the Dominion of Canada and by stages began accruing the indicia of sovereignty from the United Kingdom.[14] Prior to this, Ontario and Quebec were united as the Province of Canada. Over the following years, Manitoba (1870), British Columbia (1871), and Prince Edward Island (1873) were added as provinces.[14]\\r\\nThe British Crown had claimed two large areas north-west of the Canadian colony, known as Rupert's Land and the North-Western Territory and assigned them to the Hudson's Bay Company. In 1870, the company relinquished its claims for S300,000 ($1.5 million), assigning the vast territory to the Government of Canada.[15] Subsequently, the area was re-organized into the province of Manitoba and the Northwest Territories.[15] The Northwest Territories were vast at first, encompassing all of current northern and western Canada, except for the British holdings in the Arctic islands and the Colony of British Columbia; the Territories also included the northern two-thirds of Ontario and Quebec, and almost all of present Manitoba, with the 1870 province of Manitoba originally being confined to a small area in the south of today's province.[16] The British claims to the Arctic islands were transferred to Canada in 1880, adding to the size of the Northwest Territories. 1898 saw the Yukon Territory, later renamed simply as Yukon, carved from the parts of the Northwest Territories surrounding the Klondike gold fields. On September 1, 1905, a portion of the Northwest Territories south of the 60th parallel north became the provinces of Alberta and Saskatchewan.[16] In 1912, the boundaries of Quebec, Ontario, and Manitoba were expanded northward: Manitoba's to the 60 parallel, Ontario's to Hudson Bay and Quebec's to encompass the District of Ungava.[17]\\r\\nIn 1869, the people of Newfoundland voted to remain a British colony over fears that taxes would increase with Confederation, and that the economic policy of the Canadian government would favour mainland industries.[18] In 1907, Newfoundland acquired dominion status.[19] In the middle of the Great Depression in Canada with Newfoundland facing a prolonged period of economic crisis, the legislature turned over political control to the Commission of Government in 1933.[20] Following Canada's participation in World War II, in a 1948 referendum, a narrow majority of Newfoundland citizens voted to join the Confederation, and on March 31, 1949, Newfoundland became Canada's tenth province.[21] In 2001, it was officially renamed Newfoundland and Labrador.[22]\\r\\nIn 1903, the Alaska Panhandle Dispute fixed British Columbia's northwestern boundary.[23] This was one of only two provinces in Canadian history to have its size reduced. The second reduction, in 1927, occurred when a boundary dispute between Canada and the Dominion of Newfoundland saw Labrador increased at Quebec's expense ÿ this land returned to Canada, as part of the province of Newfoundland, in 1949.[24] In 1999, Nunavut was created from the eastern portion of the Northwest Territories.[25] Yukon lies in the western portion of The North, while Nunavut is in the east.[26]\\r\\nAll three territories combined are the most sparsely populated region in Canada, covering 3,921,739?km2 (1,514,192?sq?mi) in land area.[5] They are often referred to as a single region, The North, for organisational and economic purposes.[27] For much of the Northwest Territories' early history it was divided into several districts for ease of administration.[28] The District of Keewatin was created as a separate territory from 1876 to 1905, after which, as the Keewatin Region, it became an administrative district of the Northwest Territories.[29] In 1999, it was dissolved when it became part of Nunavut.\\r\\nTheoretically, provinces have a great deal of power relative to the federal government, with jurisdiction over many public goods such as health care, education, welfare, and intra-provincial transportation.[30] They receive \\"transfer payments\\" from the federal government to pay for these, as well as exacting their own taxes.[31] In practice, however, the federal government can use these transfer payments to influence these provincial areas. For instance, in order to receive healthcare funding under Medicare, provinces must agree to meet certain federal mandates, such as universal access to required medical treatment.[31]\\r\\nProvincial and territorial legislatures have no second chamber like the Canadian Senate. Originally, most provinces did have such bodies, known as legislative councils, with members titled councillors. These upper houses were abolished one by one, Quebec's being the last in 1968.[32] In most provinces, the single house of the legislature is known as the Legislative Assembly; the exceptions are Nova Scotia and Newfoundland and Labrador, where the chamber is called the House of Assembly, and Quebec where it is called the National Assembly.[33] Ontario has a Legislative Assembly but its members are called Members of the Provincial Parliament or MPPs.[34] The legislative assemblies use a procedure similar to that of the Canadian House of Commons. The head of government of each province, called the premier, is generally the head of the party with the most seats.[35] This is also the case in Yukon, but the Northwest Territories and Nunavut have no political parties at the territorial level.[36] The Queen's representative to each province is the Lieutenant Governor.[37] In each of the territories there is an analogous Commissioner, but he or she represents the federal government rather than the monarch.[38]\\r\\nMost provinces have rough provincial counterparts to major federal parties. However, these provincial parties are not usually formally linked to the federal parties that share the same name.[39] For example, no provincial Conservative or Progressive Conservative Party shares an organizational link to the federal Conservative Party of Canada, and neither do provincial Green Parties to the Green Party of Canada. Provincial New Democratic Parties, on the other hand, are fully integrated with the federal New Democratic Party ÿ meaning that provincial parties effectively operate as sections, with common membership, of the federal party. The Liberal Party of Canada shares such an organizational integration with the provincial Liberals in New Brunswick, Newfoundland and Labrador, Nova Scotia, and Prince Edward Island. Other provincial Liberal Parties are unaffiliated with their federal counterpart.[39]\\r\\nSome provinces have provincial political parties with no clear federal equivalent, such as the Alberta Party, Saskatchewan Party, and Wildrose Party.\\r\\nThe provincial political climate of Quebec is quite different: the main split is between sovereignty, represented by the Parti Qubcois and Qubec solidaire, and federalism, represented primarily by the Quebec Liberal Party.[40] The Coalition Avenir Qubec, meanwhile, takes an abstentionist position on the question and does not support or oppose sovereignty.\\r\\nCurrently, the only minority provincial/territorial government is held by the British Columbia Liberal Party after receiving 43 out of 87 seats in the 2017 general election.\\r\\nThe Canadian National Vimy Memorial, near Vimy, Pas-de-Calais, and the Beaumont-Hamel Newfoundland Memorial, near Beaumont-Hamel, France are ceremonially considered Canadian territory.[43] In 1922, the French government donated the land used for the Vimy Memorial \\"freely, and for all time, to the Government of Canada the free use of the land exempt from all taxes\\".[44] The site of the Somme battlefield near Beaumont-Hamel site was purchased in 1921 by the people of the Dominion of Newfoundland.[43] These sites do not, however, enjoy extraterritorial status and are thus subject to French law.\\r\\nSince Confederation in 1867, there have been several proposals for new Canadian provinces and territories. The Constitution of Canada requires an amendment for the creation of a new province[45] but the creation of a new territory requires only an act of Parliament, a legislatively simpler process.[46]\\r\\nIn late 2004, Prime Minister Paul Martin surprised some observers by expressing his personal support for all three territories gaining provincial status \\"eventually\\". He cited their importance to the country as a whole and the ongoing need to assert sovereignty in the Arctic, particularly as global warming could make that region more open to exploitation leading to more complex international waters disputes.[47]","input":"How many provinces and territories are in canada?"},{"output":"Bank of America","context":"\\r\\n\\r\\nThe Golden Gate Bridge is a suspension bridge spanning the Golden Gate, the one-mile-wide (1.6?km) strait connecting San Francisco Bay and the Pacific Ocean. The structure links the American city of San Francisco, California ÿ the northern tip of the San Francisco Peninsula ÿ to Marin County, carrying both U.S. Route 101 and California State Route 1 across the strait. The bridge is one of the most internationally recognized symbols of San Francisco, California, and the United States. It has been declared one of the Wonders of the Modern World by the American Society of Civil Engineers.[7]\\r\\n\\r\\nThe Frommer's travel guide describes the Golden Gate Bridge as \\"possibly the most beautiful, certainly the most photographed, bridge in the world.\\"[8][9] At the time of its opening in 1937, it was both the longest and the tallest suspension bridge in the world, with a main span of 4,200 feet (1,280?m) and a total height of 746 feet (227?m).\\r\\n\\r\\nBefore the bridge was built, the only practical short route between San Francisco and what is now Marin County was by boat across a section of San Francisco Bay. A ferry service began as early as 1820, with a regularly scheduled service beginning in the 1840s for the purpose of transporting water to San Francisco.[10]\\r\\n\\r\\nThe Sausalito Land and Ferry Company service, launched in 1867, eventually became the Golden Gate Ferry Company, a Southern Pacific Railroad subsidiary, the largest ferry operation in the world by the late 1920s.[10][11] Once for railroad passengers and customers only, Southern Pacific's automobile ferries became very profitable and important to the regional economy.[12] The ferry crossing between the Hyde Street Pier in San Francisco and Sausalito in Marin County took approximately 20 minutes and cost $1.00 per vehicle, a price later reduced to compete with the new bridge.[13] The trip from the San Francisco Ferry Building took 27 minutes.\\r\\n\\r\\nMany wanted to build a bridge to connect San Francisco to Marin County. San Francisco was the largest American city still served primarily by ferry boats. Because it did not have a permanent link with communities around the bay, the city's growth rate was below the national average.[14] Many experts said that a bridge could not be built across the 6,700?ft (2,042?m) strait, which had strong, swirling tides and currents, with water 372?ft (113?m) deep[15] at the center of the channel, and frequent strong winds. Experts said that ferocious winds and blinding fogs would prevent construction and operation.[14]\\r\\n\\r\\nAlthough the idea of a bridge spanning the Golden Gate was not new, the proposal that eventually took hold was made in a 1916 San Francisco Bulletin article by former engineering student James Wilkins.[16] San Francisco's City Engineer estimated the cost at $100 million, which would have been $2.12 billion in 2009, and impractical for the time. He asked bridge engineers whether it could be built for less.[10] One who responded, Joseph Strauss, was an ambitious engineer and poet who had, for his graduate thesis, designed a 55-mile-long (89?km) railroad bridge across the Bering Strait.[17] At the time, Strauss had completed some 400 drawbridgesmost of which were inlandand nothing on the scale of the new project.[3] Strauss's initial drawings[16] were for a massive cantilever on each side of the strait, connected by a central suspension segment, which Strauss promised could be built for $17?million.[10]\\r\\n\\r\\nLocal authorities agreed to proceed only on the assurance that Strauss would alter the design and accept input from several consulting project experts.[citation needed] A suspension-bridge design was considered the most practical, because of recent advances in metallurgy.[10]\\r\\n\\r\\nStrauss spent more than a decade drumming up support in Northern California.[18] The bridge faced opposition, including litigation, from many sources. The Department of War was concerned that the bridge would interfere with ship traffic. The navy feared that a ship collision or sabotage to the bridge could block the entrance to one of its main harbors. Unions demanded guarantees that local workers would be favored for construction jobs. Southern Pacific Railroad, one of the most powerful business interests in California, opposed the bridge as competition to its ferry fleet and filed a lawsuit against the project, leading to a mass boycott of the ferry service.[10]\\r\\n\\r\\nIn May 1924, Colonel Herbert Deakyne held the second hearing on the Bridge on behalf of the Secretary of War in a request to use federal land for construction. Deakyne, on behalf of the Secretary of War, approved the transfer of land needed for the bridge structure and leading roads to the \\"Bridging the Golden Gate Association\\" and both San Francisco County and Marin County, pending further bridge plans by Strauss.[19] Another ally was the fledgling automobile industry, which supported the development of roads and bridges to increase demand for automobiles.[13]\\r\\n\\r\\nThe bridge's name was first used when the project was initially discussed in 1917 by M.M. O'Shaughnessy, city engineer of San Francisco, and Strauss. The name became official with the passage of the Golden Gate Bridge and Highway District Act by the state legislature in 1923, creating a special district to design, build and finance the bridge.[20] San Francisco and most of the counties along the North Coast of California joined the Golden Gate Bridge District, with the exception being Humboldt County, whose residents opposed the bridge's construction and the traffic it would generate.[21]\\r\\n\\r\\nStrauss was chief engineer in charge of overall design and construction of the bridge project.[14] However, because he had little understanding or experience with cable-suspension designs,[22] responsibility for much of the engineering and architecture fell on other experts. Strauss's initial design proposal (two double cantilever spans linked by a central suspension segment) was unacceptable from a visual standpoint. The final graceful suspension design was conceived and championed by Leon Moisseiff, the engineer of the Manhattan Bridge in New York City.[23]\\r\\n\\r\\nIrving Morrow, a relatively unknown residential architect, designed the overall shape of the bridge towers, the lighting scheme, and Art Deco elements, such as the tower decorations, streetlights, railing, and walkways. The famous International Orange color was originally used as a sealant for the bridge.[24] The US Navy had wanted it to be painted with black and yellow stripes to ensure visibility by passing ships.[14]\\r\\n\\r\\nSenior engineer Charles Alton Ellis, collaborating remotely with Moisseiff, was the principal engineer of the project.[25] Moisseiff produced the basic structural design, introducing his \\"deflection theory\\" by which a thin, flexible roadway would flex in the wind, greatly reducing stress by transmitting forces via suspension cables to the bridge towers.[25] Although the Golden Gate Bridge design has proved sound, a later Moisseiff design, the original Tacoma Narrows Bridge, collapsed in a strong windstorm soon after it was completed, because of an unexpected aeroelastic flutter.[26] Ellis was also tasked with designing a \\"bridge within a bridge\\" in the southern abutment, to avoid the need to demolish Fort Point, a preÿCivil War masonry fortification viewed, even then, as worthy of historic preservation. He penned a graceful steel arch spanning the fort and carrying the roadway to the bridge's southern anchorage.[27]\\r\\n\\r\\nEllis was a Greek scholar and mathematician who at one time was a University of Illinois professor of engineering despite having no engineering degree. He eventually earned a degree in civil engineering from the University of Illinois prior to designing the Golden Gate Bridge and spent the last twelve years of his career as a professor at Purdue University. He became an expert in structural design, writing the standard textbook of the time.[28] Ellis did much of the technical and theoretical work that built the bridge, but he received none of the credit in his lifetime. In November 1931, Strauss fired Ellis and replaced him with a former subordinate, Clifford Paine, ostensibly for wasting too much money sending telegrams back and forth to Moisseiff.[28] Ellis, obsessed with the project and unable to find work elsewhere during the Depression, continued working 70 hours per week on an unpaid basis, eventually turning in ten volumes of hand calculations.[28]\\r\\n\\r\\nWith an eye toward self-promotion and posterity, Strauss downplayed the contributions of his collaborators who, despite receiving little recognition or compensation,[22] are largely responsible for the final form of the bridge. He succeeded in having himself credited as the person most responsible for the design and vision of the bridge.[28] Only much later were the contributions of the others on the design team properly appreciated.[28] In May 2007, the Golden Gate Bridge District issued a formal report on 70 years of stewardship of the famous bridge and decided to give Ellis major credit for the design of the bridge.\\r\\n\\r\\nThe Golden Gate Bridge and Highway District, authorized by an act of the California Legislature, was incorporated in 1928 as the official entity to design, construct, and finance the Golden Gate Bridge.[14] However, after the Wall Street Crash of 1929, the District was unable to raise the construction funds, so it lobbied for a $30?million bond measure. The bonds were approved in November 1930,[17] by votes in the counties affected by the bridge.[29] The construction budget at the time of approval was $27?million. However, the District was unable to sell the bonds until 1932, when Amadeo Giannini, the founder of San Franciscoÿbased Bank of America, agreed on behalf of his bank to buy the entire issue in order to help the local economy.[10]\\r\\n\\r\\nConstruction began on January 5, 1933.[10] The project cost more than $35?million,[30]  ($493?million in 2016 dollars[31]) completing ahead of schedule and $1.3 million under budget.[32]\\r\\nThe Golden Gate Bridge construction project was carried out by the McClintic-Marshall Construction Co., a subsidiary of Bethlehem Steel Corporation founded by Howard H. McClintic and Charles D. Marshall, both of Lehigh University.\\r\\n\\r\\nStrauss remained head of the project, overseeing day-to-day construction and making some groundbreaking contributions. A graduate of the University of Cincinnati, he placed a brick from his alma mater's demolished McMicken Hall in the south anchorage before the concrete was poured. He innovated the use of movable safety netting beneath the construction site, which saved the lives of many otherwise-unprotected ironworkers. Of eleven men killed from falls during construction, ten were killed on February 17, 1937, when the bridge was near completion when the net failed under the stress of a scaffold that had fallen.[33] The workers' platform that was attached to a rolling hanger on a track collapsed when the bolts that were connected to the track were too small and the amount of weight was too great to bear. The platform fell into the safety net, but was too heavy and the net gave way. Two out of the twelve workers survived the 200-foot (61?m) fall into the icy waters, including the 37-year-old foreman, Slim Lambert. Nineteen others who were saved by the net over the course of construction became members of their Half Way to Hell Club.[34]\\r\\n\\r\\nThe project was finished and opened May 27, 1937.  The Bridge Round House diner was then included in the southeastern end of the Golden Gate Bridge, adjacent to the tourist plaza which was renovated in 2012.[35]  The Bridge Round House, an Art Deco design by Alfred Finnila completed in 1938, has been popular throughout the years as a starting point for various commercial tours of the bridge and an unofficial gift shop.[36]  The diner was renovated in 2012[35] and the gift shop was then removed as a new, official gift shop has been included in the adjacent plaza.[36]\\r\\n\\r\\nDuring the bridge work, the Assistant Civil Engineer of California Alfred Finnila had overseen the entire iron work of the bridge as well as half of the bridge's road work.[37]  With the death of Jack Balestreri in April 2012, all workers involved in the original construction are now deceased.\\r\\n\\r\\nThe bridge-opening celebration began on May 27, 1937, and lasted for one week. The day before vehicle traffic was allowed, 200,000 people crossed either on foot or on roller skates.[10] On opening day, Mayor Angelo Rossi and other officials rode the ferry to Marin, then crossed the bridge in a motorcade past three ceremonial \\"barriers\\", the last a blockade of beauty queens who required Joseph Strauss to present the bridge to the Highway District before allowing him to pass. An official song, \\"There's a Silver Moon on the Golden Gate\\", was chosen to commemorate the event. Strauss wrote a poem that is now on the Golden Gate Bridge entitled \\"The Mighty Task is Done.\\" The next day, President Roosevelt pushed a button in Washington, D.C. signaling the official start of vehicle traffic over the Bridge at noon. As the celebration got out of hand there was a small riot in the uptown Polk Gulch area. Weeks of civil and cultural activities called \\"the Fiesta\\" followed. A statue of Strauss was moved in 1955 to a site near the bridge.[16]\\r\\n\\r\\nIn May 1987, as part of the 50th anniversary celebration, the Golden Gate Bridge district again closed the bridge to automobile traffic and allowed pedestrians to cross the bridge. However, this celebration attracted 750,000 to 1,000,000 people, and ineffective crowd control meant the bridge became congested with roughly 300,000 people, causing the center span of the bridge to flatten out under the weight.[38] Although the bridge is designed to flex in that way under heavy loads, and was estimated not to have exceeded 40% of the yielding stress of the suspension cables,[39] bridge officials stated that uncontrolled pedestrian access was not being considered as part of the 75th anniversary on Sunday, May 27, 2012,[40][41][42] because of the additional law enforcement costs required \\"since 9/11\\".[43]\\r\\n\\r\\nA pedestrian poses at the old railing on opening day, 1937.\\r\\n\\r\\nOpening of the Golden Gate Bridge\\r\\n\\r\\nOfficial invitation to the opening of the bridge. This copy was sent to the City of Seattle.\\r\\n\\r\\nUntil 1964, the Golden Gate Bridge had the longest suspension bridge main span in the world, at 4,200 feet (1,300?m). Since 1964 its main span length has been surpassed by ten bridges; it now has the second-longest main span in the United States, after the Verrazano-Narrows Bridge in New York City. The total length of the Golden Gate Bridge from abutment to abutment is 8,981 feet (2,737?m).[44]\\r\\n\\r\\nThe Golden Gate Bridge's clearance above high water averages 220 feet (67?m) while its towers, at 746 feet (227?m) above the water,[44] were the world's tallest on a suspension bridge until 1993 when it was surpassed by the Mezcala Bridge, in Mexico.\\r\\n\\r\\nThe weight of the roadway is hung from 250 pairs of vertical suspender ropes, which are attached to two main cables. The main cables pass over the two main towers and are fixed in concrete at each end. Each cable is made of 27,572 strands of wire. The total length of galvanized steel wire used to fabricate both main cables is estimated to be 80,000 miles (130,000?km).[44]\\r\\n\\r\\nIn 1961, BART completed a study that determined the bridge's suspension section was capable of supporting planned rapid transit rail service on a new lower deck.[45] These plans were not followed out.\\r\\n\\r\\nThe bridge has approximately 1,200,000 total rivets.[citation needed]\\r\\n\\r\\nThe color of the bridge is officially an orange vermilion called international orange.[46] The color was selected by consulting architect Irving Morrow[47] because it complements the natural surroundings and enhances the bridge's visibility in fog.\\r\\nAesthetics was the foremost reason why the first design of Joseph Strauss was rejected. Upon re-submission of his bridge construction plan, he added details, such as lighting, to outline the bridge's cables and towers.[48] In 1999, it was ranked fifth on the List of America's Favorite Architecture by the American Institute of Architects.\\r\\n\\r\\nThe bridge was originally painted with red lead primer and a lead-based topcoat, which was touched up as required. In the mid-1960s, a program was started to improve corrosion protection by stripping the original paint and repainting the bridge with zinc silicate primer and vinyl topcoats.[49][46] Since 1990, acrylic topcoats have been used instead for air-quality reasons. The program was completed in 1995 and it is now maintained by 38 painters who touch up the paintwork where it becomes seriously corroded.[50]\\r\\n\\r\\nA view of the Golden Gate Bridge from the Marin Headlands on a foggy morning at sunrise\\r\\n\\r\\nView of the northern tower of the bridge\\r\\n\\r\\nMost maps and signage mark the bridge as part of the concurrency between U.S. Route 101 and California State Route 1. Although part of the National Highway System, the bridge is not officially part of California's Highway System.[51] For example, under the California Streets and Highways Code  401, Route 101 ends at \\"the approach to the Golden Gate Bridge\\" and then resumes at \\"a point in Marin County opposite San Francisco\\". The Golden Gate Bridge, Highway and Transportation District has jurisdiction over the segment of highway that crosses the bridge instead of the California Department of Transportation (Caltrans).\\r\\n\\r\\nThe movable median barrier between the lanes is moved several times daily to conform to traffic patterns. On weekday mornings, traffic flows mostly southbound into the city, so four of the six lanes run southbound. Conversely, on weekday afternoons, four lanes run northbound. During off-peak periods and weekends, traffic is split with three lanes in each direction.[52]\\r\\n\\r\\nFrom 1968 to 2015, opposing traffic was separated by small, plastic pylons, and during that time, there were 16 fatalities resulting from 128 head-on collisions.[53] To improve safety, the speed limit on the Golden Gate Bridge was reduced from 50 to 45?mph (80 to 72?km/h) on October 1, 1983.[54] Although there had been discussion concerning the installation of a movable barrier since the 1980s, only in March 2005 did the Bridge Board of Directors commit to finding funding to complete the $2?million study required prior to the installation of a movable median barrier.[53] Installation of the resulting barrier was completed on January 11, 2015, following a closure of 45.5 hours to private vehicle traffic, the longest in the bridge's history.  The new barrier system, including the zipper trucks, cost approximately $30.3 million to purchase and install.[53][55]\\r\\n\\r\\nThe bridge is popular with pedestrians and bicyclists, and was built with walkways on either side of the six vehicle traffic lanes. Initially, they were separated from the traffic lanes by only a metal curb, but railings between the walkways and the traffic lanes were added in 2003, primarily as a measure to prevent bicyclists from falling into the roadway.[56]\\r\\n\\r\\nThe main walkway is on the eastern side, and is open for use by both pedestrians and bicycles in the morning to mid-afternoon during weekdays (5 am to 3:30?pm), and to pedestrians only for the remaining daylight hours (until 6?pm, or 9?pm during DST). The eastern walkway is reserved for pedestrians on weekends (5 am to 6?pm, or 9?pm during DST), and is open exclusively to bicyclists in the evening and overnight, when it is closed to pedestrians. The western walkway is open only for bicyclists and only during the hours when they are not allowed on the eastern walkway.[57]\\r\\n\\r\\nBus service across the bridge is provided by two public transportation agencies: San Francisco Muni and Golden Gate Transit. Muni offers Saturday and Sunday service on the Marin Headlands Express bus line, and Golden Gate Transit runs numerous bus lines throughout the week.[58][59] The southern end of the bridge, near the toll plaza and parking lot, is also accessible daily from 5:30?a.m. to midnight by Muni line 28.[60] The Marin Airporter, a private company, also offers service across the bridge between Marin County and San Francisco International Airport.[61]\\r\\n\\r\\nA visitor center and gift shop, dubbed the \\"Bridge Pavilion\\", is located on the San Francisco side of the bridge, adjacent to the southeast parking lot.  It opened in 2012, in time for the bridge's 75th anniversary celebration.  A cafe, outdoor exhibits, and restroom facilities are located nearby.[62] On the Marin side of the bridge, only accessible from the northbound lanes, is the H. Dana Bower Rest Area and Vista Point,[63] named after the first landscape architect for the California Division of Highways.[64]\\r\\n\\r\\nLands and waters under and around the bridge are homes to varieties of wildlife such as bobcats and sea lions.[65][66] Three species of cetaceans that had been absent in the area for many years show recent recoveries/(re)colonizations vicinity to the bridge, and researchers study them to strengthen protections, concerning actions by public and recommending to watch whales either from the bridge and nearby, or to use a local whale watching operator.[67][68][69]\\r\\n\\r\\nWhen the Golden Gate Bridge opened in 1937, the toll was 50 cents per car, collected in each direction. It was reduced to 40 cents each way in 1950, then lowered to 25 cents in 1955. In 1968, the bridge was converted to only collect tolls from southbound traffic, with the toll amount reset back to 50 cents.[70]\\r\\n\\r\\nThe last of the construction bonds were retired in 1971, with $35?million in principal and nearly $39?million in interest raised entirely from bridge tolls.[54] Tolls continued to be collected and subsequently incrementally raised; by 1991, the toll was $3.[70]\\r\\n\\r\\nThe bridge began accepting tolls via the FasTrak electronic toll collection system in 2002, with $4 tolls for FasTrak users and $5 for those paying cash.[70] In November 2006, the Golden Gate Bridge, Highway and Transportation District recommended a corporate sponsorship program for the bridge to address its operating deficit, projected at $80?million over five years. The District promised that the proposal, which it called a \\"partnership program,\\" would not include changing the name of the bridge or placing advertising on the bridge itself. In October 2007, the Board unanimously voted to discontinue the proposal and seek additional revenue through other means, most likely a toll increase.[71][72] The District later increased the toll amounts in 2008 to $5 for FasTrak users and $6 to those paying cash.[70]\\r\\n\\r\\nIn an effort to save $19.2?million over the following 10 years, the Golden Gate District voted in January 2011 to eliminate all toll takers by 2012 and use only open road tolling.[73] Subsequently, this was delayed and toll taker elimination occurred in March 2013. The cost savings have been revised to $19 million over an eight-year period. In addition to FasTrak, the Golden Gate District implemented the use of license plate tolling (branded as \\"Pay-by-Plate\\"), and also a one time payment system for drivers to pay before or after their trip on the bridge. Twenty-eight positions were eliminated as part of this plan.[74]\\r\\n\\r\\nOn April 7, 2014, the toll for users of FasTrak was increased from $5 to $6, while the toll for drivers using either the license plate tolling or the one time payment system was raised from $6 to $7. Bicycle, pedestrian, and northbound motor vehicle traffic remain toll free. For vehicles with more than two axles, the toll rate is $7 per axle for those using license plate tolling or the one time payment system, and $6 per axle for FasTrak users. During peak traffic hours, carpool vehicles carrying two or more people and motorcycles pay a discounted toll of $4; drivers must have Fastrak to take advantage of this carpool rate.[74] The Golden Gate Transportation District then planned to increase the tolls by 25 cents in July 2015, and then by another 25 cents each of the next three years.[75]\\r\\n\\r\\nIn March 2008, the Golden Gate Bridge District board approved a resolution to start congestion pricing at the Golden Gate Bridge, charging higher tolls during the peak hours, but rising and falling depending on traffic levels. This decision allowed the Bay Area to meet the federal requirement to receive $158?million in federal transportation funds from USDOT Urban Partnership grant.[77] As a condition of the grant, the congestion toll was to be in place by September 2009.[78][79]\\r\\n\\r\\nThe first results of the study, called the Mobility, Access and Pricing Study (MAPS), showed that a congestion pricing program is feasible.[80] The different pricing scenarios considered were presented in public meetings in December 2008.[81]\\r\\n\\r\\nIn August 2008, transportation officials ended the congestion pricing program in favor of varying rates for metered parking along the route to the bridge including on Lombard Street and Van Ness Avenue.[82]\\r\\n\\r\\nThe Golden Gate Bridge is the second-most used suicide site/suicide bridge in the world, after the Nanjing Yangtze River Bridge.[83] The deck is about 245 feet (75?m) above the water.[84] After a fall of four seconds, jumpers hit the water at around 75?mph or about 120?km/h. Most of the jumpers die from impact trauma. About 5% of the jumpers survive the initial impact but generally drown or die of hypothermia in the cold water.[85][86]\\r\\n\\r\\nAfter years of debate and over an estimated 1,500 deaths, suicide barriers began to be installed in April 2017. Construction will take approximately four years at a cost of over $200 million.[87]\\r\\n\\r\\nSince its completion, the Golden Gate Bridge has been closed because of weather conditions only three times: on December 1, 1951, because of gusts of 69?mph (111?km/h); on December 23, 1982, because of winds of 70?mph (113?km/h); and on December 3, 1983, because of wind gusts of 75?mph (121?km/h).[49]\\r\\n\\r\\nAn anemometer, placed midway between the two towers on the west side of the bridge, has been used to measure wind speeds. Another anemometer was placed on one of the towers.\\r\\n\\r\\n\\r\\n\\r\\nModern knowledge of the effect of earthquakes on structures led to a program to retrofit the Golden Gate to better resist seismic events. The proximity of the bridge to the San Andreas Fault places it at risk for a significant earthquake. Once thought to have been able to withstand any magnitude of foreseeable earthquake, the bridge was actually vulnerable to complete structural failure (i.e., collapse) triggered by the failure of supports on the 320-foot (98?m) arch over Fort Point.[88] A $392?million program was initiated to improve the structure's ability to withstand such an event with only minimal (repairable) damage. One challenging undertaking is completing this program without disrupting traffic. A complex electro-hydraulic synchronous lift system was custom built for construction of temporary support towers and a series of intricate lifts, transferring the loads from the existing bridge onto the temporary supports. This was completed with engineers from Balfour Beatty and Enerpac, accomplishing this task without disrupting day-to-day San Francisco commuter traffic.[89][90] The retrofit was planned to be completed in 2012.[90][91]\\r\\n\\r\\n\\r\\n\\r\\nThe former elevated approach to the Golden Gate Bridge through the San Francisco Presidio, known as Doyle Drive, dated to 1933 and was named after Frank P. Doyle, President and son of the founder of the Exchange Bank in Santa Rosa, and the man who, more than any other person, made it possible to build the Golden Gate Bridge.[92] The highway carried about 91,000 vehicles each weekday between downtown San Francisco and the North Bay and points north.[93] The road was deemed \\"vulnerable to earthquake damage\\", had a problematic 4-lane design, and lacked shoulders, and a San Francisco County Transportation Authority study recommended that it be replaced. Construction on the $1?billion[94] replacement, temporarily known as the Presidio Parkway, began in December 2009.[95]\\r\\nThe elevated Doyle Drive was demolished on the weekend of April 27ÿ30, 2012, and traffic used a part of the partially completed Presidio Parkway, until it was switched onto the finished Presidio Parkway on the weekend of July 9ÿ12, 2015. As of May 2012, an official at Caltrans said there is no plan to permanently rename the portion known as Doyle Drive.[96]\\r\\n\\r\\nAs a prominent American landmark, the Golden Gate Bridge has been used in numerous media which includes books, films and video games.","input":"Who paid to build the golden gate bridge?"},{"output":"west/north central Iowa","context":"The production of corn (Zea mays mays, also known as 'maize') plays a major role in the economy of the United States. The country is the largest producer of corn in the world, with 96,000,000 acres (39,000,000?ha) of land reserved for corn production. Corn growth is dominated by west/north central Iowa and east central Illinois. Approximately 13% of its annual yield is exported.[1]\\r\\n\\r\\n\\r\\nCorn spread across North America a few thousand years ago.[2] The original corn plant known as teosinte is still grown in Mexico. Newer varieties are much larger, due to efforts of Native Americans and scientific research. It is now the third leading grain crop in the world.[3]\\r\\nBy the time scientific assessment of conduciveness to grow corn in the United States was undertaken by Meriwether Lewis in 1804, the immigrant settlers had already spread its growth in many parts of the country due to its suitability in varying climatic and soil conditions. Once the suitability of land in the central part of the country, the Midwestern United States, was scientifically established as fertile and rich by Lewis and Clark, settlers moved to the area in large numbers and started growing large corn crops, reaping large benefits.\\r\\nOver the centuries, the crop varieties underwent changes to get better yields while farming methods were improved. As a result, the fertile belt soon came to be known as \\"the Corn Belt\\". Hybrid cropping techniques were widely practiced from the late 1880s, and the hybrid varieties developed with cross and re-cross breeding techniques developed by university research. This ushered a new age of agriculture. The 1% area devoted to hybrid varieties in 1934 rose to 78% in the 1940s and continued to rise thereafter. In the 1950s, Henry A. Wallace, former Vice President and former Secretary of Agriculture, and an early developer of hybrid seeds, observed that \\"the Corn Belt had developed into the most productive agricultural civilization the world has ever seen.\\" This trend has continued and now the corn production level in American farms is a significant 20% higher per acre than in the rest of the world.[4]\\r\\nAs the growth of corn has spread to extensive production in 14 states (though it is grown to a lesser extent in all the other US states), a coalition of farmers associations in all these states has been established. This association is known as the Corn Farmers Coalition, which is a union of the National Corn Growers Association and 14 state corn associations (including Iowa Corn).[5]\\r\\nThe total production of corn in the US for the year 2013-14 is reported to be 13.016 billion bushels of which the major use is for manufacture of ethanol and its co-product (Distillers' Dried Grains with Solubles) accounting for 37% (27% + 10%) or 4,845 million bushels (3,552 + 1,293). The other uses are given in the table.[6]\\r\\ncereals.\\r\\nThe final estimate of corn production for the years 1950 to 1959 in the United States is given as some three billion bushels and in recent years, some nine billion bushels are produced each year.[2] Corn growth is dominated by west north central Iowa and east central Illinois.[7] In 2011, the national average production was 147 bushels per acre, and reported to be 20 bushels per acre more than the yield in 2002. Based on a national contest in 2011 when an average of 300 bushels per acre was achieved others are sure to follow suit which result in a yield of 300 bushels per acre by 2030 from the same extent land holdings under corn.[4][6]\\r\\nTwo methods are used to produce ethanol from corn and other plants such as sugar cane. The residual product (DDGS) is about 33% of the input stream and is used as livestock feed. Ethanol is blended with gasoline to produce E10 and E85 fuels for automobile vehicles. Its manufacture has created 400,000 jobs in the US and its environmental friendliness is recorded in the form of reduced gas emissions of 25.3 million metric tons. One bushel of corn can produce 2.8 gallons of ethanol, 17.5?lbs of livestock feed and 18?lbs of carbon dioxide.[8]\\r\\nIt is also reported that every acre of corn ultimately results in reducing 8 tonnes (7.9 long tons; 8.8 short tons) of greenhouse gas emission.[citation needed] On account of great demand for ethanol corn is fetching higher prices. This has resulted in farmers increasing acreage under corn by adopting crop rotations between corn and soybeans; the latter crop's production has thus declined. Further more acreage under corn has also been allowed to be increased under the Federal Agriculture Improvement and Reform Act of 1996 overriding the act of 1983, which had been fixed at 60,200,000 hectares (149,000,000 acres).[9]\\r\\nThere are 80,000,000 acres (32,000,000?ha) of land dedicated exclusively to corn cultivation in the United States. The United States is the world's leading producer of corn,[10] having produced 333,010,910 tonnes (327,751,510 long tons; 367,081,690 short tons) of the crop in the year 2009.[11]\\r\\nOut of 316,000 corn farms about 300,000 farms (95% of them) are owned by families. More than 30% of corn farms are operated by women.[12][13]\\r\\nHighest yield of over 12 billion bushels have been recorded up to 2011 with 12.4 billion bushels reported in 2011 with yields of more than 140 bushels per acre. A milestone in production in the US is that the farmers take out 20% more corn per acre than in any other part of the world.[14] Farming practice is based on irrigation only in about 11% area while the balance area is under un-irrigated conditions. The farm practices have also resulted in implementing conservation measures which have reduced soil erosion to the extent of 44%.\\r\\nSee also US Agricultural subsidies\\r\\nCorn in the United States has been subsidized since the 1930s, when a drop in demand from post-war Europe caused a food glut and prices crashed. In the 1980s, subsidies increased substantially.[15]\\r\\nUS subsidies for corn have averaged 4.7 billion dollars per year over the twenty years from 1995-2014 inclusive.[16] 2014 projections were that the US would spend $97.29 billion/year on farm and food programs over the next decade.[17]\\r\\nIn 2014, the \\"direct payments\\" portion of subsidies was abolished, and subsidies were mostly replaced with two programs. Farmers may pick Price Loss Coverage (a guarantee that the farmer will get a certain price for their crop) and Agricultural Risk Coverage (which guarantees a certain amount of revenue).[16][15]\\r\\nThe subsidies have been criticized for:\\r\\nUS$267 is spent by the average American annually on purchasing corn.[19] In 2015, one bushel of corn costs $3.50.[20] The value of individual corn farms varies from location to location, depending on the amount of bushels produced and the quality of corn.[21] Other factors such as the weather or economic crises may cause corn prices to fluctuate or to rise.[22][23][24] The value of corn is increasing, due to the country's greater demand and reliance for corn.[24]\\r\\nIowa, the largest producer of corn in the US, grows three times as much corn as Mexico. Iowa harvested 3,548 acres (1,436?ha) of sweet corn in 2007. In 2011, the state had 92,300 corn farms on 30,700,000 acres (12,400,000?ha), the average size being 333 acres (135?ha), and the average dollar value per acre being US$6,708. In the same year, there were 13.7 million harvested acres of corn for grain, producing 2.36 billion bushels, which yielded 172.0 bu/acre, with US$14.5 billion of corn value of production.[25] Almost 1.88 billion bushels of corn were grown in the state in 2012 on 13.7 million acres of land, while the 2013 projections are 2.45 billion bushels of corn on 13.97 million acres of land.[26]\\r\\nNebraska is known as the \\"Cornhusker State\\" ÿ and is the third largest corn-producing state in the United States.[27]\\r\\nCorn is Minnesota's largest crop. In 1922-31, production averaged 30.4 bushels per acre; in 1947-56, it average 46.6 bushels per acre; in 1973, it averaged 91.4 bushels per acre; and in 1994, the average was 142 bushels per acre. In 1935-46, a shift to hybrid varieties occurred.[28] In 2010, the state produced 1.29 billion bushels.[29] In 2012, Minnesota's farmers produced the largest corn crop in the state's history, at 1.37 billion bushels harvested, equaling 165 bushels per acre, on 250,000 acres.[30]\\r\\nThe first corn varieties grown in Illinois were those obtained from local Indians or varieties brought to Illinois from the New England states. After the Civil War, varieties were developed which were adapted to the state's soils and climate, such as Reid's Yellow Dent. During the period of 1900-05, there were 10,500,000 acres planted, with a decline to 8,862,000 acres in 1925-30.[31] In 2012, Illinois sowed 12.8 million acres of corn in 2012, ranking fourth in corn production, behind Iowa, Minnesota, and Nebraska. The state averaged 105 bushels per acre in 2012, down from 157-bushel per acre in 2010 and 2011.[32]\\r\\nAlthough the state of Alaska has a cold temperate, some Alaskan farmers still manage to grow corn, through means such as greenhouse farming. Corn is popular among Alaskans.[33]\\r\\nCorn is a popular crop in the state of Indiana; it is mostly grown as animal feed.[34] Indiana is located in the United States' Corn Belt.[35]\\r\\nThe state of Texas is a great producer of corn; the final estimate of corn produced in 2010 was some 301 million bushels on 2,300,000 acres (930,000?ha) of land, totaling to $1.2 billion of crop.[36]\\r\\nCorn was introduced to Alabama in around the eighteenth century; there have been traces of corn found in Nuyuka's Upper Creek village which dates back to the period.[37]","input":"Where is corn mostly grown in the u.s?"},{"output":"fifteen dams on the main stem of the Colorado","context":"This is a list of dams on the Colorado River system of the southwestern United States and northwestern Mexico. The Colorado runs 1,450?mi (2,330?km) from the Rocky Mountains to the Gulf of California, draining parts of seven U.S. states and two Mexican states. The river system is one of the most heavily developed in the world, with fifteen dams on the main stem of the Colorado and hundreds more on tributaries. Collectively, dams in the Colorado River basin can hold four to five times the river's annual flow, generating hydroelectricity and supplying irrigation and municipal water for over 35 million people.\\r\\nDams on tributaries are listed if they are taller than 250?ft (76?m), store more than 50,000?acre{ft (62,000?dam3), or are otherwise historically notable. Tributary dams are organized into two lists; those in the Upper Basin, defined as the half of the Colorado River basin above Lee's Ferry, Arizona, and the Lower Basin. The Upper Basin include such tributary systems as the Green, Gunnison and San Juan. Tributaries in the Lower Basin include the Little Colorado, Virgin and Gila. A key tributary of the San Juan River, the Animas River, was severely affected by the EPA's accidental August 2015 Gold King Mine waste water spill.\\r\\nNotes:","input":"How many dams are in the colorado river?"},{"output":"Lovey","context":"\\"Lovey\\" Howell (ne Wentworth), referred to as \\"Mrs. Howell\\" by characters other than her husband, is a fictional character from the 1964 to 1967 television show Gilligan's Island. Played by Natalie Schafer, the character was a rich, spoiled socialite, married to Thurston Howell III.\\r\\nWhile Mr. Howell always called her \\"Lovey\\", the other castaways always called her \\"Mrs. Howell\\". There are only two times someone besides Thurston calls her Lovey: in the pilot, when the radio is announcing the missing people, the announcer says \\"Thurston Howell III and his wife, international hostess Lovey Howell\\"; and in the episode in which Gilligan thinks he wins the lottery and invites all the people into the Howells' club, The Professor greets Mr. and Mrs. Howell as Thurston and Lovey.\\r\\nIn episode 31 of season 2, \\"Mr. and Mrs.????\\", in which the Howells were having marital problems, she mentioned her maiden name was Wentworth. Her actual first name was never mentioned in the series itself or its unaired pilot episode.\\r\\nNot much information was revealed about her life before being marooned with her fellow castaways, but she introduced herself in episode 6 of season 2 (\\"Quick Before It Sinks\\") as \\"Mrs. Thurston Howell III, from New York, Palm Beach and of course, Paris, mon cher.\\" She also mentions in episode 17 of season 2:\\"You've Been Disconnected\\" that she spoke fluent French and Italian, hinting at a classical education. In episode 9 of season 3 (\\"Ring Around Gilligan\\"), Mr. Howell mentions her having studied at Vassar, saying \\"Mommy warned me about you Vassar girls and your long gym classes\\".\\r\\nDuring episode 4 of season 2:\\"Smile, You're on Mars Camera\\" Thurston Howell indicates that Lovey, or at least her family, had money of their own; Mr. Howell describes her as being an heiress who is \\"loaded\\". It is also revealed that Mrs. Howell's father gave them an oil company (apparently dry) in \\"Dustbowl, Oklahoma\\" as a wedding gift, which she claims he thought was a football stadium (episode 13 of season 1:\\"Three Million Dollars More or Less\\").\\r\\nLovey claimed she was a member of the Daughters of the American Revolution (DAR) (episode 30 of season 2, \\"'V' for Vitamins\\"), to have been presented at the Court of St. James and to have been selected the \\"Queen of the Prune-Bowl Parade\\" (episode 30 of season 3, \\"Gilligan, the Goddess\\").\\r\\nAlthough spoiled and preoccupied with social status, Mrs. Howell was also kind and genuinely cared about the well-being of her fellow castaways. She frequently served as something of a mother figure to the two younger female castaways, Ginger Grant and Mary Ann Summers, offering advice (though she sometimes also displays jealousy toward the two younger women). One of the Island \\"Visitors\\" she can't stand is socialite Erika-Tiffany Smith {played by Zsa Zsa Gabor} because her name appears before the Howells in the \\"Social Register\\". She also claims to be close friends with \\"Grace and Prince Rainier\\" on several episodes.\\r\\nSeveral times she acted as a motherly figure to Gilligan, such as psychoanalyzing him, adopting him, and praising him for his accomplishments when no one else did. She often plotted with her husband several times to steal/access/manipulate things from the other survivors. She once tried to get Gilligan and Mary Ann to wed, kept a gold mine secret from the rest of the group, divulged secrets about stolen jewelry from a parrot, schemed to convince a burnt out artist to leave the island and got through to an uncivilized jungle boy played by Kurt Russell.\\r\\nIt was once quoted by Thurston Howell himself that their brilliance together was exceeded only by their greed. They had also proven to have a house in each state and several staffs of servants, including an upstairs maid, a downstairs maid and a butler who served him breakfast in bed. Thurston once quoted that the neighborhood of one million dollars apiece was considered a slum area where they resided.\\r\\nAlthough the Howells were portrayed as childless in the original series, their son Thurston Howell IV (portrayed by David Ruprecht)was introduced in the reunion movie The Harlem Globetrotters on Gilligan's Island.","input":"What was mrs howell's first name on gilligan's island?"},{"output":"Harry S. Truman","context":"","input":"Who is the president during the korean war?"},{"output":"original sin does not have the character of a personal fault in any of Adam's descendants ... but the consequences for nature, weakened and inclined to evil, persist in man","context":"Original sin, also called ancestral sin,[1] is the Christian doctrine of humanity's state of sin resulting from the fall of man, stemming from Adam and Eve's rebellion in Eden, namely the sin of disobedience in consuming from the tree of knowledge of good and evil.[2] This condition has been characterized in many ways, ranging from something as insignificant as a slight deficiency, or a tendency toward sin yet without collective guilt, referred to as a \\"sin nature\\", to something as drastic as total depravity or automatic guilt of all humans through collective guilt.[3]\\r\\nThe concept of original sin was first alluded to in the 2nd century by Irenaeus, Bishop of Lyon in his controversy with certain dualist Gnostics[citation needed]. Other church fathers such as Augustine also developed the doctrine,[2] seeing it as based on the New Testament teaching of Paul the Apostle (Romans 5:12ÿ21 and 1 Corinthians 15:22) and the Old Testament verse of Psalms 51:5.[4][5][6][7][8] Tertullian, Cyprian, Ambrose and Ambrosiaster considered that humanity shares in Adam's sin, transmitted by human generation. Augustine's formulation of original sin was popular among Protestant reformers, such as Martin Luther and John Calvin, who equated original sin with concupiscence (or \\"hurtful desire\\"), affirming that it persisted even after baptism and completely destroyed freedom.[2] The Jansenist movement, which the Catholic Church declared to be heretical, also maintained that original sin destroyed freedom of will.[9]\\r\\n\\r\\n\\r\\nThe doctrine of ancestral fault (? ??϶ϫ progonikon hamartema), i.e. the sins of the forefathers leading to punishment of their descendants, was presented as a tradition of immemorial antiquity in ancient Greek religion by Celsus in his True Doctrine, a polemic attacking Christianity. Celsus is quoted as attributing to \\"a priest of Apollo or of Zeus\\" the saying that \\"the mills of the gods grind slowly, even to children's children, and to those who are born after them\\".[10] The idea of divine justice taking the form of collective punishment is also ubiquitous in the Hebrew Bible.[11]\\r\\nSt Paul's idea of redemption hinged upon the contrast between the sin of Adam and the death and resurrection of Jesus. \\"Therefore, just as sin entered the world through one man, and death through sin, and in this way death came to all people, because all sinned.\\"[12] \\"For as in Adam all die, so in Christ all will be made alive.\\"[13] Up till then the transgression in the Garden of Eden had not been given great significance. As the Jesus scholar, Geza Vermes has said:\\r\\nPaul believed that Adam's transgression in a mysterious way affected the nature of the human race. The primeval sin, a Pauline creation with no biblical or post-biblical Jewish precedent, was irreparable by ordinary human effort.[14]\\r\\nThe formalized Christian doctrine of original sin was first developed in the 2nd century by Irenaeus, the Bishop of Lyon, in his struggle against Gnosticism.[2] Irenaeus contrasted their doctrine with the view that the Fall was a step in the wrong direction by Adam, with whom, Irenaeus believed, his descendants had some solidarity or identity.[15] Irenaeus believed that Adam's sin had grave consequences for humanity, that it is the source of human sinfulness, mortality and enslavement to sin, and that all human beings participate in his sin and share his guilt.[16]\\r\\nThe Greek Fathers emphasized the cosmic dimension of the Fall, namely that since Adam human beings are born into a fallen world, but held fast to belief that man, though fallen, is free.[2] They thus did not teach that human beings are deprived of free will and involved in total depravity, which is one understanding of original sin.[17][18] During this period the doctrines of human depravity and the inherently sinful nature of human flesh were taught by Gnostics, and orthodox Christian writers took great pains to counter them.[19][20] Christian apologists insisted that God's future judgment of humanity implied humanity must have the ability to live righteously.[21][22]\\r\\nHistorian Robin Lane Fox argues that the foundation of the doctrine of original sin as accepted by the Church was ultimately based on a mistranslation of Paul the Apostle's Epistle to the Romans (Romans 5:12ÿ21) by Augustine, in his On the Grace of Christ, and on Original Sin\\".[23]\\r\\nAugustine of Hippo (354ÿ430) taught that Adam's sin[24] is transmitted by concupiscence, or \\"hurtful desire\\",[25][26] resulting in humanity becoming a massa damnata (mass of perdition, condemned crowd), with much enfeebled, though not destroyed, freedom of will.[2] When Adam sinned, human nature was thenceforth transformed. Adam and Eve, via sexual reproduction, recreated human nature. Their descendants now live in sin, in the form of concupiscence, a term Augustine used in a metaphysical, not a psychological sense.[27] Augustine insisted that concupiscence was not a being but a bad quality, the privation of good or a wound.[28] He admitted that sexual concupiscence (libido) might have been present in the perfect human nature in paradise, and that only later it became disobedient to human will as a result of the first couple's disobedience to God's will in the original sin.[29] In Augustine's view (termed \\"Realism\\"), all of humanity was really present in Adam when he sinned, and therefore all have sinned. Original sin, according to Augustine, consists of the guilt of Adam which all humans inherit. As sinners, humans are utterly depraved in nature, lack the freedom to do good, and cannot respond to the will of God without divine grace. Justo Gonzalez interprets Augustine's teaching that grace is irresistible, results in conversion, and leads to perseverance.[30]\\r\\nAugustine articulated his explanation in reaction to Pelagianism, which insisted that humans have of themselves, without the necessary help of God's grace, the ability to lead a morally good life, and thus denied both the importance of baptism and the teaching that God is the giver of all that is good. Pelagius claimed that the influence of Adam on other humans was merely that of bad example. Augustine held that the effects of Adam's sin are transmitted to his descendants not by example but by the very fact of generation from that ancestor. A wounded nature comes to the soul and body of the new person from his/her parents, who experience libido (or concupiscence). Augustine's view was that human procreation was the way the transmission was being effected. He did not blame, however, the sexual passion itself, but the spiritual concupiscence present in human nature, soul and body, even after baptismal regeneration.[31] Christian parents transmit their wounded nature to children, because they give them birth, not the \\"re-birth\\".[32] Augustine used Ciceronian Stoic concept of passions, to interpret St. Paul's doctrine of universal sin and redemption. In that view, also sexual desire itself as well as other bodily passions were consequence of the original sin, in which pure affections were wounded by vice and became disobedient to human reason and will. As long as they carry a threat to the dominion of reason over the soul they constitute moral evil, but since they do not presuppose consent, one cannot call them sins. Humanity will be liberated from passions, and pure affections will be restored only when all sin has been washed away and ended, that is in the resurrection of the dead.[33][34]\\r\\nAugustine believed that the only definitive destinations of souls are heaven and hell. He concluded that unbaptized infants go to hell as a consequence of original sin.[35][36] The Latin Church Fathers who followed Augustine adopted his position, which became a point of reference for Latin theologians in the Middle Ages.[37] In the later medieval period, some theologians continued to hold Augustine's view, others held that unbaptized infants suffered no pain at all: unaware of being deprived of the beatific vision, they enjoyed a state of natural, not supernatural happiness. Starting around 1300, unbaptized infants were often said to inhabit the \\"limbo of infants\\".[38] The Catechism of the Catholic Church, 1261 declares: \\"As regards children who have died without Baptism, the Church can only entrust them to the mercy of God, as she does in her funeral rites for them. Indeed, the great mercy of God who desires that all men should be saved, and Jesus' tenderness toward children which caused him to say: 'Let the children come to me, do not hinder them',[39] allow us to hope that there is a way of salvation for children who have died without Baptism. All the more urgent is the Church's call not to prevent little children coming to Christ through the gift of holy Baptism.\\" But the theory of Limbo, while it \\"never entered into the dogmatic definitions of the Magisterium ... remains ... a possible theological hypothesis\\".[40]\\r\\nIn the works of John Cassian (c. 360 ÿ 435), Conference XIII recounts how the wise monk Chaeremon, of whom he is writing, responded to puzzlement caused by his own statement that \\"man even though he strive with all his might for a good result, yet cannot become master of what is good unless he has acquired it simply by the gift of Divine bounty and not by the efforts of his own toil\\" (chapter 1). In chapter 11, Cassian presents Chaeremon as speaking of the cases of Paul the persecutor and Matthew the publican as difficulties for those who say \\"the beginning of free will is in our own power\\", and the cases of Zaccheus and the good thief on the cross as difficulties for those who say \\"the beginning of our free will is always due to the inspiration of the grace of God\\", and as concluding: \\"These two then; viz., the grace of God and free will seem opposed to each other, but really are in harmony, and we gather from the system of goodness that we ought to have both alike, lest if we withdraw one of them from man, we may seem to have broken the rule of the Church's faith: for when God sees us inclined to will what is good, He meets, guides, and strengthens us: for 'At the voice of thy cry, as soon as He shall hear, He will answer thee'; and: 'Call upon Me', He says, 'in the day of tribulation and I will deliver thee, and thou shalt glorify Me'. And again, if He finds that we are unwilling or have grown cold, He stirs our hearts with salutary exhortations, by which a good will is either renewed or formed in us.\\"[41]\\r\\nCassian did not accept the idea of total depravity, on which Martin Luther was to insist.[42] He taught that human nature is fallen or depraved, but not totally. Augustine Casiday states that, at the same time, Cassian \\"baldly asserts that God's grace, not human free will, is responsible for 'everything which pertains to salvation' ÿ even faith\\".[43] Cassian pointed out that people still have moral freedom and one has the option to choose to follow God. Colm Luibhid says that, according to Cassian, there are cases where the soul makes the first little turn,[44] but in Cassian's view, according to Casiday, any sparks of goodwill that may exist, not directly caused by God, are totally inadequate and only direct divine intervention ensures spiritual progress;[45] and Lauren Pristas says that \\"for Cassian, salvation is, from beginning to end, the effect of God's grace\\".[46]\\r\\nOpposition to Augustine's ideas about original sin, which he had developed in reaction to Pelagianism, arose rapidly.[47] After a long and bitter struggle the general principles of Augustine's teaching were confirmed within Western Christianity by many councils, especially the Second Council of Orange in 529.[2] However, while the Church condemned Pelagius, it did not endorse Augustine entirely[48] and, while Augustine's authority was accepted, he was interpreted in the light of writers such as Cassian.[49] Some of the followers of Augustine identified original sin with concupiscence[50] in the psychological sense, but this identification was challenged by the 11th-century Saint Anselm of Canterbury, who defined original sin as \\"privation of the righteousness that every man ought to possess\\", thus separating it from concupiscence. In the 12th century the identification of original sin with concupiscence was supported by Peter Lombard and others,[citation needed] but was rejected by the leading theologians in the next century, chief of whom was Thomas Aquinas. He distinguished the supernatural gifts of Adam before the Fall from what was merely natural, and said that it was the former that were lost, privileges that enabled man to keep his inferior powers in submission to reason and directed to his supernatural end. Even after the fall, man thus kept his natural abilities of reason, will and passions. Rigorous Augustine-inspired views persisted among the Franciscans, though the most prominent Franciscan theologians, such as Duns Scotus and William of Ockham, eliminated the element of concupiscence.\\r\\nMartin Luther (1483ÿ1546) asserted that humans inherit Adamic guilt and are in a state of sin from the moment of conception. The second article in Lutheranism's Augsburg Confession presents its doctrine of original sin in summary form:\\r\\nIt is also taught among us that since the fall of Adam all men who are born according to the course of nature are conceived and born in sin. That is, all men are full of evil lust and inclinations from their mothers' wombs and are unable by nature to have true fear of God and true faith in God. Moreover, this inborn sickness and hereditary sin is truly sin and condemns to the eternal wrath of God all those who are not born again through Baptism and the Holy Spirit. Rejected in this connection are the Pelagians and others who deny that original sin is sin, for they hold that natural man is made righteous by his own powers, thus disparaging the sufferings and merit of Christ.[51]\\r\\nLuther, however, also agreed with the Roman Catholic doctrine of the Immaculate Conception (that Mary was conceived free from original sin) by saying:\\r\\n[Mary] is full of grace, proclaimed to be entirely without sin. God's grace fills her with everything good and makes her devoid of all evil. God is with her, meaning that all she did or left undone is divine and the action of God in her. Moreover, God guarded and protected her from all that might be hurtful to her.[52]\\r\\nProtestant Reformer John Calvin (1509ÿ1564) developed a systematic theology of Augustinian Protestantism by interpretation of Augustine of Hippo's notion of original sin. Calvin believed that humans inherit Adamic guilt and are in a state of sin from the moment of conception. This inherently sinful nature (the basis for the Calvinistic doctrine of \\"total depravity\\") results in a complete alienation from God and the total inability of humans to achieve reconciliation with God based on their own abilities. Not only do individuals inherit a sinful nature due to Adam's fall, but since he was the federal head and representative of the human race, all whom he represented inherit the guilt of his sin by imputation. Redemption by Jesus Christ is the only remedy.\\r\\nJohn Calvin defined original sin in his Institutes of the Christian Religion as follows:\\r\\nOriginal sin, therefore, seems to be a hereditary depravity and corruption of our nature, diffused into all parts of the soul, which first makes us liable to God's wrath, then also brings forth in us those works which Scripture calls \\"works of the flesh\\" (Gal 5:19). And that is properly what Paul often calls sin. The works that come forth from it ÿ such as adulteries, fornications, thefts, hatreds, murders, carousings ÿ he accordingly calls \\"fruits of sin\\" (Gal 5:19ÿ21), although they are also commonly called \\"sins\\" in Scripture, and even by Paul himself.[53]\\r\\nThe Council of Trent (1545ÿ1563), while not pronouncing on points disputed among Catholic theologians, condemned the teaching that in baptism the whole of what belongs to the essence of sin is not taken away, but is only cancelled or not imputed, and declared the concupiscence that remains after baptism not truly and properly \\"sin\\" in the baptized, but only to be called sin in the sense that it is of sin and inclines to sin.[54]\\r\\nIn 1567, soon after the close of the Council of Trent, Pope Pius V went beyond Trent by sanctioning Aquinas's distinction between nature and supernature in Adam's state before the Fall, condemned the identification of original sin with concupiscence, and approved the view that the unbaptized could have right use of will.[2]\\r\\nThe Catechism of the Catholic Church says:\\r\\nBy his sin Adam, as the first man, lost the original holiness and justice he had received from God, not only for himself but for all humans.\\r\\nAdam and Eve transmitted to their descendants human nature wounded by their own first sin and hence deprived of original holiness and justice; this deprivation is called \\"original sin\\".\\r\\nAs a result of original sin, human nature is weakened in its powers, subject to ignorance, suffering and the domination of death, and inclined to sin (this inclination is called \\"concupiscence\\").[55]\\r\\nThe Catholic Church teaches that every human person born on this earth is made in the image of God.[56][57] Within man \\"is both the powerful surge toward the good because we are made in the image of God, and the darker impulses toward evil because of the effects of Original Sin\\".[58] Furthermore, it explicitly denies that we inherit guilt from anyone, maintaining that instead we inherit our fallen nature. In this it differs from the Calvinist/Protestant position that each person actually inherits Adam's guilt, and teaches instead that \\"original sin does not have the character of a personal fault in any of Adam's descendants ... but the consequences for nature, weakened and inclined to evil, persist in man\\".[59] \\"In other words, human beings do not bear any 'original guilt' from Adam and Eve's particular sin.\\"[60]\\r\\nThe Church has always held baptism to be \\"for the remission of sins\\", and, as mentioned in Catechism of the Catholic Church, 403, infants too have traditionally been baptized, though not guilty of any actual personal sin. The sin that through baptism was remitted for them could only be original sin, with which they were connected by the very fact of being a human. The first comprehensive theological explanation of this practice of baptizing infants, guilty of no actual personal sin, was given by Saint Augustine of Hippo, not all of whose ideas on original sin have been adopted by the Catholic Church. Indeed, the Church has condemned the interpretation of some of his ideas by certain leaders of the Protestant Reformation.\\r\\nThe Catechism of the Catholic Church explains that in \\"yielding to the tempter, Adam and Eve committed a personal sin, but this sin affected the human nature that they would then transmit in a fallen state ... original sin is called \\"sin\\" only in an analogical sense: it is a sin \\"contracted\\" and not \\"committed\\"a state and not an act\\" (Catechism of the Catholic Church, 404). This \\"state of deprivation of the original holiness and justice ... transmitted to the descendants of Adam along with human nature\\" (Compendium of the Catechism of the Catholic Church, 76) involves no personal responsibility or personal guilt on their part (cf. Catechism of the Catholic Church, 405). Personal responsibility and guilt were Adam's, who because of his sin, was unable to pass on to his descendants a human nature with the holiness with which it would otherwise have been endowed, in this way implicating them in his sin. The doctrine of original sin thus does not impute the sin of the father to his children, but merely states that they inherit from him a \\"human nature deprived of original holiness and justice\\", which is \\"transmitted by propagation to all mankind\\".[61]\\r\\nIn the theology of the Catholic Church, original sin is regarded as the general condition of sinfulness, that is the absence of holiness and perfect charity into which humans are born, distinct from the actual sins that a person commits. This teaching explicitly states that \\"original sin does not have the character of a personal fault in any of Adam's descendants\\".[59] In other words, human beings do not bear any \\"original guilt\\" from Adam's particular sin, which is his alone. The prevailing view, also held in Eastern Orthodoxy, is that human beings bear no guilt for the sin of Adam. The Catholic Church teaches: \\"By our first parents' sin, the devil has acquired a certain domination over man, even though man remains free.\\"[62]\\r\\nThe Catholic doctrine of the Immaculate Conception of Mary is that Mary was conceived free from original sin: \\"the most Blessed Virgin Mary was, from the first moment of her conception, by a singular grace and privilege of almighty God and by virtue of the merits of Jesus Christ, Savior of the human race, preserved immune from all stain of original sin\\".[63] The doctrine sees her as an exception to the general rule that human beings are not immune from the reality of original sin.\\r\\nSoon after the Second Vatican Council, biblical theologian Herbert Haag raised the question: Is original sin in Scripture?[64] According to his exegesis, Genesis 2:25 indicates that Adam and Eve were created from the beginning naked of the divine grace, an originary grace that, then, they would never have had and even less would have lost due to the subsequent events narrated. On the other hand, while supporting a continuity in the Bible about the absence of preternatural gifts (Latin: dona praeternaturalia)[65] with regard to the ophitic event, Haag never makes any reference to the discontinuity of the loss of access to the tree of life.\\r\\nThe Eastern Orthodox version of original sin is the view that sin originates with the Devil, \\"for the devil sinneth from the beginning (1 John iii. 8)\\".[66] They acknowledge that the introduction of ancestral sin[67][better?source?needed] into the human race affected the subsequent environment for humanity (see also traducianism). However, they never accepted Augustine of Hippo's notions of original sin and hereditary guilt.[68][better?source?needed]\\r\\nOrthodox Churches accept the teachings of John Cassian, as do Catholic Churches eastern and western,[42] in rejecting the doctrine of total depravity, by teaching that human nature is \\"fallen\\", that is, depraved, but not totally. Augustine Casiday states that Cassian \\"baldly asserts that God's grace, not human free will, is responsible for 'everything which pertains to salvation' ÿ even faith\\".[43] Cassian points out that people still have moral freedom and one has the option to choose to follow God. Colm Luibhid says that, according to Cassian, there are cases where the soul makes the first little turn,[44] while Augustine Casiday says that, in Cassian's view, any sparks of goodwill that may exist, not directly caused by God, are totally inadequate and only direct divine intervention ensures spiritual progress.[45] and Lauren Pristas says that \\"for Cassian, salvation is, from beginning to end, the effect of God's grace\\".[46]\\r\\nEastern Orthodoxy accepts the doctrine of ancestral sin: \\"Original sin is hereditary. It did not remain only Adam and Eve's. As life passes from them to all of their descendants, so does original sin.\\"[69] \\"As from an infected source there naturally flows an infected stream, so from a father infected with sin, and consequently mortal, there naturally proceeds a posterity infected like him with sin, and like him mortal.\\"[70]\\r\\nThe Orthodox Church in America makes clear the distinction between \\"fallen nature\\" and \\"fallen man\\" and this is affirmed in the early teaching of the Church whose role it is to act as the catalyst that leads to true or inner redemption. Every human person born on this earth bears the image of God undistorted within themselves.[71] In the Orthodox Christian understanding, they explicitly deny that humanity inherited guilt from anyone. Rather, they maintain that we inherit our fallen nature. While humanity does bear the consequences of the original, or first, sin, humanity does not bear the personal guilt associated with this sin. Adam and Eve are guilty of their willful action; we bear the consequences, chief of which is death.\\"[72]\\r\\nThe view of the Eastern Orthodox Church varies on whether Mary is free of all actual sin or concupiscence. Some Patristic sources imply that she was cleansed from sin at the Annunciation, while the liturgical references are unanimous that she is all-holy from the time of her conception.[73][74]\\r\\nThe original formularies of the Church of England also continue in the Reformation understanding of original sin. In the Thirty-Nine Articles, Article IX \\"Of Original or Birth-sin\\" states:\\r\\nOriginal Sin standeth not in the following of Adam, (as the Pelagians do vainly talk); but it is the fault and corruption of the Nature of every man, that naturally is ingendered of the offspring of Adam; whereby man is very far gone from original righteousness, and is of his own nature inclined to evil, so that the flesh lusteth always contrary to the spirit; and therefore in every person born into this world, it deserveth God's wrath and damnation. And this infection of nature doth remain, yea in them that are regenerated; whereby the lust of the flesh, called in the Greek, ϫ ϫϰ?, which some do expound the wisdom, some sensuality, some the affection, some the desire, of the flesh, is not subject to the Law of God. And although there is no condemnation for them that believe and are baptized, yet the Apostle doth confess, that concupiscence and lust hath of itself the nature of sin.[75]\\r\\nHowever, more recent doctrinal statements (e.g. the 1938 report Doctrine in the Church of England) permit a greater variety of understandings of this doctrine. The 1938 report summarizes:\\r\\nMan is by nature capable of communion with God, and only through such communion can he become what he was created to be. \\"Original sin\\" stands for the fact that from a time apparently prior to any responsible act of choice man is lacking in this communion, and if left to his own resources and to the influence of his natural environment cannot attain to his destiny as a child of God.[76]\\r\\nThe Methodist Church upholds Article VII in the Articles of Religion in the Book of Discipline of the United Methodist Church:\\r\\nOriginal sin standeth not in the following of Adam (as the Pelagians do vainly talk), but it is the corruption of the nature of every man, that naturally is engendered of the offspring of Adam, whereby man is very far gone from original righteousness, and of his own nature inclined to evil, and that continually.[77]\\r\\nSeventh-day Adventists believe that humans are inherently sinful due to the fall of Adam,[78] but they do not totally accept the Augustinian/Calvinistic understanding of original sin, taught in terms of original guilt, but hold more to what could be termed the \\"total depravity\\" tradition.[79] Seventh-day Adventists have historically preached a doctrine of inherited weakness, but not a doctrine of inherited guilt.[80] According to Augustine and Calvin, humanity inherits not only Adam's depraved nature but also the actual guilt of his transgression, and Adventists look more toward the Wesleyan model.[81]\\r\\nIn part, the Adventist position on original sin reads:\\r\\nThe nature of the penalty for original sin, i.e., Adam's sin, is to be seen as literal, physical, temporal, or actual death ÿ the opposite of life, i.e., the cessation of being. By no stretch of the scriptural facts can death be spiritualised as depravity. God did not punish Adam by making him a sinner. That was Adams own doing. All die the first death because of Adams sin regardless of their moral character ÿ children included.[81]\\r\\nEarly Adventists Pioneers (such as George Storrs and Uriah Smith) tended to de-emphasise the morally corrupt nature inherited from Adam, while stressing the importance of actual, personal sins committed by the individual. They thought of the \\"sinful nature\\" in terms of physical mortality rather than moral depravity.[81] Traditionally, Adventists look at sin in terms of willful transgressions, and that Christ triumphed over sin. Adventism believes that Christ is both our Substitute and our Example.[82] They base their belief on texts such as \\"Whosoever committeth sin transgresseth also the law: for sin is the transgression of the law.\\" (1 John 3:4)[83]\\r\\nThough believing in the concept of inherited sin from Adam, there is no dogmatic Adventist position on original sin. Related articles dealing with the subject are publicly available on the General Conference of the Seventh-day Adventist Churchs official website on theological doctrine, the Biblical Research Institute.[84]\\r\\nAccording to the theology of the Christian Congregation of Jehovah's Witnesses, all humans are born sinners, because of inheriting sin, corruption, and death from Adam. They teach that Adam was originally created perfect and sinless, but with free will; that the Devil, who was originally a perfect angel, but later developed feelings of pride and self-importance, seduced Eve, and then through her, persuaded Adam to disobey God, and to obey the Devil instead, rebelling against God's sovereignty, thereby making themselves sinners, and because of that, transmitting a sinful nature to all of their future offspring.[85][86] Instead of destroying the Devil right away, as well as destroying the disobedient couple, God decided to test the loyalty of the rest of humankind, and to prove that man cannot be independent of God successfully, that man is lost without God's laws and standards, and can never bring peace to the earth, and that Satan was a deceiver, murderer, and liar.[87]\\r\\nWitnesses believe that all men possess \\"inherited sin\\" from the \\"one man\\" Adam and they teach that verses such as Romans 5:12-22, Psalm 51:5, Job 14:4, and 1st Corinthians 15:22 show that man is born corrupt, and dies because of inherited sin and imperfection, that inherited sin is the reason and cause for sickness and suffering, made worse by the Devil's wicked influence. They believe Jesus is the \\"second Adam\\", being the sinless Son of God and the Messiah, and that he came to undo Adamic sin; and that salvation and everlasting life can only be obtained through faith and obedience to the second Adam.[85][86][87][88][89][90] They believe that \\"sin\\" is \\"missing the mark\\" of God's standard of perfection, and that everyone is born a sinner, due to being the offspring of sinner Adam.[91]\\r\\nThe Book of Mormon, a text sacred to Mormonism, explains that the opportunity to live here in a world where we can learn good and bad is a gift from God, and not a punishment for Adam's and Eve's choice.[92] As Mormon founder Joseph Smith taught, humans had an essentially godlike nature, and were not only holy in a premortal state, but had the potential to progress eternally to become like God.[93] He wrote as one of his church's Articles of Faith, \\"We believe that men will be punished for their own sins, and not for Adams transgression.\\"[94] Later Mormons took this creed as a rejection of the doctrine of original sin and any notion of inherited sinfulness.[93] Thus, while modern Mormons will agree that the fall of Adam brought consequences to the world, including the possibility of sin, they generally reject the idea that any culpability is automatically transmitted to Adam and Eve's offspring.[95] Children under the age of eight are regarded as free of all sin and therefore do not require baptism.[96] Children who die prior to age eight are believed to be saved in the highest degree of heaven.[97]\\r\\nIn Swedenborgianism, exegesis of the first 11 chapters of Genesis from The First Church, has a view that Adam is not an individual person. Rather, he is a symbolic representation of the \\"Most Ancient Church\\", having a more direct contact with heaven than all other successive churches.[98][99] Swedenborg's view of original sin is referred to as hereditary evil, which passes from generation to generation.[100] It cannot be completely abolished by an individual man, but can be tempered when someone reforms their own life,[101] and are thus held accountable only for their own sins.[102]\\r\\nMost Quakers (also known as the Religious Society of Friends), including the founder of Quakerism, George Fox, believe in the doctrine of Inward light, a doctrine which states that there is \\"that of God in everyone\\".[103] This has led to a common belief among many liberal and universalist Quakers affiliated with the Friends General Conference and Britain Yearly Meeting, based on the ideas of Quaker Rufus Jones among others, that rather than being burdened by original sin, human beings are inherently good, and the doctrine of universal reconciliation, that is, that all people will eventually be saved and reconciled with God.\\r\\nHowever, this rejection of the doctrine of original sin or the necessity of salvation is not something that most conservative or evangelical Quakers affiliated with Friends United Meeting or Evangelical Friends Church International tend to agree with. Although the more conservative and evangelical Quakers also believe in the doctrine of inward light, they interpret it in a manner consistent with the doctrine of original sin, namely, that people may or may not listen to the voice of God within them and be saved, and people who do not listen will not be saved, thus there cannot be any universal reconciliation under the more conservative Quaker theology. Many more liberal Quaker meetings have no official creed and allow individual Quakers to have a wide variety of different beliefs, although following the recommendations of one's yearly meeting's Faith and Practice book is strongly recommended even though not mandatory.\\r\\nAlso, not all local Quaker meetings are entirely liberal or conservative and some have members adhering to different Quaker traditions, and a Quaker meeting can be affiliated with both liberal and conservative umbrella organizations. For instance, New York Yearly Meeting, which is mostly liberal, is affiliated with both the liberal Friends General Conference as well as the conservative Friends United Meeting, even though they adhere much more to the liberal teachings of Friends General Conference than the conservative teachings of Friends United Meeting.[104][105] Thus, at any given Quaker meeting, individual Quakers may have differing views on a wide variety of theological issues including original sin, with more conservative Quaker meetings having everyone believe in original sin while at more liberal Quaker meetings there is typically a variety of beliefs and usually a majority of people would not believe in original sin but a minority would believe in original sin.\\r\\nThe doctrine of \\"inherited sin\\" is not found in most of mainstream Judaism. Although some in Orthodox Judaism place blame on Adam for overall corruption of the world, and though there were some Jewish teachers in Babylon[106] who believed that death was a punishment brought upon humanity on account of Adam's sin, that is not the dominant view in most of Judaism today. Modern Judaism generally teaches that humans are born sin-free and untainted, and choose to sin later and bring suffering to themselves.[107][108]\\r\\nJewish theologians are divided in regard to the cause of what is called \\"original sin\\". Some teach that it was due to Adam's yielding to temptation in eating of the forbidden fruit and has been inherited by his descendants; the majority of chazalic opinions, however, do not hold Adam responsible for the sins of humanity,[109] teaching that, in Genesis 8:21 and 6:5-8, God recognized that Adam did not willfully sin. However, Adam is recognized by some[106] as having brought death into the world by his disobedience. Because of his sin, his descendants will live a mortal life, which will end in death of their bodies.[110]\\r\\nThe concept of inherited sin does not exist in Islam.[111][112] Islam teaches that Adam and Eve sinned, but then sought forgiveness and thus were forgiven by God.[113]\\r\\nThe Qur'an says that after Adam and Eve sinned, they asked for repentance and it was granted:\\r\\nThey said: \\"Our Lord, we have wronged ourselves souls. If You forgive us not and bestow not upon us Your mercy, we shall certainly be of the losers.\\r\\nThus did Adam disobey his Lord, so he went astray. Then his Lord chose him, and turned to him with forgiveness, and gave him guidance.\\r\\nThe Qur'an further says about individual responsibility:\\r\\nThat no burdened person (with sins) shall bear the burden (sins) of another. And that man can have nothing but what he does (of good and bad). And that his deeds will be seen, Then he will be recompensed with a full and the best [fair] recompense.","input":"What is original sin in the catholic church?"},{"output":"Ram Nath Kovind","context":"Ram Nath Kovind (born 1 October 1945) is the 14th and current President of India, in office since 25 July 2017[1]. Previously he had served as the Governor of Bihar from 2015 to 2017[2][3] and was a Member of Parliament,?Rajya Sabha?from 1994 to 2006. Kovind was nominated as a presidential candidate by the ruling NDA coalition and won the 2017 presidential election, becoming the second Dalit to be elected to the post of President.[4]\\r\\nBefore entering politics, he was a lawyer for 16 years and practiced in the Delhi High Court and the Supreme Court until 1993.[5]\\r\\n\\r\\n\\r\\nKovind was born on 1st October, 1945 in Paraukh village in the Kanpur Dehat district, Uttar Pradesh.[6][7] His father Maikulal was a landless Kori (a Dalit weaving community) who ran a small shop to support his family. He was the youngest of five brothers and two sisters. He was born in a mud hut, which eventually collapsed. He was only five when his mother died of burns when their thatched dwelling caught fire. Kovind later donated the land to the community.[8]\\r\\nAfter his elementary school education, he had to walk each day to Kanpur village, 8?km away, to attend junior school, as nobody in the village had a bicycle.[9] He holds a bachelor's degree in commerce and a LLB from DAV College (affiliated with Kanpur University).[10][11][6]\\r\\nAfter graduating in law from a Kanpur college, Kovind went to Delhi to prepare for the civil services examination. He passed this exam on his third attempt, but he did not join because he was selected for an allied service instead of IAS and thus started practicing law.[12]\\r\\nKovind enrolled as an advocate in 1971 with the bar council of Delhi. He was Central Government Advocate in the Delhi High Court from year 1977 to year 1979. Between 1977 & 1978, he also served as the personal assistant of Prime Minister of India Morarji Desai.[13] In 1978, he became an advocate-on-record of the Supreme Court of India and served as a standing counsel for the Central Government in the Supreme Court of India from 1980 to 1993. He practiced in the Delhi High Court and Supreme Court until 1993. As an advocate,he provided free legal aid to weaker sections of society, women and the poor under the Free Legal Aid Society of New Delhi.[10]\\r\\nHe joined the BJP in 1991.[13] He was President of the BJP Dalit Morcha between 1998 and 2002 and President of the All-India Koli Samaj[when?]. He also served as national spokesperson of the party.[14] He donated his ancestral home in Derapur to the RSS.[13] Soon after joining the BJP, he contested for Ghatampur assembly constituency, but lost and later contested for Bhognipur(in 2007) (both in Uttar Pradesh) assembly constituency on the BJP ticket but lost again.[15]\\r\\nIn 1997, Kovind joined the protest against certain orders from the Central government that had adverse effects on the SC/ST workers. Later, three amendments were made to the Constitution that revoked the orders, by the NDA government headed by Atal Bihari Vajpayee.[16]\\r\\nHe was elected and became a Rajya Sabha MP from the state of Uttar Pradesh in April 1994. He served a total of twelve years, two consecutive terms, until March 2006. As a member of parliament, he served on the Parliamentary Committee for Welfare of Scheduled Castes/Tribes, Home Affairs, Petroleum and Natural Gas, Social Justice and Empowerment, Law and Justice. He also served as the chairman of the Rajya Sabha House Committee. During his career as a parliamentarian, under the Members of Parliament Local Area Development Scheme, he focused on education in rural areas by helping in construction of school buildings in Uttar Pradesh and Uttrakhand. As a member of parliament, he visited Thailand, Nepal, Pakistan, Singapore, Germany, Switzerland, France, the United Kingdom and the United States on study tours.[11]\\r\\nHe has served on the Board of management of Dr. B.R Ambedkar University, Lucknow,[when?] and as on the Board of Governors of IIM Calcutta[when?]. He has also represented India at the UN and addressed the United Nations General Assembly in October 2002.[17]\\r\\nOn 8 August 2015, the then President of India appointed Kovind as Governor of Bihar.[18] On 16 August 2015, the acting Chief Justice of Patna High Court, Iqbal Ahmad Ansari, administered the oath to Kovind as the 35th Governor of Bihar. The function took place at Raj Bhawan, Patna.[19]\\r\\nKovind's appointment was criticised by then Chief Minister of Bihar Nitish Kumar as it came months before State Assembly elections and the appointment made without consulting State Government as recommended by Sarkaria Commission.[20] However, Kovind's term as Governor, was praised for constituting a judicial commission to investigate irregularities in promotion of undeserving teachers, mis-management of funds and appointment of undeserving candidates in universities.[13] In June 2017, when Kovind was announced as candidate for Presidential election, Nitish Kumar backed Kovind's choice and praised Kovind as being unbaised and working closely with the State Government during his Governorship.[21]\\r\\nAfter nomination for the post of 14th President of India, he resigned from his post as Governor of Bihar, and President of India Pranab Mukherjee accepted his resignation on 20 June 2017.[22] He won election on 20 July 2017.[23]\\r\\nRam Nath Kovind received 65.65% of the valid votes, against former Speaker of the Lok Sabha - Meira Kumar, the presidential candidate of the Opposition who received 34.35% of the total votes. Kovind received 2,930 votes (From MPs and MLAs) amounting to Electoral College votes of 702,044 (65.65%) as compared to 1,844 votes with a value of 367,314 (34.35%) votes for Meira Kumar lagging far behind with 367,314 votes, and 77 votes were invalid.[24] He became only the second Dalit representative to become President after K. R. Narayanan, and also is the first BJP candidate to be elected to the post.[25] The tally of votes (367,314) polled by Meira Kumar is only the second highest for a losing candidate, that of Neelam Sanjeeva Reddy or Neelam Sanjiva Reddy in the 1969 Presidential elections being the highest ever; he received 405,427 votes as against 420,077 by V V Giri, the winner.\\r\\nRam Nath Kovind took the oath as the 14th President of India on 25 July 2017.[26]\\r\\nKovind married Savita Kovind on 30 May 1974. They have a son, Prashant Kumar, and a daughter, Swati.[6]\\r\\nIn 2010, he was reported to have said that \\"Islam and Christianity are alien to the nation\\" as spokesperson of the BJP.[32][33] As reported by IANS and published by Hindustan Times, he made this comment in response to the Ranganath Misra Commission which recommended 15 percent reservation for religious and linguistic minorities in government jobs.[34] Although more recently, the issue was raised in the media if whether or not he was misquoted and that he in fact said Islam and Christianity are alien to the notion (of caste) as opposed to what was reported as \\"nation\\".[35][36]","input":"What is the name of current indian president?"},{"output":"48-hour maximum working week","context":"Working time is the period of time that a person spends at paid labor. Unpaid labor such as personal housework or caring for children or pets is not considered part of the working week.\\r\\nMany countries regulate the work week by law, such as stipulating minimum daily rest periods, annual holidays, and a maximum number of working hours per week. Working time may vary from person to person, often depending on location, culture, lifestyle choice, and the profitability of the individual's livelihood. For example, someone who is supporting children and paying a large mortgage will need to work more hours to meet basic costs of living than someone of the same earning power without children. Because fewer people than ever are having children,[1] choosing part time work is becoming more popular.[2]\\r\\nStandard working hours (or normal working hours) refers to the legislation to limit the working hours per day, per week, per month or per year. If an employee needs to work overtime, the employer will need to pay overtime payments to employees as required in the law. Generally speaking, standard working hours of countries worldwide are around 40 to 44 hours per week (but not everywhere: from 35 hours per week in France[3] to up to 112 hours per week in North Korean labor camps [4]), and the additional overtime payments are around 25% to 50% above the normal hourly payments. Maximum working hours refers to the maximum working hours of an employee. The employee cannot work more than the level specified in the maximum working hours law.[5]\\r\\nSince the 1960s, the consensus among anthropologists, historians, and sociologists has been that early hunter-gatherer societies enjoyed more leisure time than is permitted by capitalist and agrarian societies;[6][7] for instance, one camp of !Kung Bushmen was estimated to work two-and-a-half days per week, at around 6 hours a day.[8] Aggregated comparisons show that on average the working day was less than five hours.[6]\\r\\nSubsequent studies in the 1970s examined the Machiguenga of the Upper Amazon and the Kayapo of northern Brazil. These studies expanded the definition of work beyond purely hunting-gathering activities, but the overall average across the hunter-gatherer societies he studied was still below 4.86 hours, while the maximum was below 8 hours.[6] Popular perception is still aligned with the old academic consensus that hunter-gatherers worked far in excess of modern humans' forty-hour week.[7]\\r\\nThe industrial revolution made it possible for a larger segment of the population to work year-round, because this labor was not tied to the season and artificial lighting made it possible to work longer each day. Peasants and farm laborers moved from rural areas to work in urban factories, and working time during the year increased significantly.[9] Before collective bargaining and worker protection laws, there was a financial incentive for a company to maximize the return on expensive machinery by having long hours. Records indicate that work schedules as long as twelve to sixteen hours per day, six to seven days per week were practiced in some industrial sites.[citation needed]\\r\\nOver the 20th century, work hours shortened by almost half, mostly due to rising wages brought about by renewed economic growth, with a supporting role from trade unions, collective bargaining, and progressive legislation. The workweek, in most of the industrialized world, dropped steadily, to about 40 hours after World War II. The limitation of working hours is also proclaimed by the Universal Declaration of Human Rights,[10] International Covenant on Economic, Social and Cultural Rights,[11] and European Social Charter.[12] The decline continued at a faster pace in Europe: for example, France adopted a 35-hour workweek in 2000. In 1995, China adopted a 40-hour week, eliminating half-day work on Saturdays (though this is not widely practiced). Working hours in industrializing economies like South Korea, though still much higher than the leading industrial countries, are also declining steadily.\\r\\nTechnology has also continued to improve worker productivity, permitting standards of living to rise as hours decline.[13] In developed economies, as the time needed to manufacture goods has declined, more working hours have become available to provide services, resulting in a shift of much of the workforce between sectors.\\r\\nEconomic growth in monetary terms tends to be concentrated in health care, education, government, criminal justice, corrections, and other activities that are regarded as necessary for society rather than those that contribute directly to the production of material goods.[citation needed]\\r\\nIn the mid-2000s, the Netherlands was the first country in the industrialized world where the overall average working week dropped to less than 30 hours.[14]\\r\\nMost countries in the developed world have seen average hours worked decrease significantly.[15][16] For example, in the U.S in the late 19th century it was estimated that the average work week was over 60 hours per week.[17] Today the average hours worked in the U.S is around 33,[18] with the average man employed full-time for 8.4 hours per work day, and the average woman employed full-time for 7.7 hours per work day.[19] The front runners for lowest average weekly work hours are the Netherlands with 27 hours,[20] and France with 30 hours.[21] At current rates the Netherlands is set to become the first country to reach an average work week under 21 hours.[22] In a 2011 report of 26 OECD countries, Germany had the lowest average working hours per week at 25.6 hours.[23]\\r\\nThe New Economics Foundation has recommended moving to a 21-hour standard work week to address problems with unemployment, high carbon emissions, low well-being, entrenched inequalities, overworking, family care, and the general lack of free time.[24][25][26] Actual work week lengths have been falling in the developed world.[27]\\r\\nFactors that have contributed to lowering average work hours and increasing standard of living have been:\\r\\nRecent articles[28][29] supporting a four-day week have argued that reduced work hours would increase consumption and invigorate the economy. However, other articles state that consumption would decrease.[30][31] Other arguments for the four-day week include improvements to workers' level of education (due to having extra time to take classes and courses) and improvements to workers' health (less work-related stress and extra time for exercise). Reduced hours also save money on day care costs and transportation, which in turn helps the environment with less carbon-related emissions. These benefits increase workforce productivity on a per-hour basis.\\r\\nThe structure of the work week varies considerably for different professions and cultures. Among salaried workers in the western world, the work week often consists of Monday to Friday or Saturday with the weekend set aside as a time of personal work and leisure. Sunday is set aside in the western world because it is the Christian sabbath.\\r\\nThe traditional American business hours are 9:00?a.m. to 5:00?p.m., Monday to Friday, representing a workweek of five eight-hour days comprising 40 hours in total. These are the origin of the phrase 9-to-5, used to describe a conventional and possibly tedious job.[32] Negatively used, it connotes a tedious or unremarkable occupation. The phrase also indicates that a person is an employee, usually in a large company, rather than self-employed. More neutrally, it connotes a job with stable hours and low career risk, but still a position of subordinate employment. The actual time at work often varies between 35 and 48 hours in practice due to the inclusion, or lack of inclusion, of breaks. In many traditional white collar positions, employees were required to be in the office during these hours to take orders from the bosses, hence the relationship between this phrase and subordination. Workplace hours have become more flexible, but even still the phrase is commonly used.\\r\\n\\r\\nSeveral countries have adopted a workweek from Monday morning until Friday noon, either due to religious rules (observation of shabbat in Israel whose workweek is Sunday to Friday afternoon) or the growing predominance of a 35ÿ37.5 hour workweek in continental Europe. Several of the Muslim countries have a standard Sunday through Thursday or Saturday through Wednesday workweek leaving Friday for religious observance, and providing breaks for the daily prayer times.[citation needed]\\r\\nSouth Korea has the fastest shortening working time in the OECD,[36] which is the result of the government's proactive move to lower working hours at all levels to increase leisure and relaxation time, which introduced the mandatory forty-hour, five-day working week in 2004 for companies with over 1,000 employees. Beyond regular working hours, it is legal to demand up to 12 hours of overtime during the week, plus another 16 hours on weekends.[citation needed] The 40-hour workweek expanded to companies with 300 employees or more in 2005, 100 employees or more in 2006, 50 or more in 2007, 20 or more in 2008 and a full inclusion to all workers nationwide in July 2011.[37] The government has continuously increased public holidays to 16 days in 2013, more than the 10 days of the United States and double that of the United Kingdom's 8 days.[38] Despite those efforts, South Korea's work hours are still relatively long, with an average 2,163 hours per year in 2012.[39]\\r\\nWork hours in Japan are decreasing, but many Japanese still work long hours.[40] Recently, Japan's Ministry of Health, Labor and Welfare (MHLW) issued a draft report recommending major changes to the regulations that govern working hours. The centerpiece of the proposal is an exemption from overtime pay for white-collar workers.[citation needed]Japan has enacted an 8-hour work day and 40-hour work week (44 hours in specified workplaces). The overtime limits are: 15 hours a week, 27 hours over two weeks, 43 hours over four weeks, 45 hours a month, 81 hours over two months and 120 hours over three months; however, some workers get around these restrictions by working several hours a day without 'clocking in' whether physically or metaphorically.[41][citation needed] The overtime allowance should not be lower than 125% and not more than 150% of the normal hourly rate.[42]\\r\\nIn most European Union countries, working time is gradually decreasing.[43] The European Union's working time directive imposes a 48-hour maximum working week that applies to every member state except the United Kingdom and Malta (which have an opt-out, meaning that UK-based employees may work longer than 48 hours if they wish, but they cannot be forced to do so).[44] France has enacted a 35-hour workweek by law, and similar results have been produced in other countries through collective bargaining.[citation needed] A major reason for the low annual hours worked in Europe is a relatively high amount of paid annual leave.[45] Fixed employment comes with four to six weeks of holiday as standard. In the UK, for example, full-time employees are entitled to 28 days of paid leave a year.[46] It is commonly understood working hours in the UK are 09.00 to 17.00.[citation needed]\\r\\nMexican laws mandate a maximum of 48 hours of work per week, but they are rarely observed or enforced due to loopholes in the law, the volatility of labor rights in Mexico, and its underdevelopment relative to other members countries of the Organisation for Economic Co-operation and Development?(OECD). Indeed, private sector employees often work overtime without receiving overtime compensation. Fear of unemployment and threats by employers explain in part why the 48-hour work week is disregarded.[citation needed].\\r\\nArticles 161 to 167 of the Substantive Work Code in Colombia provide for a maximum of 48 hours of work a week.[47]\\r\\nIn Australia, between 1974 and 1997 no marked change took place in the average amount of time spent at work by Australians of \\"prime working age\\" (that is, between 25 and 54 years of age). Throughout this period, the average time spent at work by prime working-age Australians (including those who did not spend any time at work) remained stable at between 27 and 28 hours per week. This unchanging average, however, masks a significant redistribution of work from men to women. Between 1974 and 1997, the average time spent at work by prime working-age Australian men fell from 45 to 36 hours per week, while the average time spent at work by prime working-age Australian women rose from 12 to 19 hours per week. In the period leading up to 1997, the amount of time Australian workers spent at work outside the hours of 9 a.m. to 5 p.m. on weekdays also increased.[48]\\r\\nIn 2009, a rapid increase in the number of working hours was reported in a study by The Australia Institute. The study found the average Australian worked 1855 hours per year at work. According to Clive Hamilton of The Australia Institute, this surpasses even Japan. The Australia Institute believes that Australians work the highest number of hours in the developed world.[49]\\r\\nFrom January 1, 2010, Australia enacted a 38-hour workweek in accordance with the Fair Work Act 2009, with an allowance for additional hours as overtime.[50]\\r\\nThe vast majority of full-time employees in Australia work additional overtime hours. A 2015 survey found that of Australia's 7.7 million full-time workers, 5 million put in more than 40 hours a week, including 1.4 million who worked more than 50 hours a week and 270,000 who put in more than 70 hours.[51]\\r\\nIn 2006, the average man employed full-time worked 8.4 hours per work day, and the average woman employed full-time worked 7.7 hours per work day.[19] There is no mandatory minimum amount of paid time off for sickness or holiday. The majority of jobs in America do not offer paid time off. [52] Because of the pressure of working, time is increasingly viewed as a commodity.[53]\\r\\nBy 1946 the United States government had inaugurated the 40-hour work week for all federal employees.[54] Beginning in 1950, under the Truman Administration, the United States became the first known industrialized nation to explicitly (albeit secretly) and permanently forswear a reduction of working time. Given the military-industrial requirements of the Cold War, the authors of the then secret National Security Council Report 68 (NSC-68)[55] proposed the US government undertake a massive permanent national economic expansion that would let it siphon off a part of the economic activity produced to support an ongoing military buildup to contain the Soviet Union. In his 1951 Annual Message to the Congress, President Truman stated:\\r\\nIn terms of manpower, our present defense targets will require an increase of nearly one million men and women in the armed forces within a few months, and probably not less than four million more in defense production by the end of the year. This means that an additional 8 percent of our labor force, and possibly much more, will be required by direct defense needs by the end of the year. These manpower needs will call both for increasing our labor force by reducing unemployment and drawing in women and older workers, and for lengthening hours of work in essential industries.[56]\\r\\nAccording to the Bureau of Labor Statistics, the average non-farm private sector employee worked 34.5 hours per week as of June 2012.[57]\\r\\nAs President Trumans 1951 message had predicted, the share of working women rose from 30 percent of the labor force in 1950 to 47 percent by 2000 ÿ growing at a particularly rapid rate during the 1970s.[58] According to a Bureau of Labor Statistics report issued May 2002, \\"In 1950, the overall participation rate of women was 34 percent ... The rate rose to 38 percent in 1960, 43 percent in 1970, 52 percent in 1980, and 58 percent in 1990 and reached 60 percent by 2000. The overall labor force participation rate of women is projected to attain its highest level in 2010, at 62 percent.[58] The inclusion of women in the work force can be seen as symbolic of social progress as well as of increasing American productivity and hours worked.\\r\\nBetween 1950 and 2007 official price inflation was measured to 861 percent. President Truman, in his 1951 message to Congress, predicted correctly that his military buildup will cause intense and mounting inflationary pressures. Using the data provided by the United State Bureau of Labor Statistics, Erik Rauch has estimated productivity to have increased by nearly 400%.[59] According to Rauch, if productivity means anything at all, a worker should be able to earn the same standard of living as a 1950 worker in only 11 hours per week.\\r\\nIn the United States, the working time for upper-income professionals has increased compared to 1965, while total annual working time for low-skill, low-income workers has decreased.[60] This effect is sometimes called the \\"leisure gap\\".\\r\\nThe average working time of married couples ÿ of both spouses taken together ÿ rose from 56 hours in 1969 to 67 hours in 2000.[61]\\r\\nMany professional workers put in longer hours than the forty-hour standard.[62] In professional industries like investment banking and large law firms, a forty-hour workweek is considered inadequate and may result in job loss or failure to be promoted.[63][64] Medical residents in the United States routinely work long hours as part of their training.\\r\\nWorkweek policies are not uniform in the U.S. Many compensation arrangements are legal, and three of the most common are wage, commission, and salary payment schemes. Wage earners are compensated on a per-hour basis, whereas salaried workers are compensated on a per-week or per-job basis, and commission workers get paid according to how much they produce or sell.\\r\\nUnder most circumstances, wage earners and lower-level employees may be legally required by an employer to work more than forty hours in a week; however, they are paid extra for the additional work. Many salaried workers and commission-paid sales staff are not covered by overtime laws. These are generally called \\"exempt\\" positions, because they are exempt from federal and state laws that mandate extra pay for extra time worked.[65] The rules are complex, but generally exempt workers are executives, professionals, or sales staff.[66] For example, school teachers are not paid extra for working extra hours. Business owners and independent contractors are considered self-employed, and none of these laws apply to them.\\r\\nGenerally, workers are paid time-and-a-half, or 1.5 times the worker's base wage, for each hour of work past forty. California also applies this rule to work in excess of eight hours per day,[67] but exemptions[68] and exceptions[69] significantly limit the applicability of this law.\\r\\nIn some states, firms are required to pay double-time, or twice the base rate, for each hour of work past 60, or each hour of work past 12 in one day in California, also subject to numerous exemptions and exceptions.[67] This provides an incentive for companies to limit working time, but makes these additional hours more desirable for the worker. It is not uncommon for overtime hours to be accepted voluntarily by wage-earning workers. Unions often treat overtime as a desirable commodity when negotiating how these opportunities shall be partitioned among union members.\\r\\nThe work time in Brazil is 44 hours per week, usually 8 hours per day and 4 hours on Saturday or 8.8 hours per day. On duty/no meal break jobs are 6 hours per day. Public servants work 40 hours a week.\\r\\nIt is worth noting that in Brazil meal time is not usually counted as work. There is a 1-hour break for lunch and work schedule is typically 8h00 or 9h00-noon, 13h00 -18h00. In larger cities people have lunch meal on/near the work site, while in smaller cities a sizable fraction of the employees might go home for lunch.\\r\\nA 30-day vacation is mandatory by law and there are about 13 to 15 holidays a year, depending on the municipality.\\r\\nChina adopted a 40-hour week, eliminating half-day work on Saturdays (though this was not widely practiced initially). [70]\\r\\nTraditionally, Chinese have worked long hours, and this has led to many deaths from overwork, with the state media reporting in 2014 that 600,000 people were dying annually from overwork. Despite this, work hours have reportedly been falling for about three decades due to rising productivity, better labor laws, and the spread of the two-day weekend. The trend has affected both factories and white-collar companies that have been responding to growing demands for easier work schedules.[71][72]\\r\\nHong Kong has no legislation regarding maximum and normal working hours. The average weekly working hours of full-time employees in Hong Kong is 49 hours.[73] According to the Price and Earnings Report 2012 conducted by UBS, while the global and regional average were 1,915 and 2,154 hours per year respectively, the average working hours in Hong Kong is 2,296 hours per year, which ranked the fifth longest yearly working hours among 72 countries under study.[74] In addition, from the survey conducted by the Public Opinion Study Group of the University of Hong Kong, 79% of the respondents agree that the problem of overtime work in Hong Kong is severe, and 65% of the respondents support the legislation on the maximum working hours.[75] In Hong Kong, 70% of surveyed do not receive any overtime remuneration.[76] These show that people in Hong Kong concerns the working time issues. As Hong Kong implemented the minimum wage law in May 2011, the Chief Executive, Donald Tsang, of the Special Administrative Region pledged that the government will standardize working hours in Hong Kong.[77]\\r\\nOn 26 November 2012, the Labour Department of the HKSAR released the \\"Report of the policy study on standard working hours\\". The report covers three major areas, including: (1) the regimes and experience of other places in regulating working hours, (2) latest working time situations of employees in different sectors, and (3) estimation of the possible impact of introducing standard working hour in Hong Kong.[78] Under the selected parameters, from most loosen to most stringent, the estimated increase in labour cost vary from 1.1 billion to 55 billion HKD, and affect 957,100 (36.7% of total employees) to 2,378,900 (91.1%of total) employees.[73]\\r\\nVarious sectors of the community show concerns about the standard working hours in Hong Kong. The points are summarized as below:\\r\\nHong Kong Catholic Commission For Labour Affairs urges the government to legislate the standard working hours in Hong Kong, and suggests a 44 hours standard, 54 hours maximum working hours in a week. The organization thinks that long working time adversely affects the family and social life and health of employees; it also indicates that the current Employment Ordinance does not regulate overtime pays, working time limits nor rest day pays, which can protect employees rights.\\r\\nGenerally, business sector agrees that it is important to achieve work-life balance, but does not support a legislation to regulate working hours limit. They believe \\"standard working hours\\" is not the best way to achieve work-life balance and the root cause of the long working hours in Hong Kong is due to insufficient labor supply. The Managing Director of Century Environmental Services Group, Catherine Yan, said Employees may want to work more to obtain a higher salary due to financial reasons. If standard working hour legislation is passed, employers will need to pay a higher salary to employees, and hence the employers may choose to segment work tasks to employer more part time employees instead of providing overtime pay to employees. She thinks this will lead to a situation that the employees may need to find two part-time jobs to earn their living, making them wasting more time on transportation from one job to another.[79]\\r\\nThe Chairman of the Hong Kong General Chamber of Commerce, Chow Chung-kong believes that it is so difficult to implement standard working hours that apply across-the-board, specifically, to accountants and barristers.[80] In addition, he believes that standard working hours may decrease individual employees' working hours and would not increase their actual income. It may also lead to an increase of number of part-timers in the labor market.\\r\\nAccording to a study conducted jointly by the Business, Economic and Public Affairs Research Centre and Enterprise and Social Development Research Centre of Hong Kong Shue Yan University, 16% surveyed companies believe that a standard working hours policy can be considered, and 55% surveyed think that it would be difficult to implement standard working hours in businesses.[81]\\r\\nEmployer representative in the Labour Advisory Board, Stanley Lau, said that standard working hours will completely alter the business environment of Hong Kong, affect small and medium enterprise and weaken competitiveness of businesses. He believes that the government can encourage employers to pay overtime salary, and there is no need to regulate standard working hours.[82]\\r\\nOn 17ÿ18 October 2012, the Legislative Council members in Hong Kong debated on the motion \\"legislation for the regulation of working hours\\". Cheung Kwok-che proposed the motion \\"That is the Council urges the Government to introduce a bill on the regulation of working hours within this legislative session, the contents of which must include the number of standard weekly hours and overtime pay\\".[83] As the motion was not passed by both functional constituencies and geographical constituencies, it was negatived.[84]\\r\\nThe Hong Kong Federation of Trade Unions suggested a standard 44-hour work week with overtime pay of 1.5 times the usual pay. It believes the regulation of standard working hour can prevent the employers to force employees to work (overtime) without pay.[85]\\r\\nElizabeth Quat of Democratic Alliance for the Betterment and Progress of Hong Kong (DAB), believed that standard working hour was a labor policy and was not related to family-friendly policies. Vice President of Young DAB, Wai-hung Chan, stated that standard working hour would bring limitations to small and medium enterprises. He thought that the government should discuss the topic with public more before legislating the standard working hour.\\r\\nThe Democratic Party suggested a 44-hour standard work week and compulsory overtime pay to help achieve the balance between work, rest and entertainment of people in Hong Kong.[86]\\r\\nThe Labour Party believed regulating working hours such as standard working hour can help achieving the work-life balance.[87] It suggests an 8-hour work day, a 44-hour standard work week, a 60-hour maximum work week and an overtime pay of 1.5 times the usual pay.[76]\\r\\nPoon Siu-ping of Federation of Hong Kong and Kowloon Labour Unions thought that it is possible to set work hour limit for all industries; and the regulation on working hours can ensure the overtime payment by employers to employees, and protect employees health.\\r\\nThe Civic party suggests \\"to actively study setting weekly standard working hours at 44 hours to align with family-friendly policies\\" in LegCo Election 2012.[88]\\r\\nMember of Economic Synergy, Jeffery Lam, believes that standard working hour would adversely affect productivity, tense the employer-employee relationship, and increase the pressure faced by business who suffer from inadequate workers. He does not support the regulation on working hours at the current situation.[89]\\r\\nMatthew Cheung Kin-chung, the Secretary for Labour and Welfare Bureau, said the Executive Council has already received the government report on working hours in June, and the Labour Advisory Board and the LegCos Manpower Panel will receive the report in late November and December respectively.[90] On 26 November 2012, the Labour Department released the report, and the report covered the regimes and experience of practicing standard working hours in selected regions, current work hour situations in different industries, and the impact assessment of standard working hours. Also, Matthew Cheung mentioned that the government will form a select committee by first quarter of 2013, which will include government officials, representative of labor unions and employers associations, academics and community leaders, to investigate the related issues. He also said that it would \\"perhaps be unrealistic\\" to put forward a bill for standard working hours in the next one to two years.[91]\\r\\nYip Siu-fai, Professor of the Department of Social Work and Social Administration of HKU, said that professions such as nurse, accountants, have a long work time, and this may affect their social life. He believes standard working hour can help making Hong Kong to become a family-friendly workplace and increase fertility rate. Randy Chiu, Professor of Department of Management of HKBU, said that standard working hour could avoid excessively long working hours of employees.[92] He also said that nowadays Hong Kong attains almost full employment, has a high rental price and severe inflation, recently implemented minimum wage, and affected by gloomy global economy; he also mentioned that comprehensive considerations on macroeconomic situations are needed, and pushed to the awareness that it is perhaps inappropriate to adopt working time regulation examples in other countries to Hong Kong.[93]\\r\\nLee Shu-Kam, Associate Professor of the Department of Economics and Finance of HKSYU, believed standard working hours cannot achieve work-life balance. He referenced the research to the US by the University of California, Los Angeles in 1999 and pointed out that in the industries and regions which the wage elasticity is low, the effects of standard working hours on lowering actual working time and increasing wage is limited: for regions where labor supply is inadequate, standard working hours can protect employees benefit yet cause unemployment; but for the regions, such as Japan, where the problem does not exist, standard working hour would only lead to unemployment.[94] In addition, he said the effect of standard working hour is similar to that of giving overtime pay, say, making employees to favor overtime work more. In this sense, standard working hour does not match its principle: to shorten work time and increase recreation time of employees.[95] He believed that the key point is to help employees to achieve work-life balance and get a win-win situation of employers and employees.\\r\\nFrancis Lui, Head and Professor of the Department of Economics of Hong Kong University of Science and Technology, believed that the standard working hour may not lower work time but increase unemployment. He used Japan as an example to illustrate that the implementation of standard working hours lowered productivity per head and demotivated the economy. He also said that even if the standard working hours can shorten employees weekly working hours, they may need to work for more years to earn sufficient amount of money for retirement, i.e. delay their retirement age. The working time in the whole life may not change.[96]\\r\\nLok-sang Ho, Professor of Economics and Director of the Centre for Public Policy Studies of Lingnan University, pointed out that as different employees perform various jobs and under different degrees of pressures, it may not appropriate to establish standard working hours in Hong Kong; and he proposed 50-hour maximum work week to protect workers health.[97]\\r\\nSingapore enacts an 8-hour normal work day, a 44-hour normal working week, and a maximum 48-hour work week. It is to note that if the employee works no more than five days a week, the employees normal working day is 9-hour and the working week is 44 hours. Also, if the number of hours worked of the worker is less than 44 hours every alternate week, the 44-hour weekly limit may be exceeded in the other week. Yet, this is subjected to the pre-specification in the service contract and the maximum should not exceed 48 hours per week or 88 hours in any consecutive two week time. In addition, a shift worker can work up to 12 hours a day, provided that the average working hours per week do not exceed 44 over a consecutive 3-week time. The overtime allowance per overtime hour must not be less than 1.5 times of the employees hour basic rates.[98]\\r\\nThe Kapauku people of Papua think it is bad luck to work two consecutive days. The !Kung Bushmen work just two-and-a-half days per week, rarely more than six hours per day.[99]\\r\\nThe work week in Samoa is approximately 30 hours,[100] and although average annual Samoan cash income is relatively low, by some measures, the Samoan standard of living is quite good.\\r\\nIn India, particularly in smaller companies, someone generally works for 11 hours a day and 6 days a week. No Overtime is paid for extra time. Law enforcement is negligible in regulating the working hours. A typical office will open at 09:00 or 09:30 and officially end the work day at about 19:00. However, many workers and especially managers will stay later in the office due to additional work load. However, large Indian companies and MNC offices located in India tend to follow a 5-day, 8- to 9-hour per day working schedule. The Government of India in some of its offices also follows a 5-day week schedule.[citation needed]\\r\\nNigeria has public servants that work 35 hours per week.[citation needed]\\r\\nMany modern workplaces are experimenting with accommodating changes in the workforce and the basic structure of scheduled work. Flextime allows office workers to shift their working time away from rush-hour traffic; for example, arriving at 10:00?am and leaving at 6:00?pm. Telecommuting permits employees to work from their homes or in satellite locations (not owned by the employer), eliminating or reducing long commute times in heavily populated areas. Zero-hour contracts establish work contracts without minimum-hour guarantees; workers are paid only for the hours they work.","input":"What is a typical work day in germany?"},{"output":"at St. VincentÿSt. Mary High School in his hometown of Akron, Ohio","context":"","input":"Where did lebron james play high school basketball?"},{"output":"Spanish and Mayan cuisine","context":"Many traditional foods in Guatemalan cuisine are based on Spanish and Mayan cuisine and prominently feature corn, chilies and beans as key ingredients.\\r\\nThere are also foods that are commonly eaten on certain days of the week. For example, it is a popular custom to eat paches (a kind of tamale made from potatoes) on Thursday. Certain dishes are also associated with special occasions, such as fiambre for All Saints Day on November 1 and tamales, which are common around Christmas.\\r\\n\\r\\n\\r\\nThere are reportedly hundreds of varieties of tamales throughout Guatemala. The key variations include the ingredients in the masa or dough (corn, potatoes, rice), in the filling (meat, fruits, nuts), and what it is wrapped with (leaves, husks). Tamales in Guatemala tend to be wrapped in green 'maxan' leaves (Calathea lutea), while Chuchitos hi  which resemble Mexican tamales  are wrapped in corn husks.\\r\\nThe masa is made out of corn that is not sweet, such as what is known as feed corn in the U.S.A. In Guatemala, this non-sweet corn is called maize and the corn that Americans are used to eating on the cob (sweet corn), Guatemalans call elote. Tamales in Guatemala are more typically wrapped in plantain or banana leaves and maxan leaves than corn husks. Additionally Guatemalan tamales use cooked masa, which is prepared in a time-consuming process that requires a significant amount of work.","input":"What kind of food do they have in guatemala?"},{"output":"173 UN member states plus the Cook Islands, the Holy See and Niue","context":"\\r\\n\\r\\nThe Berne Convention for the Protection of Literary and Artistic Works, usually known as the Berne Convention, is an international agreement governing copyright, which was first accepted in Berne, Switzerland, in 1886.[1]\\r\\n\\r\\nThe Berne Convention formally mandated several aspects of modern copyright law; it introduced the concept that a copyright exists the moment a work is \\"fixed\\", rather than requiring registration. It also enforces a requirement that countries recognize copyrights held by the citizens of all other parties to the convention.\\r\\n\\r\\nThe Berne Convention requires its parties to treat the copyright of works of authors from other parties to the convention (known as members of the Berne Union) at least as well as those of its own nationals. For example, French copyright law applies to anything published or performed in France, regardless of where it was originally created.\\r\\n\\r\\nIn addition to establishing a system of equal treatment that harmonised copyright amongst parties, the agreement also required member states to provide strong minimum standards for copyright law.\\r\\n\\r\\nCopyright under the Berne Convention must be automatic; it is prohibited to require formal registration. However, when the United States joined the Convention 1 March 1989,[2] it continued to make statutory damages and attorney's fees only available for registered works.\\r\\n\\r\\nHowever, in Moberg v Leygues, a 2009 decision of a Delaware Federal District Court, decided that the protections of the Berne Convention are supposed to essentially be \\"frictionless,\\" meaning no registration requirements can be imposed on a work from a different Berne member country. This means Berne member countries can require works originating in their own country to be registered and/or deposited, but cannot require these formalities of works from other Berne member countries.[3]\\r\\n\\r\\nUnder Article?3, the protection of the Convention applies to nationals and residents of countries that are party to the convention, and to works first published or simultaneously published (under Article?3(4), \\"simultaneously\\" is defined as \\"within 30?days\\"[4]) in a country that is party to the convention.[4] Under Article?4, it also applies to cinematic works by persons who have their headquarters or habitual residence in a party country, and to architectural works situated in a party country.[5]\\r\\n\\r\\nThe Convention relies on the concept of \\"country of origin\\". Often determining the country of origin is straightforward: when a work is published in a party country and nowhere else, this is the country of origin. However, under Article?5(4), when a work is published simultaneously in several party countries (under Article?3(4), \\"simultaneously\\" is defined as \\"within 30?days\\"[4]), the country with the shortest term of protection is defined as the country of origin.[6]\\r\\n\\r\\nFor works simultaneously published in a party country and one or more non-parties, the party country is the country of origin. For unpublished works or works first published in a non-party country (without publication within 30?days in a party country), the author's nationality usually provides the country of origin, if a national of a party country. (There are exceptions for cinematic and architectural works.)[6]\\r\\n\\r\\nIn the Internet age, unrestricted publication online may be considered publication in every sufficiently internet-connected jurisdiction in the world. It is not clear what this may mean for determining \\"country of origin\\". In Kernel v. Mosley, a U.S.?court \\"concluded that a work created outside of the United States, uploaded in Australia and owned by a company registered in Finland was nonetheless a U.S.?work by virtue of its being published online\\". However other U.S.?courts in similar situations have reached different conclusions, e.g. H?kan Moberg v. 33T LLC.[7] The matter of determining the country of origin for digital publication remains a topic of controversy among law academics as well.[8]\\r\\n\\r\\nThe Berne Convention states that all works except photographic and cinematographic shall be copyrighted for at least 50?years after the author's death, but parties are free to provide longer terms,[9] as the European Union did with the 1993 Directive on harmonising the term of copyright protection. For photography, the Berne Convention sets a minimum term of 25?years from the year the photograph was created, and for cinematography the minimum is 50?years after first showing, or 50?years after creation if it hasn't been shown within 50?years after the creation. Countries under the older revisions of the treaty may choose to provide their own protection terms, and certain types of works (such as phonorecords and motion pictures) may be provided shorter terms.\\r\\n\\r\\nIf the author is unknown, because for example the author was deliberately anonymous or worked under a pseudonym, the Convention provides for a term of 50?years after publication (\\"after the work has been lawfully made available to the public\\"). However, if the identity of the author becomes known, the copyright term for known authors (50?years after death) applies.[9]\\r\\n\\r\\nAlthough the Berne Convention states that the copyright law of the country where copyright is claimed shall be applied, Article?7(8) states that \\"unless the legislation of that country otherwise provides, the term shall not exceed the term fixed in the country of origin of the work\\",[9] i.e.,?an author is normally not entitled a longer copyright abroad than at home, even if the laws abroad give a longer term. This is commonly known as \\"the rule of the shorter term\\". Not all countries have accepted this rule.\\r\\n\\r\\nAs to works, protection must include \\"every production in the literary, scientific and artistic domain, whatever the mode or form of its expression\\" (Article 2(1) of the Convention).\\r\\n\\r\\nSubject to certain allowed reservations, limitations or exceptions, the following are among the rights that must be recognized as exclusive rights of authorization:\\r\\n\\r\\nThe Berne Convention does not explicitly authorize countries to permit \\"fair uses\\" of copyrighted works in other publications or broadcasts in a broad sense.[10] Berne only permits a limited form of copyright exceptions and limitations known as the \\"three-step test.\\" Copyright exceptions permitted by the Berne Convention are scattered in several provisions due to the historical reason of Berne negotiations. For example, Article 10(2) permits Berne members to provide for a \\"teaching exception\\" within their copyright statutes. But this exception is limited to a use for illustration of the subject matter taught and it must be related to teaching activities.[11] In other words, a broad exception in a manner similar to the \\"fair use\\" jurisprudence of American copyright law is not expressly permitted by Berne. Nevertheless, the WTO Panel has ruled, in United State - Section 110(5) of the U.S. Copyright Act, that although the three-step test under both Berne and TRIPs required a \\"certain and well-defined exception,\\" this did not rule out open norms such as the US copyright law.\\r\\n\\r\\nThe Agreed Statement of the parties to the WIPO Copyright Treaty of 1996 states that: \\"It is understood that the mere provision of physical facilities for enabling or making a communication does not in itself amount to communication within the meaning of this Treaty or the Berne Convention.\\"[12] This language may mean that Internet service providers are not liable for the infringing communications of their users.[12] Critics claim that the convention does not mention any other rights of consumers of works except for fair use.[13]\\r\\n\\r\\nThere is a legal debate about whether the U.S. Fair Use doctrine is lawful under the Three-step test.[14] However, fair use derives from the First Amendment, and thus would apply regardless of whether Berne permitted fair use or not.\\r\\n\\r\\nThe Berne Convention was developed at the instigation of Victor Hugo[15] of the Association Littraire et Artistique Internationale.[16] Thus it was influenced by the French \\"right of the author\\" (droit d'auteur), which contrasts with the Anglo-Saxon concept of \\"copyright\\" which only dealt with economic concerns.[17] Under the Convention, copyrights for creative works are automatically in force upon their creation without being asserted or declared. An author need not \\"register\\" or \\"apply for\\" a copyright in countries adhering to the Convention. As soon as a work is \\"fixed\\", that is, written or recorded on some physical medium, its author is automatically entitled to all copyrights in the work and to any derivative works, unless and until the author explicitly disclaims them or until the copyright expires. Foreign authors are given the same rights and privileges to copyrighted material as domestic authors in any country that ratified the Convention.\\r\\n\\r\\nBefore the Berne Convention, copyright legislation remained uncoordinated at an international level.[18] So for example a work published in the United Kingdom by a British national would be covered by copyright there, but could be copied and sold by anyone in France. Dutch publisher Albertus Willem Sijthoff, who rose to prominence in the trade of translated books, wrote to Queen Wilhelmina of the Netherlands in 1899 in opposition to the convention over concerns that its international restrictions would stifle the Dutch print industry.[19]\\r\\n\\r\\nThe Berne Convention followed in the footsteps of the Paris Convention for the Protection of Industrial Property of 1883, which in the same way had created a framework for international integration of the other types of intellectual property: patents, trademarks and industrial designs.[20]\\r\\n\\r\\nLike the Paris Convention, the Berne Convention set up a bureau to handle administrative tasks. In 1893 these two small bureaux merged and became the United International Bureaux for the Protection of Intellectual Property (best known by its French acronym BIRPI), situated in Berne.[21] In 1960, BIRPI moved to Geneva, to be closer to the United Nations and other international organizations in that city.[22] In 1967 it became the World Intellectual Property Organization (WIPO), and in 1974 became an organization within the United Nations.[21]\\r\\n\\r\\nThe Berne Convention was completed in Paris in 1886, revised in Berlin in 1908, completed in Berne in 1914, revised in Rome in 1928, in Brussels in 1948, in Stockholm in 1967 and in Paris in 1971, and was amended in 1979.[23]\\r\\n\\r\\nThe World Intellectual Property Organization Copyright Treaty was adopted in 1996 to address the issues raised by information technology and the Internet, which were not addressed by the Berne Convention.[24]\\r\\n\\r\\nThe first version of the Berne Convention treaty was signed on 9 September 1886, by Belgium, France, Germany, Haiti, Italy, Liberia, Spain, Switzerland, Tunisia, and the United Kingdom.[25] They ratified it on September 5, 1887.[26]\\r\\n\\r\\nAlthough the United Kingdom ratified the convention in 1887, it did not implement large parts of it until 100 years later with the passage of the Copyright, Designs and Patents Act 1988.\\r\\n\\r\\nThe United States acceded to the convention on November 16, 1988, and the convention entered into force for the United States on March 1, 1989.[27][26]\\r\\nThe United States initially refused to become a party to the Convention, since that would have required major changes in its copyright law, particularly with regard to moral rights, removal of the general requirement for registration of copyright works and elimination of mandatory copyright notice. This led first to the U.S. ratifying the Buenos Aires Convention (BAC) in 1910, and later the Universal Copyright Convention (UCC) in 1952 to accommodate the wishes of other countries. \\r\\nWith the WIPO's Berne revision on Paris 1971,[28] many other countries joined the treaty, as expressed by Brazil federal law of 1975.[29]\\r\\n\\r\\nOn 1 March 1989, the U.S. Berne Convention Implementation Act of 1988 was enacted, and the U.S.?Senate advised and consented to ratification of the treaty, making the United States a party to the Berne Convention,[30] and making the Universal Copyright Convention nearly obsolete.[31] Except for extremely technical points not relevant, with the accession of Nicaragua in 2000, every nation that is a member of the Buenos Aires Convention is also a member of Berne, and so the BAC has also become nearly obsolete and is essentially deprecated as well.[who?]\\r\\n\\r\\nSince almost all nations are members of the World Trade Organization, the Agreement on Trade-Related Aspects of Intellectual Property Rights requires non-members to accept almost all of the conditions of the Berne Convention.\\r\\n\\r\\nAs of February 2018, there are 176 states that are parties to the Berne Convention. This includes 173 UN member states plus the Cook Islands, the Holy See and Niue.","input":"What countries are part of the berne convention?"},{"output":"Takeru Kobayashi","context":"Takeru Kobayashi (ݛ W, Kobayashi Takeru, born March 15, 1978) is a Japanese competitive eater. He holds many records, including eight Guinness Records, for eating hot dogs, meatballs, Twinkies, tacos, hamburgers, pizza, ice cream and pasta.\\r\\n\\r\\n\\r\\nBorn in Nagano, Japan, Kobayashi set his first record at his rookie appearance on July 4, 2001, when he ate 50 hot dogs in 12 minutes at the Nathan's Coney Island Hot Dog Eating Contest, doubling the previous record of 25. The record was so unexpected that when Kobayashi got to the later numbers, the organizers ran out of signs indicating how many dogs Kobayashi had eaten and had to resort to handwritten signs. Kobayashi would go on to break his own record three times in winning the contest six consecutive times (2001ÿ2006).\\r\\nIn the 2006 Krystal Square Off, Kobayashi's mark of 97 hamburgers was 30 better than his winning total in 2005 and 28 better than the World Record he set in 2004. At a speed-eating contest in Hong Kong on August 13, 2005, Kobayashi consumed 83 vegetarian jiaozi dumplings in 8 minutes.[2] The next day, he ate 100 roasted pork buns in 12 minutes.[3] Kobayashi also won the 2005 Alka-Seltzer US Open of Competitive Eating, a three-hour IFOCE elimination tournament on ESPN,[4] as well as the Glutton Bowl, a two-hour IFOCE eating special that aired on the Fox Network in 2002.[5][6] However, on Fox's 2003 show Man vs. Beast, Kobayashi lost in an eating competition against a 1089-pound (494 kg) Kodiak bear, when he ate 31 bunless hot dogs in 2 minutes and 36 seconds to the bear's 50.[7] In a 2014 interview, Kobayashi claims to have beaten the bear in the rehearsal.[8] (In October 2012, Kobayashi broke the record held by the bear at the Texas state fair.)[citation needed]\\r\\nOn August 5, 2006, Kobayashi set yet another world record at the Johnsonville World Bratwurst Eating Championship in Sheboygan, Wisconsin, by downing 58 bratwurst sausages in 10 minutes, shattering the previous record of 35 set the previous year by Sonya Thomas.[9]\\r\\nOn September 23, 2006, Takeru Kobayashi set the world record at the Phantom Food Festival in Boston, for eating 41 Summer Shack lobster rolls in 10 minutes, replacing the previous record of 22 rolls. Other world-eating records held by Kobayashi include 17.7 pounds (8.0?kg) of cow brains in 15 minutes and 20 pounds (9.1?kg) of rice balls in 30 minutes.\\r\\nOn June 25, 2007, Kobayashi announced on his blog that he seriously injured his jaw during training. He stated that he could only open his jaw about the width of a fingertip. Kobayashi's participation in the July 4, 2007, Nathan's contest continued as scheduled. He was able to eat a personal record 63 hot dogs, though his mark was bettered by Joey Chestnut's 66.[10]\\r\\nOn July 4, 2008, Kobayashi once again competed in the Nathan's contest. He ate 59 hot dogs.[11]\\r\\nKobayashi went on to defeat Joey Chestnut, on May 31, 2009, in a Pizza Hut P'Zone competition at Sony Studios in Culver City, California. The competition aired on Spike TV on June 21.[12]\\r\\nIn July 2009, Kobayashi visited Puerto Rico in a special appearance for Taco Bell's Why Pay More Challenge, eating 64 tacos in 15 minutes for a local charity.[13]\\r\\nOn July 4, 2009, he competed again in the Nathan's contest. He ate 64.5 hot dogs and buns.[14]\\r\\nOn September 27, 2009, Kobayashi defeated Chestnut again with a score of 93 (68 Krystals, 5 Big Angus Burgers), earning the $20,000 top prize. Chestnut was second, with 81, and Pat \\"Deep Dish\\" Bertoletti finished third, with 76.[15]\\r\\nOn July 4, 2011, Kobayashi competed on the rooftop of a Manhattan bar simultaneously with the Nathan's Contest at Coney Island via a live video simulcast of the event. Kobayashi finished 69 hot dogs, one more than the officially recognized previous world record. That world record stood as the highest ever eaten until 2016 when Joey Chestnut ate a record 70 at that year's Nathan's Hot Dog Eating Contest.[16]\\r\\nOn January 23, 2012. Kobayashi went on The Wendy Williams Show to set the record for eating the most Twinkies in one minute, for the \\"Save The Twinkie\\" campaign, and set a new world record of 14 Twinkies.\\r\\nOn February 3, 2012, Kobayashi set the new Wing Bowl record for eating chicken wings at Wing Bowl XX, held at the Wells Fargo Center in Philadelphia. His total was 337 wings in his first competition in that event.[17]\\r\\nOn August 26, 2012, Kobayashi set the new world record at the New York State Fair in Syracuse for eating 110 hot dogs in 10 minutes.\\r\\nIn October 2012, Kobayashi set the new world record at the Texas State Fair for eating 60 hot dogs in 2 minutes 35 seconds.\\r\\nOn June 30, 2012, Kobayashi revealed the Major League Eating (MLE) contract he was required to sign in order to compete in Nathan's Fourth of July hot dog eating competition. The year-long contract limited him to $40,000 and took away any rights to endorse or engage in anything outside of what MLE mandated.[18]\\r\\nOn July 4, 2012, Kobayashi competed in the Crif Dog Classic. He ate 58.5 hot dogs and buns.\\r\\nOn October 11, 2012, Kobayashi set the new world record at the Gringo Bandito Taco Challenge by eating 106 tacos in 10 minutes[19]\\r\\nOn July 21, 2013, Kobayashi defended his title at the Gringo Bandito Taco Challenge.\\r\\nOn October 6, 2013, Kobayashi won \\"LET 'EM EAT\\" Canada's biggest pizza eating contest for the fourth year in a row.[20]\\r\\nOn August 4, 2014, Kobayashi set the new world record at \\"LET 'EM EAT\\" Canada's biggest pizza eating contest by eating 62 slices of pizza (15 and a half pizzas) in 12 minutes.[21]\\r\\nKobayashi expands his stomach for a competition by eating larger and larger amounts of food, and then exercises to ensure that fat will not impede expansion of his stomach during a competition.[22]\\r\\nKobayashi's official web site gives his height as 173?cm (5 ft 8 in) and his weight as 58?kg (128?lb). However he's weighed as much as 87?kg (192?lb) according to a June 29, 2006 blog entry.[23] As of July 4, 2009, Kobayashi weighed in at 60?kg (132?lb) for the annual Fourth of July hot dog eating competition on Coney Island.[24]\\r\\nKobayashi is also known for his trademark body wiggle, referred to by some as the \\"Kobayashi Shake\\", to force food down his esophagus and settle more compactly in his stomach.[25] He eats hot dogs by splitting the frankfurter in half, dipping the buns in water, and then stuffing both parts in his mouth. He calls this the Solomon Method.[26]\\r\\nOn June 28, 2010, Kobayashi announced he would not compete in the Nathan's Fourth of July Hot Dog Eating Competition. The impasse was reportedly due to the MLE's insistence that Kobayashi signed an exclusive contract with the organization that would prevent him from competing in contests not sanctioned by MLE.[39]\\r\\nOn July 4, 2010, Kobayashi was in attendance at the Nathan's International Hot Dog Eating Contest, watching from the crowd. Wearing a black T-shirt that read \\"Free Kobi\\", Kobayashi mingled with the crowd, standing inside a police-barricaded pen just under the stage. After the competition ended, he slipped up the stage stairs and went onto the stage. Although he was initially ushered by security officers up to the stage, one security officer (thought to have been requested by George Shea), quickly arrested him from the back. Kobayashi resisted in surprise of what was happening to him, hanging on to the barricades and fences before being taken to the police car. Some witnesses reported that Kobayashi was attempting to congratulate the winner, Joey Chestnut.[40] Co-host and MLE President Richard Shea stated that \\"[Kobayashi] tried to jump on stage during the awards ceremony to disrupt it.\\"[41] With the crowd chanting at him, Kobayashi was arrested. He was charged with resisting arrest, trespassing, and obstructing government administration and subsequently was taken to jail awaiting an appearance in Brooklyn Criminal Court.[42]\\r\\nKobayashi's interpreter and publicist, Maggie James, said he had originally gone in hopes to cheer on his fellow competitive eaters, but after arriving and the chanting from the fans, he was swooped onto the stage due to the excitement. She said \\"There's a contract dispute, they weren't giving him his freedom. It was unfair.\\"[43]\\r\\nKobayashi told reporters he had a sandwich and a glass of milk while being held. \\"I am very hungry\\", he said. \\"I wish there were hot dogs in jail.\\"[44]\\r\\nOn August 5, 2010, all charges against Kobayashi were dismissed by a judge in Brooklyn.[45] Despite his record six consecutive victories in their annual event, Nathan's removed Kobayashi's image from their \\"Wall of Fame\\" in 2011.[46]\\r\\nAfter Kobayashi left Nathan's, the hot dog contest lost sponsorship from Old Navy, Heinz Tomato Ketchup, Pepto-Bismol, and was down year-to-year. With an average 0.7 HH U.S. rating, it was off just a tenth of a point from 2012, when it aired on ESPN. ESPN averaged 1.949 million viewers for 2011's Nathan's Famous Hot Dog Eating Contest, but went down 41% to 1.15 million viewers in 2013.[47][48]\\r\\nIn 2011, Kobayashi was still barred from the annual Nathan's event due to the contract dispute. On July 4, he competed on the rooftop of a Manhattan bar, 230 Fifth, for the duration of the Coney Island contest. Two official judges from the Athletic Association of NY observed Kobayashi while the live broadcast of the event played next to him on a large television screen. Kobayashi finished with 69 hot dogs, becoming the first to set this world record, which was surpassed on July 4, 2016 by Joey Chestnut's 70 hotdogs at the annual Nathan's competition.\\r\\nIn 2012, Kobayashi was still barred from the annual Nathan's event due to the contract dispute. Kobayashi ate 58.5 at Crif dog classic, eating a different type hot dog from Nathan's Famous Fourth of July International Hot Dog Eating Contest.[49]\\r\\nKobayashi ate 113 bunless Nathan's hot dogs at 230 Fifth.[50]\\r\\nIn 2005, Kobayashi appeared in a commercial for ESPN's SportsCenter.[51]\\r\\nIn 2007, Kobayashi appeared in commercials for MasterCard[52][53] and Coors Light.[54]\\r\\nIn 2008 Kobayashi appeared in a Western Canada Lottery Corporation commercial.[55]\\r\\nOn May 30, 2009, Kobayashi attended the Spike Guys' Choice Awards.\\r\\nIn November 2010, Kobayashi appeared in a magazine featuring men's clothing published as an offshoot of V magazine, VMAN 20th Winter.[56]\\r\\nIn November 2010, Kobayashi competed against Donkey Kong in a banana eating contest at the Rio-Can Centre in Toronto as part of the launch for Nintendo's Donkey Kong Country Returns.[57]\\r\\nOn July 7, 2011, Kobayashi made a guest appearance at Hewlett Packard event 2011. The other guests were Snoop Dogg, Sugar Ray, Dan Finnerty, Third Eye Blind, Candlebox.[58]\\r\\nIn 2011, Kobayashi appeared in TVB commercial.[citation needed]\\r\\nIn Spring 2011, Kobayashi appeared in his first major fashion editorial in Canada's The Block magazine as a model. Additionally, Kobayashi is an aspiring dog trainer, with six labradoodles he calls his \\"hot dogs.\\"[59]\\r\\nOn March 20, 2012, Kobayashi appeared in a Jake and Amir video produced by College Humor.[60]\\r\\nIn June 2012, Kobayashi made a special guest appearance and a taco demonstration at The Offspring's release party for their album Days Go By.[61]\\r\\nIn 2012, Kobayashi appeared in Eight O'Clock Coffee,[62] and Hofmann[63] commercial.\\r\\nIn 2012, Kobayashi appeared on a celebrity edition of Fear Factor.[citation needed]\\r\\nIn 2013, Kobayashi appeared in Just-Eat[64] and Thuzio [65] commercials.\\r\\nKobayashi has been featured on Late Night with Jimmy Falon, Saturday Night Live,[66] MTV's True Life, MTV After Hours With Josh Horowitz,[67] The Daily Show with Jon Stewart, The Wendy Williams Show and has done original features with Buzzfeed.com,[68][68] CollegeHumor.com[69] and SI.com,[70][71] and is a featured user on the foodie mobile and web-based app Foodspotting.\\r\\nOn July 4, 2013, Kobayashi unveiled his new line of all-beef midwestern grain-fed hot dogs, known officially as \\"Kobi Dogs\\"[72] at Eventi Hotel in New York.\\r\\nOn September 16, 2014, he appeared in a YouTube video to be competing with a hamster. The video ends in Kobayashi acknowledging his defeat by putting a medal around the hamster's neck.[73][74]\\r\\nOn September 22, 2015, Kobayashi, along with Patrick Bertoletti, Kevin Strahle and Bob Shoudt consumed a 40-pound goat in 13 minutes and 22 seconds at Taco In A Bag in Chicago.[75]\\r\\n2015, Takeru Kobayashi consumes 110 hot dogs without buns in 10 minutes.[76]\\r\\nKobayashi's eating abilities partially inspired the King of the Hill (season 7) episode, \\"The Fat and the Furious.\\" This episode originally aired on November 10, 2002.\\r\\nKobayashi also helped inspire the 2005 CSI: Crime Scene Investigation (season 6) episode \\"Dog eat Dog.\\" The episode features a victim who ate himself to death, in part from eating several hot dogs at a hot dog eating contest.\\r\\nKobayashi is mentioned by Will Ferrell's character, Ricky Bobby in the 2006 film, Talladega Nights: The Ballad of Ricky Bobby.\\r\\nReferenced in movie Step Brothers by Randy (Rob Riggle) after Derek (Adam Scott) threatens that Randy will, \\"...eat Brennan's (Will Ferrell's) dick\\", if Brennan botches the Catalina Wine Mixer, Randy adds, \\"Like Kobayashi!\\"[77]\\r\\nKobayashi appears in the ending cinematic of level three, \\"Around the World in 80 Bites\\" in the 2007 video game, The Simpsons Game.\\r\\nIn The Simpsons episode \\"Luca$\\" in the 25th season, the character Lucas Bortner wears a t-shirt with a photo of Kobayashi eating and refers to him in a dialogue.\\r\\nIn the 1000 Ways To Die skit \\"Vom-Ate-Dead\\" a professional eater modeled after Takeru Kobayashi is cornered by an emetophiliac woman who forces him to vomit on her, which causes her death when she chokes on an undigested piece of hot dog.\\r\\nKobayashi appears as himself on a 2012 episode of Jake and Amir, a CollegeHumor web series. He becomes aggressive when Jake tries to feed him Hotdogs.","input":"Who is the world hot dog eating champion?"},{"output":"IATA: SAN","context":"The revisions to be redacted have not been specified.  Please check the talk page for more information.\\r\\n\\r\\nNote to the nominator: Make sure the page has already been reverted to a non-infringing revision or that infringing text has been removed or replaced before submitting this request. This template is reserved for obvious cases only, for other cases refer to Wikipedia:Copyright problems.\\r\\n\\r\\n\\r\\nNote to admins: In case of doubt, remove this template and post a message asking for review at WT:CP.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nSan Diego International Airport (IATA: SAN, ICAO: KSAN, FAA LID: SAN), formerly known as Lindbergh Field, is an international airport 3?mi (4.8?km) northwest of Downtown San Diego, California, United States. It is owned and operated by the San Diego County Regional Airport Authority.[5][6] San Diego International Airport covers 663 acres (268 ha) of land.[5]\\r\\n\\r\\nIn 2015, traffic at San Diego International exceeded 20 million passengers, serving more than 500 scheduled operations carrying about 50,000 passengers each day.[3] While primarily serving domestic traffic, San Diego has nonstop international flights to Canada, Germany,  Japan, Mexico, Switzerland, and the United Kingdom.[7]\\r\\n\\r\\nSan Diego is the largest metropolitan area in the United States that is not an airline hub or secondary hub; however, San Diego is a focus city for Alaska Airlines and Southwest Airlines. The top five carriers in San Diego during 2015, by seat capacity, were Southwest Airlines (42.7%), American Airlines (14.0%), United Airlines (11.2%), Alaska Airlines (10.1%), and Delta Air Lines (9.9%).[8]\\r\\n\\r\\nSan Diego International is the busiest single runway airport in the United States and third-busiest single runway in the world, behind Mumbai and London Gatwick.[9][10] [Note 1] Due to the short usable length of the runway, proximity to the skyscrapers of Downtown San Diego, and steep landing approach as a result of the nearby Peninsular Ranges, SAN has been called \\"the busiest, most difficult single runway in the world.\\"[11][12][13][14] SAN operates in controlled airspace served by the Southern California TRACON, which is some of the busiest airspace in the world.[15]\\r\\n\\r\\nThe airport is near the site of the Ryan Airlines factory, but it is not the same as Dutch Flats, the Ryan airstrip where Charles Lindbergh flight tested the Spirit of St. Louis before his historic 1927 transatlantic flight. The site of Dutch Flats is on the other side of the Marine Corps Recruit Depot, in the Midway neighborhood, near the intersection of Midway and Barnett avenues.[16]\\r\\n\\r\\nInspired by Lindbergh's flight and excited to have made his plane, the city of San Diego passed a bond issue in 1928 for the construction of a two-runway municipal airport. Lindbergh encouraged the building of the airport and agreed to lend his name to it.[17] The new airport, dedicated on August 16, 1928, was San Diego Municipal Airport ÿ Lindbergh Field.\\r\\n\\r\\nThe airport was the first federally certified airfield to serve all aircraft types, including seaplanes. The original terminal was on the northeast side of the field, on Pacific Highway. The airport was also a testing facility for several early US sailplane designs, notably those by William Hawley Bowlus (superintendent of construction on the Spirit of St. Louis) who also operated the Bowlus Glider School at Lindbergh Field from 1929ÿ1930. On June 1, 1930, a regular San DiegoÿLos Angeles airmail route started. The airport gained international airport status in 1934, and a United States Coast Guard Air Base next to the field was commissioned in April 1937. The Coast Guard's fixed-wing aircraft used Lindbergh Field until the mid-1990s when their fixed-wing aircraft were retired.[citation needed]\\r\\n\\r\\nA major defense contractor and contributor to World War II heavy bomber production, Consolidated Aircraft, later known as Convair, had their headquarters on the border of Lindbergh Field, and built many of their military aircraft there. Convair used the airport for test and delivery flights from 1935 to 1995.[18]\\r\\n\\r\\nThe US Army Air Corps took over the field in 1942, improving it to handle the heavy bombers being manufactured in the region. Two camps were established at the airport during World War II and were named Camp Consair and Camp Sahara.[19] This transformation, including an 8,750?ft (2,670?m) runway, made the airport \\"jet-ready\\" long before jet airliners came into service.[20] The May 1952 C&GS chart shows an 8,700-ft runway 9 and a 4,500-ft runway 13.\\r\\n\\r\\nPacific Southwest Airlines (PSA) established its headquarters in San Diego and started service at Lindbergh Field in 1949. The April 1957 Official Airline Guide shows 42 departures per day: 14 American, 13 United, 6 Western, 6 Bonanza, and 3 PSA (5 PSA on Friday and Sunday). American had a nonstop flight to Dallas and one to El Paso; aside from that, nonstop flights did not reach beyond California and Arizona. Nonstop flights to Chicago started in 1962 and to New York in 1967.\\r\\n\\r\\nThe first scheduled jet flights at Lindbergh Field were in 1960, with American Airlines flying to Phoenix and United Airlines to San Francisco, using the Boeing 720.[citation needed]\\r\\n\\r\\nThe original terminal was on the north side of the airport and was used until the 1960s; the current Terminal 1 opened on the south side of the airport on March 5, 1967. Terminal 2 opened on July 11, 1979. These terminals were designed by Paderewski Dean & Associates.[21] A third terminal, dubbed the Commuter Terminal, opened July 23, 1996. Terminal 2 was expanded by 300,000 square feet (27,871?m2) in 1998, and opened on January 7, 1998.  The expanded Terminal 2 and the Commuter Terminal were designed by Gensler and SGPA Architecture and Planning.[22][23] As downtown San Diego developed, the airport's 3,600?ft (1,100?m) second runway was closed as its short length provided no operational benefits other than to support the smallest of aircraft.\\r\\n\\r\\nThe airport was originally built and operated by the City of San Diego through the sale of municipal bonds to be repaid by airport users. In 1962 it was transferred to the San Diego Unified Port District by a state law. In 2001 the San Diego County Regional Airport Authority was created and assumed jurisdiction over the airport.[24]\\r\\n\\r\\nSan Diego International Airport's expansion and enhancement program for Terminal 2 was dubbed \\"The Green Build\\". Additions include 10 gates on the west side of Terminal 2 West, a two-level roadway separating arriving and departing passengers, additional security lanes, and an expanded concession area.[25]  It was completed in August 13, 2013 and cost US$900 million.[26] In January 2016 the airport opened a new consolidated rental car facility on the north side of the airport. The US$316 million, 2-million-square-foot (190,000?m2) facility houses 14 rental car companies and is served by shuttle buses to and from the terminals.[27] A new three-story parking structure in front of Terminal 2 was launched in July 2016 and completed in May, 2018.[28][26]\\r\\n\\r\\nThe Airport Development Plan (ADP) is the next master-planning phase for San Diego International Airport.[29] In 2006, a county-wide ballot measure to move the airport was defeated. Therefore, the airport will continue in its current location for the foreseeable future. The ADP identifies improvements that will enable the airport to meet demand through 2035, which is approximately when projected passenger activity levels will reach capacity for the airport's single runway. An additional runway is not being considered.\\r\\n\\r\\nIn a broad sense, the ADP envisions the replacement of Terminal 1 and other related improvements. As a first step in the ADP, several potential concepts were developed. These concepts represented the first step in a comprehensive planning process.\\r\\n\\r\\nExtensive public outreach was conducted to obtain input from residents and airport stakeholders in the San Diego region. The Airport Authority Board eventually selected a preferred alternative and a detailed environmental analysis is now under way. The environmental review and planning process is expected to conclude in spring 2017.\\r\\n\\r\\nA new immigrations and customs part of Terminal 2 is under construction. Currently international arrivals are handled at gates 20, 21, and 22. By 2018, international arrivals will be handled at gates 47, 48, 49, 50 and 51. This expansion was all due to the sharp rise of international travels handled at San Diego International Airport.[30][31]\\r\\n\\r\\nCalifornia State Assembly Bill 93 created the San Diego County Regional Airport Authority (SDCRAA) in 2001.[24] The SDCRAA projects that SAN will be constrained due to congestion between 2015-2022.[32] In June 2006, SDCRAA board members selected Marine Corps Air Station Miramar as its preferred site for a replacement airport, despite military objections. On November 7, 2006, San Diego County residents defeated an advisory relocation that included a joint use proposal measure.[33][34]\\r\\n\\r\\nMultiple studies have been conducted on where to place an airport dating back to 1923. The first study developed the site location plan for Lindbergh field. Eighteen studies were conducted by private groups, most in the early days by those who were opposed to Lindbergh being built instead of on land set aside at what is now Montgomery Field. One was a revisiting of a study done in the 1980s by the City in 1994 when Naval Air Station Miramar closed and was then immediately transferred to the US Marine Corps as Marine Corps Air Station Miramar. Another was by the City of San Diego in 1984 and another that started in 1996 and sat dormant with SANDAG until the airport authority was formed. This study is the first study ever done to look for a new site by an agency that actually had jurisdiction over the issue, and the first non-site specific comprehensive study of the entire region.\\r\\n\\r\\nThree professors from the University of California San Diego (UCSD), Fred Spiess, PhD, former director of the Scripps Institution of Oceanography; Walter Munk, PhD, Secretary of the Navy/Chief of Naval Operations Chair for Scripps Institute of Oceanography and Frieder Seible, PhD, former dean of the Jacobs School of Engineering, proposed building a floating airport in deep water west of the end of Interstate 8 in Ocean Beach. Their plan would use Lindbergh Field terminals and parking facilities for passenger ticketing and security then whisk them by tram via Interstate 8 and an Archimedes bridge to the floating airport, a trip of perhaps 15 minutes at 60?mph (97?km/h). Several groups have approached the Airport Board for sponsorship to apply for federal funds to develop the plan, but the proposal has never been given serious consideration. However, the advent of 3D concrete printing and special polymerized, fiber-reinforced concrete mixesan efficient method of constructionmay make this proposal less expensive than building a new airport on land.[35][36]\\r\\n\\r\\nIn 2013, graduate business students at California State University San Marcos (CSUSM) released several studies around a new proposed international airport in San Diego County. The first group of studies are known as San Diego Airport Exploratory Study 1.0.[37] Qualcomm sponsored the CSUSM airport exploratory studies.[38] The research project examined several topics: Demand Projections,[39] Economic Implications,[40] Infrastructure, Traffic & Transportation,[41] Political Implications[42] and Potential Demographic Changes.[43]  The study discussed the political implications of developing a new international airport at three different locations: Marine Corps Air Station Miramar, Campo/Boulevard and Marine Corps Base Camp Pendleton.  The CSUSM airport studies build upon the Ricondo & Associates Consulting Group 2006 report that evaluated the advantages and disadvantages of building an airport at each of these sites.[42] The San Diego Airport Exploratory Study 1.0 determined the only feasible location would be on the southern edge of Camp Pendleton [39] at the existing Munn Field air strip. The location is on approximately 5000 acres, comprising 4% of the total land at Camp Pendleton.\\r\\n\\r\\nIn 2014 and 2015, Qualcomm sponsored further studies known as San Diego Airport Exploratory Study 2.0 [44] and San Diego Airport Exploratory Study 2.1.[45]  The studies have generated media attention in local newspapers,[46] KPBS radio [47][48]  and local news stations such as KUSI.[49][50][51]  \\r\\nOn December 9, 2015, students presented the San Diego Airport Exploratory Study 2.1 report at California State University San Marcos.  Qualcomm founder Irwin Jacobs[51] and real estate investor Malin Burnham were among those in attendance at the presentation.\\r\\n\\r\\nThe airport has nearly completed a substantial expansion of concessions. 73 new shops and food and beverage locations have opened throughout the terminals.[52]\\r\\nThree airline lounges are located in the airport in Terminal 2: Delta SkyClub, United Club, and a joint Airspace Lounge/American Airlines Admirals Club.[53]\\r\\n\\r\\nUntil 2015, major rental cars companies operated out of ground-level facilities across Harbor Drive from the airport, with each company operating its own shuttle. Other companies were located on private property near the airport. In January 2016 the airport opened a consolidated rental car facility on the north side of the airport, housing 14 rental car agencies with capacity for 19. An on-airport shuttle bus service transports passengers to and from the airport. The same shuttle bus also serves passengers from off-site rental car companies, and is intended to carry passengers from a nearby trolley stop as well.[54]\\r\\n\\r\\nSan Diego International Airport has two terminals:\\r\\n\\r\\nThe airport is home to the largest airport USO center in the world.[56]\\r\\n\\r\\nThe airport promotes education about its history,[57] and sponsors an \\"airport explorers\\" program.[58]\\r\\n\\r\\nThere are several well-known pieces of art on display at the airport.[59] Inside Terminal 2 is a recreation of The Spirit of St. Louis.  \\"At the Gate\\", a popular piece with tourists, depicts comical characters patiently waiting for their planes.[60]  Terminal 2 also features \\"The Spirit of Silence,\\" a meditation room designed by public artist Norie Sato.[61]\\r\\n\\r\\nBBA Aviation's Signature Flight Support (previously known as Landmark Aviation[83]) is the fixed-base operator (FBO) at San Diego International Airport.[84] It services all aircraft ranging from the single-engine Cessna aircraft to the four-engine Boeing 747. Generally, it services corporate traffic to the airport. The FBO ramp is located at the northeast end of the airfield.\\r\\n\\r\\nThe airport consists of a single runway designated runway 9/27 for its magnetic compass headings of 095 degrees (106 True) and 275 degrees (286 True) orienting it directly east and west in relation to magnetic north. The runway is an asphalt and concrete design with dimensions of 9,401 feet (2,865?m) x 200 feet (61?m).  A displaced threshold exists in both directions. For runway 27 the first 1,810 feet (550?m) are displaced and for runway 9 the first 1,000 feet (300?m) are displaced.\\r\\n\\r\\nTypical prevailing winds in San Diego are from the west and most takeoffs and landings at the airport are from east to west using runway 27.\\r\\n\\r\\nThe approach from the east is steeper than most because the terrain drops from 266?ft (81?m) to sea level in less than one nautical mile. The runway is west of a hill with several obstructions, including Interstate 5 and trees in Balboa Park. Contrary to local lore, the parking structure off the end of the runway was built in the 1980s long after previous obstructions were built up east of I-5 and does not affect the approach.\\r\\n\\r\\nLanding at the airport from the east offers closeup views of skyscrapers, Petco Park (home of the San Diego Padres), the San Diego Bay, and the San DiegoÿCoronado Bridge from the left side of the aircraft. On the right, Balboa Park, site of the 1915-1916 Panama-California Exposition, can be seen.\\r\\n\\r\\nRunway 27 (landing east to west), is a localizer and RNP approach with minimums down to 1.5?mi (2.4?km).  For Runway 9, the visibility is .75?mi (1.21?km).  When visibility gets below 1.5?mi (2.4?km), it forces arriving aircraft to use Runway 9 (landing west to east), which has better landing minimums. The terrain east of the airport often imposes weight limits on many departing aircraft. As a result, some aircraft must take off to the west. While safe, these \\"head to head\\" operations slow the flow of aircraft for sequencing and create delays in the air and on the ground.\\r\\n\\r\\nTerrain east and west of the airport greatly impacts the available runway length. Runway 27 (heading west) has a climb gradient of 353?ft/nmi (58.1?m/km) feet per nautical mile. Taking off to the east requires a 610?ft/nmi (100?m/km) climb rate.\\r\\n\\r\\nSan Diego International Airport does not have standard 1,000?ft (300?m) runway safety areas at the end of each runway. An engineered materials arrestor system (EMAS) has been installed at the west end of the runway to halt any aircraft overruns. The east end of the runway does not have such a system as its use would reduce the runway length by at least 400?ft (120?m), further impacting the runway's capability for departures to the west. Instead, the use of declared distances reduces the mathematical length of Runway 9 (west to east operations) by declaring that the easternmost end of Runway 9 is 1,121?ft (342?m) shorter with a net length of 8,280?ft (2,520?m).[89]\\r\\n\\r\\nSAN is in a populated area. To appease the concerns of the airport's neighbors regarding noise and possible ensuing lawsuits, a curfew was put in place in 1979. Takeoffs are allowed between 6:30?a.m. and 11:30?p.m. Outside those hours, they are subject to a large fine. Arrivals are permitted 24 hours per day.[90] While several flights have scheduled departure times before 6:30?a.m., these times are pushback times; the first takeoff roll is at 6:30?a.m.\\r\\n\\r\\nAs of June, 2017, San Diego International Airport is served by 18 passenger airlines and five cargo airlines that fly nonstop to 65 destinations in the United States, Canada, Mexico, Great Britain, Japan, and most recently Germany and Switzerland. Several carriers including Alaska, Southwest, and Spirit have increased their flights to and from San Diego. Additional service between SAN and Los Cabos (Mexico), Dallas, Portland, Boston, Washington D.C./Baltimore, Burbank, and Tokyo were added in 2014; however, Burbank has since been discontinued.[91][92][93]\\r\\n\\r\\nBritish Airways resumed nonstop service to London Heathrow Airport on June 1, 2011 with a Boeing 777-200ER. The airline had dropped this route in October 2003, after the worldwide downturn in aviation after the September 11 attacks in 2001.  The airline had been flying nonstop to London Heathrow (previously London Gatwick on its 777-200s); however, the route had originally been flown from Gatwick via Phoenix Sky Harbor International Airport on a Boeing 747-400.  After the September 11 attacks, the route was reduced to six days a week, then five, and then cancelled. In June 2010 the European Union approved the new Atlantic Joint Business Agreement between British Airways, American Airlines, and Iberia Airlines, which dropped many of the provisions of the Bermuda II treaty and its restrictions on airlines flying to Heathrow. Oneworld members now can earn mileage on any American Airlines, British Airways, or Japan Airlines flight.[94] On March 27, 2016, British Airways changed the aircraft on this flight from the 3-class 777-200 to the 4-class 777-300, increasing passenger and cargo capacity, and to provide first class seats. In November 2015, British Airways announced that it would fly the Boeing 747-400 on the London-San Diego route, and is now used in seasonal service. Before that, 747-400 was flown into the city until 2003 via Phoenix Sky Harbor (which then continued onto Gatwick, not Heathrow).[95]\\r\\n\\r\\nJapan Airlines began service to TokyoÿNarita on December 2, 2012, using the Boeing 787 aircraft. This is the airport's first nonstop flight to Asia. The flights used the 787 until its grounding when service was temporarily replaced with a 777-200ER.  The last 777 flight was May 31, 2013. On June 1, 2013, 787 service resumed, this time daily. This route is covered under the Pacific Joint Business Agreement between Oneworld partners Japan Airlines and American Airlines.[96][97]\\r\\n\\r\\nOn Thursday, June 9, 2016, Condor Airlines  announced thrice-weekly seasonal service from Frankfurt am Main International Airport to San Diego, with Monday flights beginning May 1, 2017, through October 2, 2017, Thursday flights beginning May 4, 2017, through October 5, 2017, and Saturday flights beginning July 8, 2017, through September 2, 2017. Flights will be on a Boeing 767-300 aircraft.[98] On June 21, 2016, Edelweiss Air announced twice-weekly seasonal service from Zrich Airport, beginning Monday, June 9, 2017, with the second flight of the week on Fridays. Flights will be on an Airbus A340-300 aircraft.[99] On June 13, 2017, Lufthansa announced five weekly flights from Frankfurt to San Diego beginning in summer 2018.[100]\\r\\n\\r\\nThe busiest route by flight count is to Los Angeles with 25 daily round trips on United Express, American Eagle, and Delta Connection.  The busiest route by available seats per day is to San Francisco with just over 2,816 seats on 21 daily round trips on United Airlines, Southwest Airlines, and Alaska Airlines.\\r\\n\\r\\nIn January 2008, San Diego International Airport entered the blogosphere with the launch of the first employee blogÿthe Ambassablog[101]ÿfor a major US airport. Written by front-line employees, the blog features regular posts on airport activities, events, and initiatives; reader comments; and several multimedia and interactive features. It has been presented as a case study in employee blogging to several public agencies at the federal, state, and local levels.\\r\\n\\r\\nIn February 2008, San Diego International Airport was one of the first major airports in the US to adopt a formal sustainability policy, which expresses the airport's commitment to a four-layer approach to sustainability known as EONS. As promulgated by Airports Council International ÿ North America, EONS represents an integrated \\"quadruple bottom line\\" of (E)conomic viability, (O)perational excellence, (N)atural resource conservation and preservation and (S)ocial responsibility.\\r\\n\\r\\nIn May 2008, California Attorney General Jerry Brown announced an agreement with San Diego International Airport on reducing greenhouse gas emissions associated with the airport's proposed master plan improvements. In announcing the agreement, the Attorney General's office said \\"San Diego airport will play a key leadership role in helping California meet its aggressive greenhouse gas reduction targets.\\"\\r\\n\\r\\nThere are three public transportation options:\\r\\n\\r\\nSan Diego International Airport is testing a new system of airfield lights called Runway Status Lights (RWSL) for the US Federal Aviation Administration (FAA). It completed the rehabilitation of the north taxiway in 2010. A project that included replacing its airfield lighting and signage with energy efficient LED lights where possible.[citation needed]\\r\\n\\r\\nBecause of the airport's close proximity to downtown San Diego, FAA regulations do not allow any building within a 1.5-mile (2.4?km) radius of the runway to be taller than 500 feet (150?m).[102]\\r\\n\\r\\nCoast Guard Air Station San Diego is located near the southeast corner of the airport. The installation originally supported seaplane operations, with seaplane ramps into San Diego Bay, as well as land-based aircraft and helicopter operations using the airport's runway.\\r\\n\\r\\nThe air station is separated from the rest of the airfield which necessitated moving aircraft across North Harbor Drive, a busy, 6-lane city street, to reach SAN's runway. Stoplights halted vehicle traffic while aircraft crossed North Harbor Drive. This was a common occurrence during the 1970s, 1980s, and early 1990s, when the station had both HH-3F Pelican and HH-60J Jayhawk helicopters and HU-25 Guardian jets assigned.[citation needed]\\r\\n\\r\\nAs of  March?3, 2011[update], the airport is one of a few locations proposed to be the southern terminus of San Diego-Los Angeles branch of the California High-Speed Rail System.[111] Other station proposals include the SDCCU Stadium site and Downtown San Diego.[111] The San Diego portion of the system is the last phase of the project, with estimates putting completion sometime in the 2030s and travel time between Los Angeles and San Diego taking 1 hour and 20 minutes.[112]\\r\\n\\r\\nThe high-speed rail option will be a cheaper alternative to commuter flights to the Los Angeles Metropolitan Area and in-state flights to Central California and the San Francisco Bay Area.[113]\\r\\n\\r\\nA portion of the southeast infield at San Diego International Airport is set aside as a nesting site for the endangered California Least Tern. The least tern nests on three ovals from March through September. The birds lay their eggs in the sand and gravel surface at the southwest end of the airfield. The San Diego Zoological Society monitors the birds from May through September. The terns nest on the airfield because they do not have to compete with beach goers and the airport fence keeps dogs and other animals out, while the airplane activity helps keep predatory hawks away from the nests. Approximately 135 nests were established there in 2007.[114]\\r\\n\\r\\n\\r\\n\\r\\n Media related to San Diego International Airport at Wikimedia Commons","input":"What is the code for san diego airport?"},{"output":"alcohol","context":"A date rape drug, also referred to as a predator drug, is any drug that is an incapacitating agent which, when administered to another person, incapacitates the person and renders them vulnerable to a drug facilitated sexual assault (DFSA), including rape. The most common types of DFSA are those in which a victim consumes a recreational drug such as alcohol administered surreptitiously.[1] The most common form of DFSA is alcohol-related,[2] with the victim in most cases consuming the alcohol voluntarily. Other date rape drugs include rohypnol, ketamine and gamma-hydroxybutyrate (GHB).[3]\\r\\n\\r\\n\\r\\nThere is currently no comprehensive data on how frequently DFSA occurs with the use of surreptitious drug administration. The lack of data is due to the low report rate of assaults, and because rape victims who do report are often either never tested for these drugs, are tested for the wrong ones, or the tests are administered after the drug has been metabolized and left their body.[citation needed]\\r\\nA 1999 study of 1,179 urine specimens from victims of suspected DFSAs in 49 American states found six (0.5%) positive for Rohypnol, 97 (8%) positive for other benzodiazepines, 48 (4.1%) positive for GHB, 451 (38%) positive for alcohol and 468 (40%) negative for any of the drugs searched for.[4] A similar study of 2,003 urine samples of victims of suspected DFSAs found less than 2% tested positive for Rohypnol or GHB.[5] The samples used in these studies could only be verified as having been submitted within a 72-hour time frame or a 48-hour time frame. Some of the substances tested for, such as GHB, are not detectable after 8ÿ12 hours.\\r\\nA three-year study in the UK detected sedatives or disinhibiting drugs that victims said they had not voluntarily taken in the urine of two percent of suspected DFSA victims. In 65% of the 1014 cases included in this study, testing could not be conducted within a time frame that would allow detection of GHB.[6][7] A 2009 Australian study found that of 97 instances of patients admitted to hospital believing their drinks might have been spiked, illicit drugs were detected in 28% of samples, and nine cases were identified as \\"plausible drink spiking cases\\". This study defined a \\"plausible drink spiking case\\" in such a way that cases where (a) patients believed that their drink had been spiked, and (b) lab tests showed agents that patients said they had not ingested would still be ruled out as plausible if the patient did not also (c) exhibit \\"signs and symptoms\\" that were considered \\"consistent with agents detected by laboratory screening.\\"[8]\\r\\nThe drug most commonly used to facilitate sexual assault is alcohol, consumed voluntarily. Practically any drug (administered openly or surreptitiously) that facilitates rape could be considered a date rape drug. Since the mid-1990s, the media and researchers have documented a new form of DFSA featuring date rape drugs such as rohypnol and ketamine. Other drugs used in DFSA include hypnotics such as zopiclone and the widely available zolpidem (Ambien), sedatives such as neuroleptics (anti-psychotics), chloral hydrate and some histamine H1 antagonists, common recreational drugs such as ethanol, marijuana, cocaine, and less common anticholinergics, barbiturates, opioids, PCP, scopolamine,[9] nasal spray ingredient oxymetazoline,[10][11][12] and certain solvents like GHB, GBL, and BD.\\r\\nResearchers agree that the most common form of DFSA is alcohol-related,[2] with the victim in most cases consuming the alcohol voluntarily. In most jurisdictions, alcohol is legal and readily available and is used in the majority of sexual assaults.[10] Many perpetrators use alcohol because their victims often willingly drink it, and can be encouraged to drink enough to lose inhibitions or consciousness. Sex with an unconscious victim is considered rape in most jurisdictions and some assailants have committed \\"rapes of convenience\\" whereby they assaulted a victim after he or she had become unconscious from drinking too much.[13]\\r\\nAlcohol consumption is known to have effects on sexual behavior and aggression. During social interactions alcohol consumption causes more biased appraisal of a partners sexual motives while impairing communication about and enhancing misperception of sexual intentions, effects exacerbated by peer influence about how to behave when drinking.[14] The effects of alcohol at the point of forced sex commonly include an impaired ability to rectify misperceptions, and a diminished ability to resist sexual advances and aggressive sexual behavior.[14]\\r\\nThe Blade released a special report, \\"The Making of an Epidemic,\\" criticizing a study conducted in the 1990s that concluded that 55% of rape victims had been intoxicated. According to The Blade, the study specifically ignored an Ohio statute that excluded \\"...situations where a person plies his intended partner with drink or drugs in hopes that lowered inhibition might lead to a liaison.\\" The author of the study later admitted that the wording of the survey had been ambiguous.[15]\\r\\nThe increase of sexual assaults on college campuses has been attributed to the social expectations of students to participate in alcohol consumption; social norm dictate that students drink heavily and engage in casual sex.[16]\\r\\nVarious studies have concluded the following:\\r\\nZolpidem (Ambien) is one of the most common date-rape drugs according to the U.S. Drug Enforcement Administration.[18]\\r\\nBenzodiazepines (tranquilizers), such as Valium, Librium, Xanax, and Ativan, are prescribed to treat anxiety, panic attacks, insomnia, and several other conditions, and are also frequently used recreationally. Benzodiazepines are often used in DFSA, with the most notorious being flunitrazepam (chemical name) or Rohypnol (proprietary or brand name), also known as \\"roofies,\\" \\"rope,\\" and \\"roaches.\\"[19][20]\\r\\nThe benzodiazepines midazolam and temazepam were the two most common benzodiazepines utilized for date rape.[21]\\r\\nBenzodiazepines can be detected in urine through the use of drug tests administered by medical officials or sold at pharmacies and performed at home. Most tests will detect benzodiazepines for a maximum of 72 hours after it was taken. Most general benzodiazepine detection tests will not detect Rohypnol: the drug requires a test specifically designed for that purpose. One new process can detect a 2?mg dose of Rohypnol for up to 28 days post-ingestion.[11][22] Other tests for Rohypnol include blood and hair tests. Because the most commonly used drug tests often yield false negatives for Rohypnol, experts recommend use of gas chromatography-mass spectrometry analysis.[1][5][23]\\r\\nThis drug is also known as Whitleys, Trip-and-Fall, Ruffies, Rophies, Rope, Roopies, Roofies, Roches, Roach-2, Roach, Mind Erasers, Rib, Lunch Money, R-2, Poor Mans Quaalude, Mexican Valium, LA Rochas, Forget Pill, and Circles. Rohypnols comes as a pill that dissolves in liquids. Some are small, round, and white. Newer pills are oval and green-gray in color. When slipped into a drink, a dye in these new pills makes clear liquids turn bright blue and dark drinks turn cloudy. But this color change might be hard to see in a dark drink, like cola or dark beer, or in a dark room.[24]\\r\\nIn one 2002 survey of 53 women who used Rohypnol recreationally, 10% said they were physically or sexually assaulted while under its influence.[5] If enough of the drug is taken, a person may experience a state of automatism or dissociation. After the drug wears off, users may find themselves unable to remember what happened while under its influence (anterograde amnesia), and feeling woozy, hung-over, confused, dizzy, sluggish and uncoordinated, often with an upset stomach. They may also have some difficulty moving their limbs normally.[1][5][23]\\r\\nRohypnol is believed to be commonly used in DFSA in the United States, the United Kingdom, and throughout Europe, Asia and South America.[25] Although Rohypnol's use in DFSA has been covered extensively in the news media, researchers disagree about how common such use actually is. Law enforcement manuals describe it as one of the drugs most commonly implicated in DFSA,[1] but according to research conducted by Michael Robertson from the San Diego Medical Examiner's office and Dr. Mahmoud El Sohly of El Sohly Laboratories, test results indicated that flunitrazepam was only used in around 1% of reported date rapes according to Robertson and 0.33% according to urine lab tests done by El Sohly, of the rape-kits that actually get tested in time. Despite having a long half-life (18ÿ28 hours) an incorrect belief is that Rohypnol is undetectable 12 hours after administration which may result in victims failing to get a blood or urine test the following day.\\r\\nGamma-hydroxybutyrate (GHB) is a central nervous system depressant. It has no odor, tastes salty,[26] but is detectable when mixed in a drink.\\r\\nGHB is used recreationally to stimulate euphoria, to increase sociability, to promote libido and lower inhibitions.[27] It is sold under names such as Rufies, Luiquid E and Liquid X. It is usually taken orally, by the capful or teaspoon.[28]\\r\\nFrom 1996 to 1999, 22 reports of GHB being used in DFSA were made to the United States Drug Enforcement Administration. A 26-month study of 1,179 urine samples from suspected DFSAs across the United States found 4% positive for GHB.[27] The National Drug Intelligence Center (NDIC) says that in the United States GHB had surpassed Rohypnol as the substance most commonly used in DFSA, likely because GHB is much more easily available, cheaper and leaves the body more quickly.[27][29] GHB is only detectable in urine for six to twelve hours after ingestion.[29]\\r\\nThere were three stories in the media about Rohypnol in 1993, 25 in 1994 and 854 in 1996. In early 1996 Newsweek magazine published \\"Roofies: The date-rape drug\\" which ended with the line \\"Don't take your eyes off your drink.\\" That summer, researchers say all major American urban and regional newspapers covered date rape drugs, with headlines such as \\"Crackdown sought on date rape drug\\" (Los Angeles Times), \\"Drug zaps memory of rape victims\\" (San Francisco Chronicle) and \\"Slow DEA Action Gives Women No Relief from the Threat of New Date-Rape Drug\\" (Detroit News). Date rape drugs were also covered in media aimed at young women such as Seventeen and Sassy magazines. In 1997 and 1998, the date rape drug story received extensive coverage on CNN, ABC's 20/20 and Primetime Live, the Oprah Winfrey Show, and the fictitious TV shows Beverly Hills 90210 and South Park. Women were instructed to never drink from punch bowls, never leave a drink unattended, try no new drinks, drink nothing with an unusual taste or appearance, take their own drinks to parties, and drink nothing opened by another person.\\r\\nNews media has been criticized for overstating the DFSA threat, for providing \\"how to\\" material for potential date rapists and for advocating \\"grossly excessive protective measures for women, particularly in coverage between 1996 and 1998.[30][31] Law enforcement representatives and feminists have also been criticized for supporting the overstatements for their own purposes.[32]\\r\\nCraig Webber states that this extensive coverage has created or amplified a moral panic[33] rooted in societal anxieties about rape, hedonism and the increased freedoms of women in modern culture. Goode et al say it has given a powerful added incentive for the suppression of party drugs,[31] has inappropriately undermined the long-established argument that recreational drug use is purely a consensual and victimless crime. By shining a spotlight on premeditated criminal behavior, Philip Jenkins states that it has relieved the culture from having to explore and evaluate more nuanced forms of male sexual aggression towards people, such as those displayed in date rapes that were not facilitated by the surreptitious administration of drugs.[34]\\r\\nFor similar moral panics around social tensions manifesting via discussion of drugs and sex crime, researchers point to the opium scare of the late 19th century, in which \\"sinister Chinese\\" were said to use opium to coerce white women into sexual slavery. Similarly, in the Progressive Era, a persistent urban legend told of white middle-class women being surreptitiously drugged, abducted and sold into sexual slavery to Latin American brothels.[35][36] This analysis doesn't contradict instances when date rape drugs are used or sexual trafficking occurs; its focus is on actual prevalence of certain crimes relative to media coverage of it. As such the media coverage does not imply a racially implicated victim, but the racial profiling of perpetrators comes up with Islamic Yemen to influence media coverage in the UK and abroad.[clarify][37]\\r\\nMichael Abramson, a patent attorney who believes he was once drugged at a bar, has invented a cup and plastic straw that detect date rape drugs and signal their presence by changing color.[38]","input":"What is the most common drug to incapacitate someone?"},{"output":"John Cornyn","context":"The Senate Majority and Minority Leaders are two United States Senators and members of the party leadership of the United States Senate. These leaders serve as the chief Senate spokespeople for the political parties respectively holding the majority and the minority in the United States Senate, and manage and schedule the legislative and executive business of the Senate. They are elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference.\\r\\nBy rule, the Presiding Officer gives the Majority Leader priority in obtaining recognition to speak on the floor of the Senate. The Majority Leader customarily serves as the chief representative of their party in the Senate, and sometimes even in all of Congress if the House of Representatives and thus the office of Speaker of the House is controlled by the opposition party.\\r\\nThe Assistant Majority and Minority Leaders of the United States Senate (commonly called Senate Majority and Minority Whips) are the second-ranking members of each party's leadership. The main function of the Majority and Minority Whips is to gather votes on major issues. Because they are the second ranking members of the Senate, if there is no floor leader present, the whip may become acting floor leader. Before 1969, the official titles were Majority Whip and Minority Whip.\\r\\n\\r\\n\\r\\nThe Senate is currently composed of 51 Republicans, 47 Democrats, and 2 independents, both of whom caucus with the Democrats.\\r\\nThe current leaders are long-time Senators Mitch McConnell (R) from Kentucky and Chuck Schumer (D) from New York. The current Assistant Leaders/Whips are long-time Senators John Cornyn (R) from Texas and Dick Durbin (D) from Illinois.\\r\\nThe Democrats began the practice of electing floor leaders in 1920 while they were in the minority. John W. Kern was a Democratic Senator from Indiana. While the title was not official, he is considered[by whom?] to be the first Senate party leader from 1913 through 1917 (and in turn, the first Senate Democratic Leader), while serving concurrently as Chairman of the Senate Democratic Caucus. In 1925 the majority (at the time) Republicans also adopted this language when Charles Curtis became the first (official) Majority Leader[citation needed], although his immediate predecessor Henry Cabot Lodge is considered the first (unofficial) Senate Majority Leader.\\r\\nThe Constitution designates the Vice President of the United States as President of the United States Senate. The Constitution also calls for a President pro tempore to serve as the leader of the body when the President of the Senate (the Vice President) is absent. In practice, neither the Vice President nor the President pro temporecustomarily the most senior (longest-serving) Senator in the majority partyactually presides over the Senate on a daily basis; that task is given to junior Senators of the majority party. Since the Vice President may be of a different party than the majority and is not a member subject to discipline, the rules of procedure of the Senate give the presiding officer very little power and none beyond the presiding role. For these reasons, it is the Majority Leader who, in practice, manages the Senate. This is in contrast to the House of Representatives where the elected Speaker of the House has a great deal of discretionary power and generally presides over votes on bills.[citation needed]\\r\\nThe Democratic Party first selected a leader in 1920. The Republican Party first formally designated a leader in 1925.","input":"Who is the republican whip in the senate?"},{"output":"Candle in the Wind","context":"\\"Candle in the Wind\\" is a threnody with music and lyrics by Elton John and Bernie Taupin. It was originally written in 1973, in honour of Marilyn Monroe,[1] who had died 11 years earlier.\\r\\nIn 1997, John performed a rewritten version of the song as a tribute to Diana, Princess of Wales. This version of the song was released as a single and reached No. 1 in many countries, proving a much greater success than the original, officially being listed as the second best-selling single of all time, behind Bing Crosby's \\"White Christmas\\".\\r\\n\\r\\n\\r\\nThe original version in the key of E major appeared on John's 1973 album Goodbye Yellow Brick Road recorded in May 1973 and released in 1974. The lyrics of the song are a sympathetic portrayal of the life of Marilyn Monroe. (The song's opening line \\"Goodbye, Norma Jean\\" refers to Monroe's real name, Norma Jeane (more commonly spelled Jean) Baker.) In the Eagle Vision documentary on the making of Goodbye Yellow Brick Road, Taupin said the song is about \\"the idea of fame or youth or somebody being cut short in the prime of their life. The song could have been about James Dean, it could have been about Montgomery Clift, it could have been about Jim Morrison ... how we glamorise death, how we immortalise people.\\" The single release of the original song reached No. 11 in the UK charts in 1974. At the time, it was not released as a single in the United States (\\"Bennie and the Jets\\" was chosen instead). Taupin was inspired to write the song after hearing the phrase \\"candle in the wind\\" used in tribute to Janis Joplin.\\r\\nThis version is ranked #347 on Rolling Stone's list of The 500 Greatest Songs of All Time. On the other hand, composer Gruff Rhys called it the worst song he had ever heard.[2]\\r\\nDuring a concert on 7 April 1990, at Farm Aid IV, John dedicated the song to Ryan White, who had been suffering from AIDS. White died from AIDS complications the next day.\\r\\nOn 14 December 1986, a live version of the song was recorded in Sydney, Australia. It was released in 1987 on the album Live in Australia with the Melbourne Symphony Orchestra and as a single. In 1988, it reached number five on the UK Singles Chart and number six on the US Billboard Hot 100.\\r\\nGrammy Awards\\r\\n\\"Candle in the Wind 1997\\" or \\"Goodbye England's Rose\\" is a re-recording of \\"Candle in the Wind\\" as a tribute to Diana, Princess of Wales. Released in 1997, the song peaked at No. 1 in the United Kingdom, becoming John's fourth No. 1 single. It also peaked at No. 1 in several other countries. The Guinness Book of Records in 2007 stated that \\"Candle in the Wind 1997\\" is the biggest-selling single \\"since records began\\", but that Bing Crosby's \\"White Christmas\\" has sold the most copies.[4] The record of this version was produced by George Martin.\\r\\nUsing the same vocal take as the original 1973 recording, engineer Greg Penny stripped away all instrumentation except Davey Johnstone's acoustic guitar. Even the double-tracking of the lead vocal was removed, leaving Elton and the original backing vocal arrangement of Dee Murray, Nigel Olsson and Davey Johnstone. The remix first appeared as a bonus track on the 30th Anniversary edition of Goodbye Yellow Brick Road and subsequently on the 2003 EP Remixed.","input":"What song did elton john wrote for marilyn monroe?"},{"output":"Tontine Coffee House","context":"nyse.com\\r\\nThe New York Stock Exchange (abbreviated as NYSE and nicknamed \\"The Big Board\\"),[6] is an American stock exchange located at 11 Wall Street, Lower Manhattan, New York City, New York. It is by far[7][8] the world's largest stock exchange by market capitalization of its listed companies at US$21.3?trillion as of June 2017.[3] The average daily trading value was approximately US$169?billion in 2013. The NYSE trading floor is located at 11 Wall Street and is composed of 21 rooms used for the facilitation of trading. A fifth trading room, located at 30 Broad Street, was closed in February 2007. The main building and the 11 Wall Street building were designated National Historic Landmarks in 1978.\\r\\nThe NYSE is owned by Intercontinental Exchange, an American holding company that it also lists (NYSE:?ICE). Previously, it was part of NYSE Euronext (NYX), which was formed by the NYSE's 2007 merger with Euronext.[9]\\r\\nThe NYSE has been the subject of several lawsuits regarding fraud or breach of duty[10][11] and in 2004 was sued by its former CEO for breach of contract and defamation.[12]\\r\\n\\r\\n\\r\\nThe earliest recorded organization of securities trading in New York among brokers directly dealing with each other can be traced to the Buttonwood Agreement. Previously securities exchange had been intermediated by the auctioneers who also conducted more mundane auctions of commodities such as wheat and tobacco.[13] On May 17, 1792 twenty four brokers signed the Buttonwood Agreement which set a floor commission rate charged to clients and bound the signers to give preference to the other signers in securities sales. The earliest securities traded were mostly governmental securities such as War Bonds from the Revolutionary War and First Bank of the United States stock,[13] although Bank of New York stock was a non-governmental security traded in the early days.[14] The Bank of North America along with the First Bank of the United States and the Bank of New York were the first shares traded on the New York Stock Exchange.[15]\\r\\nIn 1817 the stockbrokers of New York operating under the Buttonwood Agreement instituted new reforms and reorganized. After sending a delegation to Philadelphia to observe the organization of their board of brokers, restrictions on manipulative trading were adopted as well as formal organs of governance.[13] After re-forming as the New York Stock and Exchange Board the broker organization began renting out space exclusively for securities trading, which previously had been taking place at the Tontine Coffee House. Several locations were used between 1817 and 1865, when the present location was adopted.[13]\\r\\nThe invention of the electrical telegraph consolidated markets, and New York's market rose to dominance over Philadelphia after weathering some market panics better than other alternatives.[13] The Open Board of Stock Brokers was established in 1864 as a competitor to the NYSE. With 354 members, the Open Board of Stock Brokers rivaled the NYSE in membership (which had 533) \\"because it used a more modern, continuous trading system superior to the NYSEs twice-daily call sessions.\\" The Open Board of Stock Brokers merged with the NYSE in 1869. Robert Wright of Bloomberg writes that the merger increased the NYSE's members as well as trading volume, as \\"several dozen regional exchanges were also competing with the NYSE for customers. Buyers, sellers and dealers all wanted to complete transactions as quickly and cheaply as technologically possible and that meant finding the markets with the most trading, or the greatest liquidity in todays parlance. Minimizing competition was essential to keep a large number of orders flowing, and the merger helped the NYSE to maintain its reputation for providing superior liquidity.\\"[16] The Civil War greatly stimulated speculative securities trading in New York. By 1869 membership had to be capped, and has been sporadically increased since. The latter half of the nineteenth century saw rapid growth in securities trading.[17]\\r\\nSecurities trade in the latter nineteenth and early twentieth centuries was prone to panics and crashes. Government regulation of securities trading was eventually seen as necessary, with arguably the most dramatic changes occurring in the 1930s after a major stock market crash precipitated an economic depression.\\r\\nThe Stock Exchange Luncheon Club was situated on the seventh floor from 1898 until its closure in 2006.[18]\\r\\nThe main building, located at 18 Broad Street, between the corners of Wall Street and Exchange Place, was designated a National Historic Landmark in 1978,[19] as was the 11 Wall Street building.[5][20][21]\\r\\nThe NYSE announced its plans to merge with Archipelago on April 21, 2005, in a deal intended to reorganize the NYSE as a publicly traded company. NYSE's governing board voted to merge with rival Archipelago on December 6, 2005, and became a for-profit, public company. It began trading under the name NYSE Group on March 8, 2006. A little over one year later, on April 4, 2007, the NYSE Group completed its merger with Euronext, the European combined stock market, thus forming NYSE Euronext, the first transatlantic stock exchange.\\r\\nWall Street is the leading US money center for international financial activities and the foremost US location for the conduct of wholesale financial services. \\"It comprises a matrix of wholesale financial sectors, financial markets, financial institutions, and financial industry firms\\" (Robert, 2002). The principal sectors are securities industry, commercial banking, asset management, and insurance.\\r\\nPrior to the acquisition of NYSE Euronext by the ICE in 2013, Marsh Carter was the Chairman of the NYSE and the CEO was Duncan Niederauer. Presently,[when?] the chairman is Jeffrey Sprecher.[22] In 2016, NYSE owner Intercontinental Exchange Inc. earned $419 million in listings-related revenues.[23]\\r\\nThe exchange was closed shortly after the beginning of World War I (July 31, 1914), but it partially re-opened on November 28 of that year in order to help the war effort by trading bonds,[24] and completely reopened for stock trading in mid-December.\\r\\nOn September 16, 1920, a bomb exploded on Wall Street outside the NYSE building, killing 33 people and injuring more than 400. The perpetrators were never found. The NYSE building and some buildings nearby, such as the JP Morgan building, still have marks on their fa?ades caused by the bombing.\\r\\nThe Black Thursday crash of the Exchange on October 24, 1929, and the sell-off panic which started on Black Tuesday, October 29, are often blamed for precipitating the Great Depression. In an effort to try to restore investor confidence, the Exchange unveiled a fifteen-point program aimed to upgrade protection for the investing public on October 31, 1938.\\r\\nOn October 1, 1934, the exchange was registered as a national securities exchange with the U.S. Securities and Exchange Commission, with a president and a thirty-three-member board. On February 18, 1971, the non-profit corporation was formed, and the number of board members was reduced to twenty-five.\\r\\nOne of Abbie Hoffman's well-known publicity stunts took place in 1967, when he led members of the Yippie movement to the Exchange's gallery. The provocateurs hurled fistfuls of dollars toward the trading floor below. Some traders booed, and some laughed and waved. Three months later the stock exchange enclosed the gallery with bulletproof glass.[25] Hoffman wrote a decade later, \\"We didn't call the press; at that time we really had no notion of anything called a media event.\\"[26]\\r\\nOn October 19, 1987, the Dow Jones Industrial Average (DJIA) dropped 508 points, a 22.6% loss in a single day, the second-biggest one-day drop the exchange had experienced. Black Monday was followed by Terrible Tuesday, a day in which the Exchange's systems did not perform well and some people had difficulty completing their trades.\\r\\nSubsequently, there was another major drop for the Dow on October 13, 1989the Mini-Crash of 1989. The crash was apparently caused by a reaction to a news story of a $6.75?billion leveraged buyout deal for UAL Corporation, the parent company of United Airlines, which broke down. When the UAL deal fell through, it helped trigger the collapse of the junk bond market causing the Dow to fall 190.58 points, or 6.91 percent.\\r\\nSimilarly, there was a panic in the financial world during the year of 1997; the Asian Financial Crisis. Like the fall of many foreign markets, the Dow suffered a 7.18% drop in value (554.26 points) on October 27, 1997, in what later became known as the 1997 Mini-Crash but from which the DJIA recovered quickly. This was the first time that the \\"circuit breaker\\" rule had operated.\\r\\nOn January 26, 2000, an altercation during filming of the music video for \\"Sleep Now in the Fire,\\" which was directed by Michael Moore, caused the doors of the exchange to be closed and the band Rage Against the Machine to be escorted from the site by security[27] after band members attempted to gain entry into the exchange.\\r\\nIn the aftermath of the September 11 attacks, the NYSE was closed for 4 trading sessions, resuming on Monday, September 17, one of the rare times the NYSE was closed for more than one session and only the third time since March 1933. On the first day, the NYSE suffered a 7.1% drop in value (684 points), after a week, it dropped by 14% (1370 points). An estimated of $1.4 trillion was lost within five days of trading.[28] The NY Stock Exchange is only 5 blocks from Ground Zero.\\r\\nOn May 6, 2010, the Dow Jones Industrial Average posted its largest intraday percentage drop since the October 19, 1987, crash, with a 998-point loss later being called the 2010 Flash Crash (as the drop occurred in minutes before rebounding). The SEC and CFTC published a report on the event, although it did not come to a conclusion as to the cause. The regulators found no evidence that the fall was caused by erroneous (\\"fat finger\\") orders.[29]\\r\\nOn October 29, 2012, the stock exchange was shut down for 2 days due to Hurricane Sandy.[30] The last time the stock exchange was closed due to weather for a full two days was on March 12 and 13 in 1888.[31]\\r\\nOn May 1, 2014, the stock exchange was fined $4.5?million by the Securities and Exchange Commission to settle charges it violated market rules.[32]\\r\\nOn August 14, 2014, Berkshire Hathaway's A Class shares, the highest priced shares on the NYSE, hit $200,000 a share for the first time.[33]\\r\\nOn July 8, 2015, technical issues affected the stock exchange, halting trading at 11:32?am ET. The NYSE reassured stock traders that the outage was \\"not a result of a cyber breach,\\" and the Department of Homeland Security confirmed that there was \\"no sign of malicious activity.\\"[34] Trading eventually resumed at 3:10?pm ET the same day.\\r\\nThe New York Stock Exchange is closed on New Years Day, Martin Luther King, Jr. Day, Washington's Birthday, Good Friday, Memorial Day, Fourth of July, Labor Day, Thanksgiving and Christmas. When those holidays occur on a weekend, the holiday is observed on the closest weekday. In addition, the Stock Exchange closes early on the day before Independence Day, the day after Thanksgiving, and the day before Christmas.[35]\\r\\nThe New York Stock Exchange (sometimes referred to as \\"the Big Board\\"[36]) provides a means for buyers and sellers to trade shares of stock in companies registered for public trading. The NYSE is open for trading Monday through Friday from 9:30?am ÿ 4:00?pm ET, with the exception of holidays declared by the Exchange in advance.\\r\\nThe NYSE trades in a continuous auction format, where traders can execute stock transactions on behalf of investors. They will gather around the appropriate post where a specialist broker, who is employed by a NYSE member firm (that is, he/she is not an employee of the New York Stock Exchange), acts as an auctioneer in an open outcry auction market environment to bring buyers and sellers together and to manage the actual auction. They do on occasion (approximately 10% of the time) facilitate the trades by committing their own capital and as a matter of course disseminate information to the crowd that helps to bring buyers and sellers together. The auction process moved toward automation in 1995 through the use of wireless hand held computers (HHC). The system enabled traders to receive and execute orders electronically via wireless transmission. On September 25, 1995, NYSE member Michael Einersen, who designed and developed this system, executed 1000 shares of IBM through this HHC ending a 203-year process of paper transactions and ushering in an era of automated trading.\\r\\nAs of January 24, 2007, all NYSE stocks can be traded via its electronic hybrid market (except for a small group of very high-priced stocks). Customers can now send orders for immediate electronic execution, or route orders to the floor for trade in the auction market. In the first three months of 2007, in excess of 82% of all order volume was delivered to the floor electronically.[37] NYSE works with US regulators like the SEC and CFTC to coordinate risk management measures in the electronic trading environment through the implementation of mechanisms like circuit breakers and liquidity replenishment points.[38]\\r\\nUntil 2005, the right to directly trade shares on the exchange was conferred upon owners of the 1366 \\"seats.\\" The term comes from the fact that up until the 1870s NYSE members sat in chairs to trade. In 1868, the number of seats was fixed at 533, and this number was increased several times over the years. In 1953, the number of seats was set at 1,366. These seats were a sought-after commodity as they conferred the ability to directly trade stock on the NYSE, and seat holders were commonly referred to as members of the NYSE. The Barnes family is the only known lineage to have five generations of NYSE members: Winthrop H. Barnes (admitted 1894), Richard W.P. Barnes (admitted 1926), Richard S. Barnes (admitted 1951), Robert H. Barnes (admitted 1972), Derek J. Barnes (admitted 2003). Seat prices varied widely over the years, generally falling during recessions and rising during economic expansions. The most expensive inflation-adjusted seat was sold in 1929 for $625,000, which, today, would be over six million dollars. In recent times, seats have sold for as high as $4?million in the late 1990s and as low as $1?million in 2001. In 2005, seat prices shot up to $3.25?million as the exchange entered into an agreement to merge with Archipelago and became a for-profit, publicly traded company. Seat owners received $500,000 in cash per seat and 77,000 shares of the newly formed corporation. The NYSE now sells one-year licenses to trade directly on the exchange. Licenses for floor trading are available for $40,000 and a license for bond trading is available for as little as $1,000 as of 2010.[39] Neither are resell-able, but may be transferable during a change of ownership of a corporation holding a trading license.\\r\\nFollowing the Black Monday market crash in 1987, NYSE imposed trading curbs to reduce market volatility and massive panic sell-offs. Following the 2011 rule change, at the start of each trading day, the NYSE sets three circuit breaker levels at levels of 7% (Level 1), 13% (Level 2), and 20% (Level 3) of the average closing price of the S&P 500 for the preceding trading day. Level 1 and Level 2 declines result in a 15-minute trading halt unless they occur after 3:25?pm, when no trading halts apply. A Level 3 decline results in trading being suspended for the remainder of the day.[40] (The biggest one-day decline in the S&P 500 since 1987 was the 9.0% drop on October 15, 2008.)\\r\\nIn the mid-1960s, the NYSE Composite Index (NYSE: NYA) was created, with a base value of 50 points equal to the 1965 yearly close.[41] This was done to reflect the value of all stocks trading at the exchange instead of just the 30 stocks included in the Dow Jones Industrial Average. To raise the profile of the composite index, in 2003 the NYSE set its new base value of 5,000 points equal to the 2002 yearly close. Its close at the end of 2013 was 10,400.32.\\r\\nIn October 2008 NYSE Euronext completed acquisition of the American Stock Exchange (AMEX) for $260 million in stock.[56]\\r\\nOn February 15, 2011, NYSE and Deutsche B?rse announced their merger to form a new company, as yet unnamed, wherein Deutsche B?rse shareholders will have 60% ownership of the new entity, and NYSE Euronext shareholders will have 40%.\\r\\nOn February 1, 2012, the European Commission blocked the merger of NYSE with Deutsche B?rse, after commissioner Joaquin Almunia stated that the merger \\"would have led to a near-monopoly in European financial derivatives worldwide.\\"[57] Instead, Deutsche B?rse and NYSE will have to sell either their Eurex derivatives or LIFFE shares in order to not create a monopoly. On February 2, 2012, NYSE Euronext and Deutsche B?rse agreed to scrap the merger.[58]\\r\\nIn April 2011, Intercontinental Exchange (ICE), an American futures exchange, and NASDAQ OMX Group had together made an unsolicited proposal to buy NYSE Euronext for approximately US$11,000,000,000, a deal in which NASDAQ would have taken control of the stock exchanges.[59] NYSE Euronext rejected this offer twice, but it was finally terminated after the United States Department of Justice indicated their intention to block the deal due to antitrust concerns.[59]\\r\\nIn December 2012, ICE had proposed to buy NYSE Euronext in a stock swap with a valuation of $8?billion.[9][59] NYSE Euronext shareholders would receive either $33.12 in cash, or $11.27 in cash and approximately a sixth of a share of ICE. The chairman and CEO of ICE, Jeffrey Sprecher, will retain those positions, but four members of the NYSE board of directors will be added to the ICE board.[9]\\r\\nThe NYSE's opening and closing bells mark the beginning and the end of each trading day. The 'opening bell' is rung at 9:30?am ET to mark the start of the day's trading session. At 4?pm ET the 'closing bell' is rung and trading for the day stops. There are bells located in each of the four main sections of the NYSE that all ring at the same time once a button is pressed.[60] There are three buttons that control the bells, located on the control panel behind the podium which overlooks the trading floor. The main bell, which is rung at the beginning and end of the trading day, is controlled by a green button. The second button, colored orange, activates a single-stroke bell that is used to signal a moment of silence. A third, red button controls a backup bell which is used in case the main bell fails to ring.[61]\\r\\nThe signal to start and stop trading was not always a bell. The original signal was a gavel (which is still in use today along with the bell), but during the late 1800s, the NYSE decided to switch the gavel for a gong to signal the day's beginning and end. After the NYSE changed to its present location at 18 Broad Street in 1903, the gong was switched to the bell format that is currently being used.\\r\\nA common sight today is the highly publicized events in which a celebrity or executive from a corporation stands behind the NYSE podium and pushes the button that signals the bells to ring. Many consider the act of ringing the bells to be quite an honor and a symbol of a lifetime of achievement. Furthermore, due to the amount of coverage that the opening/closing bells receive, many companies coordinate new product launches and other marketing-related events to start on the same day as when the company's representatives ring the bell. This daily tradition wasn't always this highly publicized either. In fact, it was only in 1995 that the NYSE began having special guests ring the bells on a regular basis. Prior to that, ringing the bells was usually the responsibility of the exchange's floor managers.[60]\\r\\nMany of the people who ring the bell are business executives whose companies trade on the exchange. However, there have also been many famous people from outside the world of business that have rung the bell. Athletes such as Joe DiMaggio of the New York Yankees and Olympic swimming champion Michael Phelps, entertainers such as rapper Snoop Dogg, singer and actress Liza Minnelli[62] and members of the band Kiss, and politicians such as Mayor of New York City Rudy Giuliani and President of South Africa Nelson Mandela have all had the honor of ringing the bell. Two United Nations Secretaries General have also rung the bell. On April 27, 2006, Secretary-General Kofi Annan rang the opening bell to launch the United Nations Principles for Responsible Investment.[63] On July 24, 2013, Secretary-General Ban Ki-moon rang the closing bell to celebrate the NYSE joining the United Nations Sustainable Stock Exchanges Initiative.[64]\\r\\nIn addition there have been many bell-ringers who are famous for heroic deeds, such as members of the New York police and fire departments following the events of 9/11, members of the United States Armed Forces serving overseas, and participants in various charitable organizations.\\r\\nThere have also been several fictional characters that have rung the bell, including Mickey Mouse, the Pink Panther, Mr. Potato Head, the Aflac Duck, and Darth Vader.[65]\\r\\n0ÿ9 - A - B - C - D - E - F - G - H - I - J - K - L - M - N - O - P - Q - R - S - T - U - V - W - X - Y - Z\\r\\nNotes\\r\\nBibliography","input":"Where did traders in new york originally meet?"},{"output":"unknown","context":"Music is found in every known culture, past and present, varying widely between times and places. Since all people of the world, including the most isolated tribal groups, have a form of music, it may be concluded that music is likely to have been present in the ancestral population prior to the dispersal of humans around the world. Consequently, music may have been in existence for at least 50,000 years and the first music may have been invented in Africa and then evolved to become a fundamental constituent of human life.[1][2]\\r\\n\\r\\nA culture's music is influenced by all other aspects of that culture, including social and economic organization and experience, climate, and access to technology.  The emotions and ideas that music expresses, the situations in which music is played and listened to, and the attitudes toward music players and composers all vary between regions and periods. \\"Music history\\" is the distinct subfield of musicology and history which studies music (particularly Western art music) from a chronological perspective.\\r\\n\\r\\nPrehistoric music, once more commonly called primitive music, is the name given to all music produced in preliterate cultures (prehistory), beginning somewhere in very late geological history. Prehistoric music is followed by ancient music in most of Europe (1500 BC) and later music in subsequent European-influenced areas, but still exists in isolated areas.\\r\\n\\r\\nPrehistoric music thus technically includes all of the world's music that has existed before the advent of any currently extant historical sources concerning that music, for example, traditional Native American music of preliterate tribes and Australian Aboriginal music. However, it is more common to refer to the \\"prehistoric\\" music of non-European continents ÿ especially that which still survives ÿ as folk, indigenous or traditional music. The origin of music is unknown as it occurred prior to recorded history. Some suggest that the origin of music likely stems from naturally occurring sounds and rhythms. Human music may echo these phenomena using patterns, repetition and tonality. Even today, some cultures have certain instances of their music intending to imitate natural sounds. In some instances, this feature is related to shamanistic beliefs or practice.[3][4] It may also serve entertainment (game)[5][6] or practical (luring animals in hunt)[5] functions.\\r\\n\\r\\nIt is probable that the first musical instrument was the human voice itself, which can make a vast array of sounds, from singing, humming and whistling through to clicking, coughing and yawning. As for other musical instruments, in 2008 archaeologists discovered a bone flute in the Hohle Fels cave near Ulm, Germany.[7][8][9] Considered to be about 35,000 years old, the five-holed flute has a V-shaped mouthpiece and is made from a vulture wing bone.  The oldest known wooden pipes were discovered near Greystones, Ireland, in 2004. A wood-lined pit contained a group of six flutes made from yew wood, between 30 and 50?cm long, tapered at one end, but without any finger holes. They may once have been strapped together.[10]\\r\\n\\r\\nIt has been suggested that the \\"Divje Babe Flute\\", a cave bear femur dated to be approximately 43,500 years old, is the world's oldest musical instrument and was produced by Neanderthals.[citation needed] Claims that the femur is indeed a musical instrument are, however, contested by alternative theories including the suggestion that the femur may have been gnawed by carnivores to produce holes.[citation needed]\\r\\n\\r\\nThe prehistoric age is considered to have ended with the development of writing, and with it, by definition, prehistoric music.  \\"Ancient music\\" is the name given to the music that followed. The \\"oldest known song\\" was written in cuneiform, dating to 3400 years ago from Ugarit. It was deciphered by Anne Draffkorn Kilmer, and was demonstrated to be composed in harmonies of thirds, like ancient gymel,[11] and also was written using a Pythagorean tuning of the diatonic scale. The oldest surviving example of a complete musical composition, including musical notation, from anywhere in the world, is the Seikilos epitaph.\\r\\n\\r\\nDouble pipes, such as those used by the ancient Greeks, and ancient bagpipes, as well as a review of ancient drawings on vases and walls, etc., and ancient writings (such as in Aristotle, Problems, Book XIX.12) which described musical techniques of the time, indicate polyphony. One pipe in the aulos pairs (double flutes) likely served as a drone or \\"keynote,\\" while the other played melodic passages. Instruments, such as the seven holed flute and various types of stringed instruments have been recovered from the Indus valley civilization archaeological sites.[12]\\r\\n\\r\\nIndian classical music (marga) can be found from the scriptures of the Hindu tradition, the Vedas. Samaveda, one of the four vedas, describes music at length.\\r\\n\\r\\nRavanahatha (ravanhatta, rawanhattha, ravanastron or ravana hasta veena) is a bowed fiddle popular in Western India. It is believed to have originated among the Hela civilization of Sri Lanka in the time of King Ravana. This string instrument has been recognised as one of the oldest string instruments in world history.\\r\\n\\r\\nThe history of musical development in Iran (Persian music) dates back to the prehistoric era. The great legendary king, Jamshid, is credited with the invention of music. Music in Iran can be traced back to the days of the Elamite Empire (2500-644 BC). Fragmentary documents from various periods of the country's history establish that the ancient Persians possessed an elaborate musical culture. The Sassanid period (AD 226-651), in particular, has left us ample evidence pointing to the existence of a lively musical life in Persia. The names of some important musicians such as Barbod, Nakissa and Ramtin, and titles of some of their works have survived.\\r\\n\\r\\nThe Early music era may also include contemporary but traditional or folk music, including Asian music, Persian music, music of India, Jewish music, Greek music, Roman music, the music of Mesopotamia, the music of Egypt, and Muslim music. \\r\\n\\r\\nGreece\\r\\nGreek written history extends far back into Ancient Greece, and was a major part of ancient Greek theatre. In ancient Greece, mixed-gender choruses performed for entertainment, celebration and spiritual reasons.  Instruments included the double-reed aulos and the plucked string instrument, the lyre, especially the special kind called a kithara. Music was an important part of education in ancient Greece, and boys were taught music starting at age six.\\r\\n\\r\\nAccording to Easton's Bible Dictionary, Jubal was named by the Bible as the inventor of musical instruments (Gen. 4:21). The Hebrews were much given to the cultivation of music. Their whole history and literature afford abundant evidence of this. After the Deluge, the first mention of music is in the account of Laban's interview with Jacob (Gen. 31:27). After their triumphal passage of the Red Sea, Moses and the children of Israel sang their song of deliverance (Ex. 15). But the period of Samuel, David, and Solomon was the golden age of Hebrew music, as it was of Hebrew poetry. Music was now for the first time systematically cultivated. It was an essential part of training in the schools of the prophets (1 Sam. 10:5). There now arose also a class of professional singers (2 Sam. 19:35; Eccl. 2:8). Solomon's Temple, however, was the great school of music. In the conducting of its services large bands of trained singers and players on instruments were constantly employed (2 Sam. 6:5; 1 Chr. 15:16; 23;5; 25:1-6). In private life also music seems to have held an important place among the Hebrews (Eccl. 2:8; Amos 6:4-6; Isa. 5:11, 12; 24:8, 9; Ps. 137; Jer. 48:33; Luke 15:25).[13]\\r\\n\\r\\nMusic and theatre scholars studying the history and anthropology of Semitic and early Judeo-Christian culture, have also discovered common links between theatrical and musical activity in the classical cultures of the Hebrews with those of the later cultures of the Greeks and Romans. The common area of performance is found in a \\"social phenomenon called litany,\\" a form of prayer consisting of a series of invocations or supplications. The Journal of Religion and Theatre notes that among the earliest forms of litany, \\"Hebrew litany was accompanied by a rich musical tradition:\\"[14]\\r\\n\\r\\nEarly music is music of the European classical tradition from after the fall of the Roman Empire, in 476 AD, until the end of the Baroque era in the middle of the 18th century.[citation needed] Music within this enormous span of time was extremely diverse, encompassing multiple cultural traditions within a wide geographic area; many of the cultural groups out of which medieval Europe developed already had musical traditions, about which little is known.  What unified these cultures in the Middle Ages was the Roman Catholic Church, and its music served as the focal point for musical development for the first thousand years of this period.\\r\\n\\r\\nWhile musical life was undoubtedly rich in the early Medieval era, as attested by artistic depictions of instruments, writings about music, and other records, the only repertory of music which has survived from before 800 to the present day is the plainsong liturgical music of the Roman Catholic Church, the largest part of which is called Gregorian chant. Pope Gregory I, who gave his name to the musical repertory and may himself have been a composer, is usually claimed to be the originator of the musical portion of the liturgy in its present form, though the sources giving details on his contribution date from more than a hundred years after his death.  Many scholars believe that his reputation has been exaggerated by legend.  Most of the chant repertory was composed anonymously in the centuries between the time of Gregory and Charlemagne.\\r\\n\\r\\nDuring the 9th century several important developments took place.  First, there was a major effort by the Church to unify the many chant traditions, and suppress many of them in favor of the Gregorian liturgy.  Second, the earliest polyphonic music was sung, a form of parallel singing known as organum.  Third, and of greatest significance for music history, notation was reinvented after a lapse of about five hundred years, though it would be several more centuries before a system of pitch and rhythm notation evolved having the precision and flexibility that modern musicians take for granted.\\r\\n\\r\\nSeveral schools of polyphony flourished in the period after 1100:  the St. Martial school of organum, the music of which was often characterized by a swiftly moving part over a single sustained line; the Notre Dame school of polyphony, which included the composers Lonin and Protin, and which produced the first music for more than two parts around 1200; the musical melting-pot of Santiago de Compostela in Galicia, a pilgrimage destination and site where musicians from many traditions came together in the late Middle Ages, the music of whom survives in the Codex Calixtinus; and the English school, the music of which survives in the Worcester Fragments and the Old Hall Manuscript.  Alongside these schools of sacred music a vibrant tradition of secular song developed, as exemplified in the music of the troubadours, trouvres and Minnes?nger.  Much of the later secular music of the early Renaissance evolved from the forms, ideas, and the musical aesthetic of the troubadours, courtly poets and itinerant musicians, whose culture was largely exterminated during the Albigensian Crusade in the early 13th century.\\r\\n\\r\\nForms of sacred music which developed during the late 13th century included the motet, conductus, discant, and clausulae.  One unusual development was the Geisslerlieder, the music of wandering bands of flagellants during two periods: the middle of the 13th century (until they were suppressed by the Church); and the period during and immediately following the Black Death, around 1350, when their activities were vividly recorded and well-documented with notated music.  Their music mixed folk song styles with penitential or apocalyptic texts. The 14th century in European music history is dominated by the style of the ars nova, which by convention is grouped with the medieval era in music, even though it had much in common with early Renaissance ideals and aesthetics.  Much of the surviving music of the time is secular, and tends to use the formes fixes:  the ballade, the virelai, the lai, the rondeau, which correspond to poetic forms of the same names.  Most pieces in these forms are for one to three voices, likely with instrumental accompaniment:  famous composers include Guillaume de Machaut and Francesco Landini.\\r\\n\\r\\nThe beginning of the Renaissance in music is not as clearly marked as the beginning of the Renaissance in the other arts, and unlike in the other arts, it did not begin in Italy, but in northern Europe, specifically in the area currently comprising central and northern France, the Netherlands, and Belgium.  The style of the Burgundian composers, as the first generation of the Franco-Flemish school is known, was at first a reaction against the excessive complexity and mannered style of the late 14th century  ars subtilior, and contained clear, singable melody and balanced polyphony in all voices.  The most famous composers of the Burgundian school in the mid-15th century are Guillaume Dufay, Gilles Binchois, and Antoine Busnois.\\r\\n\\r\\nBy the middle of the 15th century, composers and singers from the Low Countries and adjacent areas began to spread across Europe, especially into Italy, where they were employed by the papal chapel and the aristocratic patrons of the arts (such as the Medici, the Este, and the Sforza families).  They carried their style with them: smooth polyphony which could be adapted for sacred or secular use as appropriate.  Principal forms of sacred musical composition at the time were the mass, the motet, and the laude; secular forms included the chanson, the frottola, and later the madrigal.\\r\\n\\r\\nThe invention of printing had an immense influence on the dissemination of musical styles, and along with the movement of the Franco-Flemish musicians, contributed to the establishment of the first truly international style in European music since the unification of Gregorian chant under Charlemagne.[citation needed] Composers of the middle generation of the Franco-Flemish school included Johannes Ockeghem, who wrote music in a contrapuntally complex style, with varied texture and an elaborate use of canonical devices; Jacob Obrecht, one of the most famous composers of masses in the last decades of the 15th century; and Josquin des Prez, probably the most famous composer in Europe before Palestrina, and who during the 16th century was renowned as one of the greatest artists in any form. Music in the generation after Josquin explored increasing complexity of counterpoint; possibly the most extreme expression is in the music of Nicolas Gombert, whose contrapuntal complexities influenced early instrumental music, such as the canzona and the ricercar, ultimately culminating in Baroque fugal forms.\\r\\n\\r\\nBy the middle of the 16th century, the international style began to break down, and several highly diverse stylistic trends became evident:  a trend towards simplicity in sacred music, as directed by the Counter-Reformation Council of Trent, exemplified in the music of Giovanni Pierluigi da Palestrina; a trend towards complexity and chromaticism in the madrigal, which reached its extreme expression in the avant-garde style of the Ferrara School of Luzzaschi and the late century madrigalist Carlo Gesualdo; and the grandiose, sonorous music of the Venetian school, which used the architecture of the Basilica San Marco di Venezia to create antiphonal contrasts.  The music of the Venetian school included the development of orchestration, ornamented instrumental parts, and continuo bass parts, all of which occurred within a span of several decades around 1600.  Famous composers in Venice included the Gabrielis, Andrea and Giovanni, as well as Claudio Monteverdi, one of the most significant innovators at the end of the era.\\r\\n\\r\\nMost parts of Europe had active and well-differentiated musical traditions by late in the century.  In England, composers such as Thomas Tallis and William Byrd wrote sacred music in a style similar to that written on the continent, while an active group of home-grown madrigalists adapted the Italian form for English tastes:  famous composers included Thomas Morley, John Wilbye and Thomas Weelkes. Spain developed instrumental and vocal styles of its own, with Toms Luis de Victoria writing refined music similar to that of Palestrina, and numerous other composers writing for the new guitar.  Germany cultivated polyphonic forms built on the Protestant chorales, which replaced the Roman Catholic Gregorian Chant as a basis for sacred music, and imported the style of the Venetian school (the appearance of which defined the start of the Baroque era there).  In addition, German composers wrote enormous amounts of organ music, establishing the basis for the later Baroque organ style which culminated in the work of J.S. Bach.  France developed a unique style of musical diction known as musique mesure, used in secular chansons, with composers such as Guillaume Costeley and Claude Le Jeune prominent in the movement.\\r\\n\\r\\nOne of the most revolutionary movements in the era took place in Florence in the 1570s and 1580s, with the work of the Florentine Camerata, who ironically had a reactionary intent:  dissatisfied with what they saw as contemporary musical depravities, their goal was to restore the music of the ancient Greeks.  Chief among them were Vincenzo Galilei, the father of the astronomer, and Giulio Caccini.  The fruits of their labors was a declamatory melodic singing style known as monody, and a corresponding staged dramatic form:  a form known today as opera.  The first operas, written around 1600, also define the end of the Renaissance and the beginning of the Baroque eras.\\r\\n\\r\\nMusic prior to 1600 was modal rather than tonal.  Several theoretical developments late in the 16th century, such as the writings on scales on modes by Gioseffo Zarlino and Franchinus Gaffurius, led directly to the development of common practice tonality.  The major and minor scales began to predominate over the old church modes, a feature which was at first most obvious at cadential points in compositions, but gradually became pervasive.  Music after 1600, beginning with the tonal music of the Baroque era, is often referred to as belonging to the common practice period.\\r\\n\\r\\nThe Baroque era took place from 1600 to 1750, as the Baroque artistic style flourished across Europe and, during this time, music expanded in its range and complexity. Baroque music began when the first operas (dramatic solo vocal music accompanied by orchestra) were written. During the Baroque era, polyphonic contrapuntal music, in which multiple, simultaneous independent melody lines were used, remained important (counterpoint was important in the vocal music of the Medieval era).[clarification needed] German, Italian, French, Dutch, Polish, Spanish, Portuguese, and English Baroque composers wrote for small ensembles including strings, brass, and woodwinds, as well as for choirs and keyboard instruments such as pipe organ, harpsichord, and clavichord. During this period several major music forms were defined that lasted into later periods when they were expanded and evolved further, including the fugue, the invention, the sonata, and the concerto.[15] The late Baroque style was polyphonically complex and richly ornamented. Important composers from the Baroque era include Johann Sebastian Bach, Arcangelo Corelli, Fran?ois Couperin, Girolamo Frescobaldi, George Frideric Handel, Jean-Baptiste Lully, Claudio Monteverdi, Georg Philipp Telemann and Antonio Vivaldi.\\r\\n\\r\\nThe music of the Classical period is characterized by homophonic texture, or an obvious melody with accompaniment.  These new melodies tended to be almost voice-like and singable, allowing composers to actually replace singers as the focus of the music.  Instrumental music therefore quickly replaced opera and other sung forms (such as oratorio) as the favorite of the musical audience and the epitome of great composition.  However, opera did not disappear: during the classical period, several composers began producing operas for the general public in their native languages (previous operas were generally in Italian).\\r\\n\\r\\nAlong with the gradual displacement of the voice in favor of stronger, clearer melodies, counterpoint also typically became a decorative flourish, often used near the end of a work or for a single movement.  In its stead, simple patterns, such as arpeggios and, in piano music, Alberti bass (an accompaniment with a repeated pattern typically in the left hand), were used to liven the movement of the piece without creating a confusing additional voice.  The now-popular instrumental music was dominated by several well-defined forms: the sonata, the symphony, and the concerto, though none of these were specifically defined or taught at the time as they are now in music theory.  All three derive from sonata form, which is both the overlying form of an entire work and the structure of a single movement. Sonata form matured during the Classical era to become the primary form of instrumental compositions throughout the 19th century.\\r\\n\\r\\nThe early Classical period was ushered in by the Mannheim School, which included such composers as Johann Stamitz, Franz Xaver Richter, Carl Stamitz, and Christian Cannabich. It exerted a profound influence on Joseph Haydn and, through him, on all subsequent European music. Wolfgang Amadeus Mozart was the central figure of the Classical period, and his phenomenal and varied output in all genres defines our perception of the period. Ludwig van Beethoven and Franz Schubert were transitional composers, leading into the Romantic period, with their expansion of existing genres, forms, and even functions of music.\\r\\n\\r\\nIn the Romantic period, music became more expressive and emotional, expanding to encompass literature, art, and philosophy. Famous early Romantic composers include Schumann, Chopin, Mendelssohn, Bellini, and Berlioz. The late 19th century saw a dramatic expansion in the size of the orchestra, and in the role of concerts as part of urban society. Famous composers from the second half of the century include Johann Strauss II, Brahms, Liszt, Tchaikovsky, Verdi, and Wagner. Between 1890 and 1910, a third wave of composers including Dvo?k, Mahler, Richard Strauss, Puccini, and Sibelius built on the work of middle Romantic composers to create even more complex?ÿ and often much longer?ÿ musical works. A prominent mark of late 19th century music is its nationalistic fervor, as exemplified by such figures as Dvo?k, Sibelius, and Grieg. Other prominent late-century figures include Saint-Sa?ns, Faur, Rachmaninoff and Franck.\\r\\n\\r\\nThe 20th century saw a revolution in music listening as the radio gained popularity worldwide and new media and technologies were developed to record, capture, reproduce and distribute music. Music performances became increasingly visual with the broadcast and recording of music videos and concerts.[citation needed] Music of all kinds also became increasingly portable. Headphones allowed people sitting next to each other to listen to entirely different performances or share the same performance.[citation needed]\\r\\n\\r\\n20th-century music brought a new freedom and wide experimentation with new musical styles and forms that challenged the accepted rules of music of earlier periods.[citation needed] The invention of musical amplification and electronic instruments, especially the synthesizer, in the mid-20th century revolutionized popular music and accelerated the development of new forms of music.[citation needed]\\r\\n\\r\\nAs for classical music, two fundamental schools determined the course of the century: that of Arnold Schoenberg and that of Igor Stravinsky.[citation needed]\\r\\n\\r\\nClassical music is a broad, imprecise category, including music produced in, or rooted in the traditions of art, ecclesiastical and concert music. A music is classical if it includes some of the following features: a learned tradition, support from the church or government, or greater cultural capital. Classical music is also described as complex, lasting, transcendent, and abstract.[citation needed] In many cultures a classical tradition coexisted with traditional or popular music, occasionally for thousands of years, and with different levels of mutual borrowing with the parallel tradition.\\r\\n\\r\\nSub-Saharan African music is by a strong rhythmic interest that exhibits common characteristics in all regions of this vast territory, so that Arthur Morris Jones (1889ÿ1980) has described the many local approaches as constituting one main system. \\r\\nC. K.  also affirms the profound homogeneity of approach. West African rhythmic techniques carried over the Atlantic were fundamental ingredients in various musical styles of the Americas: samba, forr܇, maracatu and coco in Brazil, Afro-Cuban music and Afro-American musical genres such as blues, jazz, rhythm & blues, funk, soul, reggae, hip hop and rock and roll were thereby of immense importance in 20th-century popular music.[clarification needed]\\r\\n\\r\\nByzantine music (Greek: ϫ? 翼?) is the music of the Byzantine Empire composed to Greek texts as ceremonial, festival, or church music. Greek and foreign historians agree that the ecclesiastical tones and in general the whole system of Byzantine music is closely related to the ancient Greek system. It remains the oldest genre of extant music, of which the manner of performance and (with increasing accuracy from the 5th century onwards) the names of the composers, and sometimes the particulars of each musical work's circumstances, are known.\\r\\n\\r\\nAsian music covers the music cultures of Arabia, Central Asia, East Asia, South Asia, and Southeast Asia.\\r\\n\\r\\nIndian music is one of the oldest musical traditions in the world.[16] The Indus Valley civilization left sculptures which show dance[17] and musical instruments (some no longer in use), like the seven holed flute. Various types of stringed instruments and drums have been recovered from Harrappa and Mohenjo Daro by excavations carried out by Sir Mortimer Wheeler.[18] The Rigveda has elements of present Indian music, with a musical notation to denote the metre and the mode of chanting.[19] Early Indian musical tradition also speaks of three accents and vocal music known as \\"Samagan\\" (Sama meaning melody and Gan meaning to sing).[20] The classical music of India includes two major traditions: the southern Carnatic music and the northern Hindustani classical music. India's classical music tradition is millennia long and remains important to the lives of Indians today as a source of religious inspiration, cultural expression, and entertainment.\\r\\n\\r\\nIndian classical music (marga) is monophonic, and based on a single melody line or raga rhythmically organized through talas. Carnatic music is largely devotional; the majority of the songs are addressed to the Hindu deities. There are a lot of songs emphasising love and other social issues. In contrast to Carnatic music, Hindustani music was not only influenced by ancient Hindu musical traditions, Vedic philosophy and native Indian sounds but also by the Persian performance practices of the Afghan Mughals. The origins of Indian classical music can be found from the oldest of scriptures, part of the Hindu tradition, the Vedas. Samaveda, one of the four vedas describes music at length.\\r\\n\\r\\nChinese classical music is the traditional art or court music of China. It has a long history stretching for more than three thousand years. It has its own unique systems of musical notation, as well as musical tuning and pitch, musical instruments and styles or musical genres.  Chinese music is pentatonic-diatonic, having a scale of twelve notes to an octave (5+7 = 12) as does European-influenced music.[citation needed]\\r\\n\\r\\nPersian music is the music of Persia and Persian language countries: musiqi, the science and art of music, and muzik, the sound and performance of music (Sakata 1983). See: Music of Iran, Music of Afghanistan, Music of Tajikistan, Music of Uzbekistan.\\r\\n\\r\\nTo the right are some music samples.","input":"When was the first piece of music made?"},{"output":"Geoffrey Chaucer","context":"The Canterbury Tales (Middle English: Tales of Caunterbury[2]) is a collection of 24 stories that runs to over 17,000 lines written in Middle English by Geoffrey Chaucer between 1387ÿ1400.[3] In 1386, Chaucer became Controller of Customs and Justice of Peace and, three years later, Clerk of the King's work in 1389.[4] It was during these years that Chaucer began working on his most famous text, The Canterbury Tales. The tales (mostly written in verse, although some are in prose) are presented as part of a story-telling contest by a group of pilgrims as they travel together on a journey from London to Canterbury to visit the shrine of Saint Thomas Becket at Canterbury Cathedral. The prize for this contest is a free meal at the Tabard Inn at Southwark on their return.\\r\\nAfter a long list of works written earlier in his career, including Troilus and Criseyde, House of Fame, and Parliament of Fowls, The Canterbury Tales is near-unanimously seen as Chaucer's magnum opus. He uses the tales and descriptions of its characters to paint an ironic and critical portrait of English society at the time, and particularly of the Church. Chaucer's use of such a wide range of classes and types of people was without precedent in English. Although the characters are fictional, they still offer a variety of insights into customs and practices of the time. Often, such insight leads to a variety of discussions and disagreements among people in the 14th century. For example, although various social classes are represented in these stories and all of the pilgrims are on a spiritual quest, it is apparent that they are more concerned with worldly things than spiritual. Structurally, the collection resembles The Decameron, which Chaucer may have read during his first diplomatic mission to Italy in 1372.\\r\\nIt has been suggested that the greatest contribution of The Canterbury Tales to English literature was the popularisation of the English vernacular in mainstream literature, as opposed to French, Italian or Latin. English had, however, been used as a literary language centuries before Chaucer's time, and several of Chaucer's contemporariesJohn Gower, William Langland, the Pearl Poet, and Julian of Norwichalso wrote major literary works in English. It is unclear to what extent Chaucer was seminal in this evolution of literary preference.\\r\\nWhile Chaucer clearly states the addressees of many of his poems, the intended audience of The Canterbury Tales is more difficult to determine. Chaucer was a courtier, leading some to believe that he was mainly a court poet who wrote exclusively for nobility.\\r\\nThe Canterbury Tales is generally thought to have been incomplete at the end of Chaucer's life. In the General Prologue,[5] some thirty pilgrims are introduced. According to the Prologue, Chaucer's intention was to write two stories from the perspective of each pilgrim on the way to and from their ultimate destination, St. Thomas Becket's shrine (making for a total of four stories per pilgrim). Although perhaps incomplete, The Canterbury Tales is revered as one of the most important works in English literature. It is also open to a wide range of interpretations.[6]\\r\\n\\r\\n\\r\\nThe question of whether The Canterbury Tales is a finished work has not been answered to date. There are 84 manuscripts and four incunable editions of the work, dating from the late medieval and early Renaissance periods, more than for any other vernacular literary text with the exception of The Prick of Conscience. This is taken as evidence of the Tales' popularity during the century after Chaucer's death.[7] Fifty-five of these manuscripts are thought to have been originally complete, while 28 are so fragmentary that it is difficult to ascertain whether they were copied individually or as part of a set.[8] The Tales vary in both minor and major ways from manuscript to manuscript; many of the minor variations are due to copyists' errors, while it is suggested that in other cases Chaucer both added to his work and revised it as it was being copied and possibly as it was being distributed. Determining the text of the work is complicated by the question of the narrator's voice which Chaucer made part of his literary structure.\\r\\nEven the oldest surviving manuscripts of the Tales are not Chaucer's originals. The very oldest is probably MS Peniarth 392 D (called \\"Hengwrt\\"), written by a scribe shortly after Chaucer's death. The most beautiful, on the other hand, is the Ellesmere Manuscript, a manuscript whose order and many editors have followed even down to the present day.[9][10] The first version of The Canterbury Tales to be published in print was William Caxton's 1476 edition. Only 10 copies of this edition are known to exist, including one held by the British Library and one held by the Folger Shakespeare Library.\\r\\nIn 2004, Linne Mooney claimed that she was able to identify the scrivener who worked for Chaucer as an Adam Pinkhurst. Mooney, then a professor at the University of Maine and a visiting fellow at Corpus Christi College, Cambridge, said she could match Pinkhurst's signature, on an oath he signed, to his handwriting on a copy of The Canterbury Tales that might have been transcribed from Chaucer's working copy.[11]\\r\\nIn the absence of consensus as to whether or not a complete version of the Tales exists, there is also no general agreement regarding the order in which Chaucer intended the stories to be placed.[12][13]\\r\\nTextual and manuscript clues have been adduced to support the two most popular modern methods of ordering the tales. Some scholarly editions divide the Tales into ten \\"Fragments\\". The tales that make up a Fragment are closely related and contain internal indications of their order of presentation, usually with one character speaking to and then stepping aside for another character. However, between Fragments, the connection is less obvious. Consequently, there are several possible orders; the one most frequently seen in modern editions follows the numbering of the Fragments (ultimately based on the Ellesmere order).[12] Victorians frequently used the nine \\"Groups\\", which was the order used by Walter William Skeat whose edition Chaucer: Complete Works was used by Oxford University Press for most of the twentieth century, but this order is now seldom followed.[12]\\r\\nAn alternative ordering (seen in an early manuscript containing The Canterbury Tales, the early-fifteenth century Harley MS. 7334) places Fragment VIII before VI. Fragments I and II almost always follow each other, just as VI and VII, IX and X do in the oldest manuscripts. Fragments IV and V, by contrast, vary in location from manuscript to manuscript.\\r\\nChaucer wrote in late Middle English, which has clear differences from Modern English. From philological research, we know certain facts about the pronunciation of English during the time of Chaucer. Chaucer pronounced -e at the end of words, so that care was [?ka?r?], not /?k??r/ as in Modern English. Other silent letters were also pronounced, so that the word knight was [kni?t], with both the k and the gh pronounced, not /?na?t/. In some cases, vowel letters in Middle English were pronounced very differently from Modern English, because the Great Vowel Shift had not yet happened. For instance, the long e in wepyng \\"weeping\\" was pronounced as [e?], as in modern German or Italian, not as /i?/. Below is an IPA transcription of the opening lines of The Merchant's Prologue:\\r\\nAlthough no manuscript exists in Chaucer's own hand, two were copied around the time of his death by Adam Pinkhurst, a scribe with whom he seems to have worked closely before, giving a high degree of confidence that Chaucer himself wrote the Tales.[16] Because the final -e sound was lost soon after Chaucer's time, scribes did not accurately copy it, and this gave scholars the impression that Chaucer himself was inconsistent in using it.[17] It has now been established, however, that -e was an important part of Chaucer's grammar, and helped to distinguish singular adjectives from plural and subjunctive verbs from indicative.[18]\\r\\nNo other work prior to Chaucer's is known to have set a collection of tales within the framework of pilgrims on a pilgrimage. It is obvious, however, that Chaucer borrowed portions, sometimes very large portions, of his stories from earlier stories, and that his work was influenced by the general state of the literary world in which he lived. Storytelling was the main entertainment in England at the time, and storytelling contests had been around for hundreds of years. In 14th-century England the English Pui was a group with an appointed leader who would judge the songs of the group. The winner received a crown and, as with the winner of The Canterbury Tales, a free dinner. It was common for pilgrims on a pilgrimage to have a chosen \\"master of ceremonies\\" to guide them and organise the journey.[19] Harold Bloom suggests that the structure is mostly original, but inspired by the \\"pilgrim\\" figures of Dante and Virgil in The Divine Comedy.[20] New research suggests that the General Prologue, in which the innkeeper and host Harry Bailey introduces each pilgrim, is a pastiche of the historical Harry Bailey's surviving 1381 poll-tax account of Southwark's inhabitants.[21]\\r\\nThe Decameron by Giovanni Boccaccio contains more parallels to The Canterbury Tales than any other work. Like the Tales, it features a number of narrators who tell stories along a journey they have undertaken (to flee from the Black Death). It ends with an apology by Boccaccio, much like Chaucer's Retraction to the Tales. A quarter of the tales in The Canterbury Tales parallel a tale in the Decameron, although most of them have closer parallels in other stories. Some scholars thus find it unlikely that Chaucer had a copy of the work on hand, surmising instead that he must have merely read the Decameron at some point,[22] while a new study claims he had a copy of the Decameron and used it extensively as he began work on his own collection.[23] Each of the tales has its own set of sources that have been suggested by scholars, but a few sources are used frequently over several tales. They include poetry by Ovid, the Bible in one of the many vulgate versions in which it was available at the time (the exact one is difficult to determine), and the works of Petrarch and Dante. Chaucer was the first author to utilise the work of these last two, both Italians. Boethius' Consolation of Philosophy appears in several tales, as the works of John Gower do. Gower was a known friend to Chaucer. A full list is impossible to outline in little space, but Chaucer also, lastly, seems to have borrowed from numerous religious encyclopaedias and liturgical writings, such as John Bromyard's Summa praedicantium, a preacher's handbook, and Jerome's Adversus Jovinianum.[24] Many scholars say there is a good possibility Chaucer met Petrarch or Boccaccio.[25][26][27][incomplete short citation][28][incomplete short citation][29]\\r\\nThe Canterbury Tales is a collection of stories built around a frame narrative or frame tale, a common and already long established genre of its period. Chaucer's Tales differs from most other story \\"collections\\" in this genre chiefly in its intense variation. Most story collections focused on a theme, usually a religious one. Even in the Decameron, storytellers are encouraged to stick to the theme decided on for the day. The idea of a pilgrimage to get such a diverse collection of people together for literary purposes was also unprecedented, though \\"the association of pilgrims and storytelling was a familiar one\\".[30] Introducing a competition among the tales encourages the reader to compare the tales in all their variety, and allows Chaucer to showcase the breadth of his skill in different genres and literary forms.[31]\\r\\nWhile the structure of the Tales is largely linear, with one story following another, it is also much more than that. In the General Prologue, Chaucer describes not the tales to be told, but the people who will tell them, making it clear that structure will depend on the characters rather than a general theme or moral. This idea is reinforced when the Miller interrupts to tell his tale after the Knight has finished his. Having the Knight go first gives one the idea that all will tell their stories by class, with the Monk following the Knight. However, the Miller's interruption makes it clear that this structure will be abandoned in favour of a free and open exchange of stories among all classes present. General themes and points of view arise as the characters tell their tales, which are responded to by other characters in their own tales, sometimes after a long lapse in which the theme has not been addressed.[32]\\r\\nLastly, Chaucer does not pay much attention to the progress of the trip, to the time passing as the pilgrims travel, or to specific locations along the way to Canterbury. His writing of the story seems focused primarily on the stories being told, and not on the pilgrimage itself.[33]\\r\\nThe variety of Chaucer's tales shows the breadth of his skill and his familiarity with many literary forms, linguistic styles, and rhetorical devices. Medieval schools of rhetoric at the time encouraged such diversity, dividing literature (as Virgil suggests) into high, middle, and low styles as measured by the density of rhetorical forms and vocabulary. Another popular method of division came from St. Augustine, who focused more on audience response and less on subject matter (a Virgilian concern). Augustine divided literature into \\"majestic persuades\\", \\"temperate pleases\\", and \\"subdued teaches\\". Writers were encouraged to write in a way that kept in mind the speaker, subject, audience, purpose, manner, and occasion. Chaucer moves freely between all of these styles, showing favouritism to none.[34] He not only considers the readers of his work as an audience, but the other pilgrims within the story as well, creating a multi-layered rhetorical puzzle of ambiguities. Thus Chaucer's work far surpasses the ability of any single medieval theory to uncover.[35]\\r\\nWith this, Chaucer avoids targeting any specific audience or social class of readers, focusing instead on the characters of the story and writing their tales with a skill proportional to their social status and learning. However, even the lowest characters, such as the Miller, show surprising rhetorical ability, although their subject matter is more lowbrow. Vocabulary also plays an important part, as those of the higher classes refer to a woman as a \\"lady\\", while the lower classes use the word \\"wenche\\", with no exceptions. At times the same word will mean entirely different things between classes. The word \\"pitee\\", for example, is a noble concept to the upper classes, while in the Merchant's Tale it refers to sexual intercourse. Again, however, tales such as the Nun's Priest's Tale show surprising skill with words among the lower classes of the group, while the Knight's Tale is at times extremely simple.[36]\\r\\nChaucer uses the same meter throughout almost all of his tales, with the exception of Sir Thopas and his prose tales. It is a decasyllable line, probably borrowed from French and Italian forms, with riding rhyme and, occasionally, a caesura in the middle of a line. His meter would later develop into the heroic meter of the 15th and 16th centuries and is an ancestor of iambic pentameter. He avoids allowing couplets to become too prominent in the poem, and four of the tales (the Man of Law's, Clerk's, Prioress', and Second Nun's) use rhyme royal.[37]\\r\\nThe Canterbury Tales was written during a turbulent time in English history. The Catholic Church was in the midst of the Western Schism and, though it was still the only Christian authority in Europe, was the subject of heavy controversy. Lollardy, an early English religious movement led by John Wycliffe, is mentioned in the Tales, which also mention a specific incident involving pardoners (sellers of indulgences, which were believed to relieve the temporal punishment due for sins that were already forgiven in the Sacrament of Confession) who nefariously claimed to be collecting for St. Mary Rouncesval hospital in England. The Canterbury Tales is among the first English literary works to mention paper, a relatively new invention that allowed dissemination of the written word never before seen in England. Political clashes, such as the 1381 Peasants' Revolt and clashes ending in the deposing of King Richard II, further reveal the complex turmoil surrounding Chaucer in the time of the Tales' writing. Many of his close friends were executed and he himself moved to Kent to get away from events in London.[38]\\r\\nWhile some readers look to interpret the characters of The Canterbury Tales as historical figures, other readers choose to interpret its significance in less literal terms. After analysis of Chaucer's diction and historical context, his work appears to develop a critique of society during his lifetime. Within a number of his descriptions, his comments can appear complimentary in nature, but through clever language, the statements are ultimately critical of the pilgrim's actions. It is unclear whether Chaucer would intend for the reader to link his characters with actual persons. Instead, it appears that Chaucer creates fictional characters to be general representations of people in such fields of work. With an understanding of medieval society, one can detect subtle satire at work.[39]\\r\\nThe Tales reflect diverse views of the Church in Chaucer's England. After the Black Death, many Europeans began to question the authority of the established Church. Some turned to lollardy, while others chose less extreme paths, starting new monastic orders or smaller movements exposing church corruption in the behaviour of the clergy, false church relics or abuse of indulgences.[40] Several characters in the Tales are religious figures, and the very setting of the pilgrimage to Canterbury is religious (although the prologue comments ironically on its merely seasonal attractions), making religion a significant theme of the work.[41]\\r\\nTwo characters, the Pardoner and the Summoner, whose roles apply the Church's secular power, are both portrayed as deeply corrupt, greedy, and abusive. Pardoners in Chaucer's day were those people from whom one bought Church \\"indulgences\\" for forgiveness of sins, who were guilty of abusing their office for their own gain. Chaucer's Pardoner openly admits the corruption of his practice while hawking his wares.[42] Summoners were Church officers who brought sinners to the Church court for possible excommunication and other penalties. Corrupt summoners would write false citations and frighten people into bribing them to protect their interests. Chaucer's Summoner is portrayed as guilty of the very kinds of sins for which he is threatening to bring others to court, and is hinted as having a corrupt relationship with the Pardoner.[43] In The Friar's Tale, one of the characters is a summoner who is shown to be working on the side of the devil, not God.[44]\\r\\nChurchmen of various kinds are represented by the Monk, the Prioress, the Nun's Priest, and the Second Nun. Monastic orders, which originated from a desire to follow an ascetic lifestyle separated from the world, had by Chaucer's time become increasingly entangled in worldly matters. Monasteries frequently controlled huge tracts of land on which they made significant sums of money, while peasants worked in their employ.[45] The Second Nun is an example of what a Nun was expected to be: her tale is about a woman whose chaste example brings people into the church. The Monk and the Prioress, on the other hand, while not as corrupt as the Summoner or Pardoner, fall far short of the ideal for their orders. Both are expensively dressed, show signs of lives of luxury and flirtatiousness and show a lack of spiritual depth.[46] The Prioress's Tale is an account of Jews murdering a deeply pious and innocent Christian boy, a blood libel against Jews that became a part of English literary tradition.[47] The story did not originate in the works of Chaucer and was well known in the 14th century.[48]\\r\\nPilgrimage was a very prominent feature of medieval society. The ultimate pilgrimage destination was Jerusalem,[49] but within England Canterbury was a popular destination. Pilgrims would journey to cathedrals that preserved relics of saints, believing that such relics held miraculous powers. Saint Thomas Becket, Archbishop of Canterbury, had been murdered in Canterbury Cathedral by knights of Henry II during a disagreement between Church and Crown. Miracle stories connected to his remains sprang up soon after his death, and the cathedral became a popular pilgrimage destination.[50] The pilgrimage in the work ties all of the stories together and may be considered a representation of Christians' striving for heaven, despite weaknesses, disagreement, and diversity of opinion.[51]\\r\\nThe upper class or nobility, represented chiefly by the Knight and his Squire, was in Chaucer's time steeped in a culture of chivalry and courtliness. Nobles were expected to be powerful warriors who could be ruthless on the battlefield yet mannerly in the King's Court and Christian in their actions.[52] Knights were expected to form a strong social bond with the men who fought alongside them, but an even stronger bond with a woman whom they idealised to strengthen their fighting ability.[53] Though the aim of chivalry was to noble action, its conflicting values often degenerated into violence. Church leaders frequently tried to place restrictions on jousts and tournaments, which at times ended in the death of the loser. The Knight's Tale shows how the brotherly love of two fellow knights turns into a deadly feud at the sight of a woman whom both idealise. To win her, both are willing to fight to the death. Chivalry was in Chaucer's day on the decline, and it is possible that The Knight's Tale was intended to show its flaws, although this is disputed.[54] Chaucer himself had fought in the Hundred Years' War under Edward III, who heavily emphasised chivalry during his reign.[55] Two tales, Sir Topas and The Tale of Melibee are told by Chaucer himself, who is travelling with the pilgrims in his own story. Both tales seem to focus on the ill-effects of chivalrythe first making fun of chivalric rules and the second warning against violence.[56]\\r\\nThe Tales constantly reflect the conflict between classes. For example, the division of the three estates: the characters are all divided into three distinct classes, the classes being \\"those who pray\\" (the clergy), \\"those who fight\\" (the nobility), and \\"those who work\\" (the commoners and peasantry).[57] Most of the tales are interlinked by common themes, and some \\"quit\\" (reply to or retaliate against) other tales. Convention is followed when the Knight begins the game with a tale, as he represents the highest social class in the group. But when he is followed by the Miller, who represents a lower class, it sets the stage for the Tales to reflect both a respect for and a disregard for upper class rules. Helen Cooper, as well as Mikhail Bakhtin and Derek Brewer, call this opposition \\"the ordered and the grotesque, Lent and Carnival, officially approved culture and its riotous, and high-spirited underside.\\"[58] Several works of the time contained the same opposition.[58]\\r\\nChaucer's characters each express differentsometimes vastly differentviews of reality, creating an atmosphere of testing, empathy, and relativism.[34] As Helen Cooper says, \\"Different genres give different readings of the world: the fabliau scarcely notices the operations of God, the saint's life focuses on those at the expense of physical reality, tracts and sermons insist on prudential or orthodox morality, romances privilege human emotion.\\" The sheer number of varying persons and stories renders the Tales as a set unable to arrive at any definite truth or reality.[59]\\r\\nThe concept of liminality figures prominently within The Canterbury Tales.[34] A liminal space, which can be both geographical as well as metaphorical or spiritual, is the transitional or transformational space between a real (secure, known, limited) world and an unknown or imaginary space of both risk and possibility.[60] The notion of a pilgrimage is itself a liminal experience, because it centers on travel between destinations and because pilgrims undertake it hoping to become more holy in the process. Thus, the structure of The Canterbury Tales itself is liminal; it not only covers the distance between London and Canterbury, but the majority of the tales refer to places entirely outside the geography of the pilgrimage. Jean Jost summarises the function of liminality in The Canterbury Tales,\\r\\n\\"Both appropriately and ironically in this raucous and subversive liminal space, a ragtag assembly gather together and tell their equally unconventional tales. In this unruly place, the rules of tale telling are established, themselves to be both disordered and broken; here the tales of game and earnest, solas and sentence, will be set and interrupted. Here the sacred and profane adventure begins, but does not end. Here, the condition of peril is as prominent as that of protection. The act of pilgrimaging itself consists of moving from one urban space, through liminal rural space, to the next urban space with an ever fluctuating series of events and narratives punctuating those spaces. The goal of pilgrimage may well be a religious or spiritual space at its conclusion, and reflect a psychological progression of the spirit, in yet another kind of emotional space.\\"[61]\\r\\nLiminality is also evident in the individual tales. An obvious instance of this is the Friars Tale in which the yeoman devil is a liminal figure because of his transitory nature and function; it is his purpose to issue souls from their current existence to hell, an entirely different one.[62] The Franklins Tale is a Breton Lai tale, which takes the tale into a liminal space by invoking not only the interaction of the supernatural and the mortal, but also the relation between the present and the imagined past.[63]\\r\\nIt is sometimes argued that the greatest contribution that this work made to English literature was in popularising the literary use of the vernacular English, rather than French or Latin. English had, however, been used as a literary language for centuries before Chaucer's life, and several of Chaucer's contemporariesJohn Gower, William Langland, and the Pearl Poetalso wrote major literary works in English. It is unclear to what extent Chaucer was responsible for starting a trend rather than simply being part of it.[citation needed] It is interesting to note that, although Chaucer had a powerful influence in poetic and artistic terms, which can be seen in the great number of forgeries and mistaken attributions (such as The Floure and the Leafe, which was translated by John Dryden), modern English spelling and orthography owe much more to the innovations made by the Court of Chancery in the decades during and after his lifetime.[citation needed]\\r\\nWhile Chaucer clearly states the addressees of many of his poems (the Book of the Duchess is believed to have been written for John of Gaunt on the occasion of his wife's death in 1368), the intended audience of The Canterbury Tales is more difficult to determine. Chaucer was a courtier, leading some to believe that he was mainly a court poet who wrote exclusively for the nobility. He is referred to as a noble translator and poet by Eustache Deschamps and by his contemporary John Gower. It has been suggested that the poem was intended to be read aloud, which is probable as this was a common activity at the time. However, it also seems to have been intended for private reading as well, since Chaucer frequently refers to himself as the writer, rather than the speaker, of the work. Determining the intended audience directly from the text is even more difficult, since the audience is part of the story. This makes it difficult to tell when Chaucer is writing to the fictional pilgrim audience or the actual reader.[64]\\r\\nChaucer's works may have been distributed in some form during his lifetime in part or in whole. Scholars speculate that manuscripts were circulated among his friends, but likely remained unknown to most people until after his death. However, the speed with which copyists strove to write complete versions of his tale in manuscript form shows that Chaucer was a famous and respected poet in his own day. The Hengwrt and Ellesmere manuscripts are examples of the care taken to distribute the work. More manuscript copies of the poem exist than for any other poem of its day except The Prick of Conscience, causing some scholars to give it the medieval equivalent of bestseller status. Even the most elegant of the illustrated manuscripts, however, is not nearly as highly decorated as the work of authors of more respectable works such as John Lydgate's religious and historical literature.[65]\\r\\nJohn Lydgate and Thomas Occleve were among the first critics of Chaucer's Tales, praising the poet as the greatest English poet of all time and the first to show what the language was truly capable of poetically. This sentiment was universally agreed upon by later critics into the mid-15th century. Glosses included in The Canterbury Tales manuscripts of the time praised him highly for his skill with \\"sentence\\" and rhetoric, the two pillars by which medieval critics judged poetry. The most respected of the tales was at this time the Knight's, as it was full of both.[66]\\r\\nThe incompleteness of the Tales led several medieval authors to write additions and supplements to the tales to make them more complete. Some of the oldest existing manuscripts of the tales include new or modified tales, showing that even early on, such additions were being created. These emendations included various expansions of the Cook's Tale, which Chaucer never finished, The Plowman's Tale, The Tale of Gamelyn, the Siege of Thebes, and the Tale of Beryn.[67]\\r\\nThe Tale of Beryn, written by an anonymous author in the 15th century, is preceded by a lengthy prologue in which the pilgrims arrive at Canterbury and their activities there are described. While the rest of the pilgrims disperse throughout the town, the Pardoner seeks the affections of Kate the barmaid, but faces problems dealing with the man in her life and the innkeeper Harry Bailey. As the pilgrims turn back home, the Merchant restarts the storytelling with Tale of Beryn. In this tale, a young man named Beryn travels from Rome to Egypt to seek his fortune only to be cheated by other businessmen there. He is then aided by a local man in getting his revenge. The tale comes from the French tale Brinus and exists in a single early manuscript of the tales, although it was printed along with the tales in a 1721 edition by John Urry.[68]\\r\\nJohn Lydgate wrote The Siege of Thebes in about 1420. Like the Tale of Beryn, it is preceded by a prologue in which the pilgrims arrive in Canterbury. Lydgate places himself among the pilgrims as one of them and describes how he was a part of Chaucer's trip and heard the stories. He characterises himself as a monk and tells a long story about the history of Thebes before the events of the Knight's Tale. John Lydgate's tale was popular early on and exists in old manuscripts both on its own and as part of the Tales. It was first printed as early as 1561 by John Stow, and several editions for centuries after followed suit.[69]\\r\\nThere are actually two versions of The Plowman's Tale, both of which are influenced by the story Piers Plowman, a work written during Chaucer's lifetime. Chaucer describes a Plowman in the General Prologue of his tales, but never gives him his own tale. One tale, written by Thomas Occleve, describes the miracle of the Virgin and the Sleeveless Garment. Another tale features a pelican and a griffin debating church corruption, with the pelican taking a position of protest akin to John Wycliffe's ideas.[70]\\r\\nThe Tale of Gamelyn was included in an early manuscript version of the tales, Harley 7334, which is notorious for being one of the lower-quality early manuscripts in terms of editor error and alteration. It is now widely rejected by scholars as an authentic Chaucerian tale, although some scholars think he may have intended to rewrite the story as a tale for the Yeoman. Dates for its authorship vary from 1340 to 1370.[71]\\r\\nMany literary works (both fiction and non-fiction alike) have used a similar frame narrative to The Canterbury Tales as an homage. Science-fiction writer Dan Simmons wrote his Hugo Award winning novel Hyperion based on an extra-planetary group of pilgrims. Evolutionary biologist Richard Dawkins used The Canterbury Tales as a structure for his 2004 non-fiction book about evolution titled The Ancestor's Tale: A Pilgrimage to the Dawn of Evolution. His animal pilgrims are on their way to find the common ancestor, each telling a tale about evolution.\\r\\nHenry Dudeney's book The Canterbury Puzzles contains a part reputedly lost from what modern readers know as Chaucer's tales.\\r\\nHistorical-mystery novelist P.C. Doherty wrote a series of novels based on The Canterbury Tales, making use of both the story frame and Chaucer's characters.\\r\\nCanadian author Angie Abdou translates The Canterbury Tales to a cross section of people, all snow-sports enthusiasts but from different social backgrounds, converging on a remote back-country ski cabin in British Columbia in the 2011 novel The Canterbury Trail.\\r\\nThe Two Noble Kinsmen, by William Shakespeare and John Fletcher, a retelling of \\"The Knight's Tale\\", was first performed in 1613 or 1614 and published in 1634. In 1961, Erik Chisholm completed his opera, The Canterbury Tales. The opera is in three acts: The Wyf of Baths Tale, The Pardoners Tale and The Nuns Priests Tale. Nevill Coghill's modern English version formed the basis of a musical version that was first staged in 1964.\\r\\nA Canterbury Tale, a 1944 film jointly written and directed by Michael Powell and Emeric Pressburger, is loosely based on the narrative frame of Chaucer's tales. The movie opens with a group of medieval pilgrims journeying through the Kentish countryside as a narrator speaks the opening lines of the General Prologue. The scene then makes a now-famous transition to the time of World War II. From that point on, the film follows a group of strangers, each with his or her own story and in need of some kind of redemption, who are making their way to Canterbury together. The film's main story takes place in an imaginary town in Kent and ends with the main characters arriving at Canterbury Cathedral, bells pealing and Chaucer's words again resounding. A Canterbury Tale is recognised as one of the Powell-Pressburger team's most poetic and artful films. It was produced as wartime propaganda, using Chaucer's poetry, referring to the famous pilgrimage, and offering photography of Kent to remind the public of what made Britain worth fighting for. In one scene a local historian lectures an audience of British soldiers about the pilgrims of Chaucer's time and the vibrant history of England.[72]\\r\\nPier Paolo Pasolini's 1972 film The Canterbury Tales features several of the tales, some of which keep close to the original tale and some of which are embellished. The Cook's Tale, for instance, which is incomplete in the original version, is expanded into a full story, and the Friar's Tale extends the scene in which the Summoner is dragged down to hell. The film includes these two tales as well as the Miller's Tale, the Summoner's Tale, the Wife of Bath's Tale, and the Merchant's Tale.[73]\\r\\nOn April 26, 1986, American radio personality Garrison Keillor opened \\"The News from Lake Wobegon\\" portion of the first live TV broadcast of his A Prairie Home Companion radio show with a reading of the original Middle English text of the General Prologue. He commented, \\"Although those words were written more than 600 years ago, they still describe spring.\\"\\r\\nTelevision adaptations include Alan Plater's 1975 re-telling of the stories in a series of plays for BBC2: Trinity Tales. In 2003, BBC again featured modern re-tellings of selected tales.[74]\\r\\nThe Knight\\r\\nThe Squire\\r\\nThe Reeve\\r\\nThe Miller\\r\\nThe Cook\\r\\nThe Wife of Bath\\r\\nThe Franklin\\r\\nThe Shipman\\r\\nThe Manciple\\r\\nThe Merchant\\r\\nThe Clerk of Oxford\\r\\nThe Sergeant of Law\\r\\nThe Physician\\r\\nThe Parson\\r\\nThe Monk\\r\\nThe Prioress\\r\\nThe Second Nun\\r\\nThe Nun's Priest\\r\\nThe Friar\\r\\nThe Summoner\\r\\nThe Pardoner\\r\\nThe Canon Yeoman\\r\\nGeoffrey Chaucer","input":"Who is the author of the canterbury tales?"},{"output":"Anamudi","context":"Anamudi (Malayalam pronunciation:??[a?n?m??i], ??????) is a mountain located in the Indian state of Kerala. It is the highest peak in the Kerala and South India, at an elevation of 2,695 metres (8,842?ft)[1][2] and a topographic prominence of 2,479 metres (8,133?ft).[3] It lies on the border of Devikulam Taluk, Idukki district and Kothamangalam Taluk, Ernakulam district.[5] The name Anamudi literally translates to \\"elephant's forehead,\\" a reference to the resemblance of the mountain to an elephant's head.[6]\\r\\nThe first recorded ascent of Anamudi was by General Douglas Hamilton of the Madras Army on May 4, 1862,[citation needed] but it is likely that there had been earlier ascents by local people.\\r\\nAnamudi peak is one of only three ultra prominent peaks in South India. It is also the peak with the greatest topographic isolation within India.[7]It is the highest point in India south of Himalayas. Thus it is known as \\"Everest of South India\\".[8]\\r\\n\\r\\n\\r\\nAnamudi is the highest peak in the Western Ghats in India,[9] having an elevation of 2,695 metres (8,842?ft).[1][2][4] Anamudi is also the highest point in South India.[10] This gives Anamudi its relatively large topographic prominence of 2,479 metres (8,133?ft), the associated key saddle being over 2,000 kilometres (1,200?mi) away at 283520N 762759E? / ?28.58889N 76.46639E? / 28.58889; 76.46639 in Haryana state just to the west of Delhi.[3] The peak is the highest point of the Periyar river basin.[11]\\r\\nThe peak is not exceptionally dramatic in term of steepness or local relief and is a Fault-block mountain.[12] It is located in the southern region of Eravikulam National Park at the junction of the Cardamom Hills, the Anaimalai Hills and the Palani Hills. The nearest town is Munnar, 13 kilometres (8.1?mi). The easiest route to the summit of Anamudi is a technically easy hike on grass slopes, starting from a rolling hill plateau with a base elevation of about 2,000 metres (6,600?ft). The north and south slopes are gentle, while the east and west slopes are steeper, with more difficult rock faces.\\r\\nAnamudi and the Eravikulam National Park surrounding it is home to the largest surviving population of the Nilgiri tahr(Nilgiritragus hylocrius). Asian elephants, gaur, Bengal tigers, and the Nilgiri marten (Martes gwatkinsii) are some of the species of animals found here .[10] The Anamudi peak[13] area is also habitat of a unique frog Raorchestes resplendens.[14] This newly discovered species is located in the Eravikulam National Park and is restricted to less than three km2 on the summit of Anamudi.[15] The summit of the Anamudi is vegetated with patches of stunted Arundinaria densifolia and Gaultheria fragrantissima (wintergreen), Anaphalis sp., Impatiens and some species of Eriocaulon.[16]\\r\\nAnamudi as seen from an aircraft. The peak is in the center of the image with a small white cloud seen behind it's rounded summit\\r\\nAnamudi, on the right, as seen on the descent towards Cochin International Airport from the Delhi-Kochi flight\\r\\nAnamudi, Mannamala and Meesapulimala - the three highest peaks of the Western Ghats - all visible from a plane on the descent towards Kochi airport","input":"Which is the highest peak of southern india?"},{"output":"more than 180","context":"\\r\\n\\r\\nXavier Hernndez Creus (Catalan:?[??ai ?r?nand?s ?k??ws] Spanish:?[?t?aj e??nandeĲ  ?k?eus]; born 25 January 1980) is a Spanish professional footballer who plays as a central midfielder for Qatari club Al Sadd SC. He is known for playing for the Spain national team and Barcelona.\\r\\n\\r\\nXavi came through La Masia, the Barcelona youth academy, at age 11 and made his first-team debut against Mallorca in August 1998. Since then, he has played 700 matches, scored 82 goals and made more than 180 assists for over 50 players. Xavi is the first player in Barcelona's history to play 150 international matches.[3] Highly regarded for his humble persona and team ethos, Xavi is viewed as being the embodiment of the tiki-taka passing style of play, and is widely considered to be one of the greatest central midfielders of all time.[4][5][6][7] He is also considered by many to be the greatest Spanish player ever.[6][8][9]\\r\\n\\r\\nXavi won the U-20 World Cup for Spain in 1999, and the Olympic Silver Medal at the 2000 Olympics. Since making his senior team debut in 2000, he has been capped 133 times for the Spain national team, and has been seen as an influential figure in the team's tremendous success. He played an integral role in Spain's victory at the 2010 World Cup, as well as their successes at both Euro 2008 and Euro 2012. He had a 91% passing success rate at the 2010 World Cup and was named in the World Cup All-Star Team.[6] He was named Player of the Tournament by UEFA at Euro 2008.[10] He was also named in the UEFA Euro Team of the Tournament in 2008 as well as 2012. After the 2014 World Cup, Xavi announced his retirement from international football.[11]\\r\\n\\r\\nWith Barcelona, Xavi won eight La Liga titles and four UEFA Champions League titles. He has won 31 trophies for Barcelona and Spain, a total only surpassed by Andrs Iniesta as the most successful Spanish player in history.[12][13] He came third in the 2009 FIFA World Player of the Year, followed by third place for its successor award, the FIFA Ballon d'Or, in 2010 and 2011. In 2011, he was runner up to Lionel Messi for the UEFA Best Player in Europe Award. He has been awarded the IFFHS World's Best Playmaker award for four years: 2008, 2009, 2010, 2011. Xavi has been included in the UEFA Team of the Year for five years (2008, 2009, 2010, 2011, 2012), and has been voted into the FIFA World XI on six occasions: 2008, 2009, 2010, 2011, 2012, 2013. At Euro 2012, with his two assists in the final, Xavi became the first player to register assists in two separate Euro finals, having set up the lone goal in the final four years earlier.[14] He was awarded the Prince of Asturias Award for Sports in 2012.\\r\\n\\r\\nXavi on learning the team ethos of Barcelona while at the club's youth system, La Masia.[15]\\r\\n\\r\\nBorn in Terrassa, Barcelona, Catalonia,[16] Xavi is a product of FC Barcelona's La Masia youth system, which he joined at the age of 11. His father, Joaquim, was a former player for Sabadell in the first division.[17] Xavi made his way through the youth and reserve teams and was a key member of Josep Maria Gonzalvo's Barcelona B team that won promotion to the Second Division.\\r\\n\\r\\nAlthough he was initially inspired by compatriot playmaker Pep Guardiola at Barcelona,[18] as a child Xavi also watched a lot of English football, and held midfielders John Barnes, Paul Gascoigne and Matt Le Tissier in high regard,[19] as well as high praise especially for Paul Scholes.[20]\\r\\n\\r\\nXavi's progression through the teams earned him a first-team appearance in a Copa Catalunya match against Lleida on 5 May 1998[21] and he scored his first goal on 18 August 1998 in the Super Cup final against Mallorca. His debut in La Liga came against Valencia on 3 October 1998 in a 3ÿ1 victory for Barcelona. Initially featuring intermittently both for the reserve and senior teams, Xavi scored the only goal in a 1ÿ0 victory over Real Valladolid when Barcelona were in tenth position in the league. Sustained impressive performance meant that he became a key member of Louis van Gaal's title-winning team. Xavi finished his debut season with 26 matches played and winning the Spanish league. He also was named 1999 La Liga Breakthrough Player of the Year. Xavi became Barcelona's principal playmaker after an injury to Pep Guardiola in the 1999ÿ2000 season.\\r\\n\\r\\nIn these years, Barcelona was on the verge of bankruptcy and struggling to keep its place in La Liga's elite. Playing in the midfield, but with more defensive role, Xavi made 20 assists and scored 7 goals in those two seasons. On 16 March 2002, he scored his first goal in El Clsico against Real Madrid.[22]\\r\\n\\r\\nXavi was named the vice-captain in the 2004ÿ05 season, in which he helped Barcelona win La Liga and the 2004 Supercopa de Espa?a.[23] He was named La Liga Spanish Player of the Year in 2005.[23]\\r\\n\\r\\nIn the 2005ÿ06 season, Xavi tore the ligaments in his left knee in training; he was out of action for four months, missing a part of the season, but returned in April and was on the substitutes bench for Barcelona's win in the 2006 UEFA Champions League final against Arsenal. He also won La Liga and the Supercopa de Espa?a again.[23]\\r\\n\\r\\nFormer Barcelona coach Pep Guardiola, September 2008.[24]\\r\\n\\r\\nAfter being named Player of the Tournament at Euro 2008, Xavi spoke to Bayern Munich about a transfer, but newly appointed Barcelona coach Pep Guardiola convinced him that he was too important to the club to be allowed to leave.[25]\\r\\n\\r\\nHe was a main part of Barcelona's treble and scored the fourth goal in the 4ÿ1 win in the 2009 Copa del Rey Final against Athletic Bilbao, with a free kick. In La Liga, among many games, one of the most significant is the 6ÿ2 Clsico victory over Real Madrid on 2 May; he assisted four out of six goals (once to Carles Puyol, once to Thierry Henry and twice to Lionel Messi). \\r\\n\\r\\nXavi helped Barcelona win the 2009 UEFA Champions League Final 2ÿ0 against Manchester United, assisting the second goal by crossing to Messi for his header.[26] Prior to the match, Manchester United coach Sir Alex Ferguson heaped praise on the much lauded central midfield combination of Xavi and Andrs Iniesta, stating, \\"I don't think Xavi and Iniesta have ever given the ball away in their lives. They get you on that carousel and they can leave you dizzy.\\"[27] Xavi was voted \\"UEFA Champions League best midfielder\\" for his contribution during Barcelona's victorious 2008ÿ09 Champions League campaign.[28]\\r\\n\\r\\nXavi was La Liga's highest assisting player with 20 assists.[29] He was also the highest assisting player in the Champions League with seven assists. Xavi earned 29 assists overall that season. Xavi was under contract to Bar?a until 2014 after extending his contract during the 2008ÿ09 season.[30] The new contract made him one of the club's biggest earners, with a salary of ?7.5?million a year.[30]\\r\\n\\r\\nDuring the 2009ÿ10 season, journalists increasingly noted Xavi's contribution to the Barcelona team. For example:\\r\\n\\r\\n\\"Quite simply the best midfielder of modern football. It could even be argued that Xavi and Matth?us are the two best in this position in history. World class for several years now, it is the past three seasons in particular where the 30-year-old has been untouchable. Xavis passing is up there with Michel Platini, he creates countless goals with genius through balls while virtually never relinquishing possession.\\"[31]\\r\\n\\r\\nIn the 2009ÿ10 season, Xavi again topped the assists table and provided both the assists in Barcelona's 2ÿ0 victory against Real Madrid at the Santiago Bernabu Stadium. He was acclaimed the second-best player of Barcelona in a season-long voting as Barcelona won the Liga title with a record 99 points.[32] On 3 June 2010, Madrid-based newspaper Marca awarded Xavi third place in the annual Trofeo Alfredo di Stfano award for the best player in La Liga, behind Lionel Messi and Cristiano Ronaldo.[33]\\r\\n\\r\\nXavi speaking in 2011 about tiki-taka style of play introduced to the club by Dutch legend Johan Cruyff.[34]\\r\\n\\r\\nOn 9 June 2010, Xavi signed a new four-year contract with the club, which may be automatically renewed up to 30 June 2016 based on number of games played.[35] On 29 November, he scored his third goal against arch-rivals Real Madrid in a 5ÿ0 home win. On 18 December, he scored another goal against Espanyol in a 1ÿ5 win. In the Champions League, Xavi scored a valuable goal against Arsenal during a home match in the UEFA Champions League that saw Barcelona progress to the quarter-finals with an assist by teammate David Villa.[36]\\r\\n\\r\\nXavi was one of the three finalists for the 2010 FIFA Ballon d'Or, alongside Barcelona teammates Lionel Messi and Andrs Iniesta. He finished third on the vote, behind Messi and Iniesta.[37][38] He narrowly defeated Messi to win Player of the Year designation from World Soccer magazine.[39]\\r\\n\\r\\nOn 2 January 2011, in a league match against Levante, Xavi made his 549th appearance for the club in all competitions, matching the record held by Migueli. Following this match, Xavi became the player with the most appearances for Barcelona of all time.[40] On 28 May, Xavi was imperious in the 2011 UEFA Champions League Final at Wembley Stadium in London as Barcelona defeated Manchester United in the showpiece for the second time in three seasons, winning 3ÿ1.[23]\\r\\n\\r\\nXavi began the 2011ÿ12 season in fine goalscoring form and seemed to grow in his influence of the team despite the long anticipated return of Cesc Fbregas and the promotion of Thiago to create added competition for places in Bar?a's attacking midfield positions.\\r\\n\\r\\nOn 18 December, in the 2011 FIFA Club World Cup Final in Yokohama, Barcelona won 4ÿ0 against Brazilian side Santos as Xavi scored a goal and made an assist to Lionel Messi.[41] After the ball was slightly behind him, Xavi brought the ball down with a cocked leg, effectively using his ankle to control it, before slipping a pass through to Messi, who scored the first goal.[41]\\r\\n\\r\\nXavi scored the winning goal in the Group H game against Milan, a vital match for Barcelona's progression in the UEFA Champions League. In total, Xavi gave the best goalscoring return of his career in 2011ÿ12 season with ten Liga goals, two in the Copa del Rey ÿ which Barcelona won ÿ and one in the FIFA Club World Cup final success.[23]\\r\\n\\r\\nOn 18 December 2012, Barcelona renewed Xavis contract, extending it until 30 June 2016.[42]\\r\\nXavi scored a goal against Real Madrid in a 3ÿ2 win for Barcelona. Xavi was named into the FIFA World XI, along with teammates Dani Alves, Andrs Iniesta and Lionel Messi.[43] Barcelona had virtually secured their La Liga title by the start of 2013, eventually equalling Real Madrid's 100-point record of the previous season.[44][45]\\r\\n\\r\\nOn 16 January 2014, Xavi made his 700th appearance for the first team against Getafe in the Copa del Rey.[46] For the first time in five years, Barcelona ended the season without a major trophy; they were defeated in the Copa del Rey final by Real Madrid with Gareth Bale scoring a late winner, and lost the league in the last game to Atltico Madrid.[47][48]\\r\\n\\r\\nIn June 2014, it was announced that Xavi would be leaving the club.[49][50][51] On 22 July, however, after talks with newly appointed manager and former teammate Luis Enrique, Xavi decided to stay at Camp Nou for one more season.[52] He was also appointed as club captain. On 25 April 2015, Xavi made his 500th La Liga appearance, becoming the eighth player in history to do so.[53] On 4 June, a farewell tribute was held at Barcelona for Xavi with players, managers, friends and family paying tribute to him.[54][55][56]\\r\\n\\r\\nOn 6 June 2015, Xavi appeared as a 78th-minute substitute for Andrs Iniesta to make his final appearance for Barcelona during the 2015 UEFA Champions League Final, as the club won its fifth European Cup/Champions League by beating Juventus at Berlin's Olympiastadion. Xavi, as club captain, lifted the trophy.\\r\\n[57] This made Barcelona the first club in history to win the treble of domestic league, domestic cup and European Cup twice.[58] Xavi, Iniesta, Messi, Gerard Piqu, Pedro, Sergio Busquets and Dani Alves are the only players to have been a part of both treble-winning teams.[58]\\r\\n\\r\\nOn 21 May 2015, Xavi announced that he would join Qatari club Al Sadd at the end of the 2014ÿ15 season on a three-year contract. According to his agent, the deal would involve him becoming an ambassador for the 2022 FIFA World Cup in the country, and also start his coaching qualifications.[59] He made his debut for Al-Sadd in a 4ÿ0 win over Mesaimeer on 13 September 2015, assisting in the team's first goal.[60] In the following match, he scored his first goal for the club in a 2ÿ2 draw with Umm Salal.[61] Al Sadd ended the league campaign in third position putting them in a place for next season AFC Champions League. Xavi scored three goals during the season. In the Champions League, Al Sadd were knocked out from the qualifying rounds by Emirati side Al Jazira on penalties; Xavi missed his spot kick.\\r\\n\\r\\nXavi won his first trophy with Al Sadd following a 2ÿ1 victory over El Jaish in the Qatar Cup final on 29 April 2017.[62] On 10 November 2017, Xavi said that he would retire when his contract with Al Sadd expired at the end of the 2017ÿ18 season, and would later pursue a coaching career.[63] However, he postponed these plans and signed a two-year contract extension on 24 May 2018.[64]\\r\\n\\r\\nXavi played for Spain at the 2000 Olympics, 2002 FIFA World Cup, UEFA Euro 2004, 2006 World Cup, Euro 2008, 2009 FIFA Confederations Cup, 2010 World Cup, Euro 2012, 2013 Confederations Cup and the 2014 World Cup.\\r\\n\\r\\nXavi was named Euro 2008's player of the tournament after Spain defeated Germany 1ÿ0 in the final.[65] Xavi was dominant in midfield, where his metronomic passing and reading of the game was pivotal to Spain's success, in addition to making tackles, shaping attacks and driving Spain to their first silverware since the 1964 European Championship.[65] Andy Roxburgh, head of UEFA's Technical Committee, said, \\"We have chosen Xavi because he epitomizes the Spanish style of play. He was influential in the whole possession, passing and penetrating kind of game that Spain played.\\"[10]\\r\\n\\r\\nXavi scored the first goal of Spain in the semi-final against Russia, which Spain won 0ÿ3. In the final, he made a pass, rolled towards the German area, from which Fernando Torres scored the winning goal.[66]\\r\\n\\r\\nXavi was named in Spain's squad for the 2010 World Cup in South Africa, with Spain eventually winning their first World Cup.[67] He provided the most number of accurate passes, 599 with a passing success rate of 91%, and he crossed the ball inside the 18-yard box more than any other player performing in the tournament.[6] For example, in the final he made 57 accurate forward half passes.[68] Xavi also covered a lot of distance on the pitch ÿ 80.20 kilometres throughout the competitions, which average at approximately 11.5 kilometres per game.[69] In the final, he covered a distance of almost 15 kilometres.[70]\\r\\n\\r\\nXavi is the beating heart of this Spanish team, the man dictating the tiki-taka pulse of pass after pass. He may be just 5ft 7in with a curiously hunched gait but no player more influences the way his whole team plays. He doesn't score, doesn't really tackle: he just passes and passes with a precision and wit unmatched by any of his peers.\\r\\nDuring the round of 16 match against Portugal, Xavi provided a backheel pass in the 63rd minute to David Villa. Although Villa had his shot with his left foot blocked by goalkeeper Eduardo, he then put in the rebound with his right foot for the winning goal.[72] Xavi crossed a corner out to the edge of the six-yard box in the semi-finals against Germany, where Carles Puyol made a header into the top-right corner.[73]\\r\\n\\r\\nXavi played for Spain at Euro 2012 which Spain won by defeating Italy 4ÿ0 in the final. Xavi attempted 136 passes (127 completed, 94% success rate) during Spain's 4ÿ0 victory in the group stage match against the Republic of Ireland, more than any other player in a European Championship match. The previous record had been set by Ronald Koeman at 117 in a Euro 1992 match between the Netherlands and Denmark. Xavi and Andrs Iniesta made 229 passes in this match, more than the combined Irish 11 managed. \\"Pum, pum, pum, pum\\" was how Xavi described the rhythmic sound of the ball constantly moving between himself and his midfield partner.[74]\\r\\n\\r\\nWith Xavi providing two assists in the final, one for Jordi Alba, and another one (like four years ago) for Fernando Torres, he became the first player to register assists in two European Championship finals.[14] Spain's UEFA Euro 2012 victory made Xavi the most successful player in Spanish football history, a feat that he previously shared with Carles Puyol, who missed the entire tournament.[14]\\r\\n\\r\\nOn 5 August 2014, following the 2014 World Cup where Spain were eliminated at the group stage, Xavi announced his retirement from international football, having made 133 appearances in a 14-year period.[75][76] Spain's World Cup-winning manager Vicente del Bosque paid tribute, stating that Xavi was \\"a key part of the team's style of play\\" and \\"he was more important to us than even the manager\\", also adding, \\"We will miss him both on and off the pitch. He is a player who we hold in great esteem both personally and as a player. He is and always will be a person and a player who is greatly valued by the federation, the coaching staff and by myself.\\"[77]\\r\\n\\r\\nXavi on his own style of play, November 2014.[19]\\r\\n\\r\\nThe claim that Xavi is one of the best central midfielders of all-time[4][5][6] relies largely on his ability to find and exploit space as a playmaker. As he said, \\"That's what I do: look for spaces. All day. I'm always looking.\\"[78] Finding space, he would appear for a teammate to receive and then move the ball on, with his coach Pep Guardiola putting it. \\"I get the ball, I give the ball, I get the ball, I give the ball.\\"[6] Xavis signature move when in possession is performing a 360 degree turn, known as la pelopina, that allowed him to move away from the opposing player, giving him space and time to think about his next pass.[71]\\r\\n\\r\\nXavi's vision, pinpoint accurate passing and world-class ball control allowed him to dictate the flow of play while rarely relinquishing possession, most notably displayed during Spain's 2010 World Cup victory where he had a 91% passing success rate for the tournament.[6] His ability to control some matches has earned him the sobriquet, \\"The Puppet Master\\".[79][80] A style introduced to the club by former coach Johan Cruyff, Barcelona president Sandro Rosell believed Xavi, together with Lionel Messi, Andrs Iniesta and Sergio Busquets, perfected the club's tiki-taka style of play.[81]\\r\\n\\r\\nXavi has a sponsorship deal with German sportswear and equipment supplier Adidas and has appeared in Adidas commercials alongside Lionel Messi, Luis Surez and Robin van Persie.[82] Xavi features in EA Sports' FIFA video game series, and was the top passer in FIFA 15 with a 91 rating.[83]\\r\\n\\r\\nIn November 2014, Xavi appeared in FIFA's \\"11 against Ebola\\" campaign with a selection of top football players from around the world, including Cristiano Ronaldo, Neymar, Gareth Bale and Didier Drogba.[84] Under the slogan \\"Together, we can beat Ebola\\", FIFA's campaign was done in conjunction with the Confederation of African Football (CAF) and health experts, with the players holding up 11 messages to raise awareness of the disease and ways to combat it.[84][85]\\r\\n\\r\\nSince July 2013, Xavi has been married to Nuria Cunillera. Their daughter, Asia, was born on 3 January 2016.[86]\\r\\n\\r\\n^1?1 Members of the Spain national football team who won the 2010 FIFA World Cup were jointly awarded \\r\\n^2?2 Jointly awarded with Iker Casillas","input":"How many assists does xavi have in his career?"},{"output":"September 6, 1787","context":"","input":"When did the us adopt the electoral college?"},{"output":"headquarters is in South Burlington, Vermont, with its main factory in Waterbury, Vermont.","context":"Ben & Jerry's Homemade Holdings Inc, trading and commonly known as Ben & Jerry's, is an American company that manufactures ice cream, frozen yogurt, and sorbet. It was founded in 1978 in Burlington, Vermont, and sold in 2000 to Anglo-Dutch conglomerate Unilever. Today it operates globally as a fully owned subsidiary of Unilever. Its present-day headquarters is in South Burlington, Vermont, with its main factory in Waterbury, Vermont.\\r\\n\\r\\n\\r\\nBen Cohen and Jerry Greenfield were childhood friends from New York. While Greenfield finished college, he found himself unable to make his way into medical school. Cohen dropped out of school.[2] In 1977, Cohen and Greenfield completed a correspondence course on ice cream making from Pennsylvania State University's creamery. Cohen has severe anosmia, a lack of a sense of smell or taste, and so relied on \\"mouth feel\\" and texture to provide variety in his diet. This led to the company's trademark chunks being mixed in with their ice cream.[3] On May 5, 1978, with a $12,000[4] investment, the two business partners opened an ice cream parlor in a renovated gas station in downtown Burlington, Vermont. In 1979, they marked their anniversary by holding the first-ever free cone day, now an international annual celebration at every Ben & Jerry's store.[3]\\r\\nIn 1980, they rented space in an old spool and bobbin mill on South Champlain Street in Burlington and started packing their ice cream in pints. In 1981, the first Ben & Jerry's franchise opened on Route 7 in Shelburne, Vermont. In 1983, Ben & Jerrys ice cream was used to build the worlds largest ice cream sundae in St. Albans, Vermont; the sundae weighed 27,102 pounds (12,293?kg). That same year, the cows on their cartons were redesigned by local artist Woody Jackson.[5]\\r\\nIn 1984, H?agen-Dazs wanted to limit distribution of Ben & Jerrys in Boston, prompting Ben & Jerrys to file suit against the parent company, Pillsbury, in its now famous Whats the Doughboy Afraid Of? campaign.[6] In 1987, H?agen-Dazs again tried to enforce exclusive distribution, and Ben & Jerrys filed its second lawsuit against the Pillsbury Company.\\r\\nIn 1985, the Ben & Jerrys Foundation was established at the end of the year with a gift from Ben & Jerry's to fund community-oriented projects; it was then provided with 7.5% of the companys annual pre-tax profits. In 1986, Ben & Jerrys launched its Cowmobile, a modified mobile home used to distribute free scoops of Ben & Jerrys ice cream in a unique, cross-country marketing drive쨕driven and served by Ben and Jerry themselves. The Cowmobile burned to the ground outside of Cleveland four months later, but there were no injuries. Ben said it looked like the worlds largest baked Alaska.[7]\\r\\nIn 1988, the two men won the title of U.S. Small Business Persons Of The Year, awarded by U.S. President Ronald Reagan.[8] Also that year, the first brownies were ordered from Greyston Bakery, which led to the development of the popular Chocolate Fudge Brownie flavor.[9] In 1992, Ben & Jerrys joined in a co-operative campaign with the national non-profit Children's Defense Fund; the campaign goal was to bring childrens basic needs to the top of the national agenda. Over 70,000 postcards were sent to Congress concerning kids and other national issues. In 1995, they hired Robert Holland, Jr. as CEO after holding a \\"Yo! I'm your C.E.O.\\" essay contest as part of the search.[10] Holland left after 20 months following philosophical differences and was replaced by Perry Odak in 1997.[11]\\r\\nIn 1989, Ben & Jerrys revealed their opposition of the use of rBGH (recombinant bovine growth hormone) in all their products. This genetically engineered hormone is sometimes given to cows in order to boost milk production, but Ben & Jerrys does not support this practice and is in favor of utilizing less chemically intensive ingredients for the safety of consumers and the environment.[12]\\r\\nIn 1994, Ben & Jerry's: The Inside Scoop, written by Fred \\"Chico\\" Lager, former CEO of Ben & Jerry's Ice Cream, was published. This book tracks the history of how Ben & Jerry's Ice Cream got started. The book focuses on \\"How Two Real Guys Built a Business with a Social Conscience and a Sense of Humor.\\"[13]\\r\\nIn April 2000, Ben & Jerry's sold the company to Anglo-Dutch multinational food giant Unilever.[14] Unilever said it hopes to carry on the tradition of engaging \\"in these critical, global economic and social missions\\". Although the founders' names are still attached to the product, they do not hold any board or management position and are not involved in day-to-day management of the company.[15]\\r\\nIn 2001, Ben & Jerry's U.S. completed the transition to \\"Eco-Pint\\" packaging, which packaged all pint flavors in environmentally friendly unbleached paperboard Eco-Pint containers, a decision it later reversed. The use of brown-kraft unbleached paperboard had been a critical first step toward a totally biodegradable pint made without added chlorine. Due to what they described as increasing supply, quality, and cost challenges, Ben & Jerry's discontinued their use of the Eco-Pint in 2006, transitioning to a pint container made out of a bleached paperboard that it said was more readily available.[16]\\r\\nOn Earth Day in 2005, when a vote in the U.S. Senate proposed the opening of the Arctic National Wildlife Refuge to oil drilling, Ben & Jerry's launched a protest by creating the largest ever Baked Alaska, which weighed 900 pounds (410?kg), and placed it in front of the U.S. Capitol Building.[17][18]\\r\\nIn March 2009, \\"CyClone Dairy\\"[19] launched an advertising campaign and a website to promote its milk products, which purportedly came exclusively from cloned cows.[20] On April 1, 2009 (April Fool's Day), Ben & Jerry's announced that it was behind this fake company. Ben & Jerry's had created the tongue-in-cheek hoax to raise awareness of the increasing presence of products from cloned animals within American food[21][22] and to campaign for a tracking system of cloned-animal products.[23] The hoax was revealed on April Fool's Day with the message: \\"We believe you should have the right to choose which foods you eat ÿ and not to eat cloned foods if you dont want to. And that's why Ben & Jerrys believes we need a national clone tracking system, so people and companies can know where their food is coming from.\\"[24]\\r\\nIn 2010, Jostein Solheim, a Unilever executive from Norway, became the new CEO of the company and had this to say about the transition: \\"My mantra that I've repeated a hundred times since starting at Ben & Jerry's is: Change is a wonderful thing,'\\" he said. \\"The world needs dramatic change to address the social and environmental challenges we are facing. Values led businesses can play a critical role in driving that positive change. We need to lead by example, and prove to the world that this is the best way to run a business. Historically, this company has been and must continue to be a pioneer to continually challenge how business can be a force for good and address inequities inherent in global business.\\"[25]\\r\\nOn February 24, 2012, Ben & Jerry's released a new Greek Frozen Yogurt line, which came in several flavors: \\"Strawberry Shortcake\\", \\"Blueberry Vanilla Graham\\", \\"Raspberry Fudge Chunk\\", \\"Banana Peanut Butter\\",[26] and \\"Vanilla\\" (scoop shop exclusive):[27] On April 12, 2013, \\"Pineapple Passionfruit\\", \\"Vanilla Honey Caramel\\", and \\"Liz Lemon\\" were added to the Greek Yogurt line.[28] The Liz Lemon flavor was inspired by a character of the same name created by actress Tina Fey as the main character on the NBC television sitcom 30 Rock.[29]\\r\\nIn 2013, Ben & Jerrys committed to making their products GMO-free in support of mandatory GMO labeling legislation.[30]\\r\\nThe \\"Vermonster\\" is a large ice cream sundae served in a \\"Vermonster Bucket\\" in Ben & Jerry's \\"scoop shops.\\" Its ingredients are 20 scoops of ice cream, 4 bananas, 4 ladles of hot fudge, 3 chocolate chip cookies, 1 chocolate fudge brownie, 10 scoops of walnuts, 2 scoops each of any 4 toppings, and whipped cream. It contains 14,000 calories (kcal), and 500 grams (17.64?oz) of fat. Since 2009, the Vermonster Challenge is an annual charity event held by Ben & Jerry's in which teams of four compete to finish a Vermonster and win free ice cream for a year.[31]\\r\\n\\"Chubby Hubby\\" consists of vanilla malt ice cream swirled with fudge and peanut butter, and containing pretzel nuggets covered in fudge and filled with peanut butter. During the month of September 2009, Ben and Jerry's, in partnership with Freedom to Marry, renamed \\"Chubby Hubby\\" to \\"Hubby Hubby,\\" in celebration of the legalization of same-sex marriage in the company's home state of Vermont. The carton featured the image of two men getting married beneath a rainbow.[32][33][34]\\r\\nOn March 13, 2012, Ben & Jerrys announced it would be changing the name of one of its ice cream flavours in the UK in support of equal marriage rights for same-sex couples. \\"Oh! My! Apple Pie!\\" would become \\"Apple-y Ever After\\" and tubs would feature a gay couple atop a wedding cake decorated with rainbows.[35]\\r\\n\\"Chocolate Chip Cookie Dough\\" was temporarily renamed \\"I Dough, I Dough\\" in the United States during the summer of 2015. This was in celebration of United States Supreme Court's ruling in support of same-sex marriage. The proceeds from sales were to go to the Human Rights Campaign (a nonprofit advocacy group for LGBT rights).[36]\\r\\nBen Cohen and Jerry Greenfield appeared on The Colbert Report on March 5, 2007, to promote their new ice cream flavor, \\"Stephen Colbert's AmeriCone Dream\\", and Cohen's progressive advocacy group TrueMajority.\\r\\nThe company renamed a flavour, \\"Yes Pecan!\\", in reference to Barack Obama's victory in the 2008 U.S. presidential election. They decided in January 2009 to donate all proceeds made on the sale of that flavor to the Common Cause Education Fund.[37]\\r\\nOn March 2, 2011, Cohen and Greenfield appeared on Late Night with Jimmy Fallon and unveiled their new flavour of ice cream, \\"Late Night Snack\\", whose carton features a picture of Jimmy Fallon on it.[38]\\r\\nOn February 17, 2015, Cohen and Greenfield appeared on The Tonight Show starring Jimmy Fallon and unveiled their new flavor of ice cream, \\"The Tonight Dough\\", which all of its proceeds go to the SeriousFun Children's Network that supports camps for children with major illnesses.[39]\\r\\nIn 2015, Charoset flavoured ice cream became widely available in time for the Passover holiday.[40][41][42][43][44][45]\\r\\nIn April 2015, the company confirmed that it was working on vegan options, after hearing consumers' feedback, led by a petition and FARM organization.[46] In early February 2016, the company announced a new all-vegan line with 4 flavors. Two of these are versions of existing flavors - \\"Chunky Monkey\\" and \\"Chocolate Fudge Brownie\\" - and two are all-new vegan-only flavors: \\"Coffee Caramel Fudge\\" and \\"Peanut Butter & Cookies\\".[47]\\r\\nFree Cone Day is an annual event held between late March and early May, in which Ben & Jerry's scoop shops give out free ice cream cups and cones. Free Cone Day was first held on Saturday, May 5, 1979, by Ben and Jerry as a customer and staff appreciation event for the first anniversary of their store's opening.\\r\\nOver one million free cones are given away each year, prompting the company's ad slogan \\"Be One In A Million.\\" Charitable organizations are often present at the stores each year and enjoy a significant amount of fundraising success. Often, local celebrities show up at various stores, promoting the day and the charities there.[48] Sometimes the event is scheduled to coincide with Earth Day and sometimes volunteers are on hand with clipboards and voter registration forms to help those who would like to register to vote.\\r\\nThe Center for Science in the Public Interest, a consumer-advocacy group, urged Ben & Jerry's to stop labeling their ice cream as \\"all natural,\\" due to the company's use of corn syrup, alkalised cocoa, and other chemically modified ingredients.[49] In September 2010, the company agreed to stop labeling their ice cream and frozen yogurt as \\"all natural.\\"[50]\\r\\nIn 2011, Ben & Jerry's released a flavor named Schweddy Balls, in homage to the Saturday Night Live (SNL) skit of the same name. An American group named \\"One Million Moms\\" protested, saying that the name was too explicit for grocery store shelvesspokesperson Monica Cole explained to the media: \\"I realize it could be a lot worse, but are they going to progressively get worse if we don't say something? Maybe they'll think twice before they come up with another inappropriate name for ice cream.\\" However, the expression of disdain was not unanimous among U.S. parents, as mother Gina Ragusa said to The Huffington Post: \\"We just think it's funny, that's all, and honestly we all really want to try it\\", adding that she consistently checks for the item's availability at her local supermarket.[51] Actor Alec Baldwin, who appeared in the SNL skit as baker Pete Schweddy, hosted the September 24, 2011 episode of the 37th season of the show and responded to the protests by stating that a new flavor called \\"Go Fudge Yourself\\" had been produced for those in opposition to the tribute. Following the initial release of the flavor, Baldwin informed the media that \\"thanks to Ben & Jerry's, the goodness of the Schweddy family recipe won't go with me to the great beyond,\\" as he had previously feared that his association with the SNL episode would remain permanent until his death.[52]\\r\\nFollowing rumors that suggested that Ben & Jerry's supported the defense of Mumia Abu-Jamalwho was convicted in 1982 of killing Philadelphia Police officer Daniel Faulkner[53]the company confirmed that Cohen did sign a petition, as a private citizen, asking that \\"the system of American justice be followed fully in the case\\".[54]\\r\\nControversy emerged in 2006 after the company released a flavor of ice cream called \\"Black and Tan\\". It had named the flavor after the alcoholic drink, which is made by mixing stout with pale ale, but the \\"Black and Tans\\" are also known as a paramilitary police force of British World War I veterans recruited during the Irish Revolution. At the time that the flavor was released, the Irish nationalist movement was still offended by the historical association of the title.[55]\\r\\nIn 2012, Vermonters for a Just Peace in Palestine/Israel (VTJP)[56] contacted Ben Cohen, Jerry Greenfield and the CEO of Ben & Jerrys after learning that ice cream produced by Ben & Jerrys franchise in Israel[57] was being sold in Israeli settlements in the West Bank and East Jerusalem. Leafleting occurred at locations in Vermont, New York and California on 'Free Cone Day' in April 2013[58] and April 2014.[59] As of November 2014[update], 232 organizations across the United States and in seventeen countries worldwide have signed a letter written by VTJP calling on Ben & Jerry's to end its commercial ties to such settlements.[60]\\r\\nIn late April 2014, Ben & Jerry's signed onto the \\"Fight for the Reef\\" campaign, a partnership between the World Wide Fund for Nature (WWF)-Australia and the Australian Marine Conservation Society (AMCS). Premier Campbell Newman and Queensland state senator Matthew Canavan both said in statements that Ben & Jerry's was making misleading statements that exaggerated the detrimental impact that proposed government programs would have on the Great Barrier Reef,[61][61] and Environment Minister Andrew Powell said that \\"The only people taking a scoop out of the reef is Ben and Jerrys and Unilever. If you understand the facts, youd want to be boycotting Ben and Jerrys\\". Australian Ben & Jerry's brand manager Kalli Swaik responded that \\"Ben & Jerrys believes that dredging and dumping in world heritage waters surrounding the marine park area will be detrimental to the reef ecology. It threatens the health of one of Australias most iconic treasures.\\"[62]\\r\\nIn April 2016, Ben & Jerry's cofounders, Ben Cohen and Jerry Greenfield, were both arrested at the Democracy Awakening protests on the U.S. Capitol steps in Washington, D.C.[63]\\r\\nIn May 2017, Ben and Jerry's announced they would not serve two scoops of the same ice cream flavor in Australia, due to the refusal of the Australian government to legalize same-sex marriage. They said this would encourage \\"fans to contact their MP's to tell them the time has come-make marriage equality legal!\\". This stance they said will continue for however long it takes for same-sex marriage to be legalized.[64][65]\\r\\nIn February 2012, a Ben & Jerry's franchise near Harvard University created a limited edition frozen yogurt flavor named \\"Taste the Lin-Sanity,\\" in honor of Asian-American basketball player Jeremy Lin, a Harvard alumnus. At inception, the product contained vanilla frozen yogurt, lychee honey swirls, and fortune cookie pieces, leading to a widely publicized controversy about racial stereotyping due to the association of the fortune cookie ingredient with Chinese culture. The latter ingredient was later replaced with waffle cookies, as the fortune cookies became soggy and the franchise received returns from customers. Ben & Jerry's general manager for Boston and Cambridge explained to the media: \\"we obviously weren't looking to offend anybody and the majority of the feedback about it has been positive.\\"[66] Ben & Jerry's released an official statement shortly after the launch of the product apologizing to those who were offended.[67]\\r\\nIn September 2014, anti-hazing activists raised concerns about the ice cream flavor \\"Hazed & Confused,\\" which had been released earlier that year. The concern was that the name could be perceived as belittling of hazing and bullying problems. The company has noted that the name was based on the word hazelnut and a play on the phrase \\"dazed and confused\\", which is both a song popularized by Led Zeppelin and a coming-of-age comedy film from the early 1990s.[68] The decision was made in October to not rename the flavor.[69]\\r\\nThe Organic Consumers Association announced in July 2017 that it found traces of the herbicide glyphosate in 10 of 11 samples of the companys ice creams.[70] The traces were found to be at levels below the ceiling set by the Environmental Protection Agency for environmental contamination.\\r\\nBen and Jerry's has locations around the world.[71]\\r\\nBen & Jerry's used to have a policy that no employee's rate of pay shall exceed five times that of entry-level employees.[75] In 1995, entry-level employees were paid $12 hourly, and the highest paid employee was President and Chief Operating Officer Chuck Lacy, who earned $150,000 annually. When Ben Cohen resigned as Chief Executive Officer and Ben & Jerry's announced the search for a new CEO in 1995, the company ended the five-to-one-ratio policy.[76]","input":"Where is ben and jerry's ice cream located?"},{"output":"Panther De Ville","context":"The Panther De Ville is a neo-classic luxury vehicle which was produced by Panther Westwinds, the British specialty maker, from 1974 to 1985. The De Ville was conceived by Robert Jankel to appeal to the taste of nouveau riche customers, including singer Elton John and actor Oliver Reed. About 60 De Villes were hand-built, including eleven two-door convertibles (for many years Britain's most expensive listed production car), and one pink and gold six-door limousine.[1]\\r\\nWith a wheelbase of 142 inches (3,600?mm), the tubular-framed De Ville used a straight-six engine or a V12 engine from Jaguar Cars. The flowing wing lines and big headlights of the De Ville were styled to imitate the Bugatti Royale.[1] The cockpit of the De Ville was modern, without the exterior's pretense of pre-war styling.[2]\\r\\nThe Panther De Ville was equipped with Jaguar suspension, power steering and automatic transmission, so it was an easy car to drive and quite quick, although poor aerodynamics tended to keep the top speed low. Interiors were lavish and often featured TV sets and drinks bars. The doors of the De Ville were from the BMC 1800 family car.[1]\\r\\nA Panther De Ville was used in Disney's 1996 live-action movie 101 Dalmatians as Cruella de Vil's car. The Jaguar engine in the car was replaced with a small-block Chevrolet V8.[1]\\r\\nThe Panther de Ville was hand painted by Alexander Mitchell","input":"What kind of car does cruella deville drive in 101 dalmatians?"},{"output":"November 3, 1834","context":"Mission San Francisco Solano was the 21st, last and northernmost mission in Alta California.[7] It was the only mission built in Alta California after Mexico gained independence from Spain. The difficulty of its beginning demonstrates the confusion resulting from that change in governance. The California Governor wanted a robust Mexican presence north of the San Francisco Bay to keep the Russians who had established Fort Ross on the Pacific coast from moving further inland. A young Franciscan friar from Mission San Francisco de Asis wanted to move to a location with a better climate and access to a larger number of potential converts.[8]\\r\\nThe Mission was successful given its short eleven year life but was smaller in number of converts and with lower productivity and diversity of industries than the older California missions.[9]\\r\\nThe mission building is now part of the Sonoma State Historic Park and is located in the city of Sonoma, California.\\r\\n\\r\\n\\r\\nFr. Jos Altimira age 33, arrived from Barcelona, Spain to serve at Mission San Francisco de Ass. The mission was not thriving because of its climate and had established a medical asistencia (\\"sub-mission\\") in San Rafael to help the missions ill neophytes (baptized Native Americans) recover their health. California Governor Luis Argello was interested in blocking the Russians at Bodega Bay and Fort Ross from moving further inland. Together they developed and presented to the party church authorities and the territory (legislature) a plan for moving Mission San Francisco de Ass and the San Rafael asistencia to a new location north of the Bay. The legislature approved but the church authorities did not respond (they had forwarded the plan to their superiors in Mexico).[10] Under the old Spanish regime, founding a new mission required the approval of both New Spain's Bishop and the Kings Viceroy.[11]\\r\\nBeginning in 1823, while waiting for a response from the church authorities, Fr. Altimira, with military escorts, began exploring north of the Bay for a suitable mission site. On July 4, 1823, the soldiers placed a large redwood cross on the place in the Sonoma Valley where they expected the new Mission San Francisco de Assis to be established. Then they celebrated Mass to consecrate the location. They then returned south to begin gathering men and materials to begin construction.[12]\\r\\nThe area around the selected site was not empty. It was near the northeast corner of the territory of the Coast Miwok,[13] Southern Pomo to the northwest, Wappo to the northeast, Suisunes and Ptwin peoples to the east.[14][15] A detachment of soldiers from the Presidio of San Francisco would be provided to protect the Mission and guard the neophytes.[16]\\r\\nAltimira with soldiers and neophytes primarily from Mission San Francisco de Ass returned to the Sonoma area near the end of August. Altimira decided there was a better place to build on the other side of the valley. Just after starting he received a letter from Father-President Sarria who refused Altimira permission to continue building. Fr. Altimira obeyed and the month of September saw continuing negotiations between Californias civil and religious leaders. On September 30 an agreement was reached: a new mission could be built and Fr. Altimira would be its minister, but Mission San Francisco de Ass would not be closed and the San Rafael asistencia had already been designated as a full mission (Mission San Rafael Arcngel).[17]\\r\\nBeginning in October 1823 Fr. Altimira had the opportunity to build his new mission at the location he chose but since Mission San Francisco de Ass would remain open this Mission needed a different patron saint. Altimira chose San Francisco Solano, a 17th-century Franciscan missionary to South America.[18] His company of soldiers and neophytes set about building all the facilities needed in a California mission. His annual report for 1823 listed no baptisms, one marriage, one funeral, a population of 482 Indians (all transferred from other missions) and 1341 animals.[19] The work had started too late in the year for anything to be planted and harvested.\\r\\nOn April 4, 1824, Passion Sunday, Father Altimira proudly dedicated his church. It was a crude, temporary structure but it symbolized development at the Mission. The church was built of whitewashed boards but was well furnished and decorated. Many of the articles were gifts from the Russians at Fort Ross. It also held a canvas painting of San Francisco Solano which had been donated by the Father-President. Furthermore, the Mission had been promised a relic of the patron saint to put in the altar.[20]\\r\\nThe Mission continued to develop until an argument arose about the sharing of the bountiful 1826 harvest Indians, not living at the Mission, were unhappy with the amount allocated for their work and burned some of the wooden buildings in protest. Fr. Altimira with a few faithful neophytes fled to Mission San Rafael Arcngel.[21]\\r\\nFr. Buenaventura Fortuni, an aging Spanish Franciscan who had been working at Mission San Jos, was assigned to replace Altimira.[22] Fr. Fortuni quickly reestablished order and morale and the work of building the mission restarted. He arranged the main buildings to form a large, square enclosure.\\r\\nIn 1830 Fr. Fortuni, having labored alone at this mission for 3 1/2 years, felt the need to transfer to another mission where the work load could be shared.[23] He was 58 years old when he was replaced by Fr. Jos Gutirrez, a Franciscan friar from South America. The Mexican government had in 1826 required that all the Spanish friars who would not pledge loyalty to Mexico leave. Fr. Fortuni had been exempted from this rule but all new churchmen would be required to take the pledge.[22]\\r\\nFr. Gutierrez continued to build and increased the agricultural effort.[24] By 1832 the mission had 27 rooms in the convento or priest's quarters, with a great adobe church at the east end, and a wooden storehouse (the original mission chapel) at the west end. Completing this enclosure were workshops where the Indians were taught to be craftsmen and created the items needed to help the mission be self-sufficient. Along the back of the courtyard were the living quarters and workrooms for the young Indian girls. In addition to the quadrangle, there were orchards, gardens, vineyards, fields of grain, a gristmill, houses for the soldiers and Indian families, a jail, a cemetery and an infirmary.[25]\\r\\nThe most successful year of this mission's short life span (11 years) was 1832. In his annual report for that year, Fr. Gutierrez recorded the following: 127 baptisms, 34 marriages, and 70 deaths; a total of 996 neophytes (coming from 35 area villages[26]); the livestock inventory included 6,000 sheep and goats, 900 horses, 13 mules, 50 pigs and 3,500 head of cattle. Crops were measured in fanegas, or Spanish bushels, a variable measure of volume generally between 50 and 60 liters. In 1832 the mission produced 800 fanegas of wheat, 1025 fanegas of barley, 52 fanegas of peas, 300 fanegas of corn, 32 fanegas of beans, and 2 fanegas of garbanzos.[27]\\r\\nIn 1833 the Mexican Congress decided to close all of the missions in Alta California with the passage of the Mexican secularization act of 1833. Governor Figueroa issued a regulation (Reglamento Provisional para la secularization de las Misiones) on August 9, 1834, outlining the requirements for the distribution of property (land, cattle, and equipment) to each missions neophytes.[28] Among the provisions were that \\"5. To each head of a family and to all over 20 years old, will be given from the Mission lands a lot not over 400 nor less than 100 varas square\\" (28 to 7 acres). Plus \\"6. ...pro rata...one-half of the livestock\\" and \\"7. ... half or less of the existing chattels, tools, and seed...\\".[29]\\r\\nMission San Francisco Solano officially ceased to exist on November 3, 1834, when it was designated a First Class Parish. The Spanish missionaries were to be replaced by parish priests - the first was Fr. Lorenzo Quijas who had earlier been assigned to Sonoma and San Rafael.[30]In,\\r\\nLieutenant (teniente) Mariano Vallejo, Commandant of the Presidio of San Francisco, was named administrator (comisionado) to oversee the closing of the Mission under the Reglamento.[31] Fr. Quijas moved back to San Rafael in July, 1835, after many disputes with Guadalupe Antonio Ortega, Vallejo's majordomo, to whom Vallejo had delegated the work of secularization. Ortega (sometimes call Sergeant Ortega[32]) was uneducated, coarse and licentious\\".[33] Right after returning to San Rafael, Padre Quijas wrote a letter to Commissary Perfect Garcia Diego, his superior, complaining about the situation in Sonoma and specifically the \\"...abominable deeds of Ortega....\\" Quijas then gives names of witnesses to be called against Ortega.[34] Upon receipt of the letter Fr. Diego forward it to Governor Jos Figueroa demanding some action against Ortega. The Governor was critically ill and died at the end of the following month. No action was taken.[35] It wasn't until the summer of 1837, because of new scandals and unsatisfactory accounts, that Ortega was removed.[36]\\r\\nAfter Fr. Quijas left, the neophyte population decreased rapidly (most returning to their home villages - taking their movable property with them, or moving to ranchos {including Vallejo's Petaluma Adobe} to work, or staying in Sonoma as servants).[37] Some former Mission Indians reportedly received their allotted land and cattle from the Mission (none of these small plots of land were permanently recorded.)[38] In August 1839, the government sent William Edward Petty Hartnell as Visitador General de Misiones to check compliance with the Reglamento but Vallejo avoided responding - claiming he did not have time because of military affairs. No effective review of the secularization of the Sonoma mission was even completed.[39]\\r\\nThe mission buildings rapidly fell into disrepair. The town of Sonoma was growing and building materials were in great demand. Roof tiles, timbers, and adobe bricks were salvaged from the mission buildings. After the settlers had cannibalized the old buildings, nature began recycling the remnants.[14]\\r\\nIn 1841, Mariano Vallejo ordered a small adobe chapel to be built on the location of the first wooden mission chapel. It became the church of the parish and replaced the large mission church which was rapidly deteriorating. It stood on the west end of the Convento so is often thought to be the church of the old mission.[40]\\r\\nDuring 1863 President Abraham Lincoln transferred ownership of all the mission churches in California to the Roman Catholic Church. In 1881, the Sonoma church property was sold to a local businessman and a new parish church was built across town. At one time, the old adobe chapel was used as a warehouse. The Convento may have been used as a winery.[14]\\r\\nIn 1903, the two remaining mission buildings were purchased by California Historic Landmarks League and became part of the California Park System in 1906. By 1913, both had been reconstructed. After the 1940s, the former church and Convento were remodeled along more authentic lines suited to exhibits devoted exclusively to mission history.[14]\\r\\nDedicated in 1999, the Sonoma Mission Indian Memorial honors the more than 800 native people (including over 200 children) who died while living and working at the Mission between 1824 and 1839. Their Christian names, as recorded by the priests in the Missions records, are inscribed on this granite memorial.[41] European diseases such as measles and smallpox, for which Native Americans had no inherited resistance, with the overcrowded and unhealthful living conditions at all California missions (especially for women and children) contributed to the high death rate.[42]\\r\\nOn June 1, 1932, Mission San Francisco Solano was designated California Historical Landmark #3.\\r\\nInterior of Vallejo's Chapel\\r\\nIndian Memorial\\r\\nMission Solano State Landmark Plaque","input":"When did the san francisco solano mission close?"},{"output":"Yentl","context":"","input":"What film did barbra streisand win for best director?"},{"output":"30 November 1872","context":"Association football, more commonly known as football or soccer, can be traced to as far back as the Medieval period in Britain (medieval football). The modern game of association football originates from the formation of The Football Association in London, England in 1863 based on multiple efforts to standardize the varying forms of the game. This allowed clubs to play each other without dispute and which specifically banned handling of the ball (except by goalkeepers) and hacking during open field play. After the fifth meeting of the association a schism emerged between association football and the rules played by the Rugby school, later to be called rugby football. At the time, football clubs had played by their own, individual codes and game-day rules usually had to be agreed upon before a match could commence. For example, the Sheffield Rules that applied to most matches played in the Sheffield area were a different code. Football has been an Olympic sport ever since the second modern Summer Olympic Games in 1900.\\r\\n\\r\\n\\r\\nThe first set of football rules was drawn up at the University of Cambridge in 1848 and became particularly influential in the development of subsequent codes, including association football. Known as the Cambridge Rules, they were written at Trinity College, Cambridge, at a meeting attended by representatives from Eton, Harrow, Shrewsbury, Rugby and Winchester schools, though they were not universally adopted. During the 1850s, many clubs unconnected to schools or universities were formed throughout the English-speaking world, to play various forms of football. Some came up with their own distinct codes of rules, most notably the Sheffield Football Club, formed by former public school pupils in 1857, which led to formation of a Sheffield Football Association in 1867.\\r\\nDuring the early 1860s, there were increasing attempts in England to unify and reconcile the various football games that were played in the public schools as well in the industrial north under the Sheffield Rules. In 1862, J. C. Thring, who had been one of the driving forces behind the original Cambridge Rules, was a master at Uppingham School and issued his own rules of what he called \\"The Simplest Game\\" (aka the Uppingham Rules). In early October 1863, a revised version of the Cambridge Rules was drawn up by a seven-member committee representing former pupils of Eton, Harrow, Shrewsbury, Rugby, Marlborough and Westminster.\\r\\nEbenezer Cobb Morley, a solicitor from Hull, wrote to Bell's Life newspaper in 1863, proposing a governing body for football. Morley was to become the FA's first secretary (1863ÿ66) and its second president (1867ÿ74), but is particularly remembered for drafting the first Laws of the Game at his home in Barnes, London, that are today played the world over. For this, he is considered not just the father of the Football Association, but of Association Football itself.\\r\\nPublic schools such as Charterhouse and Westminster School were influential in forming the new rules, at both schools the pupils' surroundings meant they were confined to playing their football in the cloisters, making the rough and tumble of the handling game that was developing at other schools such as Rugby impossible, and necessitating a different code of rules. Forest School was also influential in formulating the new rules, being present at the fifth meeting of the F.A. on 1 December 1863, and having several members at the influential Forest Club.[1] During the formulation of the rules of Association Football in the 1860s representatives of Charterhouse and Westminster pushed for a passing game, in particular rules that allowed forward passing (\\"passing on\\"). Other schools (in particular Eton College, Shrewsbury School and Harrow) favoured a dribbling game with a tight off-side rule. It is claimed that Stoke Ramblers were formed in 1863 when former pupils of Charterhouse School formed a football club while apprentices at the North Staffordshire Railway works in Stoke-on-Trent.[2] By 1867 the Football Association had chosen in favour of the Charterhouse and Westminster game and adopted a \\"loose\\" off-side rule that permitted forward passing.[3]\\r\\nOn the evening of 26 October 1863, representatives of several football clubs in the Greater London area met at the Freemasons' Tavern on Long Acre in Covent Garden. This was the first meeting of The Football Association (FA). It was the world's first official football body and for this reason is not preceded with the word English. The first meeting resulted in the issuing of a request for representatives of the public schools to join the association. With the exception of Thring at Uppingham, most schools declined.In total, six meetings of the FA were held between October and December 1863. Committee member J. F. Alcock, said: \\"The Cambridge Rules appear to be the most desirable for the Association to adopt.\\"\\r\\nAfter the third meeting, a draft set of rules were published by the FA. However, at the beginning of the fourth meeting, attention was drawn to the recently published Cambridge Rules of 1863. The Cambridge rules differed from the draft FA rules in two significant areas; namely running with (carrying) the ball and hacking (kicking opposing players in the shins). The two contentious FA rules were as follows:\\r\\nIX. A player shall be entitled to run with the ball towards his adversaries' goal if he makes a fair catch, or catches the ball on the first bound; but in case of a fair catch, if he makes his mark he shall not run.\\r\\nX. If any player shall run with the ball towards his adversaries' goal, any player on the opposite side shall be at liberty to charge, hold, trip or hack him, or to wrest the ball from him, but no player shall be held and hacked at the same time.\\r\\nAt the fifth meeting a motion was proposed that these two rules be removed from the FA rules. Most of the delegates supported this suggestion but F. W. Campbell, the representative from Blackheath and the first FA treasurer, objected strongly. He said, \\"hacking is the true football\\". The motion was carried nonetheless and  at the final meeting  Campbell withdrew his club from the FA. After the final meeting on 8 December the FA published the \\"Laws of Football\\", the first comprehensive set of rules for the game later known as association football. The game also came to be called \\"soccer\\" as a shortening of \\"Association\\" around the same time as Rugby football, colloquially referred to as \\"rugger\\", was developing as the main ball carrying version of English football, and \\"soccer\\" remains a common descriptor in countries with other prominent football codes today.[citation needed]\\r\\nThese first FA laws contained elements that are no longer part of association football, but which are still recognisable in other games (e.g. Rugby Union and Australian rules football): for instance, if a player first touched the ball behind the opponents' goal line, his side was entitled to a \\"free kick\\" at goal, from that point and fifteen yards [approximately 4.5 metres] in front of the goal line; and a player could make a catch and claim a \\"mark\\", which entitled him to a free kick from or behind that point (see Laws 7 and 8 respectively). The laws of the game agreed on by the FA members stipulated a maximum length and breadth for the pitch, the procedure for kicking off, and definition of terms, including goal, throw in, offside. Passing the ball by hand was still permitted provided the ball was caught \\"fairly or on the first bounce\\". Despite the specifications of footwear having no \\"tough nails, iron plates and gutta percha\\" there were no specific rule on number of players, penalties, foul play or the shape of the ball; captains of the participating teams were expected to agree on these things prior to the match.\\r\\nThe laws laid down by the FA had an immediate effect, with Sheffield F.C. and Nottingham (now Notts County) playing an annual fixture on the FA code, among others. As more teams joined the code in the 1860s, the sport veered away from its origins in public schools, came to be played with round balls and by teams that had settled on 11 players each. The rule eliminating passing of the ball forwards by making all players in front of the ball 'offside' (much like in rugby today) was dropped. A Sheffield against London game in 1866 had allowed the FA to observe how the rules were affecting the game; subsequently handling of the ball was abolished except for one player on each team, the goalkeeper. A red tape was added between the two goalposts to indicate the top of the goal, and a national competition was proposed. 1867 saw the introduction of the first competition and oldest existing trophy in football, the Youdan Cup.\\r\\nOn 20 July 1871, C. W. Alcock, a gentleman from Sunderland and a former pupil of Harrow School proposed that \\"a Challenge Cup should be established in connection with the [Football] Association\\",[4] the idea that gave birth to the competition. At the first FA Cup in 1872, Wanderers and Royal Engineers met in the final in front of 2,000 paying spectators. Despite the Royal Engineers being the heavy favourites, one of their players sustained a broken collar bone early on and since substitutions had not yet been introduced, the Engineers played a man down for the rest of the match which they eventually lost 1-0.\\r\\nThe FA Cup was a success and within a few years all of the clubs in England wanted to take part. To do so they had to accept the FA code, which led to the quick spread of a universal set of rules. These rules are the basis of which all association football rules today stem from.\\r\\nLater competitions saw the 'Gentleman' or Southerners dominate with Old Etonians, Wanderers, Royal Engineers and Oxford University who amongst them took 19 titles. Queens Park withdrew in the semi-finals of the 1873 cup (which due to the format being played that year meant that all the challengers to Wanderers' trophy played a competition for the right to throw down the gauntlet and play the holders, hence the full name FA Challenge Cup) because they had trouble raising travel expenses to pay for the constant trips to England, this directly led to the formation of the Scottish FA. However, despite this, Queens Park continued to participate in the FA Cup, reaching the final twice, before the Scottish FA banned Scottish clubs from entering in 1887.\\r\\nIn 1872, Alcock purchased the Football Association Cup for S20. That year, fifteen clubs entered the competition. Queen's Park reached the semi-finals without playing due to withdrawals, but then after a goalless draw with Wanderers, were forced to withdraw as before the advent of penalties and extra time, they could not afford to come back to London for the replay. Wanderers won the cup outright in 1878 after what remains to this day one of only two hat tricks of wins ever. However they returned the cup to the FA in order for the competition to continue, on the condition that no other club could win the cup outright ever again.\\r\\nIn 1888, William McGregor a gentleman from Perthshire and a director of Aston Villa F.C was the main force between meetings held in London and Manchester involving 12 football clubs, with an eye to a league competition. These 12 clubs would later become the Football League's 12 founder members. The meetings were held in London, the main concern was that an early exit in the knockout format of the FA Cup could leave clubs with no matches for almost a year, and if that happened, not only could they suffer heavy financial losses, but fans often didn't stick around for that long without a game, and instead went to other teams. Matters were finalised on 17 April in Manchester.\\r\\nMcGregor had voted against the name The Football League, as he was concerned that it would be associated with the Irish Land League.[5] But this name still won by a majority vote and was selected. The competition guaranteed fixtures and members for all of its member clubs. The clubs were split equally among North and Midlands teams. It excluded Southern teams, who were still strictly amateur.\\r\\nA rival English league called the Football Alliance operated from 1889 to 1892. In 1892 it was decided to formally merge the two leagues, and so the Football League Second Division was formed, consisting mostly of Football Alliance clubs. The existing League clubs, plus three of the strongest Alliance clubs, comprised the Football League First Division.\\r\\nThe first international game was played in Scotland on 30 November 1872. Charles Alcock, who was elected to secretary of the FA at the age of 28, devised the idea of an international competition, inaugurating an annual Scotland-England fixture. In 1870 and 1871 he placed advertisements in Edinburgh and Glasgow newspapers, requesting players for an international between the two countries. The only response that he received stated: \\"devotees of the \\"association\\" rules will find no foemen worthy of their steel in Scotland\\"[6] For this reason the 1870 and 1871 matches were composed entirely of Scots living in England. Notably, however, Smith of the Queen's Park football club took part in most of the 1870 and 1871 international matches. As early as 1870, Alcock was adamant that these matches were open to every Scotsman [Alcock's italics] whether his lines were cast North or South of the Tweed and that if in the face of the invitations publicly given through the columns of leading journals of Scotland the representative eleven consisted chiefly of Anglo-Scotians ... the fault lies on the heads of the players of the north, not on the management who sought the services of all alike impartially. To call the team London Scotchmen contributes nothing. The match was, as announced, to all intents and purposes between England and Scotland\\".[7]\\r\\nIn 1872 the challenge was eventually taken up by Queens Park FC.[8] The first international currently recognised as official by FIFA (which took place on 30 November 1872, Glasgow, Scotland) ended in a goalless draw between the two sides and thus, one of the most bitterly disputed fixtures in footballing history was born. The 2nd game between the two sides, on the 8 March 1873, ended 4-2 in favour of England, the Scots then went on to win the next game 2-1. The fourth game ended in a 2-2 draw after which the Scots enjoyed a 3-game winning streak.\\r\\nThe first non-European international was contested on 28 November 1885, at Newark, New Jersey, between the USA and Canada, the Canadians winning 1-0.[citation needed]\\r\\nWhen football was gaining popularity during the 1870s and 1880s professionals were banned in England and Scotland. Then in the 1880s, soon after Wanderers disbanded, in the north of England, teams started hiring players known as 'professors of football', who were often professionals from Scotland (who were referred to at the time as the 'Scotch Professors'). This was the first time professionalism got into football. The clubs in working class areas, especially in Northern England and Scotland wanted professional football in order to afford playing football besides working. Several clubs were accused of employing professionals.\\r\\nThe northern clubs made of lower class paid players started to gain momentum over the amateur 'Gentleman Southerners'. The first northern club to reach the FA Cup final was Blackburn Rovers in 1882, where they lost to Old Etonians, who were the last amateur team to win the trophy.\\r\\nDuring the summer of 1885, there was pressure put on the Football Association to accept professionalism in English football, culminating in a special meeting on 20 July, after which it was announced that it was \\"in the interests of Association Football, to legalise the employment of professional football players, but only under certain restrictions\\". Clubs were allowed to pay players provided that they had either been born or had lived for two years within a six-mile radius of the ground. There were also rules preventing professional players playing for more than one club in a season, without obtaining special permission, and all professional players had to be registered with the F.A.[9]\\r\\nEarly English women's teams, such as the Dick, Kerr's Ladies from Preston, were so popular that their matches raised money for charities. The first recorded women's football match, on 23 March 1895, was held in England between a northern and southern team. The fundraising matches continued, in spite of objections. A maximum wage was placed on players, players challenged this and came close to strike action in 1909, but it was not to be for another fifty years before the maximum wage was abolished. In 1921, women were banned from playing on FA league grounds. FA history states that this ban \\"effectively destroyed the game\\" in England for the next 40 years.[10] Hakoah Vienna was probably the first non-British club to pay their players during the 1920s[citation needed].\\r\\nIn 1934 the Swedish club Malm? FF was relegated from the top division after it had been discovered that they paid their players, something that was not allowed in Swedish football at the time.\\r\\nBetween 1915 and 1919 competitive association football was suspended in England. Many footballers signed up to fight in the war and as a result many teams were depleted, and fielded guest players instead. The Football League and FA Cup were suspended and in their place regional league competitions were set up; appearances in these tournaments do not count in players' official records.\\r\\nThe oldest club in continental Europe could be the Swiss club Lausanne Football and Cricket Club, founded 1860. Football was introduced in a Belgian school in Melle (near Ghent) in 1863.[11] In the Netherlands the first football match was played in 1865 in Enschede. This was followed by the founding of Koninklijke HFC by Pim Mulier in 1879. The Koninklijke HFC was the first Dutch football club. Ten years later, in 1889, the Royal Dutch Football Association (the KNVB) was founded. Association football was introduced in the Danish club, Kj?benhavns Boldklub (KB) by English residents,[12] in Swiss club FC St. Gallen in 1879 and in Belgium Royal Antwerp FC in 1880. This makes KB, St. Gallen, Koninklijke HFC and Royal Antwerp FC the oldest still existing football clubs on Continental Europe. However, Royal Antwerp FC (nickname \\"The Great Old\\") is the only one that never merged with or into another club.\\r\\nThe Danish Football Association was founded in 1889. Italian football was played in regional groups from its foundation in 1898 until 1929 when the Serie A was organised into a national league by the Italian Football Federation. La Liga, Spain's national league, had its first season in 1928, with its participants based on the previous winners of the Copa del Rey, which began in 1902. The modern German national league, the Bundesliga was late in foundation, especially for European countries, given it wasn't founded until 1963. The German Football Association was founded as early as 1900 with the first German football champions being Leipzig in 1903. However, prior to the formation of the Bundesliga, German football was played at an amateur level in a large number of regional leagues.\\r\\nThe first organised game of football in Portugal took place in 1875 in Camacha, Madeira. Organised by the Madeira born Harry Hinton.\\r\\nThe oldest surviving team in Portugal is Acadmica, which was founded in 1876.\\r\\nOn 31 March 1914, the 3 regional associations that existed in Portugal (Lisbon, Portalegre and Porto), merged to create a national association called \\"a Uni?o Portuguesa de Futebol\\" which is the ancestor of the current national association \\"Federa??o Portuguesa de Futebol\\" which was formed on 28 May 1926.\\r\\nThe first recorded football match in Argentina was played in 1867 by British railway workers at the Buenos Aires Cricket Club Ground.[13] The game was a blend of both, association and rugby footballs, allowing the use of hands.[14] The first association football team in South America, Buenos Aires Football Club was created in Argentina that same year. The first country's league was the \\"Association Argentine Football\\" (AAF), founded in 1891 by F.L. Wooley. This league organized the first ever championship to take place in 1891,[15] making Argentina's the oldest association football league outside mainland Great Britain[16] although it only lasted for one season. Its successor, the Argentine Football Association was founded by Scottish schoolteacher Alexander Watson Hutton in 1893, remaining nowadays.[17]\\r\\nIn the 1870s an expatriate named John Miller who worked on the railway construction project in S?o Paulo together with some 3000 other immigrant families from the British Isles in the last decades of the 19th century, decided to send his young boy Charles William Miller to England for his education. In 1884 Charles aged 10 was sent to Bannisters school in Southampton. Charles was a natural footballer who quickly picked up the arts of the game. The football association was being formed at the time. Eton, Rugby, Charterhouse and other colleges all had developed their own rules to the game. As an accomplished winger and striker Charles held school honours that were to gain him entry first into the Southampton Club team and then into the County team of Hampshire.\\r\\nIn 1892 a couple of years before his return to Brazil, Miller was invited to play a game for the Corinthians, a team formed of players invited from public schools and universities.\\r\\nOn his return Miller brought some association football equipment and a rules book with him. He then went on to develop the new rules of the game amongst the community in S?o Paulo. In 1888, six years before his return, the first sports club was founded in the city, S?o Paulo Athletic Club. S?o Paulo Athletic Club won the first three years championships. Miller's skills were far and above his colleagues at this stage. He was given the honour of contributing his name to a move involving a deft flick of the ball with the heel \\"Chaleira\\".\\r\\nCharles Miller kept a strong bond with English association football throughout his life. Teams from Southampton and Corinthians Club came over to Brazil and played against S?o Paulo Athletic Club and other teams in S?o Paulo. One on occasion in 1910 a new local team was about to be formed after a tour of the Corinthians team to Brazil and Charles was asked to suggest a name for the team. He suggested they should call themselves after Corinthians.\\r\\nIn 1988 when S?o Paulo Athletic Club celebrated its centenary and the English Corinthians Team went across again to play them at Morumbi Stadium. The end of the tour was against the local professional Corinthians Paulista team with S܇crates and Rivelino amongst its players. This game was played at Paecambu Stadium in S?o Paulo and true to Corinthian principles of good clean association football the score was 1 to 0 in favour of the locals when as agreed Socrates changed shirts to play alongside the English amateurs. This did not affect the score unfortunately although a largely packed stadium was cheering on for a drawn result.\\r\\nThe Brazilian Football Confederation was founded in 1914, and the current format for the Campeonato Brasileiro was established in 1971.\\r\\nThe first association football club in the United States was the Oneida Football Club of Boston, Massachusetts, founded in 1862. It is often said that this was the first club to play association football outside Britain. However, the Oneidas were formed before the English Football Association (FA); it is not known what rules they used[18] and the club wound up within the space of a few years. According to Encyclop?dia Britannica, the club is often credited with inventing the \\"Boston Game\\", which both allowed players to kick a round ball along the ground, and to pick it up and run with it.\\r\\nThe first U.S. match known to have been inspired by FA rules was a game between Princeton and Rutgers in 1869, although the game included features such as extremely physical tackling and teams of 20 each. Other colleges emulated this development, but all of these were converted to rugby-oriented rules from soccer-oriented rules by the mid-1870s on, and they would soon become famous as early bastions of American football. (For more details see: History of American football and 1869 college football season.)\\r\\nEarly football leagues in the U.S. mostly used the name football leagues: for example, the American Football Association (founded in 1884), the American Amateur Football Association (1893), the American League of Professional Football (1894), the National Association Foot Ball League (1895), and the Southern New England Football League (1914). However, the word \\"soccer\\" was beginning to catch on, and the St Louis Soccer League was a significant regional competition between 1907 and 1939. What is now the United States Soccer Federation was originally the U.S. Football Association, formed in 1913 by the merger of the American Football Association and the American Amateur Football Association. The governing body of the sport in the U.S. did not have the word soccer in its name until 1945, when it became the U.S. Soccer Football Association. It did not drop the word football from its name until 1974, when it became the U.S. Soccer Federation.\\r\\nTwo further football leagues were started in the 1967, the United Soccer Association and the National Professional Soccer League. These merged to form the North American Soccer League in 1968, which survived until 1984. The NASL also ran an indoor league in the later years.\\r\\nIndoor soccer was a great success in the 1980s to the '90s, in part due to the input of the North American Soccer League. When the NASL folded, other leagues, including the Major Indoor Soccer League filled in to meet the demand. A new MISL exists today with seven teams operating in the 2013-2014 season.[19] However, it is unrelated to the original MISL. The highest level of soccer in the United States is Major League Soccer, formed as a result of the U.S. hosting the 1994 FIFA World Cup.\\r\\nThe need for a single body to oversee the worldwide game became apparent at the beginning of the 20th century with the increasing popularity of international fixtures. The English Football Association had chaired many discussions on setting up an international body, but was perceived as making no progress. It fell to seven other European countries to band together to form this association. FIFA (Fdration Internationale de Football Association) was founded in Paris on 21 May 1904 - the French name and acronym persist to this day, even outside French-speaking countries. Its first president was Robert Gurin.\\r\\nFIFA presided over its first international competition in 1906, however it met with little approval or success. This, in combination with economic factors, led to the swift replacement of Gurin with Daniel Burley Woolfall from England, which had become a member association by that point. The next tournament staged the football competition for the 1908 Olympics in London was more successful, despite the presence of professional footballers, contrary to the founding principles of FIFA.\\r\\nMembership of FIFA expanded beyond Europe with the application of South Africa in 1909, Argentina in 1912 and the United States in 1913.\\r\\nFIFA however floundered during World War I with many players sent off to war and the possibility of travel for international fixtures severely limited. Post-war, following the death of Woolfall, the organisation fell into the hands of Alexander Bartholomew[citation needed]. The organisation had a new leader though after Bartholomew's death in 1919. It was saved from extinction, but at the cost of the withdrawal of the Home Nations, who cited an unwillingness to participate in international competitions with their recent World War enemies.\\r\\nIn 1946 the four British nations returned. On 10 May 1947 a 'Match of the Century' between Great Britain and 'Rest of Europe XI' was played at Hampden Park in Glasgow before 135,000 spectators - Britain won 6-1. The proceeds from the match, coming to S35,000, were given to FIFA, to help re-launch it after World War Two. This was followed by FIFA's first post-war World Cup in 1950, held in Brazil. FIFA, meanwhile, continued to expand so that by the time of its fiftieth anniversary it had 84 members.\\r\\nThe first Football World Cup was played in Uruguay in 1930. In the first championship match between Argentina and Uruguay, the teams could not decide on a ball so they used Argentina's ball the first half and Uruguay's in the second. Many countries did not enter, but most of those that did came from the Americas. By 1950 however, European teams took interest, and the competition blossomed into the world's biggest footballing and sporting event.\\r\\nThe South American Copa Amrica was first contested in 1916 and preceded the World Cup. Decades later, other continental championships emerged - the AFC Asian Cup (1956), the African Cup of Nations (1957), the European Championship (1960), North America's Gold Cup (1991) and Oceania's OFC Nations Cup (1996).\\r\\nThe Brazilian team is the most successful team in the World Cup, having won five times. The runners-up are Italy and Germany (three as West Germany) with four titles, having won their latest titles in 2006 and 2014 respectively.\\r\\nThe FIFA Women's World Cup was inaugurated with the FIFA Women's World Cup 1991, hosted in China, with 12 teams sent to represent their countries.\\r\\nOver 90,185 spectators attended the 1999 FIFA Women's World Cup and nearly 1 billion viewers from 70 countries tuned in. By the FIFA Women's World Cup 2003, 16 teams competed in the championship finals. Of the four tournaments held to date (2015), the USA (1991, 1999, 2015) is the most successful team with three; and Germany (2003, 2007) have won the championship twice; Norway (1995) and Japan (2011) have each won once. Women's confederations are the same as men's: Oceania (OFC), European (UEFA), North, Central America and Caribbean (CONCACAF), South American (CONMEBOL), Asian (AFC) and African (CAF).","input":"When was football accepted as an international sport?"},{"output":"In 2015, 13.5%","context":"Poverty is a state of deprivation, lacking the usual or socially acceptable amount of money or material possessions.[1] The most common measure of poverty in the U.S. is the \\"poverty threshold\\" set by the U.S. government. This measure recognizes poverty as a lack of those goods and services commonly taken for granted by members of mainstream society.[2] The official threshold is adjusted for inflation using the consumer price index.\\r\\nMost Americans will spend at least one year below the poverty line at some point between ages 25 and 75.[3] Poverty rates are persistently higher in rural and inner city parts of the country as compared to suburban areas.[4][5]\\r\\nIn 2015, 13.5% (43.1 million) of Americans lived in poverty.[6] Starting in the 1930s, relative poverty rates have consistently exceeded those of other wealthy nations.[7] The lowest poverty rates are found in New Hampshire, Vermont, Minnesota and Nebraska, which have between 8.7% and 9.1% of their population living in poverty.[8]\\r\\nIn 2009 the number of people who were in poverty was approaching 1960s levels that led to the national War on Poverty.[9] In 2011 extreme poverty in the United States, meaning households living on less than $2 per day before government benefits, was double 1996 levels at 1.5 million households, including 2.8 million children.[10]\\r\\nIn 2012 the percentage of seniors living in poverty was 14% while 18% of children were.[11] The addition of Social Security benefits contributed more to reduce poverty than any other factor.[12]\\r\\nRecent census data shows that half the population qualifies as poor or low income,[13] with one in five Millennials living in poverty.[14] Academic contributors to The Routledge Handbook of Poverty in the United States postulate that new and extreme forms of poverty have emerged in the U.S. as a result of neoliberal structural adjustment policies and globalization, which have rendered economically marginalized communities as destitute \\"surplus populations\\" in need of control and punishment.[15]\\r\\nIn 2011, child poverty reached record high levels, with 16.7 million children living in food insecure households, about 35% more than 2007 levels.[16] A 2013 UNICEF report ranked the U.S. as having the second highest relative child poverty rates in the developed world.[17] According to a 2016 study by the Urban Institute, teenagers in low income communities are often forced to join gangs, save school lunches, sell drugs or exchange sexual favors because they cannot afford food.[18]\\r\\nThere were about 643,000 sheltered and unsheltered homeless people nationwide in January 2009. Almost two-thirds stayed in an emergency shelter or transitional housing program and the other third were living on the street, in an abandoned building, or another place not meant for human habitation. About 1.56 million people, or about 0.5% of the U.S. population, used an emergency shelter or a transitional housing program between October 1, 2008 and September 30, 2009.[19] Around 44% of homeless people are employed.[20]\\r\\nIn June 2016, the IMF warned the United States that its high poverty rate needs to be tackled urgently by raising the minimum wage and offering paid maternity leave to women to encourage them to enter the labor force.[21] In December 2017, the United Nations special rapporteur on extreme poverty and human rights, Philip Alston, undertook a two-week investigation on the effects of systemic poverty in the United States, and sharply condemned \\"private wealth and public squalor\\".[22]\\r\\n\\r\\n\\r\\nThere are two basic versions of the federal poverty measure: the poverty thresholds (which are the primary version); and the poverty guidelines. The Census Bureau issues the poverty thresholds, which are generally used for statistical purposes[23]for example, to estimate the number of people in poverty nationwide each year and classify them by type of residence, race, and other social, economic, and demographic characteristics. The Department of Health and Human Services issues the poverty guidelines for administrative purposesfor instance, to determine whether a person or family is eligible for assistance through various federal programs.[24]\\r\\nThe \\"Orshansky Poverty Thresholds\\" form the basis for the current measure of poverty in the U.S. Mollie Orshansky was an economist working for the Social Security Administration (SSA). Her work appeared at an opportune moment. Orshansky's article was published later in the same year that Johnson declared war on poverty. Since her measure was absolute (i.e., did not depend on other events), it made it possible to objectively answer whether the U.S. government was \\"winning\\" this war. The newly formed United States Office of Economic Opportunity adopted the lower of the Orshansky poverty thresholds for statistical, planning, and budgetary purposes in May 1965.\\r\\nThe Bureau of the Budget (now the Office of Management and Budget) adopted Orshansky's definition for statistical use in all Executive departments. The measure gave a range of income cutoffs, or thresholds, adjusted for factors such as family size, sex of the family head, number of children under 18 years old, and farm or non-farm residence. The economy food plan (the least costly of four nutritionally adequate food plans designed by the Department of Agriculture) was at the core of this definition of poverty.[25]\\r\\nAt the time of creating the poverty definition, the Department of Agriculture found that families of three or more persons spent about one third of their after-tax income on food. For these families, poverty thresholds were set at three times the cost of the economy food plan. Different procedures were used for calculating poverty thresholds for two-person households and persons living alone. Annual updates of the SSA poverty thresholds were based on price changes in the economy food plan, but updates do not reflect other changes (food is no longer one-third of the after-tax income). Two changes were made to the poverty definition in 1969. Thresholds for non-farm families were tied to annual changes in the Consumer Price Index rather than changes in the cost of the economy food plan. Farm thresholds were raised from 70 to 85% of the non-farm levels.\\r\\nIn 1981, further changes were made to the poverty definition. Separate thresholds for \\"farm\\" and \\"female-householder\\" families were eliminated. The largest family size category became \\"nine persons or more.\\"[25]\\r\\nApart from these changes, the U.S. government's approach to measuring poverty has remained static for the past forty years.\\r\\nThe poverty guideline figures are not the figures the Census Bureau uses to calculate the number of poor persons. The figures that the Census Bureau uses are the poverty thresholds. The Census Bureau provides an explanation of the difference between poverty thresholds and guidelines.[27] The Census Bureau uses a set of money income thresholds that vary by family size and composition to determine who is in poverty.[28] The 2010 figure for a family of 4 with no children under 18 years of age is $22,541, while the figure for a family of 4 with 2 children under 18 is $22,162.[29] For comparison, the 2011 HHS poverty guideline for a family of 4 is $22,350.\\r\\nAnother way of looking at poverty is in relative terms. \\"Relative poverty\\" can be defined as having significantly less income and wealth than other members of society. Therefore, the relative poverty rate is a measure of income inequality. When the standard of living among those in more financially advantageous positions rises while that of those considered poor stagnates, the relative poverty rate will reflect such growing income inequality and increase. Conversely, the relative poverty rate can decrease, with low income people coming to have less wealth and income if wealthier people's wealth is reduced by a larger percentage than theirs.\\r\\nSome critics[who?] argue that relying on income disparity to determine who is impoverished can be misleading. The Bureau of Labor Statistics data suggests that consumer spending varies much less than income. In 2008, the poorest one fifth of Americans households spent on average $12,955 per person for goods and services (other than taxes), the second quintile spent $14,168, the third $16,255, the fourth $19,695, while the richest fifth spent $26,644. The disparity of expenditures is much less than the disparity of income.[30][neutrality is disputed]\\r\\nAlthough the relative approach theoretically differs largely from the Orshansky definition, crucial variables of both poverty definitions are more similar than often thought. First, the so-called standardization of income in both approaches is very similar. To make incomes comparable among households of different sizes, equivalence scales are used to standardize household income to the level of a single person household. When compared to the US Census poverty line, which is based on a defined basket of goods, for the most prevalent household types both standardization methods show very similar results.\\r\\nIn addition to family status, race/ethnicity and age also correlate with high poverty rates in the United States. Although data regarding race and poverty are more extensively published and cross tabulated, the family status correlation is by far the strongest.\\r\\nAccording to the US Census, in 2007 5.8% of all people in married families lived in poverty,[35] as did 26.6% of all persons in single parent households[35] and 19.1% of all persons living alone.[35] More than 75% of all poor households are headed by women (2012).[36]\\r\\nAmong married couple families: 5.8% lived in poverty.[35] This number varied by race and ethnicity as follows:\\r\\n5.4% of all white persons (which includes white Hispanics),[37]\\r\\n10.7% of all black persons (which includes black Hispanics),[38] and\\r\\n14.9% of all Hispanic persons (of any race)[39] living in poverty.\\r\\nAmong single parent (male or female) families: 26.6% lived in poverty.[35] This number varied by race and ethnicity as follows:\\r\\n22.5% of all white persons (which includes white Hispanics),[37]\\r\\n44.0% of all black persons (which includes black Hispanics),[38] and\\r\\n33.4% of all Hispanic persons (of any race)[39] living in poverty.\\r\\nAmong individuals living alone: 19.1% lived in poverty.[35] This number varied by race and ethnicity as follows:\\r\\n18% of white persons (which includes white Hispanics)[40]\\r\\n28.9% of black persons (which includes black Hispanics)[39] and\\r\\n27% of Hispanic persons (of any race)[41] living in poverty.\\r\\nThe US Census declared that in 2014 14.8% of the general population lived in poverty:[42]\\r\\n10.1% of all white non-Hispanic persons\\r\\n12.0% of all Asian persons\\r\\n23.6% of all Hispanic persons (of any race)\\r\\n26.2% of all African American persons\\r\\n28.3% of Native Americans / Alaska Natives\\r\\nAs of 2010 about half of those living in poverty are non-Hispanic white (19.6 million).[42] Non-Hispanic white children comprised 57% of all poor rural children.[43]\\r\\nIn FY 2009, African American families comprised 33.3% of TANF families, non-Hispanic white families comprised 31.2%, and 28.8% were Hispanic.[44]\\r\\nPoverty is also notoriously high on Native American reservations (see Reservation poverty). 7 of the 11 poorest counties in per capita income, including the 2 poorest in the U.S., encompass Lakota Sioux reservations in South Dakota.[45] This fact has been cited by some critics as a mechanism that enables the \\"kidnapping\\" of Lakota children by the state of South Dakota's Department of Social Services. The Lakota People's Law Project,[46] among other critics, allege that South Dakota \\"inappropriately equates economic poverty with neglect?... South Dakota's rate of identifying 'neglect' is 18% higher than the national average?... In 2010, the national average of state discernment of neglect, as a percent of total maltreatment of foster children prior to their being taken into custody by the state, was 78.3%. In South Dakota the rate was 95.8%.\\"[47]\\r\\nPoverty in the Pine Ridge Reservation in particular has had unprecedented effects on its residents' longevity. \\"Recent reports state the average life expectancy is 45 years old while others state that it is 48 years old for men and 52 years old for women. With either set of figures, that's the shortest life expectancy for any community in the Western Hemisphere outside Haiti, according to The Wall Street Journal.\\"[48]\\r\\nThe US Census declared that in 2010 15.1% of the general population lived in poverty:\\r\\n22% of all people under age 18\\r\\n13.7% of all people 19ÿ21 and\\r\\n9% of all people ages 65 and older[42]\\r\\nThe Organization for Economic Co-operation and Development (OECD) uses a different measure for poverty and declared in 2008 that child poverty in the US is 20% and poverty among the elderly is 23%.[49] The non-profit advocacy group Feeding America has released a study (May 2009) based on 2005ÿ2007 data from the U.S. Census Bureau and the Agriculture Department, which claims that 3.5 million children under the age of 5 are at risk of hunger in the United States. The study claims that in 11 states, Louisiana, which has the highest rate, followed by North Carolina, Ohio, Kentucky, Texas, New Mexico, Kansas, South Carolina, Tennessee, Idaho and Arkansas, more than 20 percent of children under 5 are allegedly at risk of going hungry. (Receiving fewer than 1,800 calories per day) The study was paid by ConAgra Foods, a large food company.[50]\\r\\nIn 2012, 16.1 million American children were living in poverty. Outside of the 49 million Americans living in food insecure homes, 15.9 million of them were children.[51] In 2013, child poverty reached record high levels in the U.S., with 16.7 million children living in food insecure households. Many of the neighborhoods these children live in lack basic produce and nutritious food. 47 million Americans depend on food banks, more than 30% above 2007 levels. Households headed by single mothers are most likely to be affected. 30 percent of low-income single mothers cannot afford diapers.[52] Inability to afford this necessity can cause a chain reaction, including mental, health, and behavioral problems. Some women are forced to make use of one or two diapers, using them more than once. This causes rashes and sanitation problems as well as health problems. Without diapers, children are unable to enter into daycare. The lack of childcare can be detrimental to single mothers, hindering their ability to obtain employment.[52] Worst affected are Oregon, Arizona, New Mexico, Florida, and the District of Columbia, while North Dakota, New Hampshire, Virginia, Minnesota and Massachusetts are the least affected.[16] 31 million low-income children received free or reduced-price meals daily through the National School lunch program during the 2012 federal fiscal year. Nearly 14 million children are estimated to be served by Feeding America with over 3 million being of the ages of 5 and under.[53]\\r\\nA 2014 report by the National Center on Family Homelessness states the number of homeless children in the U.S. has reached record levels, calculating that 2.5 million children, or one child in every 30, experienced homelessness in 2013. High levels of poverty, lack of affordable housing and domestic violence were cited as the primary causes.[54] A 2017 peer-reviewed study published in Health Affairs found that the U.S. has the highest levels of child mortality among 20 OECD countries.[55]\\r\\nPoverty affects individual access to quality education. The U.S. education system is often funded by local communities; therefore the quality of materials and teachers can reflect the affluence of community. That said, many communities address this by supplementing these areas with funds from other districts. Low income communities are often not able to afford the quality education that high income communities do which results in a cycle of poverty.\\r\\nThere are numerous factors related to poverty in the United States.\\r\\nIn recent years, there have been a number of concerns raised about the official U.S. poverty measure. In 1995, the National Research Council's Committee on National Statistics convened a panel on measuring poverty. The findings of the panel were that \\"the official poverty measure in the United States is flawed and does not adequately inform policy-makers or the public about who is poor and who is not poor.\\"\\r\\nThe panel was chaired by Robert Michael, former Dean of the Harris School of the University of Chicago. According to Michael, the official U.S. poverty measure \\"has not kept pace with far-reaching changes in society and the economy.\\" The panel proposed a model based on disposable income:\\r\\nMany sociologists and government officials have argued that poverty in the United States is understated, meaning that there are more households living in actual poverty than there are households below the poverty threshold.[78] A recent NPR report states that as many as 30% of Americans have trouble making ends meet and other advocates have made supporting claims that the rate of actual poverty in the US is far higher than that calculated by using the poverty threshold.[78] A study taken in 2012 estimated that roughly 38% of Americans live \\"paycheck to paycheck.\\"[79]\\r\\nAccording to William H. Chafe, if one used a relative standard for measuring poverty (a standard that took into account the rising standards of living rather than an absolute dollar figure) then 18% of families were living in poverty in 1968, not 13% as officially estimated at that time.[80]\\r\\nAs far back as 1969, the Bureau of Labor Statistics put forward suggested budgets for adequate family living. 60% of working-class Americans lived below one of these budgets, which suggested that a far higher proportion of Americans lived in poverty than the official poverty line suggested. These findings were also used by observers on the left when questioning the long-established view that most Americans had attained an affluent standard of living in the two decades following the end of the Second World War.[81][82]\\r\\nUsing a definition of relative poverty (reflecting disposable income below half the median of adjusted national income), it was estimated that, between 1979 and 1982, 17.1% of Americans lived in poverty.[83]\\r\\nAs noted above, the poverty thresholds used by the US government were originally developed during the Johnson administration's War on Poverty initiative in 1963ÿ1964.[84][85] Mollie Orshansky, the government economist working at the Social Security Administration who developed the thresholds, based the threshold levels on the cost of purchasing what in the mid-1950s had been determined by the US Department of Agriculture to be the minimal nutritionally-adequate amount of food necessary to feed a family. Orshansky multiplied the cost of the food basket by a factor of three, under the assumption that the average family spent one third of its income on food.\\r\\nWhile the poverty threshold is updated for inflation every year, the basket of food used to determine what constitutes being deprived of a socially acceptable minimum standard of living has not been updated since 1955. As a result, the current poverty line only takes into account food purchases that were common more than 50 years ago, updating their cost using the Consumer Price Index. When methods similar to Orshansky's were used to update the food basket using prices for the year 2000 instead of from nearly a half century earlier, it was found that the poverty line should actually be 200% higher than the official level being used by the government in that year.[86]\\r\\nYet even that higher level could still be considered flawed, as it would be based almost entirely on food costs and on the assumption that families still spend a third of their income on food. In fact, Americans typically spent less than one tenth of their after-tax income on food in 2000.[87] For many families, the costs of housing, health insurance and medical care, transportation, and access to basic telecommunications take a much larger bite out of the family's income today than a half century ago; yet, as noted above,[84][85] none of these costs are considered in determining the official poverty thresholds. According to John Schwarz, a political scientist at the University of Arizona:\\r\\nThe issue of understating poverty is especially pressing in states with both a high cost of living and a high poverty rate such as California where the median home price in May 2006 was determined to be $564,430.[88] In the Monterey area, where the low-pay industry of agriculture is the largest sector in the economy and the majority of the population lacks a college education, the median home price was determined to be $723,790, requiring an upper middle class income only earned by roughly 20% of all households in the county.[88][89]\\r\\nSuch fluctuations in local markets are, however, not considered in the Federal poverty threshold and thus leave many who live in poverty-like conditions out of the total number of households classified as poor.\\r\\nIn 2011, the Census Bureau introduced a new supplemental poverty measure aimed at providing a more accurate picture of the true extent of poverty in the United States. The SPM extends the official poverty measure by taking account of many of the government programs designed to assist low-income families and individuals that are not included in the current official poverty measure.[8] According to this new measure, 16% of Americans lived in poverty in 2011, compared with the official figure of 15.2%. The new measure also estimated that nearly half of all Americans lived in poverty that year, defined as living within 200% of the federal poverty line.[90]\\r\\nDuke University Professor of Public Policy and Economics Sandy Darity, Jr. says, \\"There is no exact way of measuring poverty. The measures are contingent on how we conceive of and define poverty. Efforts to develop more refined measures have been dominated by researchers who intentionally want to provide estimates that reduce the magnitude of poverty.\\"[91]\\r\\nAccording to an 2017 academic study by MIT economist Peter Temin, Americans trapped in poverty live in conditions rivaling the developing world, and are forced to contend with substandard education, dilapidated housing, and few stable employment opportunities.[92] A 2017 study published in The American Journal of Tropical Medicine and Hygiene found that hookworm, a parasite that thrives on extreme poverty, is flourishing in the Deep South. A report on the study in The Guardian stated:\\r\\nSome 12 million Americans live with diseases associated with extreme poverty.[94]\\r\\nAccording to Philip Alston, the United Nations special rapporteur on extreme poverty and human rights, 19 million people live in deep poverty (a total family income that is below one-half of the poverty threshold) in the United States as of 2017.[22]\\r\\nSome critics assert that the official U.S. poverty definition is inconsistent with how it is defined by its own citizens and the rest of the world, because the U.S. government considers many citizens statistically impoverished despite their ability to sufficiently meet their basic needs. According to a heavily criticised[95][96][97][98] 2011 paper by The Heritage Foundation research fellow Robert Rector, of the 43.6 million Americans deemed by the U.S. Census Bureau to be below the poverty level in 2009, the majority had adequate shelter, food, clothing and medical care. In addition, the paper stated that those assessed as below the poverty line in 2011 have a much higher quality of living than those who were identified by the census 40 years ago as being in poverty. For example, in 2005, 63.7% of those living in poverty had cable or satellite television. In some cases the report even said that people currently living in poverty were actually better off than middle class people of the recent past. For example, in 2005, 78.3% of households living in poverty had air conditioning, whereas in 1970, 36.0% of all households had air conditioning.[99][100]\\r\\nAccording to The Heritage Foundation, the federal poverty line also excludes income other than cash income, especially welfare benefits. Thus, if food stamps and public housing were successfully raising the standard of living for poverty stricken individuals, then the poverty line figures would not shift, since they do not consider the income equivalents of such entitlements.[101]\\r\\nA 1993 study of low income single mothers titled Making Ends Meet, by Kathryn Edin, a sociologist at the University of Pennsylvania, showed that the mothers spent more than their reported incomes because they could not \\"make ends meet\\" without such expenditures. According to Edin, they made up the difference through contributions from family members, absent boyfriends, off-the-book jobs, and church charity.\\r\\nAccording to Edin: \\"No one avoided the unnecessary expenditures, such as the occasional trip to the Dairy Queen, or a pair of stylish new sneakers for the son who might otherwise sell drugs to get them some money or something, or the Cable TV subscription for the kids home alone and you are afraid they will be out on the street if they are not watching TV.\\" However many mothers skipped meals or did odd jobs to cover those expenses. According to Edin, for \\"most welfare-reliant mothers food and shelter alone cost almost as much as these mothers received from the government. For more than one-third, food and housing costs exceeded their cash benefits, leaving no extra money for uncovered medical care, clothing, and other household expenses.\\"[102]\\r\\nSteven Pinker, writing in an op-ed for The Wall Street Journal, claims that the poverty rate, as measured by consumption, has fallen from 11% in 1988, to 3% in 2018.[103]\\r\\nIn the age of inequality, such anti-poverty policies are more important than ever, as higher inequality creates both more poverty along with steeper barriers to getting ahead, whether through the lack of early education, nutrition, adequate housing, and a host of other poverty-related conditions that dampen ones chances in life.\\r\\nThere have been many governmental and nongovernmental efforts to reduce poverty and its effects. These range in scope from neighborhood efforts to campaigns with a national focus. They target specific groups affected by poverty such as children, people who are autistic, immigrants, or people who are homeless. Efforts to alleviate poverty use a disparate set of methods, such as advocacy, education, social work, legislation, direct service or charity, and community organizing.\\r\\nRecent debates have centered on the need for policies that focus on both \\"income poverty\\" and \\"asset poverty.\\"[105] Advocates for the approach argue that traditional governmental poverty policies focus solely on supplementing the income of the poor through programs such as Aid to Families with Dependent Children (AFDC) and Food Stamps. According to the CFED 2012 Assets & Opportunity Scorecard, 27 percent of households ÿ nearly double the percentage that are income poor ÿ are living in \\"asset poverty.\\" These families do not have the savings or other assets to cover basic expenses (equivalent to what could be purchased with a poverty level income) for three months if a layoff or other emergency leads to loss of income. Since 2009, the number of asset poor families has increased by 21 percent from about one in five families to one in four families. In order to provide assistance to such asset poor families, Congress appropriated $24 million to administer the Assets for Independence Program under the supervision of the US Department for Health and Human Services. The program enables community-based nonprofits and government agencies to implement Individual Development Account or IDA programs, which are an asset-based development initiative. Every dollar accumulated in IDA savings is matched by federal and non-federal funds to enable households to add to their assets portfolio by buying their first home, acquiring a post-secondary education, or starting or expanding a small business.[106]\\r\\nAdditionally, the Earned Income Tax Credit (EITC or EIC) is a credit for people who earn low-to-moderate incomes. This credit allows them to get money from the government if their total tax outlay is less than the total credit earned, meaning it is not just a reduction in total tax paid but can also bring new income to the household. The Earned Income Tax Credit is viewed as the largest poverty reduction program in the United States. There is an ongoing debate in the U.S. about what the most effective way to fight poverty is, through the tax code with the EITC, or through the minimum wage laws.\\r\\nGovernment safety net programs put in place since the War on Poverty have helped reduce the poverty rate from 26% in 1967 to 16% in 2012, according to a Supplemental Poverty Model (SPM) created by Columbia University, while the official U.S. Poverty Rate has not changed, as the economy by itself has done little to reduce poverty. According to the 2013 Columbia University study which created the (SPM) method of measuring poverty, without such programs the poverty rate would be 29% today.[107] An analysis of the study by Kevin Drum suggests the American welfare state effectively reduces poverty among the elderly but provides relatively little assistance to the working-age poor.[108] A 2014 study by Pew Charitable Trusts shows that without social programs like food stamps, social security and the federal EITC, the poverty rate in the U.S. would be much higher.[109] Nevertheless, the U.S. has the weakest social safety net of all developed nations.[110][111][112] Sociologist Monica Prasad of Northwestern University argues that this developed because of government intervention rather than lack of it, which pushed consumer credit for meeting citizens' needs rather than applying social welfare policies as in Europe.[113]","input":"What is the official poverty rate in the us?"},{"output":"Wilfred Owen","context":"\\"Dulce et Decorum est\\" (read here, on WikiSource) is a poem written by Wilfred Owen during World War I, and published posthumously in 1920. The Latin title is taken from the Roman poet Horace and means \\"it is sweet and honorable...\\", followed by pro patria mori, which means \\"to die for one's country\\". One of Owen's most renowned works, the poem is known for its horrific imagery and condemnation of war. It was drafted at Craiglockhart in the first half of October 1917 and later revised, probably at Scarborough but possibly Ripon, between January and March 1918. The earliest surviving manuscript is dated 8 October 1917 and addressed to his mother, Susan Owen, with the message \\"Here is a gas poem done yesterday (which is not private, but not final).\\"\\r\\n\\r\\n\\r\\nFormally, the poem combines two sonnets, as it is formed by 28 lines, though the spacing of the stanzas is irregular.[citation needed] The text presents a vignette from the front lines of World War I; specifically, of British soldiers attacked with chlorine gas. In the rush when the shells with poison gas explode, one soldier is unable to get his mask on in time. The speaker of the poem describes the gruesome effects of the gas on the man and concludes that, if one were to see first-hand the reality of war, one might not repeat mendacious platitudes like dulce et decorum est pro patria mori: \\"\\"It is sweet and proper to die for the fatherland.\\"\\r\\nThroughout the poem, and particularly strong in the last stanza, there is a running commentary, a letter to Jessie Pope, a civilian propagandist of World War I, who encouraged\\"with such high zest\\"young men to join the battle, through her poetry, e.g. \\"Who's for the game?\\"\\r\\nThe first draft of the poem, indeed, was dedicated to Pope.[1] A later revision amended this to \\"a certain Poetess\\",[1] though this did not make it into the final publication, either, as Owen apparently decided to address his poem to the larger audience of war supporters in general such as the women who handed out white feathers during the conflict to men whom they regarded as cowards for not being at the front. In the last stanza, however, the original intention can still be seen in Owen's bitter address. This poem has such detailed imagery, even by today's standards, it is still thought of as an unforgettable excoriation of World War I with the use of its intense tone, it truly gives the reader an insight of what the feeling of being on the front line would have been like.\\r\\nThe title of this poem means 'It is sweet and glorious'. The title and the Latin exhortation of the final two lines are drawn from the phrase \\"Dulce et decorum est pro patria mori\\" written by the Roman poet Horace in (Ode III.2.13):[2]\\r\\nDulce et decorum est pro patria mori:\\r\\nmors et fugacem persequitur virum\\r\\nnec parcit inbellis iuventae\\r\\npoplitibus timidoque tergo.\\r\\nHow sweet and honourable it is to die for one's country:","input":""}]`),I={name:"App",components:{PoemCard:C},data(){return{visibleCount:20,poemsData:M}},computed:{visiblePoems(){return this.poemsData.slice(0,this.visibleCount)},hasMorePoems(){return this.visibleCount<this.poemsData.length}},methods:{loadMore(){this.visibleCount+=20}}},x={class:"card-container"};function B(h,t,n,c,u,i){const m=p("PoemCard");return a(),o(l,null,[t[1]||(t[1]=e("section",null,[e("div",{class:"top-Banner"},[e("div",{class:"top-Banner-Title"},[e("div",{class:"top-Banner-Title-Text"},"🎉Q&A Life🥳")])])],-1)),e("section",null,[e("div",x,[(a(!0),o(l,null,g(i.visiblePoems,(r,f)=>(a(),y(m,{key:f,poem:r},null,8,["poem"]))),128))]),i.hasMorePoems?(a(),o("button",{key:0,class:"load-more-button",onClick:t[0]||(t[0]=(...r)=>i.loadMore&&i.loadMore(...r))},"See more")):w("",!0)])],64)}const P=d(I,[["render",B]]),F=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"qapage/25.md","filePath":"qapage/25.md"}'),H={name:"qapage/25.md"},N=Object.assign(H,{setup(h){return(t,n)=>(a(),o("div",null,[b(P)]))}});export{F as __pageData,N as default};
