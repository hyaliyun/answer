import{_ as d,o as a,c as o,a as e,t as s,C as p,F as l,p as g,e as y,f as w,q as b}from"./chunks/framework.DulMeQy4.js";const v={name:"PoemCard",props:{poem:{type:Object,required:!0}}},k={class:"card"},T={class:"question"},A={class:"answer"};function S(h,t,n,c,u,i){return a(),o("div",k,[e("div",T,s(n.poem.input),1),t[0]||(t[0]=e("div",{class:"separator"},null,-1)),e("div",A,s(n.poem.output)+"🚨"+s(n.poem.context),1)])}const C=d(v,[["render",S],["__scopeId","data-v-24933d12"]]),I=JSON.parse(`[{"output":"In the 2000s there was a renewed interest in nuclear power in the US","context":"spurred by anticipated government curbs on carbon emissions","input":""},{"output":"The federal government encouraged development of nuclear power through the Nuclear Power 2010 Program","context":"which coordinates efforts for building new nuclear power plants","input":"President Barack Obama announced loan guarantees for two new reactors at Georgia Power's Vogtle Electric Generating Plant.[167][168] The reactors are \\"\\"just the first of what we hope will be many new nuclear projects"},{"output":"In 2008","context":"it was reported that The Shaw Group and Westinghouse would construct a factory at the Port of Lake Charles at Lake Charles","input":"2008"},{"output":"As of March 2009","context":"the U.S. Nuclear Regulatory Commission had received applications for permission to construct 26 new nuclear power reactors[171] with applications for another 7 expected.[172][173] Six of these reactors have been ordered.[174] However","input":"with some applications being made to keep future options open and reserving places in a queue for government incentives available for up to the first three plants based on each innovative reactor design.[172]"},{"output":"In August 2011","context":"the TVA board of directors voted to move forward with the construction of the unit one reactor at the Bellefonte Nuclear Generating Station.[175] In addition","input":"many contractors have been laid off and the ultimate cost and timing for Bellefonte 1 will depend on work at another reactor TVA is completing ÿ Watts Bar 2 in Tennessee. In February 2012"},{"output":"In 2012","context":"The NRC approved construction permits for four new nuclear reactor units at two existing plants","input":"for two proposed reactors at the Vogtle plant"},{"output":"The first two of the newly approved units were the Units 3 and 4 at the existing Vogtle Electric Generating Plant. As of December 2011","context":"construction by Southern Company on the two new nuclear units had begun","input":"respectively.[180][181] One week after Southern received the license to begin major construction on the two new reactors"},{"output":"Also in 2012","context":"Units 2 and 3 at the SCANA Virgil C. Summer Nuclear Generating Station in South Carolina were approved","input":"respectively.[177]"},{"output":"A proposed nuclear power plant","context":"the Blue Castle Project","input":"Utah in 2023.[183] The plant will use 53"},{"output":"A number of other reactors were under consideration ÿ a third reactor at the Calvert Cliffs Nuclear Power Plant in Maryland","context":"a third and fourth reactor at South Texas Nuclear Generating Station","input":"four in Florida"},{"output":"In May 2009","context":"John Rowe","input":"which operates 17 nuclear reactors"},{"output":"In July 2009","context":"the proposed Victoria County Nuclear Power Plant was delayed","input":"AmerenUE has suspended plans to build its proposed plant in Missouri because the state Legislature would not allow it to charge consumers for some of the project's costs before the plant's completion. The New York Times has reported that without that \\"\\"financial and regulatory certainty\\"\\" the company has said it could not proceed.[188] Previously"},{"output":"In February 2010","context":"the Vermont Senate voted 26 to 4 to block operation of the Vermont Yankee Nuclear Power Plant after 2012","input":"misstatements in testimony by plant officials"},{"output":"In 2010","context":"demand for nuclear power softened in America","input":"Matthew Wald from the New York Times reported that \\"\\"the nuclear renaissance is looking small and slow at the moment\\"\\".[185]"},{"output":"In the first quarter of 2011","context":"renewable energy contributed 11.7 percent of total U.S. energy production (2.245 quadrillion BTUs of energy)","input":""},{"output":"In August 2012","context":"the US Court of Appeals for the District of Columbia found that the NRC's rules for the temporary storage and permanent disposal of nuclear waste stood in violation of the National Environmental Policy Act","input":"no additional nuclear plants can ever be licensed for operation in the United States.[citation needed]"},{"output":"In March 2013","context":"the concrete for the basemat of Block 2 of the Virgil C. Summer Nuclear Generating Station was poured. First concrete for Unit 3 was completed on November 4","input":""},{"output":"In March 2013","context":"construction on unit 3 of Vogtle Electric Generating Plant started. Unit 4 was begun in November 2013.","input":""},{"output":"In 2015 the Energy Information Administration estimated that nuclear power's share of U.S. generation would fall from 19% to 15% by 2040 in its central estimate (High Oil and Gas Resource case). However","context":"as total generation increases 24% by 2040 in the central estimate","input":""},{"output":"The low price of natural gas in the US since 2008 has spurred construction of gas-fired power plants as an alternative to nuclear plants. In August 2011","context":"the head of America's largest nuclear utility said that this was not the time to build new nuclear plants","input":"but because of the low price of natural gas. John Rowe"},{"output":"In 2013","context":"four older reactors were permanently closed: San Onofre 2 and 3 in California","input":"and Kewaunee in Wisconsin.[7][8] The state of Vermont tried to shut Vermont Yankee"},{"output":"The additional cancellation of five large reactor upgrades (Prairie Island","context":"1 reactor","input":"2 reactors"},{"output":"In July 2013","context":"economist Mark Cooper named some nuclear power plants that face particularly intense challenges to their continued operation:[199]","input":""},{"output":"Cooper said that the lesson for policy makers and economists is clear: \\"\\"nuclear reactors are simply not competitive\\"\\".[199]","context":"","input":""},{"output":"In December 2010","context":"The Economist reported that the demand for nuclear power was softening in America.[192] In recent years","input":"but the number with any serious prospect of being built as of the end of 2010 was about a dozen"},{"output":"Experts see continuing challenges that will make it very difficult for the nuclear power industry to expand beyond a small handful of reactor projects that \\"\\"government agencies decide to subsidize by forcing taxpayers to assume the risk for the reactors and mandating that ratepayers pay for construction in advance\\"\\".[62]","context":"","input":""},{"output":"In August 2012","context":"Exelon stated that economic and market conditions","input":"made the \\"\\"construction of new merchant nuclear power plants in competitive markets uneconomical now and for the foreseeable future\\"\\".[203] In early 2013 UBS noted that some smaller reactors operating in deregulated markets may become uneconomic to operate and maintain"},{"output":"As of 2017","context":"the U.S. shale gas boom has lowered electricity generation costs placing severe pressure on the economics of operating older existing nuclear power plants.[208] Analysis by Bloomberg shows that over half of U.S. nuclear plants are running at a loss.[209] The Nuclear Energy Institute has estimated that 15 to 20 reactors are at risk of early closure for economic reasons.[210] Nuclear operators in Illinois and New York have obtained financial support from regulators","input":"New Jersey"},{"output":"desert","context":"Coordinates: 90S 180E? / ?90S 180E? / -90; 180\\r\\n\\r\\nThe South Pole, also known as the Geographic South Pole or Terrestrial South Pole, is one of the two points where the Earth's axis of rotation intersects its surface. It is the southernmost point on the surface of the Earth and lies on the opposite side of the Earth from the North Pole.\\r\\n\\r\\nSituated on the continent of Antarctica, it is the site of the United States AmundsenÿScott South Pole Station, which was established in 1956 and has been permanently staffed since that year. The Geographic South Pole is distinct from the South Magnetic Pole, the position of which is defined based on the Earth's magnetic field. The South Pole is at the center of the Southern Hemisphere.\\r\\n\\r\\nFor most purposes, the Geographic South Pole is defined as the southern point of the two points where the Earth's axis of rotation intersects its surface (the other being the Geographic North Pole). However, the Earth's axis of rotation is actually subject to very small \\"wobbles\\" (polar motion), so this definition is not adequate for very precise work.\\r\\n\\r\\nThe geographic coordinates of the South Pole are usually given simply as 90S, since its longitude is geometrically undefined and irrelevant. When a longitude is desired, it may be given as 0. At the South Pole, all directions face north. For this reason, directions at the Pole are given relative to \\"grid north\\", which points northwards along the prime meridian.[1] Along tight latitude circles, clockwise is east, and counterclockwise is west, opposite to the North Pole.\\r\\n\\r\\nThe Geographic South Pole is located on the continent of Antarctica (although this has not been the case for all of Earth's history because of continental drift). It sits atop a featureless, barren, windswept and icy plateau at an altitude of 2,835 metres (9,301?ft) above sea level, and is located about 1,300?km (800?mi) from the nearest open sea at Bay of Whales. The ice is estimated to be about 2,700 metres (9,000?ft) thick at the Pole, so the land surface under the ice sheet is actually near sea level.[2]\\r\\n\\r\\nThe polar ice sheet is moving at a rate of roughly 10 metres per year in a direction between 37 and 40 west of grid north,[3] down towards the Weddell Sea. Therefore, the position of the station and other artificial features relative to the geographic pole gradually shift over time.\\r\\n\\r\\nThe Geographic South Pole is marked by a stake in the ice alongside a small sign; these are repositioned each year in a ceremony on New Year's Day to compensate for the movement of the ice.[4] The sign records the respective dates that Roald Amundsen and Robert F. Scott reached the Pole, followed by a short quotation from each man, and gives the elevation as \\"9,301 FT.\\".[5][6] A new marker stake is designed and fabricated each year by staff at the site.[4]\\r\\n\\r\\nThe Ceremonial South Pole is an area set aside for photo opportunities at the South Pole Station. It is located some meters from the Geographic South Pole, and consists of a metallic sphere on a short bamboo pole, surrounded by the flags of the original Antarctic Treaty signatory states.[citation needed]\\r\\n\\r\\nAmundsen's Tent: The tent was erected by the Norwegian expedition led by Roald Amundsen on its arrival on 14 December 1911. It is currently buried beneath the snow and ice in the vicinity of the Pole. It has been designated a Historic Site or Monument (HSM 80), following a proposal by Norway to the Antarctic Treaty Consultative Meeting.[7] The precise location of the tent is unknown, but based on calculations of the rate of movement of the ice and the accumulation of snow, it is believed, as of 2010, to lie between 1.8 and 2.5?km (1.1 and 1.6 miles) from the Pole at a depth of 17?m (56?ft) below the present surface.[8]\\r\\n\\r\\nArgentine Flagpole: A flagpole erected at the South Geographical Pole in December 1965 by the First Argentine Overland Polar Expedition has been designated a Historic Site or Monument (HSM 1) following a proposal by Argentina to the Antarctic Treaty Consultative Meeting.[9]\\r\\n\\r\\nIn 1820, several expeditions claimed to have been the first to have sighted Antarctica, with the very first[clarification needed] being the Russian expedition led by Fabian Gottlieb von Bellingshausen and Mikhail Lazarev.[10] The first landing was probably just over a year later when American Captain John Davis, a sealer, set foot on the ice.[11]\\r\\n\\r\\nThe basic geography of the Antarctic coastline was not understood until the mid-to-late 19th century. American naval officer Charles Wilkes claimed (correctly) that Antarctica was a new continent, basing the claim on his exploration in 1839ÿ40,[12] while James Clark Ross, in his expedition of 1839ÿ43, hoped that he might be able to sail all the way to the South Pole. (He was unsuccessful.)[13]\\r\\n\\r\\nBritish explorer Robert Falcon Scott on the Discovery Expedition of 1901ÿ04 was the first to attempt to find a route from the Antarctic coastline to the South Pole. Scott, accompanied by Ernest Shackleton and Edward Wilson, set out with the aim of travelling as far south as possible, and on 31 December 1902, reached 8216 S.[14] Shackleton later returned to Antarctica as leader of the British Antarctic Expedition (Nimrod Expedition) in a bid to reach the Pole. On 9 January 1909, with three companions, he reached 8823' S?ÿ 112 miles (180?km) from the Pole?ÿ before being forced to turn back.[15]\\r\\n\\r\\nThe first men to reach the Geographic South Pole were the Norwegian Roald Amundsen and his party on December 14, 1911. Amundsen named his camp Polheim and the entire plateau surrounding the Pole King Haakon VII Vidde in honour of King Haakon VII of Norway. Robert Falcon Scott returned to Antarctica with his second expedition, the Terra Nova Expedition, initially unaware of Amundsen's secretive expedition. Scott and four other men reached the South Pole on January 17, 1912, thirty-four days after Amundsen. On the return trip, Scott and his four companions all died of starvation and extreme cold.\\r\\n\\r\\nIn 1914 Ernest Shackleton's Imperial Trans-Antarctic Expedition set out with the goal of crossing Antarctica via the South Pole, but his ship, the Endurance, was frozen in pack ice and sank 11 months later. The overland journey was never made.\\r\\n\\r\\nUS Admiral Richard Evelyn Byrd, with the assistance of his first pilot Bernt Balchen, became the first person to fly over the South Pole on November 29, 1929.\\r\\n\\r\\nIt was not until 31 October 1956 that humans once again set foot at the South Pole, when a party led by Admiral George J. Dufek of the US Navy landed there in an R4D-5L Skytrain (C-47 Skytrain) aircraft. The US AmundsenÿScott South Pole Station was established by air over 1956ÿ1957 for the International Geophysical Year and has been continuously staffed since then by research and support personnel.[2]\\r\\n\\r\\nAfter Amundsen and Scott, the next people to reach the South Pole overland (albeit with some air support) were Edmund Hillary (January 4, 1958) and Vivian Fuchs (January 19, 1958) and their respective parties, during the Commonwealth Trans-Antarctic Expedition. There have been many subsequent expeditions to arrive at the South Pole by surface transportation, including those by Havola, Crary and Fiennes. The first group of women to reach the pole were Pam Young, Jean Pearson, Lois Jones, Eileen McSaveney, Kay Lindsay and Terry Tickhill in 1969.[16] In 1978-79 Michele Eileen Raney became the first woman to winter at the South Pole.[17]\\r\\n\\r\\nSubsequent to the establishment, in 1987, of the logistic support base at Patriot Hills Base Camp, the South Pole became more accessible to non-government expeditions.\\r\\n\\r\\nOn December 30, 1989, Arved Fuchs and Reinhold Messner were the first to traverse Antarctica via the South Pole without animal or motorized help, using only skis and the help of wind.[18][19] Two women, Victoria E. Murden and Shirley Metz reached the pole by land on January 17, 1989.[20]\\r\\n\\r\\nThe fastest unsupported journey to the Geographic South Pole from the ocean is 24 days and one hour from Hercules Inlet and was set in 2011 by Norwegian adventurer Christian Eide,[21] who beat the previous solo record set in 2009 by American Todd Carmichael of 39 days and seven hours, and the previous group record also set in 2009 of 33 days and 23 hours.[22]\\r\\n\\r\\nThe fastest solo (female), unsupported and unassisted trek to the south pole was performed by Hannah McKeand from the UK in 2006. She made the journey in 39 days 9hrs 33mins. She started on the 19th November 2006 and finished on the 28 December 2006.[23]\\r\\n\\r\\nIn the 2011/12 summer, separate expeditions by Norwegian Aleksander Gamme and Australians James Castrission and Justin Jones jointly claimed the first unsupported trek without dogs or kites from the Antarctic coast to the South Pole and back. The two expeditions started from Hercules Inlet a day apart, with Gamme starting first, but completing according to plan the last few kilometres together. As Gamme traveled alone he thus simultaneously became the first to complete the task solo.[24][25][26]\\r\\n\\r\\nDuring the southern winter (MarchÿSeptember), the South Pole receives no sunlight at all, and from May 11 to August 1, between extended periods of twilight, it is completely dark (apart from moonlight). In the summer (SeptemberÿMarch), the sun is continuously above the horizon and appears to move in a counter-clockwise circle. However, it is always low in the sky, reaching a maximum of 23.5 in December. Much of the sunlight that does reach the surface is reflected by the white snow. This lack of warmth from the sun, combined with the high altitude (about 2,800 metres (9,200?ft)), means that the South Pole has one of the coldest climates on Earth (though it is not quite the coldest; that record goes to the region in the vicinity of the Vostok Station, also in Antarctica, which lies at a higher elevation).[27] Temperatures at the South Pole are much lower than at the North Pole, primarily because the South Pole is located at altitude in the middle of a continental land mass, while the North Pole is at sea level in the middle of an ocean, which acts as a reservoir of heat.\\r\\n\\r\\nThe South Pole is at an altitude of 9,300 feet (2,800?m) but feels like 11,000 feet (3,400?m).[28]. Centrifugal force from the spin of the planet pulls the atmosphere toward the equator. The South Pole is colder than the North Pole because of the elevation difference.[29] The North Pole is a few feet from sea level.\\r\\n\\r\\nIn midsummer, as the sun reaches its maximum elevation of about 23.5 degrees, high temperatures at the South Pole in January average at ?25.9?C (?15?F).  As the six-month \\"day\\" wears on and the sun gets lower, temperatures drop as well: they reach ?45?C (?49?F) around sunset (late March) and sunrise (late September). In midwinter, the average temperature remains steady at around ?58?C (?72?F). The highest temperature ever recorded at the AmundsenÿScott South Pole Station was ?12.3?C (9.9?F) on Christmas Day, 2011,[30] and the lowest was ?82.8?C (?117.0?F) on June 23, 1982[31][32][33]  (for comparison, the lowest temperature directly recorded anywhere on earth was ?89.2?C (?128.6?F) at Vostok Station on July 21, 1983, though ?93.2?C (?135.8?F) was measured indirectly by satellite in East Antarctica between Dome A and Dome F in August 2010[34]).\\r\\n\\r\\nThe South Pole has a desert climate, receiving very little precipitation. Air humidity is near zero. However, high winds can cause the blowing of snowfall, and the accumulation of snow amounts to about 20?cm (8?in) per year.[35] The former dome seen in pictures of the AmundsenÿScott station is partially buried due to snow storms, and the entrance to the dome had to be regularly bulldozed to uncover it. More recent buildings are raised on stilts so that the snow does not build up against the sides of them.\\r\\n\\r\\nIn most places on Earth, local time is determined by longitude, such that the time of day is more-or-less synchronised to the position of the sun in the sky (for example, at midday the sun is roughly at its highest). This line of reasoning fails at the South Pole, where the sun rises and sets only once per year, and all lines of longitude, and hence all time zones, converge. There is no a priori reason for placing the South Pole in any particular time zone, but as a matter of practical convenience the AmundsenÿScott South Pole Station keeps New Zealand Time (UTC+12/UTC+13). This is because the US flies its resupply missions (\\"Operation Deep Freeze\\") out of McMurdo Station, which is supplied from Christchurch, New Zealand.\\r\\n\\r\\nDue to its exceptionally harsh climate, there are no native resident plants or animals at the South Pole. Remarkably, though, off-course south polar skuas and snow petrels are occasionally seen there.[39]\\r\\n\\r\\nIn 2000 it was reported that microbes had been detected living in the South Pole ice.[40]","input":"What is the climate of the south pole?"},{"output":"a federal Crown corporation wholly owned by the Government of Canada","context":"The Business Development Bank of Canada (BDC; French: Banque de Dveloppement du Canada) is a federal Crown corporation wholly owned by the Government of Canada. Its mandate is to help create and develop Canadian businesses through financing, growth and transition capital, venture capital and advisory services, with a focus on small and medium-sized enterprises.[2]\\r\\n\\r\\nThe bank was founded in 1944[3] and its corporate headquarters is located in Montreal. BDC has more than 118 business centres across Canada and more than 49,000 clients.[4]\\r\\n\\r\\nBDC's debt obligations, secured by the Government of Canada, are issued to public and private sector institutions.\\r\\n\\r\\nThe bank was established by an Act of Parliament[5] as the Industrial Development Bank (IDB) in September 1944. IDB was initially an arm of the Bank of Canada, and the Governor of the Bank was also Chief Executive Officer of the IDB. During its first years, the bank's main role was to help small \\"industrial enterprises\\" convert from military production to peace-time operations after the Second World War.[6]\\r\\n\\r\\nIDB was one of the first and largest development banks in the world.[7]\\r\\n\\r\\nThe Industrial Development Bank Act was first amended in 1952 to allow the bank to offer financing to companies in the commercial airlines industry. By the mid-1950s, one out of every 10 planes in Canada was financed by IDB.[7] Later, IDB's Act was amended two more times to allow the bank to lend to companies in almost all industries.\\r\\n\\r\\nBy 1964, twenty years after its foundation, IDB had 22 branches across Canada, covering main cities such as Halifax, Montreal, Toronto, Winnipeg,  Calgary and Vancouver and also had operations in relatively rural areas.  In the mid-1970s, the bank added consulting and training to its financial offerings to help entrepreneurs better manage their businesses.[8]\\r\\n\\r\\nIn 1975, the name of the bank changed to Federal Business Development Bank (FBDB) and its venture capital operations were started. At the time, the bank was known as \\"a lender of last resort\\"supporting businesses in difficulty.[9]\\r\\n\\r\\nIn 1995, Parliament passed the Business Development Bank of Canada Act, leading to a new name and mission for the bank. The Act mandates BDC to promote entrepreneurship, with a special focus on the needs of small and medium-sized enterprises (SMEs) and to fill the market gaps and maximize financing alternatives for businesses by offering services that were complementary to those available from other financial institutions.[10]\\r\\n\\r\\nBDC is financially self-sustaining.[11] Since 1998, it has been profitable and paid a total of $417 million in dividends to its sole shareholder, the Government of Canada.[12]\\r\\nEvery ten years, the Minister of Industry must conduct a review of the provisions and operations of the BDC Act. The last legislative review was completed in December 2014.[13]\\r\\n\\r\\nBDC offers loans and advisory services with a focus on small and medium-sized companies. It reports to Parliament through the Minister of Innovation, Science and Economic Development.[14]\\r\\n\\r\\nBDC Capital, a subsidiary of BDC, offers specialized financing, including venture capital, equity as well as growth and business transition capital.[15]\\r\\n\\r\\nBDC's Venture Capital arm makes strategic investments in Canadian companies through its Energy/Cleantech Fund, Healthcare Fund and IT Fund. Notable investments include Q1 Labs, Radian6, Canopy Labs, D-Wave Systems, and Klipfolio Dashboard.[16][17]\\r\\n\\r\\nBDC is a complementary lender, offering commercial loans and investments that fill out or complete services available from private-sector financial institutions. It also provides advice to businesses through its advisory services division.[18]\\r\\n\\r\\nSmall Business Week (SBW)[19] is an annual celebration of entrepreneurs and their contribution to Canadian society. A wide range of activities is held across the country during the third week of October. Small Business Week was launched by BDC in 1979 and became a nationwide event in 1981.[20]\\r\\n\\r\\nBDC has been named one of Canada's Top 100 employers[21] every year since 2007.\\r\\n\\r\\nBDC Capital won multiple CVCA \\"Deal of the Year Award\\" for the venture capital category. It won in 2011, for its investment in Radian6 Technologies Inc.;[22] in 2012, for its investment in Q1 Labs Inc.;[23] and again in 2014, for its investment in Layer 7 Technologies.[24]","input":"What is the business development bank of canada?"},{"output":"less than a mile to the east","context":"","input":"How close is wrigley field to lake michigan?"},{"output":"Anchorage","context":"The U.S. state of Alaska is divided into 19 organized boroughs and one \\"Unorganized Borough\\". Alaska and Louisiana are the only states that do not call their first-order administrative subdivisions counties (Louisiana uses parishes instead).[1]\\r\\nMany of the most densely populated regions of the state are part of Alaska's boroughs, which function similarly to counties in other states. However, unlike county equivalents in the other 49 states, the organized boroughs do not cover the entire land area of the state. The area not part of any organized borough is referred to as the Unorganized Borough. The U.S. Census Bureau, in cooperation with the state, divides the Unorganized Borough into 10 census areas, each roughly corresponding to an election district, thus totaling 29 county equivalents. However, these areas exist solely for the purposes of statistical analysis and presentation. They have no government of their own. Boroughs and census areas are both treated as county-level equivalents by the Census Bureau.\\r\\nSome areas in the unorganized borough receive limited public services directly from the Alaska state government, usually law enforcement from the Alaska State Troopers and educational funding.\\r\\nSix consolidated city-county governments existJuneau City and Borough, Skagway Municipality, Sitka City and Borough, Yakutat City and Borough, Wrangell City and Borough, as well as the state's largest city, Anchorage. Though its legal name is the Municipality of Anchorage, it is considered a consolidated city-borough under state law.\\r\\nThe Federal Information Processing Standard (FIPS) 55-2,3,4 codes, which are used by the United States Census Bureau to uniquely identify states and counties, is provided with each entry.[2] Alaska's code is 02, so each code is of the format 02XXX. The FIPS code for each county links to census data for that county.\\r\\n\\r\\n\\r\\nThe Unorganized Borough is the portion of the U.S. state of Alaska not contained in any of its 19 organized boroughs. It encompasses over half of Alaska's area, 970,500?km2 (374,712?mi2), an area larger than any other U.S. state. As of the 2000 census 13% of Alaskans (81,803 people) reside in it.\\r\\nCurrently unique among the United States, Alaska is not entirely subdivided into organized county equivalents. (South Dakota and Texas are among states that in the past have had unorganized counties.) For the 1980 census, the United States Census Bureau divided the unorganized borough into 12 census areas to facilitate census taking in the vast unorganized area. As new boroughs incorporate, these areas have been altered or eliminated to accommodate,[9] such that there are currently 10 census areas:\\r\\n^?A:?Because census areas in the Unorganized Borough have their own FIPS codes, this code listed and linked is for the entire state of Alaska.","input":"What is the largest borough in alaska by population?"},{"output":"fifteen","context":"One Hundred and One Dalmatians, often abbreviated as 101 Dalmatians, is a 1961 American animated adventure film produced by Walt Disney and based on the 1956 novel The Hundred and One Dalmatians by Dodie Smith. The 17th Disney animated feature film, the film tells the story of a litter of Dalmatian puppies who are kidnapped by the villainous Cruella de Vil, who wants to use their fur to make into coats. Their parents, Pongo and Perdita, set out to save their children from Cruella, all the while rescuing 84 additional puppies that were bought in pet shops, bringing the total of Dalmatians to 101.\\r\\nOriginally released to theaters on January 25, 1961, by Buena Vista Distribution,[3] One Hundred and One Dalmatians was a box office success, pulling the studio out of the financial slump caused by Sleeping Beauty, a costlier production released two years prior.[4] Aside from its box office revenue, its commercial success was due to the employment of inexpensive animation techniquessuch as using xerography during the process of inking and painting traditional animation celsthat kept production costs down. It was remade into a live action film in 1996.\\r\\n\\r\\n\\r\\nSongwriter Roger Radcliffe lives in a bachelor flat in London, along with his Dalmatian, Pongo. Bored with bachelor life, Pongo decides to find a wife for Roger and a mate for himself. While watching various female dog-human pairs out the window, he spots the perfect one, a woman named Anita and her female Dalmatian, Perdita. He quickly gets Roger out of the house and drags him through the park to arrange a meeting. He and Anita fall in love and get married.[5]\\r\\nLater, Perdita gives birth to a litter of fifteen puppies. That same night, they are visited by Cruella De Vil, a wealthy former schoolmate of Anita's. She offers to buy the entire litter, but Roger says they are not for sale. A few weeks later, she hires her henchmen, Jasper and Horace, to steal them. When Scotland Yard is unable to find them, Pongo and Perdita use the \\"Twilight bark\\", a canine gossip line, to ask for help from the other dogs in London.\\r\\nColonel, an old sheepdog, along with his compatriots Captain, a gray horse, and Sergeant Tibbs, a tabby cat, find the puppies in a place called Hell Hall (Cruella's abandoned and dilapidated family estate, also known as The De Vil Place), along with many other Dalmatian puppies that she had bought from various dog stores. When Tibbs learns they are going to be made into dog-skin fur coats, Colonel quickly sends word back to London. Upon receiving the message, Pongo and Perdita leave town to retrieve their puppies. Winter has come, and they have to cross the Stour River which is running fast and laden with slabs of broken ice. Meanwhile, Tibbs overhears Cruella ordering Jasper and Horace to kill the puppies that night out of fear the police will soon find them. In response, Tibbs attempts to rescue them while Jasper and Horace are preoccupied watching television, but they finish their show and come for them before he can get them out of the house. Pongo and Perdita break in and confront Jasper and Horace just as they are about to kill the puppies. While the adult dogs attack them, Colonel and Tibbs guide the puppies from the house.\\r\\nAfter a happy reunion with their own puppies, Pongo and Perdita realize there are dozens of others with them, 99 altogether including their own. Shocked at Cruella's plans, they decide to adopt all of them, certain that Roger and Anita would never reject them. They begin making their way back to London through deep snow; all open water is frozen solid. Other animals help them along the way. Cruella, Jasper, and Horace chase them. In one town, they cover themselves with soot so they appear to be labrador retrievers, then pile inside a moving van bound for London. As it is leaving, melting snow clears off the soot and Cruella sees them. In a rage, she follows the van in her car and rams it, but Jasper and Horace, who try to cut it off from above, end up crashing into her. Both vehicles are smashed to smithereens and fall into a deep ravine; and battered, bruised and stranded, Cruella and her henchmen are defeated at last. Cruella yells in frustration as the van drives away but Jasper finally gets the courage to tell her to shut up.\\r\\nBack in London, Roger and Anita are attempting to celebrate Christmas and his first big hit, a song about Cruella, but they miss their canine friends. Suddenly, barking is heard outside and, after their nanny opens the door, the house is filled with dogs. After wiping away the rest of the soot, they are delighted to realize their companions have returned home. After counting 84 extra puppies, they decide to use the money from the song to buy a large house in the country so they can keep all 101 Dalmatians.\\r\\nDodie Smith wrote the book The Hundred and One Dalmatians in 1956. When Walt Disney read it in 1957, it immediately grabbed his attention, and he promptly acquired the rights. Smith had always secretly hoped that Disney would make it into a film.[6] Disney assigned Bill Peet to write the story, which he did, marking the first time that the story for a Disney animated film was written by a single person.[7] Writing in his autobiography, Peet was tasked by Disney to write a detailed screenplay first before storyboarding. Because Peet never learned to use a typewriter, he wrote the initial draft by hand on large yellow tablets.[8] He condensed elements of the original book while enlarging others, some of which included eliminating Cruella's husband and cat, as well compressing the two surrogate mother dogs into one character, Perdita.[9] He also retained a scene in which Pongo and Perdita exchange wedding vows in unison with their owners, by which the censor board warned that it might offend certain religious audiences if the animals repeated the exact words of a solemn religious ceremony. The scene was reworked to be less religious with Roger and Anita dressed in formal clothes.[10]\\r\\nTwo months later, Peet completed the manuscript and had it typed up. Walt said the script was \\"great stuff\\" and commissioned Peet to begin storyboarding. Additionally, Peet was charged with the recording of the voice-over process.[8] Although Disney had not been as involved in the production of the animated films as frequently as in previous years, nevertheless, he was always present at story meetings.[11] When Peet sent Dodie Smith some drawings of the characters, she wrote back saying that he had actually improved her story and that the designs looked better than the illustrations in the book.[6]\\r\\nAfter Sleeping Beauty (1959) disappointed at the box-office, there was some talk of closing down the animation department at the Disney studio.[11] During the production of it, Disney told animator Eric Larson: \\"I don't think we can continue, it's too expensive\\".[9] Despite this, he still had deep feelings towards animation because he had built the company upon it.[11]\\r\\nUb Iwerks, in charge of special processes at the studio, had been experimenting with Xerox photography to aid in animation. By 1959, he had modified a Xerox camera to transfer drawings by animators directly to animation cels, eliminating the inking process, thus saving time and money while preserving the spontaneity of the penciled elements.[12][7] However, because of its limitations, the camera was unable to deviate from a black scratchy outline and lacked the fine lavish quality of hand inking.[12] Disney would first use the Xerox process for a thorn forest in Sleeping Beauty,[9] and the first production to make full use of the process was Goliath II.[12] For One Hundred and One Dalmatians, one of the benefits of the process was that it was a great help towards animating the spotted dogs. According to Chuck Jones, Disney was able to complete the film for about half of what it would have cost if they had had to animate all the dogs and spots.[13]\\r\\nKen Anderson proposed the use of the Xerox on Dalmatians to Walt, whom by then disenchanted with animation, replied \\"Ah, yeah, yeah, you can fool around all you want to.\\"[14] For the stylized art direction, Anderson took inspiration from British cartoonist Ronald Searle,[15] whom once advised him to use a Mont Blanc pen and Indian ink for his artwork. In addition to the character animation, Anderson also sought to use Xerography on \\"the background painting because I was going to apply the same technique to the whole picture.\\"[14] Along with color stylist Walt Peregoy, the two had the line drawings be printed on a separate animation cel before being laid over the background, which gave the appearance similar to the Xeroxed animation.[11][16] Disney disliked the artistic look of the film and felt he was losing the \\"fantasy\\" element of his animated films.[11] In a meeting with Anderson and the animation staff concerning future films, Walt said, \\"We're never gonna have one of those goddamned things\\" referring to Dalmatians and its technique, and stated, \\"Ken's never going to be an art director again.\\"[14]\\r\\nKen Anderson took this to heart, but Walt eventually forgave him on his final trip to the studio in late 1966. As Anderson recalled in an interview:\\r\\nHe looked very sick, I said \\"Gee it's great to see you Walt\\", and he said \\"You know that thing you did on Dalmatians\\". He didn't say anything else, but he just gave me this look and I knew that all was forgiven and in his opinion maybe what I did on Dalmatians wasn't so bad. That was the last time I ever saw him. Then, a few weeks later, I learned he was gone.[11]\\r\\nAs with the previous Disney films, actors provided live-action reference in order to determine what would work before the animation process begun. Actress Helene Stanley performed the live-action reference for the character of Anita. She did the same work for the characters of Cinderella and Princess Aurora in Sleeping Beauty.[17] Meanwhile, Mary Wickes provided the live-action reference for Cruella de Vil.[18]\\r\\nMarc Davis was the sole animator on Cruella De Vil. During production, Davis claimed her character was partly inspired by Bette Davis (no relation), Rosalind Russell, and Tallulah Bankhead. He took further influence from her voice actress, Betty Lou Gerson, with whom he added her cheekbones to the character. He later complimented \\"[t]hat [her] voice was the greatest thing I've ever had a chance to work with. A voice like Betty Lou's gives you something to do. You get a performance going there, and if you don't take advantage of it, you're off your rocker.\\"[19] While her hair coloring originated from the illustrations in the novel, Davis found its disheveled style by looking \\"through old magazines for hairdos from 1940 till now.\\" Her coat was exaggerated to match her over-sized personality, and the lining was red because \\"there's a devil image involved.\\"[20]\\r\\nBefore starring in high-profile roles such as The Birds and The Time Machine, Australian actor Rod Taylor had extensive radio experience, and was cast as Pongo. The filmmakers deliberately cast dogs with deeper voices than their human owners so they had more power.[21] Walt Disney originally had Lisa Davis read for the role of Cruella De Vil, but she did not think that she was right for the part, and wanted to try reading the role of Anita. Disney agreed with her after the two of them read the script for a second time.[22]\\r\\nBetty Lou Gerson, who was previously the narrator for Cinderella, auditioned for the role of Cruella De Vil in front of Marc Davis and sequence director Wolfgang Reitherman, and landed it.[23] While searching for the right accent of the character, Gerson landed on a \\"phony theatrical voice, someone who's set sail from New York but hasn't quite reached England.\\"[24] During the recording process, she was thought to be imitating Talluhah Bankhead. However, Gerson disputed, \\"Well, I didn't intentionally imitate her...I was raised in Birmingham, Ala., and Tallulah was from Jasper, Ala. We both had phony English accents on top of our Southern accents and a great deal of flair. So our voices came out that way.\\"[25] In addition to voicing Mrs. Birdwell, Gerson finished recording in fourteen days.[23]\\r\\nIn order to have music involved in the narrative, Peet used an old theater trick by which the protagonist is a down-and-out songwriter. However, unlike the previous animated Disney films at the time, the songs were not composed by a team, but by Mel Leven who composed both lyrics and music.[10] Previously, Leven had composed songs for the UPA animation studio in which animators, whom transferred to work at Disney, had recommended him to Walt.[26] His first assignment was to compose \\"Cruella de Vil\\", in which Leven composed three versions of. The final version used in the film was composed as a \\"bluesy number\\" prior to a meeting with Walt in forty-five minutes.[10]\\r\\nThe other two songs included the film are \\"Kanine Krunchies Jingle\\" (sung by Lucille Bliss, who voiced Anastasia Tremaine in Disney's 1950 film Cinderella), and \\"Dalmatian Plantation\\" in which only two lines are sung by Roger at its closure. Leven had also written additional songs that were not included in the film. The first song, \\"Don't Buy a Parrot from a Sailor\\", a cockney chant, meant to be sung by Jasper and Horace at the De Vil Mansion. A second song, \\"Cheerio, Good-Bye, Toodle-oo, Hip Hip!\\" was to be sung by the dalmatian puppies as they make their way into London.[10] A third song titled \\"March of the One Hundred and One\\" was meant for the dogs to sing after escaping Cruella by van. Different, longer versions of \\"Kanine Krunchies Jingle\\" and \\"Dalmatian Plantation\\" appear on the Disneyland Records read-along album based on the film.[27]\\r\\nOne Hundred and One Dalmatians was first released in theaters on January 25, 1961. The film was re-released theatrically in 1969, 1979, 1985, and 1991.[28]\\r\\nhe film was released on VHS on April 10, 1992, as part of the Walt Disney Classics video series.[29] By June 1992, it had sold 11.1 million copies.[30] At the time, it was the sixth best-selling video of all time.[31] It was re-released on March 9, 1999, as part of the Walt Disney Masterpiece Collection video series. Due to technical issues, it was never released on Laserdisc and was delayed numerous times before its release on DVD. The film was re-released on VHS, and for the first time on DVD, on December 1999, as a Walt Disney Limited Issue for a limited sixty-day time period before going into moratorium.[32] A two-disc Platinum Edition DVD was released on March 4, 2008. It was released on Blu-ray Disc in the United Kingdom on September 3, 2012.[33] A Blu-ray Diamond Edition of it was released in North America on February 10, 2015.\\r\\nOn its initial release, the film grossed $6.2 million in domestic rentals.[34] It was also the most popular film of the year in France, with admissions of 14.7 million ranking 10th on their all time list.[35] [36] The film grossed $14 million during its initial run.[37] The film was re-released in 1969, where it earned $15 million. In its 1979 theatrical re-release, it grossed $19 million, and in 1985, the film grossed $32 million.[37] During its fourth re-release in 1991, it grossed $60.8 million.[38] The film's total domestic lifetime gross is $145 million,[31] and its total worldwide gross is $215 million.[2] Adjusted for inflation, and incorporating subsequent releases, the film has a lifetime gross of $888,264,400.[39]\\r\\nIn its initial release, the film received critical acclaim from critics, many of whom hailed it as the studio's best release since Snow White and the Seven Dwarfs (1937) and the closest to a real \\"Disney\\" film in many years.[40] Howard Thompson of The New York Times wrote, \\"While the story moves steadily toward a stark, melodramatic \\"chase\\" climax, it remains enclosed in a typical Disney frame of warm family love, human and canine.\\" However, he later opined that the \\"[s]ongs are scarce, too. A few more would have braced the final starkness.\\"[41] Variety claimed that \\"While not as indelibly enchanting or inspired as some of the studio's most unforgettable animated endeavors, this is nonetheless a painstaking creative effort.\\"[42] Time praised the film as \\"the wittiest, most charming, least pretentious cartoon feature Walt Disney has ever made.\\"[43]\\r\\nContemporary reviews have remained positive. Reviewing the film during its 1991 re-release, Roger Ebert of the Chicago Sun-Times, while giving the film three stars out of four, asserted that \\"it's an uneven film, with moments of inspiration in a fairly conventional tale of kidnapping and rescue. This is not one of the great Disney classics - it's not in the same league with Snow White or Pinocchio - but it's passable fun, and will entertain its target family audiences.\\"[44] Chicago Tribune film critic Gene Siskel, in his 1991 review, also gave the film three stars out of four.[45] Ralph Novak of People wrote \\"What it lacks in romantic extravagance and plush spectacle, this 1961 Disney film makes up for in quiet charm and subtlety. In fact, if any movie with dogs, cats and horses who talk can be said to belong in the realm of realistic drama, this is it.\\"[46] In 2011, Craig Berman of MSNBC ranked it and its 1996 remake as two of the worst children's films of all time saying, \\"The plot itself is a bit nutty. Making a coat out of dogs? Who does that? But worse than Cruella de Vil's fashion sense is the fact that your children will definitely start asking for a Dalmatian of their own for their next birthday\\".[47]\\r\\nThe review aggregator website Rotten Tomatoes reported the film received an approval rating of 98% based on 30 reviews with an average score of 8.1/10. The website's critical consensus reads, \\"With plenty of pooches and a memorable villain (Cruella De Vil), this is one of Disney's most enduring, entertaining animated films.\\"[48]\\r\\nIn the years since the original release of the film, Disney has taken the property in various directions. The earliest of these endeavors was the live-action remake, 101 Dalmatians (1996). Starring Glenn Close as Cruella De Vil, none of the animals talked in this version. Its success in theaters led to 102 Dalmatians, released on November 22, 2000.\\r\\nAfter the first live-action version of the film, an animated series titled 101 Dalmatians: The Series was launched. The designs of the characters were stylized further to allow for economic animation and to appeal to contemporary trends.\\r\\n101 Dalmatians II: Patch's London Adventure, the official sequel to the original animated film, was released direct-to-video on January 21, 2003.[51]\\r\\nDisney has announced that another live-action film is in development, but it will focus on the origin of Cruella de Vil.[52] Emma Stone is set to play Cruella and Alex Timbers is in negotiations to direct the film.[53][54]","input":"How many puppies did the mom have in 101 dalmatians?"},{"output":"Tyronn Jamar Lue","context":"As player:\\r\\n\\r\\nAs coach:\\r\\n\\r\\nTyronn Jamar Lue (/t??r?n ?lju?/, born May 3, 1977) is an American professional basketball coach and former player. He is the current head coach of the Cleveland Cavaliers of the National Basketball Association (NBA).\\r\\n\\r\\nThe 6?ft 0?in (1.83?m), 175?lb (79?kg) point guard was selected out of the University of Nebraska by the Denver Nuggets with the 23rd overall pick in the 1998 NBA draft and was traded shortly thereafter to the Los Angeles Lakers, where he won two NBA Championships in his first three seasons.\\r\\n\\r\\nAfter his playing career ended in 2009, Lue became Director of Basketball Development for the Boston Celtics.[1] In 2014, he was hired by the Cavaliers as associate head coach and was promoted to head coach midseason in 2015ÿ16, replacing the fired David Blatt.[2] That same season, Lue led the Cavaliers to their first NBA championship and became one of the few rookie coaches in the NBA to ever lead his team to a title.\\r\\n\\r\\nLue attended Raytown Senior High School in Raytown, Missouri. He later went to University of Nebraska. He played basketball and studied sociology. He was a key member of the 1995-96 team that won the NIT, defeating St. Joseph's in the finals.[3] He finished his Nebraska career ranked third all-time in assists (432), fourth in three-pointers made (145) and attempted (407), fifth in steals (154) and seventh in scoring (1,577). Declaring for the NBA draft after his junior season, he led the Cornhuskers in assists in each of his three seasons and finished his career tied with Dave Hoppen for most games with 30 or more points (7).\\r\\n\\r\\nLue opted for early entry into the 1998 NBA draft. He was selected 23rd overall by the Denver Nuggets but was traded on draft night to the Los Angeles Lakers with Tony Battie in exchange for Nick Van Exel. His first three years with the Lakers were disappointing. His playing time was limited and he suffered from injuries in 2000. But Lue excelled in the 2001 playoffs. Due to his quickness, he was specifically used to guard Allen Iverson during Game 1 of the Finals. The Lakers lost Game 1, but swept the next four games, giving them the second of three consecutive titles.\\r\\n\\r\\nIn the off-season of 2001, Lue signed with the Washington Wizards, where he got considerably more playing time and subsequently became a better point guard. He played with the Orlando Magic in 2003ÿ04 and had a lot of minutes alongside Tracy McGrady, but the team had the worst record in the NBA that season: 21ÿ61. After the season Lue, Juwan Howard and McGrady were traded to the Houston Rockets for Steve Francis, Cuttino Mobley and Kelvin Cato. In Houston, Lue did not get much playing time because of the number of point guards the Rockets had on their roster. He was traded mid-season to the Atlanta Hawks for Jon Barry. Lue starred in Atlanta, although again his team had the worst record in the NBA and their worst record in franchise history: 13ÿ69.\\r\\n\\r\\nOn February 16, 2008, Lue was acquired by the Sacramento Kings in a trade with the Hawks. He was waived by the Kings on February 28, 2008. After clearing waivers, Lue signed a contract with the Dallas Mavericks on March 4.[4]\\r\\n\\r\\nOn July 17, 2008, Lue was signed by the Milwaukee Bucks.[5]\\r\\n\\r\\nOn February 5, 2009, Lue was traded back to the Magic in exchange for Keith Bogans and cash considerations.[6]\\r\\n\\r\\nOn October 23, 2009, Boston named Lue director of basketball development.[1] In July 2013, he joined the Los Angeles Clippers' coaching staff.[7]\\r\\n\\r\\nOn June 23, 2014, Lue joined the Cleveland Cavaliers as associate head coach, making him the highest-paid assistant coach in the NBA in the process. Lue had been a top candidate for the Cavs' head coaching job, which eventually went to David Blatt.[8]\\r\\n\\r\\nOn January 22, 2016, Lue was named head coach of the Cavaliers immediately following the mid-season firing of Blatt.[9] The contract was for three years.[2]\\r\\n\\r\\nOn May 19, 2016, the Cavaliers defeated the Toronto Raptors in Game 2 of the Eastern Conference Finals, continuing their unbeaten streak in the 2016 playoffs and making Lue the first coach in NBA history to win his first 10 postseason games.[10] Eight days later, Lue led the Cavaliers to the NBA Finals in his first year of coaching, becoming one of the few coaches to make it to the Finals after replacing a head coach during the regular season. On June 19, 2016, the Cavaliers won their first NBA Championship, with Lue becoming the second rookie coach in consecutive seasons to win it all, the third person to become champion as a mid-season replacement coach (after Paul Westhead in 1979ÿ80, and Pat Riley in 1981ÿ82, also his rookie coaching season), and the 14th person to win an NBA championship as a head coach and player.[11][12]\\r\\n\\r\\nIn the 2016ÿ17 NBA season, Lue coached the Cavaliers to a 51ÿ31 record. In the playoffs, the Cavaliers went 12ÿ1 heading into the 2017 NBA Finals before losing to the Golden State Warriors in five games.\\r\\n\\r\\nOn March 19, 2018, Lue announced that he would take a leave of absence from coaching the Cavaliers, citing chest pain as one of the recurring problems.[13] Lue returned to coach before the regular season ended and helped the Cavaliers reach the 2018 NBA Finals where they lost to the Warriors in four games.\\r\\n\\r\\nLue's coaching style relies on flexibility and James's consistency while shuffling players around him to adjust to matchups. In the 2015 Finals as an assistant to David Blatt, the Cavaliers used a large front court and a slow pace to win surprise victories in games 2 and 3. In 2016, his finals team followed the Warriors' own blueprint to beat them. His style has been described as undisciplined and unprepared in the regular season, but in the playoffs he has been praised for his ability to \\"think several moves ahead and create matchup advantages.\\"[14]\\r\\n\\r\\nAt the 2016 ESPY Awards, Lue was named Best Coach/Manager, and the Cavs were named Best Team. In honor of his achievements, a portion of Walnut Street in Lue's hometown of Mexico, Missouri, was renamed Tyronn Lue Boulevard.[15]\\r\\n\\r\\nAs player:\\r\\n\\r\\nAs coach:\\r\\n\\r\\n(#) denotes interim head coach.","input":"Who's the coach of the cleveland cavaliers?"},{"output":"Despacito","context":"YouTube is an American video-sharing website headquartered in San Bruno, California. Since its establishment in 2005, the website has featured a \\"most viewed\\" section, which lists the most viewed videos on the site. Although the most viewed videos were initially viral videos, such as Evolution of Dance and Charlie Bit My Finger, the most viewed videos were increasingly related to music videos. In fact, since Lady Gaga's Bad Romance, every single video that has reached the top of the \\"most viewed YouTube videos\\" list has been a music video. Although the most viewed videos are no longer listed on the site, reaching the top of the list is still considered a tremendous feat.\\r\\nIn June 2015, only two videos, \\"Gangnam Style\\" and \\"Baby\\", had reached over a billion views. Three months later, however, ten videos had done so.[1] As of October 2017, 79 videos on the list have exceeded one billion views, with 16 of them exceeding two billion views; two of which exceed three billion views. \\"Despacito\\" became the first video to reach three billion views on August 4, 2017, followed by \\"See You Again\\" on August 6, 2017.[2][3]\\r\\nAs of October 2017, the five fastest videos to reach the one billion view mark are \\"Hello\\" (87 days), \\"Despacito\\" (97 days), \\"Shape of You\\" (97 days), \\"Sorry\\" (137 days), and \\"Chantaje\\" (140 days).[4]\\r\\nThe five fastest videos to reach two billion views are \\"Despacito\\" (154 days), \\"Shape of You\\" (188 days), \\"Sorry\\" (394 days), \\"See You Again\\" (515 days) and \\"Hello\\" (620 days).[5]\\r\\nAs of October 2017, Justin Bieber and Katy Perry each have four videos exceeding one billion views, while Taylor Swift, Calvin Harris, Shakira and Ariana Grande each have three, and Fifth Harmony, Psy, Adele, Ellie Goulding, The Weeknd, Ed Sheeran, Nicky Jam, Eminem and Maluma each have two. Swift and Perry are the only artists to have two videos exceeding two billion views.\\r\\n\\r\\n\\r\\nThe following table lists the top 100 most viewed videos on YouTube, with each total rounded to the nearest 10 million views, as well as the creator and date of publication to YouTube.[A]\\r\\nThe following table lists the current top 5 most viewed YouTube videos uploaded in each year, with each total rounded to the nearest million views, as well as the uploader and date of publication to YouTube.[J]\\r\\nAs of October 2017, only Linkin Park (2007), Gummib?r/icanrockyourworld (2007) and Taylor Swift (2014) have two videos in the top 5 of a single year, with both the English and French versions of Gummib?r's The Gummy Bear Song being in the top five videos of 2007.\\r\\nThe following table lists the last 15 videos to become YouTube's most viewed video, from October 2005 to the present.\\r\\n*The approximate number of views each video had when it became YouTube's most viewed video.\\r\\nTimeline of Most Viewed Videos (Oct 2005 - Oct 2017)\\r\\n 1 Most Viewed Video (Oct 2005 - Jun 2006)\\r\\n 1 Most Viewed Video (Apr 2006 - Jan 2010)\\r\\n 1 Most Viewed Video (Oct 2009 - Jan 2013)\\r\\n 1 Most Viewed Video (Jan 2012 - Oct 2017)","input":"What is the most streamed song on youtube?"},{"output":"green","context":"St. Patrick's blue is a name applied to several shades of blue associated with Saint Patrick and Ireland. The colour blue's association with Saint Patrick dates from the 1780s, when it was adopted as the colour of the Anglo-Irish \\"Order of St. Patrick\\". In British usage, it refers to a sky blue used by the Order of St. Patrick, whereas in Irish usage it is often a dark, rich blue.[1][2] While green is now the usual national colour of Ireland. St. Patrick's blue is still found in symbols of both the state and the island.[3]\\r\\n\\r\\n\\r\\nThe Order of St. Patrick was established in 1783 as the senior order of chivalry in the Kingdom of Ireland. The colour of its honours needed to differ from those of the Order of the Garter (dark blue) and the Order of the Thistle (green). Orange was considered, but the association with orangeism felt to be too sectarian, so the lighter blue of the arms of Ireland was chosen.[4] Knights and officers of the order wore a \\"sky blue\\" mantle and riband, a hat lined with \\"blue\\", and a badge ringed with \\"blue\\" enamel.[5] The name \\"St. Patrick's blue\\" was common but never officially used by the Order.[1][6] The exact shade of blue used varied over time. A sky blue tinged with green was used by Lord Iveagh in 1895 and confirmed in 1903.[1]\\r\\nThere has been debate over the extent to which blue was a national colour of Ireland prior to the creation of the Order, and whether it was associated with Saint Patrick himself independently of the Order. Jim Smyth characterised the Order's adoption of St. Patrick's Blue and Saint Patrick's Saltire as examples of invention of tradition.[7] Shane Leslie speculated that the green-blue of St Patrick's blue might be \\"but a reminiscence of the woad-stain used by all colour-loving Celts\\".[8] Constance Markievicz believed blue was \\"the old colour of Ireland\\" and incorporated it in the regalia of the Irish Citizen Army (ICA).[9] The ICA banner, the Starry Plough, has a blue field. Antiquarian nationalist Francis Joseph Bigger considered St. Patrick's blue a \\"fake colour\\" and Saint Patrick's Flag a \\"fake flag\\".[10] More recently, Peter Alter[11] and Christina Mahony[12] have supported the historicity of the colour, while Brian ܇ Cuv questioned it.[3]\\r\\nThe Irish arms used by English monarchs since Edward IV had an azure field; originally the device was three crowns (now the arms of Munster) until Henry VIII changed it to a harp. This is still the arms of the modern Irish state, and also appears in the lower left quarter of the Royal Standard of the United Kingdom.[13] In Irish mythology, Flaitheas ireann, the sovereignty of Ireland, was sometimes represented as a woman in a blue robe.[14] Although the arms of the province of Mide has a blue field, when its device was used as the arms of Ireland, the field was sable.[14] The Irish College in Paris, completed in 1776,[15] was renovated in 2002; the paint uncovered on the chapel walls was described as \\"St Patrick's blue\\" by a visiting journalist.[16] As regards green in association with Patrick: in 1681, Thomas Dineley reported people wearing crosses of green ribbon in their hats on Saint Patrick's Day.[17]\\r\\nAt a \\"National Ball\\" during Edward, Prince of Wales' 1868 visit to Ireland, his wife Alexandra wore a dress of \\"St Patrick blue\\".[18] In 1886, a garden party given by the Lord Lieutenant of Ireland to showcase Irish manufacturing had an Irish-themed dress code. The Freeman's Journal criticised some of the code as difficult to comply with, but said 'Irish poplin ties of \\"St Patrick's Blue\\"which we think looks rather green in a certain lightmay [...] be had without much strain.'[19] The Guardian's report of the party stated 'the display of the new colour, \\"St. Patrick's Blue,\\" was everywhere visible.'[20] The 1912 court uniform and dress code specified that the household of the Lord Lieutenant of Ireland should wear St. Patrick's blue,[21] as should Pages of Honour when the King was in Ireland.[22]\\r\\nThe Ireland association football team organised by the Irish Football Association (IFA) wore St Patrick's blue jerseys from 1882 until 1931, when they switched to green.[23] The IFA team is now the Northern Ireland team. The Football Association of Ireland sent an Irish Free State team to the 1924 Olympic football tournament; it wore a St Patrick's Blue change strip against Bulgaria, whose strip was Ireland's usual green.[24]\\r\\nIn the 1930s, the Army Comrades Association's Saint Patrick's blue shirts earned it the nickname of Blueshirts. It was a quasi-Fascist shirted movement which rejected green as associated with its republican opponents.[25] The saltire flag of the Blueshirts was a variant of Saint Patrick's Flag with the white background replaced with a blue background. W. T. Cosgrave described the colour as \\"in perfect, traditional, national accord with our history and in close association with the most revered and venerated memory of our patron Saint\\".[26]\\r\\nThe Irish Army Band's first uniform was St Patrick's blue, but this was soon changed to navy.[27] The Mounted Escort ceremonial cavalry of 1932ÿ48 were nicknamed \\"Blue Hussars\\" from their uniforms, whose colour was sometimes described as St. Patrick's blue.[28][29][30] The uniform introduced in 1970 for Aer Lingus air hostesses and ground crew[31] combined green and St Patrick's Blue, described in The Irish Times as \\"a sparkling new colour\\".[32] The 1970 uniform was replaced in 1975, after a design consultancy developed a common corporate image with a colour scheme of dark bottle green, bright green, and \\"a strong blue\\".[33]\\r\\nThe coat of arms of Ireland and the Standard of the President of Ireland are a gold (or) Irish harp with silver (argent) strings on a field of blue (azure).[34] The standard was introduced at the end of Douglas Hyde's term in 1945;[35] contemporary news reports describe the blue as \\"St. Patrick's Blue\\".[36][37] The arms were granted by the Chief Herald of Ireland on 9 November 1945.[38] Horses owned by the Irish National Stud are regarded as owned by the President and entitled to run in the Presidential colours.[39] The racing colours are \\"Saint Patrick's blue with gold sleeves, and a St Patrick's blue cap with gold tassel\\".[40] One such horse is Suailce,[41] which won the 2008 Irish Cesarewich.[42]\\r\\nThe official sporting colours of University College Dublin are \\"St. Patrick's Blue and Saffron\\", adopted in 1910.[43] The blue is commonly interpreted as 'light' or 'Dublin' blue;[43] the GAA county colours of County Dublin include light blue jerseys. In the National University of Ireland's academic dress code, \\"Saint Patrick's Blue\\" is the colour of the faculty of Science; Veterinary Medicine has a darker \\"Celtic Blue\\".[44] The academical dress of the Royal College of Surgeons of Ireland also features St Patrick's blue.[45] The Trinity College Dublin fencing club specifies that the azure in its colours is \\"St. Patrick's Blue (Pantone 295 as the Presedential [sic] Pennant)\\".[46]\\r\\nAmong Irish regiments of the British Army, a hackle of St. Patrick's blue is worn in the bearskins of the Irish Guards[47] and in the caubeens of the London Irish Rifles.[48] The Guards' blue was chosen in distinction to the Royal Irish Fusiliers' green hackle.[49] St Patrick's Cathedral, Dublin commemorates its historic association with the Order of Saint Patrick with St Patrick's blue on the cassocks of the choristers and under the clerical collars of the Dean and the Vicar.[50]\\r\\nA cross-border flag for Ireland may be required where a sporting team combines athletes from both the Republic of Ireland and Northern Ireland. The arms of the four provinces of Ireland on a background of Saint Patrick's blue has sometimes served this purpose.[51]","input":"What is the color of st patrick day?"},{"output":"World Golf Village near St. Augustine, Florida, in the United States","context":"The World Golf Hall of Fame is located at World Golf Village near St. Augustine, Florida, in the United States, and it is unusual among sports halls of fame in that a single site honors both men and women. It is supported by a consortium of 26 golf organizations from all over the world.[1]\\r\\n\\r\\nThe Hall of Fame Museum Building is designed by the museum architecture specialist firm of E. Verner Johnson and Associates of Boston, Massachusetts.  They also produced the museum master plan that established the overall size, mission and qualities of the overall museum and the surrounding facilities and site.\\r\\n\\r\\nThe Hall of Fame Museum features a permanent exhibition and a rolling program of temporary exhibitions. Designed by museum design firm Ralph Appelbaum Associates, the Hall of Fame and exhibition area contains exhibits on the game's history, heritage, and techniques; major players and organizations; golf course design, equipment, and dress; and new directions, such as ecological concerns in course management.[2]\\r\\n\\r\\nThe World Golf Hall of Fame was originally located in Pinehurst, North Carolina, and was privately operated by Diamondhead Corp., then owners of the Pinehurst Resort. It opened in September 1974 with an initial class of 13 members.[3] Initially it was a local project, but the PGA of America took over management in 1983 and acquired full ownership in 1986.\\r\\n\\r\\nTwo other halls of fame have been merged into the World Golf Hall of Fame. The PGA of America established one in 1940, which was merged into the Pinehurst Hall in the 1980s. The Hall of Fame of Women's Golf was established by the LPGA in 1951, with four charter members:  Patty Berg, Betty Jameson, Louise Suggs, and Babe Zaharias. It was inactive for some years, but in 1967 it moved into its first physical premises, which were in Augusta, Georgia and was renamed the LPGA Tour Hall of Fame. In 1998 it merged into the World Golf Hall of Fame.\\r\\n\\r\\nIn 1994 the global golf industry established a non-profit making body called the World Golf Foundation to promote the sport, with the creation of an enhanced Hall of Fame as one of its main objectives. Construction at the new site in St. Johns County began in 1996 and the new facility opened on May 19, 1998.\\r\\n\\r\\nIn October 2013, the Hall announced that it was reviewing its selection process and that there would be no induction ceremony in 2014.[4][5] A new process was announced in March 2014.\\r\\n\\r\\nStarting in 2014, members are inducted into the Hall of Fame in one of four categories: Male Competitor, Female Competitor, Veterans, and Lifetime Achievement categories. Elections are held every other year with induction ceremonies in odd number years beginning in 2015. The process has changed from that used from 1996 to 2013. The minimum qualifications for male and female competitors are: minimum of 40 years old, or five years removed from \\"active competition\\" and 15 or more wins on \\"approved tours\\" or two \\"major wins\\". The veterans category is primarily for those golfers whose careers ended before 1980 and includes both amateurs and professionals. The lifetime achievement category remains from the old system.[6]\\r\\n\\r\\nA 20-member selection sub-committee will choose from among the eligible candidates and select ballots for a selection committee. There will be five names each on the male and female ballots and three names each on the veterans and lifetime achievement ballots. A separate 16-member selection committee will then vote on all four ballots. Election to the Hall of Fame will require 75% of the vote and each year's election class is limited to two from each ballot and five total.[6][7]\\r\\n\\r\\nIn 2016, the Hall announced that the age requirement would be raised to 50 from 40 years old.[8]\\r\\n\\r\\nA player must have turned 50 years old prior to January 1 of the year the ballots are constructed (2016 for the 2017 induction). The \\"active competition\\" requirement will be determined by each \\"approved tour\\" that the player is/was a member of.\\r\\n\\r\\nFrom 1996 to 2013, members were inducted into the Hall of Fame in one of five categories: PGA Tour/Champions Tour, LPGA Tour, International, Lifetime Achievement, and Veterans.\\r\\n\\r\\nCurrent and former PGA Tour and Champions Tour players were eligible for this ballot if they met the following requirements (beginning with 1996 election):\\r\\n\\r\\nElection requirements:\\r\\n\\r\\nVoters voted for up to 30% of the players on the ballot. If a player was named on less than 5% of the ballots for two consecutive years, they were dropped from the ballot. Players not elected could remain on the ballot indefinitely[9] (prior to 2007 the limit was 10 years, from 2007 to 2009 the limit was 15 years).[10]\\r\\n\\r\\nLPGA Tour golfers were eligible through a point system. Since 1999, LPGA members automatically qualified for World Golf Hall of Fame membership when they meet these three criteria:\\r\\n\\r\\nBefore 1999, players had to win 30 tournaments, including two majors; 35 tournaments with one major; or 40 tournaments in all to automatically qualify. At one time, players had to win two different majors to qualify with 30 wins, but this was changed earlier in the 1990s.\\r\\n\\r\\nThis point system is still used for selection to the LPGA Hall of Fame.[11]\\r\\n\\r\\nMen and women golfers not fully eligible for PGA/Champions Tour ballot or the LPGA Tour point system were eligible for the International ballot if they met the following requirements[12] (beginning with the 1996 election):\\r\\n\\r\\nElection requirements: same as PGA Tour ballot.\\r\\n\\r\\nThere was also a \\"lifetime achievement\\" category through which anyone who had made a major contribution to the organization or promotion of the sport may be selected, for example, Bob Hope. These members were chosen by the Hall of Fame's Board of Directors. Most played golf, in some cases with some competitive success, but it was not their play alone which won them a place in the Hall of Fame.\\r\\n\\r\\nThe last category was created to honor professional or amateur players whose career concluded at least 30 years ago. These members were also chosen by the Hall of Fame's Board of Directors.\\r\\n\\r\\nNew members are inducted each year on the Monday before The Players Championship[13] (previous to 2010 in October or November), and by May 2013 there were 146 members. Beginning in 2010, the ballots are due in July with the results announced later in the year. New entrants in the Lifetime Achievement and Veteran's categories are announced at irregular intervals. For example, Frank Chirkinian was elected in the Lifetime Achievement category in an emergency election in February 2011, with the vote presumably held because he was then terminally ill with lung cancer;[14] when it became clear he would not live to attend his induction, he videotaped his acceptance speech in late February, less than two weeks before his death.[15]\\r\\n\\r\\nUnless stated otherwise these men were inducted mainly for their on-course success. The exceptions mostly correspond with the lifetime achievement category, but not quite. For example, Charlie Sifford was notable as a player but was inducted for lifetime achievement.\\r\\n\\r\\nThe first five women on this list were grandfathered in 1998 from the Hall of Fame of Women's Golf, which was founded in 1951, via the LPGA Tour Hall of Fame, which was inaugurated in 1967. The list shows the years when they were originally inducted into the Hall of Fame of Women's Golf. Unless stated otherwise the women on the list were inducted primarily for their on-course achievements.\\r\\n\\r\\nCoordinates: 295928N 812813W? / ?29.99111N 81.47028W? / 29.99111; -81.47028","input":"Where is the golf hall of fame located?"},{"output":"50 First Dates","context":"\\r\\n\\r\\nBlu-ray or Blu-ray Disc (BD) is a digital optical disc data storage format. It was designed to supersede the DVD format, and is capable of storing several hours of video in high-definition (HDTV 720p and 1080p) and ultra high-definition resolution (2160p). The main application of Blu-ray is as a medium for video material such as feature films and for the physical distribution of video games for the PlayStation 3, PlayStation 4, and Xbox One. The name \\"Blu-ray\\" refers to the blue laser (actually a violet laser) used to read the disc, which allows information to be stored at a greater density than is possible with the longer-wavelength red laser used for DVDs.\\r\\n\\r\\nThe plastic disc is 120 millimetres (4.7?in) in diameter and 1.2 millimetres (0.047?in) thick, the same size as DVDs and CDs.[5] Conventional or pre-BD-XL Blu-ray discs contain 25?GB per layer, with dual-layer discs (50?GB) being the industry standard for feature-length video discs. Triple-layer discs (100?GB) and quadruple-layer discs (128 GB) are available for BD-XL re-writer drives.[6]\\r\\n\\r\\nHigh-definition (HD) video may be stored on Blu-ray discs with up to 2160p resolution (3840G2160 pixels) and at up to 60 frames per second. DVD-Video discs were limited to a maximum resolution of 480p (NTSC, 720G480 pixels) or 576p (PAL, 720G576 pixels).[7] Besides these hardware specifications, Blu-ray is associated with a set of multimedia formats.\\r\\n\\r\\nThe BD format was developed by the Blu-ray Disc Association, a group representing makers of consumer electronics, computer hardware, and motion pictures. Sony unveiled the first Blu-ray disc prototypes in October 2000, and the first prototype player was released in April 2003 in Japan. Afterwards, it continued to be developed until its official release on June 20, 2006, beginning the high-definition optical disc format war, where Blu-ray Disc competed with the HD DVD format. Toshiba, the main company supporting HD DVD, conceded in February 2008,[8] and later released its own Blu-ray Disc player in late 2009.[9] According to Media Research, high-definition software sales in the United States were slower in the first two years than DVD software sales.[10] Blu-ray faces competition from video on demand (VOD) and the continued sale of DVDs.[11] Notably, as of January 2016, 44% of U.S. broadband households had a Blu-ray player.[12]\\r\\n\\r\\nThe information density of the DVD format was limited by the wavelength of the laser diodes used. Following protracted development, blue laser diodes operating at 405 nanometers became available on a production basis, allowing for development of a more-dense storage format that could hold higher-definition media. Sony started two projects in collaboration with Philips[13] applying the new diodes: UDO (Ultra Density Optical),[14] and DVR Blue (together with Pioneer),[15] a format of rewritable discs that would eventually become Blu-ray Disc (more specifically, BD-RE). The core technologies of the formats are similar. The first DVR Blue prototypes were unveiled at the CEATEC exhibition in October 2000 by Sony.[16] A trademark for the \\"Blue Disc\\" logo was filed February 9, 2001.[17] On February 19, 2002, the project was officially announced as Blu-ray Disc,[18][19] and Blu-ray Disc Founders was founded by the nine initial members.\\r\\n\\r\\nThe first consumer device arrived in stores on April 10, 2003: the Sony BDZ-S77, a US$3,800 BD-RE recorder that was made available only in Japan.[20] But there was no standard for prerecorded video, and no movies were released for this player. Hollywood studios insisted that players be equipped with digital rights management before they would release movies for the new format, and they wanted a new DRM system that would be more secure than the failed Content Scramble System (CSS) used on DVDs. On October 4, 2004, the name \\"Blu-ray Disc Founders\\" was officially changed to the Blu-ray Disc Association (BDA), and 20th Century Fox joined the BDA's Board of Directors.[21] The Blu-ray Disc physical specifications were completed in 2004.[22]\\r\\n\\r\\nIn January 2005, TDK announced that they had now developed an ultra-hard yet very thin polymer coating (\\"Durabis\\") for Blu-ray discs; this was a significant technical advance because a far tougher protection was desired in the consumer market to protect bare discs against scratching and damage compared to DVD, while technically Blu-ray Disc required a much thinner layer for the denser and higher frequency blue laser.[23] Cartridges, originally used for scratch protection, were no longer necessary and were scrapped. The BD-ROM specifications were finalized in early 2006.[24]\\r\\n\\r\\nAACS LA, a consortium founded in 2004,[25] had been developing the DRM platform that could be used to securely distribute movies to consumers. However, the final AACS standard was delayed,[26] and then delayed again when an important member of the Blu-ray Disc group voiced concerns.[27] At the request of the initial hardware manufacturers, including Toshiba, Pioneer, and Samsung, an interim standard was published that did not include some features, such as managed copy.[28]\\r\\n\\r\\nThe first BD-ROM players (Samsung BD-P1000) were shipped in mid-June 2006, though HD DVD players beat them to market by a few months.[29][30] The first Blu-ray Disc titles were released on June 20, 2006: 50 First Dates, The Fifth Element, Hitch, House of Flying Daggers, Underworld: Evolution, xXx (all Sony), Twister (Warner Bros.), and MGM's The Terminator.[31] The earliest releases used MPEG-2 video compression, the same method used on standard DVDs. The first releases using the newer VC-1 and AVC formats were introduced in September 2006.[32] The first movies using 50?GB dual-layer discs were introduced in October 2006.[33] The first audio-only albums were released in May 2008.[34][35]\\r\\n\\r\\nThe first mass-market Blu-ray Disc rewritable drive for the PC was the BWU-100A, released by Sony on July 18, 2006.[36] It recorded both single and dual-layer BD-Rs as well as BD-REs and had a suggested retail price of US $699. As of  June 2008[update], more than 2,500 Blu-ray Disc titles were available in Australia and the United Kingdom, with 3,500 in the United States and Canada.[37] In Japan, as of  July 2010[update], more than 3,300 titles have been released.[38]\\r\\n\\r\\nThe DVD Forum, chaired by Toshiba, was split over whether to develop the more expensive blue laser technology. In March 2002 the forum approved a proposal, which was endorsed by Warner Bros. and other motion picture studios. The proposal involved compressing high-definition video onto dual-layer standard DVD-9 discs.[39][40] In spite of this decision, however, the DVD Forum's Steering Committee announced in April that it was pursuing its own blue-laser high-definition video solution. In August, Toshiba and NEC announced their competing standard, Advanced Optical Disc.[41] It was finally adopted by the DVD Forum and renamed HD DVD the next year,[42] after being voted down twice by DVD Forum members who were also Blu-ray Disc Association membersa situation that drew preliminary investigations by the U.S. Department of Justice.[43]\\r\\n\\r\\nHD DVD had a head start in the high-definition video market, as Blu-ray Disc sales were slow to gain market share. The first Blu-ray Disc player was perceived as expensive and buggy, and there were few titles available.[44]\\r\\n\\r\\nThe appearance of the Sony PlayStation 3, which contained a Blu-ray Disc player for primary storage, helped support Blu-ray.[45] Sony also ran a more thorough and influential marketing campaign for the format.[46] AVCHD camcorders were also introduced in 2006. These recordings can be played back on many Blu-ray Disc players without re-encoding but are not compatible with HD DVD players. By January 2007, Blu-ray Discs had outsold HD DVDs,[47] and during the first three quarters of 2007, BD outsold HD DVD by about two to one. At CES 2007, Warner proposed Total Hi Defa hybrid disc containing Blu-ray on one side and HD DVD on the other, but it was never released.\\r\\n\\r\\nIn a June 28, 2007, press release, Twentieth Century Fox cited Blu-ray Disc's adoption of the BD+ anticopying system as key to their decision to support the Blu-ray Disc format.[48][49]\\r\\nOn January 4, 2008, a day before CES 2008, Warner Bros. (the only major studio still releasing movies in both HD DVD and Blu-ray Disc format) announced that it would release only in Blu-ray Disc after May 2008.[50] This effectively included other studios that came under the Warner umbrella, such as New Line Cinema and HBOthough in Europe, HBO distribution partner, the BBC, announced it would, while keeping an eye on market forces, continue to release product on both formats. This led to a chain reaction in the industry, with major U.S. retailers such as Best Buy, Walmart, and Circuit City and Canadian chains such as Future Shop dropping HD DVD in their stores. A then major European retailer, Woolworths, dropped HD DVD from its inventory.[51] Netflix and Blockbustermajor DVD rental companiessaid they would no longer carry HD DVD.\\r\\n\\r\\nFollowing these new developments, on February 19, 2008, Toshiba announced it would end production of HD DVD devices,[52] allowing Blu-ray Disc to become the industry standard for high-density optical discs. Universal Studios, the sole major movie studio to back HD DVD since its inception, said shortly after Toshiba's announcement: \\"While Universal values the close partnership we have shared with Toshiba, it is time to turn our focus to releasing new and catalog titles on Blu-ray Disc.\\"[53] Paramount Pictures, which started releasing movies only in HD DVD format during late 2007, also said it would start releasing in Blu-ray Disc. Both studios announced initial Blu-ray lineups in May 2008. With this, all major Hollywood studios supported Blu-ray.[54]\\r\\n\\r\\nAccording to Media Research, high-definition software sales in the US were slower in the first two years than DVD software sales.[10] 16.3 million DVD software units were sold in the first two years (1997ÿ98) compared to 8.3 million high-definition software units (2006ÿ07).[10][55] One reason given for this difference was the smaller marketplace (26.5 million HDTVs in 2007 compared to 100 million SDTVs in 1998).[55] Former HD DVD supporter Microsoft did not make a Blu-ray Disc drive for the Xbox 360.[56] The 360's successor Xbox One features a Blu-ray drive, as does the PS4, with both supporting 3D Blu-ray after later firmware updates.[57][58]\\r\\n\\r\\nShortly after the \\"format war\\" ended, Blu-ray Disc sales began to increase. A study by The NPD Group found that awareness of Blu-ray Disc had reached 60% of U.S. households. Nielsen VideoScan sales numbers showed that for some titles, such as 20th Century Fox's Hitman, up to 14% of total disc sales were from Blu-ray, although the average Blu-ray sales for the first half of the year were only around 5%. In December 2008, the Blu-ray Disc version of The Dark Knight sold 600,000 copies on the first day of its launch in the United States, Canada, and the United Kingdom.[59] A week after the launch, The Dark Knight BD had sold over 1.7 million copies worldwide, making it the first Blu-ray Disc title to sell over a million copies in the first week of release.[60]\\r\\n\\r\\nAccording to Singulus Technologies AG, Blu-ray is being adopted faster than the DVD format was at a similar period in its development. This conclusion was based on the fact that Singulus Technologies has received orders for 21 Blu-ray dual-layer machines during the first quarter of 2008, while 17 DVD machines of this type were made in the same period in 1997.[63] According to GfK Retail and Technology, in the first week of November 2008, sales of Blu-ray recorders surpassed DVD recorders in Japan.[64] According to the Digital Entertainment Group, the number of Blu-ray Disc playback devices (both set-top box and game console) sold in the U.S. had reached 28.5 million by the end of 2010.[62]\\r\\n\\r\\nBlu-ray faces competition from video on demand[65] and from new technologies that allow access to movies on any format or device, such as Digital Entertainment Content Ecosystem or Disney's Keychest.[66] Some commentators have suggested that renting Blu-ray will play a vital part in keeping the technology affordable while allowing it to move forward.[67] In an effort to increase sales, studios are releasing movies in combo packs with Blu-ray Discs and DVDs as well as digital copies that can be played on computers and mobile devices. Some are released on \\"flipper\\" discs with Blu-ray on one side and DVD on the other. Other strategies are to release movies with the special features only on Blu-ray Discs and none on DVDs.\\r\\n\\r\\nThe Holographic Versatile Disc (HVD), described in the ECMA-377 standard, has been in development by The Holography System Development (HSD) Forum using a green writing/reading laser (532?nm) and a red positioning/addressing laser (650?nm). It is to offer MPEG-2, MPEG-4 AVC (H.264), HEVC (H.265), and VC-1 encoding, supporting a maximum storage capacity of 6TB.[68] No systems corresponding to the Ecma International HVD standard have been released.[69] Because the Blu-ray Disc format is upgradable it poses challenges to the adoption of the HVD format. 4K Blu-ray discs and players became available in the first quarter of 2016, having a storage capacity of up to 100?GB.[70][71]\\r\\n\\r\\nAlthough the Blu-ray Disc specification has been finalized, engineers continue to work on advancing the technology. By 2005, quad-layer (128?GB) discs had been demonstrated on a drive with modified optics[72] and standard unaltered optics.[73] Hitachi stated that such a disc could be used to store 7?hours of 32?Mbit/s video (HDTV) or 3?hours and 30?minutes of 64?Mbit/s video (ultra-high-definition television). In August 2006, TDK announced that they had created a working experimental Blu-ray Disc capable of holding 200?GB of data on a single side, using six 33?GB data layers.[74]\\r\\n\\r\\nAlso, behind closed doors at CES 2007, Ritek revealed that they had successfully developed a high-definition optical disc process that extends the disc capacity to ten layers, which increases the capacity of the discs to 250?GB. However, they noted that the major obstacle is that current read/write technology does not allow additional layers.[75] JVC has developed a three-layer technology that allows putting both standard-definition DVD data and HD data on a BD/(standard) DVD combination.[76] This would have enabled the consumer to purchase a disc that can be played on DVD players and can also reveal its HD version when played on a BD player.[77] Japanese optical disc manufacturer Infinity announced the first \\"hybrid\\" Blu-ray Disc/(standard) DVD combo, to be released February 18, 2009. This disc set of the TV series \\"Code Blue\\" featured four hybrid discs containing a single Blu-ray Disc layer (25?GB) and two DVD layers (9?GB) on the same side of the disc.[78]\\r\\n\\r\\nIn January 2007, Hitachi showcased a 100?GB Blu-ray Disc, consisting of four layers containing 25?GB each.[79] Unlike TDK's and Panasonic's 100?GB discs, they claim this disc is readable on standard Blu-ray Disc drives that are currently in circulation, and it is believed that a firmware update is the only requirement to make it readable to current players and drives.[80]\\r\\nIn December 2008, Pioneer Corporation unveiled a 400?GB Blu-ray Disc (containing 16 data layers, 25?GB each) that will be compatible with current players after a firmware update. Its planned launch was in the 2009ÿ10 time frame for ROM and 2010ÿ13 for rewritable discs. Ongoing development was underway to create a 1?TB Blu-ray Disc.[81]\\r\\n\\r\\nAt CES 2009, Panasonic unveiled the DMP-B15, the first portable Blu-ray Disc player, and Sharp introduced the LC-BD60U and LC-BD80U series, the first LCD HDTVs with integrated Blu-ray Disc players. Sharp has also announced that they will sell HDTVs with integrated Blu-ray Disc recorders in the United States by the end of 2009. Set-top box recorders were not being sold in the U.S. for fear of unauthorized copying. However, personal computers with Blu-ray recorder drives were available. On January 1, 2010, Sony, in association with Panasonic, announced plans to increase the storage capacity on their Blu-ray Discs from 25?GB to 33.4?GB via a technology called i-MLSE (Maximum likelihood Sequence Estimation). The higher-capacity discs, according to Sony, would be readable on existing Blu-ray Disc players with a firmware upgrade.[82] This technology is later used on BDXL discs.[83]\\r\\n\\r\\nOn July 20, 2010, the research team of Sony and Japanese Tohoku University announced the joint development of a blue-violet laser,[84] to help create Blu-ray discs with a capacity of 1 TB using only two layers (and potentially more than 1 TB with additional layering). By comparison, the first blue laser was invented in 1996, with the first prototype discs coming four years later.\\r\\n\\r\\n\\r\\nOn January 7, 2013, Sony announced that it would release \\"Mastered in 4K\\" Blu-ray Disc titles which are sourced at 4K and encoded at 1080p.[85] \\"Mastered in 4K\\" Blu-ray Disc titles can be played on existing Blu-ray Disc players and have a larger color space using xvYCC.[85][86] On January 14, 2013, Blu-ray Disc Association president, Andy Parsons, stated that a task force was created three months prior to conduct a study concerning an extension to the Blu-ray Disc specification that would add the ability to contain 4K Ultra HD video.[87][88]\\r\\n\\r\\nOn August 5, 2015, The Blu-ray Disc Association (BDA) announced it will commence licensing the Ultra HD Blu-ray format starting August 24, 2015. The Ultra HD Blu-ray format delivered high dynamic range content that significantly expanded the range between the brightest and darkest elements, expanded color range, high frame rate (up to 60fps) and up to 3840G2160 resolution, object-based sound formats, and an optional \\"digital bridge\\" feature. New players were required to play this format, which were able to play both DVDs, traditional Blu-rays and the new format. New Ultra HD Blu-ray discs hold up to 66?GB and 100?GB of data on dual- and triple-layer discs, respectively.[89]\\r\\n\\r\\nWhile a DVD uses a 650?nm red laser, Blu-ray Disc uses a 405?nm \\"blue\\" laser diode. Although the laser is called \\"blue\\", its color is actually in the violet range. The shorter wavelength can be focused to a smaller area, thus enabling it to read information recorded in pits that are less than half the size of those on a DVD, and can consequently be spaced more closely, resulting in a shorter track pitch, enabling a Blu-ray Disc to hold about five times the amount of information that can be stored on a DVD. The lasers are GaN (gallium nitride) laser diodes that produce 405?nm light directly, that is, without frequency doubling or other nonlinear optical mechanisms.[91] Conventional DVDs use 650?nm red lasers, and CDs use 780?nm near-infrared lasers.\\r\\n\\r\\nThe minimum \\"spot size\\" on which a laser can be focused is limited by diffraction and depends on the wavelength of the light and the numerical aperture of the lens used to focus it. By decreasing the wavelength, increasing the numerical aperture from 0.60 to 0.85, and making the cover layer thinner to avoid unwanted optical effects, designers can cause the laser beam to focus on a smaller spot, which effectively allows more information to be stored in the same area.[92] For Blu-ray Disc, the spot size is 580?nm.[93] This allows a reduction of the pit size from 400?nm for DVD to 150?nm for Blu-ray Disc, and of the track pitch from 740?nm to 320?nm.[92] See compact disc for information on optical discs' physical structure. In addition to the optical improvements, Blu-ray Discs feature improvements in data encoding that further increase the amount of content that can be stored.[94]\\r\\n\\r\\nSince the Blu-ray Disc data layer is closer to the surface of the disc compared to the DVD standard, it was more vulnerable to scratches in early designs.[95] The first discs were therefore housed in cartridges for protection, resembling Professional Discs introduced by Sony in 2003. Using a cartridge would increase the price of an already expensive medium, so designers chose hard-coating of the pickup surface instead. TDK was the first company to develop a working scratch-protection coating for Blu-ray Discs, naming it Durabis. In addition, both Sony's and Panasonic's replication methods include proprietary hard-coat technologies. Sony's rewritable media are spin-coated, using a scratch-resistant and antistatic coating. Verbatim's recordable and rewritable Blu-ray Discs use their own proprietary technology, called Hard Coat.[96]\\r\\n\\r\\nThe Blu-ray Disc specification requires the testing of resistance to scratches by mechanical abrasion.[92] In contrast, DVD media are not required to be scratch-resistant, but since development of the technology, some companies, such as Verbatim, implemented hard-coating for more expensive lines of recordable DVDs.\\r\\n\\r\\nThe table shows the speeds available. Even the lowest speed (1G) is sufficient to play and record real-time 1080p video; the higher speeds are relevant for general data storage and more sophisticated handling of video. BD discs are designed to cope with at least 5000rpm of rotational speed.\\r\\n\\r\\nThe usable data rate of a Blu-ray Disc drive can be limited by the capacity of the drive's data interface. With a USB 2.0 interface, the maximum exploitable drive speed is 288?Mbit/s or 36?MB/s (also called 8G speed).[97] A USB 3.0 interface (with proper cabling) does not have this limitation,[98] nor do even the oldest version of Serial ATA (SATA, 150 MB/s)[99] nor the latest Parallel ATA (133 MB/s) standards. Blu-ray drives that are integrated into a computer (as opposed to physically separate and connected via a cable) typically have a SATA interface.[100]\\r\\n\\r\\nPre-recorded Blu-ray Disc titles usually ship in packages similar to but slightly smaller (18.5?mm shorter and 2?mm thinner: 135?mm G 171.5?mm G 13?mm[101]), as well as more rounded than a standard DVD keep case, generally with the format prominently displayed in a horizontal stripe across the top of the case (translucent blue for Blu-ray video discs, clear for Blu-ray 3D video releases, red for PlayStation 3 Greatest Hits Games, transparent for regular PlayStation 3 games, transparent dark blue for PlayStation 4 games, transparent green for Xbox One games and black for Ultra HD Blu-ray video releases). Warren Osborn and The Seastone Media Group, LLC created the package that was adopted worldwide following the Blu-ray versus HD DVD market adoption choice.[102] Because of the fact that Blu-ray cases are smaller than DVD cases, more Blu-Rays than DVDs can fit on a shelf, making Blu-ray an arguably better choice for situations with limited storage space.\\r\\n\\r\\nThe \\"Mini Blu-ray Disc\\" (also, \\"Mini-BD\\" and \\"Mini Blu-ray\\") is a compact 8-centimetre-diameter (~3?in) variant of the Blu-ray Disc that can store 7.8?GB of data in its single layer configuration, or 15.6?GB on a dual layer disc.[103] It is similar in concept to the MiniDVD and MiniCD. Recordable (BD-R) and rewritable (BD-RE) versions of Mini Blu-ray Disc have been developed specifically for compact camcorders and other compact recording devices.[104]\\r\\n\\r\\n\\"Blu-ray Disc recordable\\" refers to two optical disc formats that can be recorded with an optical disc recorder. BD-Rs can be written to once, whereas BD-REs can be erased and re-recorded multiple times. The current practical maximum speed for Blu-ray Discs is about 12G (54?MB/s).[105](1.7) Higher speeds of rotation (10,000+ rpm) cause too much wobble for the discs to be written properly,[citation needed] as with the 20G (27.7?MB/s) and 52G (7.8?MB/s) maximum speeds, respectively, of standard DVDs and CDs. Since September 2007, BD-RE is also available in the smaller 8?cm Mini Blu-ray Disc size.[104][106]\\r\\n\\r\\nOn September 18, 2007, Pioneer and Mitsubishi codeveloped BD-R LTH (\\"Low to High\\" in groove recording), which features an organic dye recording layer that can be manufactured by modifying existing CD-R and DVD-R production equipment, significantly reducing manufacturing costs.[107] In February 2008, Taiyo Yuden, Mitsubishi, and Maxell released the first BD-R LTH Discs,[108] and in March 2008, Sony's PlayStation 3 officially gained the ability to use BD-R LTH Discs with the 2.20 firmware update.[109] In May 2009 Verbatim/Mitsubishi announced the industry's first 6X BD-R LTH media, which allows recording a 25?GB disc in about 16 minutes.[110] Unlike with the previous releases of 120?mm optical discs (i.e. CDs and standard DVDs), Blu-ray recorders hit the market almost simultaneously with Blu-ray's debut.\\r\\n\\r\\nThe BD9 format was proposed to the Blu-ray Disc Association by Warner Home Video as a cost-effective alternative to the 25/50?GB BD-ROM discs. The format was supposed to use the same codecs and program structure as Blu-ray Disc video but recorded onto less expensive 8.5?GB dual-layer DVD. This red-laser media could be manufactured on existing DVD production lines with lower costs of production than the 25/50?GB Blu-ray media.[111]\\r\\n\\r\\nUsage of BD9 for releasing content on \\"pressed\\" discs never caught on. With the end of the format war, manufacturers ramped production of Blu-ray Discs and lowered prices to compete with DVDs. On the other hand, the idea of using inexpensive DVD media became popular among individual users. A lower-capacity version of this format that uses single-layer 4.7?GB DVDs has been unofficially called BD5. Both formats are being used by individuals for recording high-definition content in Blu-ray format onto recordable DVD media.[112][113] Despite the fact that the BD9 format has been adopted as part of the BD-ROM basic format, none of the existing Blu-ray player models explicitly claim to be able to read it. Consequently, the discs recorded in BD9 and BD5 formats are not guaranteed to play on standard Blu-ray Disc players. AVCHD and AVCREC also use inexpensive media like DVDs, but unlike BD9 and BD5 these formats have limited interactivity, codec types, and data rates. As of March 2011, BD9 was removed as an official BD-ROM disc.[114]\\r\\n\\r\\nThe BDXL format allows 100?GB and 128?GB write-once discs,[115][116] and 100?GB rewritable discs for commercial applications. It was defined in June 2010.[citation needed] BD-R 3.0 Format Specification (BDXL) defined a multi-layered disc recordable in BDAV format with the speed of 2G and 4G, capable of 100/128?GB and usage of UDF2.5/2.6.[117] BD-RE 4.0 Format Specification (BDXL) defined a multi-layered disc rewritable in BDAV with the speed of 2G and 4G, capable of 100?GB and usage of UDF2.5 as file system.[118]\\r\\n\\r\\nThe IH-BD (Intra-Hybrid Blu-ray) format includes a 25?GB rewritable layer (BD-RE) and a 25?GB write-once layer (BD-ROM), designed to work with existing Blu-ray Discs.[115][116]\\r\\n\\r\\nBlu-ray Disc specifies the use of Universal Disk Format (UDF) 2.50 as a convergent-friendly format for both PC and consumer electronics environments. It is used in the latest specifications of BD-ROM, BD-RE, and BD-R.[119][120][121] In the first BD-RE specification (defined in 2002), the BDFS (Blu-ray Disc File System) was used. The BD-RE 1.0 specification was defined mainly for the digital recording of high-definition television (HDTV) broadcast television. The BDFS was replaced by UDF 2.50 in the second BD-RE specification in 2005, in order to enable interoperability among consumer electronics Blu-ray recorders and personal computer systems. These optical disc recording technologies enabled PC recording and playback of BD-RE.[121][122][123] BD-R can use UDF 2.50/2.60.[124]\\r\\n\\r\\nThe Blu-ray Disc application for recording of digital broadcasting has been developed as System Description Blu-ray Rewritable Disc Format part 3 Audio Visual Basic Specifications (BDAV). The requirements related with computer file system have been specified in System Description Blu-ray Rewritable Disc Format part 2 File System Specifications version 1.0 (BDFS).[125] Initially, the BD-RE version 1.0 (BDFS) was specifically developed for recording of digital broadcasts using the Blu-ray Disc application (BDAV application). But these requirements are superseded by the Blu-ray Rewritable Disc File System Specifications version 2.0 (UDF) (a.k.a. RE 2.0) and Blu-ray Recordable Disc File System Specifications version 1.0 (UDF) (a.k.a. R 1.0). Additionally, a new application format, BDMV (System Description Blu-ray Disc Prerecorded Format part 3 Audio Visual Basic Specifications) for High Definition Content Distribution was developed for BD-ROM. The only file system developed for BDMV is the System Description Blu-ray Read-Only Disc Format part 2 File System Specifications version 1.0 (UDF) which defines the requirements for UDF 2.50.[121][125]\\r\\n\\r\\nAll BDMV application files are stored under a \\"BDMV\\" directory.[130][131][132][133]\\r\\n\\r\\nAudio, video, and other streams are multiplexed and stored on Blu-ray Discs in a container format based on the MPEG transport stream. It is also known as BDAV MPEG-2 transport stream and can use filename extension .m2ts.[130][134] Blu-ray Disc titles authored with menus are in the BDMV (Blu-ray Disc Movie) format and contain audio, video, and other streams in BDAV container.[135][136] There is also the BDAV (Blu-ray Disc Audio/Visual) format, the consumer oriented alternative to the BDMV format used for movie releases. The BDAV format is used on BD-REs and BD-Rs for audio/video recording.[136] BDMV format was later defined also for BD-RE and BD-R (in September 2006, in the third revision of BD-RE specification and second revision of BD-R specification).[119][120]\\r\\n\\r\\nBlu-ray Disc employs the MPEG transport stream recording method. That enables transport streams of digital broadcasts to be recorded as they are broadcast, without altering the format.[137] It also enables flexible editing of a digital broadcast that is recorded as is and where the data can be edited just by rewriting the playback stream. Although it is quite natural, a function for high-speed and easy-to-use retrieval is built in.[137][138] Blu-ray Disc Video use MPEG transport streams, compared to DVD's MPEG program streams. An MPEG transport stream contains one or more MPEG program streams, so this allows multiple video programs to be stored in the same file so they can be played back simultaneously (e.g. with \\"picture-in-picture\\" effect).\\r\\n\\r\\nThe BD-ROM specification mandates certain codec compatibilities for both hardware decoders (players) and movie software (content).[134][139] Windows Media Player does not come with the codecs required to play Blu-ray discs.[140]\\r\\n\\r\\nOriginally BD-ROMs stored video up to 1920G1080 pixel resolution at up to 60 (59.94) fields per second. Currently with UHD BD-ROM videos can be stored at a maximum of 3840G2160 pixel resolution at up to 60 (59.94) frames per second, progressively scanned. While most current Blu-ray players and recorders can read and write 1920G1080 video at the full 59.94p and 50p progressive format, new players for the UHD specifications will be able to read at 3840G2160 video at either 59.94p and 50p formats.\\r\\n\\r\\n^ a Only supported on UltraHD Blu-Ray with HEVC video compression standard.\\r\\n^ b Interlaced formats are listed in fields per second.\\r\\n^ c MPEG-2 at 1440G1080 was previously not included in a draft version of the specification from March 2005.[143]\\r\\n^ d These resolutions are stored anamorphically, i.e. they are stretched to the display aspect ratio by the player or display.\\r\\n\\r\\nFor video, all players are required to process H.262/MPEG-2 Part 2, H.264/MPEG-4 Part 10: AVC, and SMPTE VC-1.[144] BD-ROM titles with video must store video using one of the three mandatory formats; multiple formats on a single title are allowed. Blu-ray Disc allows video with a bit depth of 8-bits per color YCbCr with 4:2:0 chroma subsampling.[145][146] The choice of formats affects the producer's licensing/royalty costs as well as the title's maximum run time, due to differences in compression efficiency. Discs encoded in MPEG-2 video typically limit content producers to around two hours of high-definition content on a single-layer (25?GB) BD-ROM. The more-advanced video formats (VC-1 and MPEG-4 AVC) typically achieve a video run time twice that of MPEG-2, with comparable quality.\\r\\n\\r\\nMPEG-2 was used by many studios (including Paramount Pictures, which initially used the VC-1 format for HD DVD releases) for the first series of Blu-ray Discs, which were launched throughout 2006.[147] Modern releases are now often encoded in either MPEG-4 AVC or VC-1, allowing film studios to place all content on one disc, reducing costs and improving ease of use. Using these formats also frees a lot of space for storage of bonus content in HD (1080i/p), as opposed to the SD (480i/p) typically used for most titles. Some studios, such as Warner Bros., have released bonus content on discs encoded in a different format than the main feature title. For example, the Blu-ray Disc release of Superman Returns uses VC-1 for the feature film and MPEG-2 for some of its bonus content.[148] Today, Warner and other studios typically provide bonus content in the video format that matches the feature.\\r\\n\\r\\nFor audio, BD-ROM players are required to implement Dolby Digital (AC-3), DTS, and linear PCM. Players may optionally implement Dolby Digital Plus and DTS-HD High Resolution Audio as well as lossless formats Dolby TrueHD and DTS-HD Master Audio.[149] BD-ROM titles must use one of the mandatory schemes for the primary soundtrack. A secondary audiotrack, if present, may use any of the mandatory or optional codecs.\\r\\n\\r\\nFor users recording digital television programming, the recordable Blu-ray Disc standard's initial data rate of 36?Mbit/s is more than adequate to record high-definition broadcasts from any source (IPTV, cable/satellite, or terrestrial). BD Video movies have a maximum data transfer rate of 54?Mbit/s, a maximum AV bitrate of 48?Mbit/s (for both audio and video data), and a maximum video bit rate of 40?Mbit/s. This compares to HD DVD movies, which have a maximum data transfer rate of 36?Mbit/s, a maximum AV bitrate of 30.24?Mbit/s, and a maximum video bitrate of 29.4?Mbit/s.[151]\\r\\n\\r\\nAt the 2005 JavaOne trade show, it was announced that Sun Microsystems' Java cross-platform software environment would be included in all Blu-ray Disc players as a mandatory part of the standard.[152] Java is used to implement interactive menus on Blu-ray Discs, as opposed to the method used on DVD-video discs. DVDs use pre-rendered MPEG segments and selectable subtitle pictures, which are considerably more primitive and rarely seamless. At the conference, Java creator James Gosling suggested that the inclusion of a Java virtual machine, as well as network connectivity in some BD devices, will allow updates to Blu-ray Discs via the Internet, adding content such as additional subtitle languages and promotional features not included on the disc at pressing time.[153] This Java Version is called BD-J and is built on a profile of the Globally Executable MHP (GEM) standard; GEM is the worldwide version of the Multimedia Home Platform standard.\\r\\n\\r\\nThe BD-ROM specification defines four Blu-ray Disc player profiles, including an audio-only player profile (BD-Audio) that does not require video decoding or BD-J. All of the video-based player profiles (BD-Video) are required to have a full implementation of BD-J.\\r\\n\\r\\n^ a This is used for storing audio/video and title updates. It can either be built-in memory or removable media, such as a memory card or USB flash memory.\\r\\n^ b A secondary audio decoder is typically used for interactive audio and commentary.\\r\\n^ c Profile 3.0 is a separate audio-only player profile. The first Blu-ray Disc album to be released was Divertimenti, by record label Lindberg Lyd, and it has been confirmed to work on the PS3.[154][155]^ d Also known as Initial Standard profile.\\r\\n^ e Also known as Final Standard profile.\\r\\n\\r\\nOn November 2, 2007, the Grace Period Profile was superseded by Bonus View as the minimum profile for new BD-Video players released to the market.[156] When Blu-ray Disc software not authored with interactive features dependent on Bonus View or BD-Live hardware capabilities is played on Profile 1.0 players, it is able to play the main feature of the disc, but some extra features may not be available or will have limited capability.[157]\\r\\n\\r\\nThe biggest difference between Bonus View and BD-Live is that BD-Live requires the Blu-ray Disc player to have an Internet connection to access Internet-based content. BD-Live features have included Internet chats, scheduled chats with the director, Internet games, downloadable featurettes, downloadable quizzes, and downloadable movie trailers.[158][159][160] While some Bonus View players may have an Ethernet port, it is used for firmware updates and is not used for Internet-based content.[161] In addition, Profile 2.0 also requires more local storage in order to handle this content.\\r\\n\\r\\nProfile 1.0 players are not eligible for Bonus View or BD-Live compliant upgrades and do not have the function or capability to access these upgrades, with the exception of the latest players and the PlayStation 3. Internet is required to use.[162][163][164]\\r\\n\\r\\nAs with the implementation of region codes for DVDs, Blu-ray Disc players sold in a specific geographical region are designed to play only discs authorized by the content provider for that region. This is intended to permit content providers (motion picture studios, television production company etc.) to enact regional price discrimination and/or exclusive content licensing. According to the Blu-ray Disc Association, all Blu-ray Disc players and Blu-ray Disc-equipped computer systems are required to enforce regional coding. However, content providers need not use region playback codes.[166] Some current estimates suggest 70% of available [movie] Blu-ray Discs from the major studios are region-code-free and can, therefore, be played on any Blu-ray Disc player, in any region.[167]\\r\\n\\r\\nMovie distributors have different region coding policies. Among major U.S. studios, Walt Disney Pictures, Warner Bros., Paramount Pictures, Universal Studios, and Sony Pictures have released most of their titles region-free.[168][169][170][171][172][173] MGM and Lions Gate Entertainment have released a mix of region-free and region-coded titles.[174][175] 20th Century Fox has released most of their titles region-coded.[176] Vintage film restoration and distribution company The Criterion Collection uses US region coding in all Blu-ray releases.[177][178]\\r\\n\\r\\nThe Blu-ray Disc region coding scheme divides the world into three regions, labeled A, B, and C.\\r\\n\\r\\nIn circumvention of region coding restrictions, stand-alone Blu-ray Disc players are sometimes modified by third parties to allow for playback of Blu-ray Discs (and DVDs) with any region code.[179] Instructions (\\"hacks\\") describing how to reset the Blu-ray region counter of computer player applications to make them multi-region indefinitely are also regularly posted to video enthusiast websites and forums. Unlike DVD region codes, Blu-ray region codes are verified only by the player software, not by the optical drive's firmware.\\r\\n\\r\\nThe latest types of Blu-Ray players, suitable for UltraHD content, is region-free.[180]\\r\\n\\r\\nThe Blu-ray Disc format employs several layers of digital rights management (DRM) which restrict the usage of the discs.[181][182] This has led to extensive criticism of the format by organizations opposed to DRM, such as the Free Software Foundation,[183] and consumers because new releases require player firmware updates to allow disc playback.[184][185]\\r\\n\\r\\nBlu-ray equipment is required to implement the High-bandwidth Digital Content Protection (HDCP) system to encrypt the data sent by players to rendering devices through physical connections. This is aimed at preventing the copying of copyrighted content as it travels across cables. Through a protocol flag in the media stream called the Image Constraint Token (ICT), a Blu-ray Disc can enforce its reproduction in a lower resolution whenever a full HDCP-compliant link is not used. In order to ease the transition to high definition formats, the adoption of this protection method was postponed until 2011.[186]\\r\\n\\r\\nThe Advanced Access Content System (AACS) is a standard for content distribution and digital rights management. It was developed by AS Licensing Administrator, LLC (AACS LA), a consortium that includes Disney, Intel, Microsoft, Panasonic, Warner Bros., IBM, Toshiba, and Sony. Since the appearance of the format on devices in 2006, several successful attacks have been made on it. The first known attack relied on the trusted client problem. In addition, decryption keys have been extracted from a weakly protected player (WinDVD). Since keys can be revoked in newer releases,[187] this is only a temporary attack, and new keys must continually be discovered in order to decrypt the latest discs.\\r\\n\\r\\nBD+ was developed by Cryptography Research Inc. and is based on their concept of Self-Protecting Digital Content.[188] BD+, effectively a small virtual machine embedded in authorized players, allows content providers to include executable programs on Blu-ray Discs. Such programs can:[181]\\r\\n\\r\\nIf a playback device manufacturer finds that its devices have been hacked, it can potentially release BD+ code that detects and circumvents the vulnerability. These programs can then be included in all new content releases.[189] The specifications of the BD+ virtual machine are available only to licensed device manufacturers. A list of licensed commercial adopters is available from the BD+ website.\\r\\n\\r\\nThe first titles using BD+ were released in October 2007. Since November 2007, versions of BD+ protection have been circumvented by various versions of the AnyDVD HD program.[190][191] Other programs known to be capable of circumventing BD+ protection are DumpHD (versions 0.6 and above, along with some supporting software),[192] MakeMKV,[193] and two applications from DVDFab (Passkey and HD Decrypter[194]).\\r\\n\\r\\nBD-ROM Mark is a small amount of cryptographic data that is stored separately from normal Blu-ray Disc data, aiming to prevent replication of the discs. The cryptographic data is needed to decrypt the copyrighted disc content protected by AACS.[195] A specially licensed piece of hardware is required to insert the ROM Mark into the media during mastering. During replication, this ROM Mark is transferred together with the recorded data to the disc. In consequence, any copies of a disc made with a regular recorder will lack the ROM Mark data and will be unreadable on standard players.\\r\\n\\r\\nThe Blu-ray Disc Association recommends but does not require that Blu-ray Disc drives be capable of reading standard DVDs and CDs, for backward compatibility.[196] Most Blu-ray Disc players are capable of reading both CDs and DVDs; however, a few of the early Blu-ray Disc players released in 2006, such as the Sony BDP-S1, could play DVDs but not CDs.[197][198][199] In addition, Blu-ray players cannot play HD DVDs, and HD DVD players cannot play Blu-ray discs. Some Blu-ray players can also play Video CDs, and all 4K Blu-ray players can play regular Blu-ray Discs, and most can play DVDs and CDs. The PlayStation 4 does not support CDs.\\r\\n\\r\\nHigh Fidelity Pure Audio (HFPA) is a marketing initiative, spearheaded by the Universal Music Group, for audio-only Blu-ray optical discs. Launched in 2013 as a potential successor to the compact disc, it has been compared with DVD-A and SACD, which had similar aims.\\r\\n\\r\\nAVCHD was originally developed as a high definition format for consumer tapeless camcorders. Derived from the Blu-ray Disc specification, AVCHD shares a similar random access directory structure but is restricted to lower audio and video bitrates, simpler interactivity, and the use of AVC-video and Dolby AC-3 (or linear PCM) audio. Being primarily an acquisition format, AVCHD playback is not recognized by all devices that play Blu-ray Disc. Nevertheless, many such devices are capable of playing AVCHD recordings from removable media, such as DVDs, SD/SDHC memory cards, \\"Memory Stick\\" cards, and hard disk drives.[200]\\r\\n\\r\\nAVCREC uses a BDAV container to record high definition content on conventional DVDs.[201] Presently AVCREC is tightly integrated with the Japanese ISDB broadcast standard and is not marketed outside of Japan. AVCREC is used primarily in set-top digital video recorders and in this regard is comparable to HD REC.\\r\\n\\r\\nThe Blu-ray Disc Association (BDA) created a task force made up of executives from the film industry and the consumer electronics and IT sectors to help define standards for putting 3D film and 3D television content on a Blu-ray Disc.[202] On December 17, 2009, the BDA officially announced 3D specs for Blu-ray Disc, allowing backward compatibility with current 2D Blu-ray players.[203] The BDA has said, \\"The Blu-ray 3D specification calls for encoding 3D video using the \\"Stereo High\\" profile defined by Multiview Video Coding (MVC), an extension to the ITU-T H.264 Advanced Video Coding (AVC) codec currently implemented by all Blu-ray Disc players. MPEG4-MVC compresses both left and right eye views with a typical 50% overhead compared to equivalent 2D content, and can provide full 1080p resolution backward compatibility with current 2D Blu-ray Disc players.\\"[204] This means the MVC (3D) stream is backward compatible with H.264/AVC (2D) stream, allowing older 2D devices and software to decode stereoscopic video streams, ignoring additional information for the second view.\\r\\n\\r\\nSony added Blu-ray 3D support to its PlayStation 3 console via a firmware upgrade on 21 September 2010.[205] The console had previously gained 3D gaming capability via an update on 21 April 2010.[206] Since the version 3.70 software update on August 9, 2011, the PlayStation 3 can play DTS-HD Master Audio and DTS-HD High Resolution Audio while playing 3D Blu-ray.[207] Dolby TrueHD is used on a small minority of Blu-ray 3D releases, and bitstreaming implemented in slim PlayStation 3 models only (original \\"fat\\" PS3 models decode internally and send audio as LPCM).[208] As of 2018, most major home entertainment studios, such as Walt Disney Studios, Sony Pictures, etc. have discontinued the Blu-ray 3D format in North America and other regions.\\r\\n\\r\\nUltra HD Blu-ray is a new disc format, incompatible with existing Blu-ray Disc players, that supports 60fps 4K UHD video encoded in HEVC with 10-bit HDR and a wider color gamut.","input":"What was the first blu ray movie released?"},{"output":"from the 4th century","context":"The Christian cross, seen as a representation of the instrument of the crucifixion of Jesus, is the best-known symbol of Christianity.[1] It is related to the crucifix (a cross that includes a usually three-dimensional representation of Jesus' body) and to the more general family of cross symbols.\\r\\nThe basic forms of the cross are the Latin cross (?) and the Greek cross (?), with numerous variants used in text, visual art, heraldry, and in various confessional contexts.\\r\\n\\r\\n\\r\\nThe cross-shaped sign, represented in its simplest form by a crossing of two lines at right angles, greatly predates the introduction of Christianity, in both East and West. It goes back to a very remote period of human civilization. It is supposed to have been used not just for its ornamental value, but also with religious significance.[2] It may have represented the apparatus used in kindling fire, and thus as the symbol of sacred fire or as a symbol of the sun, denoting its daily rotation. It has also been interpreted as the mystic representation of lightning or of the god of the tempest, or the emblem of the Aryan pantheon and the primitive Aryan civilization.[2]\\r\\nAnother associated symbol is the ansated cross (ankh or crux ansata) used in ancient Egypt. It was often depicted in the hands of the goddess Sekhmet, and as a hieroglyphic sign of life or of the living. Egyptian Christians (Copts) adopted it as the emblem of the cross.[2] In his book, The Worship of the Dead, Colonel J. Garnier wrote: \\"The cross in the form of the 'Crux Ansata' ... was carried in the hands of the Egyptian priests and Pontiff kings as the symbol of their authority as priests of the Sun god and was called 'the Sign of Life'.\\"[3]\\r\\nAnother Egyptian symbol is the Ndj (Cross-ndj (hieroglyph)) - Uses for the hieroglyph: 1 \\"to protect, guard, avenge\\", and \\"protector, advocate, avenger\\" 2 \\"homage to thee\\", (a form of salutation to gods) 3 \\"discuss a matter with someone\\", \\"to converse\\", \\"to take counsel\\". Yet another Egyptian symbol is the nfr - meaning: beauty or perfect.\\r\\nIn the Bronze Age a representation of the cross as conceived in Christian art appeared, and the form was popularised. The more precise characterization coincided with a corresponding general change in customs and beliefs. The cross then came into use in various forms on many objects: fibulas, cinctures, earthenware fragments, and on the bottom of drinking vessels. De Mortillet believed that such use of the sign was not merely ornamental, but rather a symbol of consecration, especially in the case of objects pertaining to burial. In the proto-Etruscan cemetery of Golasecca every tomb has a vase with a cross engraved on it. True crosses of more or less artistic design have been found in Tiryns, at Mycen?, in Crete, and on a fibula from Vulci.[2]\\r\\nAccording to W. E. Vine, the cross was used by worshipers of Tammuz, an Ancient Near East deity of Babylonian origin who had the cross-shaped taw (tau) as his symbol.[4]\\r\\nJohn Pearson, Bishop of Chester (c. 1660) wrote in his commentary on the Apostles' Creed that the Greek word stauros originally signified \\"a straight standing Stake, Pale, or Palisador\\", but that, \\"when other transverse or prominent parts were added in a perfect Cross, it retained still the Original Name\\", and he declared: \\"The Form then of the Cross on which our Saviour suffered was not a simple, but a compounded, Figure, according to the Custom of the Romans, by whose Procurator he was condemned to die. In which there was not only a straight and erected piece of Wood fixed in the Earth, but also a transverse Beam fastned unto that towards the top thereof\\".[5]\\r\\n[[File:KERALA - 87.jpg|thumb|cross from kerala\\r\\n[[File:Spas vsederzhitel sinay.jpg|thumb|upright|The Sinai icon of Christ Pantocrator (6th century), showing Christ with a cruciform halo and holding a book adorned with a crux gemmata]] During the first two centuries of Christianity, the cross was rare in Christian iconography, as it depicts a purposely painful and gruesome method of public execution and Christians were reluctant to use it.[1] A symbol similar to the cross, the staurogram, was used to abbreviate the Greek word for cross in very early New Testament manuscripts such as P66, P45 and P75, almost like a nomen sacrum (nomina sacra).[6] The extensive adoption of the cross as Christian iconographic symbol arose from the 4th century.[7]\\r\\nThe earliest depiction of the Christian Cross may be the Herculaneum Cross which was found in the city of Herculaneum, which was entombed in pyroclastic material along with Pompeii during the eruption of Mount Vesuvius in AD 79. Another early depictions of the cross as a Christian symbol is the Alexamenos graffito.\\r\\nHowever, the cross symbol was already associated with Christians in the 2nd century, as is indicated in the anti-Christian arguments cited in the Octavius[8] of Minucius Felix, chapters IX and XXIX, written at the end of that century or the beginning of the next,[9] and by the fact that by the early 3rd century the cross had become so closely associated with Christ that Clement of Alexandria, who died between 211 and 216, could without fear of ambiguity use the phrase ? ϫ? ך? (the Lord's sign) to mean the cross, when he repeated the idea, current as early as the apocryphal Epistle of Barnabas, that the number 318 (in Greek numerals, п) in Genesis 14:14 was interpreted as a foreshadowing (a \\"type\\") of the cross (T, an upright with crossbar, standing for 300) and of Jesus (, the first two letter of his name ҷѮҷ, standing for 18),[10] and his contemporary Tertullian could designate the body of Christian believers as crucis religiosi, i.e. \\"devotees of the Cross\\".[11] In his book De Corona, written in 204, Tertullian tells how it was already a tradition for Christians to trace repeatedly on their foreheads the sign of the cross.[12] The crucifix, a cross upon which an image of Christ is present, is not known to have been used until the 6th century AD.[13]\\r\\nThe Jewish Encyclopedia says:[14]\\r\\nThe cross as a Christian symbol or \\"seal\\" came into use at least as early as the second century (see \\"Apost. Const.\\" iii. 17; Epistle of Barnabas, xi.-xii.; Justin, \\"Apologia,\\" i. 55-60; \\"Dial. cum Tryph.\\" 85-97); and the marking of a cross upon the forehead and the chest was regarded as a talisman against the powers of demons (Tertullian, \\"De Corona,\\" iii.; Cyprian, \\"Testimonies,\\" xi. 21ÿ22; Lactantius, \\"Divin? Institutiones,\\" iv. 27, and elsewhere). Accordingly the Christian Fathers had to defend themselves, as early as the second century, against the charge of being worshipers of the cross, as may be learned from Tertullian, \\"Apologia,\\" xii., xvii., and Minucius Felix, \\"Octavius,\\" xxix. Christians used to swear by the power of the cross\\r\\nIn contemporary Christianity, the cross is a symbol of the atonement and reminds Christians of God's love in sacrificing his own son for humanity. It represents Jesus' victory over sin and death, since it is believed that through his death and resurrection he conquered death itself. See Colossians 2:15, \\"Having disarmed the powers and authorities, he made a public spectacle of them, triumphing over them by the cross.\\r\\nCatholics, Eastern Orthodox, Oriental Orthodox, members of the major branches of Christianity with other adherents as Lutheranism, some Anglicans, and others often make the Sign of the Cross upon themselves. This was already a common Christian practice in the time of Tertullian.[15]\\r\\nThe Feast of the Cross is an important Christian feast. One of the twelve Great Feasts in Eastern Orthodoxy is the Exaltation of the Cross on September 14, which commemorates the consecration of the basilica on the site where the original cross of Jesus was reportedly discovered in 326 by Helena of Constantinople, mother of Constantine the Great. The Catholic Church celebrates the feast on the same day and under the same name (In Exaltatione Sanctae Crucis), though in English it has been called the feast of the Triumph of the Cross.\\r\\nRoman Catholic, Eastern Orthodox and Anglican bishops place a cross [+] before their name when signing a document. The dagger symbol (?) placed after the name of a dead person (often with the date of death) is sometimes taken to be a Christian cross.[16]\\r\\nAlthough Christians accepted that the cross was the gallows on which Jesus died,[17] they had already begun in the 2nd century to use it as a Christian symbol.[18] During the first three centuries of the Christian era the cross was \\"a symbol of minor importance\\" when compared to the prominence given to it later,[19] but by the second century it was nonetheless so closely associated with Christians that Tertullian could designate the body of Christian believers as crucis religiosi, i.e. \\"devotees of the Cross\\".[20] and it was already a tradition for Christians to trace repeatedly on their foreheads the sign of the cross.[21] Martin Luther at the time of the Reformation retained the cross and crucifix in the Lutheran Church. Luther wrote: \\"The cross alone is our theology.\\" He believed one knows God not through works but through suffering, the cross, and faith[need quotation to verify].[22]\\r\\nThe Protestant Reformation spurred a revival of iconoclasm, a wave of rejecting sacred images, which in some localities (such as England) included polemics against using the cross in worship. For example, during the 16th century, a minority of theologians in the Anglican and Reformed traditions Nicholas Ridley,[23] James Calfhill,[24] and Theodore Beza,[25] rejected practices that they described as cross worship. Considering it a form of idolatry, there was a dispute in 16th century England over the baptismal use of the sign of the cross and even the public use of crosses.[26] There were more active reactions to religious items that were thought as 'relics of Papacy', as happened for example in September 1641, when Sir Robert Harley, pulled down and destroyed the cross at Wigmore.[27] Writers during the 19th century indicating a pagan origin of the cross included Henry Dana Ward,[28] Mourant Brock,[29] and John Denham Parsons.[30] David Williams, writing of medieval images of monsters, says: \\"The disembodied phallus is also formed into a cross, which, before it became for Christianity the symbol of salvation, was a pagan symbol of fertility.\\"[31] The study, Gods, Heroes & Kings: The Battle for Mythic Britain states: \\"Before the fourth century CE, the cross was not widely embraced as a sign of Christianity, symbolizing as it did the gallows of a criminal.\\"[32]\\r\\nJehovah's Witnesses do not use the symbol of the cross in their worship, which they believe constitutes idolatry.[33] They believe that Jesus died on a single upright torture stake rather than a two-beam cross, arguing that the Greek term stauros indicated a single upright pole.[34] Although early Watch Tower Society publications associated with the Bible Student movement taught that Christ was executed on a cross, it no longer appeared on Watch Tower Society publications after the name Jehovah's witnesses was adopted in 1931,[35] and use of the cross was officially abandoned in 1936.[36]\\r\\nThe Church of Jesus Christ of Latter-day Saints teaches that Jesus died on a cross, however, their prophet Gordon B. Hinckley stated that \\"for us the cross is the symbol of the dying Christ, while our message is a declaration of the living Christ.\\" When asked what was the symbol of his religion, Hinckley replied \\"the lives of our people must become the only meaningful expression of our faith and, in fact, therefore, the symbol of our worship.\\"[37][38] Prophet Howard W. Hunter encouraged Latter-day Saints \\"to look to the temple of the Lord as the great symbol of your membership.\\"[39] Images of LDS temples and the Angel Moroni (who is found in statue on most temples) are commonly used to symbolize the LDS faith.[40]\\r\\nA wooden cross at Coventry Cathedral, constructed of the remnants of beams found after the Coventry Blitz\\r\\nCross of Sacrifice or War?Cross, from a Commonwealth War Graves Commission cemetery\\r\\nCross in Valle de los Cados near Madrid, the highest cross in the world\\r\\nKottakkavu Sliva, a Persian cross founded by Mar Sabor and Mar Proth, is preserved at Kottakkavu Mar Thoma Syro-Malabar Pilgrim Center, North Paravur, India.\\r\\nThe Millennium Cross in Skopje, Republic of Macedonia, one of the biggest crosses in the world\\r\\nThe Cross on the Hill, a 199-foot (61?m) cross located in Bossier City, Louisiana\\r\\nTile cross from R?dtvet Church in Oslo, Norway, built in 1978\\r\\nThe Ruthwell Cross, a stone Anglo-Saxon cross located in Ruthwell, Dumfriesshire\\r\\nThe World Trade Center Cross rises from the World Trade Center wreckage.\\r\\nThe Cross in Melaten-Friedhof, K?ln, Germany\\r\\nAltar and cross in the Chapel at Callaway Gardens","input":"When was the cross first used in christianity?"},{"output":"British engineer Robert Whitehead","context":"","input":"Who invented the first submarine used in war?"},{"output":"Sean McVay","context":"The table shows the current coaches and their records for each NFL team.\\r\\nThe longest tenured head coach on his current team is Bill Belichick, who has been with the New England Patriots since the 2000 NFL season. Belichick also has the most wins among active coaches, as well as most Super Bowl appearances (7) and Super Bowl wins (5) as head coach.[1] Other coaches to have won a Super Bowl as head coach with their current team are Mike Tomlin, Sean Payton, Mike McCarthy, John Harbaugh, and Pete Carroll.\\r\\n\\r\\n\\r\\nBill Belichick has won five Super Bowls with the New England Patriots, more than any other active coach.\\r\\nMike Tomlin, the youngest head coach to lead his team to a Super Bowl championship.\\r\\nJohn Harbaugh, the current head coach of the Baltimore Ravens.\\r\\nMarvin Lewis holds the record for most wins as a Cincinnati Bengals head coach.\\r\\nSean McVay, the youngest head coach in modern NFL history.\\r\\nUpdated through February 5, 2017.","input":"Who's the youngest coach in the nfl?"},{"output":"October 24, 1972","context":"","input":"When did the baseball player jackie robinson die?"},{"output":"Association football","context":"The sports in Afghanistan are managed by the Afghan Sports Federation, which promotes cricket, football, basketball, volleyball, golf, handball, boxing, taekwondo, weightlifting, bodybuilding, track and field, skating, bowling, snooker, chess, and other sports. Association football[1] and cricket[2] are the most popular sports in Afghanistan.[3]\\r\\n\\r\\n\\r\\nCricket as a sport, is highly followed in Afghanistan and is one of the main sports that Afghans participate in and watch on television. In the national level, cricket matches are played between provinces, mainly between the south and eastern provinces of the country. According to locals, cricket has helped Afghanistan in bringing unity. The Afghanistan national cricket team was formed in 2001 and has held matches against all other international cricket teams. The Afghans rapidly rose through the World Cricket League since early 2008. It participated in the 2009 ICC World Cup Qualifier, and qualified for the first time for the 2010 ICC World Twenty20 in the 2010 ICC World Cricket League Division One. The Afghanistan national women's cricket team was formed in 2010. They have competed in the ICC World Twenty20 since their qualification in 2010 and their debut for the Cricket World Cup since 2015.\\r\\nAs in many other countries around the world, soccer is a more popular sport played and watched in Afghanistan. The Afghanistan national football team was formed in 1922, joining FIFA in 1948 and the Asian Football Confederation (AFC) in 1954. Although it did not play in any international games from 1984 to 2003 due to internal conflicts, it is striving and hoping to make it to FIFA one day. The national stadium, which was built during the reign of King Amanullah Khan, has been used for football matches between teams from different provinces of the country as well as neighboring countries. In the national level, football matches are played between provinces or regions.[3] The Afghanistan women's national football team was formed in 2007. During the 2011 SAFF Championship, the Afghan team marked its first win over Nepal.\\r\\nBasketball was first played in Afghanistan in 1936. In 1966, the Afghanistan National Olympic Committee (ANOC) founded the Afghanistan national basketball team after receiving challenges from India and Pakistan. Tom Gouttierre, an American Peace Corps and coach of the team at Habibia High School, became the first coach. It is played by both Afghan men and women.\\r\\nAfghans have taken a recent interest in the Mixed Martial Arts. There are several gyms in Afghanistan which promote the sport and have fighters. Siyar Bahadurzada is a mixed martial artist who competes in the Ultimate Fighting Championship. He is well known for holding and wearing the Afghan flag around him before and after his professional fights.\\r\\nBahadurzada's major achievement:\\r\\nRohullah Nikpai was the first Afghan representative of his nation to win a medal for Afghanistan in the Olympics. He won Bronze in the 2008 and 2012 Olympics, the only two occasions Afghanistan have received medals. This sport has recently thrived in Afghanistan by his influence.\\r\\nNikpai's medal tally summary in the Olympics:\\r\\nBoxing has recently flourished in Afghanistan, with Hamid Rahimi having a huge influence in the country. The first ever boxing match in Afghanistan was held in 2012 with Rahimi fighting and winning by TKO (Technical Knockout).\\r\\nBodybuilding is widely enjoyed in Afghanistan. An Afghan by the name of Ahmad Yasi Salik Qaderi (\\"Mr. Muscles\\") became the overall winner of the 2017 World Championship in Bodybuilding and Fitness, which was held in Ulaanbaatar, Mongolia.[4]\\r\\nOther sports in which Afghanistan competes include volleyball, golf, track and field, team handball, rugby, weighlifting, ice skating, bowling, baseball, snooker, and chess. Saleh Mohammad is a professional Afghan snooker player, who previously represented Pakistan in international competitions but is now representing Afghanistan.[5]\\r\\nBuzkashi is a traditional sport and it is mostly played by people in northern Afghanistan and in Central Asia, as well as in northwestern parts of neighboring Pakistan.\\r\\nThe Afghanistan Rugby Federation (ARF) was formed in 2011, and is registered with the National Olympic Committee and approved by the Government of the Islamic Republic of Afghanistan.\\r\\nAfghanistan also became a member of the Federation of International Bandy in 2012.[6][7]\\r\\nIn 2015 Afghanistan held its first marathon; among those who ran the entire marathon was one woman, Zainab, age 25, who thus became the first Afghan woman to run in a marathon within her own country.[8]\\r\\nThere are small sized football stadiums in most major cities of Afghanistan, which were built before the 1970s and they lack modern seatings. They will only improve once more if people turn to sport and the nation's economy picks up, including the security situation and proper investors are found. The President of the Afghanistan Cricket Board, Omar Zakhilwal, announced in October 2010 that the government was planning to construct standard cricket grounds in all 34 provinces in the next two years.[9] There is also another larger gymnasium under construction in Kabul. Currently, there is only the Olympic Committee Gymnasium, which is constantly used by teams of different sports.\\r\\nThe following are some of the major stadiums in Afghanistan:","input":"What is the most popular sport in afghanistan?"},{"output":"a horse show class for very young children, generally under the age of 7 years","context":"Leadline is a horse show class for very young children, generally under the age of 7 years. An adult or older child actually leads the horse in-hand, while the child that is judged sits on the horse and usually holds the reins, but only for the sake of appearance, as the actual control of the animal rests with the handler on the ground. Rules vary tremendously from one geographical region to the next, but as a rule the horse is shown at a walk and a trot, and the riding child is judged on their equitation, limited to proper seat, leg and hand position, to a lesser extent on poise. The child is usually not asked to actually control the animal, though in some locations a judge may award extra points if the child initiates certain commands to the horse and even more points if the horse actually responds. In many areas, judge may also ask the children simple questions about themselves or their horse, primarily to gauge the child's poise and manners more than equine knowledge. Occasionally, other elements, such as games or other group exercises may be added.\\r\\nAttire is generally the same as for equivalent equitation classes, though in some locations, children may also exhibit in costumes. In most cases, the handler is not judged, though some exhibitors nonetheless turn out with matching clothing for handler and rider. Equestrian helmets are usually encouraged, and sometimes mandated.\\r\\nAward policies also vary widely. As a general rule, an attempt is made to provide every participant some sort of award. At some shows, there is no actual evaluation of the riders and all children are given identical awards for participation, often blue (first place) ribbons. At others, a first place award will be given, with all other participants given smaller but equivalent awards. Yet others rank the top five to eight places but also provide participation awards, or ribbons identical to the lowest placing, to all entrants so no child leaves without an award. A few shows maintain the practice of providing awards in the same manner as regular horse show classes, even if this means some children do not receive any type of award. In lieu of a trophy, some shows award stuffed animals or other age-appropriate items.","input":"What is lead line in a horse show?"},{"output":"Dr Pepper","context":"Dr Pepper is a carbonated soft drink marketed as having a unique flavor. The drink was created in the 1880s by pharmacist Charles Alderton in Waco, Texas and first served around 1885. Dr Pepper was first nationally marketed in the United States in 1904, and is now also sold in Europe, Asia, Canada, Mexico, Australia, and South America, as well as New Zealand and South Africa as an imported good. Variants include a version without high fructose corn syrup, Diet Dr Pepper, as well as a line of additional flavors, first introduced in the 2000s.\\r\\n\\r\\n\\r\\nThe U.S. Patent Office recognizes December 1, 1885, as the first time Dr Pepper was served.[citation needed] It was introduced nationally in the United States at the 1904 Louisiana Purchase Exposition as a new kind of soda pop, made with 23 flavors. Its introduction in 1885 preceded the introduction of Coca-Cola by one year.\\r\\nIt was formulated by Brooklyn-born pharmacist Charles Alderton in Morrison's Old Corner Drug Store in Waco, Texas.[1] To test his new drink, he first offered it to store owner Wade Morrison, who also found it to his liking. Patrons at Morrison's soda fountain soon learned of Alderton's new drink and began ordering a \\"Waco\\".[2] Alderton gave the formula to Morrison, who named it Dr. Pepper (later stylized as \\"Dr Pepper\\").\\r\\nAs with Coca-Cola, the formula for Dr Pepper is a trade secret, and allegedly the recipe is kept as two halves in safe deposit boxes in two separate Dallas banks.[3] A persistent rumor since the 1930s is that the drink contains prune juice,[3] but the official Dr Pepper FAQ refutes this with \\"Dr Pepper is a unique blend of natural and artificial flavors; it does not contain prune juice.\\"[1] The origin of the rumor is unknown; some believe it was started by a deliveryman for a competitor trying to cast aspersions based on prune juice's laxative effects,[3] but it may simply be because many people feel that Dr Pepper tastes similar to prune juice.[4][5][6][7]\\r\\nIn 2009, an old ledger book filled with formulas and recipes was discovered by Bill Waters while shopping at antiques stores in the Texas Panhandle.[8] Several sheets and letterheads hinted it had come from the W.B. Morrison & Co. Old Corner Drug Store (the same store where Dr Pepper was first served in 1885) and faded letters on the book's cover spelled out \\"Castles Formulas\\". John Castles was a partner of Morrison's for a time and worked at that location as early as 1880. One recipe in the book titled \\"D Peppers Pepsin Bitters\\" was of particular interest, and some speculated it could be an early recipe for Dr Pepper. However, Dr Pepper Snapple Group insists it is not the formula for Dr Pepper, but is instead a medicinal recipe for a digestive aid. The book was put up for auction in May 2009, but no one purchased it.[9]\\r\\nTheories about the origins of the soft drink's name abound.[10][11] One possible reason why the name was chosen was the practice, common at the time of the drinks creation, of including Dr. in the names of products to convey the impression that they were healthful.[12]\\r\\nA theory often cited is that the drink was named after an actual doctor, one Charles T. Pepper of Rural Retreat, Virginia. Morrison may have named the drink after the doctor in gratitude for Pepper having given Morrison his first job.[12][13] However, Milly Walker, Collections Manager / Curator for the Dublin (Texas) Dr Pepper Bottling Co. Museum, has stated that U.S. Census records show that a young Morrison lived in Christiansburg, Virginia, 40 miles away from Rural Retreat, and that \\"there is not one piece of evidence that Morrison ever worked for Charles T. Pepper in Rural Retreat\\".[14] Another story tells of Morrison naming the drink after Charles T. Pepper because the doctor granted Morrison permission to marry Pepper's daughter,[15] but the girl in question was only eight years old at the time that Morrison moved to Waco.[12][14][16]\\r\\nA Dr. Pepper of Christiansburg is another possible inspiration for the soft drink's name. In the census that shows Morrison living in Christiansburg and working as a pharmacy clerk, a Dr. Pepper is recorded on a subsequent page. Since census takers at this time were walking from door to door, and these census entries are close to each other in the record, it appears that Morrison and this Dr. Pepper lived close to each other. Furthermore, Pepper is recorded as having a 16-year-old daughter, named Malinda or Malissa.[14]\\r\\nThe period (full stop) after Dr was used intermittently in Dr Pepper logos until the 1950s,[17] when, after some debate, it was discarded permanently, for stylistic and legibility reasons. A logo that debuted at that time had slanted text, in which Dr. resembled Di:.\\r\\nIn 1951, Dr Pepper sued the Coca-Cola company for US$750,000, asserting that nickel Coca-Colas were sold below cost and were a restraint of trade.[18]\\r\\nIn 1972, Dr Pepper sued the Coca-Cola company for trademark infringement based on a soft drink marketed by Coca-Cola called \\"Peppo\\".[19] They tried naming it Dr. Pibb, which was also determined to violate the trademark. The soft drink was later renamed Mr Pibb.\\r\\nDr Pepper became insolvent in the early 1980s, prompting an investment group to take the company private. Several years later, Coca-Cola attempted to acquire Dr Pepper, but was blocked from doing so by the Federal Trade Commission (FTC). Around the same time, Seven Up was acquired from Phillip Morris by the same investment company that bailed out Dr Pepper. Upon the failure of the Coca-Cola merger, Dr Pepper and Seven Up merged (creating Dr Pepper/Seven Up, Inc., or DPSU), giving up international branding rights in the process. After the DPSU merger, Coca-Cola obtained most non-US rights to the Dr Pepper name (with PepsiCo taking the Seven Up rights).[20]\\r\\nDr Pepper was a frequent player in the 1990s antitrust history of the United States. As part of these activities, economists and the courts have weighed in with the opinion that Dr Pepper is a \\"pepper\\" flavored drink and not a \\"cola\\". In 1995, the FTC blocked a merger between The Coca-Cola Company and Dr Pepper on grounds that included concerns about a monopoly of the \\"pepper\\" flavor category of soft drinks.[21] In 1996, Dr Pepper was involved in an antitrust case involving Jerry Jones, the Dallas Cowboys, NFL Properties, Nike, and other commercial interests active at Texas Stadium in Irving, Texas.[22] Jones had made deals with Dr Pepper and the other companies that, the league said, violated their exclusive marketing contracts with Coca-Cola and other businesses. The NFL agreed to allow Jones and other teams to pursue their own agreements.[22]\\r\\nIn 1998, the \\"pepper\\" flavor soda category was a major part of the analysis supporting an antitrust case between Coca-Cola and Pepsi.\\r\\nMuch of the soft drink industry in the United States stopped using sugar in the 1980s, in response to a series of price supports and import quotas introduced beginning in 1982 that increased the price of sugar above the global market price. As a result, most US soft drinks, including Dr Pepper, now use high fructose corn syrup instead of sugar.[36]\\r\\nA handful of United States bottling plants still use sugar to sweeten Dr Pepper. The Dr Pepper bottling plant in Dublin, Texas used to produce such a beverage, known as Dublin Dr Pepper. In the 1980s, plant owner W.P. \\"Bill\\" Kloster (June 7, 1918?ÿ  September 27, 1999) refused to convert the plant to high fructose corn syrup.[37] Other bottlers still using sugar include Temple Bottling Company, in Temple, Texas, Ab-Tex in Abilene, and West Jefferson Dr Pepper (WJDP) of West Jefferson, NC.\\r\\nOn March 25, 2007, Coca-Cola bottlers in the Dr Pepper Heartland commenced sales of 16?ounce cans of Dr Pepper made with cane sugar and featuring a logo with 'Old Doc' on them. This product was scheduled to be a limited time release.\\r\\nIn January 2009, \\"Heritage Dr Pepper\\" became available in select markets in cans and 16 oz bottles with the distinction \\"Made with Real Sugar.\\"\\r\\nBeginning in July 2010, Dr Pepper's 125th Anniversary edition in some markets was made with sugar as opposed to other sweeteners. Since Dr Pepper Corporate has no control over whether the bottlers will use sugar, there is no guarantee the soda will have sugar.[38]\\r\\nAs of January 2012, the bottling plant in Dublin, Texas, is no longer bottling Dr Pepper.[39]\\r\\nThe soft drink industry in some other countries never stopped using sugar as a sweetener. For instance in the European Union, high fructose corn syrup is subject to a production quota. In 2005, this quota was set at 303,000 tons; in comparison, the EU produced an average of 18.6 million tons of sugar annually between 1999 and 2001.[40] Therefore, most European soft drink producers, including most Dr Pepper bottling plants, still use sugar to sweeten their products. However, the bottlers of Dr Pepper in Germany and the United Kingdom use instead a combination of sugar and artificial sweeteners.\\r\\nIn the United States, Dr Pepper Snapple Group does not have a complete network of bottlers and distributors, so the drink is sometimes bottled under contract by Coca-Cola or Pepsi bottlers. Prior to the initial Cadbury Schweppes investment-turned-buyout, 30% of Dr Pepper/Seven Up products were produced and distributed by Pepsi bottlers, and another 30% by Coca-Cola bottlers. The remaining 40% were produced and distributed by independent bottlers (mainly consisting of Dr Pepper/Seven Up premerger regional bottlers) and the Dr Pepper/Seven Up Bottling Group. Currently, the majority of Pepsi and Coke bottlers bottling Dr Pepper are owned by PepsiCo and The Coca-Cola Company after their buyouts of their major bottlers.\\r\\nPresently, Dr Pepper Snapple relies on its own bottling group to bottle and distribute its products in more than 30 states. Coca-Cola and Pepsi have essentially stopped bottling and distributing CSAB products in favor of in-house alternatives, although regional exceptions can be found.[41]\\r\\nIn Canada and Poland, Cadbury-Schweppes has licensed distribution rights to PepsiCo. In Mexico, Germany, Sweden, the Netherlands, Slovakia, Finland, Austria, Czech Republic, Belgium, and Norway, Cadbury-Schweppes owns the trademark and distributes the product. In Romania, it can be found only in larger cities, imported from Belgium. In Portugal, Spain, France, Turkey, and Greece, it is almost impossible to find, as it is usually imported from the United Kingdom in particular supermarkets. In almost all of the other countries of the world, the Coca-Cola Company purchased the trademark from Cadbury-Schweppes and distributes the product. This mixed worldwide ownership of the trademark is due to antitrust regulations which prevented Coca-Cola from purchasing the rights everywhere. Dr Pepper is also available in Russia, South Korea and Ukraine. Although no longer locally bottled in Australia or New Zealand, Dr Pepper is imported and sold by United States Foods, and many other small retailers in Australia, with the UK (sugar) version sold in the British sections of Coles and Woolworths supermarkets. Dr Pepper is not available in Thailand, Italy, North Korea and Serbia. It is rarely sold in the Philippines, Indonesia, Malaysia and Singapore, as it is imported from the United States.\\r\\nDr Pepper has been sold in Japan since 1973 and is widely available in greater Tokyo, Okinawa and parts of the Tkai region, where it is distributed by local Coca-Cola bottlers. It is not actively marketed in other regions of Japan; Coca-Cola's Osaka bottler began selling Dr Pepper in 1983, but pulled the product two years later due to low sales.\\r\\n\\"Dr Pepper Time\\", according to one promotion, was at 10, 2 and 4 o'clock. During World War II, a syndicated radio program, The 10ÿ2ÿ4 Ranch (later titled 10ÿ2ÿ4 Time), aired in the South and other areas where Dr Pepper was distributed. The show featured the Sons of the Pioneers and Dick Foran.[44] In the 1960s, the tune of the chorus of \\"The Glow-Worm\\" was used in ads, with lyrics which ended, \\"It's Dr Pepper Time!\\"\\r\\nIn the 1960s, Dr Pepper released the \\"Charge\\" ad:\\r\\nCharge!!\\r\\nGet Going Again, With the Dr Pepper Difference.\\r\\nThe \\"Be a Pepper\\" series referred to fans of Dr Pepper as \\"Peppers\\", and often featured large, sequential, crowd dance scenes, intricately choreographed by Tony Stevens[45] and led onscreen by actor David Naughton. A recurring jingle was:\\r\\nI'm a Pepper, he's a Pepper,\\r\\nShe's a Pepper, we're a Pepper, Wouldn't you like to be a Pepper, too? Be a Pepper. Drink Dr Pepper.\\r\\nThis became grist for a number of pop culture references and parodies. One of the first was a July 1981 sketch on the program SCTV, in which an overly-excited injured man (Eugene Levy) extols the work of a \\"Dr Shekter\\" (Rick Moranis) who has been treating him. Levy and a group of patients wearing casts and crutches engage in their own elaborate dancing and singing (\\"Wouldn't you like to see my doctor, too?\\"), which Shekter first uses as an opportunity to explain his work, and then grows alarmed (\\"These people should not be dancing!\\"). In the 1982 sex farce Beach Girls, the slogan became \\"I'm a popper, he's a popper...\\" Wreck-Gar parodied the slogan in The Return of Optimus Prime.\\r\\nAfter appearing in a series of commercials, David Naughton had his breakthrough film role as the main character in the John Landis film An American Werewolf in London. Another famous \\"I'm a Pepper\\" dancer was Ray Bolger, the actor who played the Scarecrow in the film The Wizard of Oz.[citation needed]\\r\\nIn the early 1960s, Dr Pepper promoted the idea of serving the drink hot with lemon slices in winter. This idea appeared in the film Blast from the Past initially set in the early 1960s.\\r\\nIn 1978, Jake Holmes wrote the lyrics to \\"Be a Pepper\\". Earlier in the 1970s, Randy Newman wrote another jingle entitled \\"The Most Original Soft Drink Ever\\".[citation needed] Barry Manilow performed Holmes's jingle in concerts and on albums under the inclusion of \\"VSM?ÿ Very Strange Medley\\". A TV commercial was also created using the jingle and ran from 1977 to 1985.[46] The song noted \\"It's not a cola, it's something much much more. It's not a root beer, you get root beer by the score.\\"[47] (W.W. Clements, former CEO and president of the Dr Pepper/7-Up Company, similarly described the taste of Dr Pepper as one-of-a-kind, saying, \\"I've always maintained you cannot tell anyone what Dr Pepper tastes like because it's so different. It's not an apple, it's not an orange, it's not a strawberry, it's not a root beer, it's not even a cola. It's a different kind of drink with a unique taste all its own.\\"[48])\\r\\nDr Pepper has also been featured outside the \\"I'm a Pepper\\" motif. An example is in the video game Pikmin 2, where one of the collectible treasures is a Dr Pepper bottle cap (it is labeled as the \\"Drought Ender\\"). Also, an empty Dr Pepper bottle is featured in the book Ragweed by Newbery Medal-winning author Avi; the books illustrator, Brian Floca, is the son of a Dr Pepper bottler. Several of the classic non-\\"I'm a Pepper\\" commercials featured prominent movie stars, one being a television advertisement with Chris Rock as a child enjoying a Dr Pepper.\\r\\nThe 1980s \\"Out of the Ordinary\\" advertising campaign involved a series of postapocalyptic commercials featuring a space cowboy and an alien sidekick seeking \\"something different\\" from a simple generic cola.[49] The campaign also produced commercials featuring the movie creature Godzilla, where citizens of a Japanese town offered Dr Pepper as a libation. The commercials were prominently featured during the 1986 syndication of The Canned Film Festival, which was sponsored by the Dr Pepper Company.\\r\\nOutside the United States, Squeeze's Glenn Tilbrook and Chris Difford played for a Dr Pepper advert in the UK with the slogan, \\"Hold out for the out of the ordinary.\\"\\r\\nDr Pepper's \\"Be You\\" advertising campaign centered on commercials featuring pairs of popular musicians, including LeAnn Rimes with Reba McEntire, Paulina Rubio with Celia Cruz, Thala with Tito Puente, B2K with Smokey Robinson, Anastacia with Cyndi Lauper, Patricia Manterola with Ana Gabriel, and LL Cool J with Run-D.M.C. The campaign also featured individual musicians, notably Garth Brooks.\\r\\nDr Pepper made several appearances in the 1994 Robert Zemeckis major motion picture Forrest Gump, as it was the beverage of choice for the movie's namesake lead character, played by Tom Hanks. In one of the film's Dr Pepper scenes, Forrest's narrative suggests, \\"The best part about goin' to the White House was, they had all the food and drink that you wanted ... I must have had me 15 Dr Peppers.\\" When subsequently asked by the President how he felt, Forrest gave an honest answer of \\"I gotta pee.\\" Although, arguably the film's largest product placement installation, the depiction of Dr Pepper was perhaps not always accurate as, in another scene during the 1972 New Year's Eve celebration which Forrest attends, he drinks a Dr Pepper with a logo that was inconsistent with the timeline of the film.\\r\\nDr Pepper was introduced to the Australian market in 1997 with a short-lived TV advertising campaign and low-priced 280?ml cans sold through supermarkets. Dr Pepper was subsequently sold in 1.25-liter plastic bottles alongside other major brands until 2003. Cadbury Schweppes stated the product did not gain acceptance by Australians.[citation needed] A report on the soft drink industry by IBIS accused Cadbury Schweppes of failing in their marketing of the brand, given its global appeal. One potential problem with the marketing campaign was in advertising it as \\"American\\". The use of the Statue of Liberty moving to Australia and passing cans of Dr Pepper on to two Australian males made its imported (i.e. \\"non-Australian\\") status clear.\\r\\nAfter withdrawing from the Australian market, Dr Pepper arrived without fanfare in New Zealand. Cans imported from the US are available in some specialty stores in New Zealand and Australia.\\r\\nOn the December 20, 2000, episode of the Late Show with David Letterman, Letterman jokingly referred to Dr Pepper as \\"liquid manure\\". After a representative of Dr Pepper complained, CBS agreed not to rerun that episode. Letterman repeatedly made assurances on the show that he was joking.[50]\\r\\nFrom 2001 to 2003, Diet Dr Pepper aired ads that promise authentic Dr Pepper taste, using the slogan \\"Diet Dr Pepper tastes more like regular Dr Pepper\\", parodying new ideas inferior to the originals, including XGA (not PGA) Extreme Golf, Green Bay Watch (spoof of Baywatch) and a TV show CHimPs (rather than CHiPs). These ads were produced by They Might Be Giants.\\r\\nSeveral ads for Diet Cherry Vanilla Dr Pepper appeared on television in 2005. In one, a young woman on a blind date at a restaurant, who sips into the beverage, suddenly makes her date, restaurant patrons, and even a waitress all part of a musical sequence involving The Muppets version of the song \\"Mah N Mah N\\".\\r\\nOne campaign features the Queen song \\"I Want It All\\".\\r\\nOn January 1, 2008, the company unveiled a new TV ad campaign featuring the Cheers theme song (\\"Where Everybody Knows Your Name\\") performed by Gary Portnoy.\\r\\nIn a 2008 ad, a student in a college lecture takes a sip of Dr Pepper. When he stops drinking, the Dr Pepper can sings variations of \\"Flava Licious\\" (Flavor Flav), and other people in the room start dancing.\\r\\nIn 2008, Dr Pepper in the UK restarted launching its old adverts and slogan, \\"What's the worst that can happen?\\" They also started an on-pack promotion for free ringtones with up to 20 to collect. A commercial for this included Jesse Eisenberg being forced to be on live TV without his clothes on.\\r\\nAs of 2009, the slogan of the product was \\"Drink it slow. Doctor's orders\\". Advertising supporting the slogan has celebrities with famous relations to the word \\"doctor\\" (Dr. Dre, Julius \\"Dr. J\\" Erving, Gene Simmons (writer of the Kiss song \\"Calling Dr. Love\\"), et al.) or who played fictional doctors (such as Neil Patrick Harris or Kelsey Grammer) endorsing the beverage. The ads culminate with the celebrity stating, \\"Trust me. I'm a doctor\\", followed by the new slogan appearing onscreen with a glass of Dr Pepper.[51]\\r\\nIn 2010, Dr Pepper was part of a marketing and promotional campaign with Marvel Studios to promote the summer blockbuster Iron Man 2; characters from the film adorned cans of Dr Pepper, Diet Dr Pepper, and Dr Pepper Cherry.\\r\\nIn 2011, rapper Pitbull appeared in a commercial with the slogan \\"Let's have a real good time.\\"[citation needed]\\r\\nAlso in 2011, Dr Pepper was featured in the anime Steins;Gate as Dk Pepper for copyright issues.[52]\\r\\nIn 1963, singer Donna Loren became a spokesperson for the company when she was selected in a nationwide search to be the Dr Pepper Girl.[56][57] National exposure followed for Loren as she promoted the drink via radio, print, television, calendars, billboards, and personal appearances. One of her first appearances for the company was as co-host with Dick Clark (whom she worked with regularly) of an ABC television special, Dr Pepper Celebrity Party.[58][59][60] She subsequently made hundreds of singing and personal appearances for Dr Pepper. In Dr PepperKing of Beverages, Dr Pepper historian Harry E. Ellis wrote, Sparkly, vivacious and gifted with a wonderful voice, Donna was an immediate success. She became widely known in a short period as \\"The Dr Pepper Girl,\\" appearing at special events and on programs sponsored by the Company. Miss Loren would figure prominently in Dr Peppers plans for some five years, not only as an entertainer but doing commercials for radio and TV and appearing in many forms of advertising. She appeared on 24-sheet poster boards, point-of-sale and on Dr Pepper calendars.[61]\\r\\nDonna Lorens role as Dr Pepper spokesperson led to her first appearance in the American International Pictures Beach Party film Muscle Beach Party. Loren later explained: Dr Pepper was involved in that [the Beach Party movies] and actually placed me as product placement. And because I could sing, they gave me a duet with Dick Dale, and then it just went on from there.[62] From this, she went on to appear in three more Beach Party films. Away from the company, Loren was a familiar presence in the 1960s due to her many performances on television, films, and her records for Capitol, Reprise and other labels. She represented Dr Pepper until 1968.\\r\\nOn March 26, 2008, various media outlets reported that Dr Pepper would offer \\"a free can of Dr Pepper to everyone in America\\" ÿ excluding former Guns N' Roses guitarists Buckethead and Slash ÿ if the band released the long-awaited Chinese Democracy in 2008.[63][64] Later in the day, lead vocalist Axl Rose replied to Dr Pepper on Guns N' Roses' official website and spoke of his surprise at Dr Pepper's support. Rose also said he would share his Dr Pepper with Buckethead as \\"some of Buckethead's performances are on Chinese Democracy\\".[65] After it was announced that the album would be released in 2008, Dr Pepper stated that it would uphold its pledge.[66]\\r\\nDr Pepper's online distribution of free coupons upon the album's release November 23, 2008, proved inadequate. Lawyers for the band threatened Dr Pepper's parent company with a lawsuit two days after the album's release. In a letter to Dr Pepper, Rose's lawyer Alan Gutman said, \\"The redemption scheme your company clumsily implemented for this offer was an unmitigated disaster which defrauded consumers and, in the eyes of vocal fans, ruined Chinese Democracy's release.\\"[67] Rose's lawyer also demanded that the company make a full-page apology that would appear in The Wall Street Journal, USA Today, The New York Times and The Los Angeles Times.[68][69] In a later interview, Rose claimed he told his lawyers it was a non-issue and was surprised by their actions.[70]\\r\\nThe Dr Pepper Museum, located in the Artesian Manufacturing and Bottling Company building at 300 South Fifth Street in downtown Waco, Texas, opened to the public in 1991. The building was the first building to be built specifically to bottle Dr Pepper. It was completed in 1906, and Dr Pepper was bottled there until the 1960s. The museum has three floors of exhibits, a working old-fashioned soda fountain, and a gift store of Dr Pepper memorabilia.\\r\\nThe museum founder, Wilton Lanning, died in January 2018.[71]\\r\\nThe company sells more Dr Pepper in the Roanoke Valley area of Virginia than any other metropolitan area east of the Mississippi River. Roanoke is approximately 90?miles east of the hometown of Dr. Charles T. Pepper, which is Rural Retreat, Virginia, and 30 miles east of Christiansburg, Virginia, home of Dr. Pepper and Morrison referred to in the census information above. John William \\"Bill\\" Davis opened the first Dr. Pepper plant east of the Mississippi in Roanoke in 1936; subsequently the city was named the \\"Dr Pepper Capital of the World\\" and broke world records for its mass consumption of Dr Pepper in the late 1950s.[72] Dr Pepper donated a portion of its sales revenue in the Roanoke area to finance restoration of a circa-1950s neon Dr Pepper sign, which has the company's \\"10ÿ2ÿ4\\" logo from the time, in downtown Roanoke. In October 2015, the city of Roanoke declared October 24 (10-2-4) to be its official Dr. Pepper Day.[73]","input":"What popular soft drink had its birthplace in waco texas?"},{"output":"Estonia","context":"","input":"What was the first satellite state to break away from the ussr?"},{"output":"May 3, 2011","context":"The Amazing World of Gumball is a British-American animated comedy television series created by Ben Bocquelet for Cartoon Network. Since its debut on May 3, 2011,[1] 207 episodes of the series have been broadcast; its sixth season premiered on January 5, 2018. The Amazing World of Gumball revolves around the life of a 12-year-old cat named Gumball Watterson (Logan Grove, seasons 1 to 3; Jacob Hopkins, season 3 to 5; Nicolas Cantu, rest of season 5 onwards) and his frequent shenanigans in the fictional American world of Elmore. Throughout the series, he interacts with characters such as his former pet, brother and best friend Darwin (Kwesi Boakye, seasons 1 to 3; Terrell Ransom Jr., season 3 to 5; Donielle T. Hansley Jr, rest of season 5; Christian J. Simon, season 6 onwards), sister Anais (Kyla Rae Kowaleski), and parents Nicole and Richard (Teresa Gallagher and Dan Russell).\\r\\nBocquelet conceived The Amazing World of Gumball in 2007 while working as a development artist for Cartoon Network Studios Europe.[2][3] Having been asked by the network to pitch an idea for a new series for Adult Swim, he took sketches of several unused characters he had created for commercials and started developing a concept for a new series based on them.[4] When Adult Swim saw the pilot, they didn't accept it because they thought it was \\"too cute\\". As Cartoon Network was heavily interested in \\"family and school shows\\" at the time, Bocquelet decided to make the series a kid series called The Amazing World of Gumball which was a combination of both.[3][4] He pitched the series to Daniel Lennard, vice president of Original Series and Development at Turner Broadcasting, who was impressed by the series' premise and ultimately greenlit its production.[5] The first series to be greenlit by Cartoon Network Studios Europe,[6] The Amazing World of Gumball premiered on May 3, 2011 with the episode \\"The DVD\\", which was watched by 2.120 million viewers in the United States.[7]\\r\\nOn June 2, 2014, Cartoon Network announced that the series had been renewed a fourth and fifth season.[8] Both seasons combined will consist of 40 half-hour episodes.[9]\\r\\nOn June 22, 2016, Cartoon Network renewed the series for a sixth season which will consist of 44 episodes.[10] On September 7, 2016, Ben Bocquelet announced on Twitter that the sixth season would be his last working with the show,[11] but production will continue without him.[12]\\r\\nThe series is rated TV-Y7-FV in the United States (and has a PG certificate in the UK on all DVD releases) for comedic fantasy violence, mild to moderate suggestive content (including use of sexual innuendo), rude (often dark) humor, and scenes of peril, threat, and danger.[13]\\r\\nAs of March 9, 2018, 207 episodes of the series have aired.","input":"When did the amazing world of gumball first air?"},{"output":"Pawan Kumar Chamling","context":"The Chief Minister of Sikkim, an east Indian state, is the head of the Government of Sikkim. As per the Constitution of India, the Governor of Sikkim is the state's de jure head, but de facto executive authority rests with the chief minister. Following elections to the Sikkim Legislative Assembly, the governor usually invites the party (or coalition) with a majority of seats to form the government. The governor appoints the chief minister, whose council of ministers are collectively responsible to the assembly. Given that he has the confidence of the assembly, the chief minister's term is for five years and is subject to no term limits.[1]\\r\\n\\r\\nSince 1975, Sikkim has had five chief ministers. The first was Kazi Lhendup Dorjee of the Indian National Congress. Pawan Kumar Chamling of the Sikkim Democratic Front has served as Chief Minister of Sikkim since 1994. He has occupied the office longer than all his predecessors put together.","input":"Who is the present health minister of sikkim?"},{"output":"from 12 to 15 years","context":"The Shiba Inu (ݷ??) is the smallest of the six original and distinct spitz breeds of dog from Japan.[1]\\r\\nA small, agile dog that copes very well with mountainous terrain, the Shiba Inu was originally bred for hunting.[1][2] It looks similar to and is often mistaken for other Japanese dog breeds like the Akita Inu or Hokkaido, but the Shiba Inu is a different breed with a distinct blood line, temperament and smaller size than other Japanese dog breeds.[3][4][5]\\r\\n\\r\\n\\r\\nInu is the Japanese word for dog, but the origin of the prefix \\"Shiba\\" is less clear. The word shiba means \\"brushwood\\" in Japanese, and refers to a type of tree or shrub whose leaves turn red in the fall.[6] This leads some to believe that the Shiba was named with this in mind, either because the dogs were used to hunt in wild shrubs, or because the most common color of the Shiba Inu is a red color similar to that of the shrubs. However, in an old Nagano dialect, the word shiba also had the meaning of \\"small\\", thus this might be a reference to the dog's diminutive stature.[6] Therefore, the Shiba Inu is sometimes translated as \\"Little Brushwood Dog\\".[1]\\r\\nThe Shiba's frame is compact with well-developed muscles.[7] Males are 35 to 43?cm (14 to 17?in) at the withers. Females are 33 to 41?cm (13 to 16?in). The preferred size is the middle of the range for each sex. Average weight at preferred size is approximately 10?kg (22?lb) for males, 8?kg (18?lb) for females. Bones are moderate.\\r\\nThe Shiba is double coated, with the outer coat being stiff and straight and the undercoat soft and thick. Fur is short and even on the fox-like face, ears, and legs. Guard hairs stand off the body and are about 4 to 5?cm (1?1?2 to 2?in) long at the withers. Tail hair is slightly longer and stands open in a brush. Shibas may be red, black and tan, or sesame (red with black-tipped hairs),[7] with a cream, buff, or grey undercoat. They may also be white (cream), though this color is considered a \\"major fault\\" by the American Kennel Club and should never be intentionally bred in a show dog, as the required markings known as \\"urajiro\\" (膡) are not visible; \\"Urajiro\\" literally translates to \\"underside white\\".[2] Conversely, a white (cream) coat is perfectly acceptable according to the British Kennel Club breed standard.[8]\\r\\nThe urajiro (cream to white ventral color) is required in the following areas on all coat colors: on the sides of the muzzle, on the cheeks, inside the ears, on the underjaw and upper throat inside of legs, on the abdomen, around the vent and the ventral side of the tail. On reds: commonly on the throat, forechest, and chest. On blacks and sesames: commonly as a triangular mark on both sides of the forechest.[9]\\r\\nShibas tend to exhibit an independent nature[7] and sometimes show aggression. This is more prevalent between female Shibas and is influenced by the breed's strong prey drive. The Shiba Inu is best in a home without other small dogs or young children, but consistent obedience training and early socialization can make all the difference. The breed also interacts fairly well with cats.[2]\\r\\nFrom the Japanese breed standard:\\r\\nA spirited boldness, a good nature, and an unaffected forthrightness, which together yield dignity and natural beauty. The Shiba has an independent nature and can be reserved toward strangers but is loyal and affectionate to those who earn his respect. They can be aggressive toward other dogs.\\r\\nThe terms \\"spirited boldness\\" (, kan'i), \\"good nature\\" (im, rysei), and \\"artlessness\\" (\\\\, soboku) have subtle interpretations that have been the subject of much commentary.[10]\\r\\nThe Shiba is a relatively fastidious breed and feels the need to maintain itself in a clean state. They can often be seen licking their paws and legs, much like a cat. They generally go out of their way to keep their coats clean. Because of their fastidious and proud nature, Shiba puppies are easy to housebreak and in many cases will housebreak themselves. Having their owner simply place them outside after meal times and naps is generally enough to teach the Shiba the appropriate method of toileting.[11]\\r\\nA distinguishing characteristic of the breed is the so-called \\"shiba scream\\". When sufficiently provoked or unhappy, the dog will produce a loud, high pitched scream. This can occur when attempting to handle the dog in a way that it deems unacceptable.[1][12][13] The animal may also emit a very similar sound during periods of great joy, such as the return of the owner after an extended absence, or the arrival of a favored human guest.\\r\\nThe Shiba Inu has been identified as a basal breed that predates the emergence of the modern breeds in the 19th Century.\\r\\nOriginally, the Shiba Inu was bred to hunt and flush small game, such as birds and rabbits. Despite efforts to preserve the breed, the Shiba nearly became extinct during World War II[7] due to a combination of food shortage and a post-war distemper epidemic.[1] All subsequent dogs were bred from the only three surviving bloodlines.[14] These bloodlines were the Shinshu Shiba from Nagano Prefecture, the Mino Shiba from Gifu Prefecture, and the San'in Shiba from Tottori and Shimane Prefectures.[6] The Shinshu Shibas possessed a solid undercoat, with a dense layer of guard-hairs, and were small and red in color.\\r\\nThe Mino Shibas tended to have thick, prick ears, and possessed a sickle tail, rather than the common curled tail found on most modern Shibas. The San'in Shibas were larger than most modern shibas, and tended to be black, without the common tan and white accents found on modern black-and-tan shibas.[6] When the study of Japanese dogs was formalized in the early and mid-20th century, these three strains were combined into one overall breed, the Shiba Inu.[6] The first Japanese breed standard for the Shiba, the Nippo Standard, was published in 1934. In December 1936, the Shiba Inu was recognized as a Natural Monument of Japan through the Cultural Properties Act, largely due to the efforts of Nippo (Nihon Ken Hozonkai), the Association for the Preservation of the Japanese Dog.[6][15]\\r\\nIn 1954, an armed service family brought the first Shiba Inu to the United States.[14] In 1979, the first recorded litter was born in the United States.[14] The Shiba was recognized by the American Kennel Club in 1992 and added to the AKC Non-Sporting Group in 1993.[1][16] It is now primarily kept as a pet both in Japan and abroad.[17] In the United States, the growing popularity of the Shiba Inu is evident as the American Kennel Club Registration Statistics ranked the breed in 44th place in 2016; a rise from 50th place in 2012.[18]\\r\\nOverall, the Shiba Inu is a healthy dog breed.[19][20][21][22] Health conditions known to affect this breed are allergies, glaucoma, cataracts, hip dysplasia, entropion, and luxating patella.[23] Periodic joint examinations are recommended throughout the life of the dog but problems are generally discovered early in the dog's life. Eye tests should be performed yearly as eye problems can develop over time. By two years of age, Shiba Inus can be considered fully free from joint problems if none have been discovered by this point, since at this age the skeleton is fully developed.\\r\\nAs with any dog, Shibas should be walked or otherwise exercised daily.[24]\\r\\nTheir average life expectancy is from 12 to 15 years. Exercise, especially daily walks, is preferred for this breed to live a long and healthy life.[14] The oldest known Shiba, Pusuke, died at age 26 in early December 2011 and was the oldest dog alive at the time.\\r\\nThese dogs are very clean, so grooming needs will likely be minimal. A Shiba Inu's coat is coarse; short to medium length with the outer coat being 2.5 to 3.2?cm (1 to 1?1?4?in) long, and is naturally waterproof so there is little need for regular bathing. They also have a thick undercoat that can protect them from temperatures well below freezing. However, shedding, also known as blowing coat, can be a nuisance. Shedding is heaviest during the seasonal change and particularly during the summer season, but daily brushing can temper this problem. It is recommended that owners never shave or cut the coat of Shiba Inu, as the coat is needed to protect them from both cold and hot temperatures.[25]\\r\\nShiba Inu from profile\\r\\nA cream coated Shiba Inu\\r\\nTwo red with a black and tan Shiba Inus\\r\\nA red Shiba Inu","input":"What is the lifespan of a shiba inu?"},{"output":"western hemisphere","context":"Largest metropolitan areas\\r\\nLargest cities\\r\\nThe Americas (also collectively called America)[5][6][7] comprise the totality of the continents of North and South America.[8][9][10] Together, they make up most of the land in Earth's western hemisphere[11][12][13][14][15][16] and comprise the New World.\\r\\nAlong with their associated islands, they cover 8% of Earth's total surface area and 28.4% of its land area. The topography is dominated by the American Cordillera, a long chain of mountains that runs the length of the west coast. The flatter eastern side of the Americas is dominated by large river basins, such as the Amazon, St. Lawrence River / Great Lakes basin, Mississippi, and La Plata. Since the Americas extend 14,000?km (8,700?mi) from north to south, the climate and ecology vary widely, from the arctic tundra of Northern Canada, Greenland, and Alaska, to the tropical rain forests in Central America and South America.\\r\\nHumans first settled the Americas from Asia between 42,000 and 17,000 years ago. A second migration of Na-Dene speakers followed later from Asia. The subsequent migration of the Inuit into the neoarctic around 3500 BCE completed what is generally regarded as the settlement by the indigenous peoples of the Americas.\\r\\nThe first known European settlement in the Americas was by the Norse explorer Leif Ericson.[17] However, the colonization never became permanent and was later abandoned. The voyages of Christopher Columbus from 1492 to 1502 resulted in permanent contact with European (and subsequently, other Old World) powers, which led to the Columbian exchange. Diseases introduced from Europe and West Africa devastated the indigenous peoples, and the European powers colonized the Americas.[18] Mass emigration from Europe, including large numbers of indentured servants, and importation of African slaves largely replaced the indigenous peoples.\\r\\nDecolonization of the Americas began with the American Revolution in 1776 and Haitian Revolution in 1791. Currently, almost all of the population of the Americas resides in independent countries; however, the legacy of the colonization and settlement by Europeans is that the Americas share many common cultural traits, most notably Christianity and the use of Indo-European languages: primarily Spanish, English, Portuguese, French, and to a lesser extent Dutch.\\r\\nThe population is over 1 billion, with over 65% of them living in one of the three most populous countries (the United States, Brazil, and Mexico). As of the beginning of the 2010s, the most populous urban agglomerations are Mexico City (Mexico), New York (U.S.), Sao Paulo (Brazil), Los Angeles (U.S.), Buenos Aires (Argentina) and Rio de Janeiro (Brazil), all of them megacities (metropolitan areas with ten million inhabitants or more).\\r\\n\\r\\n\\r\\nThe name America was first recorded in 1507 (together with the related term Amerigen) in the Cosmographiae Introductio, apparently written by Matthias Ringmann, in reference to South America.[20] It was first applied to both North and South America by Gerardus Mercator in 1538. America derives from Americus, the Latin version of Italian explorer Amerigo Vespucci's first name. America accorded with the feminine names of Asia, Africa, and Europa.[21]\\r\\nIn modern English, North and South America are generally considered separate continents, and taken together are called the Americas in the plural, parallel to similar situations such as the Carolinas. When conceived as a unitary continent, the form is generally the continent of America in the singular. However, without a clarifying context, singular America in English commonly refers to the United States of America.[7]\\r\\nIn some countries of the world (including France, Italy, Portugal, Spain, Romania, Greece, and the countries of Latin America), America is considered a continent encompassing the North America and South America subcontinents,[22][23] as well as Central America.[24][25][26][27][28]\\r\\nThe first inhabitants migrated into the Americas from Asia. Habitation sites are known in Alaska and the Yukon from at least 20,000 years ago, with suggested ages of up to 40,000 years.[30][31][32] Beyond that, the specifics of the Paleo-Indian migration to and throughout the Americas, including the dates and routes traveled, are subject to ongoing research and discussion.[33] Widespread habitation of the Americas occurred during the late glacial maximum, from 16,000 to 13,000 years ago.[32][34]\\r\\nThe traditional theory has been that these early migrants moved into the Beringia land bridge between eastern Siberia and present-day Alaska around 40,000ÿ17,000 years ago,[35] when sea levels were significantly lowered during the Quaternary glaciation.[33][36] These people are believed to have followed herds of now-extinct pleistocene megafauna along ice-free corridors that stretched between the Laurentide and Cordilleran ice sheets.[37] Another route proposed is that, either on foot or using primitive boats, they migrated down the Pacific coast to South America.[38] Evidence of the latter would since have been covered by a sea level rise of hundreds of meters following the last ice age.[39] Both routes may have been taken, although the genetic evidences suggests a single founding population.[40] The micro-satellite diversity and distributions specific to South American Indigenous people indicates that certain populations have been isolated since the initial colonization of the region.[41]\\r\\nA second migration occurred after the initial peopling of the Americas;[42] Na Dene speakers found predominantly in North American groups at varying genetic rates with the highest frequency found among the Athabaskans at 42% derive from this second wave.[43] Linguists and biologists have reached a similar conclusion based on analysis of Amerindian language groups and ABO blood group system distributions.[42][44][45][46] Then the people of the Arctic small tool tradition a broad cultural entity that developed along the Alaska Peninsula, around Bristol Bay, and on the eastern shores of the Bering Strait around 2,500?BCE (4,500?years ago) moved into North America.[47] The Arctic small tool tradition, a Paleo-Eskimo culture branched off into two cultural variants, including the Pre-Dorset, and the Independence traditions of Greenland.[48] The descendants of the Pre-Dorset cultural group, the Dorset culture was displaced by the final migrants from the Bering sea coast line the ancestors of modern Inuit, the Thule people by 1000?Common Era (CE).[48] Around the same time as the Inuit migrated into Greenland, Viking settlers began arriving in Greenland in 982 and Vinland shortly thereafter, establishing a settlement at L'Anse aux Meadows, near the northernmost tip of Newfoundland.[49] The Viking settlers quickly abandoned Vinland, and disappeared from Greenland by 1500.[50]\\r\\nThe pre-Columbian era incorporates all period subdivisions in the history and prehistory of the Americas before the appearance of significant European influences on the American continents, spanning the time of the original settlement in the Upper Paleolithic to European colonization during the Early Modern period. The term Pre-Columbian is used especially often in the context of the great indigenous civilizations of the Americas, such as those of Mesoamerica (the Olmec, the Toltec, the Teotihuacano, the Zapotec, the Mixtec, the Aztec, and the Maya) and the Andes (Inca, Moche, Muisca, Ca?aris).\\r\\nMany pre-Columbian civilizations established characteristics and hallmarks which included permanent or urban settlements, agriculture, civic and monumental architecture, and complex societal hierarchies. Some of these civilizations had long faded by the time of the first permanent European arrivals (c. late 15thÿearly 16th centuries), and are known only through archeological investigations. Others were contemporary with this period, and are also known from historical accounts of the time. A few, such as the Maya, had their own written records. However, most Europeans of the time viewed such texts as pagan, and much was destroyed in Christian pyres. Only a few hidden documents remain today, leaving modern historians with glimpses of ancient culture and knowledge.[51]\\r\\nAlthough there had been previous trans-oceanic contact, large-scale European colonization of the Americas began with the first voyage of Christopher Columbus in 1492. The first Spanish settlement in the Americas was La Isabela in northern Hispaniola. This town was abandoned shortly after in favor of Santo Domingo de Guzmn, founded in 1496, the oldest American city of European foundation. This was the base from which the Spanish monarchy administered its new colonies and their expansion. On the continent, Panama City on the Pacific coast of Central America, founded on August 5, 1519, played an important role, being the base for the Spanish conquest of South America. The spread of new diseases brought by Europeans and Africans killed many of the inhabitants of North America and South America,[52][53] with a general population crash of Native Americans occurring in the mid-16th century, often well ahead of European contact.[54] European immigrants were often part of state-sponsored attempts to found colonies in the Americas. Migration continued as people moved to the Americas fleeing religious persecution or seeking economic opportunities. Millions of individuals were forcibly transported to the Americas as slaves, prisoners or indentured servants.\\r\\nDecolonization of the Americas began with the American Revolution and the Haitian Revolution in the late 1700s. This was followed by numerous Latin American wars of independence in the early 1800s. Between 1811 and 1825, Paraguay, Argentina, Chile, Gran Colombia, the United Provinces of Central America, Mexico, Brazil, Peru, and Bolivia gained independence from Spain and Portugal in armed revolutions. After the Dominican Republic won independence from Haiti, it was re-annexed by Spain in 1861, but reclaimed its independence in 1865 at the conclusion of the Dominican Restoration War. The last violent episode of decolonization was the Cuban War of Independence which became the SpanishÿAmerican War, which resulted in the independence of Cuba in 1898, and the transfer of sovereignty over Puerto Rico from Spain to the United States.\\r\\nPeaceful decolonization began with the purchase by the United States of Louisiana from France in 1803, Florida from Spain in 1819, of Alaska from Russia in 1867, and the Danish West Indies from Denmark in 1916. Canada became independent of the United Kingdom, starting with the Balfour Declaration of 1926, Statute of Westminster 1931, and ending with the patriation of the Canadian Constitution in 1982. The Dominion of Newfoundland similarly achieved partial independence under the Balfour Declaration and Statute of Westminster, but was re-absorbed into the United Kingdom in 1934. It was subsequently confederated with Canada in 1949.\\r\\nThe remaining European colonies in the Caribbean began to achieve peaceful independence well after World War II. Jamaica and Trinidad and Tobago became independent in 1962, and Guyana and Barbados both achieved independence in 1966. In the 1970s, the Bahamas, Grenada, Dominica, St. Lucia, and St. Vincent and the Grenadines all became independent of the United Kingdom, and Suriname became independent of the Netherlands. Belize, Antigua and Barbuda, and Saint Kitts and Nevis achieved independence from the United Kingdom in the 1980s.\\r\\nThe northernmost point of the Americas is Kaffeklubben Island, which is the most northerly point of land on Earth.[55] The southernmost point is the islands of Southern Thule, although they are sometimes considered part of Antarctica.[56] The mainland of the Americas is the world's longest north-to-south landmass. The distance between its two polar extremities, the Boothia Peninsula in northern Canada and Cape Froward in Chilean Patagonia, is roughly 14,000?km (8,700?mi).[57] The mainland's most westerly point is the end of the Seward Peninsula in Alaska; Attu Island, further off the Alaskan coast to the west, is considered the westernmost point of the Americas. Ponta do Seixas in northeastern Brazil forms the easternmost extremity of the mainland,[57] while Nordostrundingen, in Greenland, is the most easterly point of the continental shelf.\\r\\nSouth America broke off from the west of the supercontinent Gondwana around 135?million years ago, forming its own continent.[58] Around 15?million years ago, the collision of the Caribbean Plate and the Pacific Plate resulted in the emergence of a series of volcanoes along the border that created a number of islands. The gaps in the archipelago of Central America filled in with material eroded off North America and South America, plus new land created by continued volcanism. By three million years ago, the continents of North America and South America were linked by the Isthmus of Panama, thereby forming the single landmass of the Americas.[59] The Great American Interchange resulted in many species being spread across the Americas, such as the cougar, porcupine, opossums, armadillos and hummingbirds.[60]\\r\\nThe geography of the western Americas is dominated by the American cordillera, with the Andes running along the west coast of South America[61] and the Rocky Mountains and other North American Cordillera ranges running along the western side of North America.[62] The 2,300-kilometer-long (1,400?mi) Appalachian Mountains run along the east coast of North America from Alabama to Newfoundland.[63] North of the Appalachians, the Arctic Cordillera runs along the eastern coast of Canada.[64]\\r\\nThe largest mountain ranges are the Andes and Rocky Mountains. The Sierra Nevada and the Cascade Range reach similar altitudes as the Rocky Mountains, but are significantly smaller. In North America, the greatest number of fourteeners are in the United States, and more specifically in the U.S. state of Colorado. The highest peaks of the Americas are located in the Andes, with Aconcagua of Argentina being the highest; in North America Denali (Mount McKinley) in the U.S. state of Alaska is the tallest.\\r\\nBetween its coastal mountain ranges, North America has vast flat areas. The Interior Plains spread over much of the continent, with low relief.[65] The Canadian Shield covers almost 5 million km2 of North America and is generally quite flat.[66] Similarly, the north-east of South America is covered by the flat Amazon Basin.[67] The Brazilian Highlands on the east coast are fairly smooth but show some variations in landform, while farther south the Gran Chaco and Pampas are broad lowlands.[68]\\r\\nThe climate of the Americas varies significantly from region to region. Tropical rainforest climate occurs in the latitudes of the Amazon, American cloud forests, Florida and Darien Gap. In the Rocky Mountains and Andes, dry and continental climates are observed. Often the higher altitudes of these mountains are snow-capped.\\r\\nSoutheastern North America is well known for its occurrence of tornadoes and hurricanes, of which the vast majority of tornadoes occur in the United States' Tornado Alley.[69] Often parts of the Caribbean are exposed to the violent effects of hurricanes. These weather systems are formed by the collision of dry, cool air from Canada and wet, warm air from the Atlantic.\\r\\nWith coastal mountains and interior plains, the Americas have several large river basins that drain the continents. The largest river basin in North America is that of the Mississippi, covering the second largest watershed on the planet.[70] The Mississippi-Missouri river system drains most of 31 states of the U.S., most of the Great Plains, and large areas between the Rocky and Appalachian mountains. This river is the fourth longest in the world and tenth most powerful in the world.\\r\\nIn North America, to the east of the Appalachian Mountains, there are no major rivers but rather a series of rivers and streams that flow east with their terminus in the Atlantic Ocean, such as the Hudson River, Saint John River, and Savannah River. A similar instance arises with central Canadian rivers that drain into Hudson Bay; the largest being the Churchill River. On the west coast of North America, the main rivers are the Colorado River, Columbia River, Yukon River, Fraser River, and Sacramento River.\\r\\nThe Colorado River drains much of the Southern Rockies and parts of the Great Basin and Range Province. The river flows approximately 1,450 miles (2,330?km) into the Gulf of California,[71] during which over time it has carved out natural phenomena such as the Grand Canyon and created phenomena such as the Salton Sea. The Columbia is a large river, 1,243 miles (2,000?km) long, in central western North America and is the most powerful river on the West Coast of the Americas. In the far northwest of North America, the Yukon drains much of the Alaskan peninsula and flows 1,980 miles (3,190?km)[72] from parts of Yukon and the Northwest Territory to the Pacific. Draining to the Arctic Ocean of Canada, the Mackenzie River drains waters from the Arctic Great Lakes of Arctic Canada, as opposed to the Saint-Lawrence River that drains the Great Lakes of Southern Canada into the Atlantic Ocean. The Mackenzie River is the largest in Canada and drains 1,805,200 square kilometers (697,000?sq?mi).[73]\\r\\nThe largest river basin in South America is that of the Amazon, which has the highest volume flow of any river on Earth.[74] The second largest watershed of South America is that of the Paran River, which covers about 2.5 million km2.[75]\\r\\nNorth America and South America began to develop a shared population of flora and fauna around 2.5 million years ago, when continental drift brought the two continents into contact via the Isthmus of Panama. Initially, the exchange of biota was roughly equal, with North American genera migrating into South America in about the same proportions as South American genera migrated into North America. This exchange is known as the Great American Interchange. The exchange became lopsided after roughly a million years, with the total spread of South American genera into North America far more limited in scope than the spread on North American genera into South America.[76]\\r\\nThere are 35 sovereign states in the Americas, as well as an autonomous country of Denmark, three overseas departments of France, three overseas collectivities of France,[77] and one uninhabited territory of France, eight overseas territories of the United Kingdom, three constituent countries of the Netherlands, three public bodies of the Netherlands, two unincorporated territories of the United States, and one uninhabited territory of the United States.[78]\\r\\nThe total population of the Americas is about 951 million people and is divided as follows:[citation needed]\\r\\nThere are three urban centers that each hold titles for being the largest population area based on the three main demographic concepts:[100]\\r\\nIn accordance with these definitions, the three largest population centers in the Americas are: Mexico City, anchor to the largest metropolitan area in the Americas; New York City, anchor to the largest urban area in the Americas; and S?o Paulo, the largest city proper in the Americas. All three cities maintain Alpha classification and large scale influence. Mexico City is the largest city in the Americas and the Western Hemisphere and Northern Hemisphere.\\r\\nMexico City ÿ The largest metropolitan area in the Americas, with a population of 22,300,000 in 2017.\\r\\nS?o Paulo ÿ Largest city with a population of 12,038,175(city) in 2016.\\r\\nNew York City ÿ Largest urban area in the Americas, with a population of 18,351,295 in 2010.\\r\\nThe population of the Americas is made up of the descendants of four large ethnic groups and their combinations.\\r\\nThe majority of the population live in Latin America, named for its predominant cultures, rooted in Latin Europe (including the two dominant languages, Spanish and Portuguese, both Romance languages), more specifically in the Iberian nations of Portugal and Spain (hence the use of the term Ibero-America as a synonym). Latin America is typically contrasted with Anglo-America, where English, a Germanic language, is prevalent, and which comprises Canada (with the exception of francophone Canada rooted in Latin Europe [France]see Qubec and Acadia) and the United States. Both countries are located in North America, with cultures deriving predominantly from Anglo-Saxon and other Germanic roots.\\r\\nThe most prevalent faiths in the Americas are as follows:\\r\\nOther faiths include Buddhism; Hinduism; Sikhism; Bah' Faith; a wide variety of indigenous religions, many of which can be categorized as animistic; new age religions and many African and African-derived religions. Syncretic faiths can also be found throughout the Americas.\\r\\nVarious languages are spoken in the Americas. Some are of European origin, others are spoken by indigenous peoples or are the mixture of various idioms like the different creoles.\\r\\nThe most widely spoken language in the Americas is Spanish.[134] The dominant language of Latin America is Spanish, though the most populous nation in Latin America, Brazil, speaks Portuguese. Small enclaves of French-, Dutch- and English-speaking regions also exist in Latin America, notably in French Guiana, Suriname, and Belize and Guyana respectively. Haitian Creole is dominant in the nation of Haiti, where French is also spoken. Native languages are more prominent in Latin America than in Anglo-America, with Nahuatl, Quechua, Aymara and Guaran as the most common. Various other native languages are spoken with less frequency across both Anglo-America and Latin America. Creole languages other than Haitian Creole are also spoken in parts of Latin America.\\r\\nThe dominant language of Anglo-America is English. French is also official in Canada, where it is the predominant language in Quebec and an official language in New Brunswick along with English. It is also an important language in Louisiana, and in parts of New Hampshire, Maine, and Vermont. Spanish has kept an ongoing presence in the Southwestern United States, which formed part of the Viceroyalty of New Spain, especially in California and New Mexico, where a distinct variety of Spanish spoken since the 17th century has survived. It has more recently become widely spoken in other parts of the United States because of heavy immigration from Latin America. High levels of immigration in general have brought great linguistic diversity to Anglo-America, with over 300 languages known to be spoken in the United States alone, but most languages are spoken only in small enclaves and by relatively small immigrant groups.\\r\\nThe nations of Guyana, Suriname, and Belize are generally considered[by whom?] not to fall into either Anglo-America or Latin America because of their language differences from Latin America, geographic differences from Anglo-America, and cultural and historical differences from both regions; English is the primary language of Guyana and Belize, and Dutch is the primary language of Suriname.\\r\\nMost of the non-native languages have, to different degrees, evolved differently from the mother country, but are usually still mutually intelligible. Some have combined, however, which has even resulted in completely new languages, such as Papiamento, which is a combination of Portuguese, Spanish, Dutch (representing the respective colonizers), native Arawak, various African languages, and, more recently English. The lingua franca Portu?ol, a mixture of Portuguese and Spanish, is spoken in the border regions of Brazil and neighboring Spanish-speaking countries.[135] More specifically, Riverense Portu?ol is spoken by around 100,000 people in the border regions of Brazil and Uruguay. Because of immigration, there are many communities where other languages are spoken from all parts of the world, especially in the United States, Brazil, Argentina, Canada, Chile, Costa Rica and Uruguayvery important destinations for immigrants.[136][137][138]\\r\\n???????Northern America\\r\\nSpeakers of English generally refer to the landmasses of North America and South America as the Americas, the Western Hemisphere, or the New World.[139] The adjective American may be used to indicate something pertains to the Americas,[3] but this term is primarily used in English to indicate something pertaining to the United States.[3][140][141] Some non-ambiguous alternatives exist, such as the adjective Pan-American,[142] or New Worlder as a demonym for a resident of the closely related New World.[4] Use of America in the hemispherical sense is sometimes retained, or can occur when translated from other languages.[143] For example, the Association of National Olympic Committees (ANOC) in Paris maintains a single continental association for \\"America\\", represented by one of the five Olympic rings.[144]\\r\\nAmerican linguist H.L. Mencken said, \\" The Latin-Americans use Norteamericano in formal writing, but, save in Panama, prefer nicknames in colloquial speech.\\"[145] To avoid \\"American\\" one can use constructed terms in their languages derived from \\"United States\\" or even \\"North America\\".[141][146][147] In Canada, its southern neighbor is often referred to as \\"the United States\\", \\"the U.S.A.\\", or (informally) \\"the States\\", while U.S. citizens are generally referred to as \\"Americans\\".[141] Most Canadians resent being referred to as \\"Americans\\".[141]\\r\\nIn Spanish, Amrica is a single continent composed of the subcontinents of Sudamrica and Norteamrica, the land bridge of Centroamrica, and the islands of the Antillas. Americano or americana in Spanish refers to a person from Amrica in a similar way that europeo or europea refers to a person from Europa. The terms sudamericano/a, centroamericano/a, antillano/a and norteamericano/a can be used to more specifically refer to the location where a person may live.\\r\\nCitizens of the United States of America are normally referred to by the term estadounidense (rough literal translation: \\"United Statesian\\") instead of americano or americana which is discouraged,[148][149] and the country's name itself is officially translated as Estados Unidos de Amrica (United States of America), commonly abbreviated as Estados Unidos (EEUU).[149] Also, the term norteamericano (North American) may refer to a citizen of the United States. This term is primarily used to refer to citizens of the United States, and less commonly to those of other North American countries.[148]\\r\\nIn Portuguese, Amrica[150] is a single continent composed of Amrica do Sul (South America), Amrica Central (Central America) and Amrica do Norte (North America).[151] It can be ambiguous, as Amrica can be used to refer to the United States of America, but is avoided in print and formal environments.[152][153]\\r\\nIn French the word amricain may be used for things relating to the Americas; however, similar to English, it is most often used for things relating to the United States. Panamricain may be used as an adjective to refer to the Americas without ambiguity.[154] French speakers may use the noun Amrique to refer to the whole landmass as one continent, or two continents, Amrique du Nord and Amrique du Sud. In French, Amrique is also used to refer to the United States, making the term ambiguous. Similar to English usage, les Amriques or des Amriques is used to refer unambiguously to the Americas.\\r\\nIn Dutch, the word Amerika mostly refers to the United States.[citation needed] Although the United States is equally often referred to as de Verenigde Staten (\\"the United States\\") or de VS (\\"the US\\"), Amerika relatively rarely refers to the Americas, but it is the only commonly used Dutch word for the Americas. This often leads to ambiguity; and to stress that something concerns the Americas as a whole, Dutch uses a combination, namely Noord- en Zuid-Amerika (North and South America).\\r\\nLatin America is generally referred to as Latijns Amerika or Midden-Amerika for Central America.\\r\\nThe adjective Amerikaans is most often used for things or people relating to the United States. There are no alternative words to distinguish between things relating to the United States or to the Americas. Dutch uses the local alternative for things relating to elsewhere in the Americas, such as Argentijns for Argentine, etc.\\r\\nThe following is a list of multinational organizations in the Americas.\\r\\nThe U.S. has the fastest-growing economy in the Americas according to a 2016 study conducted by the International Monetary Fund (IMF),[155][156] and has the highest GDP per capita in the Americas as well.[156][155] Countries in the northern part of the Americas tend to have healthier and stronger economies than countries in the southern part of the Americas.[156][155]\\r\\nIn 2016, five to seven countries in the southern part of the Americas had weakening economies in decline, compared to only three countries in the northern part of the Americas.[156][155] Haiti has the lowest GDP per capita in the Americas, although its economy was growing slightly as of 2016.[156][155]\\r\\nCoordinates: 19N 96W? / ?19N 96W? / 19; -96\\r\\n\\r\\nAfrica\\r\\n\\r\\nAntarctica\\r\\n\\r\\nAsia\\r\\n\\r\\nAustralia\\r\\n\\r\\nEurope\\r\\n\\r\\nNorth America\\r\\n\\r\\nSouth America\\r\\n\\r\\nAfro-Eurasia\\r\\n\\r\\nAmerica\\r\\n\\r\\nEurasia\\r\\n\\r\\nOceania","input":"What hemisphere is north and south america in?"},{"output":"treaties","context":"The Treaty Clause is part of Article II, Section 2, Clause 2, of the United States Constitution, that empowers the President of the United States to propose and chiefly negotiate agreements between the United States and other countries, which, upon receiving the advice and consent of a two-thirds supermajority vote of the United States Senate, become treaties under international law.\\r\\n\\r\\n\\r\\n[The President] shall have Power, by and with the Advice and Consent of the Senate, to make Treaties, provided two thirds of the Senators present concur...\\r\\nThe body of law governing U.S. foreign policy recognizes three mechanisms by which the United States enters into binding international obligations. The term \\"treaty\\" is used in a more restricted legal sense than in international law. U.S. law distinguishes what it calls treaties from congressional-executive agreements and executive agreements.[1] All three classes are considered treaties under international law; they are distinct only from the perspective of internal United States law. Distinctions among the three concern their method of ratification: by two-thirds of the Senate, by normal legislative process, or by the President alone, respectively. The Treaty Clause empowers the President to make or enter into treaties with the \\"advice and consent\\" of two-thirds of the Senate. In contrast, normal legislation becomes law after approval by simple majorities in both the Senate and the House of Representatives and the signature of the President.\\r\\nThroughout American history, presidents have also made international agreements through congressional-executive agreements, that are ratified with only a majority from both houses of Congress, or executive agreements, made by the Presidentin the exercise of his Constitutional executive powersalone.[1] Though the Constitution does not expressly provide for any alternative to the Article II treaty procedure, Article I, Section 10 of the Constitution does distinguish between treaties (which states are forbidden to make) and agreements (which states may make with the consent of Congress).[2] The Supreme Court has considered congressional-executive and executive agreements to be valid, and they have been common throughout American history. Thomas Jefferson explained that the Article II treaty procedure is not necessary when there is no long-term commitment:\\r\\nIt is desirable, in many instances, to exchange mutual advantages by Legislative Acts rather than by treaty: because the former, though understood to be in consideration of each other, and therefore greatly respected, yet when they become too inconvenient, can be dropped at the will of either party: whereas stipulations by treaty are forever irrevocable but by joint consent....[3]\\r\\nA further distinction embodied in U.S. law is between self-executing treaties, which do not require additional legislative action, and non-self-executing treaties which do require the enactment of new laws.[1][4] These various distinctions of procedure and terminology do not affect the binding status of accords under international law. Nevertheless, they do have major implications under U.S. domestic law. In Missouri v. Holland, the Supreme Court ruled that the power to make treaties under the U.S. Constitution is a power separate from the other enumerated powers of the federal government, and hence the federal government can use treaties to legislate in areas which would otherwise fall within the exclusive authority of the states. By contrast, a congressional-executive agreement can only cover matters which the Constitution explicitly places within the powers of Congress and the President.[1] Likewise, a sole-executive agreement can only cover matters within the President's authority or matters in which Congress has delegated authority to the President.[1] For example, a treaty may prohibit states from imposing capital punishment on foreign nationals, but a congressional-executive agreement or sole-executive agreement cannot.\\r\\nIn general, arms control agreements are often ratified by the treaty mechanism.[5] At the same time, trade agreements (such as the North American Free Trade Agreement and United States accession to the World Trade Organization) are generally voted on as a CEA, and such agreements typically include an explicit right to withdraw after giving sufficient written notice to the other parties.[6] If an international commercial accord contains binding \\"treaty\\" commitments, then a two-thirds vote of the Senate may be required.[7]\\r\\nBetween 1946 and 1999, the United States completed nearly 16,000 international agreements. Only 912 of those agreements were treaties, submitted to the Senate for approval as outlined in Article II of the Constitution. Since the Franklin Roosevelt presidency, only 6% of international accords have been completed as Article II treaties.[1] Most of these executive agreements consist of congressional-executive agreements.\\r\\nAmerican law is that international accords become part of the body of U.S. federal law.[1] Consequently, Congress can modify or repeal treaties by subsequent legislative action, even if this amounts to a violation of the treaty under international law. This was held, for instance, in the Head Money Cases. The most recent changes will be enforced by U.S. courts entirely independent of whether the international community still considers the old treaty obligations binding upon the U.S.[1]\\r\\nAdditionally, an international accord that is inconsistent with the U.S. Constitution is void under domestic U.S. law, the same as any other federal law in conflict with the Constitution. This principle was most clearly established in the case of Reid v. Covert.[8] The Supreme Court could rule an Article II treaty provision to be unconstitutional and void under domestic law, although it has not yet done so.\\r\\nIn Goldwater v. Carter,[9] Congress challenged the constitutionality of then-president Jimmy Carter's unilateral termination of a defense treaty. The case went before the Supreme Court and was never heard; a majority of six Justices ruled that the case should be dismissed without hearing an oral argument, holding that \\"The issue at hand ... was essentially a political question and could not be reviewed by the court, as Congress had not issued a formal opposition.\\" In his opinion, Justice Brennan dissented, \\"The issue of decision making authority must be resolved as a matter of constitutional law, not political discretion; accordingly, it falls within the competence of the courts\\". Presently, there is no official Supreme Court ruling on whether the President has the power to break a treaty without the approval of Congress, and the courts also declined to interfere when President George W. Bush unilaterally withdrew the United States from the ABM Treaty in 2002, six months after giving the required notice of intent.[10]\\r\\nPresidents have regarded the Article II treaty process as necessary where an international accord would bind a future president. For example, Theodore Roosevelt explained:\\r\\nThe Constitution did not explicitly give me power to bring about the necessary agreement with Santo Domingo. But the Constitution did not forbid my doing what I did. I put the agreement into effect, and I continued its execution for two years before the Senate acted; and I would have continued it until the end of my term, if necessary, without any action by Congress. But it was far preferable that there should be action by Congress, so that we might be proceeding under a treaty which was the law of the land and not merely by a direction of the Chief Executive which would lapse when that particular executive left office. I therefore did my best to get the Senate to ratify what I had done.[11]\\r\\nA sole-executive agreement can only be negotiated and entered into through the president's authority (1) in foreign policy, (2) as commander-in-chief of the armed forces, (3) from a prior act of Congress, or (4) from a prior treaty.[1] Agreements beyond these competencies must have the approval of Congress (for congressional-executive agreements) or the Senate (for treaties).\\r\\nIn 1972, Congress passed legislation requiring the president to notify Congress of any executive agreements that are formed.[12]\\r\\nAlthough the nondelegation doctrine prevents Congress from delegating its legislative authority to the executive branch, Congress has allowed the executive to act as Congress's \\"agent\\" in trade negotiations, such as by setting tariffs, and, in the case of Trade Promotion Authority, by solely authoring the implementing legislation for trade agreements. The constitutionality of this delegation was upheld by the Supreme Court in Field v. Clark (1892).\\r\\nWarren F. Kimball, Alliances, Coalitions, and Ententes - The American alliance system: an unamerican tradition","input":"What kinds of treaties can the president make?"},{"output":"countries where the theft of alloy wheels is a serious problem","context":"A lug nut or wheel nut is a fastener, specifically a nut, used to secure a wheel on a vehicle. Typically, lug nuts are found on automobiles, trucks (lorries), and other large vehicles using rubber tires.\\r\\n\\r\\n\\r\\nA lug nut is a nut with one rounded or conical (tapered) end, used on steel and most aluminum wheels. A set of lug nuts are typically used to secure a wheel to threaded wheel studs and thereby to a vehicle's axles.\\r\\nSome designs (Audi, BMW, Mercedes-Benz, Saab, Volkswagen) use lug bolts instead of nuts, which screw into a tapped (threaded) hole in the wheel's hub or drum brake or disc. This configuration is commonly known as a bolted joint.\\r\\nThe conical lug's taper is normally 60 degrees (although 45 is common for wheels designed for racing applications), and is designed to center the wheel accurately on the axle, and to reduce the tendency for the nut to loosen, due to fretting induced precession, as the car is driven. One popular alternative to the conical lug seating design is the spherical, or ball seat. Automotive manufactures as Audi, BMW and Honda use this design rather than a tapered seat, but the nut performs the same function.[dubious ÿ discuss][citation needed] Older style (non-ferrous) alloy wheels have a 1/2 to 1?inch cylindrical shank slipping into the wheel to center it and a washer that applies pressure to clamp the wheel to the axle.\\r\\nWheel lug nuts may have different shapes. Aftermarket alloy and forged rims often require specific lug nuts to match their mounting holes, so it is often required to get a new set of lug nuts when the rims are changed.\\r\\nThere are 4 common lug nut types:\\r\\nLug nuts may be removed using a lug, socket or impact wrench. If the wheel is to be removed then an automotive jack to raise the vehicle and some wheel chocks would be used as well. Wheels that have hubcaps or hub covers need these removed beforehand, typically with a screwdriver,[1] flatbar, or prybar. Lug nuts can be difficult to remove, as they may become frozen to the wheel stud. In such cases a breaker bar or repeated blows from an impact wrench can be used to free them. Alternating between tightening and loosening can free especially stubborn lug nuts.\\r\\nLug nuts must be installed in an alternating pattern, commonly referred to as a star pattern.[2] This ensures a uniform distribution of load across the wheel mounting surface. When installing lug nuts, it is recommended to tighten them with a calibrated torque wrench. While a lug, socket or impact wrench may be used to tighten lug nuts the final tightening should be performed by a torque wrench, ensuring an accurate and adequate load is applied. Torque specifications vary by vehicle and wheel type. Both vehicle and wheel manufacturers provide recommended torque values which should be consulted when an installation is done. Failure to abide by the recommended torque value can result in damage to the wheel and brake rotor/drum. Additionally, under tightened lug nuts may come loose with time.\\r\\nIn order to allow early detection of loose lug nuts, some large vehicles are fitted with loose wheel nut indicators. The indicator spins with the nut, so that it can be detected with a visual inspection.\\r\\nIn countries where the theft of alloy wheels is a serious problem, locking nuts (or bolts, as applicable) are available - or already fitted by the vehicle manufacturer - which require a special adaptor (\\"key\\") between the nut and the wrench to fit and remove. The key is normally unique to each set of nuts. Only one locking nut per wheel is normally used, so they are sold in sets of four. Most designs can be defeated using a hardened removal tool which uses a left-hand self-cutting thread to grip the locking nut, although more advanced designs have a spinning outer ring to frustrate such techniques.\\r\\nIn the United States, vehicles manufactured by the Chrysler Corporation used left-hand and right-hand screw thread for different sides of the vehicle to prevent loosening prior to the 1975 models. Most Buicks, Pontiacs, and Oldsmobiles used both left-handed and right-handed lug nuts prior to model year 1965.[3] It was later realized that the taper seat performed the same function. Modern vehicles use right-hand threads on all wheels.","input":"When are wheel lock lug nuts commonly used?"},{"output":"28 October 1974","context":"The Equal Credit Opportunity Act (ECOA) is a United States law (codified at 15 U.S.C.??1691 et seq.), enacted 28 October 1974,[1] that makes it unlawful for any creditor to discriminate against any applicant, with respect to any aspect of a credit transaction, on the basis of race, color, religion, national origin, sex, marital status, or age (provided the applicant has the capacity to contract);[2] to the fact that all or part of the applicant's income derives from a public assistance program; or to the fact that the applicant has in good faith exercised any right under the Consumer Credit Protection Act. The law applies to any person who, in the ordinary course of business, regularly participates in a credit decision,[3] including banks, retailers, bankcard companies, finance companies, and credit unions.\\r\\n\\r\\nThe part of the law that defines its authority and scope is known as Regulation B,[4] from the (b) that appears in Title 12 part 1002's official identifier: 12 C.F.R.  1002.1(b) (2017).[5] Failure to comply with Regulation B can subject a financial institution to civil liability for actual and punitive damages in individual or class actions. Liability for punitive damages can be as much as $10,000 in individual actions and the lesser of $500,000 or 1% of the creditor's net worth in class actions.[6]\\r\\n\\r\\nAmong other things, the ECOA states that it is illegal for creditors to:[3][1]\\r\\n\\r\\nThe ECOA states that creditors must:\\r\\n\\r\\nWhen the Banking committee marked up the ECOA, congresswoman Lindy Boggs added the provision banning discrimination due to sex or marital status without informing the other members of the committee beforehand, personally inserting the language on her own and photocopying new versions of the bill.[7] She then told the other committee members, \\"Knowing the members composing this committee as well as I do, I'm sure it was just an oversight that we didn't have 'sex' or 'marital status' included. I've taken care of that, and I trust it meets with the committee's approval.\\"[7] The committee unanimously approved the bill.[7]","input":"When was the equal credit opportunity act passed?"},{"output":"it failed to enforce its rule, and its vast territory was divided into several successor polities","context":"","input":"What really caused the fall of the vast roman empire?"},{"output":"15 August 1947","context":"Independence Day is annually celebrated on 15 August, as a national holiday in India commemorating the nation's independence from the United Kingdom on 15 August 1947, the UK Parliament passed the Indian Independence Act 1947 transferring legislative sovereignty to the Indian Constituent Assembly. India still retained King George VI as head of state until its transition to full republican constitution. India attained independence following the Independence Movement noted for largely nonviolent resistance and civil disobedience led by the Indian National Congress (INC). Independence coincided with the partition of India, in which the British India was divided along religious lines into the Dominions of India and Pakistan; the partition was accompanied by violent riots and mass casualties, and the displacement of nearly 15 million people due to religious violence. On 15 August 1947, the Prime Minister, Jawaharlal Nehru raised the Indian national flag above the Lahori Gate of the Red Fort in Delhi. On each subsequent Independence Day, the prime minister customarily raises the flag and gives an address to the nation.[1]\\r\\nThe holiday is observed throughout India with flag-hoisting ceremonies, parades and cultural events. There is a national holiday, and schools and government offices distribute sweets, but no official work is done.[2][3]\\r\\n\\r\\n\\r\\nEuropean traders had established outposts in the Indian subcontinent by the 17th century. Through overwhelming military strength, the British East India company subdued local kingdoms and established themselves as the dominant force by the 18th century. Following the First War of Independence of 1857, the Government of India Act 1858 led the British Crown to assume direct control of India. In the decades following, civic society gradually emerged across India, most notably the Indian National Congress Party, formed in 1885.[4][5]:123 The period after World War I was marked by British reforms such as the MontagueÿChelmsford Reforms, but it also witnessed the enactment of the repressive Rowlatt Act and calls for self-rule by Indian activists. The discontent of this period crystallised into nationwide non-violent movements of non-cooperation and civil disobedience, led by Mohandas Karamchand Gandhi.[5]:167\\r\\nDuring the 1930 s, reform was gradually legislated by the British; Congress won victories in the resulting elections.[5]:195ÿ197 The next decade was beset with political turmoil: Indian participation in World War II, the Congress' final push for non-cooperation, and an upsurge of Muslim nationalism led by the All-India Muslim League. The escalating political tension was capped by Independence in 1947. The jubilation was tempered by the bloody partition of the subcontinent into India and Pakistan.[5]:203\\r\\nAt the 1929 Lahore session of the Indian National Congress, the Purna Swaraj declaration, or \\"Declaration of the Independence of India\\" was promulgated,[6] and 15 August was declared as Independence Day.[6] The Congress called on people to pledge themselves to civil disobedience and \\"to carry out the Congress instructions issued from time to time\\" until India attained complete independence.[7] Celebration of such an Independence Day was envisioned to stoke nationalistic fervour among Indian citizens, and to force the British government to consider granting independence.[8]:19 The Congress observed 26 January as the Independence Day between 1930 and 1946.[9][10] The celebration was marked by meetings where the attendants took the \\"pledge of independence\\".[8]:19ÿ20 Jawaharlal Nehru described in his autobiography that such meetings were peaceful, solemn, and \\"without any speeches or exhortation\\".[11] Gandhi envisaged that besides the meetings, the day would be spent \\"...?in doing some constructive work, whether it is spinning, or service of 'untouchables,' or reunion of Hindus and Mussalmans, or prohibition work, or even all these together\\".[12] Following actual independence in 1947, the Constitution of India came into effect on and from 26 January 1950; since then 26 January is celebrated as Republic Day.\\r\\nIn 1946, the Labour government in Britain, its exchequer exhausted by the recently concluded World War II, realised that it had neither the mandate at home, the international support, nor the reliability of native forces for continuing to control an increasingly restless India.[5]:203[13][14][15] In February 1947, Prime Minister Clement Attlee announced that the British government would grant full self-governance to British India by June 1948 at the latest.[16]\\r\\nThe new viceroy, Lord Mountbatten, advanced the date for the transfer of power, believing the continuous contention between the Congress and the Muslim League might lead to a collapse of the interim government.[17] He chose the second anniversary of Japan's surrender in World War II, 15 August, as the date of power transfer.[17] The British government announced on 3 June 1947 that it had accepted the idea of partitioning British India into two states;[16] the successor governments would be given dominion status and would have an implicit right to secede from the British Commonwealth. The Indian Independence Act 1947 (10 & 11 Geo 6 c. 30) of the Parliament of the United Kingdom partitioned British India into the two new independent dominions of India and Pakistan (including what is now Bangladesh) with effect from 15 August 1947, and granted complete legislative authority upon the respective constituent assemblies of the new countries.[18] The Act received royal assent on 18 July 1947.\\r\\nMillions of Muslim, Sikh and Hindu refugees trekked the newly drawn borders in the months surrounding independence.[20] In Punjab, where the borders divided the Sikh regions in halves, massive bloodshed followed; in Bengal and Bihar, where Mahatma Gandhi's presence assuaged communal tempers, the violence was mitigated. In all, between 250,000 and 1,000,000 people on both sides of the new borders died in the violence.[21] While the entire nation was celebrating the Independence Day, Gandhi stayed in Calcutta in an attempt to stem the carnage.[22] On 14 August 1947, the Independence Day of Pakistan, the new Dominion of Pakistan came into being; Muhammad Ali Jinnah was sworn in as its first Governor General in Karachi.\\r\\nThe Constituent Assembly of India met for its fifth session at 11 pm on 14 August in the Constitution Hall in New Delhi.[23] The session was chaired by the president Rajendra Prasad. In this session, Jawaharlal Nehru delivered the Tryst with Destiny speech proclaiming India's independence.\\r\\nLong years ago we made a tryst with destiny, and now the time comes when we shall redeem our pledge, not wholly or in full measure, but very substantially. At the stroke of the midnight hour, when the world sleeps, India will awake to life and freedom. A moment comes, which comes but rarely in history, when we step out from the old to the new, when an age ends, and when the soul of a nation, long suppressed, finds utterance. It is fitting that at this solemn moment, we take the pledge of dedication to the service of India and her people and to the still larger cause of humanity.\\r\\nThe members of the Assembly formally took the pledge of being in the service of the country. A group of women, representing the women of India, formally presented the national flag to the assembly.\\r\\nThe Dominion of India became an independent country as official ceremonies took place in New Delhi. Nehru assumed office as the first prime minister, and the viceroy, Lord Mountbatten, continued as its first governor general.[19]:6 Gandhi's name was invoked by crowds celebrating the occasion; Gandhi himself however took no part in the official events. Instead, he marked the day with a 24-hour fast, during which he spoke to a crowd in Calcutta, encouraging peace between Hindu and Muslim.[19]:10\\r\\nIndependence Day, one of the three National holidays in India (the other two being the Republic Day on 26 January and Mahatma Gandhi's birthday on 2 October), is observed in all Indian states and union territories. On the eve of Independence Day, the President of India delivers the \\"Address to the Nation\\". On 15 August, the prime minister hoists the Indian flag on the ramparts of the historical site Red Fort in Delhi. Twenty-one gun shots are fired in honour of the solemn occasion.[25] In his speech, the prime minister highlights the past year's achievements, raises important issues and calls for further development. He pays tribute to the leaders of the Indian independence movement. The Indian national anthem, \\"Jana Gana Mana\\", is sung. The speech is followed by march past of divisions of the Indian Armed Forces and paramilitary forces. Parades and pageants showcase scenes from the independence struggle and India's diverse cultural traditions. Similar events take place in state capitals where the Chief Ministers of individual states unfurl the national flag, followed by parades and pageants.[26][27]\\r\\nFlag hoisting ceremonies and cultural programmes take place in governmental and non-governmental institutions throughout the country.[28] Schools and colleges conduct flag hoisting ceremonies and cultural events. Major government buildings are often adorned with strings of lights.[29] In Delhi and some other cities, kite flying adds to the occasion.[25][30] National flags of different sizes are used abundantly to symbolise allegiance to the country.[31] Citizens adorn their clothing, wristbands, cars, household accessories with replicas of the tri-colour.[31] Over a period of time, the celebration has changed emphasis from nationalism to a broader celebration of all things India.[32][33]\\r\\nThe Indian diaspora celebrates Independence Day around the world with parades and pageants, particularly in regions with higher concentrations of Indian immigrants.[34] In some locations, such as New York and other US cities, 15 August has become \\"India Day\\" among the diaspora and the local populace. Pageants celebrate \\"India Day\\" either on 15 August or an adjoining weekend day.[35]\\r\\nKashmiris throughout the Kashmir Valley have observed India's independence day as a 'black day' since late 1980's. General strike and a civil curfew is organized with ceremonial burning of the Indian flag. Most shops remain closed and a reduced traffic is seen on the roads.[36][37][38][39] Black flags are hoisted on buildings throughout the India-administered Kashmir. Its purpose is to send a message to the international community that India has usurped Kashmiris inalienable right to self-determination through the use of force.[40]\\r\\nAs early as three years after independence, the Naga National Council called for a boycott of Independence Day in northeast India.[41] Separatist protests in this region intensified in the 1980s; calls for boycotts and terrorist attacks by insurgent organisations such as the United Liberation Front of Assam and the National Democratic Front of Bodoland, marred celebrations.[42] With increasing insurgency in Jammu and Kashmir from the late 1980s,[43] separatist protesters boycotted Independence Day there with bandh (strikes), use of black flags and by flag burning.[44][37][45] Terrorist outfits such as Lashkar-e-Taiba, the Hizbul Mujahideen and the Jaish-e-Mohammed have issued threats, and have carried out attacks around Independence Day.[46] Boycotting of the celebration has also been advocated by insurgent Maoist rebel organisations.[47][48]\\r\\nIn the anticipation of terrorist attacks, particularly from militants, security measures are intensified, especially in major cities such as Delhi and Mumbai and in troubled states such as Jammu and Kashmir.[49][50] The airspace around the Red Fort is declared a no-fly zone to prevent aerial attacks[51] and additional police forces are deployed in other cities.[52]\\r\\nOn Independence Day and Republic Day, patriotic songs in regional languages are broadcast on television and radio channels.[53] They are also played alongside flag hoisting ceremonies.[53] Patriotic films are broadcast.[28] Over the decades, according to The Times of India, the number of such films broadcast has decreased as channels report that audiences are oversaturated with patriotic films.[54] The population cohort that belong to the Generation Next often combine nationalism with popular culture during the celebrations. This mixture is exemplified by outfits and savouries dyed with the tricolour and designer garments that represent India's various cultural traditions.[32][55] Retail stores offer Independence Day sales promotions.[56][57] Some news reports have decried the commercialism.[56][58][59] Indian Postal Service publishes commemorative stamps depicting independence movement leaders, nationalistic themes and defence-related themes on 15 August.[60]\\r\\nIndependence and partition inspired literary and other artistic creations.[61] Such creations mostly describe the human cost of partition, limiting the holiday to a small part of their narrative.[62][63] Salman Rushdie's novel Midnight's Children (1980), which won the Booker Prize and the Booker of Bookers, wove its narrative around children born at midnight of 14ÿ15 August 1947 with magical abilities.[63] Freedom at Midnight (1975) is a non-fiction work by Larry Collins and Dominique Lapierre that chronicled the events surrounding the first Independence Day celebrations in 1947. Few films centre on the moment of independence,[64][65][66] instead highlighting the circumstances of partition and its aftermath.[64][67][68] On the Internet, Google has commemorated Independence Day since 2003 with a special doodle on its Indian homepage.[69]","input":"When did india get independence from british rule?"},{"output":"Bart Gets an \\"F\\"","context":"\\r\\n\\r\\n\\"Bart Gets an \\"F\\"\\" is the first episode of The Simpsons' second season. It originally aired on the Fox network in the United States on October 11, 1990. In the episode, Bart fails four consecutive history exams and the school psychiatrist recommends that Bart repeat the fourth grade. Bart vows that he will start to do better and attempts to get the resident class genius Martin Prince to help him, but after that backfires, Bart prays for help. That night, Springfield is hit with a massive blizzard and the school is closed, giving Bart another day to study.\\r\\n\\r\\nThe episode was written by David M. Stern and directed by David Silverman. Mayor Quimby makes his first appearance and the episode was the first to feature a new opening sequence. \\"Bart Gets an \\"F\\"\\" was the third episode produced for the second season, but it was chosen to be the season premiere because it prominently featured Bart.\\r\\n\\r\\nDue to the success of the first season of The Simpsons, Fox decided to switch the show's time slot to Thursday at 8:00?p.m. EST where it would air opposite of NBC's The Cosby Show, the number one show at the time. Through the summer, several news outlets published stories about the supposed \\"Bill vs. Bart\\" rivalry and heavily hyped the first episode of the second season. Several critics predicted that \\"Bart Gets an \\"F\\"\\" would do considerably worse in the ratings than The Cosby Show. However, the final Nielsen rating for the episode was 18.4 and a 29% share of the audience, finishing second in its time slot behind The Cosby Show, which had an 18.5 rating and 29% share. It finished eighth in the weekly ratings, but was watched by an estimated 33.6 million viewers, making it the number one show in terms of actual viewers that week. It became the highest rated and most watched program in the history of the Fox network and remained in that position until January 1, 1995. Currently, it is still the highest rated episode in the history of The Simpsons.\\r\\n\\r\\nThe episode has received positive reviews from television critics and was ranked 31st on Entertainment Weekly's 1999 list of \\"The 100 Greatest Moments in Television\\".\\r\\n\\r\\nBart presents a book report at Springfield Elementary School on Treasure Island, but throughout his presentation, it is blatantly obvious that he did not read the book. After school, Mrs. Krabappel tells Bart his grades have steadily gotten worse and warns him about an upcoming exam on Colonial America, but Bart does not pay attention, and puts off studying. The next day at school, Bart feigns illness and that night, Lisa warns Bart he cannot evade his responsibilities forever, but Bart calls Milhouse for the test answers. After school the next day, an overconfident Bart hands in his test, only to get a poor score that is even worse than Milhouse's substandard test and have Mrs. Krabappel take remedial action.\\r\\n\\r\\nHomer and Marge are called in to meet with Mrs. Krabappel and school psychiatrist Dr. J. Loren Pryor. Dr. Pryor says that Bart is an underachiever and recommends that he should repeat the fourth-grade. Homer and Marge consider that holding Bart back might not be such a bad idea. However, Bart is against this idea, and vows that he will start to do better and will pass. In desperation, Bart asks Martin Prince to help him get a passing grade. Martin is initially reluctant to help Bart, but agrees to do so when Bart agrees to show him how to become more popular. The two boys initially help each other out, but Martin starts to take on some of Bart's poor character traits. He decides to quit being a bookworm and hang out with his new friends and play practical jokes, and abandons Bart. Left with little time to study on his own, Bart prays to God and asks that something miraculous happen to make him miss school the next day so he can have more time to study. That night, Springfield is hit with a massive blizzard, and the schools are closed.\\r\\n\\r\\nAfter receiving word of the school closures, Bart prepares for a fun snow day. However, Lisa reminds him of his prayer, and Bart decides to make good with God by studying while everyone is outside having fun. The next day, he finishes the test and asks Mrs. Krabappel to grade it immediately. She does so, giving him a 59, failing by just one point. Bart tears up at having failed despite all his efforts. Mrs. Krabappel is initially stunned and tries to console him, but when Bart compares his failure to George Washington's surrender of Fort Necessity to the French in 1754, Mrs. Krabappel is impressed at this obscure historical reference, realizes that Bart has put more effort and gives him an extra point for demonstrating applied knowledge, pushing his grade up to a D-, a barely passing grade. An ecstatic Bart runs throughout Springfield, exclaiming to people that he actually passed.\\r\\n\\r\\n\\"Bart Gets an \\"F\\"\\" was the first episode of The Simpsons to be written by David M. Stern. It was directed by David Silverman. Over the summer of 1990, Bart's rebellious nature was characterized by some parents and conservatives as a poor role model for children,[3] while several American public schools banned T-shirts featuring Bart next to captions such as \\"I'm Bart Simpson. Who the hell are you?\\" and \\"Underachiever ('And proud of it, man!')\\".[4] Several critics thought that the episode was a response to these controversies.[5][6] However, executive producer James L. Brooks responded that it was not, but added, \\"we're mindful of it. I do think it's important for us that Bart does badly in school. There are students like that. Besides, I'm very wary of television where everybody is supposed to be a role model. You don't run across that many role models in real life. Why should television be full of them?\\"[7] Sam Simon commented that \\"there are themes to the shows we did last year, important themes, I think it's a tribute to how well we executed them that nobody realized we had a point.\\"[8] Bart says \\"Cowabunga\\" for the second time (the first time being in \\"The Telltale Head\\"), which was commonly associated with Bart through its use as a T-shirt slogan.[9] Mayor Quimby makes his first appearance in this episode, without his trademark sash that says \\"Mayor\\".[2] The sash was later added because the writers feared that viewers would not recognize him.[10]\\r\\n\\r\\nThe episode was the first to feature a new opening sequence, which was shortened by fifteen seconds from its original length of roughly 1 minute, 30 seconds. The opening sequence for the first season showed Bart stealing a \\"Bus Stop\\" sign; whilst the new sequence featured him skateboarding past several characters who had been introduced during the previous season. Starting with this season, there were three versions of the opening: a full roughly 1 minute 15 second long version, a 45-second version and a 25-second version. This gave the show's editors more leeway.[10] David Silverman believes that the animators began to \\"come into their own\\" as they had gotten used to the characters and were able to achieve more with character acting. During the scene where Bart delivers a speech where he states he is \\"dumb as a post\\", Silverman wanted to cut from several angles very quickly to give a sense of anxiety.[10] Martin Prince's design was changed several times during the episode. There was a different model that had larger eyes and wilder hair designed for the scene where Martin betrays Bart and runs off.[10] Silverman describes the \\"Snow Day\\" sequence as one of the hardest things he ever had to animate. It features several long pans which shows many different characters engaging in various activities and was difficult to time correctly.[10] Bart's fantasy where he sees the founding fathers of the United States uses muted colors and variations of red, white and blue.[10] Silverman also had to work hard to make Bart cry without making his design look too off-putting, and this is the reason why he was shown covering his face with a piece of paper.[10]\\r\\n\\r\\nThe first season of The Simpsons had finished as high as 4th in the weekly ratings[11] and was the Fox network's first series to rank among a season's top 30 highest-rated shows[12] and Bart quickly became one of the most popular characters on television in what was termed \\"Bartmania\\".[13][14][15] Due to the success of the first season of the show, the Fox network decided to switch The Simpsons' timeslot in hopes that it would steal ratings from NBC's \\"powerhouse\\" line up, generate more advertising revenue,[16] and result in higher ratings for Beverly Hills, 90210 and Babes, which would follow the show.[17][18] The show was moved from its from 8:00?p.m. EST Sunday night slot to the same time on Thursday, where it would compete with NBC's The Cosby Show, the number one show at the time.[17] Many of the producers of The Simpsons, including James L. Brooks, were against the move. The show had been in the top 10 while airing on Sunday and they felt the move would destroy its ratings.[19] He commented that \\"Suddenly a show that was a hit is fighting for its survival, [...] We're not fighting Cosby, we just want to get healthy ratings. There have been two weeks in my life when a show I was associated with was number one in the ratings, and on Sunday night, we had a chance to be the number one show in the country. I don't think we have a chance on Thursday night.\\"[7]\\r\\n\\r\\n\\"Two Cars in Every Garage and Three Eyes on Every Fish\\" was the first episode produced for the season, but \\"Bart Gets an \\"F\\"\\" aired first because Bart was popular at the time and the producers had wanted to premiere with an episode involving him.[9] It aired opposite the fourth episode of the seventh season of The Cosby Show titled \\"Period of Adjustment\\", which saw the addition of Erika Alexander to the cast.[20] The first 13 episodes of The Simpsons had been rerun several times through the summer, and Fox heavily promoted the first new episode since May,[21] and news outlets published stories about the supposed \\"Bill vs. Bart\\" rivalry.[19][22][23]\\r\\n\\r\\nReruns of The Simpsons which aired in the Thursday time slot against new episodes of The Cosby Show were ranked as low as 73rd in the weekly ratings (compared with third place for The Cosby Show).[24][25] Several critics predicted that \\"Bart Gets an \\"F\\"\\" would do considerably worse in the ratings than The Cosby Show.[11] Greg Dawson of the Orlando Sentinel wrote that he would \\"bet dollars to plain-cake doughnuts (a Homer pet peeve) that even a fresh Simpsons won't come within five rating points of Cosby, which could get a 30 share in a power blackout.\\"[23] Fox executive Peter Chernin said that they were hoping to establish a foothold on Thursday night and that \\"if we're really lucky and very fortunate, we're going to come in second place.\\"[26]\\r\\n\\r\\nEarly overnight ratings figures for the original broadcast of the episode in 24 cities projected that The Simpsons had a 19.9 Nielsen Rating and 30% share of the audience while The Cosby Show had a 19.3 Nielsen Rating and 29% share.[27][28] However, the final rating for \\"Bart Gets an \\"F\\"\\" was an 18.4 and a 29% share of the audience, finishing second in its time slot behind The Cosby Show, which had an 18.5 rating and 29% share.[29] At the time, NBC had 208 television stations, while Fox only had 133.[30] It finished eighth in the weekly ratings, tied with Who's the Boss?, while The Cosby Show finished seventh.[31] The rating is based on the number of household televisions that were tuned into the show, but Nielsen Media Research estimated that 33.6 million viewers watched the episode, making it the number one show in terms of actual viewers that week (The Cosby Show was watched by 28.5 million, finished seventh).[32] It became the highest rated and most watched program in the history of the Fox Network.[32] It remained in that position until January 1, 1995, when a National Football League playoff game between the Minnesota Vikings and Chicago Bears achieved a Nielsen Rating of 21.0.[33] To date, it is still the highest rated episode in the history of The Simpsons.[34]\\r\\n\\r\\nBart's slapdash book report was on the Robert Louis Stevenson novel Treasure Island, while Martin presents Ernest Hemingway's The Old Man and the Sea. Later on, Martin makes remarks about the forecastle of the Pequod in reference to Moby Dick.[1] During \\"Snow Day\\", the citizens of Springfield sing \\"Winter Wonderland\\".[1] The scene where everyone in Springfield gathers around the town circle, holds hands and begins singing is a reference to How the Grinch Stole Christmas![10] \\"Hallelujah\\", the chorus from George Frideric Handel's Messiah, can be heard when it starts snowing.[35]\\r\\n\\r\\nThe episode has received positive reviews from television critics. The authors of the book I Can't Believe It's a Bigger and Better Updated Unofficial Simpsons Guide, Warren Martyn and Adrian Wood, wrote, \\"A cracking opener to the second season - especially memorable for the sequence in which Bart prays for school to be cancelled the following day only to find himself exiled from the ensuing winter wonderland.\\"[2] Virginia Mann of The Record felt that it was \\"not as wildly funny as last season's best episodes, [but still] well-done, humorous, and, at times, poignant.\\"[5] The episode was praised for its emotional scenes. Tom Shales wrote that the episode is \\"not only funny, it's touching\\" and praised it for its scenes where Bart prays, writing \\"There are few if any other entertainment shows on television that get into philosophical matters even this deeply. The Simpsons can be as thoughtful as a furrow-browed Bill Moyers pontification - yet infinitely more amusing.\\"[36] Hal Boedeker of The Miami Herald felt that it \\"pulls off a finale that's thoughtful without being preachy, tender without being sappy. Despite the tears, the show keeps its edge. And the way TV usually smears on the schmaltz, that's quite an achievement.\\"[37] Phil Kloer of The Atlanta Journal-Constitution wrote \\"The episode does a good job of emphasizing the importance of studying without getting gooky. For all the talk about the anarchy of The Simpsons, the show sometimes has smuggled in an occasional message, as it does again.\\"[38] In his book The Gospel According to the Simpsons, Mark I. Pinsky writes that \\"Bart Gets an \\"F\\"\\" offers the most detailed portrayal of the dynamic of prayer on The Simpsons.\\"[35] Steve L. Case later included the episode in his book Toons That Teach, a list of 75 cartoons that help teach biblical lessons.[39]\\r\\n\\r\\nThe episode was ranked 31st on Entertainment Weekly's list of \\"The 100 Greatest Moments in Television\\", writing that it \\"stands as classic irreverent family TV\\".[40] In 2007, Larina Adamson, a supervising producer on The Simpsons, named \\"Bart Gets an 'F'\\" as her favorite episode of the series.[41] In 2010 the BBC named \\"Bart Gets an \\"F\\"\\" as one of the ten most memorable episodes of the show, calling it \\"insightful and poignant\\".[42]","input":"What is the most watched simpsons episode ever?"},{"output":"fifteen","context":"King Arthur is a legendary British leader who, according to medieval histories and romances, led the defence of Britain against Saxon invaders in the late 5th and early 6th centuries AD. The details of Arthur's story are mainly composed of folklore and literary invention, and his historical existence is debated and disputed by modern historians.[2] The sparse historical background of Arthur is gleaned from various sources, including the Annales Cambriae, the Historia Brittonum, and the writings of Gildas. Arthur's name also occurs in early poetic sources such as Y Gododdin.[3]\\r\\nArthur is a central figure in the legends making up the Matter of Britain. The legendary Arthur developed as a figure of international interest largely through the popularity of Geoffrey of Monmouth's fanciful and imaginative 12th-century Historia Regum Britanniae (History of the Kings of Britain).[4] In some Welsh and Breton tales and poems that date from before this work, Arthur appears either as a great warrior defending Britain from human and supernatural enemies or as a magical figure of folklore, sometimes associated with the Welsh Otherworld, Annwn.[5] How much of Geoffrey's Historia (completed in 1138) was adapted from such earlier sources, rather than invented by Geoffrey himself, is unknown.\\r\\nAlthough the themes, events and characters of the Arthurian legend varied widely from text to text, and there is no one canonical version; Geoffrey's version of events often served as the starting point for later stories. Geoffrey depicted Arthur as a king of Britain who defeated the Saxons and established an empire over Britain, Ireland, Iceland, Norway and Gaul. Many elements and incidents that are now an integral part of the Arthurian story appear in Geoffrey's Historia, including Arthur's father Uther Pendragon, the wizard Merlin, Arthur's wife Guinevere, the sword Excalibur, Arthur's conception at Tintagel, his final battle against Mordred at Camlann, and final rest in Avalon. The 12th-century French writer Chrtien de Troyes, who added Lancelot and the Holy Grail to the story, began the genre of Arthurian romance that became a significant strand of medieval literature. In these French stories, the narrative focus often shifts from King Arthur himself to other characters, such as various Knights of the Round Table. Arthurian literature thrived during the Middle Ages but waned in the centuries that followed until it experienced a major resurgence in the 19th century. In the 21st century, the legend lives on, not only in literature but also in adaptations for theatre, film, television, comics and other media.\\r\\n\\r\\n\\r\\nThe historical basis for the King Arthur legend has long been debated by scholars. One school of thought, citing entries in the Historia Brittonum (History of the Britons) and Annales Cambriae (Welsh Annals), sees Arthur as a genuine historical figure, a Romano-British leader who fought against the invading Anglo-Saxons some time in the late 5th to early 6th century. The Historia Brittonum, a 9th-century Latin historical compilation attributed in some late manuscripts to a Welsh cleric called Nennius, contains the first datable mention of King Arthur, listing twelve battles that Arthur fought. These culminate in the Battle of Badon, where he is said to have single-handedly killed 960 men. Recent studies, however, question the reliability of the Historia Brittonum.[7]\\r\\nThe other text that seems to support the case for Arthur's historical existence is the 10th-century Annales Cambriae, which also link Arthur with the Battle of Badon. The Annales date this battle to 516ÿ518, and also mention the Battle of Camlann, in which Arthur and Medraut (Mordred) were both killed, dated to 537ÿ539. These details have often been used to bolster confidence in the Historia's account and to confirm that Arthur really did fight at Badon. Problems have been identified, however, with using this source to support the Historia Brittonum's account. The latest research shows that the Annales Cambriae was based on a chronicle begun in the late 8th century in Wales. Additionally, the complex textual history of the Annales Cambriae precludes any certainty that the Arthurian annals were added to it even that early. They were more likely added at some point in the 10th century and may never have existed in any earlier set of annals. The Badon entry probably derived from the Historia Brittonum.[8]\\r\\nThis lack of convincing early evidence is the reason many recent historians exclude Arthur from their accounts of sub-Roman Britain. In the view of historian Thomas Charles-Edwards, \\"at this stage of the enquiry, one can only say that there may well have been an historical Arthur [but ...] the historian can as yet say nothing of value about him\\".[9] These modern admissions of ignorance are a relatively recent trend; earlier generations of historians were less sceptical. The historian John Morris made the putative reign of Arthur the organising principle of his history of sub-Roman Britain and Ireland, The Age of Arthur (1973). Even so, he found little to say about an historical Arthur.[10]\\r\\nPartly in reaction to such theories, another school of thought emerged which argued that Arthur had no historical existence at all. Morris's Age of Arthur prompted the archaeologist Nowell Myres to observe that \\"no figure on the borderline of history and mythology has wasted more of the historian's time\\".[11] Gildas' 6th-century polemic De Excidio et Conquestu Britanniae (On the Ruin and Conquest of Britain), written within living memory of Badon, mentions the battle but does not mention Arthur.[12] Arthur is not mentioned in the Anglo-Saxon Chronicle or named in any surviving manuscript written between 400 and 820.[13] He is absent from Bede's early-8th-century Ecclesiastical History of the English People, another major early source for post-Roman history that mentions Badon.[14] The historian David Dumville has written: \\"I think we can dispose of him [Arthur] quite briefly. He owes his place in our history books to a 'no smoke without fire' school of thought?... The fact of the matter is that there is no historical evidence about Arthur; we must reject him from our histories and, above all, from the titles of our books.\\"[15]\\r\\nSome scholars argue that Arthur was originally a fictional hero of folkloreor even a half-forgotten Celtic deitywho became credited with real deeds in the distant past. They cite parallels with figures such as the Kentish Hengist and Horsa, who may be totemic horse-gods that later became historicised. Bede ascribed to these legendary figures a historical role in the 5th-century Anglo-Saxon conquest of eastern Britain.[16] It is not even certain that Arthur was considered a king in the early texts. Neither the Historia nor the Annales calls him \\"rex\\": the former calls him instead \\"dux bellorum\\" (leader of battles) and \\"miles\\" (soldier).[17]\\r\\nHistorical documents for the post-Roman period are scarce, so a definitive answer to the question of Arthur's historical existence is unlikely. Sites and places have been identified as \\"Arthurian\\" since the 12th century,[18] but archaeology can confidently reveal names only through inscriptions found in secure contexts. The so-called \\"Arthur stone\\", discovered in 1998 among the ruins at Tintagel Castle in Cornwall in securely dated 6th-century contexts, created a brief stir but proved irrelevant.[19] Other inscriptional evidence for Arthur, including the Glastonbury cross, is tainted with the suggestion of forgery.[20] Although several historical figures have been proposed as the basis for Arthur,[21] no convincing evidence for these identifications has emerged.\\r\\nThe origin of the Welsh name \\"Arthur\\" remains a matter of debate. Some suggest it is derived from the Roman nomen gentile (family name) Artorius, of obscure and contested etymology[22] (but possibly of Messapian[23][24][25] or Etruscan origin).[26][27][28] Some scholars have suggested it is relevant to this debate that the legendary King Arthur's name only appears as Arthur, or Arturus, in early Latin Arthurian texts, never as Artrius (though Classical Latin Artrius became Arturius in some Vulgar Latin dialects). However, this may not say anything about the origin of the name Arthur, as Artrius would regularly become Art(h)ur when borrowed into Welsh.[29]\\r\\nAnother possibility is that it is derived from a Brittonic patronym *Arto-rؐg-ios (the root of which, *arto-rؐg- \\"bear-king\\" is to be found in the Old Irish personal name Art-ri) via a Latinized form Artrius.[30] Less likely is the commonly proposed derivation from Welsh arth \\"bear\\" + (g)wr \\"man\\" (earlier *Arto-uiros in Brittonic); there are phonological difficulties with this theorynotably that a Brittonic compound name *Arto-uiros should produce Old Welsh *Artgur and Middle/Modern Welsh *Arthwr and not Arthur (in Welsh poetry the name is always spelled Arthur and is exclusively rhymed with words ending in -ur ÿ never words ending in -wr ÿ which confirms that the second element cannot be [g]wr \\"man\\").[31][32]\\r\\nAn alternative theory, which has gained only limited acceptance among professional scholars, derives the name Arthur from Arcturus, the brightest star in the constellation Bo?tes, near Ursa Major or the Great Bear.[33] Classical Latin Arcturus would also have become Art(h)ur when borrowed into Welsh, and its brightness and position in the sky led people to regard it as the \\"guardian of the bear\\" (which is the meaning of the name in Ancient Greek) and the \\"leader\\" of the other stars in Bo?tes.[34]\\r\\nA similar first name is Old Irish Art~r, which is believed to be derived directly from an early Old Welsh or Cumbric Artur.[35] The earliest historically attested bearer of the name is a son or grandson of edn mac Gabrin (d. AD 609).[36]\\r\\nThe creator of the familiar literary persona of Arthur was Geoffrey of Monmouth, with his pseudo-historical Historia Regum Britanniae (History of the Kings of Britain), written in the 1130s. The textual sources for Arthur are usually divided into those written before Geoffrey's Historia (known as pre-Galfridian texts, from the Latin form of Geoffrey, Galfridus) and those written afterwards, which could not avoid his influence (Galfridian, or post-Galfridian, texts).\\r\\nThe earliest literary references to Arthur come from Welsh and Breton sources. There have been few attempts to define the nature and character of Arthur in the pre-Galfridian tradition as a whole, rather than in a single text or text/story-type. A 2007 academic survey that does attempt this by Thomas Green identifies three key strands to the portrayal of Arthur in this earliest material.[37] The first is that he was a peerless warrior who functioned as the monster-hunting protector of Britain from all internal and external threats. Some of these are human threats, such as the Saxons he fights in the Historia Brittonum, but the majority are supernatural, including giant cat-monsters, destructive divine boars, dragons, dogheads, giants, and witches.[38] The second is that the pre-Galfridian Arthur was a figure of folklore (particularly topographic or onomastic folklore) and localised magical wonder-tales, the leader of a band of superhuman heroes who live in the wilds of the landscape.[39] The third and final strand is that the early Welsh Arthur had a close connection with the Welsh Otherworld Annwn. On the one hand, he launches assaults on Otherworldly fortresses in search of treasure and frees their prisoners. On the other, his warband in the earliest sources includes former pagan gods, and his wife and his possessions are clearly Otherworldly in origin.[40]\\r\\nOne of the most famous Welsh poetic references to Arthur comes in the collection of heroic death-songs known as Y Gododdin (The Gododdin), attributed to 6th-century poet Aneirin. One stanza praises the bravery of a warrior who slew 300 enemies, but says that despite this, \\"he was no Arthur\\" ÿ that is, his feats cannot compare to the valour of Arthur.[41] Y Gododdin is known only from a 13th-century manuscript, so it is impossible to determine whether this passage is original or a later interpolation, but John Koch's view that the passage dates from a 7th-century or earlier version is regarded as unproven; 9th- or 10th-century dates are often proposed for it.[42] Several poems attributed to Taliesin, a poet said to have lived in the 6th century, also refer to Arthur, although these all probably date from between the 8th and 12th centuries.[43] They include \\"Kadeir Teyrnon\\" (\\"The Chair of the Prince\\"),[44] which refers to \\"Arthur the Blessed\\"; \\"Preiddeu Annwn\\" (\\"The Spoils of Annwn\\"),[45] which recounts an expedition of Arthur to the Otherworld; and \\"Marwnat vthyr pen[dragon]\\" (\\"The Elegy of Uther Pen[dragon]\\"),[46] which refers to Arthur's valour and is suggestive of a father-son relationship for Arthur and Uther that pre-dates Geoffrey of Monmouth.\\r\\nOther early Welsh Arthurian texts include a poem found in the Black Book of Carmarthen, \\"Pa gur yv y porthaur?\\" (\\"What man is the gatekeeper?\\").[48] This takes the form of a dialogue between Arthur and the gatekeeper of a fortress he wishes to enter, in which Arthur recounts the names and deeds of himself and his men, notably Cei (Kay) and Bedwyr (Bedivere). The Welsh prose tale Culhwch and Olwen (c.?1100), included in the modern Mabinogion collection, has a much longer list of more than 200 of Arthur's men, though Cei and Bedwyr again take a central place. The story as a whole tells of Arthur helping his kinsman Culhwch win the hand of Olwen, daughter of Ysbaddaden Chief-Giant, by completing a series of apparently impossible tasks, including the hunt for the great semi-divine boar Twrch Trwyth. The 9th-century Historia Brittonum also refers to this tale, with the boar there named Troy(n)t.[49] Finally, Arthur is mentioned numerous times in the Welsh Triads, a collection of short summaries of Welsh tradition and legend which are classified into groups of three linked characters or episodes to assist recall. The later manuscripts of the Triads are partly derivative from Geoffrey of Monmouth and later continental traditions, but the earliest ones show no such influence and are usually agreed to refer to pre-existing Welsh traditions. Even in these, however, Arthur's court has started to embody legendary Britain as a whole, with \\"Arthur's Court\\" sometimes substituted for \\"The Island of Britain\\" in the formula \\"Three XXX of the Island of Britain\\".[50] While it is not clear from the Historia Brittonum and the Annales Cambriae that Arthur was even considered a king, by the time Culhwch and Olwen and the Triads were written he had become Penteyrnedd yr Ynys hon, \\"Chief of the Lords of this Island\\", the overlord of Wales, Cornwall and the North.[51]\\r\\nIn addition to these pre-Galfridian Welsh poems and tales, Arthur appears in some other early Latin texts besides the Historia Brittonum and the Annales Cambriae. In particular, Arthur features in a number of well-known vitae (\\"Lives\\") of post-Roman saints, none of which are now generally considered to be reliable historical sources (the earliest probably dates from the 11th century).[52] According to the Life of Saint Gildas, written in the early 12th century by Caradoc of Llancarfan, Arthur is said to have killed Gildas' brother Hueil and to have rescued his wife Gwenhwyfar from Glastonbury.[53] In the Life of Saint Cadoc, written around 1100 or a little before by Lifris of Llancarfan, the saint gives protection to a man who killed three of Arthur's soldiers, and Arthur demands a herd of cattle as wergeld for his men. Cadoc delivers them as demanded, but when Arthur takes possession of the animals, they turn into bundles of ferns.[54] Similar incidents are described in the medieval biographies of Carannog, Padarn, and Eufflam, probably written around the 12th century. A less obviously legendary account of Arthur appears in the Legenda Sancti Goeznovii, which is often claimed to date from the early 11th century (although the earliest manuscript of this text dates from the 15th century).[55] Also important are the references to Arthur in William of Malmesbury's De Gestis Regum Anglorum and Herman's De Miraculis Sanctae Mariae Laudensis, which together provide the first certain evidence for a belief that Arthur was not actually dead and would at some point return, a theme that is often revisited in post-Galfridian folklore.[56]\\r\\nThe first narrative account of Arthur's life is found in Geoffrey of Monmouth's Latin work Historia Regum Britanniae (History of the Kings of Britain), completed c.?1138.[57] This work is an imaginative and fanciful account of British kings from the legendary Trojan exile Brutus to the 7th-century Welsh king Cadwallader. Geoffrey places Arthur in the same post-Roman period as do Historia Brittonum and Annales Cambriae. He incorporates Arthur's father, Uther Pendragon, his magician advisor Merlin, and the story of Arthur's conception, in which Uther, disguised as his enemy Gorlois by Merlin's magic, sleeps with Gorlois's wife Igerna at Tintagel, and she conceives Arthur. On Uther's death, the fifteen-year-old Arthur succeeds him as King of Britain and fights a series of battles, similar to those in the Historia Brittonum, culminating in the Battle of Bath. He then defeats the Picts and Scots before creating an Arthurian empire through his conquests of Ireland, Iceland and the Orkney Islands. After twelve years of peace, Arthur sets out to expand his empire once more, taking control of Norway, Denmark and Gaul. Gaul is still held by the Roman Empire when it is conquered, and Arthur's victory naturally leads to a further confrontation between his empire and Rome's. Arthur and his warriors, including Kaius (Kay), Beduerus (Bedivere) and Gualguanus (Gawain), defeat the Roman emperor Lucius Tiberius in Gaul but, as he prepares to march on Rome, Arthur hears that his nephew Modredus (Mordred)whom he had left in charge of Britainhas married his wife Guenhuuara (Guinevere) and seized the throne. Arthur returns to Britain and defeats and kills Modredus on the river Camblam in Cornwall, but he is mortally wounded. He hands the crown to his kinsman Constantine and is taken to the isle of Avalon to be healed of his wounds, never to be seen again.[58]\\r\\nHow much of this narrative was Geoffrey's own invention is open to debate. Certainly, Geoffrey seems to have made use of the list of Arthur's twelve battles against the Saxons found in the 9th-century Historia Brittonum, along with the battle of Camlann from the Annales Cambriae and the idea that Arthur was still alive.[60] Arthur's personal status as the king of all Britain would also seem to be borrowed from pre-Galfridian tradition, being found in Culhwch and Olwen, the Triads, and the saints' lives.[61] Finally, Geoffrey borrowed many of the names for Arthur's possessions, close family, and companions from the pre-Galfridian Welsh tradition, including Kaius (Cei), Beduerus (Bedwyr), Guenhuuara (Gwenhwyfar), Uther (Uthyr) and perhaps also Caliburnus (Caledfwlch), the latter becoming Excalibur in subsequent Arthurian tales.[62] However, while names, key events, and titles may have been borrowed, Brynley Roberts has argued that \\"the Arthurian section is Geoffrey's literary creation and it owes nothing to prior narrative.\\"[63] So, for instance, the Welsh Medraut is made the villainous Modredus by Geoffrey, but there is no trace of such a negative character for this figure in Welsh sources until the 16th century.[64] There have been relatively few modern attempts to challenge this notion that the Historia Regum Britanniae is primarily Geoffrey's own work, with scholarly opinion often echoing William of Newburgh's late-12th-century comment that Geoffrey \\"made up\\" his narrative, perhaps through an \\"inordinate love of lying\\".[65] Geoffrey Ashe is one dissenter from this view, believing that Geoffrey's narrative is partially derived from a lost source telling of the deeds of a 5th-century British king named Riotamus, this figure being the original Arthur, although historians and Celticists have been reluctant to follow Ashe in his conclusions.[66]\\r\\nWhatever his sources may have been, the immense popularity of Geoffrey's Historia Regum Britanniae cannot be denied. Well over 200 manuscript copies of Geoffrey's Latin work are known to have survived, and this does not include translations into other languages.[67] Thus, for example, around 60 manuscripts are extant containing Welsh-language versions of the Historia, the earliest of which were created in the 13th century; the old notion that some of these Welsh versions actually underlie Geoffrey's Historia, advanced by antiquarians such as the 18th-century Lewis Morris, has long since been discounted in academic circles.[68] As a result of this popularity, Geoffrey's Historia Regum Britanniae was enormously influential on the later medieval development of the Arthurian legend. While it was by no means the only creative force behind Arthurian romance, many of its elements were borrowed and developed (e.g., Merlin and the final fate of Arthur), and it provided the historical framework into which the romancers' tales of magical and wonderful adventures were inserted.[69]\\r\\nThe popularity of Geoffrey's Historia and its other derivative works (such as Wace's Roman de Brut) is generally agreed to be an important factor in explaining the appearance of significant numbers of new Arthurian works in continental Europe during the 12th and 13th centuries, particularly in France.[70] It was not, however, the only Arthurian influence on the developing \\"Matter of Britain\\". There is clear evidence that Arthur and Arthurian tales were familiar on the Continent before Geoffrey's work became widely known (see for example, the Modena Archivolt),[71] and \\"Celtic\\" names and stories not found in Geoffrey's Historia appear in the Arthurian romances.[72] From the perspective of Arthur, perhaps the most significant effect of this great outpouring of new Arthurian story was on the role of the king himself: much of this 12th-century and later Arthurian literature centres less on Arthur himself than on characters such as Lancelot and Guinevere, Percival, Galahad, Gawain, Ywain, and Tristan and Iseult. Whereas Arthur is very much at the centre of the pre-Galfridian material and Geoffrey's Historia itself, in the romances he is rapidly sidelined.[73] His character also alters significantly. In both the earliest materials and Geoffrey he is a great and ferocious warrior, who laughs as he personally slaughters witches and giants and takes a leading role in all military campaigns,[74] whereas in the continental romances he becomes the roi fainant, the \\"do-nothing king\\", whose \\"inactivity and acquiescence constituted a central flaw in his otherwise ideal society\\".[75] Arthur's role in these works is frequently that of a wise, dignified, even-tempered, somewhat bland, and occasionally feeble monarch. So, he simply turns pale and silent when he learns of Lancelot's affair with Guinevere in the Mort Artu, whilst in Chrtien de Troyes's Yvain, the Knight of the Lion, he is unable to stay awake after a feast and has to retire for a nap.[76] Nonetheless, as Norris J. Lacy has observed, whatever his faults and frailties may be in these Arthurian romances, \\"his prestige is neveror almost nevercompromised by his personal weaknesses?... his authority and glory remain intact.\\"[77]\\r\\nArthur and his retinue appear in some of the Lais of Marie de France,[79] but it was the work of another French poet, Chrtien de Troyes, that had the greatest influence with regard to the development of Arthur's character and legend.[80] Chrtien wrote five Arthurian romances between c.?1170 and 1190. Erec and Enide and Cligs are tales of courtly love with Arthur's court as their backdrop, demonstrating the shift away from the heroic world of the Welsh and Galfridian Arthur, while Yvain, the Knight of the Lion, features Yvain and Gawain in a supernatural adventure, with Arthur very much on the sidelines and weakened. However, the most significant for the development of the Arthurian legend are Lancelot, the Knight of the Cart, which introduces Lancelot and his adulterous relationship with Arthur's queen (Guinevere), extending and popularising the recurring theme of Arthur as a cuckold, and Perceval, the Story of the Grail, which introduces the Holy Grail and the Fisher King and which again sees Arthur having a much reduced role.[81] Chrtien was thus \\"instrumental both in the elaboration of the Arthurian legend and in the establishment of the ideal form for the diffusion of that legend\\",[82] and much of what came after him in terms of the portrayal of Arthur and his world built upon the foundations he had laid. Perceval, although unfinished, was particularly popular: four separate continuations of the poem appeared over the next half century, with the notion of the Grail and its quest being developed by other writers such as Robert de Boron, a fact that helped accelerate the decline of Arthur in continental romance.[83] Similarly, Lancelot and his cuckolding of Arthur with Guinevere became one of the classic motifs of the Arthurian legend, although the Lancelot of the prose Lancelot (c.?1225) and later texts was a combination of Chrtien's character and that of Ulrich von Zatzikhoven's Lanzelet.[84] Chrtien's work even appears to feed back into Welsh Arthurian literature, with the result that the romance Arthur began to replace the heroic, active Arthur in Welsh literary tradition.[85] Particularly significant in this development were the three Welsh Arthurian romances, which are closely similar to those of Chrtien, albeit with some significant differences: Owain, or the Lady of the Fountain is related to Chrtien's Yvain; Geraint and Enid, to Erec and Enide; and Peredur son of Efrawg, to Perceval.[86]\\r\\nUp to c.?1210, continental Arthurian romance was expressed primarily through poetry; after this date the tales began to be told in prose. The most significant of these 13th-century prose romances was the Vulgate Cycle (also known as the Lancelot-Grail Cycle), a series of five Middle French prose works written in the first half of that century.[88] These works were the Estoire del Saint Grail, the Estoire de Merlin, the Lancelot propre (or Prose Lancelot, which made up half the entire Vulgate Cycle on its own), the Queste del Saint Graal and the Mort Artu, which combine to form the first coherent version of the entire Arthurian legend. The cycle continued the trend towards reducing the role played by Arthur in his own legend, partly through the introduction of the character of Galahad and an expansion of the role of Merlin. It also made Mordred the result of an incestuous relationship between Arthur and his sister and established the role of Camelot, first mentioned in passing in Chrtien's Lancelot, as Arthur's primary court.[89] This series of texts was quickly followed by the Post-Vulgate Cycle (c.?1230ÿ40), of which the Suite du Merlin is a part, which greatly reduced the importance of Lancelot's affair with Guinevere but continued to sideline Arthur, and to focus more on the Grail quest.[88] As such, Arthur became even more of a relatively minor character in these French prose romances; in the Vulgate itself he only figures significantly in the Estoire de Merlin and the Mort Artu. During this period, Arthur was made one of the Nine Worthies, a group of three pagan, three Jewish and three Christian exemplars of chivalry. The Worthies were first listed in Jacques de Longuyon's Voeux du Paon in 1312, and subsequently became a common subject in literature and art.[90]\\r\\nThe development of the medieval Arthurian cycle and the character of the \\"Arthur of romance\\" culminated in Le Morte d'Arthur, Thomas Malory's retelling of the entire legend in a single work in English in the late 15th century. Malory based his bookoriginally titled The Whole Book of King Arthur and of His Noble Knights of the Round Tableon the various previous romance versions, in particular the Vulgate Cycle, and appears to have aimed at creating a comprehensive and authoritative collection of Arthurian stories.[91] Perhaps as a result of this, and the fact that Le Morte D'Arthur was one of the earliest printed books in England, published by William Caxton in 1485, most later Arthurian works are derivative of Malory's.[92]\\r\\nThe end of the Middle Ages brought with it a waning of interest in King Arthur. Although Malory's English version of the great French romances was popular, there were increasing attacks upon the truthfulness of the historical framework of the Arthurian romances ÿ established since Geoffrey of Monmouth's time ÿ and thus the legitimacy of the whole Matter of Britain. So, for example, the 16th-century humanist scholar Polydore Vergil famously rejected the claim that Arthur was the ruler of a post-Roman empire, found throughout the post-Galfridian medieval \\"chronicle tradition\\", to the horror of Welsh and English antiquarians.[93] Social changes associated with the end of the medieval period and the Renaissance also conspired to rob the character of Arthur and his associated legend of some of their power to enthrall audiences, with the result that 1634 saw the last printing of Malory's Le Morte d'Arthur for nearly 200 years.[94] King Arthur and the Arthurian legend were not entirely abandoned, but until the early 19th century the material was taken less seriously and was often used simply as a vehicle for allegories of 17th- and 18th-century politics.[95] Thus Richard Blackmore's epics Prince Arthur (1695) and King Arthur (1697) feature Arthur as an allegory for the struggles of William III against James II.[95] Similarly, the most popular Arthurian tale throughout this period seems to have been that of Tom Thumb, which was told first through chapbooks and later through the political plays of Henry Fielding; although the action is clearly set in Arthurian Britain, the treatment is humorous and Arthur appears as a primarily comedic version of his romance character.[96]\\r\\nJohn Dryden's masque King Arthur is still performed, largely thanks to Henry Purcell's music, though seldom unabridged.\\r\\nIn the early 19th century, medievalism, Romanticism, and the Gothic Revival reawakened interest in Arthur and the medieval romances. A new code of ethics for 19th-century gentlemen was shaped around the chivalric ideals embodied in the \\"Arthur of romance\\". This renewed interest first made itself felt in 1816, when Malory's Le Morte d'Arthur was reprinted for the first time since 1634.[98] Initially, the medieval Arthurian legends were of particular interest to poets, inspiring, for example, William Wordsworth to write \\"The Egyptian Maid\\" (1835), an allegory of the Holy Grail.[99] Pre-eminent among these was Alfred Tennyson, whose first Arthurian poem \\"The Lady of Shalott\\" was published in 1832.[100] Arthur himself played a minor role in some of these works, following in the medieval romance tradition. Tennyson's Arthurian work reached its peak of popularity with Idylls of the King, however, which reworked the entire narrative of Arthur's life for the Victorian era. It was first published in 1859 and sold 10,000 copies within the first week.[101] In the Idylls, Arthur became a symbol of ideal manhood who ultimately failed, through human weakness, to establish a perfect kingdom on earth.[102] Tennyson's works prompted a large number of imitators, generated considerable public interest in the legends of Arthur and the character himself, and brought Malory's tales to a wider audience.[103] Indeed, the first modernisation of Malory's great compilation of Arthur's tales was published in 1862, shortly after Idylls appeared, and there were six further editions and five competitors before the century ended.[104]\\r\\nThis interest in the 'Arthur of romance' and his associated stories continued through the 19th century and into the 20th, and influenced poets such as William Morris and Pre-Raphaelite artists including Edward Burne-Jones.[105] Even the humorous tale of Tom Thumb, which had been the primary manifestation of Arthur's legend in the 18th century, was rewritten after the publication of Idylls. While Tom maintained his small stature and remained a figure of comic relief, his story now included more elements from the medieval Arthurian romances and Arthur is treated more seriously and historically in these new versions.[106] The revived Arthurian romance also proved influential in the United States, with such books as Sidney Lanier's The Boy's King Arthur (1880) reaching wide audiences and providing inspiration for Mark Twain's satiric A Connecticut Yankee in King Arthur's Court (1889).[107] Although the 'Arthur of romance' was sometimes central to these new Arthurian works (as he was in Burne-Jones's \\"The Sleep of Arthur in Avalon\\", 1881-1898), on other occasions he reverted to his medieval status and is either marginalized or even missing entirely, with Wagner's Arthurian operas providing a notable instance of the latter.[108] Furthermore, the revival of interest in Arthur and the Arthurian tales did not continue unabated. By the end of the 19th century, it was confined mainly to Pre-Raphaelite imitators,[109] and it could not avoid being affected by World War I, which damaged the reputation of chivalry and thus interest in its medieval manifestations and Arthur as chivalric role model.[110] The romance tradition did, however, remain sufficiently powerful to persuade Thomas Hardy, Laurence Binyon and John Masefield to compose Arthurian plays,[111] and T. S. Eliot alludes to the Arthur myth (but not Arthur) in his poem The Waste Land, which mentions the Fisher King.[112]\\r\\nIn the latter half of the 20th century, the influence of the romance tradition of Arthur continued, through novels such as T. H. White's The Once and Future King (1958) and Marion Zimmer Bradley's The Mists of Avalon (1982) in addition to comic strips such as Prince Valiant (from 1937 onward).[114] Tennyson had reworked the romance tales of Arthur to suit and comment upon the issues of his day, and the same is often the case with modern treatments too. Bradley's tale, for example, takes a feminist approach to Arthur and his legend, in contrast to the narratives of Arthur found in medieval materials,[115] and American authors often rework the story of Arthur to be more consistent with values such as equality and democracy.[116] The romance Arthur has become popular in film and theatre as well. T. H. White's novel was adapted into the Lerner and Loewe stage musical Camelot (1960) and Walt Disney's animated film The Sword in the Stone (1963); Camelot, with its focus on the love of Lancelot and Guinevere and the cuckolding of Arthur, was itself made into a film of the same name in 1967. The romance tradition of Arthur is particularly evident and, according to critics, successfully handled in Robert Bresson's Lancelot du Lac (1974), ric Rohmer's Perceval le Gallois (1978) and perhaps John Boorman's fantasy film Excalibur (1981); it is also the main source of the material used in the Arthurian spoof Monty Python and the Holy Grail (1975).[117]\\r\\nRe-tellings and re-imaginings of the romance tradition are not the only important aspect of the modern legend of King Arthur. Attempts to portray Arthur as a genuine historical figure of c.?500, stripping away the \\"romance\\", have also emerged. As Taylor and Brewer have noted, this return to the medieval \\"chronicle tradition\\" of Geoffrey of Monmouth and the Historia Brittonum is a recent trend which became dominant in Arthurian literature in the years following the outbreak of the Second World War, when Arthur's legendary resistance to Germanic invaders struck a chord in Britain.[118] Clemence Dane's series of radio plays, The Saviours (1942), used a historical Arthur to embody the spirit of heroic resistance against desperate odds, and Robert Sherriff's play The Long Sunset (1955) saw Arthur rallying Romano-British resistance against the Germanic invaders.[119] This trend towards placing Arthur in a historical setting is also apparent in historical and fantasy novels published during this period.[120] In recent years the portrayal of Arthur as a real hero of the 5th century has also made its way into film versions of the Arthurian legend, most notably the TV series' Arthur of the Britons (1972ÿ73), Merlin (2008ÿ12), The Legend of King Arthur (1979), and Camelot (2011) [121] and the feature films King Arthur (2004), The Last Legion (2007) and King Arthur: Legend of the Sword (2017).[122]\\r\\nArthur has also been used as a model for modern-day behaviour. In the 1930s, the Order of the Fellowship of the Knights of the Round Table was formed in Britain to promote Christian ideals and Arthurian notions of medieval chivalry.[123] In the United States, hundreds of thousands of boys and girls joined Arthurian youth groups, such as the Knights of King Arthur, in which Arthur and his legends were promoted as wholesome exemplars.[124] However, Arthur's diffusion within contemporary culture goes beyond such obviously Arthurian endeavours, with Arthurian names being regularly attached to objects, buildings, and places. As Norris J. Lacy has observed, \\"The popular notion of Arthur appears to be limited, not surprisingly, to a few motifs and names, but there can be no doubt of the extent to which a legend born many centuries ago is profoundly embedded in modern culture at every level.\\"[125]","input":"How old was arthur pendragon when he became king?"},{"output":"Phoenix","context":"Miranda v. Arizona, 384 U.S. 436 (1966), was a landmark decision of the United States Supreme Court. In a 5ÿ4 majority, the Court held that both inculpatory and exculpatory statements made in response to interrogation by a defendant in police custody will be admissible at trial only if the prosecution can show that the defendant was informed of the right to consult with an attorney before and during questioning and of the right against self-incrimination before police questioning, and that the defendant not only understood these rights, but voluntarily waived them.\\r\\nThis case has a significant impact on law enforcement in the United States, by making what became known as the Miranda rights part of routine police procedure to ensure that suspects were informed of their rights. The Supreme Court decided Miranda with three other consolidated cases: Westover v. United States, Vignera v. New York, and California v. Stewart.\\r\\nThe Miranda warning (often shortened to \\"Miranda\\", or \\"Mirandizing\\" a suspect) is the name of the formal warning that is required to be given by law enforcement in the United States to criminal suspects in police custody (or in a custodial situation) before they are interrogated, in accordance with the Miranda ruling. Its purpose is to ensure the accused are aware of, and reminded of, these rights before questioning or actions that are reasonably likely to elicit an incriminating response.\\r\\nPer the U.S. Supreme Court decision Berghuis v. Thompkins (June 1, 2010), criminal suspects who are aware of their right to silence and to an attorney, but choose not to \\"unambiguously\\" invoke them, may find any subsequent voluntary statements treated as an implied waiver of their rights, and used as or as part of evidence. At least one scholar has argued that Thompkins effectively gutted Miranda.[1]\\r\\n\\r\\n\\r\\nDuring the 1960s, a movement which provided defendants with legal aid emerged from the collective efforts of various bar associations.\\r\\nIn the civil realm, it led to the creation of the Legal Services Corporation under the Great Society program of President Lyndon Baines Johnson. Escobedo v. Illinois, a case which closely foreshadowed Miranda, provided for the presence of counsel during police interrogation. This concept extended to a concern over police interrogation practices, which were considered by many[who?] to be barbaric and unjust. Coercive interrogation tactics were known in period slang as the \\"third degree\\".[citation needed]\\r\\nOn March 13, 1963, Ernesto Miranda was arrested, by the Phoenix Police Department, based on circumstantial evidence linking him to the kidnapping and rape of an eighteen-year-old woman ten days earlier.[2] After two hours of interrogation by police officers, Miranda signed a confession to the rape charge on forms that included the typed statement: \\"I do hereby swear that I make this statement voluntarily and of my own free will, with no threats, coercion, or promises of immunity, and with full knowledge of my legal rights, understanding any statement I make may be used against me.\\"[3]\\r\\nHowever, at no time was Miranda told of his right to counsel. Before being presented with the form on which he was asked to write out the confession he had already given orally, he was not advised of his right to remain silent, nor was he informed that his statements during the interrogation would be used against him. At trial, when prosecutors offered Miranda's written confession as evidence, his court-appointed lawyer, Alvin Moore, objected that because of these facts, the confession was not truly voluntary and should be excluded. Moore's objection was overruled and based on this confession and other evidence, Miranda was convicted of rape and kidnapping. He was sentenced to 20ÿ30 years of imprisonment on each charge, with sentences to run concurrently. Moore filed Miranda's appeal to the Arizona Supreme Court, claiming that Miranda's confession was not fully voluntary and should not have been admitted into the court proceedings. The Arizona Supreme Court affirmed the trial court's decision to admit the confession in State v. Miranda, 401 P.2d 721 (Ariz. 1965). In affirmation, the Arizona Supreme emphasized heavily the fact that Miranda did not specifically request an attorney.[4]\\r\\nAttorney John Paul Frank, former law clerk to Justice Hugo Black, represented Miranda in his appeal to the U.S. Supreme Court.[5]\\r\\nChief Justice Earl Warren, a former prosecutor, delivered the opinion of the Court[6], ruling that due to the coercive nature of the custodial interrogation by police (Warren cited several police training manuals which had not been provided in the arguments), no confession could be admissible under the Fifth Amendment self-incrimination clause and Sixth Amendment right to an attorney unless a suspect had been made aware of his rights and the suspect had then waived them:\\r\\nThe person in custody must, prior to interrogation, be clearly informed that he has the right to remain silent, and that anything he says will be used against him in court; he must be clearly informed that he has the right to consult with a lawyer and to have the lawyer with him during interrogation, and that, if he is indigent, a lawyer will be appointed to represent him.[7]\\r\\nThus, Miranda's conviction was overturned. The Court also made clear what had to happen if the suspect chose to exercise his or her rights:\\r\\nIf the individual indicates in any manner, at any time prior to or during questioning, that he wishes to remain silent, the interrogation must cease... If the individual states that he wants an attorney, the interrogation must cease until an attorney is present. At that time, the individual must have an opportunity to confer with the attorney and to have him present during any subsequent questioning.\\r\\nWarren pointed to the existing practice of the Federal Bureau of Investigation (FBI) and the rules of the Uniform Code of Military Justice, both of which required notifying a suspect of his right to remain silent; the FBI warning included notice of the right to counsel.\\r\\nHowever, the dissenting justices accused the majority of overreacting to the problem of coercive interrogations, and anticipated a drastic effect. They believed that, once warned, suspects would always demand attorneys, and deny the police the ability to gain confessions.\\r\\nIn a separate concurrence in part, dissent in part, Justice Tom C. Clark argued that the Warren Court went \\"too far too fast\\". Instead, Justice Clark would use the \\"totality of the circumstances\\" test enunciated by Justice Goldberg in Haynes v. Washington. Under this test, the court would:\\r\\nconsider in each case whether the police officer prior to custodial interrogation added the warning that the suspect might have counsel present at the interrogation and, further, that a court would appoint one at his request if he was too poor to employ counsel. In the absence of warnings, the burden would be on the State to prove that counsel was knowingly and intelligently waived or that in the totality of the circumstances, including the failure to give the necessary warnings, the confession was clearly voluntary.\\r\\nIn dissent, Justice John Marshall Harlan II wrote that \\"nothing in the letter or the spirit of the Constitution or in the precedents squares with the heavy-handed and one-sided action that is so precipitously taken by the Court in the name of fulfilling its constitutional responsibilities\\". Harlan closed his remarks by quoting former Justice Robert H. Jackson: \\"This Court is forever adding new stories to the temples of constitutional law, and the temples have a way of collapsing when one story too many is added.\\"\\r\\nJustice Byron White took issue with the court announcing a new constitutional right when it had no \\"factual and textual bases\\" in the Constitution or previous opinions of the Court for the rule announced in the opinion. He stated: \\"The proposition that the privilege against self-incrimination forbids in-custody interrogation without the warnings specified in the majority opinion and without a clear waiver of counsel has no significant support in the history of the privilege or in the language of the Fifth Amendment.\\" Nor did Justice White believe it had any basis in English common law.\\r\\nWhite further warned of the dire consequences of the majority opinion:\\r\\nI have no desire whatsoever to share the responsibility for any such impact on the present criminal process.\\r\\nIn some unknown number of cases, the Court's rule will return a killer, a rapist or other criminal to the streets and to the environment which produced him, to repeat his crime whenever it pleases him. As a consequence, there will not be a gain, but a loss, in human dignity.\\r\\nMiranda was retried in 1967 after the original case against him was thrown out. This time the prosecution, instead of using the confession, introduced other evidence and called witnesses. One witness was Twila Hoffman, a woman with whom Miranda was living at the time of the offense; she testified that he had told her of committing the crime.[8][9] Miranda was convicted in 1967 and sentenced to serve 20 to 30 years.[9] The Supreme Court of Arizona affirmed,[10] and the United States Supreme Court denied review.[11] Miranda was paroled in 1972. After his release, he returned to his old neighborhood and made a modest living autographing police officers' \\"Miranda cards\\" which contained the text of the warning, for reading to arrestees. He was stabbed to death during an argument in a bar on January 31, 1976.[12] A suspect was arrested, but he, unlike Miranda, exercised his right to remain silent. With no evidence against him, he was released.[13]\\r\\nAnother three defendants whose cases had been tied in with Miranda's ÿ an armed robber, a stick-up man, and a bank robber ÿ either made plea bargains to lesser charges or were found guilty again despite the exclusion of their confessions.[14]\\r\\nThe Miranda decision was widely criticized when it came down, as many felt it was unfair to inform suspected criminals of their rights, as outlined in the decision. Richard Nixon and other conservatives denounced Miranda for undermining the efficiency of the police, and argued the ruling would contribute to an increase in crime. Nixon, upon becoming President, promised to appoint judges who would be \\"strict constructionists\\" and who would exercise judicial restraint. Many supporters of law enforcement were angered by the decision's negative view of police officers.[citation needed]\\r\\nAfter the Miranda decision, the nation's police departments were required to inform arrested persons or suspects of their rights under the ruling prior to custodial interrogation.[15] Such information is called a Miranda warning. Since it is usually required that the suspects be asked if they understand their rights, courts have also ruled that any subsequent waiver of Miranda rights must be knowing, intelligent, and voluntary.[16]\\r\\nMany American police departments have pre-printed Miranda waiver forms which a suspect must sign and date (after hearing and reading the warnings again) if an interrogation is to occur.[17][18]\\r\\nData from the FBI Uniform Crime Reports shows a sharp reduction in the clearance rate of violent and property crimes after Miranda.[19] However, according to other studies from the 1960s and 1970s, \\"contrary to popular belief, Miranda had little, if any, effect on detectives ability to solve crimes.\\"[9]\\r\\nThe federal Omnibus Crime Control and Safe Streets Act of 1968 purported to overrule Miranda for federal criminal cases and restore the \\"totality of the circumstances\\" test that had prevailed previous to Miranda.[20] The validity of this provision of the law, which is still codified at 18 U.S.C. ?3501, was not ruled on for another 30 years because the Justice Department never attempted to rely on it to support the introduction of a confession into evidence at any criminal trial.[citation needed]\\r\\nMiranda was undermined by several subsequent decisions which seemed to grant exceptions to the \\"Miranda warnings\\", challenging its claim to be a necessary corollary of the Fifth Amendment. The exceptions and developments that occurred over the years included:\\r\\nUnited States v. Garibay (1998) pointed out an important matter in regards to expansion of Miranda. Garibay barely spoke English and clearly showed a lack of understanding; indeed, \\"the agent admitted that he had to rephrase questions when the defendant appeared confused.\\"[22] The U.S. Court of Appeals ruled a \\"clear error\\" when the district court found that Garibay had \\"knowingly and intelligently waived his Miranda rights\\" due to the defendants low I.Q. and poor English language skills.[23] The court investigated many facets of his waiver and discovered that Mr. Garibay was missing all items that they were looking for: he never signed a waiver, he only received his warnings verbally and in English, and no interpreter was provided although they were available. With an opinion that stressed \\"the requirement that a defendant 'knowingly and intelligently' waive his Miranda rights\\", the Court reversed Garibay's conviction and remanded his case.[23]\\r\\nMiranda survived a strong challenge in Dickerson v. United States, 530 U.S. 428 (2000), where the validity of Congress's overruling of Miranda through ?3501 was tested. At issue was whether the Miranda warnings were actually compelled by the Constitution, or were rather merely measures enacted as a matter of judicial policy.[citation needed] In Dickerson, the Court, speaking through Chief Justice Rehnquist, upheld Miranda 7ÿ2 and stated that \\"the warnings have become part of our national culture\\". In dissent, Justice Scalia argued Miranda warnings were not constitutionally required. He cited several cases demonstrating a majority of the then-current court, counting himself, and Justices Kennedy, O'Connor, and Thomas, as well as the Chief Justice (who had just delivered a contrary opinion), \\"[were] on record as believing that a violation of Miranda is not a violation of the Constitution\\".[citation needed]\\r\\nOver time, interrogators began to think of techniques to honor the \\"letter\\" but not the \\"spirit\\" of Miranda. In the case of Missouri v. Seibert, 542 U.S. 600 (2004), the Supreme Court halted one of the more controversial practices. Missouri police were deliberately withholding Miranda warnings and questioning suspects until they obtained confessions, then giving the warnings, getting waivers, and getting confessions again. Justice Souter wrote for the plurality: \\"Strategists dedicated to draining the substance out of Miranda cannot accomplish by training instructions what Dickerson held Congress could not do by statute.\\"[24]\\r\\nBerghuis v. Thompkins (2010) is a ruling where the Supreme Court held that a suspect's \\"ambiguous or equivocal\\" statement or no statements do not mean that police must end an interrogation.[25] At least one scholar has argued that Thompkins effectively gutted Miranda. In The Right to Remain Silent, Charles Weisselberg wrote that \\"the majority in Thompkins rejected the fundamental underpinnings of Miranda v. Arizonas prophylactic rule and established a new one that fails to protect the rights of suspects\\" and that\\r\\nBut in Thompkins, neither Michigan nor the Solicitor General were able to cite any decision in which a court found that a suspect had given an implied waiver after lengthy questioning. Thompkins persevered for almost three hours before succumbing to his interrogators. In finding a waiver on these facts, Thompkins gives us an implied waiver doctrine on steroids.[1]","input":"When did the miranda vs arizona take place?"},{"output":"a non-academic activity that all students, regardless of nationality, must participate in","context":"In Singapore, a co-curricular activity (CCA), previously known as an extracurricular activity (ECA), is a non-academic activity that all students, regardless of nationality, must participate in. This policy was introduced by the Ministry of Education (MOE). \\r\\n\\r\\nThrough CCAs, students discover their interests and talents while developing values and competencies that will prepare them for a rapidly changing world. CCA also promotes friendships among students from diverse backgrounds as they learn, play and grow together. Participation in CCA fosters social integration and deepens students sense of belonging to their school, community and nation. [1]\\r\\n\\r\\nCCAs also give students in their early teens actual public responsibilities. Red Cross and St John members, for example, are often required to render first aid at public events. Most uniformed groups require precision, management and organisational skills, providing training to prepare students for the outside world.\\r\\n\\r\\nHowever, CCA records are rarely considered by potential employers.\\r\\n\\r\\nCCA choices vary widely from school to school, although schools at each education level are required to conform to national standards prescribed for that level.\\r\\n\\r\\nIn primary schools, CCAs are often introduced to students at Primary Three. Not all primary schools make CCA participation compulsory. In primary schools, Brownies are likened to junior Girl Guides.\\r\\n\\r\\nIn secondary schools, CCAs are treated more seriously. Students are required to pick at least one Core CCA to join at Secondary One. Belonging to a Core CCA is compulsory, and the students may choose a second CCA if they wish. At the end of the fourth/fifth year, 1 to 2 'O' Level points are removed from the examination aggregate (a lower aggregate indicates better marks). Although the marks are few, it is believed by many that they may make a difference when the students are considered for the most popular post-secondary educational institutions. For example, one minimum prerequisite for admission Raffles Institution at Year Five, via the 'O' Levels, is an already perfect score with the maximum of 4 points removed.\\r\\n\\r\\nCCAs are held outside of curriculum hours and the activities partaken depend on the nature of CCA. For example, Uniformed groups do foot drills and team-building exercises while competitive sportsmen spend most of the time training and learning techniques from their instructors.\\r\\n\\r\\nCCA groups typically feature an Executive Committee. In musical groups and CCAs catering to specific interests, the Executive Committee typically consists of a Chairman, Secretary and Treasurer, among other positions.\\r\\n\\r\\nMany former students return to their alma mater after graduation to help impart what they have learned to their juniors. Some do so within a formal framework, such as those in the uniformed groups (where ex-cadets are appointed as cadet officers), or the Voluntary Adult Leader scheme (for those above age 20). Others do so on a casual basis.\\r\\n\\r\\nMany CCA-related competitions are held in Singapore, creating a competitive environment which provide CCA groups an objective to work towards.\\r\\n\\r\\nThe Ministry of Education organises competitions for competitive sports at the zonal and national level, respectively the yearly Zonal and National Schools Competitions. MOE also organises the biennial Singapore Youth Festival (SYF) for the Aesthetics CCAs.\\r\\n\\r\\nNote that Band may either count as a uniformed group or a performing arts group.\\r\\n\\r\\nIn some schools, instead of separate clubs for Language, Debate and Drama (and even Culture), these domains are grouped under the heading of Language Debate and Drama Societies, an example of which is the English Literary Drama and Debate Society (ELDDS).","input":"What is a co curricular activity at the school?"},{"output":"Lagavulin 16","context":"Lagavulin distillery is an Islay single malt Scotch whisky distillery in Lagavulin on the island of Islay, Scotland.\\r\\nThe standard Lagavulin single malt is 16 years old (43%), though they regularly release a 12-year-old cask strength variety, a Distiller's edition finished in Pedro Ximnez casks, and 25- and 30-year-old varieties.\\r\\nLagavulin is produced by United Distillers & Vintners, which in turn is owned by Diageo plc. It is marketed under their Classic Malts brand.\\r\\nThe name Lagavulin is an anglicisation of Lag a' Mhuilinn, Scottish Gaelic for hollow of the mill.\\r\\n\\r\\n\\r\\nThe distillery of Lagavulin officially dates from 1816, when John Jonston and Archibald Campbell constructed two distilleries on the site. One of them became Lagavulin, taking over the otherwhich one is not exactly known. Records show illicit distillation in at least ten illegal distilleries on the site as far back as 1742, however. In the 19th century, several legal battles ensued with their neighbour Laphroaig, brought about after the distiller at Lagavulin, Sir Peter Mackie, leased the Laphroaig distillery. It is said that Mackie attempted to copy Laphroaig's style. Since the water and peat at Lagavulin's premises was different from that at Laphroaig's, the result was different. The Lagavulin distillery is located in the village of the same name.\\r\\nLagavulin is known for its producer's use of a slow distillation speed and pear shaped pot stills. The two wash stills have a capacity of 11,000 litres and the two spirit stills of 12,500 litres each.[1]\\r\\nInternational spirits ratings competitions have generally given Lagavulin's 16-year spirit extremely high scores. The San Francisco World Spirits Competition, for instance, gave the 16-year four consecutive double gold medals between 2005 and 2008 and has awarded it gold medals in the years since.[2] Wine Enthusiast Magazine put the 16-year in its 90ÿ95 point interval in 2004.[2] Spirits ratings aggregator proof66.com, which averages scores from the San Francisco Spirits Competition, Wine Enthusiast, and others, classifies the spirit in its highest (\\"Tier 1\\") performance category.\\r\\nThe single malt whisky that Frank is discussing with Jim in the supermarket in the 2002 film Bad Company (2002 film) is from the Lagavulin distillery.[3]\\r\\nIn Robert Ludlum's The Hades Factor (2000), wealthy villain Victor Tremont drinks Lagavulin's \\"fifty-year-old\\" whisky and \\"bought the entire supply each year from the distillery on Islay\\".\\r\\nLagavulin 16 is the preferred scotch of Ron Swanson, a character on NBC's Parks and Recreation. In \\"London Part II\\", the second episode of the show's sixth season, he visits the distillery itself. Nick Offerman, the actor who plays the character, is a Lagavulin drinker in real life, referring to it in an interview as \\"mother's milk\\".[4] In the show's finale, Ron purchases a 51% share in the distillery when urged to diversify his personal portfolio of assets.\\r\\nIn Stieg Larsson's The Girl Who Kicked the Hornets' Nest, Lisbeth Salander visits The Rock Hotel on Gibraltar after her acquittal. She asks for a drink of Lagavulin after studying the bottles behind the bar and, after tasting it, pushes it away and requests \\"something that could not be used to tar a boat\\".[5]\\r\\nIn \\"Dead Irish Writers\\" (season 3, episode 15) of the NBC series The West Wing, British Ambassador Lord John Marbury expresses a preference for Lagavulin in a conversation with White House Communications Director Toby Ziegler.[6]\\r\\nIn the 2015 film, Aloha, General Dixon (Alec Baldwin) and Colonel \\"Fingers\\" Lacy (Danny McBride) are seen drinking a whisky from the Lagavulin distillery.[7]\\r\\nIn the book Golden Son by Pierce Brown, Lagavulin is the favorite whisky of Lorn au Arcos. Despite the apparent destruction of the British Isles, scotch whisky is still in production.\\r\\nIn \\"Doors & Corners,\\" the second episode of the second season of The Expanse (TV series), Deputy Undersecretary of the United Nations Chrisjen Avasarala orders Admiral Souther a Lagavulin.\\r\\nIn french noir writer Jean Claude Izzo's books (\\"Total chaos\\", \\"Chourmo\\", \\"Solea\\"), the main character Fabio Montale regularly drinks Lagavulin whisky.\\r\\nIn Netflixs adaptation of Lost in Space (2018 TV series) one of the items smuggled to Alpha Centauri is Lagavulin.\\r\\nCoordinates: 553807.8N 60734.3W? / ?55.635500N 6.126194W? / 55.635500; -6.126194","input":"What whiskey does ron swanson drink on parks and recreation?"},{"output":"Deepak Raj Joshee","context":"Executive:\\r\\n\\r\\nFederal Parliament:\\r\\n\\r\\nJudiciary:\\r\\n\\r\\nThe Chief Justice of Nepal (Nepali: ?????? ?????????) is the head of the judicial branch of Nepal and the chief judge of the Supreme Court of Nepal. The Chief Justice is the highest judicial officer in the country, and acts as a chief administrative officer for all the judicial system.\\r\\n\\r\\nThe current acting Chief Justice is Deepak Raj Joshee, who took over the role on 15 March 2018.[1]\\r\\n\\r\\nThe list of Chief Justices of Nepal are as follows:[2][3]","input":"Who is the present chief justice of nepal?"},{"output":"The New Zealand Kiwis","context":"?Tonga 0ÿ74 New Zealand?\\r\\n(Auckland, New Zealand; 1999)\\r\\nThe New Zealand national rugby league team (Mori: Tؐma rؐki motu Aotearoa) has represented New Zealand in rugby league since 1907. Administered by the New Zealand Rugby League, they are commonly known as the Kiwis, after the native bird of that name. The team's colour's are majority black with white and the players perform a haka before every match they play as a challenge to their opponents. The New Zealand Kiwis are currently second in the RLIF World Rankings. Since the 1980s, most New Zealand representatives have been based overseas, in the professional National Rugby League and Super League competitions. Before that players were selected entirely from clubs in domestic New Zealand leagues.\\r\\nA New Zealand side first played in a 1907 professional rugby tour which pre-dated the birth of rugby league football in the Southern Hemisphere, making it the second oldest national side after England. Since then the Kiwis have regularly competed in international competition, touring Europe and Australia throughout the 20th century. New Zealand have competed in every Rugby League World Cup since the first in 1954, reaching three consecutive tournament finals between 2000ÿ2013. In 2008, New Zealand won the World Cup for the first time. They also contest the Baskerville Shield against England.\\r\\n\\r\\n\\r\\nRugby football was introduced into New Zealand by Charles John Monro, son of the then speaker of the New Zealand House of Representatives, Sir David Monro. He had been sent to Christ's College, East Finchley in north London, where he became an enthusiastic convert to the new code. He brought the game back to his native Nelson, and arranged the first rugby match between Nelson College and Nelson Football Club, played on 14 May 1870.\\r\\nWhen New Zealand's national rugby team (the All Blacks) toured Britain in 1905 they witnessed the growing popularity of the breakaway non-amateur Northern Union's games. On his return in 1906, All Black George William Smith met the Australian entrepreneur J J Giltinan to discuss the potential of professional rugby in Australasia. The first New Zealand team to play professional rugby was known as the All Blacks. To avoid confusion, the terms professional All Blacks or All Golds are used.\\r\\nIn the meantime, a lesser known New Zealand rugby player, Albert Henry Baskerville (or Baskiville) was ready to recruit a group of players for a Great Britain pro tour. It is believed that Baskerville became aware of the profits to be made from such a venture while he was working at the Wellington Post Office in 1906. A colleague had a coughing fit and dropped a British newspaper. Baskerville picked it up and noticed a report about a Northern Union (NU) match that over 40,000?people had attended.\\r\\nBaskerville wrote to the NRFU asking if they would host a New Zealand touring party. The 1905 All Blacks tour was still fresh in English minds, thus the NU saw the upcoming competitive New Zealand tour as exceptional opportunity to raise the profile and finances of the NU game. The NU agreed to the tour provided that some of those original All Blacks were included in the New Zealand team. George Smith arrived back in New Zealand and after learning of Baskerville's plans, the two teamed up and began signing players.\\r\\nThe New Zealand Rugby Union became aware of the tour and promptly applied pressure to any All Black or New Zealand representative player it suspected of involvement. They had the New Zealand Government's Agent General in London deliver a statement to the British press in an effort to undermine the tour's credibility. This had little effect and by that time the professional All Blacks were already sailing across the Tasman to give Australia its first taste of professional rugby.\\r\\nIt was during this time that references to the professional All Blacks as the All Golds first appeared.\\r\\nClearly, \\"All Golds\\" was a play on the amateur \\"All Blacks\\" name but it was also an insult to the players, criticising the arrangement where they would each share in the wealth of the tour. The name \\"All Golds\\" is now thought to have originated in a New Zealand newspaper in May/June 1907 (see panel below), amidst editorial arguments over whether it was honourable for the proposed \\"professional All Blacks\\" team to be paid.\\r\\nThe first documented use in Australia was in a headline in the Sydney Morning Herald (7 August), just before Baskerville's team arrived. Interestingly, those same Herald articles also had a tag for those who supported the amateur rugby union calling them the \\"Lily Whites\\" (who were clean, pure, and repelled the evils of professionalism).\\r\\nThe All Golds name is now carried by the Gloucestershire All Golds a Semi-professional team who are based in Gloucestershire and compete in the RFL League 1, formerly Championship 1 and currently known as Kingstone Press League 1 for sponsorship reasons, is a professional rugby league competition based in England. They also take part in the annual Challenge Cup and League 1 Cup. The Club bears the name in honor of the 3rd test match played at the clubs home ground in Cheltenham.\\r\\nProfessional rugby in the southern hemisphere kicked off with the professional All Blacks playing a professional rebel New South Wales team organised by Smith's contact, James Giltinan. The games drew little interest to start with, but were a major success for the rugby rebels of Australia, as they finally had the money to start the first professional Rugby Football League and hence change the face of rugby in Australia.\\r\\nNew Zealand made it to Great Britain in 1907. They included Australian Dally Messenger in their party. He missed the first Test defeat, but played in the two Tests which the All Golds won. At this time professional rugby, under the banner of the Northern Union, was not played by the RFU rules which was all the All Golds knew. The All Golds took on a week of intensive training after which they started the tour. From a New Zealander's point of view, the tour may not have been successful, but to the All Gold's credit they performed well considering they only had a week to learn the rules. However, from the NU's point of view the tour would have been a godsend, because professional rugby was left in a better state than when they arrived. The tour's results were seen the following year when the NRFU clubs more than doubled their membership numbers. The tour had obviously excited the public, raised the profile of their game as well as the game's finances.\\r\\nDuring their return voyage from England, the All Golds made a stop-over in Australia where they discovered that professional rugby was being played with Northern Union rules, under the banner of the New South Wales Rugby Football League (NSWRFL). The All Golds played another 10 games in Australia, boosting the finances of the fledging NSWRFL premiership; making the All Golds tour one of the most significant contributions to the sport of rugby league in Australia.\\r\\nThe All Golds were Hercules Richard \\"Bumper\\" Wright (captain), George William Smith (vice-captain), Albert Baskirville (secretary), Herbert Turtill, Harold Rowe, Duncan McGregor, Dally Messenger, Edgar Wrigley, Joseph Lavery, Richard Wynyard, William Wynyard, Lance Todd, Edward Tyne, William Tyler, Arthur Kelly, Tom \\"Angry\\" Cross, William Massa Johnston, Eric Watkins, Conrad Byrne, Adam Lile, Daniel Gilchrist, Arthur Callam, Charles Pearce, William Trevarthen, Charles Dunning, William Mackrell, Daniel Fraser (assistant manager), Jim Gleeson (treasurer), and H.J. Palmer (financial manager).[3]\\r\\nThe 1910 Great Britain Lions tour of Australia and New Zealand saw the Kiwis' first ever test on home soil, with the British side proving too strong. In 1911 the New Zealand national team toured Australia.\\r\\n1920 represented a high point for New Zealand rugby league (and sport in that country) with two matches against the touring British Lions rugby league team drawing 40,000 fans each in Auckland's Domain.\\r\\nThe NZRU was able to control a lot of what the New Zealand Rugby League (NZRL) was able to get its hands on. In 1926, the Union took legal action, trying to stop the League from using the name, the \\"All Blacks\\" as their touring name. The NZRL felt that they had equal ownership to the name and were not ready to relinquish it. However, by this time the press had already nicknamed the team 'the Kiwis', because of the badge included on their jerseys. Despite the League trying to discourage its usage, the name has stuck ever since.\\r\\nThe 1926ÿ27 New Zealand tour of Britain involved several skirmishes within the Kiwi party.[4] Problems began on the boat journey over, with disputes developing about aspects of the trip and a rift developed between tour manager and coach, Mr Mair, an Australian and seven forwards. The disputes continued once the party arrived in Britain, with one of the rebels being involved in a street fight with another member of the tour party after the opening match. In mid-November, following further disturbances, which almost led to the tour party being evicted from their Harrogate hotel, it was decided that coach Mair would withdraw from team selection and match tactics for a period of four weeks. The tour, and the costly disputes, continued, with the rebels eventually setting sail for home a week earlier than their colleagues. Three months later all seven players were banned for life by the New Zealand authorities. New Zealand did not visit Europe again until 1939.\\r\\nIn 1938, for a tour of Australia, the New Zealand Rugby League officially adopted the name the \\"New Zealand Kiwis\\". This side was also the first to wear a white \\"V\\" on their jerseys.[5]\\r\\nIn 1947 at Bradford in England, a crowd of 42,680 saw New Zealand play, setting a new record for the team on British soil.[6]\\r\\nThe Kiwis were unbeaten in any test series from 1948 to 1951 and won six of their nine tests. They toured England and France in 1951ÿ52. New Zealand were hosted by France for the first ever World Cup match in 1954's inaugural tournament. They lost that match as well as the remaining two to finish last. In the 1957 World Cup New Zealand got their first World Cup win, with victory against France in one of their three matches. The Kiwis got exactly the same result in the 1960 World Cup too.\\r\\nDuring the 1961ÿ63 era, New Zealand won seven out of ten test matches, including a 2ÿ0 series win over Great Britain, then considered the top rugby league nation in the world. The most outstanding performance by the New Zealanders was their record-breaking 19ÿ0 win over Britain in 1962. It was only the second time a British team had been held scoreless. New Zealand in the period 1960ÿ65 won the Courtney Goodwill Trophy for most successful test-playing nation,\\r\\nThe Kiwis again got a sole win against France from their three matches of the 1970 World Cup.\\r\\nIn 1971, the Kiwis embarked on a 34 match tour of England and France. While they only won half of the matches they played against English club teams, they won the test series 2ÿ1 (winning the first test 18ÿ13, the second 17ÿ14 but losing the third 3ÿ12). They won the series against France 2ÿ0 (winning the first test 27ÿ11, the second 24ÿ2 but drawing the third 3ÿ3).\\r\\nIn the 1972 World Cup New Zealand failed to win a game. In the season-long 1975 World Series, New Zealand won 2 out of their 8 games, finishing second last. The 1977 World Cup brought the Kiwis the familiar result of a sole win against France. A World Cup rated Test took place on Sunday 7 July 1985 at Carlaw Park. Australia's 18ÿ0 defeat was the first time they had failed to score in a Test against New Zealand.\\r\\nOn Tuesday 7 July 1987, New Zealand team warmed up for internationals against Papua New Guinea and Australia by taking on the Queensland state team at Lang Park, Brisbane. They then went on to record a 22ÿ16 victory.\\r\\nAustralia crashed to a defeat on Tuesday 21 July 1987, when the Kangaroo dominance of the international game suffered a rare setback. New Zealand were their opponents in a match which had been arranged to fill the gap created by the non-appearance of France. New Zealand won the game 13ÿ6. The next four internationals between Australia and New Zealand were all staged in New Zealand, and all were won by the Kangaroos. In the final Test match of the 1988 Great Britain Lions tour, New Zealand just pipped the British 12ÿ10 in a freezing encounter in Christchurch for a place in the final against Australia. Played at Eden Park in Auckland, it was the most hyped game in the history of rugby league in New Zealand, and the crowd of 47,363 was the biggest ever for a game in Auckland. Sadly for the Kiwis, the final proved to be a huge anti-climax and they were outplayed by the Aussies.\\r\\nOn 20 June 1993 the first all-professional, and all-overseas based, Kiwis side took the field against Australia.[5] New Zealand almost got their revenge on Australia in the 1995 World Cup semi-final when, with the scores level at 20ÿ20, a last minute drop-goal attempt by skipper Matthew Ridge brushed the wrong side of the post, allowing the game to go into extra-time. From there, Australia went on to win. Due to the NZRL's alignment with Super League, a New Zealand team selected only from Australasian-based players competed in 1997's Super League Tri-series against Queensland and New South Wales.\\r\\nIn 1998 the Kiwis travelled to the UK for a three-test series against Great Britain, winning the first and second tests and drawing the third.\\r\\nThe Kiwis handed England their biggest ever loss to again make the final in the 2000 World Cup,[7] but again went down to the Kangaroos 40ÿ12. Gary Freeman coached New Zealand from 2001ÿ02. Beginning in 2002, a 'New Zealand A' team was selected from players in the domestic New Zealand competition. New Zealand A toured France and the United States in 2002, and the United Kingdom in 2003. In 2004 they hosted New South Wales Country.\\r\\nIn 2003 the Kiwis played their 100th international against Australia.[8] Brian McClennan was appointed national coach of New Zealand in June 2005. His appointment was controversial, mainly because he had no professional coaching experience.[9]\\r\\n2005 would be considered one of the Kiwis greatest years, as they captured the 2005 Tri-Nations title, effectively becoming \\"de facto\\" World champions as the three best countries competed in that competition. In the course of winning the Tri-Nations the Kiwis defeated Australia in Sydney for the first time in half a century. In London, the Kiwis posted their highest score ever against Great Britain and, in winning the final, posted the first shut out of Australia in 20?years. The 24ÿ0 result at Elland Road, Leeds equalled the Kiwis' biggest ever win against Australia ÿ a 49ÿ25 win in Brisbane almost 50?years previously. It was Australia's first defeat in a series or tournament since 1978.[10] In New Zealand, Brian McClennan earned praise from the press and signed an extension to his coaching tenure with the Kiwis. In 2006 the Kiwis lost both mid-season tests to the Kangaroos and Lions. The 2006 Tri-nations brought controversy when New Zealand fielded an ineligible player, Nathan Fien, against Great Britain and were later stripped of the two points earned from their 18ÿ14 win.[11]\\r\\nIn July 2007, the Leeds Rhinos announced that Brian McClennan would be joining the club as Head Coach on a two-year contract from 1 December 2007. McClennan subsequently resigned as national coach and his position was taken up by Gary Kemble in August 2007.\\r\\nUnder Gary Kemble the Kiwis went on to lose the first of their three test series against Great Britain going down 14ÿ20 against the Lions at Huddersfield on 27 October 2007.[12] Following the loss an Australian newspaper reported that former Kiwi captain Hugh McGahan was concerned at Kemble's poor start and suggested that former Australian and current Brisbane Broncos coach Wayne Bennett should be pursued for the role.[13] McGahan later claimed that his comments had been grossly misreported by the journalist.\\r\\nIn the second test of the series on 3 November 2007, Kemble returned to KC Stadium, the ground on which he had spent a large portion of his playing days with Hull F.C. It was to be a disastrous homecoming, however, as the Kiwis suffered their second humiliating defeat under Kemble when beaten 44ÿ0 by an inspired Great Britain.[14] The win gave Great Britain an unassailable series lead leaving the Kiwis with only pride to play for in the final test at JJB Stadium in Wigan.\\r\\nIntense criticism followed the second test loss, some of it directed at the players, some of it toward the management of the NZRL. However, Kemble also copped severe criticism from some quarters with one commentator suggesting that Kemble \\"must be sacked at the series-end\\" and describing him as a \\"captain of calamity\\".[15] Following the loss, Kiwi's captain Roy Asotasi hinted at the possibility of internal issues for the players in adjusting to Kemble's coaching style when he compared Kemble's approach with that of his predecessor McClennan describing them as \\"very different\\" and reporting that the group was \\"still trying to gel\\".[16] Following the loss Kemble acknowledged that he was contemplating resigning from his post if the Kiwis were whitewashed 3ÿ0 by Great Britain.[17]\\r\\nDespite a far more spirited performance in the third test the Kiwis were unable to prevent the whitewash losing 22ÿ28 after leading 12ÿ0 early in the match. Kemble was forthright in expressing his desire to remain Kiwi coach following the loss with the general feeling being that the loss was \\"close enough\\" for him to retain the position.[18] Kemble was the subject of some ridicule for post-match comments which suggested that the Kiwis \\"almost won the test series\\" despite an aggregate score of 92ÿ36 across the three tests.[19]\\r\\nIn December 2007 the NZRL held its annual awards dinner. Being one hundred years since the inception of rugby league in New Zealand, a 13-man New Zealand team of the century was named on the evening, with Cliff Johnson named at captain. Mark Graham was named in the second-row and was also honoured as the greatest Kiwi player of the century.\\r\\nKemble resigned as head coach after captain Roy Asotasi and David Kidwell both publicly said that they had no confidence in his coaching abilities. Subsequently, former Kiwi, Stephen Kearney was appointed coach with Wayne Bennett given a role as an advisor.\\r\\nThe All Golds were revived again in 2008 as a testimonial match for Ruben Wiki, where they defeated the New Zealand Maori team 44ÿ10 at Yarrow Stadium in New Plymouth.\\r\\nStephen Kearney's first match as New Zealand coach was against Australia in the 2008 Centenary Test. The match celebrated 100 years since the first Trans-Tasman clash began. Later on in that year, the 2008 World Cup kicked off. Wayne Bennett assisted Kearney in the competition. Kearney's Kiwis advanced from the group stage in second position which saw them face England for a second time in the tournament, this time in the semi-final. In the group stage, the Kiwis were at one point trailing 24ÿ8 before scoring 26 consecutive points and keep the Poms scoreless in the second half to go on to win 36ÿ24. The Kiwis didn't have to recover a deficit in the semi-final as they went on to another World Cup final meeting with Australia. In a see-saw like battle, the Kiwis created history after full-time when they won the final 34ÿ20. This was New Zealand's first ever World Cup title, as well as the Australian team's first loss since Great Britain defeated them in November 2006 and their first loss to New Zealand since November 2005. It was also their first loss in a World Cup match since 1995 and in a final since 1972.[20]\\r\\nAt the end of the 2010 domestic season, New Zealand played in the 2010 Four Nations. During the Round-robin fixtures, New Zealand produced their biggest win on home soil with a 64-point win over Papua New Guinea.[21] The Kiwis qualified for the final where they played Australia at the same venue as the 2008 World Cup final. The Kiwis won at Lang Park once again after a try in the final seconds of the game sealed the low scoring game, and 2010 Four Nations title, for the Kiwis, winning 16ÿ12.[22]\\r\\nIn 2013 New Zealand headed into the World Cup as defending champions for the first time. The Kiwis advanced out of their group with ease despite suffering a scare against Samoa in their opening game. Kearney's men then advanced to the semi-finals, courtesy of a win over Scotland, where they then met hosts England at Wembley. It was a see-saw affair which saw the Kiwis trailing 18ÿ14 with one minute on the clock remaining until star play-maker Shaun Johnson produced a magical moment to level the scores and then convert his try after the siren to win the match and make the Kiwis advance to a third consecutive World Cup Final. This was the first time they had beaten England or Great Britain in England since 2005.[23] The win also made the Kiwis equal their best winning streak record of five games. In the final, watched by a record international attendance of 74,468, they were outclassed at 'The Theatre of Dreams' as Australia earned revenge for their 2008 final defeat. After the World Cup Kearney was put under pressure after it was found out players were mixing sleeping pills and energy drinks during the tournament which affected performances. He had to reapply for his job and was only given a two-year contract, unlike the usual one that would end after the next World Cup, which the NZRL said was because he needed to \\"re-establish the culture at the Kiwis which was damaged so badly at the World Cup\\".[24] Kearney immediately took action, naming six debutantes for the first test-match of 2014.\\r\\nAt the end of the 2014 club season New Zealand played in the 2014 Four Nations. In the first game they continued their good record against Australia at the 'Cauldron' with another win setting the record at 3 wins in the past 4 meetings against the Kangaroos at the venue. In the second game the Kiwis faced another scare against Samoa when they were on the verge of a 12ÿ10 defeat with just minutes remaining before Kearney's men scored near full-time to avoid the shock result. After defeating England in their final game they qualified for the final held in Wellington where they'd face Australia again. New Zealand won the match 22ÿ18 and therefore the Four Nations, their second tournament title. This was their first win over Australia on home soil since 2003 where they beat the Aussies at North Harbour Stadium. In the final, winger Manu Vatuvei scored two tries to become New Zealand's top try-scorer after tallying his 20th try during the match to beat a record that's been held by Nigel Vagana since 2006. This was the first time New Zealand went through an entire tournament unbeaten.[25]\\r\\nIn March 2015, Kearney's successful management saw the NZRL announce an extension to his contract until the end of the 2017 World Cup.[24] The 2015 Anzac Test was held in Suncorp Stadium. The Kiwis continued their great record in Brisbane with a commanding 26ÿ12 victory giving them their first Anzac test win since 1998. It was also the first time since 1953 that New Zealand beat Australia in three consecutive test matches.[26] This win also meant New Zealand equaled their best winning streak record once again just like they did after the 2013 World Cup semi-final. On 15 May 2015, the new RLIF World Rankings were announced and it read that, for just the second time, New Zealand were officially the best team in the World. New Zealand were last ranked World number one back in 2008 after their World Cup success.[27] At the end of the year New Zealand were unable to beat their winning record after losing the Baskerville Series to England. At the conclusion of that series, it was revealed that Kearney had the best record out of any New Zealand coach to date as he was involved in the most matches (41), won the most matches (23), and won the most matches against Australia (5). He also revealed that despite the defeat to England he wanted the Kiwis to 'dominate the next decade of international rugby league'.[28]\\r\\nBefore the 2016 Anzac Test, controversy had occurred when regular New Zealand players Manu Vatuvei and Ben Matulino were caught partying and mixing energy drinks with prescription drugs. Kearney toughened his stance and announced he'd not be selecting them for the test-match in a decision which saw commends from fellow New Zealand internationals.[29] These players were just two of ten regular name players missing from the New Zealand side for the fixture, with others missing through injury or private issues. New Zealand lost the match 16ÿ0 and in the process ended their winning streak against Australia. This was also the first time New Zealand scored zero points in a test match since 2007.\\r\\nIn September 2016, Kearney left New Zealand after accepting an offer to coach New Zealand's only NRL team.\\r\\nDavid Kidwell's first match as New Zealand coach was against Australia in Perth. This was the first time an international rugby league game was held in Western Australia. In the 2016 Four Nations, New Zealand beat England for the first time on English soil since the tournament was introduced in 2009. In New Zealand's last round-robin fixture, they drew with Scotland and, in the process, gave the 'fourth nation' their first ever point in the tournament's history. Despite the shock result, New Zealand still managed to qualify for a rematch with Trans-Tasman rivals, Australia, in the final. After losing by just 6 points in the round-robin stage meeting, the Kiwis were thrashed by the Kangaroos in the final 34ÿ8.\\r\\nAfter the final Anzac Test fixture was played, Kidwell spoke on behalf on the NZRL to announce that captain Jesse Bromwich, and Kevin Proctor were suspended from playing in the World Cup after they were caught buying cocaine from a stranger after the test-match.\\r\\nControversy occurred before the 2017 World Cup, after star forward, Jason Taumalolo, decided to switch allegiances after choosing to play for Tonga at the World Cup rather than his country of birth. David Fusitu'a, Sio Siua Taukeiaho and Manu Ma'u, who were all in contention for World Cup selection, also followed Taumololo's lead. Australian forward Andrew Fifita did likewise, after initially being selected in the Australian team, making Tonga one of the tournament favourites.[30] It was at first believed, Taumalolo switched to play for Tonga in protest for the NZRL's decision to suspend Bromwich and Proctor from playing in the World Cup.[31] However, it was later confirmed he just wanted to play for his family, and grow the game in the Pacific Island nation. Former New Zealand coach, Graham Lowe, and former New Zealand captain, Benji Marshall, hit out at Taumalolo for his decision.[32][33] New Zealand began the World Cup with convincing victories over both Samoa, and Scotland. In their final pool match, they took on the 'most-talked about team of the tournament', Tonga. After leading 16ÿ2 at halftime, New Zealand created unwanted history, becoming the first tier-1 nation to lose to a tier-2 nation in a World Cup match.[34] New Zealand then took on Fiji in the quarter-final. Kidwell's side went on to create back-to-back defeats to tier-2 nations and therefore be eliminated in the quarterfinals. This was the lowest ever finish the Kiwis created in a World Cup tournament.[35] After the match, critics and media were hitting out at Kidwell and his players with the way they handled the defeat.[36] After the Kiwis' most embarrassing World Cup performance in history, pressure was put on Kidwell to leave.[37]\\r\\nThe haka is a very respected tradition to the Kiwis, which is why they turn down offers to do it for sponsorship/promotional purposes.[38] The initiation haka is used as a form of team-bonding for the Kiwis, although this haka has a unique twist. All new players and staff are required to perform an initiation haka, even journalists have been forced into it.[39] This usually happens during an open training session. Staff and players gather into a circle, with new players and staff standing inside the circle. They then strip down to their underwear and start performing a haka. As the haka starts, the team members forming the circle start to lower themselves, revealing the performance to media and fans who have shown up.[40][41][42][43][44]\\r\\nNew Zealand have been playing international matches since 1907.\\r\\nA red box around the year indicates tournaments played within New Zealand\\r\\nThe shirt in the years suffered several variations, such as the checkered chevron of 1994[45], the half black and white shirt with a silver fern in 1995[46], the chevron forming the inscription \\"KIWIS\\" in 1997[47] and the curved and broad white chevron that runs to the sleeves in the early 2000s[48] or the gold and white double chevron in 2014.\\r\\nOn the back of the shorts, Revera is on the bottom left and the bottom right.\\r\\nThe New Zealand national team squad for the 2017 Rugby League World Cup.[49]\\r\\nCaptains Category:New Zealand national rugby league team captains\\r\\nTeam of the Century (1907ÿ2006)\\r\\nThe current coach of New Zealand is former international, David Kidwell.\\r\\nMost Test appearances\\r\\nMost Test Tries\\r\\nMost Test Goals\\r\\nMost Test Points\\r\\nMost points (all matches)\\r\\nMost tries in a match\\r\\nMost points in a match\\r\\nMost games as Captain\\r\\nNew Zealand are also represented by a second-string representative team called \\"New Zealand A\\".\\r\\nBetween 2002 and 2003 the New Zealand Residents were referred to as New Zealand 'A'","input":"What is the nz rugby league team called?"},{"output":"Rusty Paul","context":"Rusty Paul is the American mayor of Sandy Springs, Georgia, United States.  Previously a Sandy Springs city councilman, Paul was elected to succeed Eva Galambos in November 2013.[1] He was sworn in as the city's second Mayor on January 7, 2014.\\r\\n\\r\\nPrior to his service on the Sandy Springs city council, Paul was a State Senator representing northern Fulton County for several years.  Paul was also Chairman of the Georgia Republican Party in the 1990s.  Before this, Paul served on the Stone Mountain city council.  Paul has also served as Assistant Secretary for Congressional and Intergovernmental Relations for the Housing and Urban Development Department under Secretary Jack Kemp[2][3]\\r\\n\\r\\nHe grew up in the Birmingham, Alabama area.[4]","input":"Who is the mayor of sandy springs ga?"},{"output":"in the early 17th century","context":"Saint Patrick's Day, or the Feast of Saint Patrick (Irish: L Fhile Pdraig, \\"the Day of the Festival of Patrick\\"), is a cultural and religious celebration held on 17 March, the traditional death date of Saint Patrick (c.?AD?385ÿ461), the foremost patron saint of Ireland.\\r\\nSaint Patrick's Day was made an official Christian feast day in the early 17th century and is observed by the Catholic Church, the Anglican Communion (especially the Church of Ireland),[4] the Eastern Orthodox Church, and the Lutheran Church. The day commemorates Saint Patrick and the arrival of Christianity in Ireland,[3] and celebrates the heritage and culture of the Irish in general.[5] Celebrations generally involve public parades and festivals, cilidhs, and the wearing of green attire or shamrocks.[6] Christians who belong to liturgical denominations also attend church services[5][7] and historically the Lenten restrictions on eating and drinking alcohol were lifted for the day, which has encouraged and propagated the holiday's tradition of alcohol consumption.[5][6][8][9]\\r\\nSaint Patrick's Day is a public holiday in the Republic of Ireland,[10] Northern Ireland,[11] the Canadian province of Newfoundland and Labrador (for provincial government employees), and the British Overseas Territory of Montserrat. It is also widely celebrated by the Irish diaspora around the world, especially in Great Britain, Canada, the United States, Brazil, Argentina, Australia, and New Zealand. Saint Patrick's Day is celebrated in more countries than any other national festival.[12] Modern celebrations have been greatly influenced by those of the Irish diaspora, particularly those that developed in North America. In recent years, there has been criticism of Saint Patrick's Day celebrations for having become too commercialised and for fostering negative stereotypes of the Irish.\\r\\n\\r\\n\\r\\nPatrick was a 5th-century Romano-British Christian missionary and bishop in Ireland. Much of what is known about Saint Patrick comes from the Declaration, which was allegedly written by Patrick himself. It is believed that he was born in Roman Britain in the fourth century, into a wealthy Romano-British family. His father was a deacon and his grandfather was a priest in the Christian church. According to the Declaration, at the age of sixteen, he was kidnapped by Irish raiders and taken as a slave to Gaelic Ireland.[13] It says that he spent six years there working as a shepherd and that during this time he \\"found God\\". The Declaration says that God told Patrick to flee to the coast, where a ship would be waiting to take him home. After making his way home, Patrick went on to become a priest.\\r\\nAccording to tradition, Patrick returned to Ireland to convert the pagan Irish to Christianity. The Declaration says that he spent many years evangelising in the northern half of Ireland and converted \\"thousands\\". Patrick's efforts against the druids were eventually turned into an allegory in which he drove \\"snakes\\" out of Ireland (Ireland never had any snakes).\\r\\nTradition holds that he died on 17 March and was buried at Downpatrick. Over the following centuries, many legends grew up around Patrick and he became Ireland's foremost saint.\\r\\nToday's St Patrick's Day celebrations have been greatly influenced by those that developed among the Irish diaspora, especially in North America. Until the late 20th century, St Patrick's Day was often a bigger celebration among the diaspora than it was in Ireland.[12]\\r\\nCelebrations generally involve public parades and festivals, Irish traditional music sessions (cilithe), and the wearing of green attire or shamrocks.[6] There are also formal gatherings such as banquets and dances, although these were more common in the past. St Patrick's Day parades began in North America in the 18th century but did not spread to Ireland until the 20th century.[14] The participants generally include marching bands, the military, fire brigades, cultural organisations, charitable organisations, voluntary associations, youth groups, fraternities, and so on. However, over time, many of the parades have become more akin to a carnival. More effort is made to use the Irish language, especially in Ireland, where the week of St Patrick's Day is \\"Irish language week\\". Recently, famous landmarks have been lit up in green on St Patrick's Day.\\r\\nChristians may also attend church services,[5][7] and the Lenten restrictions on eating and drinking alcohol are lifted for the day. Perhaps because of this, drinking alcohol ÿ particularly Irish whiskey, beer, or cider ÿ has become an integral part of the celebrations.[5][6][8][9] The St Patrick's Day custom of \\"drowning the shamrock\\" or \\"wetting the shamrock\\" was historically popular, especially in Ireland. At the end of the celebrations, a shamrock is put into the bottom of a cup, which is then filled with whiskey, beer, or cider. It is then drunk as a toast to St Patrick, Ireland, or those present. The shamrock would either be swallowed with the drink or taken out and tossed over the shoulder for good luck.[15][16][17]\\r\\nOn St Patrick's Day, it is customary to wear shamrocks, green clothing or green accessories (the \\"wearing of the green\\"), the colour associated with Catholics in Ireland (orange is the colour associated with Protestant Christians). St Patrick is said to have used the shamrock, a three-leaved plant, to explain the Holy Trinity to the pagan Irish.[18][19] This story first appears in writing in 1726, though it may be older. In pagan Ireland, three was a significant number and the Irish had many triple deities, a fact that may have aided St Patrick in his evangelisation efforts.[20][21] Patricia Monaghan says there is no evidence that the shamrock was sacred to the pagan Irish.[20] However, Jack Santino speculates that it may have represented the regenerative powers of nature, and was recast in a Christian context??icons of St Patrick often depict the saint \\"with a cross in one hand and a sprig of shamrocks in the other\\".[22] Roger Homan writes, \\"We can perhaps see St Patrick drawing upon the visual concept of the triskele when he uses the shamrock to explain the Trinity\\".[23]\\r\\nThe colour green has been associated with Ireland since at least the 1640s, when the green harp flag was used by the Irish Catholic Confederation. Green ribbons and shamrocks have been worn on St Patrick's Day since at least the 1680s.[24] The Friendly Brothers of St Patrick, an Irish fraternity founded in about 1750,[25] adopted green as its colour.[26] However, when the Order of St. Patrickan Anglo-Irish chivalric orderwas founded in 1783 it adopted blue as its colour, which led to blue being associated with St Patrick. During the 1790s, green would become associated with Irish nationalism, due to its use by the United Irishmen. This was a republican organisationled mostly by Protestants but with many Catholic memberswho launched a rebellion in 1798 against British rule. The phrase \\"wearing of the green\\" comes from a song of the same name, which laments United Irishmen supporters being persecuted for wearing green. Throughout the 19th and 20th centuries, the colour green and its association with St Patrick's Day grew.[27]\\r\\nThe wearing of the 'St Patrick's Day Cross' was also a popular custom in Ireland until the early 20th century. These were a Celtic Christian cross made of paper that was \\"covered with silk or ribbon of different colours, and a bunch or rosette of green silk in the centre\\".[28]\\r\\nSaint Patrick's feast day, as a kind of national day, was already being celebrated by the Irish in Europe in the ninth and tenth centuries. In later times, he became more and more widely seen as the patron of Ireland.[29] Saint Patrick's feast day was finally placed on the universal liturgical calendar in the Catholic Church due to the influence of Waterford-born Franciscan scholar Luke Wadding[30] in the early 1600s. Saint Patrick's Day thus became a holy day of obligation for Roman Catholics in Ireland. It is also a feast day in the Church of Ireland, which is part of the worldwide Anglican Communion. The church calendar avoids the observance of saints' feasts during certain solemnities, moving the saint's day to a time outside those periods. St Patrick's Day is occasionally affected by this requirement, when 17 March falls during Holy Week. This happened in 1940, when Saint Patrick's Day was observed on 3 April to avoid it coinciding with Palm Sunday, and again in 2008, where it was officially observed on 15 March.[31] St Patrick's Day will not fall within Holy Week again until 2160.[32][33] However, the popular festivities may still be held on 17 March or on a weekend near to the feast day.[34]\\r\\nIn 1903, St Patrick's Day became an official public holiday in Ireland. This was thanks to the Bank Holiday (Ireland) Act 1903, an act of the United Kingdom Parliament introduced by Irish Member of Parliament James O'Mara.[35] O'Mara later introduced the law which required that public houses be shut on 17 March after drinking got out of hand, a provision that was repealed in the 1970s.\\r\\nThe first St Patrick's Day parade in Ireland was held in Waterford in 1903. The week of St Patrick's Day 1903 had been declared Irish Language Week by the Gaelic League and in Waterford they opted to have a procession on Sunday 15 March. The procession comprised the Mayor and members of Waterford Corporation, the Trades Hall, the various trade unions and bands who included the 'Barrack St Band' and the 'Thomas Francis Meagher Band'.[36] The parade began at the premises of the Gaelic League in George's St and finished in the Peoples Park, where the public were addressed by the Mayor and other dignitaries.[37][38] On Tuesday 17 March, most Waterford businessesincluding public houseswere closed and marching bands paraded like they had two days previously.[39] The Waterford Trades Hall had been emphatic that the National Holiday be observed.[37]\\r\\nOn St Patrick's Day 1916, the Irish Volunteers ÿ an Irish nationalist paramilitary organisation ÿ held parades throughout Ireland. The authorities recorded 38 St Patrick's Day parades, involving 6,000 marchers, almost half of whom were said to be armed.[40] The following month, the Irish Volunteers launched the Easter Rising against British rule. This marked the beginning of the Irish revolutionary period and led to the Irish War of Independence and Civil War. During this time, St Patrick's Day celebrations in Ireland were muted, although the day was sometimes chosen to hold large political rallies.[41] The celebrations remained low-key after the creation of the Irish Free State; the only state-organized observance was a military procession and trooping of the colours, and an Irish-language mass attended by government ministers.[42] In 1927, the Irish Free State government banned the selling of alcohol on St Patrick's Day, although it remained legal in Northern Ireland. The ban was not repealed until 1961.[43]\\r\\nThe first official, state-sponsored St Patrick's Day parade in Dublin took place in 1931.[44]\\r\\nIn Northern Ireland, the celebration of St Patrick's Day was affected by sectarian divisions.[45] A majority of the population were Protestant Ulster unionists who saw themselves as British, while a substantial minority were Catholic Irish nationalists who saw themselves as Irish. Although it was a public holiday, Northern Ireland's unionist government did not officially observe St Patrick's Day.[45] During the conflict known as the Troubles (late 1960sÿlate 1990s), public St Patrick's Day celebrations were rare and tended to be associated with the Catholic community.[45] In 1976, loyalists detonated a car bomb outside a pub crowded with Catholics celebrating St Patrick's Day in Dungannon; four civilians were killed and many injured. However, some Protestant unionists attempted to 're-claim' the festival, and in 1985 the Orange Order held its own St Patrick's Day parade.[45] Since the end of the conflict in 1998 there have been cross-community St Patrick's Day parades in towns throughout Northern Ireland, which have attracted thousands of spectators.[45]\\r\\nIn the mid-1990s the government of the Republic of Ireland began a campaign to use St Patrick's Day to showcase Ireland and its culture.[46] The government set up a group called St Patrick's Festival, with the aims:\\r\\nThe first St Patrick's Festival was held on 17 March 1996. In 1997, it became a three-day event, and by 2000 it was a four-day event. By 2006, the festival was five days long; more than 675,000 people attended the 2009 parade. Overall 2009's five-day festival saw almost 1?million visitors, who took part in festivities that included concerts, outdoor theatre performances, and fireworks.[48] Skyfest forms the centrepiece of the festival.\\r\\nThe topic of the 2004 St Patrick's Symposium was \\"Talking Irish\\", during which the nature of Irish identity, economic success, and the future were discussed. Since 1996, there has been a greater emphasis on celebrating and projecting a fluid and inclusive notion of \\"Irishness\\" rather than an identity based around traditional religious or ethnic allegiance. The week around St Patrick's Day usually involves Irish language speakers using more Irish during Seachtain na Gaeilge (\\"Irish Language Week\\").[citation needed]\\r\\nChristian leaders in Ireland have expressed concern about the secularisation of St Patrick's Day. In The Word magazine's March 2007 issue, Fr Vincent Twomey wrote, \\"It is time to reclaim St Patrick's Day as a church festival\\". He questioned the need for \\"mindless alcohol-fuelled revelry\\" and concluded that \\"it is time to bring the piety and the fun together\\".[49]\\r\\nAs well as Dublin, many other cities, towns, and villages in Ireland hold their own parades and festivals, including Cork, Belfast, Derry, Galway, Kilkenny, Limerick, and Waterford.\\r\\nThe biggest celebrations outside the cities are in Downpatrick, County Down, where Saint Patrick is said to be buried. The shortest St. Patrick's Day parade in the world formerly took place in Dripsey, County Cork. The parade lasted just 23.4 metres and traveled between the village's two pubs. The annual event began in 1999, but ceased after five years when one of the two pubs closed.[50]\\r\\nIn Great Britain, Queen Elizabeth The Queen Mother used to present bowls of shamrock flown over from Ireland to members of the Irish Guards, a regiment in the British Army consisting mostly of soldiers from Ireland. The Irish Guards still wear shamrock on this day, flown in from Ireland.[51]\\r\\nChristian denominations in Great Britain observing his feast day include The Church of England and the Roman Catholic Church.[52]\\r\\nHorse racing at the Cheltenham Festival attracts large numbers of Irish people, both residents of Britain and many who travel from Ireland, and usually coincides with St Patrick's Day.[53]\\r\\nBirmingham holds the largest St Patrick's Day parade in Britain with a city centre parade[54] over a two-mile (3?km) route through the city centre. The organisers describe it as the third biggest parade in the world after Dublin and New York.[55]\\r\\nLondon, since 2002, has had an annual St Patrick's Day parade which takes place on weekends around the 17th, usually in Trafalgar Square. In 2008 the water in the Trafalgar Square fountains was dyed green.\\r\\nLiverpool has the highest proportion of residents with Irish ancestry of any English city.[56] This has led to a long-standing celebration on St Patrick's Day in terms of music, cultural events and the parade.\\r\\nManchester hosts a two-week Irish festival in the weeks prior to St Patrick's Day. The festival includes an Irish Market based at the city's town hall which flies the Irish tricolour opposite the Union Flag, a large parade as well as a large number of cultural and learning events throughout the two-week period.[57]\\r\\nThe first St Patrick's Day parade took place in Russia in 1992.[58] Since 1999, there has been a yearly \\"Saint Patrick's Day\\" festival in Moscow and other Russian cities.[59] The official part of the Moscow parade is a military-style parade and is held in collaboration with the Moscow government and the Irish embassy in Moscow. The unofficial parade is held by volunteers and resembles a carnival. In 2014, Moscow Irish Week was celebrated from 12 to 23 March, which includes St Patrick's Day on 17 March. Over 70 events celebrating Irish culture in Moscow, St Petersburg, Yekaterinburg, Voronezh, and Volgograd were sponsored by the Irish Embassy, the Moscow City Government, and other organisations.[60]\\r\\nIn 2017, the Russian Orthodox Church added the feast day of Saint Patrick to its liturgical calendar, to be celebrated on 30 March?[O.S. 17 March].[61]\\r\\nThe Scottish town of Coatbridge, where the majority of the town's population are of Irish descent,[62][63] also has a Saint Patrick's Day Festival which includes celebrations and parades in the town centre.[63][64]\\r\\nGlasgow has a considerably large Irish population; due, for the most part, to the Irish immigration during the 19th century. This immigration was the main cause in raising the population of Glasgow by over 100,000 people.[65] Due to this large Irish population, there are many Irish-themed pubs and Irish interest groups who hold yearly celebrations on St Patrick's day in Glasgow. Glasgow has held a yearly St Patrick's Day parade and festival since 2007.[66]\\r\\nWhile Saint Patrick's Day in Switzerland is commonly celebrated on 17 March with festivities similar to those in neighbouring central European countries, it is not unusual for Swiss students to organise celebrations in their own living spaces on St Patrick's Eve. Most popular are usually those in Zurich's Kreis 4. Traditionally, guests also contribute with beverages and dress in green.[67]\\r\\nSt Patrick's Parades are now held in many locations across Japan.[68] The first parade, in Tokyo, was organised by The Irish Network Japan (INJ) in 1992.\\r\\nThe Irish Association of Korea has celebrated Saint Patrick's Day since 1976 in Seoul, the capital city of South Korea. The place of the parade and festival has been moved from Itaewon and Daehangno to Cheonggyecheon.[69]\\r\\nIn Malaysia, the St Patrick's Society of Selangor, founded in 1925, organises a yearly St Patrick's Ball, described as the biggest St Patrick's Day celebration in Asia. Guinness Anchor Berhad also organises 36 parties across the country in places like the Klang Valley, Penang, Johor Bahru, Malacca, Ipoh, Kuantan, Kota Kinabalu, Miri and Kuching.\\r\\nThe tiny island of Montserrat is known as the \\"Emerald Island of the Caribbean\\" because of its founding by Irish refugees from Saint Kitts and Nevis. Montserrat is one of three places where St Patrick's Day is a public holiday, along with Ireland and the Canadian province of Newfoundland & Labrador. The holiday in Montserrat also commemorates a failed slave uprising that occurred on 17 March 1768.[70]\\r\\nAstronauts on board the International Space Station have celebrated the festival in different ways. Irish-American Catherine Coleman played a hundred-year-old flute belonging to Matt Molloy and a tin whistle belonging to Paddy Moloney, both members of the Irish music group The Chieftains, while floating weightless in the space station on Saint Patrick's Day in 2011.[71][72][73] Her performance was later included in a track called \\"The Chieftains in Orbit\\" on the group's album, Voice of Ages.[74]\\r\\nChris Hadfield took photographs of Ireland from earth orbit, and a picture of himself wearing green clothing in the space station, and posted them online on Saint Patrick's Day in 2013. He also posted online a recording of himself singing \\"Danny Boy\\" in space.[75][76]\\r\\nOne of the longest-running and largest St Patrick's Day parades in North America occurs each year in Montreal,[77] whose city flag includes a shamrock in its lower-right quadrant. The yearly celebration has been organised by the United Irish Societies of Montreal since 1929. The parade has been held yearly without interruption since 1824. St Patrick's Day itself, however, has been celebrated in Montreal since as far back as 1759 by Irish soldiers in the Montreal Garrison following the British conquest of New France.\\r\\nIn Manitoba, the Irish Association of Manitoba runs a yearly three-day festival of music and culture based around St Patrick's Day.[78]\\r\\nIn 2004, the CelticFest Vancouver Society organised its first yearly festival in downtown Vancouver to celebrate the Celtic Nations and their cultures. This event, which includes a parade, occurs each year during the weekend nearest St Patrick's Day.[79]\\r\\nIn Quebec City, there was a parade from 1837 to 1926. The Quebec City St-Patrick Parade returned in 2010 after more than 84 years. For the occasion, a portion of the New York Police Department Pipes and Drums were present as special guests.\\r\\nThere has been a parade held in Toronto since at least 1863.[80] The Toronto Maple Leafs hockey team was known as the Toronto St. Patricks from 1919 to 1927, and wore green jerseys. In 1999, when the Maple Leafs played on St Patrick's Day, they wore green St Patrick's retro uniforms. There is a large parade in the city's downtown on the Sunday before 17 March which attracts over 100,000 spectators.[citation needed]\\r\\nSome groups, notably Guinness, have lobbied to make Saint Patrick's Day a national holiday.[81]\\r\\nIn March 2009, the Calgary Tower changed its top exterior lights to new green CFL bulbs just in time for St Patrick's Day. Part of an environmental non-profit organisation's campaign (Project Porchlight), the green represented environmental concerns. Approximately 210 lights were changed in time for Saint Patrick's Day, and resembled a Leprechaun's hat. After a week, white CFLs took their place. The change was estimated to save the Calgary Tower some $12,000 and reduce greenhouse gas emissions by 104 tonnes.[82]\\r\\nSt Patrick's Day, while not a legal holiday in the United States, is nonetheless widely recognised and observed throughout the country as a celebration of Irish and Irish-American culture. Celebrations include prominent displays of the colour green, religious observances, numerous parades, and copious consumption of alcohol.[83] The holiday has been celebrated in North America since the late 18th century.\\r\\nIn Buenos Aires, a party is held in the downtown street of Reconquista, where there are several Irish pubs;[84][85] in 2006, there were 50,000 people in this street and the pubs nearby.[86] Neither the Catholic Church nor the Irish community, the fifth largest in the world outside Ireland,[87] take part in the organisation of the parties.\\r\\nIn recent decades, St Patrick's Day celebrations have been criticised, particularly for their association with public drunkenness and disorder. Some argue that the festivities have become too commercialised and tacky,[88][89] and have strayed from their original purpose of honouring St Patrick and Irish heritage.[88][90][91] Journalist Niall O'Dowd has criticised recent attempts to recast St Patrick's Day as a celebration of multiculturalism rather than a celebration of Irishness.[92]\\r\\nSt Patrick's Day celebrations have also been criticised for fostering demeaning stereotypes of Ireland and Irish people.[88] An example is the wearing of 'leprechaun outfits',[93] which are based on derogatory 19th century caricatures of the Irish.[94] In the run up to St Patrick's Day 2014, the Ancient Order of Hibernians successfully campaigned to stop major American retailers from selling novelty merchandise that promoted negative Irish stereotypes.[95]\\r\\nSome have described St Patrick's Day celebrations outside Ireland as displays of \\"Plastic Paddyness\\"; where foreigners appropriate and misrepresent Irish culture, claim Irish identity, and enact Irish stereotypes.[96]\\r\\nLGBT groups in the US were banned from marching in St. Patrick's Day parades in New York City and Boston, resulting in the landmark Supreme Court decision of Hurley v. Irish-American Gay, Lesbian, & Bisexual Group of Boston. In New York City, the ban was lifted in 2014,[97] but LGBT groups still find that barriers to participation exist.[98] In Boston, the ban on LGBT group participation was lifted in 2015.[99]\\r\\n(federal) = federal holidays, (state) = state holidays, (religious) = religious holidays, (week) = weeklong holidays, (month) = monthlong holidays, (36) = Title 36 Observances and Ceremonies\\r\\nBold indicates major holidays commonly celebrated in the United States, which often represent the major celebrations of the month.","input":"When did st patrick's day become a holiday?"},{"output":"public","context":"","input":"Is uc berkeley a private or public school?"},{"output":"Catherine and William Booth","context":"The Salvation Army is a Protestant Christian movement and an international charitable organization structured in a quasi-military fashion. The organisation reports a worldwide membership of over 1.5 million,[2] consisting of soldiers, officers and adherents known as Salvationists. Its founders Catherine and William Booth sought to bring salvation to the poor, destitute and hungry by meeting both their \\"physical and spiritual needs\\". It is present in 128 countries,[3] running charity shops, operating shelters for the homeless and disaster relief and humanitarian aid to developing countries.\\r\\nThe theology of the Salvation Army is derived from that of Methodism although it is distinctive in institution and practice. The Army's doctrine is typical of evangelical Protestant denomination. The Army's purposes are \\"the advancement of the Christian religion?... of education, the relief of poverty, and other charitable objects beneficial to society or the community of mankind as a whole\\".[4]\\r\\nThe Army was founded in 1865 in London by one-time Methodist circuit-preacher William Booth and his wife Catherine as the East London Christian Mission. In 1878 Booth reorganised the mission, becoming its first General and introducing the military structure which has been retained to the present day.[5] The current world leader of The Salvation Army is General Andr Cox, who was elected by the High Council of The Salvation Army on 3 August 2013.[citation needed]\\r\\n\\r\\n\\r\\nThe Salvation Army was founded in London's East End in 1865 by one-time Methodist Reform Church minister William Booth and his wife Catherine as the East London Christian Mission. The name \\"The Salvation Army\\" developed from an incident on 19 and 20 May. William Booth was dictating a letter to his secretary George Scott Railton and said, \\"We are a volunteer army.\\" Bramwell Booth heard his father and said, \\"Volunteer! I'm no volunteer, I'm a regular!\\" Railton was instructed to cross out the word \\"volunteer\\" and substitute the word \\"salvation\\".[6] The Salvation Army was modelled after the military, with its own flag (or colours) and its own hymns, often with words set to popular and folkloric tunes sung in the pubs. Booth and the other soldiers in \\"God's Army\\" would wear the Army's own uniform, for meetings and ministry work. He became the \\"General\\" and his other ministers were given appropriate ranks as \\"officers\\". Other members became \\"soldiers\\".[7]\\r\\nWhen William Booth became known as the General, Catherine is known as the \\"Mother of The Salvation Army\\". William preached to the poor, and Catherine spoke to the wealthy, gaining financial support for their work. She also acted as a religious minister, which was unusual at the time; the Foundation Deed of the Christian Mission states that women had the same rights to preach as men. William Booth described the organisation's approach: \\"The three S's best expressed the way in which the Army administered to the 'down and outs': first, soup; second, soap; and finally, salvation.\\"[8]\\r\\nIn 1880, the Salvation Army started its work in three other countries: Australia, Ireland, and the United States. It was not always an Officer of The Salvation Army who started the Salvation Army in a new country; sometimes Salvationists emigrated to countries and started operating as \\"the Salvation Army\\" on their own authority. When the first official officers arrived in Australia and the United States, they found groups of Salvationists already waiting for them and started working with each other.\\r\\nThe Salvation Army's main converts were at first alcoholics, morphine addicts, prostitutes and other \\"undesirables\\" unwelcome in polite Christian society, which helped prompt the Booths to start their own church.[9] The Booths did not include the use of sacraments (mainly baptism and Holy Communion) in the Army's form of worship, believing that many Christians had come to rely on the outward signs of spiritual grace rather than on grace itself.[10] Other beliefs are that its members should completely refrain from drinking alcohol (Holy Communion is not practiced), smoking, taking illegal drugs and gambling.[11] Its soldiers wear a uniform tailored to the country in which they work; the uniform can be white, grey, navy, fawn and are even styled like a sari in some areas. Any member of the public is welcome to attend their meetings. As the Salvation Army grew rapidly in the late 19th century, it generated opposition in England. Opponents, grouped under the name of the Skeleton Army, disrupted Salvation Army meetings and gatherings, with tactics such as throwing rocks, bones, rats, and tar as well as physical assaults on members of the Salvation Army. Much of this was led by pub owners who were losing business because of the Army's opposition to alcohol and targeting of the frequenters of saloons and public houses.[12]\\r\\nThe Salvation Army's reputation in the United States improved as a result of its disaster relief efforts following the Galveston Hurricane of 1900 and the 1906 San Francisco earthquake. The familiar use of bell ringers to solicit donations from passers-by \\"helps complete the American portrait of Christmas.\\"[according to whom?] In the U.S. alone, over 25,000 volunteers with red kettles are stationed near retail stores during the weeks preceding Christmas for fundraising.[9] The church remains a highly visible and sometimes controversial presence in many parts of the world.\\r\\nIn 1994, the Chronicle of Philanthropy, an industry publication, released the results of the largest study of charitable and non-profit organisation popularity and credibility. The study showed that The Salvation Army was ranked as the 4th \\"most popular charity/non-profit in America\\" of over 100 charities researched, with 47% of Americans over the age of 12 choosing 'Love' and 'Like A Lot' for The Salvation Army.[13]\\r\\nCharity Watch rates the Salvation Army an \\"A-\\" to an \\"A\\",[14] indicating a high level of financial efficiency and organisational transparency.\\r\\nAs of 1 September 2015 the Salvation Army operates in 127 countries and provides services in 175 different languages.[15] For administrative purposes, the Salvation Army divides itself geographically into territories, which are then sub-divided into divisions. In larger areas, regional and area commands are also introduced as sub-divisions of divisions. Each territory has an administrative hub known as territorial headquarters (THQ). Likewise, each division has a divisional headquarters (DHQ). Each of these territories is led by a territorial commander who receives orders from the Salvation Army's International Headquarters in London. A territory is normally led by an officer holding the rank of colonel (for small territories) or commissioner for larger territories. In some countries, the work of The Salvation Army may be called a command, led by a command commander. A larger command is typically led by an officer holding the rank of colonel.\\r\\nIts stated membership (as quoted from 2010 Year Book) includes 16,938 active and 9,190 retired officers, 1,122,326 soldiers, 189,176 Adherents, 39,071 Corps Cadets, 378,009 Junior Soldiers, around 104,977 other employees and more than 4.5 million volunteers. Members of the Salvation Army also include \\"adherents\\"; these are people who do not make the commitment to be a soldier but who recognise the Salvation Army as their church. (According to the 2006 Salvation Army Year Book, in the United States there are 85,148 Senior Soldiers and 28,377 Junior Soldiers, 17,396 Adherents and around 60,000 employees.)\\r\\nThe current world leader of the Salvation Army is General Andr Cox.\\r\\nThe Salvation Army is one of the world's largest providers of social aid,[citation needed] with expenditures including operating costs of $2.6 billion in 2004, helping more than 32 million people in the U.S. alone. In addition to community centres and disaster relief, the organisation does work in refugee camps, especially among displaced people in Africa. The Salvation Army has received an A? rating from the American Institute of Philanthropy. In the United Kingdom, the Salvation Army is no longer the largest non-governmental provider of social services; however, it still provides a significant service to people in need. The Salvation Army is the second largest charity in the United States, with private donations of almost $2 billion for the fiscal year ending 30 September 2007.[16]\\r\\nIn 2004, the Army in the United States received a $1.6 billion donation in the will of Joan B. Kroc, third wife of former McDonald's CEO Ray Kroc. This donation was among the larger individual philanthropic gifts ever given to a single organisation. The donation came with certain restrictions that caused some controversy.[17]\\r\\nThe International Congress of the Salvation Army is normally held every 10 years[18] as a conference for all Salvationists from around the world to meet. The first such conference took place in London, UK, from 28 May to 4 June 1886, and subsequent Congressional meetings were held sporadically until 1904 and then 1990.[19] The seventh International Congress in Atlanta, Georgia, United States, from 28 June to 2 July 2000, was the first held outside of the UK.[20] The latest International Congress was held in London on 1ÿ5 July 2015, in commemoration of the 150th Anniveresary of the Salvation Army's founding.\\r\\nOfficers are given Marching Orders to change ministries within the Salvation Army. Usually, officers are given new Marching Orders every two to five years and reassigned to different posts, sometimes moving great distances.\\r\\nA Moscow court ruled that the Salvation Army was a paramilitary organisation subject to expulsion. In October 2006, the European Court of Human Rights ruled the decision illegal.[21]\\r\\nThe beliefs of the Salvation Army rest upon these eleven doctrines:[22]\\r\\nThe denomination does not celebrate the Christian sacraments of Baptism and Holy Communion; although its officers conduct marriages, it holds a traditional Protestant belief that marriage was not instituted by Christ and therefore is not a sacrament.\\r\\nThe Salvation Army opposes euthanasia and assisted suicide. Its official stance on abortion is that \\"The Salvation Army believes in the sanctity of all human life and considers each person to be of infinite value and each life a gift from God to be cherished, nurtured and redeemed. Human life is sacred because it is made in the image of God and has an eternal destiny. (Genesis 1:27) Sacredness is not conferred, nor can it be taken away by human agreement.\\" The Salvation Army official stance admitted in 2010 exceptions in cases such as rape and incest: \\"In addition, rape and incest are brutal acts of dominance violating women physically and emotionally. This situation represents a special case for the consideration of termination as the violation may be compounded by the continuation of the pregnancy.\\"[23] It is also against the death penalty: \\"The Salvation Army recognises that the opinions of Salvationists are divided on the moral acceptability of capital punishment and its effectiveness as a deterrent. However, to advocate in any way the continuance or restoration of capital punishment in any part of the world would be inconsistent with the Army's purposes and contrary to the Army's belief that all human life is sacred and that each human being, however wretched, can become a new person in Christ.\\"\\r\\nIn 2012, the Salvation Army published a \\"Positional Statement on Homosexuality\\" after receiving adverse publicity about their position on homosexuality.[24]\\r\\nThe Bible teaches that God's intention for humankind is that society should be ordered on the basis of lifelong, legally sanctioned heterosexual unions. ... A disposition towards homosexuality is not in itself blameworthy nor is the disposition seen as rectifiable at will. ... Homosexual practice however, is, in the light of Scripture, clearly unacceptable. Such activity is chosen behaviour and is thus a matter of the will. It is therefore able to be directed or restrained in the same way heterosexual urges are controlled. Homosexual practice would render any person ineligible for full membership (soldiership) in the Army.[25]\\r\\nAs of 2016 the organisation will not appoint homosexual people to posts as ministers, preferring individuals \\"whose values are consistent with the church's philosophy\\".[26] (See also ?Criticism by LGBT activists section.)\\r\\nThe ordination of women is permitted in the Salvation Army. Salvation Army officers were previously allowed to marry only other officers (this rule varies in different countries); but this rule has been relaxed in recent years. Husbands and wives usually share the same rank and have the same or similar assignments; the major exception to this is the General's spouse, who holds the rank of commissioner.\\r\\nThe Salvation Army flag is a symbol of the Army's war against sin and social evils. The red on the flag symbolises the blood shed by Jesus Christ, the yellow for the fire of the Holy Spirit and the blue for the purity of God the Father.\\r\\nThe oldest official emblem of The Salvation Army is the crest.\\r\\nIn 1878 Captain W.H. Ebdon suggested a logo, and in 1879 it was to be found on the letterhead of the Salvation Army Headquarters. The captain's suggested design was changed only slightly and a crown was added.\\r\\nThe Army's crest proclaims Biblical truth though its symbolism:\\r\\n1 ÿ The existence of a Holy God; 2 ÿ The evil of sin are against God and man; 3 ÿ There will be punishment for sin that is fair and everlasting; 4 ÿ Jesus died on the cross for the human race; 5 ÿ Salvation is for all mankind and is free to all who accept Jesus Christ; 6 ÿ It is the responsibility of every Christian to do whatever they can do to spread the Gospel; 7 ÿ God rewards those who are faithful with eternal life in Heaven with Him.\\r\\nThe Red Shield has its origins in Salvation Army work during wartime. At the end of the 19th century, Staff-Captain Mary Murray was sent by William Booth to support British troops serving in the Boer War in South Africa. Then, in 1901, this same officer was given the task of establishing the Naval and Military League, the forerunner of the Red Shield Services.\\r\\nSalvation Army officers serving in the Red Shield Services in wartime performed many functions. The Doughnut Girls of World War I are an early example, serving refreshments to troops in the trenches. They also provided first aid stations, ambulances, chaplaincy, social clubs, Christian worship and other front-line services.[27]\\r\\nThis symbol is still used in Blue Shield Services that serve the British Armed Forces but it is widely used as a simple, more readily identifiable symbol in many Salvation Army settings. It is common to see the Red Shield used on casual Salvation Army uniform. It is now official Salvation Army policy in the UK that the red shield should be used as the external symbol of the Salvation Army, with the Crest only being used internally. Therefore, any new Salvation Army building will now have the red shield on the outside rather than the crest which certainly would have been used on its Corps (church) buildings.[28]\\r\\nIn Australia, the Red Shield has become one of the country's most identified and trusted symbols, leading the Australian Salvation Army to prefer to use this symbol over the logo on its uniform, corps buildings and advertising materials. In the 5th volume of Australian Superbrands it was recorded that \\"Research reveals that the popular Salvation Army slogan 'Thank God for the Salvos' has almost total recognition amongst the Australian public, achieving 93 per cent aided awareness\\".[29]\\r\\nSalvation Army officers and soldiers often wear uniforms. The idea that they should do so originated with Elijah Cadman who, at the Salvation Army's 'War Congress' in August 1878 said \\"I would like to wear a suit of clothes that would let everyone know I meant war to the teeth and salvation for the world\\". The uniform identifies the wearer as a Salvationist and a Christian. It also symbolises availability to those in need. The uniform takes many forms internationally but is characterised by the 'S' insignia for 'Salvation' and carries the meaning 'Saved to Serve', or 'Saved to Save'.[citation needed] Other letters are substituted to conform with local language.\\r\\nSince 1983 there has been an official Salvation Army tartan. It was designed by Captain Harry Cooper, for the Perth Citadel Corps centenary commemoration in Scotland. It is based upon the colours of the Salvation Army flag, with which it shares the same symbolism. However, it is rarely seen outside Scotland.[30]\\r\\nThe Salvation Army has a unique form of salute which involves raising the right hand above shoulder-height with the index finger pointing upwards. It signifies recognition of a fellow citizen of heaven, and a pledge to do everything possible to get others to heaven also.[31] In the case of saluting in response to applause, in circumstances such as a musical festival or being applauded for a speech, it also signifies that the Salvationist wishes to give Glory to God and not themselves.\\r\\nIn some instances, the salute is accompanied with a shout of 'hallelujah!'\\r\\nAs the popularity of the organisation grew and Salvationists worked their way through the streets of London attempting to convert individuals, they were sometimes confronted with unruly crowds. A family of musicians (the Frys, from Alderbury, Wiltshire) began working with the Army as their \\"bodyguards\\" and played music to distract the crowds.[32]\\r\\nThe tradition of having musicians available continued and eventually grew into standard brass bands. These are still seen in public at Army campaigns, as well as at other festivals, parades and at Christmas. Across the world the brass band has been an integral part of the Armys ministry and an immediately recognisable symbol to Salvationists and non-Salvationists alike. The Salvation Army also has choirs; these are known as Songster Brigades, normally comprising the traditional soprano, alto, tenor and bass singers. The premier Songster Brigade in the Salvation Army is the International Staff Songsters (ISS).\\r\\nThe standard of playing is high and the Army operates bands at the international level, such as the International Staff Band (a brass band) which is the equal of professional ensembles although it does not participate in the brass band contest scene, and territorial levels such as the New York Staff Band. Some professional brass players and contesting brass band personnel have Salvation Army backgrounds. Many Salvation Army corps have brass bands that play at Salvation Army meetings, although not all.\\r\\nThe Salvation Army also fielded large concertina bands. From the turn of the (20th) century to the Second World War between a third and a half of all SA officers in Britain played concertina. For an evangelist the concertina's portability, its ability to play both melody and chords, and most especially the fact that the player can sing or speak while playing, were all distinct advantages over brass instruments.[33][34][35]\\r\\nThe Army tradition in music is to use the popular idiom of the day to reach people for Jesus. The Army's Joy Strings were a hit pop group in the 1960s and early 1970s in the UK and beyond, reaching the charts and being featured on national television. Another popular band is The Insyderz, an American ska-core group popular in the 1990s and early 2000s. Hundreds of bands carry on this Salvation Army tradition, such as New Zealand's Moped, Chamberlain, Vatic, Agent C, and The Lads; England's Electralyte; Australia's Soteria Music Ministries, Summer Carnival Band, Crown of Thorns and Escape; and America's transMission, The Singing Company, HAB, BurN, and CJD ÿ Cookies, Juice, & Donuts. Saytunes is a website designed to encourage and promote these contemporary Salvation Army bands and artists.\\r\\nAnother significant musical feature of the Salvation Army is its use of tambourines. With coloured ribbons representing the colours of the Salvation Army flag, timbrels play an integral facet of music in the Salvation army. They are mainly played by women.\\r\\nLocal corps usually sing contemporary worship music songs in Sunday worship services, as well as traditional hymns and music accompanied by the brass band.\\r\\nToday it is becoming common in Salvation Army corps not to have a full brass band. This is reflective for many social and cultural reasons.[citation needed] Some Salvation Army corps make use of smaller ensembles of musicians. Often this ensemble consists simply of a guitar, piano or a keyboard, drums and sometimes a bass guitar and other instruments, especially during \\"Youth Fellowships\\".[citation needed]\\r\\nThe music played does tend to also take on a more contemporary style as is reflected in modern music today. The early Salvation Army bands were known for their excitement and public appeal, and the modern ensemble keeps to this ideology. Traditional hymns are still used in worship services and these are blended with other musical pieces from Christian Music Publishers such as Vineyard Music, Hillsong, and Planet Shakers to name but a few.\\r\\nIn the USA the Salvation Army's first major forays into disaster relief resulted from the tragedies of the Galveston Hurricane of 1900 and the 1906 San Francisco earthquake. The Salvationists' nationwide appeals for financial and material donations yielded tremendous support, enabling the Army to provide assistance to thousands. General Evangeline Booth, when she offered the services of Salvationists to President Woodrow Wilson during the First World War, thrust Salvation Army social and relief work to newer heights. Today the Salvation Army is best known for its charitable efforts.\\r\\nThe Salvation Army is a non-governmental relief agency and is usually among the first to arrive with help after natural or man-made disasters. They have worked to alleviate suffering and help people rebuild their lives. After the Indian Ocean tsunami in 2004, they arrived immediately at some of the worst disaster sites to help retrieve and bury the dead. Since then they have helped rebuild homes and construct new boats for people to recover their livelihood. Members were prominent among relief organisations after Hurricane Hugo and Hurricane Andrew and other such natural disasters in the United States. In August 2005, they supplied drinking water to poor people affected by the heat wave in the United States. Later in 2005 they responded to hurricanes Katrina and Rita. They have helped the victims of an earthquake in Indonesia in May 2006.\\r\\nSince Hurricane Katrina struck the Gulf Coast, the Salvation Army has allocated donations of more than $365 million to serve more than 1.7 million people in nearly every state. The Armys immediate response to Hurricane Katrina included the mobilisation of more than 178 canteen feeding units and 11 field kitchens which together have served more than 5.7 million hot meals, 8.3 million sandwiches, snacks and drinks. Its SATERN (Salvation Army Team Emergency Radio Network)[36] network of amateur ham-radio operators picked up where modern communications left off to help locate more than 25,000 survivors. Salvation Army pastoral care counsellors were on hand to comfort the emotional and spiritual needs of 277,000 individuals. As part of the overall effort, Salvation Army officers, employees and volunteers have contributed more than 900,000 hours of service.\\r\\nThe Salvation Army was one of the first relief agencies on the scene of the September 11 attacks in New York City in 2001. They also provided prayer support for families of missing people.\\r\\nThe Salvation Army, along with the American National Red Cross, Southern Baptist Convention, and other disaster relief organisations, are national members of the National Voluntary Organizations Active in Disaster (NVOAD).[37]\\r\\nAlso among the disaster relief capabilities is the Red Shield Defence Services, often called the SallyMan for short. The effort that they put in is similar to that of a chaplain, and reaches many more, offering cold drinks, hot drinks, and some biscuits for the soldiers of the military to have, though, if a SallyMan is on deployment, the locals are offered a share in the produce.\\r\\nIn Australia the Salvation Army have Emergency Services Support Units throughout the country, providing food and other welfare to members of the Emergency Services such as bushfires, floods, land search, and other both large- and small-scale emergency operations undertaken by Police, Fire, Ambulance and State Emergency Service members, and the general public affected by these events.\\r\\nThe Salvation Army is well known for its network of thrift stores or charity shops, colloquially referred to as \\"the Sally Ann\\" in Canada and \\"Salvos Stores\\" in Australia, which raise money for its rehabilitation programs by selling donated used items such as clothing, housewares and toys. Clothing collected by Salvation Army stores that are not sold on location are often sold wholesale on the global second hand clothing market.\\r\\nThe Salvation Army's fundraising shops in the United Kingdom participate in the UK government's Work Programme, a workfare programme where benefit claimants must work for no compensation for 20 to 40 hours per week over periods that can be as long as 6 months.[38][39][40][41]\\r\\nIn many countries, the Salvation Army is most recognised during the Christmas season with its volunteers and employees who stand outside of businesses and play/sing Christmas carols, or ring bells to inspire passers-by to place donations of cash and cheques inside red kettles. A tradition has developed in the United States in which, in some places, gold coins or rings or bundles of large bills are anonymously inserted into the kettles. This was first recorded in 1982, in Crystal Lake, Illinois, a suburb of Chicago.[42][43] The red kettles are not only used during the Christmas season though. They are used throughout the year at other fundraising events, such as on National Doughnut Day in the U.S. On this day, some donut shops that teamed up with the Salvation Army have a red kettle set up for donations.[44]\\r\\nThe Red Shield Appeal is an annual fundraising campaign in some territories, such as the UK and Australia. Each year, officers, soldiers, employees and volunteers take to the streets worldwide to participate in door-to-door or street collections. The money raised is specifically channelled towards The Salvation Army's social work in each respective territory. Within the territory defined by the United Kingdom and Ireland (UKIT) this collection is known as the Annual Appeal, and it often carries another name that the generally public would more readily know ÿ in 2012 becoming The Big Collection.\\r\\nThe Family Tracing Service (sometimes known as the Missing Persons Service) was established in 1885, and the service is now available in most of the countries where The Salvation Army operates. The Tracing Service's objective is to restore (or to sustain) family relationships where contact has been lost, whether recently or in the distant past. Thousands of people are traced every year on behalf of their relatives.\\r\\nThe Salvation Army includes many youth groups, which primarily consist of its Sunday schools and the Scout and Guide packs that are sometimes set up. The Scout and Guide packs are affiliated and sponsored by the Salvation Army but are open units allowing anyone to join, these units/pack observe Christian standards and encourage the young people to investigate and develop in their Christian faith. Some territories have Salvation Army Guards and Legions Association (SAGALA). In the United States these internal youth groups that are specifically for females are known as Girl Guards (older females) and Sunbeams (younger females). Adventure Corps serves boys who are enrolled in school for first through eighth grade, and is sometimes separated into Rangers (older males) and Explorers (younger males).\\r\\nIn the 21st century, the Salvation Army in the United Kingdom created a branch for the youth, called Alove,[45] the Salvation Army for a new generation. Its purpose is to free the youth of the church and their communities to express themselves and their faith in their own ways. Its mission statement is \\"Calling a generation to dynamic faith, radical lifestyle, adventurous mission and a fight for justice\\", and it emphasises worship, discipleship, missions, and social action. Alove is a member of the National Council for Voluntary Youth Services (NCVYS).[46]\\r\\nGeneral George Carpenter founded the Cross of the Order Of Distinguished Auxiliary Service in 1941 to express the Salvation Army's gratitude for service given to the organisation by non-Salvationists. The Cross has been awarded to parliamentarians, lawyers, doctors, financiers and members of the nobility.[47]\\r\\nSome in the United States have alleged that the Salvation Army discriminates against lesbian, gay, bisexual, and transgender (LGBT) individuals in its hiring practices.[57][58] The Salvation Army states that it does not \\"discriminate against hiring gays and lesbians for the majority of its roughly 55,000 jobs\\".[59] Because the Salvation Army is a church, Title VII of the U.S. Civil Rights Act of 1964 allows it to inquire into people's religious beliefs in its hiring practices. In 2001, the Salvation Army pressed the Bush Administration to exempt it and other religious groups from anti-discrimination legislation which it felt infringed on the organisation's religious freedoms. This request was denied, and was sharply rebuked by David Smith, then-spokesperson for the Human Rights Campaign. \\"Gays and lesbians are taxpayers, too,\\" said Smith. \\"Their money should not be used by religious groups to fund discriminatory practices against them.\\"[59]\\r\\nThe Salvation Army Western Territory approved a plan in October 2001 to start offering domestic partnership benefits to gay employees.[citation needed] Members of various evangelical Christian interest groups protested the decision. Focus on the Family founder James Dobson excoriated the Salvation Army for abandoning its \\"moral integrity\\" and urged his radio listeners to bombard the organisation's offices with phone calls and letters. The American Family Association also accused the Salvation Army of a \\"monstrous ... appeasement of sin\\" that resulted in a \\"betrayal of the church\\". In November 2001 the Salvation Army nationwide rescinded the Western Territory's decision with an announcement that it would only provide benefits coverage for different-sex spouses and dependent children of its employees.\\r\\nIn Australia there has been a dramatic shift from the International Salvation Army teachings; a small number of Australian Officers have identified themselves as being proudly and openly homosexual.[citation needed] These gay and lesbian officers, supported by a growing number of straight officers, soldiers and others, have been a vocal part of seeking reform of their movement from within and a change to the internal rigid structures. The Salvation Army in Australia now seeks to be inclusive and accepting of all the LGBTIQ community. The Salvation Army is being regularly presented with forums and discourses from within their own parishoners and ministers about the valid acceptance of same sex couples and the possible future marriages of same sex partners. There is a move afoot to re-examine their methodology, theology and relevance in a modern day society.\\r\\nIn 2004, the Salvation Army said that it would close operations in New York City unless it was exempted from a municipal ordinance requiring them to offer benefits to gay employees' partners. The City Council refused to make the exemption. Mayor Michael R. Bloomberg's administration chose not to enforce the ordinance. The administration's right to decline to enforce the ordinance was upheld by the New York State Court of Appeals in 2006.[60]\\r\\nOn 15 December 2012, in Canada, Andrea Le Good noticed a Salvation Army bell-ringer carrying a sign reading \\"if you support gay rights: please do not donate\\". While the bell-ringer claimed he had permission from the charity to wear the sign, Salvation Army spokeswoman Kyla Ferns said that it had no part in the sign, and that the bell-ringer was pulled away immediately when the charity learned about it.[61] The charity's website describes marriage as heterosexual by definition, and a published document called on homosexuals to embrace celibacy as a way of life. The same document also states that there is no scriptural support for the mistreatment of homosexuals.[61]\\r\\nIn February 2000, the Salvation Army in the United Kingdom publicly opposed (in their publication The War Cry and in a letter to a Scottish Parliament committee) the repeal of Section 28 of the Local Government Act 1988, which prevented local authorities from \\"intentionally promot[ing] homosexuality\\".[62] However, the organisation's UK website states that it offers \\"unconditional assistance and support regardless of race, religion, gender or sexual orientation, respecting the identity and choices of all those referred to them. ... As well as having a right to be dealt with professionally, people can expect from us encouragement and a respect for their individual beliefs, ambitions and preferences\\".[63]\\r\\nBefore the passing of the Homosexual Law Reform Act 1986 by the New Zealand Parliament, the Salvation Army was active throughout New Zealand gathering signatures for a petition seeking to prevent the bill's passing. In 2006, the Army released a statement regretting the ill feelings that persisted following this activity. It stated in part \\"We do understand though that The Salvation Army's official opposition to the Reform Bill was deeply hurtful to many, and are distressed that ill-feeling still troubles our relationship with segments of the gay community. We regret any hurt that may remain from that turbulent time and our present hope is to rebuild bridges of understanding and dialogue between our movement and the gay community.\\"[64]\\r\\nDespite documented events of Salvation Army's volunteers and their views, the organisation has issued an LGBT Statement as a response. The statement does not address any documented news events of discrimination and claims to debunk these events as urban myth.[65]\\r\\nIn November 2013 it was made known that the Salvation Army was referring LGBT individuals to one of several conversion therapy groups.[65] As a response the Salvation Army removed such referrals from their website.[66]\\r\\nA positional statement on the Salvation Army UK and Ireland site states:\\r\\nThe Salvation Army teaches that sexual acts should take place only in a monogamous heterosexual marriage, believing that this reflects God's intentions for sexual behaviour and provides the best environment for raising children.[67]\\r\\nThe positional statement is, however, intended explicitly for members of the Salvation Army[68] and the Salvation Army mission statement as of 2013 states:\\r\\nThe Salvation Army stands against homophobia, which victimises people and can reinforce feelings of alienation, loneliness and despair. We want to be an inclusive church community where members of the LGBT community find welcome and the encouragement to develop their relationship with God ... Our international mission statement is very clear on this point when it says we will \\"meet human needs in [Jesus'] name without discrimination\\". Anyone who comes through our doors will be welcomed with love and service, based on their need and our capacity to provide.[69]\\r\\nAs of late 2013, activists were still calling on the Salvation Army to change its stance on LGBT issues, citing ongoing discrimination.[70][71]\\r\\nDuring the 2010 Christmas season, the Salvation Army in Calgary, Alberta, refused to accept toys based on the Harry Potter and Twilight franchises because of a perceived conflict with the organisation's religious principles. One volunteer claimed that the toys were destroyed instead of being given to other agencies. The volunteer also criticised the Salvation Army for accepting violence-themed toys such as plastic rifles while not accepting Harry Potter or Twilight toys. A Salvation Army captain said that the toys were given to other organisations, not disposed of.[72] This policy is however, not universal, as the Wetaskiwin chapter of the Salvation Army has accepted Harry Potter toys. One captain called the series \\"a classic story of good winning over evil\\".[73]\\r\\nAlso during the 2010 Christmas season, the Salvation Army in Vancouver, BC, came under fire for a program that provided goodie bags to federal inmates for Christmas by playing Santa to incarcerated criminals.[74] This was however simply an extended outreach as part of the Salvation Army's prisoner visitation program established over a century ago.\\r\\nIn 2004, the Salvation Army's New York division was named in a lawsuit filed by 18 current and former employees of its social service arm, claiming that the organisation asked about the religious and sexual habits of employees in programs funded by local and state government. One member claimed the organisation forced them to agree \\"to preach the Gospel of Jesus Christ\\".[75] Proselytising or otherwise pursuing religious motives in a government-funded program is generally considered a violation of the Establishment Clause of the US Constitution. While the employment-discrimination portion of the lawsuit was dismissed in 2005, government agencies agreed in a 2010 settlement to set up monitoring systems to ensure that the Army did not violate church-state separation in its publicly funded projects. The organisation did not dispute allegations that nine-year-olds in a city-funded foster care program were put through a \\"confirmation-like\\" ceremony, where they were given Bibles and prayed over.[76]\\r\\nFrom the 1940s to the 1980s the Salvation Army in Australia sheltered approximately 30,000 children. In 2006 the Australian division of the Salvation Army acknowledged that sexual abuse may have occurred during this time and issued an apology. In it, the Army explicitly rejected a claim, made by a party unnamed in the apology, that there were as many as 500 potential claimants.[77]\\r\\nIn 2013 it was reported that private settlements totalling A$15.5?million had been made in Victoria relating to 474 abuse cases; a Salvation Army spokesman said that \\"This should not have happened and this was a breach of the trust placed in us\\" and that they were \\"deeply sorry\\" whilst claiming that the abuse was \\"the result of individuals and not a culture within the organization\\".[78][79][80]\\r\\nOn 28 January 2014, the Royal Commission into Institutional Responses to Child Sexual Abuse, a royal commission of inquiry initiated in 2013 by the Australian Government and supported by all of its state governments,[81] began an investigation into abuse cases at the Alkira Salvation Army Home for Boys at Indooroopilly; the Riverview Training Farm (also known as Endeavour Training Farm) at Riverview?ÿ  both in Queensland; the Bexley Boys Home at Bexley; and the Gill Memorial Home at Goulburn?ÿ  both in New South Wales. The investigation also examined the Salvation Army's processes in investigating, disciplining, removing and/or transferring anyone accused of, or found to have engaged in, child sexual abuse in these homes.[82][83][84][85] On 27 March 2014, the Royal Commission began an investigation into the handling by the Salvation Army (Eastern Territory) of claims of child sexual abuse between 1993 and 2014.[86][87][88]\\r\\nThe Royal Commission published a case study report on the findings and recommendations for one of the above-mentioned case studies.[89]\\r\\nThe Salvation Army has been criticized for making use of the UK Government's workfare schemes across Britain.[90] The UK campaign group Boycott Workfare lists them as a participant in Workfare.[91]","input":"Who was the founder of the salvation army?"},{"output":"28 July 1914","context":"","input":"When did the first world war break out?"},{"output":"Friday before the first full week of August","context":"\\r\\n\\r\\nThe Sturgis Motorcycle Rally is an American motorcycle rally held annually in Sturgis, South Dakota, for ten days[2] usually during the first full week of August.  In 2015 the city of Sturgis officially expanded the dates to have the rally start on the Friday before the first full week of August and end on the second Sunday. It was begun in 1938 by a group of Indian Motorcycle riders and was originally held for stunts and races. Attendance has historically been around 500,000 people, reaching a high of over 700,000 in 2015. The event generates around $800 million in revenue.[1][3]\\r\\n\\r\\nThe first rally was held by Indian Motorcycle riders on August 14, 1938, by the Jackpine Gypsies motorcycle club.[4] The club still owns and operates the tracks, hillclimb, and field areas where the rally is centered. The first event was called the \\"Black Hills Classic\\" and consisted of a single race with nine participants and a small audience. The founder is Clarence \\"Pappy\\" Hoel. He purchased an Indian motorcycle franchise in Sturgis in 1936 and formed the Jackpine Gypsies that same year.[4] The Jackpine Gypsies were inducted to the Motorcycle Hall of Fame in 1997.[5] Hoel was inducted into the AMA Hall of Fame the following year, in 1998.[6]\\r\\n\\r\\nThe focus of a motorcycle rally was originally racing and stunts. In 1961, the rally was expanded to include the Hillclimb and Motocross races.[4] This could include half-mile track racing (the first year in Sturgis, there were 19 participants), intentional board wall crashes, ramp jumps and head-on collisions with automobiles.\\r\\n\\r\\nThe Sturgis Rally has been held every year, with exceptions during World War II. For instance, in 1942, the event was not held due to gasoline rationing.[4]\\r\\n\\r\\nThe South Dakota Department of Transportation provides official traffic counts, which sometimes differ from official attendance figures.[7]\\r\\n\\r\\nFor many years the city has been in a licensing agreement with a community non-profit, Sturgis Motorcycle Rally, Inc., and its predecessor-in-interest, the Sturgis Area Chamber of Commerce, that generates millions of dollars in royalties and sponsorship dollars.[13]  In 2012 the City Council reaffirmed this relationship through a unanimous proclamation.\\r\\n\\r\\nThe City of Sturgis has calculated that the Rally brings over $800 million to South Dakota annually.[13] The City of Sturgis earned almost $270,000 in 2011 from selling event guides and sponsorships. The rally makes up 95% of the city's annual revenue.[14]\\r\\n\\r\\nThere were 405 individuals jailed at the 2004 rally, and approximately $250,000 worth of motorcycles stolen annually.[15] Rally-goers are a mix of white-collar and blue-collar workers and are generally welcomed as an important source of income for Sturgis and surrounding areas.[16] The rally turns local roads into \\"parking lots\\",[16] and draws local law enforcement away from routine patrols.[17] Furthermore, the large numbers of people visiting the town and region served as a model for the state of Oregon in preparation for the solar eclipse of August 21, 2017, given the expected impact on emergency services.[18]\\r\\n\\r\\nThe Lakota Indian tribe in coalition with other tribes has protested the large amount of alcohol distributed at the event so close to the sacred Bear Butte, but also acknowledged that income from the event was important to the region and also benefits some members of the tribes.[19]\\r\\n\\r\\nThere has been a number of mysterious, unsolved deaths at the Rally.[20]\\r\\n\\r\\nMany attendees of the Sturgis Rally have families, bring their children and drive campers towing motorcycle trailers to the rally, and ride their motorcycles just the last few miles. The director of the rally estimated in 2005 that less than half the attendees actually rode there.[21] Shipping companies transport thousands of motorcycles to Sturgis for attendees who arrive via airline.[21]\\r\\n\\r\\nThe Black Hills Run is a route favored by motorcycle riders, across the Black Hills from Deadwood to Custer State Park, South Dakota.  It reached the height of its popularity between 1939 and 1941.  The popularity of the Sturgis Motorcycle Rally attracted additional attention to the route in recent years. The pine forested mountains of the Black Hills make for a unique scenic motorcycle ride.[22]\\r\\n\\r\\nThe Rapid City Journal features daily coverage of the Sturgis Rally.[23]\\r\\n\\r\\nThe Seattle Times covered some of the 2008 Sturgis Rally while rock band Judd Hoos was playing at the Loud American Roadhouse.[24]\\r\\n\\r\\nIn 1997, the crew from the COPS television series attended the rally, as well as Dennis Rodman.[25]\\r\\n\\r\\nFrom 1996 to 1999, World Championship Wrestling held a pay-per-view event called Road Wild (Hog Wild for the 1996 event).[26]\\r\\n\\r\\nAnnual television coverage of the festival by the VH1 Classic network includes interviews and performances as well as rock music videos. The rally was featured in 2005 as part of the ESPN SportsCenter promotion 50 States in 50 Days.[27]\\r\\n\\r\\nStarting in 2009 an American reality television series began airing on the truTV network: Full Throttle Saloon, showing the inner operations at the world's largest biker bar just prior to the rally opening and for the duration of the rally each year.\\r\\n\\r\\nSturgis was also featured on American Pickers Season 4, Episode 6, \\"What Happens In Sturgis...\\". Originally aired January 2, 2012 on the History Channel. \\". . .When Mike tells Frank let's pack up for a trip to South Dakota, Frank says he can't. He's secretly going to his 30th annual trip to the legendary Sturgis motorcycle rally, but says he'll cover the shop. . .\\".[28] Sturgis has also been featured in the TV Show Pawn Stars in which Richard Harrison, Corey Harrison  visit Sturgis with Chumlee Russell on his birthday.\\r\\n\\r\\nThe Travel Channel has aired two one-hour (43 minutes runtime) documentaries about Sturgis:\\r\\n\\r\\n\\r\\n\\r\\nCoordinates: 442452N 1033032W? / ?44.41444N 103.50889W? / 44.41444; -103.50889","input":"When does the sturgis south dakota rally start?"},{"output":"about 0.041% (equal to 410 ppm) by volume","context":"Carbon dioxide (CO2) is an important trace gas in Earth's atmosphere. CO2 is a major greenhouse gas and plays a vital role in regulating Earth's surface temperature through the greenhouse effect.[1] Carbon dioxide is an integral part of the carbon cycle, a biogeochemical cycle in which carbon is exchanged between the Earth's oceans, soil, rocks and the biosphere. Plants and other photoautotrophs use solar energy to produce carbohydrate from atmospheric carbon dioxide and water by photosynthesis. Almost all other organisms depend on carbohydrate derived from photosynthesis as their primary source of energy and carbon compounds.\\r\\nReconstructions show that concentrations of CO2 in the atmosphere have varied from as high as 7,000 parts per million (ppm) during the Cambrian period about 500 million years ago to as low as 180 ppm during the Quaternary glaciation of the last two million years. Global annual mean CO2 concentration has increased by more than 45% since the start of the Industrial Revolution. The concentration was 280 ppm during the 10,000 years up to the mid-18th century,[2] increasing to 407 ppm as of mid-2017.[3][4] The present concentration is the highest in at least the past 800,000 years[5] and likely the highest in the past 20 million years.[6] The increase has been caused by human activities, particularly the burning of fossil fuels and deforestation.[7] This increase of CO2 and other greenhouse gases in Earth's atmosphere has produced the current episode of global warming. About 30ÿ40% of the CO2 released by humans into the atmosphere dissolves into oceans, rivers and lakes,[8][9] which has produced ocean acidification.\\r\\n\\r\\n\\r\\nOver the past 400,000 years, CO2 concentrations have shown several cycles of variation from about 180 parts per million during the deep glaciations of the Holocene and Pleistocene to 280 parts per million during the interglacial periods. Each part per million by volume of CO2 in the atmosphere contains approximately 2.13 gigatonnes of carbon.[10] Currently CO2 constitutes about 0.041% (equal to 410 ppm) by volume of the atmosphere,[11][12] which corresponds to approximately 3200 gigatons of CO2, which includes approximately 870 gigatons of carbon. The global mean CO2 concentration is currently rising at a rate of approximately 2 ppm/year and accelerating.[13][14]\\r\\nFollowing the start of the Industrial Revolution, atmospheric CO2 concentration has increased to over 400 parts per million and continues to increase. The daily average concentration of atmospheric CO2 at Mauna Loa Observatory first exceeded 400 ppm on 10 May 2013.[15] This has caused the phenomenon of global warming.[16] The global average concentration of CO2 in Earth's atmosphere is currently about 0.04%,[17] or 400 parts per million by volume (ppm).[13][18] There is an annual fluctuation of about 3ÿ9 ppm which is negatively correlated with the Northern Hemisphere's growing season. The Northern Hemisphere dominates the annual cycle of CO2 concentration because it has much greater land area and plant biomass than the Southern Hemisphere. Concentrations reach a peak in May as the Northern Hemisphere spring greenup begins, and decline to a minimum in October, near the end of the growing season.[19]\\r\\nSince global warming is attributed to increasing atmospheric concentrations of greenhouse gases such as CO2, scientists closely monitor atmospheric CO2 concentrations and their impact on the present-day biosphere. At the scientific recording station in Mauna Loa, the concentration reached 400 ppm for the first time in May 2013,[15][20] although this concentration had already been reached in the Arctic in June 2012.[21] The National Geographic wrote that the concentration of carbon dioxide in the atmosphere is this high \\"for the first time in 55 years of measurementand probably more than 3 million years of Earth history.\\"[22] The current concentration may be the highest in the last 20 million years.[6]\\r\\nCarbon dioxide concentrations have varied widely over the Earth's 4.54 billion year history. Carbon dioxide is believed to have been present in Earth's first atmosphere, shortly after Earth's formation. Earth's second atmosphere emerged after the lighter gases, hydrogen and helium, escaped to space or like oxygen were bound up in molecules and is thought to have consisted largely of nitrogen, carbon dioxide and inert gases[which?] produced by outgassing from volcanism, supplemented by gases[which?] produced during the late heavy bombardment of Earth by asteroids.[citation needed] The production of free oxygen by cyanobacterial photosynthesis eventually led to the oxygen catastrophe that ended Earth's second atmosphere and brought about the Earth's third atmosphere (the modern atmosphere) 2.4 billion years before the present. Carbon dioxide concentrations dropped from 7,000 parts per million during the Cambrian period about 500 million years ago to as low as 180 parts per million during the Quaternary glaciation of the last two million years.[citation needed]\\r\\nOn long timescales, atmospheric CO2 concentration is determined by the balance among geochemical processes including organic carbon burial in sediments, silicate rock weathering, and volcanism. The net effect of slight imbalances in the carbon cycle over tens to hundreds of millions of years has been to reduce atmospheric CO2. On a timescale of billions of years, such downward trend appears bound to continue indefinitely as occasional massive historical releases of buried carbon due to volcanism will become less frequent (as earth mantle cooling and progressive exhaustion of internal radioactive heat proceeds further). The rates of these processes are extremely slow; hence they are of no relevance to the atmospheric CO2 concentration over the next hundreds or thousands of years.\\r\\nIn billion-year timescales, it is predicted that plant, and therefore animal, life on land will die off altogether, since by that time most of the remaining carbon in the atmosphere will be sequestered underground, and natural releases of CO2 by radioactivity-driven tectonic activity will have continued to slow down.[27] The loss of plant life would also result in the eventual loss of oxygen. Some microbes are capable of photosynthesis at concentrations of CO2 of a few parts per million and so the last life forms would probably disappear finally due to the rising temperatures and loss of the atmosphere when the sun becomes a red giant some four billion years from now.[28]\\r\\nThe most direct method for measuring atmospheric carbon dioxide concentrations for periods before instrumental sampling is to measure bubbles of air (fluid or gas inclusions) trapped in the Antarctic or Greenland ice sheets. The most widely accepted of such studies come from a variety of Antarctic cores and indicate that atmospheric CO2 concentrations were about 260ÿ280 ppmv immediately before industrial emissions began and did not vary much from this level during the preceding 10,000 years.[29] The longest ice core record comes from East Antarctica, where ice has been sampled to an age of 800,000 years.[5] During this time, the atmospheric carbon dioxide concentration has varied between 180ÿ210 ppm during ice ages, increasing to 280ÿ300 ppm during warmer interglacials.[30][31] The beginning of human agriculture during the current Holocene epoch may have been strongly connected to the atmospheric CO2 increase after the last ice age ended, a fertilization effect raising plant biomass growth and reducing stomatal conductance requirements for CO2 intake, consequently reducing transpiration water losses and increasing water usage efficiency.[32]\\r\\nVarious proxy measurements have been used to attempt to determine atmospheric carbon dioxide concentrations millions of years in the past. These include boron and carbon isotope ratios in certain types of marine sediments, and the number of stomata observed on fossil plant leaves. While these measurements give much less precise estimates of carbon dioxide concentration than ice cores, there is evidence for very high CO2 volume concentrations between 200 and 150 million years ago of over 3,000 ppm, and between 600 and 400 million years ago of over 6,000 ppm.[6] In more recent times, atmospheric CO2 concentration continued to fall after about 60 million years ago. About 34 million years ago, the time of the EoceneÿOligocene extinction event and when the Antarctic ice sheet started to take its current form, CO2 is found to have been about 760 ppm,[33] and there is geochemical evidence that concentrations were less than 300 ppm by about 20 million years ago. Carbon dioxide decrease, with a tipping point of 600 ppm, was the primary agent forcing Antarctic glaciation.[34] Low CO2 concentrations may have been the stimulus that favored the evolution of C4 plants, which increased greatly in abundance between 7 and 5 million years ago.[35]\\r\\nAncient-Earth climate reconstruction is a vibrant field with numerous studies and reconstructions that sometimes reinforce one another and sometimes disagree with each other. Academically, one study disputed the claim of stable CO2 concentrations during the present interglacial of the last 10,000 years. Based on an analysis of fossil leaves, Wagner et al.[36] argued that CO2 levels during the last 7,000ÿ10,000 year period were significantly higher (~300 ppm) and contained substantial variations that may be correlated to climate variations. Others have disputed such claims, suggesting they are more likely to reflect calibration problems than actual changes in CO2.[37] Relevant to this dispute is the observation that Greenland ice cores often report higher and more variable CO2 values than similar measurements in Antarctica. However, the groups responsible for such measurements (e.g. H. J Smith et al.[38]) believe the variations in Greenland cores result from in situ decomposition of calcium carbonate dust found in the ice. When dust concentrations in Greenland cores are low, as they nearly always are in Antarctic cores, the researchers report good agreement between measurements of Antarctic and Greenland CO2 concentrations.\\r\\nEarths natural greenhouse effect makes life as we know it possible and carbon dioxide plays a significant role in providing for the relatively warm temperature that the planet enjoys. The greenhouse effect is a process by which thermal radiation from a planetary atmosphere warms the planet's surface beyond the temperature it would have in the absence of its atmosphere.[39][40][41] Without the greenhouse effect, the Earth's temperature would be about ?18?C (-0.4?F) [42][43] compared to Earth's actual surface temperature of approximately 14?C (57.2?F).[44]\\r\\nCarbon dioxide is believed to have played an important effect in regulating Earth's temperature throughout its 4.7 billion year history. Early in the Earth's life, scientists have found evidence of liquid water indicating a warm world even though the Sun's output is believed to have only been 70% of what it is today. It has been suggested by scientists that higher carbon dioxide concentrations in the early Earth atmosphere might help explain this faint young sun paradox. When Earth first formed, Earth's atmosphere may have contained more greenhouse gases and CO2 concentrations may have been higher, with estimated partial pressure as large as 1,000?kPa (10?bar), because there was no bacterial photosynthesis to reduce the gas to carbon compounds and oxygen. Methane, a very active greenhouse gas which reacts with oxygen to produce CO2 and water vapor, may have been more prevalent as well, with a mixing ratio of 10?4 (100 parts per million by volume).[45][46]\\r\\nThough water is responsible for most (about 36-70%) of the total greenhouse effect, the role of water vapor as a greenhouse gas depends on temperature. On earth, carbon dioxide is the most relevant direct anthropologically influenced greenhouse gas. Carbon dioxide is often mentioned in the context of its increased influence as a greenhouse gas since the pre-industrial (1750) era. In the IPCC Fifth Assessment Report the increase in CO2 was estimated to be responsible for 1.82 W{m2 of the 2.63 W{m2 change in radiative forcing on earth (about 70%).[49]\\r\\nThe concept of atmospheric CO2 increasing ground temperature was first published by Svante Arrhenius in 1896.[50] The increased radiative forcing due to increased CO2 in the earth's atmosphere is based on the physical properties of CO2 and the non-saturated absorption windows where CO2 absorbs outgoing long-wave energy.\\r\\nAtmospheric carbon dioxide plays an integral role in the Earth's carbon cycle whereby carbon dioxide is removed from the atmosphere by some natural processes such as photosynthesis and deposition of carbonates, to form limestones for example, and added back to the atmosphere by other natural processes such as respiration and the acid dissolution of carbonate deposits. There are two broad carbon cycles on earth: the fast carbon cycle and the slow carbon cycle. The fast carbon cycle refers to movements of carbon between the environment and living things in the biosphere whereas the slow carbon cycle involves the movement of carbon between the atmosphere, oceans, soil, rocks and volcanism. Both carbon cycles are intrinsically interconnected and atmospheric gaseous carbon dioxide facilitates the carbon cycle.\\r\\nNatural sources of atmospheric carbon dioxide include volcanic outgassing, the combustion of organic matter, wildfires and the respiration processes of living aerobic organisms. Man-made sources of carbon dioxide include the burning of fossil fuels for heating, power generation and transport, as well as some industrial processes such as cement making. It is also produced by various microorganisms from fermentation and cellular respiration. Plants, algae and cyanobacteria convert carbon dioxide to carbohydrates by a process called photosynthesis. They gain the energy needed for this reaction from absorption of sunlight by chlorophyll and other pigments. Oxygen, produced as a by-product of photosynthesis, is released into the atmosphere and subsequently used for respiration by heterotrophic organisms and other plants, forming a cycle.\\r\\nMost sources of CO2 emissions are natural, and are balanced to various degrees by natural CO2 sinks. For example, the natural decay of organic material in forests and grasslands and the action of forest fires results in the release of about 439 gigatonnes of carbon dioxide every year, while new growth entirely counteracts this effect, absorbing 450 gigatonnes per year.[51] Although the initial carbon dioxide in the atmosphere of the young Earth was produced by volcanic activity, modern volcanic activity releases only 130 to 230 megatonnes of carbon dioxide each year.[52] These natural sources are nearly balanced by natural sinks, physical and biological processes which remove carbon dioxide from the atmosphere. For example, some is directly removed from the atmosphere by land plants for photosynthesis and it is soluble in water forming carbonic acid. There is a large natural flux of CO2 into and out of the biosphere and oceans.[53] In the pre-industrial era these fluxes were largely in balance. Currently about 57% of human-emitted CO2 is removed by the biosphere and oceans.[54][55] From pre-industrial era to 2010, the terrestrial biosphere represented a net source of atmospheric CO2 prior to 1940, switching subsequently to a net sink.[55] The ratio of the increase in atmospheric CO2 to emitted CO2 is known as the airborne fraction (Keeling et al., 1995); this varies for short-term averages and is typically about 45% over longer (5 year) periods.[55] Estimated carbon in global terrestrial vegetation increased from approximately 740 billion tons in 1910 to 780 billion tons in 1990.[56]\\r\\nCarbon dioxide in the Earth's atmosphere is essential to life and to the present planetary biosphere. Over the course of Earth's geologic history CO2 concentrations have played a role in biological evolution. The first photosynthetic organisms probably evolved early in the evolutionary history of life and most likely used reducing agents such as hydrogen or hydrogen sulfide as sources of electrons, rather than water.[57] Cyanobacteria appeared later, and the excess oxygen they produced contributed to the oxygen catastrophe,[58] which rendered the evolution of complex life possible. In recent geologic times, low CO2 concentrations below 600 parts per million might have been the stimulus that favored the evolution of C4 plants which increased greatly in abundance between 7 and 5 million years ago over plants that use the less efficient C3 metabolic pathway.[35] At current atmospheric pressures photosynthesis shuts down when atmospheric CO2 concentrations fall below 150 ppm and 200 ppm although some microbes can extract carbon from the air at much lower concentrations.[59][60] Today, the average rate of energy capture by photosynthesis globally is approximately 130?terawatts,[61][62][63] which is about six times larger than the current power consumption of human civilization.[64] Photosynthetic organisms also convert around 100ÿ115 thousand million metric tonnes of carbon into biomass per year.[65][66]\\r\\nPhotosynthetic organisms are photoautotrophs, which means that they are able to synthesize food directly from CO2 and water using energy from light. However, not all organisms that use light as a source of energy carry out photosynthesis, since photoheterotrophs use organic compounds, rather than CO2, as a source of carbon.[67] In plants, algae and cyanobacteria, photosynthesis releases oxygen. This is called oxygenic photosynthesis. Although there are some differences between oxygenic photosynthesis in plants, algae, and cyanobacteria, the overall process is quite similar in these organisms. However, there are some types of bacteria that carry out anoxygenic photosynthesis, which consumes CO2 but does not release oxygen.\\r\\nCarbon dioxide is converted into sugars in a process called carbon fixation. Carbon fixation is an endothermic redox reaction, so photosynthesis needs to supply both a source of energy to drive this process, and the electrons needed to convert CO2 into a carbohydrate. This addition of the electrons is a reduction reaction. In general outline and in effect, photosynthesis is the opposite of cellular respiration, in which glucose and other compounds are oxidized to produce CO2 and water, and to release exothermic chemical energy to drive the organism's metabolism. However, the two processes take place through a different sequence of chemical reactions and in different cellular compartments.\\r\\nMost organisms that utilize photosynthesis to produce oxygen use visible light to do so, although at least three use shortwave infrared or, more specifically, far-red radiation.[68]\\r\\nA 1993 review of scientific greenhouse studies found that a doubling of CO2 concentration would stimulate the growth of 156 different plant species by an average of 37%. The amount of gain varied significantly by species, with some showing much greater gains, and a small number showing a loss. For example, a 1979 greenhouse study compared the dry weights of cotton and maize plants grown in different greenhouses, one with double the CO2 concentration of the other. In the enriched CO2 air, the dry weight of 40-day-old cotton plants doubled, but the dry weight of 30-day-old maize plants increased by only 20%.[69][70]\\r\\nBecause of concerns that greenhouse experiments might not adequately simulate the actual environment (such as competing plants), scientists developed Free-air carbon dioxide enrichment (FACE) experiments. In this procedure, plants are grown outdoors, and the CO2 concentration of the surrounding air is artificially elevated. FACE experiments, using generally lower CO2 levels than the greenhouse studies, showed lower gains in growth than greenhouse studies, with the gains depending heavily on the species under study. A 2005 review of 12 experiments at 475-600 ppm showed an average gain of 17% in crop yield, with legumes typically showing a greater response than other species, and C4 plants generally showing less. The review also stated that the reviewed experiments have their own limitations. The studied CO2 levels were lower, and most of the experiments were carried out in temperate regions.[71]\\r\\nA 2017 article describes research that shows how increased CO2 levels have a negative impact on a variety of human food crops, by increasing the levels of carbohydrates, such as glucose, while decreasing the levels of important nutrients such as protein, iron, and zinc. Crops experiencing a decrease in protein include rice, wheat, barley and potatoes.[72]\\r\\nThe Earth's oceans contain a large amount of CO2 in the form of bicarbonate and carbonate ions  much more than the amount in the atmosphere. The bicarbonate is produced in reactions between rock, water, and carbon dioxide. One example is the dissolution of calcium carbonate:\\r\\nReactions like this tend to buffer changes in atmospheric CO2. Since the right side of the reaction produces an acidic compound, adding CO2 on the left side decreases the pH of sea water, a process which has been termed ocean acidification (pH of the ocean becomes more acidic although the pH value remains in the alkaline range). Reactions between CO2 and non-carbonate rocks also add bicarbonate to the seas. This can later undergo the reverse of the above reaction to form carbonate rocks, releasing half of the bicarbonate as CO2. Over hundreds of millions of years, this has produced huge quantities of carbonate rocks.\\r\\nUltimately, most of the CO2 emitted by human activities will dissolve in the ocean;[73] however, the rate at which the ocean will take it up in the future is less certain. Even if equilibrium is reached, including dissolution of carbonate minerals, the increased concentration of bicarbonate and decreased or unchanged concentration of carbonate ion will give rise to a higher concentration of un-ionized carbonic acid and dissolved CO2. This, along with higher temperatures, would mean a higher equilibrium concentration of CO2 in the air.\\r\\nWhile CO2 absorption and release is always happening as a result of natural processes, the recent rise in CO2 levels in the atmosphere is known to be mainly due to human (anthropogenic) activity.[74] There are 4 ways human activity, especially fossil fuel burning, is known to have caused the rapid increase in atmospheric CO2 over the last few centuries. 1) Various national statistics accounting for fossil fuel consumption, combined with knowledge of how much atmospheric CO2 is produced per unit of fossil fuel (e.g. liter of gasoline).[75] 2) By examining the ratio of various carbon isotopes in the atmosphere.[74] The burning of long-buried fossil fuels releases CO2 containing carbon of different isotopic ratios to those of living plants, enabling distinction between natural and human-caused contributions to CO2 concentration. 3) Higher atmospheric CO2 concentrations in the Northern Hemisphere, where most of the world's population lives (and emissions originate from), compared to the southern hemisphere. This difference has increased as anthropogenic emissions have increased.[76] 4) Atmospheric O2 levels are decreasing in earth's atmosphere as it reacts with the carbon in fossil fuels to form CO2.[77]\\r\\nBurning fossil fuels such as coal, petroleum, and natural gas is the leading cause of increased anthropogenic CO2; deforestation is the second major cause. In 2010, 9.14 gigatonnes of carbon (GtC, equivalent to 33.5 gigatonnes of CO2 or about 4.3 ppm in earth's atmosphere) were released from fossil fuels and cement production worldwide, compared to 6.15 GtC in 1990.[78] In addition, land use change contributed 0.87 GtC in 2010, compared to 1.45 GtC in 1990.[78] In 1997, human-caused Indonesian peat fires were estimated to have released between 13% and 40% of the average carbon emissions caused by the burning of fossil fuels around the world in a single year.[79][80][81] In the period 1751 to 1900, about 12 GtC were released as CO2 to the atmosphere from burning of fossil fuels, whereas from 1901 to 2013 the figure was about 380 GtC.[82]\\r\\nAnthropogenic carbon emissions exceed the amount that can be taken up or balanced out by natural sinks.[87] As a result, carbon dioxide has gradually accumulated in the atmosphere, and as of 2013[update], its concentration is almost 43% above pre-industrial levels.[15][20] Various techniques have been proposed for removing excess carbon dioxide from the atmosphere in carbon dioxide sinks. Currently about half of the carbon dioxide released from the burning of fossil fuels is not absorbed by vegetation and the oceans and remains in the atmosphere.[88]\\r\\nExcess CO2 emitted since the pre-industrial era is projected to remain in the atmosphere for centuries to millennia,[89] even after emissions stop. Even if human carbon dioxide emissions were to completely cease, atmospheric temperatures are not expected to decrease significantly for thousands of years.[90]\\r\\nGlobal fossil carbon emissions 1800ÿ2007.\\r\\nFalse-color image of smoke and ozone pollution from Indonesian fires, 1997.\\r\\nBiosphere CO2 flux in the northern hemisphere summer (NOAA Carbon Tracker).\\r\\nBiosphere CO2 flux in the northern hemisphere winter (NOAA Carbon Tracker).\\r\\nThe first reproducibly accurate measurements of atmospheric CO2 were from flask sample measurements made by Dave Keeling at Caltech in the 1950s.[91] A few years later in March 1958 the first ongoing measurements were started by Keeling at Mauna Loa. Measurements at Mauna Loa have been ongoing since then. Now measurements are made at many sites globally. Additional measurement techniques are also used as well. Many measurement sites are part of larger global networks. Global network data are often made publicly available on the conditions of proper acknowledgement according to the respective data user policies.\\r\\nThere are several surface measurement (including flasks and continuous in situ) networks including NOAA/ERSL,[92] WDCGG,[93] and RAMCES.[94] The NOAA/ESRL Baseline Observatory Network, and the Scripps Institution of Oceanography Network [95] data are hosted at the CDIAC at ORNL. The World Data Centre for Greenhouse Gases (WDCGG), part of GAW, data are hosted by the JMA. The Reseau Atmospherique de Mesure des Composes an Effet de Serre database (RAMCES) is part of IPSL.\\r\\nFrom these measurements, further products are made which integrate data from the various sources. These products also address issues such as data discontinuity and sparseness. GLOBALVIEW-CO2 is one of these products.[96]\\r\\nOngoing ground-based total column measurements began more recently. Column measurements typically refer to an averaged column amount denoted XCO2, rather than a surface only measurement. These measurements are made by the TCCON. These data are also hosted on the CDIAC, and made publicly available according to the data use policy.[97]\\r\\nSatellite measurements are also a recent addition to atmospheric XCO2 measurements. SCIAMACHY aboard ESA's ENVISAT made global column XCO2 measurements from 2002-2012. AIRS aboard NASAs Aqua satellite makes global XCO2 measurements and was launched shortly after ENVISAT in 2012. More recent satellites have significantly improved the data density and precision of global measurements. Newer missions have higher spectral and spatial resolutions. JAXA's GOSAT was the first dedicated GHG monitoring satellite to successfully achieve orbit in 2009. NASA's OCO-2 launched in 2014 was the second. Various other satellites missions to measure atmospheric XCO2 are planned.","input":"What percentage of the earth's atmosphere is carbon dioxide?"},{"output":"20 rounds","context":"Overtime is a method of determining a winner in an ice hockey game when the score is tied after regulation. The main methods of determining a winner in a tied game are the overtime period (commonly referred to as overtime), the shootout, or a combination of both. If league rules dictate a finite time in which overtime may be played, with no penalty shoot-out to follow, the game's winning team may or may not be necessarily determined.\\r\\n\\r\\n\\r\\nOvertime periods are extra periods beyond the third regulation period during a game, where normal hockey rules apply. Although in the past, full-length overtime periods were played, overtimes today are golden goal (a form of sudden death), meaning that the game ends immediately when a player scores a goal.\\r\\nFrom November 21, 1942,[1] when overtime was eliminated due to war time restrictions and continuing until the 1983ÿ84 season, all NHL regular-season games tied after 60 minutes of play ended as ties. On June 23, 1983, the NHL introduced a regular-season overtime period of five minutes. If the five-minute overtime period ended with no scoring, the game ended as a tie (the World Hockey Association had used a 10-minute regular season overtime period, as had the NHL prior to World War II). In the first games to go to overtime, on October 5, 1983, the Minnesota North Stars and Los Angeles Kings skated to a 3ÿ3 tie, and the Detroit Red Wings and Winnipeg Jets tied 6ÿ6. The first regular-season game decided by overtime was on October 8, 1983, as the New York Islanders beat the Washington Capitals 8ÿ7.[2]\\r\\nIn 1987ÿ88 and since 1995, the American Hockey League has awarded teams one point in the standings for an overtime loss (OTL). In 1998, the AHL introduced a rule where teams will play the five-minute overtime period with four skaters and a goaltender, rather than at full strength (five skaters), except in two-man advantage situations. In a two-man advantage situation, the team with the advantage will play with five skaters against three skaters. The rule was popular and adopted by the NHL and ECHL the next season.\\r\\nAlex Ovechkin has the record for most NHL overtime goals with 20.\\r\\nIn the Stanley Cup playoffs and in all one-game playoffs, overtime periods are played like regulation periods except for the golden goal rule ÿ in an overtime period, the game ends when one team scores a goal; the teams are at full strength (five skaters, barring penalties), there is no shootout, and each overtime period is 20 minutes with full intermissions between overtime periods. Three of the game's legendary players, Mark Messier (109 playoff goals), Mario Lemieux (77 goals), and Gordie Howe (68 goals) never scored a playoff overtime goal.\\r\\nIn many leagues (including the NHL for regular-season games since the 2005ÿ06 season) and in international competitions, a failure to reach a decision in a single overtime may lead to a shootout. Some leagues may eschew overtime periods altogether and end games in shootout should teams be tied at the end of regulation. In the ECHL, regular season overtime periods are played four on four for one five-minute period. In the Southern Professional Hockey League, regular season overtime periods are played three on three for one five-minute period, with penalties resulting in the opponents skating one additional player on ice (up to two additional players) for the penalty for the first three minutes, and a penalty shot in the final two minutes. The AHL, since the 2014ÿ15 season, extended the overtime to seven minutes, with the last three minutes reduced further to three men aside and teams getting an additional skater for each opponent's penalty.[3] The idea of using 3-on-3 skaters for the entirety of a five-minute overtime period for a regular season game was adopted by the NHL on June 24, 2015, for use in the 2015ÿ16 NHL season.[4]\\r\\nIn IIHF play, rules for overtime depend on the stage of the competition. For a round robin or preliminary round game that goes to overtime, the teams will play a maximum of five minutes of 3-on-3 hockey in the \\"golden goal\\" format. If no one scores in the five minute overtime, a three-round shootout will decide the winner. In the case of a playoff game or a bronze medal game, the teams will play a maximum of ten minutes of 4-on-4 hockey in the \\"golden goal\\" format. If there is no score in the overtime, a five-round shootout will decide the winner. If the gold medal game of a top category IIHF championship goes to overtime, the teams will play a maximum of twenty minutes of 5-on-5 hockey in the \\"golden goal\\" format. If there is no winner after the overtime, a five-round shootout will decide the winner. In all cases, the teams must change ends, and defend the same side that they did in the second period.[5]\\r\\nIn international competition, shootouts (or more formally, game-winning shots (GWS), and, in some European countries, bullets, or bullits[6][7]), are often used. Each coach selects three skaters from their team to take penalty shots one at a time against the opposing goaltender, with teams alternating shots. Each team gets one shot per round. The winner is the team with more goals after three rounds or the team that amasses an unreachable advantage before then (ex. a team gains a two-goal lead with only one round left). If the shootout is tied after three rounds, tie-breaker rounds are played one at a time (with each team taking one additional shot) until there is a winner.\\r\\nThe IIHF first adopted the game-winning-shot procedure in 1992 when a new playoff procedure in the Winter Olympics and World Championships required a winner for each game. At that time, the shootout was five rounds and only used for knock-out games. In 2006, it was reduced to three rounds and used for all games, eliminating the possibility of tied games at IIHF events. Tie-breaker rounds are still used as needed, and the same or new players can take the tie-break shots, which is also done in reverse order.[8] As of May 2016, all IIHF preliminary round games that are not decided by overtime, are decided by a three-round shootout. However, all playoff, bronze medal games and gold medal games of IIHF top level championships (especially the Olympics) are decided by five round shooutouts.[5]\\r\\nMost lower minor leagues (ECHL, Central, UHL) have featured a shootout where, at the end of regulation, a shootout similar to the international tournament format is used.\\r\\nHowever, in 2000, the ECHL adopted the AHL's four-on-four overtime before the shootout.\\r\\nFor the 2004ÿ05 AHL season, the AHL adopted a five-man shootout, which was first used in that league in 1986ÿ87. The standard five-man shootout is used after four-on-four overtime for all minor leagues in North America. The AHL switched to the NHL formatted three-man shootout for the 2014ÿ15 season.[3]\\r\\nThe Central Collegiate Hockey Association added the shootout as of the 2008ÿ09 season.\\r\\nFollowing the lead of minor leagues, as of the 2005ÿ06 season, the NHL ends exhibition and regular season games still tied after a five-minute-length, three-skaters-per-side overtime period (as of the 2015ÿ16 NHL season onwards) with a shootout. The NHL format is a three-round shootout with tiebreaker rounds as needed. All skaters (except goalies) on a team's roster must shoot before any player can shoot a second time. On December 16, 2014 the longest shootout in NHL history went to 20 rounds before Nick Bjugstad of the Florida Panthers scored to defeat the Washington Capitals; the previous record was 15 rounds.[9]\\r\\nThe shootout is not used in the playoffs for any North American league. Instead, full 20-minute overtime periods are played until one team scores a goal.\\r\\nIn the National Hockey League and American Hockey League All-Star Skills Competitions, the competition ends in a penalty shootout known as the Breakaway Relay.\\r\\nTactics are very important during penalty shots and overtime shootouts for both the shooter and the goalie. Both shooters and goalies commonly consult their teammates and coaches for advice on the opposing player's style of play. Shooters often consider the goalie's strengths and weaknesses (such as a fast glove or stick save), preferred goaltending style (such as butterfly or stand-up) and method of challenging the shooter. Goaltenders often consider the shooter's shot preference, expected angle of attack, a patented move a shooter commonly uses and even handedness of the shooter.\\r\\nMost shooters attempt to out-deke the goalie in order to create a better scoring chance. Former Detroit Red Wings forward Pavel Datsyuk and New York Rangers forward Martin St. Louis are examples of players who commonly use this tactic. However, it is not uncommon for a shooter to simply shoot for an opening without deking. This is commonly referred to as sniping. This is most commonly performed when a goalie challenges a shooter by giving them an open hole (by keeping a glove, pad or stick out of position or being out of sound goaltending position altogether to tempt the shooter to aim for the given opening). Former NHL forwards Markus N?slund and Brett Hull are two players commonly referred to as snipers. Very rarely a shooter may take a slapshot or wrist shot from the point or top of the slot. This is almost exclusively performed when a shooter either has a high level of confidence in their shot or they attempt to catch the goalie by surprise. Retired player Brian Rolston, Detroit Red Wings winger Todd Bertuzzi, Philadelphia Flyers defenseman Chris Pronger, and Vancouver Canucks winger Daniel Sedin have all used this tactic with success.\\r\\nThe longest overtime game in history was an in Norwegian GET-ligaen. The game ended after 157:14 of overtime, in the 8th overtime period.\\r\\nThis is a list of all National Hockey League (NHL) overtime games that went into at least three overtimes (winning team is bold).\\r\\nThis is a list of the longest games in the highest Swedish leagues.[13]\\r\\nThis is a list of Kontinental Hockey League games that went to at least the third overtime.\\r\\nThe longest game in Belarusian extraleague is game first of the 2015 Semi-finals on March 7, 2015. Yunost Minsk beat the HK Gomel, 2ÿ1, at Gomel Ice Palace of Sports on a goal by Vitaly Kiryuschenkov at 5:11 of the sixth 20-minute overtime period. Alexander Tsetkovsky was the winning goaltender for the Yunost, making 107 saves.[20][21]\\r\\nThis is a list of Gold Medal men's games from the Olympics that needed overtime.\\r\\nThis is a list of Gold Medal women's games from the Olympics that needed overtime.\\r\\nMarch 22, 2008: Philip Gogulla of the Cologne Sharks ends the longest German hockey game ever and the third longest worldwide, scoring the ninth-overall goal in a 5:4 victory over the Mannheim Eagles. The goal comes 8:16 into the sixth overtime period for a total of 108:16 of overtime. It is the third quarter-final game (best of seven) in the K?lnarena in Cologne in front of an audience of 17,000. The game had begun at 5:30?pm and ends at 12:15?am.\\r\\nMarch 12, 2017: Joakim Jensen of the Storhamar Ishockey ends the longest hockey game in history, scoring with 2:46 left in octuple overtime for a total of 157:14 of overtime, and 217:14 of hockey played. It was Game 5 of the quarter-finals of the 2016ÿ17 GET-ligaen playoffs against the Sparta Warriors in the CC Amfi. About 1,000 out of the 5,500 people that attended the game watched the entire game. The game started at 18:00 and ended at 2:32 the next morning.[22]\\r\\nThis is a list of the longest American Hockey League (AHL) overtime games.\\r\\nThe longest game in AHL history was game five of the 2008 East Division Semi-finals on April 24, 2008. The Philadelphia Phantoms beat the Albany River Rats, 3ÿ2, at Times Union Center on a goal by Ryan Potulny at 2:58 of quintuple overtime. Scott Munroe was the winning goaltender for the Phantoms, making 65 saves. Michael Leighton was the losing goaltender for the River Rats despite making 98 saves.\\r\\nThe University of New Brunswick Varsity Reds needed 61:53 of overtime (four extra periods) to defeat the Acadia University Axemen 3ÿ2 on February 27, 2011 in game two of a best-of-five AUS semi-final series at Fredericton, New Brunswick. Nick MacNeil scored the game-winner at 11:53 of the seventh period overall.\\r\\nYork University Lions and Lakehead University Thunderwolves went to a fourth overtime period (50:13 minutes of overtime) on February 14, 2007 in Thunder Bay, Ontario, to decide a winner in OUA men's playoff hockey action. Lakehead won the game at the 13-second mark of the fourth overtime period.\\r\\nMorgan McHaffie scored at 17:14 of the sixth overtime period to lead the Queen's Golden Gaels to a 2ÿ1 win over the host Guelph Gryphons in the first game of the best-of-three OUA women's hockey final, March 2, 2011. The game, which lasted 167 minutes and 14 seconds, including 107:14 of extra time, is the longest on record in CIS or NCAA hockey ÿ women's or men's. Winning goaltender Mel Dodd-Moher made 66 saves, while Danielle Skoufranis made 44 saves in a losing cause. It is the longest game ever played sanctioned by Hockey Canada.\\r\\n* Championship Series game.\\r\\n** Game 7\\r\\nOn May 12, 2008, one of the longest games in IHL history, if not the longest, took place in Fort Wayne, Indiana. It was the seventh game of the Turner Cup Final between the hometown Fort Wayne Komets and Port Huron Icehawks. The game was tied 2ÿ2 through regulation. The first two extra periods solved nothing, but 23 seconds into the third overtime period, at some point after midnight ET, Justin Hodgman scored the winning goal to give the Komets their fifth Turner Cup title. It was the club's first since 1993, and their sixth overall, with their last championship being the Colonial Cup in 2003. The Komets would win again the following year with an easy game five victory at home, which was the first time in franchise history they won back-to-back championships. They would follow up with a third consecutive Turner Cup in 2010, again clinching on home ice, securing a dynasty.\\r\\nThe longest game in NCAA hockey history was played on March 6ÿ7, 2015. UMass beat Notre Dame, 4ÿ3, in the Hockey East first-round playoff game after 91:42 of overtime. Shane Walsh scored the winning goal for UMass.[23][24]\\r\\nThe longest game in NCAA hockey history was played at Notre Dame in South Bend, Indiana on March 6, 2015. UMass beat Notre Dame, 4ÿ3, in the Hockey East first round with 8:18 left in quintuple overtime. Shane Walsh scored the winning goal just after 1:00?am local time. The previous longest was played on March 12, 2010. Quinnipiac University beat Union College, 3ÿ2, in the ECAC Quarterfinals, as Greg Holt scored with 9:38 left in quintuple overtime. The 3rd longest game in NCAA hockey history was played on March 5, 2006. Yale University beat Union College, 3ÿ2, in the ECAC Hockey League first-round playoff game after 81:35 of overtime. David Meckler scored the winning goal with Yale shorthanded.[25]\\r\\nThe longest game in NCAA Division III hockey history, and the fourth longest in NCAA history overall, began at 7:05?pm on February 27, 2010 and ended at 12:35?am of the following day. Gustavus Adolphus College beat Augsburg College, 6ÿ5, to advance to the MIAC championship game after 78:38 of overtime. Eric Bigham scored the winning goal.[26]\\r\\nA 2000 NCAA regional final in men's ice hockey between St. Lawrence University and Boston University ended with 63:53 of overtime. Manitoba native and minor hockey buddy of Craig McAulay, Robin Carruthers scored the gwg after four periods of overtime play\\r\\nA March 30, 1991 game between Northern Michigan University and Boston University ended with Northern Michigan earning an 8ÿ7 victory over Boston University. Unlikely hero Darryl Plandowski scores in the third overtime period and fifth hour of play to give the Wildcats the title.\\r\\nA March 8, 1997 game between Colorado College and the University of WisconsinÿMadison ended with Colorado College winning, 1ÿ0, after 69:30.\\r\\nA March 14, 2003 ECAC Quarterfinal game between Colgate University and Dartmouth ended, 4ÿ3 for Colgate, after 61:05 in overtime.\\r\\nOn March 26, 2006, the Wisconsin Badgers beat the Cornell Big Red 1ÿ0 at 11:13 into the third overtime at the Midwest Regional Final in the NCAA Tournament at the Resch Center in Green Bay. It was the second-longest NCAA Tournament game in its history and the longest 1ÿ0 game in tournament history. It is currently the ninth-longest game all-time in NCAA Division I history.\\r\\nAn March 11, 2007 game between St. Cloud State University and University of Minnesota Duluth during the first round of the WCHA playoffs ended with SCSU winning, 3ÿ2, after 51:33 of overtime. It is the eighth-longest NCAA Division I game in history.\\r\\nIn the first round of the 2008 WCHA hockey tournament featuring the fourth-seeded Minnesota State University, Mankato Mavericks hosting the seventh-seeded University of Minnesota Golden Gophers, the Friday and Sunday games both went into double overtime, and the Saturday night game went into one overtime. The Gophers prevailed two games to one in the series, winning Saturday and Sunday.\\r\\nOn March 3, 2012, in the first round of the 2012 ECAC Hockey hockey tournament featuring the seventh-seeded Clarkson Golden Knights men's ice hockey team hosting the tenth-seeded RPI Engineers men's ice hockey team, Clarkson beat RPI 4ÿ3 at 13:48 in the third overtime period, after 113:48 of play. It is currently the seventh-longest game all-time in NCAA Division I history.\\r\\nOn March 10, 1996, New Hampshire defeated Providence, 3ÿ2, in an ECAC Women's Championship game after 85:35 of overtime. (This is not an NCAA record, as the NCAA did not officially recognize women's hockey until the 2001 season; however, it stands as the longest women's college hockey game)[27]\\r\\nOn March 10, 2007, Wisconsin defeated Harvard, 1ÿ0, in an NCAA Women's Quarterfinal game after 67:09 of overtime at the Kohl Center in Madison WI. Wisconsin went on to win the national championship.\\r\\nOn March 10, 2012, Cornell University defeated Boston University, 8ÿ7, in an NCAA Women's Quarterfinal game after 59:50 of overtime at Lynah Rink in Ithaca, New York, surpassing the men's game from the previous night as the longest hockey game to be played at the rink.[28]\\r\\nOn March 21, 2010, Minnesota-Duluth defeated Cornell 3ÿ2 in the NCAA championship game, after 59:26 of overtime (119:26 total game time), the longest men's or women's hockey championship game in NCAA history.[29]\\r\\nThe semi-final game for the 2007 RBC Cup, saw the host Prince George Spruce Kings taking on the Camrose Kodiaks. The game ended up being the longest game in Royal Bank Cup history at 146 minutes and 1 second. The Spruce Kings broke a 2ÿ2 tie just over six minutes into the fifth overtime period to win 3ÿ2 and clinch a berth in the RBC Cup Final against the Aurora Tigers. Jason Yuel of the Spruce Kings scored the winner while goaltender Jordan White stopped 91 of 93 shots for the victory.\\r\\nOn February 10, 2007, the Toronto Jr. Canadiens defeated the Pickering Panthers, 4ÿ3, to take a 2ÿ0 series lead in the first round of the OPJHL playoffs, after 104:32 of overtime. It is the second longest game ever played sanctioned by Hockey Canada.\\r\\nFebruary 1999, the St. Catharines Falcons defeated the Port Colborne Sailors 7ÿ6 to take a 2ÿ1 series lead in the semi-finals of the Golden Horseshoe Jr. B Hockey League Playoffs. Peter Lacey scored 11 minutes into the fifth overtime period, ending the game at 2:18?am. The game started at 7:30?pm. It is the longest junior hockey game sanctioned by Hockey Canada\\r\\nMarquette vs Orchard Lake St Marys went eight overtimes during the Michigan State Ice Hockey Division 1 Championship game before Tournament officials stopped the game in consideration of the health and welfare of the players on March 8, 2008. The 1ÿ1 tie resulted in the two teams being declared co-champions. The game lasted 109 minutes.[30] Ryan Morley Stockton of St. Mary's had a MHSAA-record 58 saves.[31]\\r\\nThe longest game in high school history was in a 1996 FCIAC quarterfinal matchup in Darien, Connecticut between archrivals Wilton and Ridgefield that went to a tenth eight-minute overtime period after 45 minutes of regulation (125:00 of hockey). Chris Ludwig of Wilton scored the game-winner while being hauled down in front of the Ridgefield net in the tenth overtime period.\\r\\nThe previous record belonged to the Aurora High SchoolÿSolon High School game in which Aurora won in the eighth overtime period of the Ohio state playoffs.[32] The winning goal was scored with 3:52 left in the 8th overtime (105th minute), setting an American record.[33]\\r\\nSince 2015, all state tournaments allow up to 5 overtime periods (4-on-4 after first overtime), after which best-of-3-round shootouts and extra rounds if needed are conducted, to eliminate co-champions.","input":"What was the longest shootout in nhl history?"},{"output":"April 1964","context":"Black Widow (Russian: ѷ ־־, transliterated Chyornaya Vdova) (Natalia Alianovna \\"Natasha\\" Romanova,[1] Russian: ^ ^־ \\"\\" ˾־, also known as Natasha Romanoff) is a fictional superhero appearing in American comic books published by Marvel Comics. Created by editor and plotter Stan Lee, scripter Don Rico, and artist Don Heck, the character first appeared in Tales of Suspense No. 52 (April 1964). The character was first introduced as a Russian spy, an antagonist of the superhero Iron Man. She later defected to the United States, becoming an agent of the fictional spy agency S.H.I.E.L.D., and a member of the superhero team the Avengers.\\r\\nScarlett Johansson portrayed the character in the films Iron Man 2 (2010), The Avengers (2012), Captain America: The Winter Soldier (2014), Avengers: Age of Ultron (2015) and Captain America: Civil War (2016) as a part of the Marvel Cinematic Universe franchise.\\r\\n\\r\\n\\r\\nThe Black Widow's first appearances were as a recurring, non-costumed, Russian-spy antagonist in the feature \\"Iron Man\\", beginning in Tales of Suspense No. 52 (April 1964). Five issues later, she recruits the besotted costumed archer and later superhero Hawkeye to her cause. Her government later supplies her with her first Black Widow costume and high-tech weaponry, but she eventually defects to the United States after appearing, temporarily brainwashed against the U.S., in the superhero-team series The Avengers No. 29 (July 1966). The Widow later becomes a recurring ally of the team before officially becoming its sixteenth member many years later.\\r\\nThe Black Widow was visually updated in 1970: The Amazing Spider-Man No. 86 (July 1970) reintroduced her with shoulder-length red hair (instead of her former short black hair), a skintight black costume, and wristbands which fired spider threads. This would become the appearance most commonly associated with the character.[2]\\r\\nIn short order, The Black Widow starred in her own series in Amazing Adventures #1ÿ8 (Aug. 1970ÿSept. 1971), sharing that split book with the feature Inhumans. The Black Widow feature was dropped after only eight issues (the Inhumans feature followed soon, ending with issue 10).[2]\\r\\nImmediately after her initial solo feature ended, the Black Widow co-starred in Daredevil #81ÿ124 (Nov. 1971ÿAug. 1975), of which #93-108 were cover titled Daredevil and the Black Widow. Daredevil writer Gerry Conway recounted, \\"It was my idea to team up Daredevil and the Black Widow, mainly because I was a fan of Natasha, and thought she and Daredevil would have interesting chemistry.\\"[2] Succeeding writers, however, felt that Daredevil worked better as a solo hero, and gradually wrote the Black Widow out of the series.[2] She was immediately recast into the super-team series The Champions as the leader of the titular superhero group, which ran for 17 issues (Oct. 1975ÿJan. 1978).[3]\\r\\nThroughout the 1980s and 1990s, the Black Widow appeared frequently as both an Avengers member and a freelance agent of S.H.I.E.L.D. She starred in a serialized feature within the omnibus comic-book series Marvel Fanfare #10ÿ13 (Aug. 1983ÿMarch 1984), written by George Prez and Ralph Macchio, with art by penciller Perez. These stories were later collected in the oversized one-shot Black Widow: Web of Intrigue No. 1 (June 1999).\\r\\nThe Widow guest-starred in issues of Solo Avengers, Force Works, Iron Man, Marvel Team-Up, and other comics. She had made frequent guest appearances in Daredevil since the late 1970s.\\r\\nShe starred in a three-issue arc, \\"The Fire Next Time\\", by writer Scott Lobdell and penciller Randy Green, in Journey into Mystery #517ÿ519 (Feb.ÿApril 1998).\\r\\nA new ongoing Black Widow comic title debuted in April 2010. The first story arc was written by Marjorie Liu with art by Daniel Acuna.[4] Beginning with issue No. 6 (Sept. 2010), the title was written by Duane Swierczynski, with artwork by Manuel Garcia and Lorenzo Ruggiero.\\r\\nBlack Widow appeared as a regular character throughout the 2010ÿ2013 Secret Avengers series, from issue #1 (July 2010) through its final issue #37 (March 2013).\\r\\nBlack Widow appears in the 2013 Secret Avengers series by Nick Spencer and Luke Ross.[5]\\r\\nBlack Widow appears in a relaunched ongoing series by writer Nathan Edmondson and artist Phil Noto. The first issue debuted in January 2014.[6]\\r\\nIn October 2015, it was announced that Mark Waid and Chris Samnee would be launching a new Black Widow series for 2016 as part of Marvel's post-Secret Wars relaunch.[7] The first issue was released in March 2016.[8]\\r\\nAside from the arcs in Marvel Fanfare and Journey into Mystery, the Black Widow has starred in four limited series and four graphic novels.\\r\\nThe three-issue Black Widow (June - Aug. 1999), under the Marvel Knights imprint, starred Romanova and fully introduced her appointed successor, Captain Yelena Belova, who had briefly appeared in an issue of the 1999 series Inhumans. The writer for the story arc, \\"The Itsy-Bitsy Spider\\" was Devin K. Grayson while J. G. Jones was the artist. The next three-issue, Marvel Knights mini-series, also titled Black Widow (Jan. - March 2001) featured both Black Widows in the story arc \\"Breakdown\\", by writers Devin Grayson and Greg Rucka with painted art by Scott Hampton.\\r\\nRomanova next starred in another solo miniseries titled Black Widow: Homecoming (Nov. 2004 - April 2005), also under the Marvel Knights imprint and written by science fiction novelist Richard K. Morgan, with art initially by Bill Sienkiewicz and later by Sienkiewicz over Goran Parlov layouts. A six-issue sequel, Black Widow: The Things They Say About Her (Nov. 2005ÿApril 2006; officially Black Widow 2: The Things They Say About Her in the series' postal indicia), by writer Morgan, penciller Sean Phillips, and inker Sienkiewicz, picks up immediately where the previous miniseries left off, continuing the story using many of the same characters.[citation needed]\\r\\nShe starred in the solo graphic novel Black Widow: The Coldest War (April 1990), and co-starred in three more: Punisher/Black Widow: Spinning Doomsday's Web (Dec. 1992); Daredevil/Black Widow: Abattoir (July 1993); and Fury/Black Widow: Death Duty (June 1995), also co-starring Marvel UK's Night Raven.[citation needed]\\r\\nBlack Widow is also featured in the short story Love Is Blindness in I Heart Marvel: Marvel Ai (2006) #1 (April 2006), where she instigates a humorous fight with Elektra over Daredevil's affections. The comic is stylized to look like Japanese animation and uses images, not words, inside the speech and thought bubbles to convey what the characters are saying/thinking.\\r\\nIn 2010, the year in which the character, called only Natasha Romanoff, made her film debut in Iron Man 2, the Black Widow received two separate miniseries. Black Widow and the Marvel Girls was an all-ages, four-issue series that chronicled her adventures with various women of the Marvel Universe, including Storm, She-Hulk, the Enchantress, and Spider-Woman. It was written by Paul Tobin, with art by Salvador Espin and Takeshi Miyazawa. The second four-issue miniseries, Black Widow: Deadly Origin, was written by Paul Cornell, and featured art by Tom Raney and John Paul Leon.[citation needed]\\r\\nNatasha was born in Stalingrad (now Volgograd), Russia. The first and best-known Black Widow is a Russian agent trained as a spy, martial artist, and sniper, and outfitted with an arsenal of high-tech weaponry, including a pair of wrist-mounted energy weapons dubbed her \\"Widow's Bite\\". She wears no costume during her first few appearances but simply evening wear and a veil. Romanova eventually defects to the U.S. for reasons that include her love for the reluctant-criminal turned superhero archer, Hawkeye.\\r\\nFirst hints to Natasha Romanova's childhood come by Ivan Petrovich, who is introduced as her middle-aged chauffeur and confidant in the Black Widow's 1970s Amazing Adventures. The man tells Matt Murdock how he had been given custody of little Natasha by a woman just before her death during the Battle of Stalingrad in autumn 1942. He had consequently felt committed to raise the orphan as a surrogate father and she had eventually trained as a Soviet spy, being eager to help her homeland.[9] In another flashback, set in the fictional island of Madripoor in 1941, Petrovich helps Captain America and the mutant Logan, who would later become the Canadian super-agent and costumed hero Wolverine, to rescue Natasha from Nazis.[10]\\r\\nA revised, retconned origin establishes her as being raised from very early childhood by the U.S.S.R.'s \\"Black Widow Ops\\" program, rather than solely by Ivan Petrovitch.[11] Petrovitch had taken her to Department X, with other young female orphans, where she was brainwashed, and trained in combat and espionage at the covert \\"Red Room\\" facility. There, she is biotechnologically and psycho-technologically enhancedan accounting that provides a rationale for her unusually long and youthful lifespan. During that time she had some training under Winter Soldier, and the pair even had a short romance.[12] Each Black Widow is deployed with false memories to help ensure her loyalty. Romanova eventually discovers this, including the fact that she had never, as she had believed, been a ballerina. She further discovers that the Red Room is still active as \\"2R\\".\\r\\nNatasha was arranged by the KGB to marry the renowned Soviet test pilot Alexei Shostakov. However, when the Soviet government decided to make Alexei into their new operative, the Red Guardian, he is told that he can have no further contact with his wife. Natasha is told that he had died and is trained as a secret agent separately.\\r\\nRomanova grew up to serve as a femme fatale. She was assigned to assist Boris Turgenov in the assassination of Professor Anton Vanko for defecting from the Soviet Union, which served as her first mission in the United States. Natasha and Turgenov infiltrated Stark Industries as part of the plan.[13] She attempted to manipulate information from American defense contractor Tony Stark, and inevitably confronted his superhero alter ego, Iron Man. The pair then battled Iron Man, and Turgenov steals and wears the Crimson Dynamo suit. Vanko sacrificed himself to save Iron Man, killing Turgenov in the process, using an unstable experimental laser light pistol.[14] Romanova later meets the criminal archer Hawkeye and sets him against Iron Man,[15] and later helped Hawkeye battle Iron Man.[16]\\r\\nNatasha once more attempted to get Hawkeye to help her destroy Iron Man. The pair almost succeeded, but when Black Widow was injured, Hawkeye retreated to get her to safety.[17] During this period, Romanova was attempting to defect from the Soviet Union and began falling in love with Hawkeye, weakening her loyalty to her country. When her employers learned the truth, the KGB had her gunned down, sending her to a hospital, convincing Hawkeye to go straight and seek membership in the Avengers.[18][19]\\r\\nThe Red Room kidnaps and brainwashes her again, and with the Swordsman and the first Power Man, she battles the Avengers.[20] She eventually breaks free from her psychological conditioning (with the help of Hawkeye), and does successfully defect, having further adventures with Spider-Man, with Hawkeye and with Daredevil.[volume?&?issue?needed] She ultimately joins the Avengers as a costumed heroine herself.[21]\\r\\nLater still, she begins freelancing as an agent of the international espionage group S.H.I.E.L.D. She is sent on a secret S.H.I.E.L.D. mission to China by Nick Fury. There, with the Avengers, she battles Col. Ling, Gen. Brushov, and her ex-husband the Red Guardian.[22] For a time, as writer Les Daniels noted in a contemporaneous study in 1971,\\r\\n...?her left-wing upbringing was put to better use, and she has lately taken to fighting realistic oppressor-of-the-people types. She helps young Puerto Ricans clean up police corruption and saves young hippies from organized crime.?... [The splash page of Amazing Adventures No. 3 (Nov. 1970)] reflects the recent trend toward involving fantastic characters in contemporary social problems, a move which has gained widespread publicity for Marvel and its competitor, DC.[23]\\r\\nDuring her romantic involvement with Matt Murdock in San Francisco, she operates as an independent superhero alongside Murdock's alter ego, Daredevil.[24] There she tries unsuccessfully to find a new career for herself as a fashion designer. Eventually, her relationship with Murdock stagnates, and after briefly working with Avengers finally breaks up with Murdock, fearing that playing \\"sidekick\\" is sublimating her identity.[25] During a HYDRA attempt to take over S.H.I.E.L.D., she is tortured to such an extent that she regresses back to an old cover identity of schoolteacher Nancy Rushman, but she is recovered by Spider-Man in time to help Nick Fury and Shang-Chi work out what had happened and restore her memory, with \\"Nancy\\" developing an attraction to Spider-Man before her memory is restored during the final fight against Madam Viper, Boomerang and the Silver Samurai.[26] She later returns to Matt Murdock's life to find he is romantically involved with another woman, Heather Glenn,[27] prompting her to leave New York.[28] Natasha ultimately realizes that Matt still only thinks of her in platonic terms, and elects to restrain herself from any advances.[29]\\r\\nAfter their breakup, the Widow moves to Los Angeles and becomes leader of the newly created and short-lived super team known as The Champions, consisting of her, Ghost Rider (Johnny Blaze), Hercules (with whom she has a brief romance), and former X-Men Angel and Iceman.[30]\\r\\nHer friends often call her \\"Natasha\\", the informal version of \\"Natalia\\". She has sometimes chosen the last-name alias \\"Romanoff\\"evidently as a private joke on those who are not aware that Russian family names use different endings for males and females. She has been hinted to be a descendant of the deposed House of Romanov and a relation to Nicholas II of Russia.\\r\\nNatasha crosses Daredevil's (Matt Murdock) path again when he attempts to slay an infant he believes to be the Anti-Christ. After Daredevil's one-time love, Karen Page, dies protecting the child, Natasha reconciles with Murdock, revealing she still loves him, but noting that he is too full of anger to commit to a relationship with her.[31]\\r\\nNatasha is challenged by Yelena Belova, a graduate from the training program through which Natasha herself was taught the espionage trade, who is the first to ever surpass Natasha's marks and considers herself the rightful successor to the \\"Black Widow\\" mantle.[32] Natasha refers to her as \\"little one\\" and \\"rooskaya (meaning \\"Russian\\"), and encourages her to discover her individuality rather than live in blind service, asking her \\"why be Black Widow, when you can be Yelena Belova?\\"[33] After several confrontations, Natasha subjects Yelena to intense psychological manipulation and suffering in order to teach her the reality of the espionage business, and an angry but disillusioned Yelena eventually returns home and temporarily quits being a spy. Although Matt Murdock is appalled by the cruelty of Natasha's treatment of Yelena, Nick Fury describes the action as Natasha's attempt at saving Yelena's life.[34] After bringing the Avengers and the Thunderbolts together to overcome Count Nefaria, Natasha supported Daredevil's short-lived efforts to form a new super-team to capture the Punisher, originally believed to be Nick Fury's murderer.[volume?&?issue?needed] Despite recruitment endeavors, however, this vigilante group folded shortly after she and her teammate Dagger fought an army of renegade S.H.I.E.L.D. androids; ironically, she soon afterward worked with both Daredevil and Punisher against the European crime syndicate managed by the Brothers Grace.[volume?&?issue?needed] Months later, her pursuit of war criminal Anatoly Krylenko led to a clash with Hawkeye, whose pessimism regarding heroic activities now rivaled her own.[volume?&?issue?needed]\\r\\nShortly after the Scarlet Witch's insanity seemingly killed Hawkeye, and again disbanded the Avengers, Natasha, weary of espionage and adventure, travelled to Arizona but was targeted. Natasha discovers that other women had been trained in the Black Widow Program, and all are now being hunted down and killed[volume?&?issue?needed] by the North Institute on behalf of the corporation Gynacon.[volume?&?issue?needed] Natasha's investigations led her back to Russia, where she was appalled to learn the previously unimagined extent of her past manipulation, and she discovered the Widows were being hunted because Gynacon, having purchased Russian biotechnology from Red Room's successor agency 2R, wanted all prior users of the technology dead. Natasha finds and kills the mastermind of the Black Widow murders: Ian McMasters, Gynacon's aging CEO, who intended to use part of their genetic structure to create a new chemical weapon.[volume?&?issue?needed] After killing McMasters, she clashed with operatives of multiple governments to help Sally Anne Carter, a girl Natasha had befriended in her investigations, whom she rescued with help from Daredevil and Yelena Belova.[volume?&?issue?needed] She soon returned the favor for Daredevil by reluctantly working with Elektra Natchios to protect his new wife, Milla Donovan, from the FBI and others, although Yelena proved beyond help when she agreed to be transformed into the new Super-Adaptoid by A.I.M. and HYDRA.[volume?&?issue?needed]\\r\\nDuring the Superhero Civil War, Natasha becomes a supporter of the Superhuman Registration Act and a member of the taskforce led by Iron Man.[35] Afterward, the registered Natasha joins the reconstituted Avengers.[volume?&?issue?needed] S.H.I.E.L.D. director Nick Fury is presumed killed,[volume?&?issue?needed] and deputy director Maria Hill incapacitated,[volume?&?issue?needed] so Natasha assumes temporary command of S.H.I.E.L.D. as the highest-ranking agent present.[volume?&?issue?needed]\\r\\nLater, Tony Stark assigns Natasha to convey the late Captain America's shield to a secure location, but is intercepted by her former lover, Bucky Barnes, the Winter Soldier, who steals the shield. Natasha and the Falcon then rescue Barnes from the Red Skull's minions, and bring him to the S.H.I.E.L.D. Helicarrier, where Stark convinces Bucky to become the new Captain America. Afterward, Natasha accompanies Bucky as his partner for a brief time until she is called back by S.H.I.E.L.D.[36] She later rejoins him and Falcon for the final confrontation with the Red Skull, helping to rescue Sharon Carter. She and Bucky have restarted their relationship.[37] She later plays an important role in the capture of Hercules. However, due to her respect of the Greek god, she let him go.[38] Soon Natasha, along with the rest of the Avengers, gets involved in the current Skrull invasion.[39] Afterwards, she stayed as Bucky's partner.[40] She also assists former director Maria Hill in delivering a special form of data to Bucky.[41]\\r\\nNorman Osborn discovered Yelena Belova breaking into an abandoned S.H.I.E.L.D. facility, and offered her the position of field leader of the new Thunderbolts. On her first mission, she and Ant-Man take control of Air Force One with the Goblin, Doc Samson, and the new President aboard.[42] It was suggested she faked her apparent death (as the Adaptoid) but it is never explained how.\\r\\nA conversation with the Ghost implies that Yelena is working as a mole for someone else and that she may even be someone else disguised as Yelena. She is later seen talking privately through a comm-link to Nick Fury.[43]\\r\\nOsborn orders Yelena to lead the current Thunderbolts to kill former Thunderbolt, Songbird. Fury orders \\"Yelena\\" to rescue and retrieve Songbird, for the information she might possess about Osborn and his operations. Yelena finds Songbird, and reveals to her that she was really Natasha Romanova in disguise.[44] She tries delivering Songbird to Fury, but the Thunderbolts have also followed them.[45] The trio are captured as Osborn reveals he had been impersonating Fury in messages all along to set Natasha up in order to strengthen the Thunderbolts and lead him to Fury. She and Songbird are brought to be executed but manage to escape when Ant-Man, Headsmen and Paladin turn on the rest of the Thunderbolts and let them go.[46]\\r\\nAt the start of the Heroic Age, Natasha is recruited by Steve Rogers into a new black-ops wing of the Avengers, dubbed the Secret Avengers. She travels to Dubai with her new teammate, Valkyrie, where they steal a dangerous artifact which the Beast then studies, noting that it seems like a distant cousin of the Serpent Crown.[47] In the story \\"Coppelia\\", she encounters a teenage clone of herself, code named \\"Tiny Dancer\\", whom she rescued from an arms dealer.[48]\\r\\nDuring the Fear Itself storyline, Black Widow and Peregrine are sent on a mission to free hostages being held in a Marseille cathedral by Rapido. He and a group of mercenaries are trying to exploit the panic over the events in Paris to steal France's nuclear arsenal.[49]\\r\\nDuring the Ends of the Earth storyline, involving one of Doctor Octopus' schemes, Natasha is one of only three heroes left standing after the Sinister Six defeat the Avengers,[50] joining Silver Sable and Spider-Man to track the Six (albeit because she was closest to Sable's cloaked ship after the Avengers were defeated rather than for her prowess).[51] She is later contacted by the Titanium Man to warn her and her allies about Doctor Octopus' attempt to rally other villains against Spider-Man.[52] She is knocked out along with Hawkeye by Iron Man during a battle against the Avengers when they were temporarily under Octavius' remote control.[53]\\r\\nDuring the incursion event between Earth-616 and Earth-1610, Natasha is involved in the final battle between the Marvel Universe's superheroes and the Ultimate Universe's Children of Tomorrow. She pilots a ship holding a handpicked few to restart humanity after the universe ends, copiloted by Jessica Drew. Her ship is shot down during the battle though, and she is killed in the ensuing explosion.[54]\\r\\nAs the evacuation of Earth-616 begins in light of the fact that Earth-1610 is about to come crashing down as part of the \\"Last Days\\" storyline, Black Widow is seen standing atop a building with Captain America who gives her a list of people to save and bring aboard the lifeboat. As she tells Sam she can't save them all, Sam explains it's Natasha's job to assist in the effort to save as many people as possible before Earth as they know it is destroyed. As she leaves, her mind transitions to Cold War Russia, where a young Natasha (here called Natalia) speaks with two Russian functionaries in the infamous \\"Red Room\\". She is given her first mission: travel to Cuba and locate a family called the Comienzas, who are at risk from Ra~l Castro's regime and who may have information of vital importance to Russia. She is told to rendezvous with another agent, her classmate Marina, and befriend the family under the guise of a Russian businesswoman. Natasha assures them of her competency and leaves. When one of the officers questions her youth, the other assures him, she's a killer. She will not disappoint. Natasha meets Marina in Cuba and the two friends catch up before meeting with the Comienzas that night at a local bar. Using her talent for deception, she casually and politely convinces the husband and wife that she's seeking inside information to help her import various goods into the country. The Comienzas explain they can't reveal said information, prompting Natasha to later explain to Marina that the family might need a little push. Not too soon she effectively began terrorizing the family into desperation. First, she plants an American flag on their doorstep to mimic someone accusing them of defecting to the United States. Later after meeting with one of the Russian officers from the Red Room to report her progress, she detonated a car bomb outside their home when the first attempt didn't make them \\"nearly desperate enough\\". Following the car bomb explosion, Natasha declares the family is indeed desperate enough to reproach for information. Before letting Natasha go, the officer announces she has one additional task before her mission is over: Marina has become too much enamored with her civilian guise, and is now a security risk. Natasha will have to eliminate her.[55] Flipping to the present, Black Widow is saving as many people as she can, but she quickly flashback to Havana. Natasha and her then Red Room partner Marina are trying to help a family defect. Natashas orders are simple. Kill the parents and make it public. When Natasha asks if she should kill the child too, her boss looks horrified that she would be so OK with that and tells her no. Having no problem following orders she sets up a meet and using a sniper rifle she takes out the pair without blinking. Next she shoots Marinas boyfriend then Marina herself. Next she shoots Marinas cat. Flipping back to the present, Black Widow is back saving people from the incursion as the reason that triggered Natasha's flashback is revealed....a man she saved is holding his cat. This dark, heartless side of the Black Widow shows why she is trying so hard to do good today.[56]\\r\\nDuring the Secret Empire storyline, Black Widow appears as a member of the underground resistance at the time when most of the United States has been taken over by Hydra and Captain America who was brainwashed by Red Skull's clone using the powers of the sentient Cosmic Cube Kobik into believing that he was a Hydra sleeper agent.[57] While Hawkeye assembles a strike force of Hercules and Quicksilver to find the Cosmic Cube fragments, Black Widow sets off to kill Rogers herself reasoning that even if Rick's theory is true, the man Rogers was would prefer to die than be used in this manner. She finds herself followed by the Champions as she establishes her version of the Red Room.[58] While preparing to shoot Captain America with a sniper rifle, she rushes to prevent Miles Morales from killing him as predicted by Ulysses,[59] and is struck by his shield, breaking her neck and killing her.[60] Despite the return of the real Steve Rogers and the downfall of Hydra, Natasha's death along with other casualties besides Rick Jones and Phil Coulson remains.[61] However, while observing a dictator who recently rose to power due to his support of Hydra, Bucky witnesses the man being assassinated in such a manner that he believes only Natasha could have pulled off the kill, and believes he sees the Black Widow depart from her chosen vantage point.[62]\\r\\nThe Black Widow is a world class athlete, gymnast, acrobat, aerialist capable of numerous complex maneuvers and feats, expert martial artist (including Karate, Judo, Jujutsu, Boxing, Aikido, Savate,[63] Ninjutsu,[64] various styles of Kung Fu and Kenpo, marksman and weapons specialist as well as having extensive espionage training.[65] She is also an accomplished ballerina.\\r\\nThe Black Widow has been enhanced by biotechnology that makes her body resistant to aging and disease and heals at an above human rate;[66] as well as psychological conditioning that suppresses her memory of true events as opposed to implanted ones of the past without the aid of specially designed system suppressant drugs.[volume?&?issue?needed]\\r\\nThe white blood cells in her body are efficient enough to fight off any microbe, foreign body and others from her body, keeping her healthy and immune to most, if not all infections, diseases and disorders.[volume?&?issue?needed]\\r\\nHer agility is greater than that of an Olympic gold medalist. She can coordinate her body with balance, flexibility, and dexterity easily.[67]\\r\\nRomanova has a gifted intellect.[68] She displays an uncanny affinity for psychological manipulation and can mask her real emotions perfectly. Like Steve Rogers, she possesses the ability to quickly process multiple information streams (such as threat assessment) and rapidly respond to changing tactical situations.[68]\\r\\nRomanova is an expert tactician. She is a very effective strategist, tactician, and field commander. She has led the Avengers and even S.H.I.E.L.D. on one occasion.[68]\\r\\nThe Black Widow uses a variety of equipment invented by Soviet scientists and technicians, with later improvements by S.H.I.E.L.D. scientists and technicians. She usually wears distinctively shaped bracelets which fire the Widow's Bite electro-static energy blasts that can deliver charges up to 30,000 volts, as well as \\"Widow's Line\\" grappling hooks, tear gas pellets, and a new element introduced during her ongoing series during the \\"Kiss or Kill\\" arc called the \\"Widow's Kiss\\"an aerosol instant knock-out gas she has modified.[68] She wears a belt of metallic discs; some are disc-charges containing plastic explosives, while others have been shown to be compartments for housing other equipment. Her costume consists of synthetic stretch fabric equipped with micro-suction cups on fingers and feet, enabling her to adhere to walls and ceilings. In the 2006 \\"Homecoming\\" mini-series, she was seen using knives, unarmed combat, and various firearms, but she has since begun using her bracelets again.[68] While in disguise as Yelena Belova, when infiltrating the then Osborn-sanctioned Thunderbolts during \\"Dark Reign\\", she used a specialized multi-lens goggle/head-carapace that demonstrated various technical abilities enhancing vision and communication.[volume?&?issue?needed] Later, she has used a modified gun based on her Widow's Bite wrist cartridge, during her adventures alongside the new Captain America.[65]\\r\\nThe Black Widow was ranked as the 176th greatest comic book character in Wizard magazine.[108] IGN also ranked her as the 74th greatest comic book character stating that wherever conspiracy and treachery are afoot, you can expect the Black Widow to appear to save the day,[109] and as #42 on their list of the \\"Top 50 Avengers\\".[110] She was ranked 31st in Comics Buyer's Guide's \\"100 Sexiest Women in Comics\\" list.[111]","input":"When did black widow first appear in mcu?"},{"output":"male is approximately 60%","context":"In physiology, body water is the water content of an animal body that is contained in the tissues, the blood, the bones and elsewhere. The percentages of body water contained in various fluid compartments add up to total body water (TBW). This water makes up a significant fraction of the human body, both by weight and by volume. Ensuring the right amount of body water is part of fluid balance, an aspect of homeostasis.\\r\\n\\r\\n\\r\\nBy weight, the average human adult male is approximately 60% water and the average adult female is approximately 50%. There can be considerable variation in body water percentage based on a number of factors like age, health, weight, and sex. In a large study of adults of all ages and both sexes, the adult human body averaged ~65% water. However, this varied substantially by age, sex, and adiposity (amount of fat in body composition). The figure for water fraction by weight in this sample was found to be 58 I8% water for males and 48 I6% for females.[1] The body water constitutes as much as 73% of the body weight of a newborn infant, whereas some obese people are as little as 45% water by weight.[2] This is due to how fat tissue does not retain water as well as lean tissue. These statistical averages will vary with factors such as type of population, age of people sampled, number of people sampled, and methodology. So there is not, and cannot be, a figure that is exactly the same for all people, for this or any other physiological measure.\\r\\nMost of animal body water is contained in various body fluids. These include intracellular fluid; extracellular fluid; plasma; interstitial fluid; and transcellular fluid.[3] Water is also contained inside organs, in gastrointestinal, cerebrospinal, peritoneal, and ocular fluids. Adipose tissue contains about 10% of water, while muscle tissue contains about 75%.[4][5]\\r\\nIn Netter's Atlas of Human Physiology, body water is broken down into the following compartments:[3]\\r\\nTotal body water can be determined using Flowing afterglow mass spectrometry measurement of deuterium abundance in breath samples from individuals. A known dose of deuterated water (Heavy water, D2O) is ingested and allowed to equilibrate within the body water. The FA-MS instrument then measures the deuterium-to-hydrogen (D:H) ratio in the exhaled breath water vapour. The total body water is then accurately measured from the increase in breath deuterium content in relation to the volume of D2O ingested.\\r\\nDifferent substances can be used to measure different fluid compartments:[8]\\r\\nIntracellular fluid may then be estimated by subtracting extracellular fluid from total body water.\\r\\nAnother method of determining total body water percentage (TBW%) is via Bioelectrical Impedance Analysis (BIA). In the traditional BIA method, a person lies on a cot and spot electrodes are placed on the hands and bare feet. Electrolyte gel is applied first, and then a weak current of frequency 50kHz is introduced. This AC waveform allows the creation of a current inside the body via the very capacitive skin without causing a DC flow or burns, and limited in the ~20mA range current for safety.[9]\\r\\nBIA has emerged as a promising technique because of its simplicity, low cost, high reproducibility and noninvasiveness. BIA prediction equations can be either generalized or population-specific, allowing this method to be potentially very accurate. Selecting the appropriate equation is important to determining the quality of the results.\\r\\nFor clinical purposes, scientists are developing a multi-frequency BIA method that may further improve the method's ability to predict a person's hydration level. New segmental BIA equipment that uses more electrodes may lead to more precise measurements of specific parts of the body.\\r\\nIn humans, total body water can be estimated based on the premorbid (or ideal) body weight and correction factor.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nT\\r\\nB\\r\\nW\\r\\n=\\r\\nw\\r\\ne\\r\\ni\\r\\ng\\r\\nh\\r\\nt\\r\\n?\\r\\nC\\r\\n\\r\\n\\r\\n{\\\\displaystyle TBW=weight*C}\\r\\n\\r\\n.\\r\\nC is a coefficient for the expected percentage of weight made up of free water. For adult, non-elderly males, C = 0.6. For adult elderly males, malnourished males, or females, C = 0.5. For adult elderly or malnourished females C = 0.45. A total body water deficit (TBWD) can then be approximated by the following formula:\\r\\n\\r\\n\\r\\nWhere [Na]t = target sodium concentration (usually 140 mEq/L), and [Na]m = measured sodium concentration.\\r\\nThe resultant value is the approximate volume of free water required to correct a hypernatremic state. In practice, the value rarely approximates the actual amount of free water required to correct a deficit due to insensible losses, urinary output, and differences in water distribution among patients. [10]\\r\\nWater in the animal body performs a number of functions: as a solvent for transportation of nutrients; as a medium for excretion; a means for heat control; as a lubricant for joints; and for shock absorption.[4]\\r\\nThe usual way of adding water to a body is by drinking. Water also enters the body with foods, especially those rich in water, such as plants, raw meat, and fish.\\r\\nThe amount of this water that is retained in animals is affected by several factors. Water amounts vary with the age of the animal. The older the vertebrate animal, the higher its relative bone mass and the lower its body water content.\\r\\nIn diseased states, where body water is affected, the fluid compartment or compartments that have changed can give clues to the nature of the problem, or problems. Body water is regulated by hormones, including anti-diuretic hormone, aldosterone and atrial natriuretic peptide.\\r\\nVolume contraction is a decrease in body fluid volume, with or without a concomitant loss of osmolytes. The loss of the body water component of body fluid is specifically termed dehydration.[11]\\r\\nSodium loss approximately correlates with fluid loss from extracellular fluid, since sodium has a much higher concentration in extracelluliar fluid (ECF) than intracellular fluid (ICF). In contrast, K+ has a much higher concentration in ICF than ECF, and therefore its loss rather correlates with fluid loss from ICF, since K+ loss from ECF causes the K+ in ICF to diffuse out of the cells, dragging water with it by osmosis.[citation needed]\\r\\nBody water: Intracellular fluid/Cytosol","input":"How many percentage of water in human body?"},{"output":"Aristotle","context":"\\r\\n\\r\\nLife on Earth:\\r\\n\\r\\nLife is a characteristic that distinguishes physical entities that have biological processes, such as signaling and self-sustaining processes, from those that do not, either because such functions have ceased (they have died), or because they never had such functions and are classified as inanimate. Various forms of life exist, such as plants, animals, fungi, protists, archaea, and bacteria. The criteria can at times be ambiguous and may or may not define viruses, viroids, or potential synthetic life as \\"living\\". Biology is the science concerned with the study of life.\\r\\n\\r\\nThe definition of life is controversial. The current definition is that organisms are open systems that maintain homeostasis, are composed of cells, have a life cycle, undergo metabolism, can grow, adapt to their environment, respond to stimuli, reproduce and evolve. However, several other biological definitions have been proposed, and there are some borderline cases of life, such as viruses or viroids. In the past, there have been many attempts to define what is meant by \\"life\\" through obsolete concepts such as odic force, hylomorphism, spontaneous generation and vitalism, that have now been disproved by biological discoveries. Abiogenesis describes the natural process of life arising from non-living matter, such as simple organic compounds. Properties common to all organisms include the need for certain core chemical elements to sustain biochemical functions.\\r\\n\\r\\nLife on Earth first appeared as early as 4.28 billion years ago, soon after ocean formation 4.41 billion years ago, and not long after the formation of the Earth 4.54 billion years ago.[1][2][3][4] Earth's current life may have descended from an RNA world, although RNA-based life may not have been the first. The mechanism by which life began on Earth is unknown, though many hypotheses have been formulated and are often based on the MillerÿUrey experiment. The earliest known life forms are microfossils of bacteria. 3.45 billion year old Australian rocks are reported to have contained microorganisms.[5][6] In 2016, scientists reported identifying a set of 355 genes thought to be present in the last universal common ancestor (LUCA) of all living organisms, already a complex organism and not the first living thing.[7]\\r\\n\\r\\nSince its primordial beginnings, life on Earth has changed its environment on a geologic time scale. To survive in most ecosystems, life must often adapt to a wide range of conditions. Some microorganisms, called extremophiles, thrive in physically or geochemically extreme environments that are detrimental to most other life on Earth. Aristotle was the first person to classify organisms. Later, Carl Linnaeus introduced his system of binomial nomenclature for the classification of species. Eventually new groups and categories of life were discovered, such as cells and microorganisms, forcing dramatic revisions of the structure of relationships between living organisms. The cell is considered the structural and functional unit of life.[8] There are two kinds of cells, prokaryotic and eukaryotic, both of which consist of cytoplasm enclosed within a membrane and contain many biomolecules such as proteins and nucleic acids. Cells reproduce through a process of cell division, in which the parent cell divides into two or more daughter cells.\\r\\n\\r\\nThough currently only known on Earth, life need not be restricted to it, and many scientists speculate in the existence of extraterrestrial life. Artificial life is a computer simulation or man-made reconstruction of any aspect of life, which is often used to examine systems related to natural life. Death is the permanent termination of all biological functions which sustain an organism, and as such, is the end of its life. Extinction is the process by which an entire group or taxon, normally a species, dies out. Fossils are the preserved remains or traces of organisms.\\r\\n\\r\\nThe definition of life has long been a challenge for scientists and philosophers, with many varied definitions put forward.[9][10][11] This is partially because life is a process, not a substance.[12][13][14] This is complicated by a lack of knowledge of the characteristics of living entities, if any, that may have developed outside of Earth.[15][16] Philosophical definitions of life have also been put forward, with similar difficulties on how to distinguish living things from the non-living.[17] Legal definitions of life have also been described and debated, though these generally focus on the decision to declare a human dead, and the legal ramifications of this decision.[18]\\r\\n\\r\\nSince there is no unequivocal definition of life, most current definitions in biology are descriptive. Life is considered a characteristic of something that preserves, furthers or reinforces its existence in the given environment. This characteristic exhibits all or most of the following traits:[11][19][20][21][22][23][24]\\r\\n\\r\\nThese complex processes, called physiological functions, have underlying physical and chemical bases, as well as signaling and control mechanisms that are essential to maintaining life.\\r\\n\\r\\nFrom a physics perspective, living beings are thermodynamic systems with an organized molecular structure that can reproduce itself and evolve as survival dictates.[25][26] Thermodynamically, life has been described as an open system which makes use of gradients in its surroundings to create imperfect copies of itself.[27] Hence, life is a self-sustained chemical system capable of undergoing Darwinian evolution.[28][29] A major strength of this definition is that it distinguishes life by the evolutionary process rather than its chemical composition.[30]\\r\\n\\r\\nOthers take a systemic viewpoint that does not necessarily depend on molecular chemistry. One systemic definition of life is that living things are self-organizing and autopoietic (self-producing). Variations of this definition include Stuart Kauffman's definition as an autonomous agent or a multi-agent system capable of reproducing itself or themselves, and of completing at least one thermodynamic work cycle.[31] This definition is extended by the apparition of novel functions over time.[32]\\r\\n\\r\\nWhether or not viruses should be considered as alive is controversial. They are most often considered as just replicators rather than forms of life.[33] They have been described as \\"organisms at the edge of life\\"[34] because they possess genes, evolve by natural selection,[35][36] and replicate by creating multiple copies of themselves through self-assembly. However, viruses do not metabolize and they require a host cell to make new products. Virus self-assembly within host cells has implications for the study of the origin of life, as it may support the hypothesis that life could have started as self-assembling organic molecules.[37][38][39]\\r\\n\\r\\nTo reflect the minimum phenomena required, other biological definitions of life have been proposed,[40] with many of these being based upon chemical systems. Biophysicists have commented that living things function on negative entropy.[41][42] In other words, living processes can be viewed as a delay of the spontaneous diffusion or dispersion of the internal energy of biological molecules towards more potential microstates.[9] In more detail, according to physicists such as John Bernal, Erwin Schr?dinger, Eugene Wigner, and John Avery, life is a member of the class of phenomena that are open or continuous systems able to decrease their internal entropy at the expense of substances or free energy taken in from the environment and subsequently rejected in a degraded form.[43][44]\\r\\n\\r\\nLiving systems are open self-organizing living things that interact with their environment. These systems are maintained by flows of information, energy, and matter.\\r\\n\\r\\nSome scientists have proposed in the last few decades that a general living systems theory is required to explain the nature of life.[45] Such a general theory would arise out of the ecological and biological sciences and attempt to map general principles for how all living systems work. Instead of examining phenomena by attempting to break things down into components, a general living systems theory explores phenomena in terms of dynamic patterns of the relationships of organisms with their environment.[46]\\r\\n\\r\\nThe idea that the Earth is alive is found in philosophy and religion, but the first scientific discussion of it was by the Scottish scientist James Hutton. In 1785, he stated that the Earth was a superorganism and that its proper study should be physiology. Hutton is considered the father of geology, but his idea of a living Earth was forgotten in the intense reductionism of the 19th century.[47]:10 The Gaia hypothesis, proposed in the 1960s by scientist James Lovelock,[48][49] suggests that life on Earth functions as a single organism that defines and maintains environmental conditions necessary for its survival.[47] This hypothesis served as one of the foundations of the modern Earth system science.\\r\\n\\r\\nThe first attempt at a general living systems theory for explaining the nature of life was in 1978, by American biologist James Grier Miller.[50] Robert Rosen (1991) built on this by defining a system component as \\"a unit of organization; a part with a function, i.e., a definite relation between part and whole.\\" From this and other starting concepts, he developed a \\"relational theory of systems\\" that attempts to explain the special properties of life. Specifically, he identified the \\"nonfractionability of components in an organism\\" as the fundamental difference between living systems and \\"biological machines.\\"[51]\\r\\n\\r\\nA systems view of life treats environmental fluxes and biological fluxes together as a \\"reciprocity of influence,\\"[52] and a reciprocal relation with environment is arguably as important for understanding life as it is for understanding ecosystems. As Harold J. Morowitz (1992) explains it, life is a property of an ecological system rather than a single organism or species.[53] He argues that an ecosystemic definition of life is preferable to a strictly biochemical or physical one. Robert Ulanowicz (2009) highlights mutualism as the key to understand the systemic, order-generating behavior of life and ecosystems.[54]\\r\\n\\r\\nComplex systems biology (CSB) is a field of science that studies the emergence of complexity in functional organisms from the viewpoint of dynamic systems theory.[55] The latter is also often called systems biology and aims to understand the most fundamental aspects of life. A closely related approach to CSB and systems biology called relational biology is concerned mainly with understanding life processes in terms of the most important relations, and categories of such relations among the essential functional components of organisms; for multicellular organisms, this has been defined as \\"categorical biology\\", or a model representation of organisms as a category theory of biological relations, as well as an algebraic topology of the functional organization of living organisms in terms of their dynamic, complex networks of metabolic, genetic, and epigenetic processes and signaling pathways.[56][57] Alternative but closely related approaches focus on the interdependance of constraints, where constraints can be either molecular, such as enzymes, or macroscopic, such as the geometry of a bone or of the vascular system.[58]\\r\\n\\r\\nIt has also been argued that the evolution of order in living systems and certain physical systems obeys a common fundamental principle termed the Darwinian dynamic.[59][60] The Darwinian dynamic was formulated by first considering how macroscopic order is generated in a simple non-biological system far from thermodynamic equilibrium, and then extending consideration to short, replicating RNA molecules. The underlying order-generating process was concluded to be basically similar for both types of systems.[59]\\r\\n\\r\\nAnother systemic definition called the operator theory proposes that \\"life is a general term for the presence of the typical closures found in organisms; the typical closures are a membrane and an autocatalytic set in the cell\\"[61] and that an organism is any system with an organisation that complies with an operator type that is at least as complex as the cell.[62][63][64][65] Life can also be modeled as a network of inferior negative feedbacks of regulatory mechanisms subordinated to a superior positive feedback formed by the potential of expansion and reproduction.[66]\\r\\n\\r\\nSome of the earliest theories of life were materialist, holding that all that exists is matter, and that life is merely a complex form or arrangement of matter. Empedocles (430 BC) argued that everything in the universe is made up of a combination of four eternal \\"elements\\" or \\"roots of all\\": earth, water, air, and fire. All change is explained by the arrangement and rearrangement of these four elements. The various forms of life are caused by an appropriate mixture of elements.[67]\\r\\n\\r\\nDemocritus (460 BC) thought that the essential characteristic of life is having a soul (psyche). Like other ancient writers, he was attempting to explain what makes something a living thing. His explanation was that fiery atoms make a soul in exactly the same way atoms and void account for any other thing. He elaborates on fire because of the apparent connection between life and heat, and because fire moves.[68]\\r\\n\\r\\nPlato's world of eternal and unchanging Forms, imperfectly represented in matter by a divine Artisan, contrasts sharply with the various mechanistic Weltanschauungen, of which atomism was, by the fourth century at least, the most prominent?... This debate persisted throughout the ancient world. Atomistic mechanism got a shot in the arm from Epicurus?... while the Stoics adopted a divine teleology?... The choice seems simple: either show how a structured, regular world could arise out of undirected processes, or inject intelligence into the system.[69]\\r\\nThe mechanistic materialism that originated in ancient Greece was revived and revised by the French philosopher Ren Descartes, who held that animals and humans were assemblages of parts that together functioned as a machine. In the 19th century, the advances in cell theory in biological science encouraged this view. The evolutionary theory of Charles Darwin (1859) is a mechanistic explanation for the origin of species by means of natural selection.[70]\\r\\n\\r\\nHylomorphism is a theory first expressed by the Greek philosopher Aristotle (322 BC). The application of hylomorphism to biology was important to Aristotle, and biology is extensively covered in his extant writings. In this view, everything in the material universe has both matter and form, and the form of a living thing is its soul (Greek psyche, Latin anima). There are three kinds of souls: the vegetative soul of plants, which causes them to grow and decay and nourish themselves, but does not cause motion and sensation; the animal soul, which causes animals to move and feel; and the rational soul, which is the source of consciousness and reasoning, which (Aristotle believed) is found only in man.[71] Each higher soul has all of the attributes of the lower ones. Aristotle believed that while matter can exist without form, form cannot exist without matter, and that therefore the soul cannot exist without the body.[72]\\r\\n\\r\\nThis account is consistent with teleological explanations of life, which account for phenomena in terms of purpose or goal-directedness. Thus, the whiteness of the polar bear's coat is explained by its purpose of camouflage. The direction of causality (from the future to the past) is in contradiction with the scientific evidence for natural selection, which explains the consequence in terms of a prior cause. Biological features are explained not by looking at future optimal results, but by looking at the past evolutionary history of a species, which led to the natural selection of the features in question.[73]\\r\\n\\r\\nSpontaneous generation was the belief that living organisms can form without descent from similar organisms. Typically, the idea was that certain forms such as fleas could arise from inanimate matter such as dust or the supposed seasonal generation of mice and insects from mud or garbage.[74]\\r\\n\\r\\nThe theory of spontaneous generation was proposed by Aristotle,[75] who compiled and expanded the work of prior natural philosophers and the various ancient explanations of the appearance of organisms; it held sway for two millennia. It was decisively dispelled by the experiments of Louis Pasteur in 1859, who expanded upon the investigations of predecessors such as Francesco Redi.[76][77] Disproof of the traditional ideas of spontaneous generation is no longer controversial among biologists.[78][79][80]\\r\\n\\r\\nVitalism is the belief that the life-principle is non-material. This originated with Georg Ernst Stahl (17th century), and remained popular until the middle of the 19th century. It appealed to philosophers such as Henri Bergson, Friedrich Nietzsche, and Wilhelm Dilthey,[81] anatomists like Marie Fran?ois Xavier Bichat, and chemists like Justus von Liebig.[82] Vitalism included the idea that there was a fundamental difference between organic and inorganic material, and the belief that organic material can only be derived from living things. This was disproved in 1828, when Friedrich W?hler prepared urea from inorganic materials.[83] This W?hler synthesis is considered the starting point of modern organic chemistry. It is of historical significance because for the first time an organic compound was produced in inorganic reactions.[82]\\r\\n\\r\\nDuring the 1850s, Hermann von Helmholtz, anticipated by Julius Robert von Mayer, demonstrated that no energy is lost in muscle movement, suggesting that there were no \\"vital forces\\" necessary to move a muscle.[84] These results led to the abandonment of scientific interest in vitalistic theories, although the belief lingered on in pseudoscientific theories such as homeopathy, which interprets diseases and sickness as caused by disturbances in a hypothetical vital force or life force.[85]\\r\\n\\r\\nThe age of the Earth is about 4.54 billion years.[86][87][88] Evidence suggests that life on Earth has existed for at least 3.5?billion years,[89][90][91][92][93][94][95][96][97] with the oldest physical traces of life dating back 3.7 billion years;[98][99][100] however, some theories, such as the Late Heavy Bombardment theory, suggest that life on Earth may have started even earlier, as early as 4.1ÿ4.4 billion years ago,[89][90][91][92][93] and the chemistry leading to life may have begun shortly after the Big Bang, 13.8 billion years ago, during an epoch when the universe was only 10ÿ17 million years old.[101][102][103]\\r\\n\\r\\nMore than 99% of all species of life forms, amounting to over five billion species,[104] that ever lived on Earth are estimated to be extinct.[105][106]\\r\\n\\r\\nAlthough the number of Earth's catalogued species of lifeforms is between 1.2 million and 2 million,[107][108] the total number of species in the planet is uncertain. Estimates range from 8 million to 100 million,[107][108] with a more narrow range between 10 and 14 million,[107] but it may be as high as 1 trillion (with only one-thousandth of one percent of the species described) according to studies realized in May 2016.[109][110] The total amount of related DNA base pairs on Earth is estimated at 5.0 x 1037 and weighs 50 billion tonnes.[111] In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon).[112] In July 2016, scientists reported identifying a set of 355 genes from the Last Universal Common Ancestor (LUCA) of all organisms living on Earth.[7]\\r\\n\\r\\nAll known life forms share fundamental molecular mechanisms, reflecting their common descent; based on these observations, hypotheses on the origin of life attempt to find a mechanism explaining the formation of a universal common ancestor, from simple organic molecules via pre-cellular life to protocells and metabolism. Models have been divided into \\"genes-first\\" and \\"metabolism-first\\" categories, but a recent trend is the emergence of hybrid models that combine both categories.[113]\\r\\n\\r\\nThere is no current scientific consensus as to how life originated. However, most accepted scientific models build on the MillerÿUrey experiment and the work of Sidney Fox, which show that conditions on the primitive Earth favored chemical reactions that synthesize amino acids and other organic compounds from inorganic precursors,[114] and phospholipids spontaneously form lipid bilayers, the basic structure of a cell membrane.\\r\\n\\r\\nLiving organisms synthesize proteins, which are polymers of amino acids using instructions encoded by deoxyribonucleic acid (DNA). Protein synthesis entails intermediary ribonucleic acid (RNA) polymers. One possibility for how life began is that genes originated first, followed by proteins;[115] the alternative being that proteins came first and then genes.[116]\\r\\n\\r\\nHowever, because genes and proteins are both required to produce the other, the problem of considering which came first is like that of the chicken or the egg. Most scientists have adopted the hypothesis that because of this, it is unlikely that genes and proteins arose independently.[117]\\r\\n\\r\\nTherefore, a possibility, first suggested by Francis Crick,[118] is that the first life was based on RNA,[117] which has the DNA-like properties of information storage and the catalytic properties of some proteins. This is called the RNA world hypothesis, and it is supported by the observation that many of the most critical components of cells (those that evolve the slowest) are composed mostly or entirely of RNA. Also, many critical cofactors (ATP, Acetyl-CoA, NADH, etc.) are either nucleotides or substances clearly related to them. The catalytic properties of RNA had not yet been demonstrated when the hypothesis was first proposed,[119] but they were confirmed by Thomas Cech in 1986.[120]\\r\\n\\r\\nOne issue with the RNA world hypothesis is that synthesis of RNA from simple inorganic precursors is more difficult than for other organic molecules. One reason for this is that RNA precursors are very stable and react with each other very slowly under ambient conditions, and it has also been proposed that living organisms consisted of other molecules before RNA.[121] However, the successful synthesis of certain RNA molecules under the conditions that existed prior to life on Earth has been achieved by adding alternative precursors in a specified order with the precursor phosphate present throughout the reaction.[122] This study makes the RNA world hypothesis more plausible.[123]\\r\\n\\r\\nGeological findings in 2013 showed that reactive phosphorus species (like phosphite) were in abundance in the ocean before 3.5 Ga, and that Schreibersite easily reacts with aqueous glycerol to generate phosphite and glycerol 3-phosphate.[124] It is hypothesized that Schreibersite-containing meteorites from the Late Heavy Bombardment could have provided early reduced phosphorus, which could react with prebiotic organic molecules to form phosphorylated biomolecules, like RNA.[124]\\r\\n\\r\\nIn 2009, experiments demonstrated Darwinian evolution of a two-component system of RNA enzymes (ribozymes) in vitro.[125] The work was performed in the laboratory of Gerald Joyce, who stated \\"This is the first example, outside of biology, of evolutionary adaptation in a molecular genetic system.\\"[126]\\r\\n\\r\\nPrebiotic compounds may have originated extraterrestrially. NASA findings in 2011, based on studies with meteorites found on Earth, suggest DNA and RNA components (adenine, guanine and related organic molecules) may be formed in outer space.[127][128][129][130]\\r\\n\\r\\nIn March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.[131]\\r\\n\\r\\nAccording to the panspermia hypothesis, microscopic lifedistributed by meteoroids, asteroids and other small Solar System bodiesmay exist throughout the universe.[132]\\r\\n\\r\\nThe diversity of life on Earth is a result of the dynamic interplay between genetic opportunity, metabolic capability, environmental challenges,[133] and symbiosis.[134][135][136] For most of its existence, Earth's habitable environment has been dominated by microorganisms and subjected to their metabolism and evolution. As a consequence of these microbial activities, the physical-chemical environment on Earth has been changing on a geologic time scale, thereby affecting the path of evolution of subsequent life.[133] For example, the release of molecular oxygen by cyanobacteria as a by-product of photosynthesis induced global changes in the Earth's environment. Because oxygen was toxic to most life on Earth at the time, this posed novel evolutionary challenges, and ultimately resulted in the formation of Earth's major animal and plant species. This interplay between organisms and their environment is an inherent feature of living systems.[133]\\r\\n\\r\\nThe biosphere is the global sum of all ecosystems. It can also be termed as the zone of life on Earth, a closed system (apart from solar and cosmic radiation and heat from the interior of the Earth), and largely self-regulating.[137] By the most general biophysiological definition, the biosphere is the global ecological system integrating all living beings and their relationships, including their interaction with the elements of the lithosphere, geosphere, hydrosphere, and atmosphere.\\r\\n\\r\\nLife forms live in every part of the Earth's biosphere, including soil, hot springs, inside rocks at least 19?km (12?mi) deep underground, the deepest parts of the ocean, and at least 64?km (40?mi) high in the atmosphere.[138][139][140] Under certain test conditions, life forms have been observed to thrive in the near-weightlessness of space[141][142] and to survive in the vacuum of outer space.[143][144] Life forms appear to thrive in the Mariana Trench, the deepest spot in the Earth's oceans.[145][146] Other researchers reported related studies that life forms thrive inside rocks up to 580?m (1,900?ft; 0.36?mi) below the sea floor under 2,590?m (8,500?ft; 1.61?mi) of ocean off the coast of the northwestern United States,[145][147] as well as 2,400?m (7,900?ft; 1.5?mi) beneath the seabed off Japan.[148] In August 2014, scientists confirmed the existence of life forms living 800?m (2,600?ft; 0.50?mi) below the ice of Antarctica.[149][150] According to one researcher, \\"You can find microbes everywhere  they're extremely adaptable to conditions, and survive wherever they are.\\"[145]\\r\\n\\r\\nThe biosphere is postulated to have evolved, beginning with a process of biopoesis (life created naturally from non-living matter, such as simple organic compounds) or biogenesis (life created from living matter), at least some 3.5 billion years ago.[151][152] The earliest evidence for life on Earth includes biogenic graphite found in 3.7 billion-year-old metasedimentary rocks from Western Greenland[98] and microbial mat fossils found in 3.48 billion-year-old sandstone from Western Australia.[99][100] More recently, in 2015, \\"remains of biotic life\\" were found in 4.1 billion-year-old rocks in Western Australia.[90][91] In 2017, putative fossilized microorganisms (or microfossils) were announced to have been discovered in hydrothermal vent precipitates in the Nuvvuagittuq Belt of Quebec, Canada that were as old as 4.28 billion years, the oldest record of life on earth, suggesting \\"an almost instantaneous emergence of life\\" after ocean formation 4.4 billion years ago, and not long after the formation of the Earth 4.54 billion years ago.[1][2][3][4] According to biologist Stephen Blair Hedges, \\"If life arose relatively quickly on Earth?... then it could be common in the universe.\\"[90]\\r\\n\\r\\nIn a general sense, biospheres are any closed, self-regulating systems containing ecosystems. This includes artificial biospheres such as Biosphere 2 and BIOS-3, and potentially ones on other planets or moons.[153]\\r\\n\\r\\nThe inert components of an ecosystem are the physical and chemical factors necessary for lifeenergy (sunlight or chemical energy), water, heat, atmosphere, gravity, nutrients, and ultraviolet solar radiation protection.[154] In most ecosystems, the conditions vary during the day and from one season to the next. To live in most ecosystems, then, organisms must be able to survive a range of conditions, called the \\"range of tolerance.\\"[155] Outside that are the \\"zones of physiological stress,\\" where the survival and reproduction are possible but not optimal. Beyond these zones are the \\"zones of intolerance,\\" where survival and reproduction of that organism is unlikely or impossible. Organisms that have a wide range of tolerance are more widely distributed than organisms with a narrow range of tolerance.[155]\\r\\n\\r\\nTo survive, selected microorganisms can assume forms that enable them to withstand freezing, complete desiccation, starvation, high levels of radiation exposure, and other physical or chemical challenges. These microorganisms may survive exposure to such conditions for weeks, months, years, or even centuries.[133] Extremophiles are microbial life forms that thrive outside the ranges where life is commonly found.[156] They excel at exploiting uncommon sources of energy. While all organisms are composed of nearly identical molecules, evolution has enabled such microbes to cope with this wide range of physical and chemical conditions. Characterization of the structure and metabolic diversity of microbial communities in such extreme environments is ongoing.[157]\\r\\n\\r\\nMicrobial life forms thrive even in the Mariana Trench, the deepest spot in the Earth's oceans.[145][146] Microbes also thrive inside rocks up to 1,900 feet (580?m) below the sea floor under 8,500 feet (2,600?m) of ocean.[145][147]\\r\\n\\r\\nInvestigation of the tenacity and versatility of life on Earth,[156] as well as an understanding of the molecular systems that some organisms utilize to survive such extremes, is important for the search for life beyond Earth.[133] For example, lichen could survive for a month in a simulated Martian environment.[158][159]\\r\\n\\r\\nAll life forms require certain core chemical elements needed for biochemical functioning. These include carbon, hydrogen, nitrogen, oxygen, phosphorus, and sulfurthe elemental macronutrients for all organisms[160]often represented by the acronym CHNOPS. Together these make up nucleic acids, proteins and lipids, the bulk of living matter. Five of these six elements comprise the chemical components of DNA, the exception being sulfur. The latter is a component of the amino acids cysteine and methionine. The most biologically abundant of these elements is carbon, which has the desirable attribute of forming multiple, stable covalent bonds. This allows carbon-based (organic) molecules to form an immense variety of chemical arrangements.[161] Alternative hypothetical types of biochemistry have been proposed that eliminate one or more of these elements, swap out an element for one not on the list, or change required chiralities or other chemical properties.[162][163]\\r\\n\\r\\nDeoxyribonucleic acid is a molecule that carries most of the genetic instructions used in the growth, development, functioning and reproduction of all known living organisms and many viruses. DNA and RNA are nucleic acids; alongside proteins and complex carbohydrates, they are one of the three major types of macromolecule that are essential for all known forms of life. Most DNA molecules consist of two biopolymer strands coiled around each other to form a double helix. The two DNA strands are known as polynucleotides since they are composed of simpler units called nucleotides.[164] Each nucleotide is composed of a nitrogen-containing nucleobaseeither cytosine (C), guanine (G), adenine (A), or thymine (T)as well as a sugar called deoxyribose and a phosphate group. The nucleotides are joined to one another in a chain by covalent bonds between the sugar of one nucleotide and the phosphate of the next, resulting in an alternating sugar-phosphate backbone. According to base pairing rules (A with T, and C with G), hydrogen bonds bind the nitrogenous bases of the two separate polynucleotide strands to make double-stranded DNA. The total amount of related DNA base pairs on Earth is estimated at 5.0 x 1037, and weighs 50 billion tonnes.[111] In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon).[112]\\r\\n\\r\\nDNA stores biological information. The DNA backbone is resistant to cleavage, and both strands of the double-stranded structure store the same biological information. Biological information is replicated as the two strands are separated. A significant portion of DNA (more than 98% for humans) is non-coding, meaning that these sections do not serve as patterns for protein sequences.\\r\\n\\r\\nThe two strands of DNA run in opposite directions to each other and are therefore anti-parallel. Attached to each sugar is one of four types of nucleobases (informally, bases). It is the sequence of these four nucleobases along the backbone that encodes biological information. Under the genetic code, RNA strands are translated to specify the sequence of amino acids within proteins. These RNA strands are initially created using DNA strands as a template in a process called transcription.\\r\\n\\r\\nWithin cells, DNA is organized into long structures called chromosomes. During cell division these chromosomes are duplicated in the process of DNA replication, providing each cell its own complete set of chromosomes. Eukaryotic organisms (animals, plants, fungi, and protists) store most of their DNA inside the cell nucleus and some of their DNA in organelles, such as mitochondria or chloroplasts.[165] In contrast, prokaryotes (bacteria and archaea) store their DNA only in the cytoplasm. Within the chromosomes, chromatin proteins such as histones compact and organize DNA. These compact structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.\\r\\n\\r\\nDNA was first isolated by Friedrich Miescher in 1869.[166] Its molecular structure was identified by James Watson and Francis Crick in 1953, whose model-building efforts were guided by X-ray diffraction data acquired by Rosalind Franklin.[167]\\r\\n\\r\\nLife is usually classified by eight levels of taxadomains, kingdoms, phyla, class, order, family, genus, and species. In May 2016, scientists reported that 1 trillion species are estimated to be on Earth currently with only one-thousandth of one percent described.[109]\\r\\n\\r\\nThe first known attempt to classify organisms was conducted by the Greek philosopher Aristotle (384ÿ322 BC), who classified all living organisms known at that time as either a plant or an animal, based mainly on their ability to move. He also distinguished animals with blood from animals without blood (or at least without red blood), which can be compared with the concepts of vertebrates and invertebrates respectively, and divided the blooded animals into five groups: viviparous quadrupeds (mammals), oviparous quadrupeds (reptiles and amphibians), birds, fishes and whales. The bloodless animals were also divided into five groups: cephalopods, crustaceans, insects (which included the spiders, scorpions, and centipedes, in addition to what we define as insects today), shelled animals (such as most molluscs and echinoderms), and \\"zoophytes\\" (animals that resemble plants). Though Aristotle's work in zoology was not without errors, it was the grandest biological synthesis of the time and remained the ultimate authority for many centuries after his death.[168]\\r\\n\\r\\nThe exploration of the Americas revealed large numbers of new plants and animals that needed descriptions and classification. In the latter part of the 16th century and the beginning of the 17th, careful study of animals commenced and was gradually extended until it formed a sufficient body of knowledge to serve as an anatomical basis for classification. In the late 1740s, Carl Linnaeus introduced his system of binomial nomenclature for the classification of species. Linnaeus attempted to improve the composition and reduce the length of the previously used many-worded names by abolishing unnecessary rhetoric, introducing new descriptive terms and precisely defining their meaning.[169]\\r\\n\\r\\nThe fungi were originally treated as plants. For a short period Linnaeus had classified them in the taxon Vermes in Animalia, but later placed them back in Plantae. Copeland classified the Fungi in his Protoctista, thus partially avoiding the problem but acknowledging their special status.[170] The problem was eventually solved by Whittaker, when he gave them their own kingdom in his five-kingdom system. Evolutionary history shows that the fungi are more closely related to animals than to plants.[171]\\r\\n\\r\\nAs new discoveries enabled detailed study of cells and microorganisms, new groups of life were revealed, and the fields of cell biology and microbiology were created. These new organisms were originally described separately in protozoa as animals and protophyta/thallophyta as plants, but were united by Haeckel in the kingdom Protista; later, the prokaryotes were split off in the kingdom Monera, which would eventually be divided into two separate groups, the Bacteria and the Archaea. This led to the six-kingdom system and eventually to the current three-domain system, which is based on evolutionary relationships.[172] However, the classification of eukaryotes, especially of protists, is still controversial.[173]\\r\\n\\r\\nAs microbiology, molecular biology and virology developed, non-cellular reproducing agents were discovered, such as viruses and viroids. Whether these are considered alive has been a matter of debate; viruses lack characteristics of life such as cell membranes, metabolism and the ability to grow or respond to their environments. Viruses can still be classed into \\"species\\" based on their biology and genetics, but many aspects of such a classification remain controversial.[174]\\r\\n\\r\\nIn the 1960s a trend called cladistics emerged, arranging taxa based on clades in an evolutionary or phylogenetic tree.[175]\\r\\n\\r\\nIn systems of scientific classification, Biota[183] is the superdomain that classifies all life.[184][185]\\r\\n\\r\\nCells are the basic unit of structure in every living thing, and all cells arise from pre-existing cells by division. Cell theory was formulated by Henri Dutrochet, Theodor Schwann, Rudolf Virchow and others during the early nineteenth century, and subsequently became widely accepted.[186] The activity of an organism depends on the total activity of its cells, with energy flow occurring within and between them.[187] Cells contain hereditary information that is carried forward as a genetic code during cell division.[188]\\r\\n\\r\\nThere are two primary types of cells. Prokaryotes lack a nucleus and other membrane-bound organelles, although they have circular DNA and ribosomes. Bacteria and Archaea are two domains of prokaryotes. The other primary type of cells are the eukaryotes, which have distinct nuclei bound by a nuclear membrane and membrane-bound organelles, including mitochondria, chloroplasts, lysosomes, rough and smooth endoplasmic reticulum, and vacuoles. In addition, they possess organized chromosomes that store genetic material. All species of large complex organisms are eukaryotes, including animals, plants and fungi, though most species of eukaryote are protist microorganisms.[189] The conventional model is that eukaryotes evolved from prokaryotes, with the main organelles of the eukaryotes forming through endosymbiosis between bacteria and the progenitor eukaryotic cell.[190]\\r\\n\\r\\nThe molecular mechanisms of cell biology are based on proteins. Most of these are synthesized by the ribosomes through an enzyme-catalyzed process called protein biosynthesis. A sequence of amino acids is assembled and joined together based upon gene expression of the cell's nucleic acid.[191] In eukaryotic cells, these proteins may then be transported and processed through the Golgi apparatus in preparation for dispatch to their destination.[192]\\r\\n\\r\\nCells reproduce through a process of cell division in which the parent cell divides into two or more daughter cells. For prokaryotes, cell division occurs through a process of fission in which the DNA is replicated, then the two copies are attached to parts of the cell membrane. In eukaryotes, a more complex process of mitosis is followed. However, the end result is the same; the resulting cell copies are identical to each other and to the original cell (except for mutations), and both are capable of further division following an interphase period.[193]\\r\\n\\r\\nMulticellular organisms may have first evolved through the formation of colonies of identical cells. These cells can form group organisms through cell adhesion. The individual members of a colony are capable of surviving on their own, whereas the members of a true multi-cellular organism have developed specializations, making them dependent on the remainder of the organism for survival. Such organisms are formed clonally or from a single germ cell that is capable of forming the various specialized cells that form the adult organism. This specialization allows multicellular organisms to exploit resources more efficiently than single cells.[194] In January 2016, scientists reported that, about 800 million years ago, a minor genetic change in a single molecule, called GK-PID, may have allowed organisms to go from a single cell organism to one of many cells.[195]\\r\\n\\r\\nCells have evolved methods to perceive and respond to their microenvironment, thereby enhancing their adaptability. Cell signaling coordinates cellular activities, and hence governs the basic functions of multicellular organisms. Signaling between cells can occur through direct cell contact using juxtacrine signalling, or indirectly through the exchange of agents as in the endocrine system. In more complex organisms, coordination of activities can occur through a dedicated nervous system.[196]\\r\\n\\r\\nThough life is confirmed only on Earth, many think that extraterrestrial life is not only plausible, but probable or inevitable.[197][198] Other planets and moons in the Solar System and other planetary systems are being examined for evidence of having once supported simple life, and projects such as SETI are trying to detect radio transmissions from possible alien civilizations. Other locations within the Solar System that may host microbial life include the subsurface of Mars, the upper atmosphere of Venus,[199] and subsurface oceans on some of the moons of the giant planets.[200][201]\\r\\nBeyond the Solar System, the region around another main-sequence star that could support Earth-like life on an Earth-like planet is known as the habitable zone. The inner and outer radii of this zone vary with the luminosity of the star, as does the time interval during which the zone survives. Stars more massive than the Sun have a larger habitable zone, but remain on the Sun-like \\"main sequence\\" of stellar evolution for a shorter time interval. Small red dwarfs have the opposite problem, with a smaller habitable zone that is subject to higher levels of magnetic activity and the effects of tidal locking from close orbits. Hence, stars in the intermediate mass range such as the Sun may have a greater likelihood for Earth-like life to develop.[202] The location of the star within a galaxy may also affect the likelihood of life forming. Stars in regions with a greater abundance of heavier elements that can form planets, in combination with a low rate of potentially habitat-damaging supernova events, are predicted to have a higher probability of hosting planets with complex life.[203] The variables of the Drake equation are used to discuss the conditions in planetary systems where civilization is most likely to exist.[204] Use of the equation to predict the amount of extraterrestrial life, however, is difficult; because many of the variables are unknown, the equation functions as more of a mirror to what its user already thinks. As a result, the number of civilizations in the galaxy can be estimated as low as 9.1 x 10?11 or as high as 156 million; for the calculations, see Drake equation.\\r\\n\\r\\nArtificial life is the simulation of any aspect of life, as through computers, robotics, or biochemistry.[205] The study of artificial life imitates traditional biology by recreating some aspects of biological phenomena. Scientists study the logic of living systems by creating artificial environmentsseeking to understand the complex information processing that defines such systems.[187] While life is, by definition, alive, artificial life is generally referred to as data confined to a digital environment and existence.\\r\\n\\r\\nSynthetic biology is a new area of biotechnology that combines science and biological engineering. The common goal is the design and construction of new biological functions and systems not found in nature. Synthetic biology includes the broad redefinition and expansion of biotechnology, with the ultimate goals of being able to design and build engineered biological systems that process information, manipulate chemicals, fabricate materials and structures, produce energy, provide food, and maintain and enhance human health and the environment.[206]\\r\\n\\r\\nDeath is the permanent termination of all vital functions or life processes in an organism or cell.[207][208] It can occur as a result of an accident, medical conditions, biological interaction, malnutrition, poisoning, senescence, or suicide. After death, the remains of an organism re-enter the biogeochemical cycle. Organisms may be consumed by a predator or a scavenger and leftover organic material may then be further decomposed by detritivores, organisms that recycle detritus, returning it to the environment for reuse in the food chain.\\r\\n\\r\\nOne of the challenges in defining death is in distinguishing it from life. Death would seem to refer to either the moment life ends, or when the state that follows life begins.[208] However, determining when death has occurred is difficult, as cessation of life functions is often not simultaneous across organ systems.[209] Such determination therefore requires drawing conceptual lines between life and death. This is problematic, however, because there is little consensus over how to define life. The nature of death has for millennia been a central concern of the world's religious traditions and of philosophical inquiry. Many religions maintain faith in either a kind of afterlife or reincarnation for the soul, or resurrection of the body at a later date.\\r\\n\\r\\nExtinction is the process by which a group of taxa or species dies out, reducing biodiversity.[210] The moment of extinction is generally considered the death of the last individual of that species. Because a species' potential range may be very large, determining this moment is difficult, and is usually done retrospectively after a period of apparent absence. Species become extinct when they are no longer able to survive in changing habitat or against superior competition. In Earth's history, over 99% of all the species that have ever lived are extinct;[211][104][105][106] however, mass extinctions may have accelerated evolution by providing opportunities for new groups of organisms to diversify.[212]\\r\\n\\r\\nFossils are the preserved remains or traces of animals, plants, and other organisms from the remote past. The totality of fossils, both discovered and undiscovered, and their placement in fossil-containing rock formations and sedimentary layers (strata) is known as the fossil record. A preserved specimen is called a fossil if it is older than the arbitrary date of 10,000 years ago.[213] Hence, fossils range in age from the youngest at the start of the Holocene Epoch to the oldest from the Archaean Eon, up to 3.4 billion years old.[214][215]","input":"Who was the first scientist who classified living things?"},{"output":"380 AD","context":"Nicene Christianity became the state church of the Roman Empire with the Edict of Thessalonica in 380 AD, when Emperor Theodosius I made it the Empire's sole authorized religion.[1][2] The Eastern Orthodox Church, Oriental Orthodoxy, and the Catholic Church each claim to be the historical continuation of this church in its original form, but do not identify with it in the caesaropapist form that it took later. Unlike Constantine I, who with the Edict of Milan of 313 AD had established tolerance for Christianity without placing it above other religions[3] and whose involvement in matters of the Christian faith extended to convoking councils of bishops who were to determine doctrine and to presiding at their meetings, but not to determining doctrine himself,[4] Theodosius established a single Christian doctrine (specified as that professed by Pope Damasus I of Rome and Pope Peter II of Alexandria) as the Empire's official religion.\\r\\nEarlier in the 4th century, following the Diocletianic Persecution of 303ÿ313 and the Donatist controversy that arose in consequence, Constantine had convened councils of Christian bishops to define the orthodoxy, or \\"correct teaching\\", of the Christian faith, expanding on earlier Christian councils. A series of ecumenical councils met during the 4th and 5th centuries, but Christianity continued to suffer rifts and schisms surrounding the issues of Arianism, Nestorianism, and Miaphysitism. In the 5th century the Western Empire decayed as a polity: invaders sacked Rome in 410 and in 455, and Odoacer, an Arian barbarian warlord, forced Romulus Augustus, the last nominal Western Emperor, to abdicate in 476. However, apart from the aforementioned schisms, the church as an institution persisted in communion, if not without tension, between the east and west. In the 6th century the Byzantine armies of the Eastern Roman Emperor Justinian I recovered Italy and other sections of the western Mediterranean shore. The Eastern Roman Empire soon lost most of these gains, but it held Rome, as part of the Exarchate of Ravenna, until 751, a period known in church history as the Byzantine Papacy. The Muslim conquests of the 7th century would begin a process of converting most of the then-Christian world in West Asia and North Africa to Islam, severely restricting the reach both of the Byzantine Empire and of its church. Missionary activity directed from Constantinople, the Byzantine capital, did not lead to a lasting expansion of the formal power of the Empire's state church, since areas outside the empire's political and military control set up their own distinct state churches, as in the case of Bulgaria in 919.\\r\\nJustinian I, who became emperor in Constantinople in 527, established the bishops of Rome, Constantinople, Alexandria, Antioch, and Jerusalem ÿ referred to[by whom?] as the Pentarchy ÿ as the leadership of the Imperial church and gave each bishop the title of \\"Patriarch\\". However, Justinian saw these bishops as under his tutelage: according to his arrangement, \\"the Emperor was the head of the Church in the sense that he had the right and duty of regulating by his laws the minutest details of worship and discipline, and also of dictating the theological opinions to be held in the Church\\".[5][6] However, by Justinian's day, the churches that now form Oriental Orthodoxy had already seceded from the Imperial state church, while in the west Christianity was mostly subject to the laws and customs of nations that owed no allegiance to the Emperor in Constantinople.[7] While eastern-born popes who were appointed or at least confirmed by the Eastern Emperor continued to be loyal to him as their political lord, they refused to accept his authority in religious matters,[8] or the authority of such a council as the imperially convoked Council of Hieria of 754. Pope Gregory III (731-741) became the last Bishop of Rome to ask the Byzantine ruler to ratify his election.[9][10] By then, the Roman Empire's state church as originally conceived had ceased to exist.[11] In the East, only the largest fragment of the Christian church was under the Emperor's control, and with the crowning of Charlemagne on 25 December 800 AD as Imperator Romanorum by the latter's ally, Pope Leo III, the de facto political split between east and west became irrevocable. Spiritually, the Chalcedonian Church, as a communion broader than the imperial state church, persisted as a unified entity, at least in theory, until the Great Schism and its formal division with the mutual excommunication in 1054 of Rome and Constantinople. Where the Emperor's power remained, the state church developed in a caesaropapist form,[12] although as the Byzantine Empire lost most of its territory to Islam, increasingly the members of the church lived outside the Byzantine state. The Eastern Roman Empire finally collapsed with the Fall of Constantinople to the Islamic Ottoman Turks in 1453.\\r\\nWestern missionary activities created a communion of churches that extended beyond the empire, the beginnings of which predated the establishment of the state church.[citation needed] The obliteration of the Empire's boundaries by Germanic peoples and an outburst of missionary activity among these peoples, who had no direct links with the Eastern Roman Empire, and among Pictic and Celtic peoples who had never been part of the Roman Empire, fostered the idea of a universal church free from association with a particular state.[13] On the contrary, \\"in the East Roman or Byzantine view, when the Roman Empire became Christian, the perfect world order willed by God had been achieved: one universal empire was sovereign, and coterminous with it was the one universal church\\"; and the state church came, by the time of the demise of the Byzantine Empire in 1453, to merge psychologically with it to the extent that its bishops had difficulty in thinking of Christianity without an emperor.[14][15]\\r\\nModern authors refer to this state church in a variety of ways: as the catholic church, the orthodox church, the imperial church, the imperial Roman church, or the Byzantine church, although some of these terms are also used for wider communions extending outside the Roman Empire.[16] The legacy of the idea of a universal church polity carries on, directly or indirectly, in today's Catholic Church and Eastern Orthodox Church, as well as in others, such as the Anglican Communion.\\r\\n\\r\\n\\r\\nBefore the end of the 1st century, the Roman authorities recognized Christianity as a separate religion from Judaism. The distinction, perhaps already made in practice at the time of the Great Fire of Rome in the year 64, was given official status by the emperor Nerva around the year 98 by granting Christians exemption from paying the Fiscus Iudaicus, the annual tax upon the Jews. Pliny the Younger, when propraetor in Bithynia in 103, assumes in his letters to Trajan that because Christians do not pay the tax, they are not Jews.[17][18][19]\\r\\nSince paying taxes had been one of the ways that Jews demonstrated their goodwill and loyalty toward the Empire, Christians had to negotiate their own alternatives to participating in the imperial cult. Their refusal to worship the Roman gods or to pay homage to the emperor as divine resulted at times in persecution and martyrdom.[17][18][19] Church Father Tertullian, for instance, attempted to argue that Christianity was not inherently treasonous, and that Christians could offer their own form of prayer for the well-being of the emperor.[20]\\r\\nChristianity spread especially in the eastern parts of the Empire and beyond its border; in the west it was at first relatively limited, but significant Christian communities emerged in Rome, Carthage, and other urban centers, becoming by the end of the 3rd century, the dominant faith in some of them. Christians accounted for approximately 10% of the Roman population by 300, according to some estimates.[21] According to Will Durant, the Christian Church prevailed over paganism because it offered a much more attractive doctrine and because the church leaders addressed human needs better than their rivals.[22]\\r\\nIn 301, the Kingdom of Armenia, which Rome considered de jure a client kingdom though yielded de facto to the Parthians (its ruling dynasty was of Parthian extraction),[23] became the first nation to adopt Christianity as its state church.\\r\\nIn 311, the dying Emperor Galerius ended the Diocletianic Persecution that he is reputed to have instigated, and in 313, Emperor Constantine issued the Edict of Milan, granting to Christians and others \\"the right of open and free observance of their worship\\".[28]\\r\\nConstantine began to utilize Christian symbols such as the Chi-Rho early in his reign but still encouraged traditional Roman religious practices including sun worship. In 330, Constantine established the city of Constantinople as the new capital of the Roman Empire. The city would gradually come to be seen as the intellectual and cultural center of the Christian world.[29]\\r\\nOver the course of the 4th century the Christian body became consumed by debates surrounding orthodoxy, i.e. which religious doctrines are the correct ones. In the early 4th century, a group in North Africa, later called Donatists, who believed in a very rigid interpretation of Christianity that excluded many who had abandoned the faith during the Diocletianic persecution, created a crisis in the western Empire.[30]\\r\\nA church synod, or council, was held in Rome in 313, followed by another in Arles in 314, the latter presided over by Constantine while still a junior emperor (see Tetrarchy). These synods ruled that the Donatist faith was heresy and, when the Donatists refused to recant, Constantine launched the first campaign of persecution by Christians against Christians, and began imperial involvement in Christian theology. However, during the reign of Emperor Julian the Apostate, the Donatists, who formed the majority party in the Roman province of Africa for 30 years,[31] were given official approval.[32]\\r\\nChristian scholars and populace within the Empire were increasingly embroiled in debates regarding christology (i.e., regarding the nature of the Christ). Opinions ranged from belief that Jesus was entirely human to belief that he was entirely divine. The most persistent debate was that between the homoousian, or Athanasian, view (the Father and the Son are one and the same, eternal), which was adopted at the council meeting that Constantine called at Nicaea in 325, and the homoiousian, or Arian, view (the Father and the Son are similar, but the Father is greater than the Son). Emperors thereby became ever more involved with the increasingly divided Church.[33]\\r\\nConstantine was of divided opinions (even as to being Christian), but he largely backed the Athanasian side, though he was baptized on his deathbed by the Arian bishop Eusebius of Nicomedia. His successor Constantius II supported a Semi-Arian position. Emperor Julian returned to the traditional (pagan) Roman/Greek religion, quickly quashed by his successor Jovian, a supporter of the Athanasian side.\\r\\nA Council of Rimini in 359 supported the Arian view. A Council of Constantinople in 360 supported a compromise (see Semi-Arianism). The Council of Constantinople in 381, called by Emperor Theodosius I reasserted the Nicene or Athanasian view and rejected the Arian view. This council further refined the definition of orthodoxy, issuing, according to tradition, the Niceno-Constantinopolitan Creed.\\r\\nOn 27 February of the previous year, Theodosius I established, with the Edict of Thessalonica, the Christianity of the First Council of Nicaea as the official state religion, reserving for its followers the title of Catholic Christians and declaring that those who did not follow the religion taught by Pope Damasus I of Rome and Pope Peter of Alexandria were to be called heretics:[34]\\r\\nIt is our desire that all the various nations which are subject to our Clemency and Moderation, should continue to profess that religion which was delivered to the Romans by the divine Apostle Peter, as it has been preserved by faithful tradition, and which is now professed by the Pontiff Damasus and by Peter, Bishop of Alexandria, a man of apostolic holiness. According to the apostolic teaching and the doctrine of the Gospel, let us believe in the one deity of the Father, the Son and the Holy Spirit, in equal majesty and in a holy Trinity. We authorize the followers of this law to assume the title of Catholic Christians; but as for the others, since, in our judgment they are foolish madmen, we decree that they shall be branded with the ignominious name of heretics, and shall not presume to give to their conventicles the name of churches. They will suffer in the first place the chastisement of the divine condemnation and in the second the punishment of our authority which in accordance with the will of Heaven we shall decide to inflict.\\r\\nIn 391, Theodosius closed all the \\"pagan\\" (non-Christian and non-Jewish) temples and formally forbade pagan worship.\\r\\nAt the end of the 4th century the Roman Empire had effectively split into two states although their economies (and the Church, which only then became a state church) were still strongly tied. The two halves of the Empire had always had cultural differences, exemplified in particular by the widespread use of the Greek language in the Eastern Empire and its more limited use in the West (Greek, as well as Latin, was used in the West, but Latin was the spoken vernacular).\\r\\nBy the time the state church of the Empire was established at the end of the 4th century, scholars in the West had largely abandoned Greek in favor of Latin. Even the Church in Rome, where Greek continued to be used in the liturgy longer than in the provinces, abandoned Greek.[35] Jerome's Vulgate had begun to replace the older Latin translations of the Bible.\\r\\nThe 5th century would see further fracturing of the state church of the Roman Empire. Emperor Theodosius?II called two synods in Ephesus, one in 431 and one in 449, the first of which condemned the teachings of Patriarch of Constantinople Nestorius, while the second supported the teachings of Eutyches against Archbishop Flavian of Constantinople.[36]\\r\\nNestorius taught that Christ's divine and human nature were distinct persons, and hence Mary was the mother of Christ but not the mother of God. Eutyches taught on the contrary that there was in Christ only a single nature, different from that of human beings in general. The First Council of Ephesus rejected Nestorius' view, causing churches centered around the School of Edessa, a city at the edge of the empire, to break with the imperial church (see Nestorian schism).[36]\\r\\nPersecuted within the Roman Empire, many Nestorians fled to Persia and joined the Sassanid Church (the future Church of the East). The Second Council of Ephesus upheld the view of Eutyches, but was overturned two years later by the Council of Chalcedon, called by Emperor Marcian. Rejection of the Council of Chalcedon led to the exodus from the state church of the majority of Christians in Egypt and many in the Levant, who preferred miaphysite theology.[36]\\r\\nThus, in addition to losing all the western empire, the state church suffered a significant diminishment even in the east within a century of its setting up. Those who upheld the Council of Chalcedon became known in Syriac as Melkites, the imperial church, followers of the emperor (in Syriac, malka).[37] This schism resulted in an independent communion of churches, including the Egyptian, Syrian, Ethiopian and Armenian churches, that is today known as Oriental Orthodoxy.[38] In spite of these schisms, however, the imperial church still represented the majority of Christians within the by now already diminished Roman Empire.[39]\\r\\nIn the 5th century, the Western Empire rapidly decayed and by the end of the century was no more. Within a few decades, Germanic tribes, particularly the Goths and Vandals, conquered the western provinces. Rome was sacked in 410 and 455, and was to be sacked again in the following century in 546.[27]\\r\\nBy 476 the Germanic chieftain Odoacer had conquered Italy and deposed the last western emperor, Romulus Augustus, though he nominally submitted to the authority of Constantinople. The Arian Germanic tribes established their own systems of churches and bishops in the western provinces but were generally tolerant of the population who chose to remain in communion with the imperial church.[27]\\r\\nIn 533 Roman Emperor Justinian in Constantinople launched a military campaign to reclaim the western provinces from the Arian Germans, starting with North Africa and proceeding to Italy. His success in recapturing much of the western Mediterranean was temporary. The empire soon lost most of these gains, but held Rome, as part of the Exarchate of Ravenna, until 751.\\r\\nJustinian definitively established Caesaropapism,[11] believing \\"he had the right and duty of regulating by his laws the minutest details of worship and discipline, and also of dictating the theological opinions to be held in the Church\\".[5] According to the entry in Liddell & Scott, the term orthodox first occurs in the Codex Justinianus: \\"We direct that all Catholic churches, throughout the entire world, shall be placed under the control of the orthodox bishops who have embraced the Nicene Creed.\\"[40]\\r\\nBy the end of the 6th century the Church within the Empire had become firmly tied with the imperial government,[41] while in the west Christianity was mostly subject to the laws and customs of nations that owed no allegiance to the emperor.[7]\\r\\nEmperor Justinian I assigned to five sees, those of Rome, Constantinople, Alexandria, Antioch and Jerusalem, a superior ecclesial authority that covered the whole of his empire. The First Council of Nicaea in 325 reaffirmed that the bishop of a provincial capital, the metropolitan bishop, had a certain authority over the bishops of the province.[42] But it also recognized the existing supra-metropolitan authority of the sees of Rome, Alexandria and Antioch,[43] and granted special recognition to Jerusalem.[44][45][46]\\r\\nConstantinople was added at the First Council of Constantinople (381)[47] and given authority initially only over Thrace. By a canon of contested validity,[48] the Council of Chalcedon (451) placed Asia and Pontus,[49] which together made up Anatolia, under Constantinople, although their autonomy had been recognized at the council of 381.[50][51]\\r\\nRome never recognized this pentarchy of five sees as constituting the leadership of the state church. It maintained that, in accordance with the First Council of Nicaea, only the three \\"Petrine\\" sees of Rome, Alexandria and Antioch had a real patriarchal function.[52] The canons of the Quinisext Council of 692, which gave ecclesiastical sanction to Justinian's decree, were also never fully accepted by the Western Church.[53]\\r\\nMuslim conquests of the territories of the patriarchates of Alexandria, Antioch and Jerusalem, most of whose Christians were in any case lost to the imperial state church since the aftermath of the Council of Chalcedon, left in effect only two patriarchates, those of Rome and Constantinople.[54] Then in 740, Emperor Leo the Isaurian reacted to papal resistance to his iconoclast policy by transferring from the jurisdiction of Rome to that of Constantinople all but a minute portion of the then existing empire.[55]\\r\\nThe Patriarch of Constantinople had already adopted the title of \\"ecumenical patriarch\\", indicating what he saw as his position in the oikoumene, the Christian world ideally headed by the emperor and the patriarch of the emperor's capital.[56][57] Also under the influence of the imperial model of governance of the state church, in which \\"the emperor becomes the actual executive organ of the universal Church\\",[58] the pentarchy model of governance of the state church regressed to a monarchy of the Patriarch of Constantinople.[58][59]\\r\\nThe Rashidun conquests began to expand the sway of Islam beyond Arabia in the 7th century, first clashing with the Roman Empire in 634. That empire and the Sassanid Persian Empire were at that time crippled by decades of war between them. By the late 8th century the Umayyad caliphate had conquered all of Persia and much of the Byzantine territory including Egypt, Palestine, and Syria.\\r\\nSuddenly much of the Christian world was under Muslim rule. Over the coming centuries the successive Muslim states became some of the most powerful in the Mediterranean world.\\r\\nThough the state church of the Roman Empire claimed religious authority over Christians in Egypt and the Levant, in reality the majority of Christians in these regions were by then miaphysites and members of other sects that had long been persecuted by Constantinople. The new Muslim rulers, in contrast, offered religious tolerance to Christians of all sects. Additionally subjects of the Muslim Empire could be accepted as Muslims simply by declaring a belief in a single deity and reverence for Muhammad (see shahada). As a result, the peoples of Egypt, Palestine and Syria largely accepted their new rulers and many declared themselves Muslims within a few generations. Muslim incursions later found success in parts of Europe, particularly Spain (see Al-Andalus).[60]\\r\\nDuring the 9th century, the Emperor in Constantinople encouraged missionary expeditions to nearby nations including the Muslim caliphate, and the Turkic Khazars.[citation needed] In 862 he sent Saints Cyril and Methodius to Slavic Great Moravia. By then most of the Slavic population of Bulgaria was Christian and Tsar Boris I himself was baptized in 864. Serbia was accounted Christian by about 870.[61] In early 867 Patriarch Photios I of Constantinople wrote that Christianity was accepted by the Kievan Rus', which however was definitively Christianized only at the close of the following century.\\r\\nOf these, the Church in Great Moravia chose immediately to link with Rome, not Constantinople: the missionaries sent there sided with the Pope during the Photian Schism (863ÿ867).[62] After decisive victories over the Byzantines at Acheloos and Katasyrtai, Bulgaria declared its Church autocephalous and elevated it to the rank of Patriarchate, an autonomy recognized in 927 by Constantinople,[63][64] but abolished by Emperor Basil II Bulgaroktonos (the Bulgar-Slayer) after his 1018 conquest of Bulgaria.\\r\\nIn Serbia, which became an independent kingdom in the early 13th century, Stephen Uro? IV Du?an, after conquering a large part of Byzantine territory in Europe and assuming the title of Tsar, raised the Serbian archbishop to the rank of patriarch in 1346, a rank maintained until after the fall of the Byzantine Empire to the Turks. No Byzantine emperor ever ruled Russian Christianity.\\r\\nExpansion of the Church in western and northern Europe began much earlier, with the conversion of the Irish in the 5th century, the Franks at the end of the same century, the Arian Visigoths in Spain soon afterwards, and the English at the end of the 6th century. By the time the Byzantine missions to central and eastern Europe began, Christian western Europe, in spite of losing most of Spain to Islam, encompassed Germany and part of Scandinavia, and, apart from the south of Italy, was independent of the Byzantine Empire and had been almost entirely so for centuries.\\r\\nThis situation fostered the idea of a universal church linked to no one particular state[13] and of which the state church of the Roman Empire was only part. Long before the Byzantine Empire came to an end, Poland also, Hungary and other central European peoples were part of a Church that in no way saw itself as the empire's state church and that, with the East-West Schism, had even ceased to be in communion with it.\\r\\nWith the defeat and death in 751 of the last Exarch of Ravenna and the end of the Exarchate, Rome ceased to be part of the Byzantine Empire. Forced to seek protection elsewhere,[65] the Popes turned to the Franks and, with the coronation of Charlemagne by Pope Leo III on 25 December 800, transferred their political allegiance to a rival Roman Emperor. More clearly than before, the church in the west, while remaining in communion with the state church of the Byzantine Empire, was not part of it. Disputes between the see of Rome, which claimed authority over all other sees, and that of Constantinople, which was now without rival in the empire, culminated perhaps inevitably[66] in mutual excommunications in 1054.\\r\\nCommunion with Constantinople was broken off by European Christians with the exception of those ruled by the empire (including the Bulgarians and Serbs) and of the fledgling Kievan or Russian Church, then a metropolitanate of the patriarchate of Constantinople. This church became independent only in 1448, just five years before the extinction of the empire,[67] after which the Turkish authorities included all their Orthodox Christian subjects of whatever ethnicity in a single millet headed by the Patriarch of Constantinople.\\r\\nThe Westerners who set up Crusader states in Greece and the Middle East appointed Latin (Western) patriarchs and other hierarchs, thus giving concrete reality and permanence to the schism.[68][69][70] Efforts were made in 1274 (Second Council of Lyon) and 1439 (Council of Florence) to restore communion between East and West, but the agreements reached by the participating eastern delegations and by the Emperor were rejected by the vast majority of Byzantine Christians.\\r\\nIn the East, the idea that the Byzantine emperor was the head of Christians everywhere persisted among churchmen as long as the empire existed, even when its actual territory was reduced to very little. In 1393, only 60 years before the fall of the capital, Patriarch Antony IV of Constantinople wrote to Basil I of Muscovy defending the liturgical commemoration in Russian churches of the Byzantine emperor on the grounds that he was \\"emperor (ϫ??) and autokrator of the Romans, that is of all Christians\\".[71] According to Patriarch Antony, \\"it is not possible among Christians to have a Church and not to have an emperor. For the empire and the Church have great unity and commonality, and it is not possible to separate them\\",[72][73][74] and \\"the holy emperor is not like the rulers and governors of other regions\\".[74][75]\\r\\nFollowing the schism between the Eastern and Western Churches, various emperors sought at times but without success to reunite the Church, invoking the notion of Christian unity between East and West in an attempt to obtain assistance from the Pope and Western Europe against the Muslims who were gradually conquering the empire's territory. But the period of the Western Crusades against the Muslims had passed before even the first of the two reunion councils was held.\\r\\nEven when persecuted by the emperor, the Eastern Church, George Pachymeres said, \\"counted the days until they should be rid not of their emperor (for they could no more live without an emperor than a body without a heart), but of their current misfortunes\\".[76] The state church had come to merge psychologically in the minds of the Eastern bishops with the empire to such an extent that they had difficulty in thinking of Christianity without an emperor.[14]\\r\\nIn Western Europe, on the other hand, the idea of a universal church linked to the Emperor of Constantinople was replaced by that in which the Roman see was supreme.[77] \\"Membership in a universal church replaced citizenship in a universal empire. Across Europe, from Italy to Ireland, a new society centered on Christianity was forming.\\"[78]\\r\\nThe Western Church came to emphasize the term Catholic in its identity, an assertion of universality, while the Eastern Church came to emphasize the term Orthodox in its identity, an assertion of holding to the true teachings of Jesus. Both churches claim to be the unique continuation of the previously united Chalcedonian Church, whose core doctrinal formulations have been retained also by many of the churches that emerged from the Protestant Reformation, including Lutheranism and Anglicanism.","input":"When did rome officially became a christian country?"},{"output":"January 1986","context":"Brain is the industry standard name for a computer virus that was released in its first form in January 1986,[1] and is considered to be the first computer virus for MS-DOS. It infects the boot sector of storage media formatted with the DOS File Allocation Table (FAT) file system. Brain was written by brothers, Basit Farooq Alvi and Amjad Farooq Alvi,[2] from Lahore, Pakistan.\\r\\n\\r\\nBrain affects the IBM PC computer by replacing the boot sector of a floppy disk with a copy of the virus. The real boot sector is moved to another sector and marked as bad. Infected disks usually have five kilobytes of bad sectors. The disk label is usually changed to ?Brain, and the following text can be seen in infected boot sectors:\\r\\n\\r\\nThere are many minor and major variations to that version of the text. The virus slows down the floppy disk drive and makes seven kilobytes of memory unavailable to DOS. Brain was written by Basit Farooq Alvi and Amjad Farooq Alvi, who at the time lived in Chah Miran, near Lahore Railway Station, in Lahore, Pakistan. The brothers told TIME magazine they had written it to protect their medical software from piracy, and it was supposed to target copyright infringement only.[3] The cryptic message \\"Welcome to the Dungeon\\", a safeguard and reference to an early programming forum on Dungeon BBS, appeared after a year because the brothers licensed a beta version of the code.  The brothers could not be contacted to receive the final release of this version of the program.\\r\\n\\r\\nBrain lacks code for dealing with hard disk partitioning, and avoids infecting hard disks by checking the most significant bit of the BIOS drive number being accessed. Brain does not infect the disk if the bit is clear, unlike other viruses at the time, which paid no attention to disk partitioning and consequently destroyed data stored on hard disks by treating them in the same way as floppy disks. Brain often went undetected, partially due to this deliberate non-destructiveness, especially when the user paid little to no attention to the slow speed of floppy disk access.\\r\\n\\r\\nThe virus came complete with the brothers' address and three phone numbers, and a message that told the user that their machine was infected and to call them for inoculation:\\r\\n\\r\\nThis program was originally used to track a heart monitoring program for the IBM PC, and pirates were distributing illicit copies of the disks. This tracking program was supposed to stop and track illegal copies of the disk, however the program also sometimes used the last 5k on an Apple floppy, making additional saves to the disk by other programs impossible.\\r\\n\\r\\nWhen the brothers began to receive a large number of phone calls from people in United Kingdom, the United States, and elsewhere, demanding that they disinfect their machines, they were stunned and tried to explain to the outraged callers that their motivation had not been malicious. Their phone lines were overloaded. The brothers, with another brother, Shahid Farooq Alvi, are still in business in Pakistan, as Brain NET Internet service providers with a company called Brain Telecommunication Limited.\\r\\n\\r\\nIn 2011, 25 years after Brain was released, Mikko Hypp?nen of F-Secure traveled to Pakistan to interview Basit and Amjad for a documentary.[4][5] Being inspired by this documentary and its wide spread, a group of Pakistani bloggers interviewed Basit and Amjad, under the banner of Bloggerine.[6]\\r\\n\\r\\nAshar is an older version of Brain. There are six variants, each with a different message.","input":"What is the name of first computer virus?"},{"output":"Laika","context":"Laika (Russian: ; c. 1954 ÿ 3 November 1957) was a Soviet space dog who became one of the first animals in space, and the first animal to orbit the Earth. Laika, a stray dog from the streets of Moscow, was selected to be the occupant of the Soviet spacecraft Sputnik 2 that was launched into outer space on 3 November 1957.\\r\\nLittle was known about the impact of spaceflight on living creatures at the time of Laika's mission, and the technology to de-orbit had not yet been developed, so Laika's survival was never expected. Some scientists believed humans would be unable to survive the launch or the conditions of outer space, so engineers viewed flights by animals as a necessary precursor to human missions.[1] The experiment aimed to prove that a living passenger could survive being launched into orbit and endure a Micro-g environment, paving the way for human spaceflight and providing scientists with some of the first data on how living organisms react to spaceflight environments.\\r\\nLaika died within hours from overheating, possibly caused by a failure of the central R-7 sustainer to separate from the payload. The true cause and time of her death were not made public until 2002; instead, it was widely reported that she died when her oxygen ran out on day six or, as the Soviet government initially claimed, she was euthanised prior to oxygen depletion.\\r\\nOn 11 April 2008, Russian officials unveiled a monument to Laika. A small monument in her honour was built near the military research facility in Moscow that prepared Laika's flight to space. It features a dog standing on top of a rocket. She also appears on the Monument to the Conquerors of Space in Moscow.\\r\\nAfter the success of Sputnik 1 in October 1957, Nikita Khrushchev, the Soviet leader, wanted a spacecraft launched on 7 November 1957, the 40th anniversary of the October Revolution. Construction had already started on a more sophisticated satellite, but it would not be ready until December; this satellite would later become Sputnik 3.[2]\\r\\nMeeting the November deadline meant building a new craft. Khrushchev specifically wanted his engineers to deliver a \\"space spectacular\\", a mission that would repeat the triumph of Sputnik 1, stunning the world with Soviet prowess. Planners settled on an orbital flight with a dog. Soviet rocket engineers had long intended a canine orbit before attempting human spaceflight; since 1951, they had lofted 12 dogs into sub-orbital space on ballistic flights, working gradually toward an orbital mission set for some time in 1958. To satisfy Khrushchev's demands, they expedited the orbital canine flight for the November launch.[3]\\r\\nAccording to Russian sources, the official decision to launch Sputnik 2 was made on 10 or 12 October, leaving less than four weeks to design and build the spacecraft.[4] Sputnik 2, therefore, was something of a rush job, with most elements of the spacecraft being constructed from rough sketches. Aside from the primary mission of sending a living passenger into space, Sputnik 2 also contained instrumentation for measuring solar irradiance and cosmic rays.[2]\\r\\nThe craft was equipped with a life-support system consisting of an oxygen generator and devices to avoid oxygen poisoning and to absorb carbon dioxide. A fan, designed to activate whenever the cabin temperature exceeded 15?C (59?F), was added to keep the dog cool. Enough food (in a gelatinous form) was provided for a seven-day flight, and the dog was fitted with a bag to collect waste. A harness was designed to be fitted to the dog, and there were chains to restrict her movements to standing, sitting, or lying down; there was no room to turn around in the cabin. An electrocardiogram monitored heart rate and further instrumentation tracked respiration rate, maximum arterial pressure, and the dog's movements.[5][6]\\r\\nLaika was found as a stray wandering the streets of Moscow. Soviet scientists chose to use Moscow strays since they assumed that such animals had already learned to endure conditions of extreme cold and hunger.[3] This specimen was an eleven-pound[7] mongrel female, approximately three years old. Another account reported that she weighed about 6?kg (13?lb). Soviet personnel gave her several names and nicknames, among them Kudryavka (Russian for Little Curly), Zhuchka (Little Bug), and Limonchik (Little Lemon). Laika, the Russian name for several breeds of dogs similar to the husky, was the name popularised around the world. The American press dubbed her Muttnik (mutt + suffix -nik) as a pun on Sputnik,[8] or referred to her as Curly.[9] Her true pedigree is unknown, although it is generally accepted that she was part husky or other Nordic breed, and possibly part terrier.[3] NASA refers to Laika as a \\"part-Samoyed terrier.\\"[10] A Russian magazine described her temperament as phlegmatic, saying that she did not quarrel with other dogs.[7] Vladimir Yazdovsky, who led the program of test dogs used on rockets, in a later publication wrote that Laika was quiet and charming.[11]\\r\\nThe Soviet Union and United States had previously sent animals only on sub-orbital flights.[12] Three dogs were trained for the Sputnik 2 flight: Albina, Mushka, and Laika.[13] Soviet space-life scientists Vladimir Yazdovsky and Oleg Gazenko trained the dogs.[14]\\r\\nTo adapt the dogs to the confines of the tiny cabin of Sputnik 2, they were kept in progressively smaller cages for periods of up to 20 days. The extensive close confinement caused them to stop urinating or defecating, made them restless, and caused their general condition to deteriorate. Laxatives did not improve their condition, and the researchers found that only long periods of training proved effective. The dogs were placed in centrifuges that simulated the acceleration of a rocket launch and were placed in machines that simulated the noises of the spacecraft. This caused their pulses to double and their blood pressure to increase by 30ÿ65 torr. The dogs were trained to eat a special high-nutrition gel that would be their food in space.[6]\\r\\nBefore the launch, one of the scientists took Laika home to play with his children. In a book chronicling the story of Soviet space medicine, Dr. Vladimir Yazdovsky wrote, \\"Laika was quiet and charming...I wanted to do something nice for her: She had so little time left to live.\\"[15]\\r\\nVladimir Yazdovsky made the final selection of dogs and their designated roles. Laika was to be the \\"flight dog\\"a sacrifice to science on a one-way mission to space.[16] Albina, who had already flown twice on a high-altitude test rocket, was to act as Laika's backup. The third dog Mushka was a \\"control dog\\"she was to stay on the ground and be used to test instrumentation and life support.[6][12]\\r\\nBefore leaving for the Baikonur Cosmodrome, Yazdovsky and Gazenko conducted surgery on the dogs, routing the cables from the transmitters to the sensors that would measure breathing, pulse, and blood pressure.[17]\\r\\nBecause the existing airstrip at Turatam near the cosmodrome was small, the dogs and crew had to be first flown aboard a Tu-104 plane to Tashkent. From there, a smaller and lighter Il-14 plane took them to Turatam. Training of dogs continued upon arrival; one after another they were placed in the capsules to get familiar with the feeding system.[16]\\r\\nAccording to a NASA document, Laika was placed in the capsule of the satellite on 31 October 1957three days before the start of the mission.[6] At that time of year, the temperatures at the launch site were extremely cold, and a hose connected to a heater was used to keep her container warm. Two assistants were assigned to keep a constant watch on Laika before launch. Just prior to liftoff on 3 November 1957, from Baikonur Cosmodrome, Laika's fur was sponged in a weak alcohol solution and carefully groomed, while iodine was painted onto the areas where sensors would be placed to monitor her bodily functions.[18]\\r\\nOne of the technicians preparing the capsule before final liftoff stated that \\"after placing Laika in the container and before closing the hatch, we kissed her nose and wished her bon voyage, knowing that she would not survive the flight.\\"[16]\\r\\nThe exact time of the liftoff varies from source to source and is mentioned as 05:30:42 Moscow Time or 07:22 Moscow Time.[16]\\r\\nAt peak acceleration Laika's respiration increased to between three and four times the pre-launch rate.[6] The sensors showed her heart rate was 103 beats/min before launch and increased to 240 beats/min during the early acceleration. After reaching orbit, Sputnik 2's nose cone was jettisoned successfully; however the \\"Block A\\" core did not separate as planned, preventing the thermal control system from operating correctly. Some of the thermal insulation tore loose, raising the cabin temperature to 40?C (104?F).[10] After three hours of weightlessness, Laika's pulse rate had settled back to 102 beats/min,[19] three times longer than it had taken during earlier ground tests, an indication of the stress she was under. The early telemetry indicated that Laika was agitated but eating her food.[10] After approximately five to seven hours into the flight, no further signs of life were received from the spacecraft.[6]\\r\\nThe Soviet scientists had planned to euthanise Laika with a poisoned serving of food. For many years, the Soviet Union gave conflicting statements that she had died either from asphyxia,[20] when the batteries failed, or that she had been euthanised. Many rumours circulated about the exact manner of her death. In 1999, several Russian sources reported that Laika had died when the cabin overheated on the fourth orbit.[4] In October 2002, Dimitri Malashenkov, one of the scientists behind the Sputnik 2 mission, revealed that Laika had died by the fourth circuit of flight from overheating. According to a paper he presented to the World Space Congress in Houston, TX, USA, \\"It turned out that it was practically impossible to create a reliable temperature control system in such limited time constraints.\\"[5]\\r\\nOver five months later, after 2,570 orbits, Sputnik 2including Laika's remainsdisintegrated during re-entry on 14 April 1958.[21]\\r\\nDue to the overshadowing issue of the Soviet vs. U.S. Space Race, the ethical issues raised by this experiment went largely unaddressed for some time. As newspaper clippings from 1957 show, the press was initially focused on reporting the political perspective, while the health and retrievalor lack thereofof Laika only became an issue later.[22]\\r\\nSputnik 2 was not designed to be retrievable, and Laika had always been intended to die.[4] The mission sparked a debate across the globe on the mistreatment of animals and animal testing in general to advance science.[14] In the United Kingdom, the National Canine Defence League called on all dog owners to observe a minute's silence, while the Royal Society for the Prevention of Cruelty to Animals (RSPCA) received protests even before Radio Moscow had finished announcing the launch. Animal rights groups at the time called on members of the public to protest at Soviet embassies.[23] Others demonstrated outside the United Nations in New York;[14] nevertheless, laboratory researchers in the U.S. offered some support for the Soviets, at least before the news of Laika's death.[14][24]\\r\\nIn the Soviet Union, there was less controversy. Neither the media, books in the following years, nor the public openly questioned the decision to send a dog into space. In 1998, after the collapse of the Soviet regime, Oleg Gazenko, one of the scientists responsible for sending Laika into space, expressed regret for allowing her to die:\\r\\nWork with animals is a source of suffering to all of us. We treat them like babies who cannot speak. The more time passes, the more I'm sorry about it. We shouldn't have done it?... We did not learn enough from this mission to justify the death of the dog.[21][22]\\r\\nIn other Warsaw Pact countries, open criticism of the Soviet space program was difficult because of political censorship, but there were notable cases of criticism in Polish scientific circles. A Polish scientific periodical, \\"Kto, Kiedy, Dlaczego\\" (\\"Who, When, Why\\"), published in 1958, discussed the mission of Sputnik 2. In the periodical's section dedicated to astronautics, Krzysztof Boru described the Sputnik 2 mission as \\"regrettable\\" and criticised not bringing Laika back to Earth alive as \\"undoubtedly a great loss for science\\".[25]\\r\\nLaika is memorialised in the form of a statue and plaque at Star City, Russia, the Russian Cosmonaut training facility.[26] Created in 1997, Laika is positioned behind the cosmonauts with her ears erect.[26] The Monument to the Conquerors of Space, constructed in 1964, also includes Laika.[27] On 11 April 2008 at the military research facility where staff had been responsible for readying Laika for the flight, officials unveiled a monument of her poised on top of a space rocket.[1] Stamps and envelopes picturing Laika were produced, as well as branded cigarettes and matches.[28]\\r\\nFuture space missions carrying dogs would be designed to be recovered. Four other dogs died in Soviet space missions: Bars and Lisichka were killed when their R-7 rocket exploded shortly after launch on 28 July 1960;[29] Pchyolka and Mushka died when Korabl-Sputnik 3 was purposely destroyed with an explosive charge to prevent foreign powers from inspecting the capsule after a wayward atmospheric reentry trajectory on 1 December 1960.[30]\\r\\nAlthough never shown, Laika is prominently mentioned in the 1985 film My Life as a Dog, in which the main character (a young Swedish boy in the late 1950s) identifies strongly with the dog.[31] Laika, a 2007 graphic novel by Nick Abadzis giving a fictionalized account of Laika's life, won the Eisner Award as \\"Best Publication for Teens\\".[32]","input":"When was the first dog sent to space?"},{"output":"380 AD","context":"Nicene Christianity became the state church of the Roman Empire with the Edict of Thessalonica in 380 AD, when Emperor Theodosius I made it the Empire's sole authorized religion.[1][2] The Eastern Orthodox Church, Oriental Orthodoxy, and the Catholic Church each claim to be the historical continuation of this church in its original form, but do not identify with it in the caesaropapist form that it took later. Unlike Constantine I, who with the Edict of Milan of 313 AD had established tolerance for Christianity without placing it above other religions[3] and whose involvement in matters of the Christian faith extended to convoking councils of bishops who were to determine doctrine and to presiding at their meetings, but not to determining doctrine himself,[4] Theodosius established a single Christian doctrine (specified as that professed by Pope Damasus I of Rome and Pope Peter II of Alexandria) as the Empire's official religion.\\r\\nEarlier in the 4th century, following the Diocletianic Persecution of 303ÿ313 and the Donatist controversy that arose in consequence, Constantine had convened councils of Christian bishops to define the orthodoxy, or \\"correct teaching\\", of the Christian faith, expanding on earlier Christian councils. A series of ecumenical councils met during the 4th and 5th centuries, but Christianity continued to suffer rifts and schisms surrounding the issues of Arianism, Nestorianism, and Miaphysitism. In the 5th century the Western Empire decayed as a polity: invaders sacked Rome in 410 and in 455, and Odoacer, an Arian barbarian warlord, forced Romulus Augustus, the last nominal Western Emperor, to abdicate in 476. However, apart from the aforementioned schisms, the church as an institution persisted in communion, if not without tension, between the east and west. In the 6th century the Byzantine armies of the Eastern Roman Emperor Justinian I recovered Italy and other sections of the western Mediterranean shore. The Eastern Roman Empire soon lost most of these gains, but it held Rome, as part of the Exarchate of Ravenna, until 751, a period known in church history as the Byzantine Papacy. The Muslim conquests of the 7th century would begin a process of converting most of the then-Christian world in West Asia and North Africa to Islam, severely restricting the reach both of the Byzantine Empire and of its church. Missionary activity directed from Constantinople, the Byzantine capital, did not lead to a lasting expansion of the formal power of the Empire's state church, since areas outside the empire's political and military control set up their own distinct state churches, as in the case of Bulgaria in 919.\\r\\nJustinian I, who became emperor in Constantinople in 527, established the bishops of Rome, Constantinople, Alexandria, Antioch, and Jerusalem ÿ referred to[by whom?] as the Pentarchy ÿ as the leadership of the Imperial church and gave each bishop the title of \\"Patriarch\\". However, Justinian saw these bishops as under his tutelage: according to his arrangement, \\"the Emperor was the head of the Church in the sense that he had the right and duty of regulating by his laws the minutest details of worship and discipline, and also of dictating the theological opinions to be held in the Church\\".[5][6] However, by Justinian's day, the churches that now form Oriental Orthodoxy had already seceded from the Imperial state church, while in the west Christianity was mostly subject to the laws and customs of nations that owed no allegiance to the Emperor in Constantinople.[7] While eastern-born popes who were appointed or at least confirmed by the Eastern Emperor continued to be loyal to him as their political lord, they refused to accept his authority in religious matters,[8] or the authority of such a council as the imperially convoked Council of Hieria of 754. Pope Gregory III (731-741) became the last Bishop of Rome to ask the Byzantine ruler to ratify his election.[9][10] By then, the Roman Empire's state church as originally conceived had ceased to exist.[11] In the East, only the largest fragment of the Christian church was under the Emperor's control, and with the crowning of Charlemagne on 25 December 800 AD as Imperator Romanorum by the latter's ally, Pope Leo III, the de facto political split between east and west became irrevocable. Spiritually, the Chalcedonian Church, as a communion broader than the imperial state church, persisted as a unified entity, at least in theory, until the Great Schism and its formal division with the mutual excommunication in 1054 of Rome and Constantinople. Where the Emperor's power remained, the state church developed in a caesaropapist form,[12] although as the Byzantine Empire lost most of its territory to Islam, increasingly the members of the church lived outside the Byzantine state. The Eastern Roman Empire finally collapsed with the Fall of Constantinople to the Islamic Ottoman Turks in 1453.\\r\\nWestern missionary activities created a communion of churches that extended beyond the empire, the beginnings of which predated the establishment of the state church.[citation needed] The obliteration of the Empire's boundaries by Germanic peoples and an outburst of missionary activity among these peoples, who had no direct links with the Eastern Roman Empire, and among Pictic and Celtic peoples who had never been part of the Roman Empire, fostered the idea of a universal church free from association with a particular state.[13] On the contrary, \\"in the East Roman or Byzantine view, when the Roman Empire became Christian, the perfect world order willed by God had been achieved: one universal empire was sovereign, and coterminous with it was the one universal church\\"; and the state church came, by the time of the demise of the Byzantine Empire in 1453, to merge psychologically with it to the extent that its bishops had difficulty in thinking of Christianity without an emperor.[14][15]\\r\\nModern authors refer to this state church in a variety of ways: as the catholic church, the orthodox church, the imperial church, the imperial Roman church, or the Byzantine church, although some of these terms are also used for wider communions extending outside the Roman Empire.[16] The legacy of the idea of a universal church polity carries on, directly or indirectly, in today's Catholic Church and Eastern Orthodox Church, as well as in others, such as the Anglican Communion.\\r\\n\\r\\n\\r\\nBefore the end of the 1st century, the Roman authorities recognized Christianity as a separate religion from Judaism. The distinction, perhaps already made in practice at the time of the Great Fire of Rome in the year 64, was given official status by the emperor Nerva around the year 98 by granting Christians exemption from paying the Fiscus Iudaicus, the annual tax upon the Jews. Pliny the Younger, when propraetor in Bithynia in 103, assumes in his letters to Trajan that because Christians do not pay the tax, they are not Jews.[17][18][19]\\r\\nSince paying taxes had been one of the ways that Jews demonstrated their goodwill and loyalty toward the Empire, Christians had to negotiate their own alternatives to participating in the imperial cult. Their refusal to worship the Roman gods or to pay homage to the emperor as divine resulted at times in persecution and martyrdom.[17][18][19] Church Father Tertullian, for instance, attempted to argue that Christianity was not inherently treasonous, and that Christians could offer their own form of prayer for the well-being of the emperor.[20]\\r\\nChristianity spread especially in the eastern parts of the Empire and beyond its border; in the west it was at first relatively limited, but significant Christian communities emerged in Rome, Carthage, and other urban centers, becoming by the end of the 3rd century, the dominant faith in some of them. Christians accounted for approximately 10% of the Roman population by 300, according to some estimates.[21] According to Will Durant, the Christian Church prevailed over paganism because it offered a much more attractive doctrine and because the church leaders addressed human needs better than their rivals.[22]\\r\\nIn 301, the Kingdom of Armenia, which Rome considered de jure a client kingdom though yielded de facto to the Parthians (its ruling dynasty was of Parthian extraction),[23] became the first nation to adopt Christianity as its state church.\\r\\nIn 311, the dying Emperor Galerius ended the Diocletianic Persecution that he is reputed to have instigated, and in 313, Emperor Constantine issued the Edict of Milan, granting to Christians and others \\"the right of open and free observance of their worship\\".[28]\\r\\nConstantine began to utilize Christian symbols such as the Chi-Rho early in his reign but still encouraged traditional Roman religious practices including sun worship. In 330, Constantine established the city of Constantinople as the new capital of the Roman Empire. The city would gradually come to be seen as the intellectual and cultural center of the Christian world.[29]\\r\\nOver the course of the 4th century the Christian body became consumed by debates surrounding orthodoxy, i.e. which religious doctrines are the correct ones. In the early 4th century, a group in North Africa, later called Donatists, who believed in a very rigid interpretation of Christianity that excluded many who had abandoned the faith during the Diocletianic persecution, created a crisis in the western Empire.[30]\\r\\nA church synod, or council, was held in Rome in 313, followed by another in Arles in 314, the latter presided over by Constantine while still a junior emperor (see Tetrarchy). These synods ruled that the Donatist faith was heresy and, when the Donatists refused to recant, Constantine launched the first campaign of persecution by Christians against Christians, and began imperial involvement in Christian theology. However, during the reign of Emperor Julian the Apostate, the Donatists, who formed the majority party in the Roman province of Africa for 30 years,[31] were given official approval.[32]\\r\\nChristian scholars and populace within the Empire were increasingly embroiled in debates regarding christology (i.e., regarding the nature of the Christ). Opinions ranged from belief that Jesus was entirely human to belief that he was entirely divine. The most persistent debate was that between the homoousian, or Athanasian, view (the Father and the Son are one and the same, eternal), which was adopted at the council meeting that Constantine called at Nicaea in 325, and the homoiousian, or Arian, view (the Father and the Son are similar, but the Father is greater than the Son). Emperors thereby became ever more involved with the increasingly divided Church.[33]\\r\\nConstantine was of divided opinions (even as to being Christian), but he largely backed the Athanasian side, though he was baptized on his deathbed by the Arian bishop Eusebius of Nicomedia. His successor Constantius II supported a Semi-Arian position. Emperor Julian returned to the traditional (pagan) Roman/Greek religion, quickly quashed by his successor Jovian, a supporter of the Athanasian side.\\r\\nA Council of Rimini in 359 supported the Arian view. A Council of Constantinople in 360 supported a compromise (see Semi-Arianism). The Council of Constantinople in 381, called by Emperor Theodosius I reasserted the Nicene or Athanasian view and rejected the Arian view. This council further refined the definition of orthodoxy, issuing, according to tradition, the Niceno-Constantinopolitan Creed.\\r\\nOn 27 February of the previous year, Theodosius I established, with the Edict of Thessalonica, the Christianity of the First Council of Nicaea as the official state religion, reserving for its followers the title of Catholic Christians and declaring that those who did not follow the religion taught by Pope Damasus I of Rome and Pope Peter of Alexandria were to be called heretics:[34]\\r\\nIt is our desire that all the various nations which are subject to our Clemency and Moderation, should continue to profess that religion which was delivered to the Romans by the divine Apostle Peter, as it has been preserved by faithful tradition, and which is now professed by the Pontiff Damasus and by Peter, Bishop of Alexandria, a man of apostolic holiness. According to the apostolic teaching and the doctrine of the Gospel, let us believe in the one deity of the Father, the Son and the Holy Spirit, in equal majesty and in a holy Trinity. We authorize the followers of this law to assume the title of Catholic Christians; but as for the others, since, in our judgment they are foolish madmen, we decree that they shall be branded with the ignominious name of heretics, and shall not presume to give to their conventicles the name of churches. They will suffer in the first place the chastisement of the divine condemnation and in the second the punishment of our authority which in accordance with the will of Heaven we shall decide to inflict.\\r\\nIn 391, Theodosius closed all the \\"pagan\\" (non-Christian and non-Jewish) temples and formally forbade pagan worship.\\r\\nAt the end of the 4th century the Roman Empire had effectively split into two states although their economies (and the Church, which only then became a state church) were still strongly tied. The two halves of the Empire had always had cultural differences, exemplified in particular by the widespread use of the Greek language in the Eastern Empire and its more limited use in the West (Greek, as well as Latin, was used in the West, but Latin was the spoken vernacular).\\r\\nBy the time the state church of the Empire was established at the end of the 4th century, scholars in the West had largely abandoned Greek in favor of Latin. Even the Church in Rome, where Greek continued to be used in the liturgy longer than in the provinces, abandoned Greek.[35] Jerome's Vulgate had begun to replace the older Latin translations of the Bible.\\r\\nThe 5th century would see further fracturing of the state church of the Roman Empire. Emperor Theodosius?II called two synods in Ephesus, one in 431 and one in 449, the first of which condemned the teachings of Patriarch of Constantinople Nestorius, while the second supported the teachings of Eutyches against Archbishop Flavian of Constantinople.[36]\\r\\nNestorius taught that Christ's divine and human nature were distinct persons, and hence Mary was the mother of Christ but not the mother of God. Eutyches taught on the contrary that there was in Christ only a single nature, different from that of human beings in general. The First Council of Ephesus rejected Nestorius' view, causing churches centered around the School of Edessa, a city at the edge of the empire, to break with the imperial church (see Nestorian schism).[36]\\r\\nPersecuted within the Roman Empire, many Nestorians fled to Persia and joined the Sassanid Church (the future Church of the East). The Second Council of Ephesus upheld the view of Eutyches, but was overturned two years later by the Council of Chalcedon, called by Emperor Marcian. Rejection of the Council of Chalcedon led to the exodus from the state church of the majority of Christians in Egypt and many in the Levant, who preferred miaphysite theology.[36]\\r\\nThus, in addition to losing all the western empire, the state church suffered a significant diminishment even in the east within a century of its setting up. Those who upheld the Council of Chalcedon became known in Syriac as Melkites, the imperial church, followers of the emperor (in Syriac, malka).[37] This schism resulted in an independent communion of churches, including the Egyptian, Syrian, Ethiopian and Armenian churches, that is today known as Oriental Orthodoxy.[38] In spite of these schisms, however, the imperial church still represented the majority of Christians within the by now already diminished Roman Empire.[39]\\r\\nIn the 5th century, the Western Empire rapidly decayed and by the end of the century was no more. Within a few decades, Germanic tribes, particularly the Goths and Vandals, conquered the western provinces. Rome was sacked in 410 and 455, and was to be sacked again in the following century in 546.[27]\\r\\nBy 476 the Germanic chieftain Odoacer had conquered Italy and deposed the last western emperor, Romulus Augustus, though he nominally submitted to the authority of Constantinople. The Arian Germanic tribes established their own systems of churches and bishops in the western provinces but were generally tolerant of the population who chose to remain in communion with the imperial church.[27]\\r\\nIn 533 Roman Emperor Justinian in Constantinople launched a military campaign to reclaim the western provinces from the Arian Germans, starting with North Africa and proceeding to Italy. His success in recapturing much of the western Mediterranean was temporary. The empire soon lost most of these gains, but held Rome, as part of the Exarchate of Ravenna, until 751.\\r\\nJustinian definitively established Caesaropapism,[11] believing \\"he had the right and duty of regulating by his laws the minutest details of worship and discipline, and also of dictating the theological opinions to be held in the Church\\".[5] According to the entry in Liddell & Scott, the term orthodox first occurs in the Codex Justinianus: \\"We direct that all Catholic churches, throughout the entire world, shall be placed under the control of the orthodox bishops who have embraced the Nicene Creed.\\"[40]\\r\\nBy the end of the 6th century the Church within the Empire had become firmly tied with the imperial government,[41] while in the west Christianity was mostly subject to the laws and customs of nations that owed no allegiance to the emperor.[7]\\r\\nEmperor Justinian I assigned to five sees, those of Rome, Constantinople, Alexandria, Antioch and Jerusalem, a superior ecclesial authority that covered the whole of his empire. The First Council of Nicaea in 325 reaffirmed that the bishop of a provincial capital, the metropolitan bishop, had a certain authority over the bishops of the province.[42] But it also recognized the existing supra-metropolitan authority of the sees of Rome, Alexandria and Antioch,[43] and granted special recognition to Jerusalem.[44][45][46]\\r\\nConstantinople was added at the First Council of Constantinople (381)[47] and given authority initially only over Thrace. By a canon of contested validity,[48] the Council of Chalcedon (451) placed Asia and Pontus,[49] which together made up Anatolia, under Constantinople, although their autonomy had been recognized at the council of 381.[50][51]\\r\\nRome never recognized this pentarchy of five sees as constituting the leadership of the state church. It maintained that, in accordance with the First Council of Nicaea, only the three \\"Petrine\\" sees of Rome, Alexandria and Antioch had a real patriarchal function.[52] The canons of the Quinisext Council of 692, which gave ecclesiastical sanction to Justinian's decree, were also never fully accepted by the Western Church.[53]\\r\\nMuslim conquests of the territories of the patriarchates of Alexandria, Antioch and Jerusalem, most of whose Christians were in any case lost to the imperial state church since the aftermath of the Council of Chalcedon, left in effect only two patriarchates, those of Rome and Constantinople.[54] Then in 740, Emperor Leo the Isaurian reacted to papal resistance to his iconoclast policy by transferring from the jurisdiction of Rome to that of Constantinople all but a minute portion of the then existing empire.[55]\\r\\nThe Patriarch of Constantinople had already adopted the title of \\"ecumenical patriarch\\", indicating what he saw as his position in the oikoumene, the Christian world ideally headed by the emperor and the patriarch of the emperor's capital.[56][57] Also under the influence of the imperial model of governance of the state church, in which \\"the emperor becomes the actual executive organ of the universal Church\\",[58] the pentarchy model of governance of the state church regressed to a monarchy of the Patriarch of Constantinople.[58][59]\\r\\nThe Rashidun conquests began to expand the sway of Islam beyond Arabia in the 7th century, first clashing with the Roman Empire in 634. That empire and the Sassanid Persian Empire were at that time crippled by decades of war between them. By the late 8th century the Umayyad caliphate had conquered all of Persia and much of the Byzantine territory including Egypt, Palestine, and Syria.\\r\\nSuddenly much of the Christian world was under Muslim rule. Over the coming centuries the successive Muslim states became some of the most powerful in the Mediterranean world.\\r\\nThough the state church of the Roman Empire claimed religious authority over Christians in Egypt and the Levant, in reality the majority of Christians in these regions were by then miaphysites and members of other sects that had long been persecuted by Constantinople. The new Muslim rulers, in contrast, offered religious tolerance to Christians of all sects. Additionally subjects of the Muslim Empire could be accepted as Muslims simply by declaring a belief in a single deity and reverence for Muhammad (see shahada). As a result, the peoples of Egypt, Palestine and Syria largely accepted their new rulers and many declared themselves Muslims within a few generations. Muslim incursions later found success in parts of Europe, particularly Spain (see Al-Andalus).[60]\\r\\nDuring the 9th century, the Emperor in Constantinople encouraged missionary expeditions to nearby nations including the Muslim caliphate, and the Turkic Khazars.[citation needed] In 862 he sent Saints Cyril and Methodius to Slavic Great Moravia. By then most of the Slavic population of Bulgaria was Christian and Tsar Boris I himself was baptized in 864. Serbia was accounted Christian by about 870.[61] In early 867 Patriarch Photios I of Constantinople wrote that Christianity was accepted by the Kievan Rus', which however was definitively Christianized only at the close of the following century.\\r\\nOf these, the Church in Great Moravia chose immediately to link with Rome, not Constantinople: the missionaries sent there sided with the Pope during the Photian Schism (863ÿ867).[62] After decisive victories over the Byzantines at Acheloos and Katasyrtai, Bulgaria declared its Church autocephalous and elevated it to the rank of Patriarchate, an autonomy recognized in 927 by Constantinople,[63][64] but abolished by Emperor Basil II Bulgaroktonos (the Bulgar-Slayer) after his 1018 conquest of Bulgaria.\\r\\nIn Serbia, which became an independent kingdom in the early 13th century, Stephen Uro? IV Du?an, after conquering a large part of Byzantine territory in Europe and assuming the title of Tsar, raised the Serbian archbishop to the rank of patriarch in 1346, a rank maintained until after the fall of the Byzantine Empire to the Turks. No Byzantine emperor ever ruled Russian Christianity.\\r\\nExpansion of the Church in western and northern Europe began much earlier, with the conversion of the Irish in the 5th century, the Franks at the end of the same century, the Arian Visigoths in Spain soon afterwards, and the English at the end of the 6th century. By the time the Byzantine missions to central and eastern Europe began, Christian western Europe, in spite of losing most of Spain to Islam, encompassed Germany and part of Scandinavia, and, apart from the south of Italy, was independent of the Byzantine Empire and had been almost entirely so for centuries.\\r\\nThis situation fostered the idea of a universal church linked to no one particular state[13] and of which the state church of the Roman Empire was only part. Long before the Byzantine Empire came to an end, Poland also, Hungary and other central European peoples were part of a Church that in no way saw itself as the empire's state church and that, with the East-West Schism, had even ceased to be in communion with it.\\r\\nWith the defeat and death in 751 of the last Exarch of Ravenna and the end of the Exarchate, Rome ceased to be part of the Byzantine Empire. Forced to seek protection elsewhere,[65] the Popes turned to the Franks and, with the coronation of Charlemagne by Pope Leo III on 25 December 800, transferred their political allegiance to a rival Roman Emperor. More clearly than before, the church in the west, while remaining in communion with the state church of the Byzantine Empire, was not part of it. Disputes between the see of Rome, which claimed authority over all other sees, and that of Constantinople, which was now without rival in the empire, culminated perhaps inevitably[66] in mutual excommunications in 1054.\\r\\nCommunion with Constantinople was broken off by European Christians with the exception of those ruled by the empire (including the Bulgarians and Serbs) and of the fledgling Kievan or Russian Church, then a metropolitanate of the patriarchate of Constantinople. This church became independent only in 1448, just five years before the extinction of the empire,[67] after which the Turkish authorities included all their Orthodox Christian subjects of whatever ethnicity in a single millet headed by the Patriarch of Constantinople.\\r\\nThe Westerners who set up Crusader states in Greece and the Middle East appointed Latin (Western) patriarchs and other hierarchs, thus giving concrete reality and permanence to the schism.[68][69][70] Efforts were made in 1274 (Second Council of Lyon) and 1439 (Council of Florence) to restore communion between East and West, but the agreements reached by the participating eastern delegations and by the Emperor were rejected by the vast majority of Byzantine Christians.\\r\\nIn the East, the idea that the Byzantine emperor was the head of Christians everywhere persisted among churchmen as long as the empire existed, even when its actual territory was reduced to very little. In 1393, only 60 years before the fall of the capital, Patriarch Antony IV of Constantinople wrote to Basil I of Muscovy defending the liturgical commemoration in Russian churches of the Byzantine emperor on the grounds that he was \\"emperor (ϫ??) and autokrator of the Romans, that is of all Christians\\".[71] According to Patriarch Antony, \\"it is not possible among Christians to have a Church and not to have an emperor. For the empire and the Church have great unity and commonality, and it is not possible to separate them\\",[72][73][74] and \\"the holy emperor is not like the rulers and governors of other regions\\".[74][75]\\r\\nFollowing the schism between the Eastern and Western Churches, various emperors sought at times but without success to reunite the Church, invoking the notion of Christian unity between East and West in an attempt to obtain assistance from the Pope and Western Europe against the Muslims who were gradually conquering the empire's territory. But the period of the Western Crusades against the Muslims had passed before even the first of the two reunion councils was held.\\r\\nEven when persecuted by the emperor, the Eastern Church, George Pachymeres said, \\"counted the days until they should be rid not of their emperor (for they could no more live without an emperor than a body without a heart), but of their current misfortunes\\".[76] The state church had come to merge psychologically in the minds of the Eastern bishops with the empire to such an extent that they had difficulty in thinking of Christianity without an emperor.[14]\\r\\nIn Western Europe, on the other hand, the idea of a universal church linked to the Emperor of Constantinople was replaced by that in which the Roman see was supreme.[77] \\"Membership in a universal church replaced citizenship in a universal empire. Across Europe, from Italy to Ireland, a new society centered on Christianity was forming.\\"[78]\\r\\nThe Western Church came to emphasize the term Catholic in its identity, an assertion of universality, while the Eastern Church came to emphasize the term Orthodox in its identity, an assertion of holding to the true teachings of Jesus. Both churches claim to be the unique continuation of the previously united Chalcedonian Church, whose core doctrinal formulations have been retained also by many of the churches that emerged from the Protestant Reformation, including Lutheranism and Anglicanism.","input":"When was christianity became the official religion of rome?"},{"output":"Manoj Bhargava","context":"Manoj Bhargava is an Indian American businessman and philanthropist. He is the founder and CEO of Innovations Ventures LLC (dba Living Essentials LLC), the company known for producing the 5-hour Energy drink.[3][4] By 2012 the brand had grown to do an estimated $1 billion in sales.[5] In 2015, Bhargava pledged 99% of his net worth to improving the well-being of the world's less fortunate.[6]\\r\\n\\r\\n\\r\\nBhargava was born in Lucknow (in the state of Uttar Pradesh), India in 1953,[1][7] and in 1967, moved with his family to Philadelphia, Pennsylvania, United States.[2][8] Bhargava's father attended the Wharton School of Business in pursuit of a doctorate degree,[1][7] while his son won a math scholarship to an \\"elite private academy\\" called The Hill School.[7] After high school graduation, Bhargava attended Princeton University for one year in 1972.[2][9]\\r\\nAfter college, Bhargava returned to India and spent the next 12 years traveling to and from a group of communal-like monasteries owned by the Hanslok Ashram.[2][8] During this period, Bhargava moved back and forth between the US and India and worked a variety of middle-class office and construction jobs.[10]\\r\\nBhargava returned to the US and joined his parents' PVC manufacturing company. In 1990, he purchased a company that produced parts for outdoor furniture.[9] He sold Prime PVC Inc. in 2006.[9][11] A subsequent company, Chemicalpartners.com, specialized in inventions and new ideas for business.[9]\\r\\nBhargava created Innovations Ventures LLC (dba Living Essentials LLC),[9] and launched 5-hour Energy in 2003.[1] By 2012, retail sales had grown to an estimated $1 billion.[1] Over time, Bhargava created additional entities or funds to support a variety of new ventures.[9] These included the capital venture company MicroDose Life Sciences,[12] a manufacturing venture laboratory called Stage 2 Innovations LLC,[13] a private equity fund called ETC Capital LLC, Plymouth Real Estate Holdings LLC[9][10] and Oakland Energy and Water Ventures.[14] In 2014, he financed a New York City-based film distribution company, Bleecker Street.[15]\\r\\nBhargava was interviewed on the ABC News show Nightline in September 2012.[16] That year, an article in Forbes magazine said Bhargava and his company, Innovations Ventures, had participated in up to 90 court cases[17] against competitors, suppliers[7] and associates[9] since 2003.[18] As of 2012, fourteen of those cases had been settled or dismissed.[18]\\r\\nIn 2013, Forbes reported Bhargava's net worth to be $1.5 billion, but he was dropped from its list of billionaires in 2014.[2] Bhargava's 2015 documentary, Billions in Change, reports he has a net worth of over $4 billion,[19] while some news articles report the $4 billion figure to be unverified.[13][20]\\r\\nBhargava is a member of the Giving Pledge campaign.[21] In 2015, he pledged to give 99% of his wealth to philanthropic causes.[19] His foundations include the Hans Foundation[1][22] and Rural India Supporting Trust.[23] In 2016, Bhargava told National Geographic that he planned to distribute 10,000 of his stationary, power generating bikes to rural homes and villages in India.[13]","input":"Who is the founder of 5 hour energy?"},{"output":"a traveling worker","context":"A hobo is a migrant worker or homeless vagabond, especially one who is impoverished. The term originated in the Westernprobably NorthwesternUnited States around 1890.[1] Unlike a \\"tramp\\", who works only when forced to, and a \\"bum\\", who does not work at all, a \\"hobo\\" is a traveling worker.\\r\\n\\r\\n\\r\\nThe origin of the term is unknown. According to etymologist Anatoly Liberman, the only certain detail about its origin is the word was first noticed in American English circa 1890.[1] Liberman points out that many folk etymologies fail to answer the question: \\"Why did the word become widely known in California (just there) by the early Nineties (just then)?\\"[1] Author Todd DePastino has suggested it may be derived from the term hoe-boy meaning \\"farmhand\\", or a greeting such as Ho, boy![2] Bill Bryson suggests in Made in America (1998) that it could either come from the railroad greeting, \\"Ho, beau!\\" or a syllabic abbreviation of \\"homeward bound\\".[3] It could also come from the words \\"homeless boy\\". H. L. Mencken, in his The American Language (4th ed., 1937), wrote:\\r\\nTramps and hobos are commonly lumped together, but see themselves as sharply differentiated. A hobo or bo is simply a migrant laborer; he may take some longish holidays, but sooner or later he returns to work. Lower than either is the bum, who neither works nor travels, save when impelled to motion by the police.[4][5]\\r\\nIt is unclear exactly when hobos first appeared on the American railroading scene. With the end of the American Civil War in the 1860s, many discharged veterans returning home began hopping freight trains. Others looking for work on the American frontier followed the railways west aboard freight trains in the late 19th century.\\r\\nIn 1906, Professor Layal Shafee, after an exhaustive study, put the number of tramps in the United States at about 500,000 (about 0.6% of the US population at the time). His article \\"What Tramps Cost Nation\\" was published by The New York Telegraph in 1911, when he estimated the number had surged to 700,000.[6]\\r\\nThe number of hobos increased greatly during the Great Depression era of the 1930s.[7] With no work and no prospects at home, many decided to travel for free by freight train and try their luck elsewhere.\\r\\nLife as a hobo was dangerous. In addition to the problems of being itinerant, poor, and far from home and support, plus the hostility of many train crews, they faced the railroads' security staff, nicknamed \\"bulls\\", who had a reputation of violence against trespassers.[citation needed] Moreover, riding on a freight train is dangerous in itself. British poet W.H. Davies, author of The Autobiography of a Super-Tramp, lost a foot when he fell under the wheels when trying to jump aboard a train. It was easy to be trapped between cars, and one could freeze to death in bad weather. When freezer cars were loaded at an ice factory, any hobo inside was likely to be killed.[8]\\r\\nAccording to Ted Conover in Rolling Nowhere (1984), at some unknown point in time, as many as 20,000 people were living a hobo life in North America. Modern freight trains are much faster and thus harder to ride than in the 1930s, but they can still be boarded in railyards.[9]\\r\\nMany hobo terms have become part of common language, such as \\"big House\\", \\"glad rags\\", \\"main drag\\", and others.\\r\\nTo cope with the uncertainties of hobo life, hobos developed a system of symbols, or a visual code. Hobos would write this code with chalk or coal to provide directions, information, and warnings to others in \\"the brotherhood\\". A symbol would indicate \\"turn right here\\", \\"beware of hostile railroad police\\", \\"dangerous dog\\", \\"food available here\\", and so on. Some commonly used signs:\\r\\nAnother version of the hobo code exists as a display in the Steamtown National Historic Site at Scranton, Pennsylvania, operated by the National Park Service. There is an exhibit of hobo codes at the National Cryptologic Museum in Annapolis Junction, Maryland.[13][14]\\r\\nThe Free Art and Technology Lab released a QR Hobo Code, with a QR stenciler, in July 2011.[15]\\r\\nAn ethical code was created by Tourist Union #63 during its 1889 National Hobo Convention in St. Louis Missouri.[16] This code was voted upon as a concrete set of laws to govern the Nationwide Hobo Body; it reads this way:\\r\\nThere are numerous hobo conventions throughout the United States each year. The ephemeral ways of hobo conventions are mostly dependent on the resources of their hosts. Some conventions are part of railroad conventions or \\"railroad days\\". Others are quasi-private affairs, hosted by long-time hobos. Still others are ad hocthat is they are held surreptitiously on private land. Some of these conventions are held in abandoned quarries, along major rivers.[citation needed]\\r\\nMost non-mainstream conventions are held at current or historical railroad stops. The most notable is the National Hobo Convention held in Britt, Iowa.[citation needed] The town first hosted the Convention in 1900, but there followed a hiatus of thirty-three years. Since 1934 the Convention has been held annually in Britt, on the second weekend in August.[17]\\r\\nThe Britt Hobo Museum exhibits a smattering of hobo history and lore. Initially just a \\"Hobo Convention\\" museum, in the late 1990s it evolved into a fuller Hobo History museum. LeAnn Castillo, a local artist and the hobo painter, exhibits her portrait collection of hobo kings and queens since 1900. All of her paintings are made from photos.[citation needed]\\r\\nFormal entertainment at the annual Convention begins before dusk, and is provided by a mix of active hobos, extended hobo families and non-hobo wannabees. Late after dark, the crowd leaves and the campfire becomes more informal. Satellite groups spring up. Stories are toldsmall and tall, poetry is recited, and cants are sung to the muted vibrations of banjos, guitars and harmonicas.[citation needed]\\r\\nActivities officially begin the Thursday of the convention weekend with a lighting of the campfire and exercise of some hobo cultural traditions (Honoring the Four Winds) before the opening entertainment. On Friday morning many visit the hobo-corner of the cemetery to pay tribute to those who have \\"Caught the Westbound,\\" with a hobo memorial service preceded by a local contingent of ex-military colorguard. Names of deceased hobos are recited (Roll Call). At around five o'clock on Friday afternoon a poetry reading attracts participants and a small crowd of onlookers.[citation needed]\\r\\nHobo-king candidates are screened the days before the annual King and Queen election and coronation. They are expected to have knowledge and experience in riding trains, and are evaluated for how well they would represent the hobo community. A quasi-qualified candidate is occasionally allowed to run. Any woman who is part of the hobo community may run for hobo Queen.[citation needed] On the Saturday morning there is a parade in the town pavilion, allowing onlookers to see those running for hobo king and queen in a last chance to campaign before the election in the early afternoon. Following the parade, mulligan stew is served to hundreds of people in the city park, cooked by local Boy Scouts. In early afternoon, the hobo King and Queen are elected by means of the volume of crowd applause.[citation needed]\\r\\nA carnival, flea market, and an annual auto show are also part of the festivities. There is also stock-car racing.[citation needed]\\r\\nExamples of characters based on hobos include:\\r\\nMusicians known for hobo songs include: Baby Gramps, Railroad Earth, Ramblin' Jack Elliott, Utah Phillips, Jimmie Rodgers, Seasick Steve, and Boxcar Willie.\\r\\nExamples of hobo songs include:","input":"What did hobos do during the great depression?"},{"output":"end of the 1970s","context":"Solar-powered calculators are hand-held electronic calculators powered by solar cells mounted on the device.[1] They were introduced at the end of the 1970s.[2]\\r\\nAmorphous silicon has been used as a photovoltaic solar cell material for devices which require very little power, such as pocket calculators, because their lower performance compared to conventional crystalline silicon solar cells is more than offset by their lower cost and simplified deposition onto a substrate. The first solar powered calculators available in the late 1970s included the Royal Solar 1, the Sharp EL-8026, and the Teal Photon.\\r\\nSolar calculators use liquid crystal displays as they are power efficient and capable of operating in the low voltage range of 1.5ÿ2?V. Some models also use a light pipe to converge light onto the solar cells.[3] However, solar calculators may not work well in indoor conditions under ambient lighting if sufficient light is not available.[4][5][6]\\r\\nAnylite Technology is the name of a solar technology used by Texas Instruments since the 1980s in some calculators. They are intended to be able to function with less light than other solar calculators. This was essentially achieved by using relatively large photovoltaic solar cells.[7] The use of Anylite technology in modern TI calculators is denoted by a lower case \\"a\\" at the end of the model number (e.g. TI-30a). In older models, such as the TI-36 Solar, Anylite Solar is printed on the calculator.[8]\\r\\nAs of the 2010s, some very cheap calculators include a \\"dummy\\" solar panel, implying that they are solar-powered, when they are actually powered only by battery.[9]","input":"When was the first solar powered calculator invented?"},{"output":"Tobacco","context":"The Colony of Virginia, chartered in 1606 and settled in 1607, was the first enduring English colony in North America, following failed proprietary attempts at settlement on Newfoundland by Sir Humphrey Gilbert[2] in 1583, and the subsequent further south Roanoke Island (modern eastern North Carolina) by Sir Walter Raleigh in the late 1580s.\\r\\nThe founder of the new colony was the Virginia Company,[3] with the first two settlements in Jamestown on the north bank of the James River and Popham Colony on the Kennebec River in modern-day Maine, both in 1607. The Popham colony quickly failed due to a famine, disease, and conflict with local Native American tribes in the first two years. Jamestown occupied land belonging to the Powhatan Confederacy, and was also at the brink of failure before the arrival of a new group of settlers and supplies by ship in 1610. Tobacco became Virginia's first profitable export, the production of which had a significant impact on the society and settlement patterns.\\r\\nIn 1624, the Virginia Company's charter was revoked by King James I, and the Virginia colony was transferred to royal authority as a crown colony. After the English Civil War in the 1640s and 50s, the Virginia colony was nicknamed \\"The Old Dominion\\" by King Charles II for its perceived loyalty to the English monarchy during the era of the Protectorate and Commonwealth of England.[4].\\r\\nFrom 1619 to 1775/1776, the colonial legislature of Virginia was the House of Burgesses, which governed in conjunction with a colonial governor. Jamestown on the James River remained the capital of the Virginia colony until 1699; from 1699 until its dissolution the capital was in Williamsburg. The colony experienced its first major political turmoil with Bacon's Rebellion of 1676.\\r\\nAfter declaring independence from the Kingdom of Great Britain in 1775, before the Declaration of Independence was officially adopted, the Virginia colony became the Commonwealth of Virginia, one of the original thirteen states of the United States, adopting as its official slogan \\"The Old Dominion\\". The entire modern states of West Virginia, Kentucky, Indiana and Illinois, and portions of Ohio and Western Pennsylvania were later created from the territory encompassed, or claimed by, the colony of Virginia at the time of further American independence in July 1776.\\r\\n\\r\\n\\r\\nThe name \\"Virginia\\" is the oldest designation for English claims in North America. In 1584, Sir Walter Raleigh sent Philip Amadas and Arthur Barlowe to explore what is now the North Carolina coast, and they returned with word of a regional king (weroance) named Wingina, who ruled a land supposedly called Wingandacoa.\\r\\nThe name Virginia for a region in North America may have been originally suggested by Sir Walter Raleigh, named for Queen Elizabeth I, in approximately 1584.[5] In addition the term Wingandacoa may have influenced the name Virginia.\\"[6][7] On his next voyage, Raleigh learned that while the chief of the Secotans was indeed called Wingina, the expression wingandacoa heard by the English upon arrival actually meant \\"What good clothes you wear!\\" in Carolina Algonquian, and was not the name of the country as previously misunderstood.[8] \\"Virginia\\" was originally a term used to refer to North America's entire eastern coast from the 34th parallel (close to Cape Fear) north to 48th parallel. This area included a large section of Canada and the shores of Acadia.[citation needed]\\r\\nThe colony was also known as the Virginia Colony, the Province of Virginia, and occasionally as the Dominion and Colony of Virginia or His Majesty's Most Ancient Colloney and Dominion of Virginia[9][10]\\r\\nIn gratitude for the loyalty of Virginians to the crown during the English Civil War, Charles II gave it the title of \\"Old Dominion\\".[citation needed] The colony seal stated from Latin, 'Behold, Virginia gives the fifth\\", with Virginia claimed as the fifth English dominion after England, France, Scotland and Ireland.\\r\\nThe state of Virginia maintains \\"Old Dominion\\" as its state nickname. The athletic teams of the University of Virginia are known as the \\"Cavaliers,\\" referring to supporters of Charles II, and Virginia has another state public university called \\"Old Dominion University\\".\\r\\nAlthough Spain, France, Sweden, and the Netherlands all had competing claims to the region, none of these prevented the English from becoming the first European power to colonize successfully the Mid-Atlantic coastline. Earlier attempts had been made by the Spanish in what is now Georgia (San Miguel de Gualdape, 1526ÿ27; several Spanish missions in Georgia between 1568 and 1684), South Carolina (Santa Elena, 1566ÿ87), North Carolina (Joara, 1567ÿ68) and Virginia (Ajacn Mission, 1570ÿ71); and by the French in South Carolina (Charlesfort, 1562ÿ63). Farther south, the Spanish colony of Spanish Florida, centered on St. Augustine, was established in 1565, while to the north, the French were establishing settlements in what is now Canada (Charlesbourg-Royal briefly occupied 1541ÿ43; Port Royal, established in 1605).\\r\\nIn 1585, Sir Walter Raleigh sent his first colonisation mission to the island of Roanoke (in present-day North Carolina), with over 100 male setters. However, when Sir Francis Drake arrived at the colony in summer 1586, the colonists opted to return to England, due to lack of supply ships, abandoning the colony. Supply ships arrived at the now-abandoned colony later in 1586; 15 soldiers were left behind to hold the island, however no trace of these men was later found.[11]\\r\\nIn 1587, Raleigh sent another group to again attempt to establish a permanent settlement. The expedition leader, John White, returned to England for supplies that same year but was unable to return to the colony due to war between England and Spain. When he finally did return in 1590, he found the colony abandoned. The houses were intact, but the colonists had completely disappeared. Although there are a number of theories about the fate of the colony, it remains a mystery and has come to be known as the \\"Lost Colony\\". Two English children were born in this colony; the first was named Virginia Dare ÿ Dare County, North Carolina, was named in honor of the baby, who was among those whose fate is unknown. The word Croatoan was found carved into a tree, the name of a tribe on a nearby island.[11]\\r\\nFollowing the failure of an earlier colonisation attempt, in the early 1600s,[clarification needed] England resumed attempts to set up a number of colonies. This time jointstock companies were used rather than giving extensive grants to a landed proprietor such as Gilbert or Raleigh.[3].\\r\\nJames granted a proprietary charter to two competing branches of the Virginia Company, which were supported by investors. These were the Plymouth Company and the London Company.[12] By the terms of the charter, the Plymouth Company was permitted to establish a colony of 100 miles (160?km) square between the 38th parallel and the 45th parallel (roughly between Chesapeake Bay and the current U.S.ÿCanada border). The London Company was permitted to establish between the 34th parallel and the 41st parallel (approximately between Cape Fear and Long Island Sound), and also owned a large portion of Atlantic and Inland Canada. In the area of overlap, the two companies were not permitted to establish colonies within one hundred miles of each other.[12] During 1606, each company organized expeditions to establish settlements within the area of their rights.\\r\\nThe London company formed Jamestown in its exclusive territory, whilst the Plymouth colony formed the Popham Colony in its exclusive territory near what is now Phippsburg, Maine.[13]\\r\\nThe London Company hired Captain Christopher Newport to lead its expedition. On December 20, 1606, he set sail from England with his flagship, the Susan Constant, and two smaller ships, the Godspeed, and the Discovery, with 105 men and boys, plus 39 sailors.[14] After an unusually long voyage of 144 days, they arrived at the mouth of the Chesapeake Bay and came ashore at the point where the southern side of the bay meets the Atlantic Ocean, an event that has come to be called the \\"First Landing\\". They erected a cross and named the point of land Cape Henry, in honor of Henry Frederick, Prince of Wales, the eldest son of King James.[citation needed]\\r\\nTheir instructions were to select a location inland along a waterway where they would be less vulnerable to the Spanish or other Europeans also seeking to establish colonies. They sailed westward into the Bay and reached the mouth of Hampton Roads, stopping at a location now known as Old Point Comfort. Keeping the shoreline to their right, they then ventured up the largest river, which they named the James, for their king. After exploring at least as far upriver as the confluence of the Appomattox River at present-day Hopewell, they returned downstream to Jamestown Island, which offered a favorable defensive position against enemy ships and deep water anchorage adjacent to the land. Within two weeks they had constructed their first fort and named their settlement Jamestown.[citation needed]\\r\\nIn addition to securing gold and other precious minerals to send back to the waiting investors in England, the survival plan for the Jamestown colonists depended upon regular supplies from England and trade with the Native Americans. The location they selected was largely cut off from the mainland and offered little game for hunting, no fresh drinking water, and very limited ground for farming. Captain Newport returned to England twice, delivering the First Supply and the Second Supply missions during 1608, and leaving the Discovery for the use of the colonists. However, death from disease and conflicts with the Natives Americans took a fearsome toll of the colonists. Despite attempts at mining minerals, growing silk, and exporting the native Virginia tobacco, no profitable exports had been identified, and it was unclear whether the settlement would survive financially.[citation needed]\\r\\nThe Powhatan Confederacy was a confederation of numerous linguistically related tribes in the eastern part of Virginia. The Powhatan Confederacy controlled a territory known as Tsenacommacah, which roughly corresponded with the Tidewater region of Virginia. It was in this territory that the English established Jamestown. At the time of the English arrival, the Powhatan were led by the paramount chief Wahunsenacawh.\\r\\nOn May 31, 1607, about 100 men and boys left England for what is now Maine. Approximately three months later, the group landed on a wooded peninsula where the Kennebec River meets the Atlantic Ocean and began building Fort St. George. By the end of the year, due to limited resources, half of the colonists returned to England. Late the next year, the remaining 45 sailed home, and the Plymouth company fell dormant.[15]\\r\\nIn 1609, with the abandonment of the Plymouth Company settlement, the London Company's Virginia charter was adjusted to include the territory north of the 34th parallel and south of the 39th parallel, with its original coastal grant extended \\"from sea to sea\\". Thus, at least according to James I's writ, the Virginia Colony in its original sense extended to the coast of the Pacific Ocean, in what is now California, with all the states in between (Kentucky, Missouri, Colorado, Utah, etc.) belonging to Virginia. For practical purposes, though, the colonists rarely ventured far inland to what was known as \\"The Virginia Wilderness\\", although the concept itself helped renew the interest of investors, and additional funds enabled an expanded effort, known as the Third Supply.[citation needed]\\r\\nFor the Third Supply, the London Company had a new ship built. The Sea Venture was specifically designed for emigration of additional colonists and transporting supplies. It became the flagship of the Admiral of the convoy, Sir George Somers. The Third Supply was the largest to date, with eight other ships joining the Sea Venture. The new Captain of the Sea Venture was the mission's Vice-Admiral, Christopher Newport. Hundreds of new colonists were aboard the ships. However, weather was to drastically affect the mission.[citation needed]\\r\\nA few days out of London, the nine ships of the third supply mission encountered a massive hurricane in the Atlantic Ocean. They became separated during the three days the storm lasted. Admiral Somers had the new Sea Venture, carrying most of the supplies of the mission, deliberately driven aground onto the reefs of Bermuda to avoid sinking. However, while there was no loss of life, the ship was wrecked beyond repair, stranding its survivors on the uninhabited archipelago, to which they laid claim for England.[16]\\r\\nThe survivors at Bermuda eventually built two smaller ships and most of them continued on to Jamestown, leaving a few on Bermuda to secure the claim. The Company's possession of Bermuda was made official in 1612, when the third and final charter extended the boundaries of 'Virginia' far enough out to sea to encompass Bermuda.[17] Bermuda has since been known officially also as The Somers Isles (in commemoration of Admiral Somers). The shareholders of the Virginia Company spun off a second company, the Somers Isles Company, which administered Bermuda from 1615 to 1684.[citation needed]\\r\\nUpon their arrival at Jamestown, the survivors of the Sea Venture discovered that the 10-month delay had greatly aggravated other adverse conditions. Seven of the other ships had arrived carrying more colonists, but little in the way of food and supplies. Combined with a drought, and hostile relations with the Native Americans, the loss of the supplies that had been aboard the Sea Venture resulted in the Starving Time in late 1609 to May 1610, during which over 80% of the colonists perished. Conditions were so adverse it appears, from skeletal evidence, that the survivors engaged in cannibalism.[18] The survivors from Bermuda had brought few supplies and food with them, and it appeared to all that Jamestown must be abandoned and it would be necessary to return to England.[citation needed]\\r\\nDuring this time, perhaps 5000 Virginians died of disease or were killed in the Indian massacre of 1622.[19]\\r\\nSamuel Argall was the captain of one of the seven ships of the Third Supply that had arrived at Jamestown in 1609 after becoming separated from the Sea Venture, whose fate was unknown. Depositing his passengers and limited supplies, he returned to England with word of the plight of the colonists at Jamestown. The King authorized another leader, Thomas West, 3rd Baron De La Warr, later better known as \\"Lord Delaware\\", to have greater powers, and the London Company organized another supply mission. They set sail from London on April 1, 1610.\\r\\nJust after the survivors of the Starving Time and those who had joined them from Bermuda had abandoned Jamestown, the ships of the new supply mission sailed up the James River with food, supplies, a doctor, and more colonists. Lord Delaware was determined that the colony was to survive, and he intercepted the departing ships about 10 miles (16?km) downstream of Jamestown. The colonists thanked Providence for the Colony's salvation.\\r\\nWest proved far harsher and more belligerent toward the Indians than any of his predecessors, engaging in wars of conquest against them. He first sent Gates to drive off the Kecoughtan from their village on July 9 1610, then gave Chief Powhatan an ultimatum to either return all English subjects and property, or face war. Powhatan responded by insisting that the English either stay in their fort or leave Virginia. Enraged, De la Warr had the hand of a Paspahegh captive cut off and sent him to the paramount chief with another ultimatum: Return all English subjects and property, or the neighboring villages would be burned. This time, Powhatan did not even respond.\\r\\nOn August 9, 1610, tired of waiting for a response from Powhatan, West sent George Percy with 70 men to attack the Paspahegh capital, burning the houses and cutting down their cornfields. They killed 65 to 75, and captured one of Wowinchopunk's wives and her children. Returning downstream, the English threw the children overboard and shot out \\"their Braynes in the water\\". The queen was put to the sword in Jamestown. The Paspahegh never recovered from this attack and abandoned their town. Another small force sent with Samuel Argall against the Warraskoyaks found that they had already fled, but he destroyed their abandoned village and cornfields as well. This event triggered the first Anglo-Powhatan War.\\r\\nAmong the individuals who had briefly abandoned Jamestown was John Rolfe, a Sea Venture survivor who had lost his wife and son in Bermuda. He was a businessman from London who had some untried seeds for new, sweeter strains of tobacco with him, as well as some untried marketing ideas. It would turn out that John Rolfe held the key to the Colony's economic success. By 1612, Rolfe's new strains of tobacco had been successfully cultivated and exported, establishing a first cash crop for export. Plantations and new outposts sprung up, initially both upriver and downriver along the navigable portion of the James, and thereafter along the other rivers and waterways of the area. The settlement at Jamestown could finally be considered permanently established.[20]\\r\\nA period of peace followed the marriage in 1614 of colonist John Rolfe to Pocahontas, the daughter of Algonquian chief Powhatan.\\r\\nThe relations with the Natives took a turn for the worse after the death of Pocahontas in England and the return of John Rolfe and other colonial leaders in May 1617. Disease, poor harvests and the growing demand for tobacco lands caused hostilities to escalate.\\r\\nAfter Wahunsenacawh's death in 1618, he was soon succeeded by his own younger brother, Opechancanough. He maintained friendly relations with the Colony on the surface, negotiating with them through his warrior Nemattanew, but by 1622, after Nemattanew had been slain, Opechancanough was ready to order a limited surprise attack on them, hoping to persuade them to move on and settle elsewhere.\\r\\nChief Opechancanough organized and led a well-coordinated series of surprise attacks on multiple English settlements along both sides of a 50-mile (80?km) long stretch of the James River, which took place early on the morning of March 22, 1622. This event came to be known as the Indian Massacre of 1622 and resulted in the deaths of 347 colonists (including men, women, and children) and the abduction of many others. The Massacre caught most of the Virginia Colony by surprise and virtually wiped out several entire communities, including Henricus and Wolstenholme Town at Martin's Hundred.\\r\\nJamestown was spared from destruction, however, due to a Virginia Indian boy named Chanco who, after learning of the planned attacks from his brother, gave warning to colonist Richard Pace with whom he lived. Pace, after securing himself and his neighbors on the south side of the James River, took a canoe across river to warn Jamestown, which narrowly escaped destruction, although there was no time to warn the other settlements.\\r\\nA year later, Captain William Tucker and Dr. John Potts worked out a truce with the Powhatan and proposed a toast using liquor laced with poison. 200 Virginia Indians were killed or made ill by the poison and 50 more were slaughtered by the colonists. For over a decade, the English settlers killed Powhatan men and women, captured children and systematically razed villages, seizing or destroying crops.\\r\\nBy 1634, a six-mile-long palisade was completed across the Virginia Peninsula. The new palisade provided some security from attacks by the Virginia Indians for colonists farming and fishing lower on the Peninsula from that point.\\r\\nOn April 18, 1644, Opechancanough again tried to force the colonists to abandon the region with another series of coordinated attacks, killing almost 500 colonists. However, this was a much less devastating portion of the growing population than had been the case in the 1622 attacks.\\r\\nThe forces of Royal Governor of Virginia William Berkeley captured the old warrior in 1646,[21] variously thought to be between 90 and 100 years old. In October, while a prisoner, Opechancanough was killed by a soldier (shot in the back) assigned to guard him.\\r\\nIn 1620, a successor to the Plymouth Company sent colonists to the New World aboard the Mayflower. Known as Pilgrims, they successfully established a settlement in what became Massachusetts. The portion of what had been Virginia north of the 40th parallel became known as New England, according to books written by Captain John Smith, who had made a voyage there.\\r\\nIn 1624, the charter of the Virginia Company was revoked by King James I and the Virginia Colony was transferred to royal authority in the form of a crown colony. Subsequent charters for the Maryland Colony in 1632 and to the eight Lords Proprietor of the Province of Carolina in 1663 and 1665 further reduced the Virginia Colony to roughly the coastal borders it held until the American Revolution. (The exact border with North Carolina was disputed until surveyed by William Byrd II in 1728.)\\r\\nAfter twelve years of peace following the Indian Wars of 1622-1632, another AngloÿPowhatan War began on March 18, 1644, as a last effort by the remnants of the Powhatan Confederacy, still under Opechancanough, to dislodge the English settlers of the Virginia Colony.[22] Around 500 colonists were killed, but that number represented a relatively low percent of the overall population, as opposed to the earlier massacre (the 1622 attack had wiped out a third; that of 1644 barely a tenth). However, Opechancanough, still preferring to use Powhatan tactics, did not make any major follow-up to this attack.\\r\\nThis was followed by a last effort by the settlers to decimate the Powhatan. In July, they marched against the Pamunkey, Chickahominy, and Powhatan proper; and south of the James, against the Appomattoc, Weyanoke, Warraskoyak, and Nansemond, as well as two Carolina tribes, the Chowanoke and Secotan.\\r\\nIn February 1645, the colony ordered the construction of three frontier forts: Fort Charles at the falls of the James, Fort James on the Chickahominy, and Fort Royal at the falls of the York. In August, Governor William Berkeley stormed Opechancanough's stronghold and captured him. All captured males in the village over age 11 were deported to Tangier Island.[23] Opechancanough, around 92 years old, was taken to Jamestown where he was shot in the back by a guard.[22] Opechancanough's death resulted in the disintegration of the Powhatan Confederacy into its component tribes, whom the colonists continued to attack. In March 1646, the colony decided to build a fourth frontier fort, Fort Henry, at the falls of the Appomattox, where the modern city of Petersburg is located.\\r\\nIn the peace treaty of October 1646, the new weroance, Necotowance, and the subtribes formerly in the Confederacy, each became tributaries to the King of England. At the same time, a racial frontier was delineated between Indian and English settlements, with members of each group forbidden to cross to the other side except by special pass obtained at one of the newly erected border forts. The extent of the Virginia colony open to patent by English colonists was defined as: All the land between the Blackwater and York rivers, and up to the navigable point of each of the major rivers - which were connected by a straight line running directly from modern Franklin on the Blackwater, northwesterly to the Appomattoc village beside Fort Henry, and continuing in the same direction to the Monocan village above the falls of the James, where Fort Charles was built, then turning sharp right, to Fort Royal on the York (Pamunkey) river. Necotowance thus ceded the English vast tracts of still-uncolonized land, much of it between the James and Blackwater. English settlements on the peninsula north of the York and below the Poropotank were also allowed, as they had already been there since 1640.\\r\\nAlthough the newer, Puritan colonies, most notably Massachusetts, were dominated by Parliamentarians, the older colonies sided with the Crown. The Virginia Company's two settlements, Virginia and Bermuda (Bermuda's Independent Puritans were expelled as the Eleutheran Adventurers, settling the Bahamas under William Sayle), Antigua and Barbados were conspicuous in their loyalty to the Crown, and were singled out by the Rump Parliament in An Act for prohibiting Trade with the Barbadoes, Virginia, Bermuda and Antego in October, 1650. This dictated that\\r\\ndue punishment [be] inflicted upon the said Delinquents, do Declare all and every the said persons in Barbada's, Antego, Bermuda's and Virginia, that have contrived, abetted, aided or assisted those horrid Rebellions, or have since willingly joyned with them, to be notorious Robbers and Traitors, and such as by the Law of Nations are not to be permitted any maner of Commerce or Traffique with any people whatsoever; and do forbid to all maner of persons, Foreiners, and others, all maner of Commerce, Traffique and Correspondency whatsoever, to be used or held with the said Rebels in the Barbada's, Bermuda's, Virginia and Antego, or either of them.\\r\\nThe Act also authorised Parliamentary privateers to act against English vessels trading with the rebellious colonies: \\"All Ships that Trade with the Rebels may be surprized. Goods and tackle of such ships not to be embezeled, till judgement in the Admiralty; Two or three of the Officers of every ship to be examined upon oath.\\"\\r\\nVirginia's population swelled with Cavaliers during and after the English Civil War. Despite the resistance of the Virginia Cavaliers, Virginian Puritan Richard Bennett was made Governor answering to Cromwell in 1652, followed by two more nominal \\"Commonwealth Governors\\". Nonetheless, the colony was rewarded for its loyalty to the Crown by Charles the II following the Restoration when he dubbed it the Old Dominion.[citation needed]\\r\\nWith the Restoration in 1660 the Governorship returned to its previous holder, Sir William Berkeley.\\r\\nIn 1676, Bacon's Rebellion challenged the political order of the colony. While a military failure, its handling did result in Governor Berkeley being recalled to England.\\r\\nIn 1679, the Treaty of Middle Plantation was signed.\\r\\nThe largest and richest and most influential of the American colonies was Virginia, where conservatives were in full control of the colonial and local governments. At the local level, Church of England parishes handled many local affairs, and they in turn were controlled not by the minister, but rather by a closed circle of rich landowners who comprised the parish vestry. Ronald L. Heinemann emphasizes the ideological conservatism of Virginia, while noting there were also religious dissenters who were gaining strength by the 1760s:\\r\\nIn actual practice, colonial Virginia never had a bishop to represent God nor a hereditary aristocracy with titles like 'duke' or 'baron'. However it did have a royal governor appointed by the king, as well as a powerful landed gentry. The status quo was strongly reinforced by what Jefferson called \\"feudal and unnatural distinctions\\" that were vital to the maintenance of aristocracy in Virginia. He targeted laws such as entail and primogeniture by which the oldest son inherited all the land. As a result increasingly large plantations, worked by white tenant farmers and by black slaves, gained in size and wealth and political power in the eastern (\\"Tidewater\\") tobacco areas. Maryland and South Carolina had similar hierarchical systems, as did New York and Pennsylvania.[25] During the Revolutionary era, all such laws were repealed by the new states.[26] The most fervent Loyalists left for Canada or Britain or other parts of the Empire. They introduced primogeniture in Upper Canada (Ontario) in 1792, and it lasted until 1851. Such laws lasted in England until 1926.[27]\\r\\nAs the English expanded out from Jamestown, encroachment of the new arrivals and their ever-growing numbers on what had been Indian lands resulted in several conflicts with the Virginia Indians. For much of the 17th century, English contact and conflict was mostly with the Algonquian peoples that populated the coastal regions, primarily the Powhatan Confederacy. Following a series of wars and the decline of the Powhatan as a political entity, the colonists expanded westward in the late 17th and 18th centuries, encountering the Shawnee, Iroquoian-speaking peoples such as the Nottoway, Meherrin, Iroquois and Cherokee, as well as Siouan-speaking peoples such as the Tutelo, Saponi, and Occaneechi.\\r\\nAs the English settlements expanded beyond the Tidewater territory traditionally occupied by the Powhatan, they encountered new groups with which there had been minimal relations with the Colony.\\r\\nIn the late 17th century, the Iroquois Confederacy expanded into the Western region of Virginia as part of the Beaver Wars. They arrived shortly before the English settlers, and displaced the resident Siouan tribes.\\r\\nLt. Gov. Alexander Spotswood made further advances in policy with the Virginia Indians along the frontier. In 1714, he established Fort Christanna to help educate and trade with several tribes with which the colony had friendly relations, as well as to help protect them from hostile tribes. In 1722, he negotiated the Treaty of Albany.\\r\\nThe cultural geography of colonial Virginia gradually evolved, with a variety of settlement and jurisdiction models experimented with. By the late 17th century and into the 18th century, the primary settlement pattern was based on plantations (to grow tobacco), farms, and some towns (mostly ports or courthouse villages).\\r\\nThe fort at Jamestown, founded in 1607, remained the primary settlement of the colonists for several years. A few strategic outposts were constructed, including Fort Algernon (1609) at the entrance to the James River.\\r\\nEarly attempts to occupy strategic locations already inhabited by natives at what is now Richmond and Suffolk failed owing to native resistance.\\r\\nA short distance farther up the James, in 1611, Thomas Dale began the construction of a progressive development at Henricus on and about what was later known as Farrars Island. Henricus was envisioned as possible replacement capital for Jamestown, and was to have the first college in Virginia. (The ill-fated Henricus was destroyed during the Indian Massacre of 1622). In addition to creating the new settlement at Henricus, Dale also established the port town of Bermuda Hundred, as well as \\"Bermuda Cittie\\" (sic) in 1613, now part of Hopewell, Virginia. He began the excavation work at Dutch Gap, using methods he had learned while serving in Holland.\\r\\nOnce tobacco had been established as an export cash crop, investors became more interested and groups of them united to create largely self-sufficient \\"hundreds.\\" The term \\"hundred\\" is a traditional English name for an administrative division of a shire (or county) to define an area which would support one hundred heads of household.[28] In the colonial era in Virginia, the \\"hundreds\\" were large developments of many acres, necessary to support land hungry tobacco crops. The \\"hundreds\\" were required to be at least several miles from any existing community. Soon, these patented tracts of land sprung up along the rivers. The investors sent shiploads of settlers and supplies to Virginia to establish the new developments. The administrative centers of Virginia's hundreds were essentially small towns or villages, and were often palisaded for defense.\\r\\nAn example was Martin's Hundred, located downstream from Jamestown on the north bank of the James River. It was sponsored by the Martin's Hundred Society, a group of investors in London. It was settled in 1618, and Wolstenholme Towne was its administrative center, named for Sir John Wolstenholme, one of the investors.\\r\\nBermuda Hundred (now in Chesterfield County) and Flowerdew Hundred (now in Prince George County) are other names which have survived over centuries. Others included Berkeley Hundred, Bermuda Nether Hundred, Bermuda Upper Hundred, Smith's Hundred, Digges Hundred, West Hundred and Shirley Hundred (and, in Bermuda, Harrington Hundreds).\\r\\nIncluding the creation of the \\"hundreds\\", the various incentives to investors in the Virginia Colony finally paid off by 1617. By this time, the colonists were exporting 50,000 pounds of tobacco to England a year and were beginning to generate enough profit to ensure the economic survival of the colony.\\r\\nIn 1619, the plantations and developments were divided into four \\"incorporations\\" or \\"citties\\" (sic), as they were called. These were Charles Cittie, Elizabeth Cittie, Henrico Cittie, and James Cittie, which included the relatively small seat of government for the colony at Jamestown Island. Each of the four \\"citties\\" (sic) extended across the James River, the main conduit of transportation of the era. Elizabeth Cittie, know initially as Kecoughtan (a Native word with many variations in spelling by the English), also included the areas now known as South Hampton Roads and the Eastern Shore.\\r\\nIn 1634, a new system of local government was created in the Virginia Colony by order of the King of England. Eight shires were designated, each with its own local officers. Within a few years, the shires were renamed counties, a system which has remained to the present day.\\r\\nIn 1630, under the governorship of John Harvey, the first settlement on the York River was founded. In 1632, the Virginia legislature voted to build a fort to link Jamestown and the York River settlement of Chiskiack and protect the colony from Indian attacks. In 1634, a palisade was built near Middle Plantation. This wall stretched across the peninsula between the York and James rivers and protected the settlements on the eastern side of the lower Peninsula from Indians. The wall also served to contain cattle.\\r\\nIn 1699, a new capital was established and built at Middle Plantation, soon renamed Williamsburg.\\r\\nIn the period following the English Civil War, the exiled King Charles II of England hoped to shore up the loyalty of several of his supporters by granting them a significant area of mostly uncharted land to control as a Proprietary in Virginia (a claim that would only be valid were the king to return to power). While under the jurisdiction of the Virginia Colony, the proprietary maintained complete control of the granting of land within that territory (and revenues obtained from it) until after the American Revolution. The grant was for the land between the Rappahannock and Potomac Rivers, which included the titular Northern Neck, but as time went on also would include all of what is today Northern Virginia and into West Virginia. Due to ambiguities of the text of the various grants causing disputes between the proprietary and the colonial government, the tract was finally demarcated via the Fairfax Line in 1746.\\r\\nIn the initial years under the Virginia Company, the colony was governed by a council, headed by a council President. From 1611 to 1618, under the orders of Sir Thomas Dale, the settlers of the colony were under a regime of martial law that became known as Dale's Code.[29]\\r\\nUnder a charter from the company in 1618, a new model of governance was put in place in 1619, which created a new House of Burgesses.[29] On July 30, 1619, burgesses met at Jamestown Church as the first elected representative legislative assembly in the New World.[29] The legal system in the colony was thereafter based around the English common law.\\r\\nFor much of the history of the Royal Colony, the formal appointed governor was absentee, often remaining in England. In his stead, a series of acting or Lieutenant Governors who were physically present held actual authority. In the later years of its history, as it became increasingly \\"civilized,\\" more governors made the journey.\\r\\nThe first settlement in the colony, Jamestown, served as the capital and main port of entry from its founding until 1699. During this time, a series of statehouses (capitols) were used and subsequently consumed by fires (both accidental, and in the case of Bacon's Rebellion, intentional). Following such a fire, in 1699 the capital was relocated inland, away from the swampy clime of Jamestown to Middle Plantation, soon to be renamed Williamsburg.\\r\\nThe capital of Virginia remained in Williamsburg, until it was moved further inland to Richmond in 1779 during the American Revolution.\\r\\nThe entrepreneurs of the Virginia Company experimented with a number of means of making the colony profitable. The orders sent with the first colonists instructed that they search for precious metals (specifically gold). While no gold was found, various products were sent back, including pitch and clapboard. In 1608, early attempts were made at breaking the Continental hold on glassmaking through the creation of a glassworks. In 1619, the colonist built the first ironworks in North America.\\r\\nIn 1612, settler John Rolfe planted tobacco obtained from Bermuda (during his stay there as part of the Third Supply). Within a few years, the crop proved extremely lucrative in the European market. As the English increasingly used tobacco products, tobacco in the American colonies became significant economic force, especially in the tidewater region surrounding the Chesapeake Bay.\\r\\nVast plantations were built along the rivers of Virginia, and social/economic systems developed to grow and distribute this cash crop. Some elements of this system included the importation and employment of slaves to grow crops. Planters would then fill large hogsheads with tobacco and convey them to inspection warehouses. In 1730, the Virginia House of Burgesses standardized and improved quality of tobacco exported by establishing the Tobacco Inspection Act of 1730, which required inspectors to grade tobacco at 40 specified locations.\\r\\nEngland supplied the great majority of colonists. In 1608, the first Poles and Slovaks arrived as part of a group of skilled craftsmen.[30][31][32][33] In 1619, the first Africans arrived, though the concept of racially based slavery did not evolve for several decades. In the mid-17th century, French Huguenots arrived in the colony.[34] In the early 18th century, German specialists arrived to establish the Germanna settlement.[35] Scots and Scots-Irish settled on the Virginia frontier.[36] Some Welsh arrived including the ancestors of Thomas Jefferson.[37]\\r\\nWith the boom in tobacco planting, there was a severe shortage of laborers to work the labor-intensive crop. One method to solve the shortage was through the usage of indentured servants.\\r\\nBy the 1640s, legal documents started to define the changing nature of indentured servants and their status as servants. In 1640, John Punch was sentenced to lifetime servitude as punishment for trying to escape from his master Hugh Gwyn. This is the earliest legal sanctioning of slavery in Virginia.[38] After this trial, the relationship between indentured servants and their masters changed, as planters saw permanent servitude a more appealing and profitable prospect than seven-year indentures.\\r\\nAs many indentured workers were illiterate, especially Africans, there were opportunities for abuse by planters and other indenture holders. Some ignored the expiration of servants' indentured contracts and tried to keep them as lifelong workers. One example is with Anthony Johnson, who argued with Robert Parker, another planter, over the status of John Casor, formerly an indentured servant of his. Johnson argued that his indenture was for life and Parker had interfered with his rights. The court ruled in favor of Johnson and ordered that Casor be returned to him, where he served the rest of his life as a slave.[39] Such documented cases marked the transformation of Negroes from indentured servants into slaves.\\r\\nIn the late 17th century, the Royal African Company, which was established by the King of England to supply the great demand for labor to the colonies, had a monopoly on the provision of African slaves to the colony.[40] As plantation agriculture was established earlier in Barbados, in the early years, slaves were shipped from Barbados (where they were seasoned) to the colonies of Virginia and Carolina.\\r\\nIn 1619, the Anglican Church was formally established as the official religion in the colony, and would remain so until shortly after the American Revolution. Establishment meant that local tax funds paid the parish costs, and that the parish had local civic functions such as poor relief. The upper class planters controlled the vestry, which ran the parish and chose the minister. The church in Virginia was controlled by the Bishop of London, who sent priests and missionaries, but there were never enough, and they reported very low standards of personal morality.[41] By the 1760s, dissenting Protestants, especially Baptists and Methodists, were growing rapidly and started challenging the Anglicans for moral leadership.[42][43][44]\\r\\nThe first printing press used in Virginia began operation in Jamestown on June 8, 1680, though within a few years it was shut down by the Governor and Crown of England for want of a license.[45] It was not until 1736 that the first newspaper, the Virginia Gazette, began circulation under printer William Parks of Williamsburg.[45]\\r\\nThe Syms-Eaton Academy, started in 1634, became the first free public school in America. Private tutors were often favored among those families who could afford them.[46]\\r\\nFor most of the 17th century, a university education for settlers of Virginia required a journey to England or Scotland.[46] Such journeys were undertaken by wealthy young men. In the early years, many settlers received their education prior to immigrating to the colony.[46]\\r\\nIn 1693, the College of William and Mary was founded at Middle Plantation (soon renamed Williamsburg). The college included a common school for Virginia Indians, supplemented by local pupils, which lasted until a 1779 overhaul of the institution's curriculum.[46] The college, located in the capital and heart of the Tidewater region, dominated the colony's intellectual climate until after independence.[46][47]\\r\\nAfter 1747, some Virginians began to attend institutions at Princeton and Philadelphia. Generations began to move west into the Piedmont and Blue Ridge areas.[46] It is in this region of Virginia that two future Presbyterian colleges trace their origins to lower level institutions founded in this time period. First, what would become HampdenÿSydney College was founded in 1775, immediately prior to the American Revolution. Likewise, Augusta Academy was a classical school that would evolve into Washington and Lee University (though would not grant its first bachelor's degree until 1785).","input":"What was the foundation of the virginia colony's early economy?"},{"output":"Afghanistan","context":"South Asia or Southern Asia (also known as the Indian subcontinent) is a term used to represent the southern region of the Asian continent, which comprises the sub-Himalayan SAARC countries and, for some authorities, adjoining countries to the west and east. Topographically, it is dominated by the Indian Plate, which rises above sea level as Nepal and northern parts of India situated south of the Himalayas and the Hindu Kush. South Asia is bounded on the south by the Indian Ocean and on land (clockwise, from west) by West Asia, Central Asia, East Asia, and Southeast Asia.\\r\\nThe current territories of Afghanistan, Bangladesh, Bhutan, Maldives, Nepal, India, Pakistan, and Sri Lanka form South Asia.[7] The South Asian Association for Regional Cooperation (SAARC) is an economic cooperation organisation in the region which was established in 1985 and includes all eight nations comprising South Asia.[8]\\r\\nSouth Asia covers about 5.2 million km2 (2 million mi2), which is 11.71% of the Asian continent or 3.5% of the world's land surface area.[7] The population of South Asia is about 1.749 billion or about one fourth of the world's population, making it both the most populous and the most densely populated geographical region in the world.[3] Overall, it accounts for about 39.49% of Asia's population, over 24% of the world's population, and is home to a vast array of peoples.[9][10][11]\\r\\nIn 2010, South Asia had the world's largest population of Hindus, Jains and Sikhs. It also has the largest population of Muslims in Asia-Pacific region,[12][13] as well as over 35 million Christians and 25 million Buddhists.[14]\\r\\n\\r\\n\\r\\nThe total area of South Asia and its geographical extent is not clear cut as systemic and foreign policy orientations of its constituents are quite asymmetrical.[16] Aside from the central region of South Asia, formerly part of the British Empire, there is a high degree of variation as to which other countries are included in South Asia.[17][18][19][20]\\r\\nModern definitions of South Asia are consistent in including Afghanistan, India, Pakistan, Bangladesh, Sri Lanka, Nepal, Bhutan and Maldives as the constituent countries.[21][22][23][24][25][26] Myanmar is included by some scholars in South Asia, but in Southeast Asia by others.[18][27] Some do not include Afghanistan,[18] others question whether Afghanistan should be considered a part of South Asia or the Middle East.[28][29]\\r\\nThe current territories of Bangladesh, India, and Pakistan, which were the core of the British Empire prior to 1947, form the central region of South Asia, in addition to Afghanistan,[21][22][23][24][25][26] which was a British protectorate until 1919, after the Afghans lost to the British in the Second Anglo-Afghan war. The mountain countries of Nepal and Bhutan, and the island countries of Sri Lanka and Maldives are generally included as well. Myanmar (formerly Burma) is often added, and by various deviating definitions based on often substantially different reasons, the British Indian Ocean Territory and the Tibet Autonomous Region are included as well.[16][30][31][32][33][34][35][36][37]\\r\\nThe common concept of South Asia is largely inherited from the administrative boundaries of the British Raj,[38] with several exceptions. The Aden Colony, British Somaliland and Singapore, though administered at various times under the Raj, have not been proposed as any part of South Asia.[39] Additionally Burma was administered as part of the Raj until 1937, but is now considered a part of Southeast Asia and is a member state of ASEAN. The 562 princely states that were protected by but not directly ruled by the Raj became administrative parts of South Asia upon joining Union of India or Dominion of Pakistan.[40][41][42] Geopolitically, it had formed the whole territory of Greater India,[27][43]\\r\\nThe South Asian Association for Regional Cooperation (SAARC), a contiguous block of countries, started in 1985 with seven countries?ÿ Bangladesh, Bhutan, India, the Maldives, Nepal, Pakistan and Sri Lanka?ÿ and added Afghanistan as an eighth member in 2007.[44][45] China and Myanmar have also applied for the status of full members of SAARC.[46][47] This bloc of countries include two independent countries that were not part of the British Raj ÿ Nepal, and Bhutan. Afghanistan was a British protectorate from 1878 until 1919, after the Afghans lost to the British in the Second Anglo-Afghan war. The World Factbook, based on geo-politics, people, and economy defines South Asia as comprising Afghanistan, Bangladesh, Bhutan, British Indian Ocean Territory, India, Maldives, Nepal, Pakistan, and Sri Lanka.[48] The South Asia Free Trade Agreement incorporated Afghanistan in 2011, and the World Bank grouping of countries in the region also includes all eight members comprising South Asia and SAARC as well,[49][50] and the same goes for the United Nations Children's Fund (UNICEF).[51][52]\\r\\nThe Centres for South Asian Studies at both the University of Michigan and the University of Virginia include Tibet along with the eight members of SAARC in their research programs, but exclude the Maldives.[59][60] The South Asian Studies Program of Rutgers University and the University of California, Berkeley Centre for South Asia Studies also include the Maldives.[61][62]\\r\\nThe South Asian Studies Program of Brandeis University defines the region as comprising \\"India, Pakistan, Bangladesh, Sri Lanka, Nepal, Bhutan, and in certain contexts Afghanistan, Burma, Maldives and Tibet\\".[63] The similar program of Columbia University includes Afghanistan, Bangladesh, India, the Maldives, Nepal, Pakistan, and Sri Lanka in their study and excludes Burma.[64]\\r\\nThe United Nations Statistics Division's scheme of sub-regions include all eight members of the SAARC as part of Southern Asia, along with Iran[65] only for statistical purposes.[66] Population Information Network (POPIN) includes Afghanistan, Bangladesh, Burma, India, Nepal, Pakistan and Sri Lanka as part of South Asia. Maldives, in view of its characteristics, was admitted as a member Pacific POPIN subregional network only in principle.[67] The HirschmanÿHerfindahl index of the United Nations Economic and Social Commission for Asia and the Pacific for the region includes only the original seven signatories of SAARC.[68]\\r\\nThe British Indian Ocean Territory is connected to the region by a publication of Jane's for security considerations.[69] The region may also include the disputed territory of Aksai Chin, which was part of the British Indian princely state of Jammu and Kashmir, but is now administered as part of the Chinese autonomous region of Xinjiang.[70]\\r\\nThe inclusion of Myanmar in South Asia is without consensus, with many considering it a part of Southeast Asia and others including it within South Asia.[18][27] Afghanistan was of importance to the British colonial empire, especially after the Second Anglo-Afghan War over 1878ÿ1880. Afghanistan remained a British protectorate until 1919, when a treaty with Vladimir Lenin included the granting of independence to Afghanistan. Following India's partition, Afghanistan has generally been included in South Asia, with some considering it a part of Southwest Asia.[16] During the Soviet war in Afghanistan (1979ÿ1989) American foreign policy considered Pakistan and Afghanistan in Southwest Asia, while others included it as a part of South Asia.[7] There is no universal agreement among scholars on which countries should be included within South Asia.[18]\\r\\nIn the past, a lack of a coherent definition for South Asia resulted in not only a lack of academic studies, but also in a lack interest for such studies.[71] The confusion existed also because of the lack of a clear boundary ÿ geographically, geopolitical, socio-culturally, economically or historically ÿ between South Asia and other parts of Asia, especially the Middle East and Southeast Asia.[72] Identification with a South Asian identity was also found to be significantly low among respondents in an older two-year survey across Bangladesh, India, Nepal, Pakistan, and Sri Lanka.[73] However, modern definitions of South Asia are very consistent in including Afghanistan, India, Pakistan, Bangladesh, Sri Lanka, Nepal, Bhutan and Maldives as the constituent countries.[21][22][23][24][25][26]\\r\\nAccording to the Oxford English Dictionary, the term \\"subcontinent\\" signifies a \\"subdivision of a continent which has a distinct geographical, political, or cultural identity\\" and also a \\"large land mass somewhat smaller than a continent\\".[74][75] Historians Catherine Asher and Cynthia Talbot state that the term \\"Indian subcontinent\\" describes a natural physical landmass in South Asia that has been relatively isolated from the rest of Eurasia.[76] The Indian subcontinent is also a geological term referring to the land mass that drifted northeastwards from ancient Gondwana, colliding with the Eurasian plate nearly 55 million years ago, towards the end of Palaeocene. This geological region largely includes Bangladesh, Bhutan, India, Maldives, Nepal, Pakistan and Sri Lanka.[77]\\r\\nThe use of the term Indian subcontinent began in the British Empire, and has been a term particularly common in its successors.[78] This region has also been labelled as \\"India\\" (in its classical and pre-modern sense), \\"Greater India\\", or as South Asia.[27][43]\\r\\nAccording to anthropologist John R. Lukacs, \\"the Indian Subcontinent occupies the major landmass of South Asia\\",[79] while the political science professor Tatu Vanhanen states, \\"the seven countries of South Asia constitute geographically a compact region around the Indian Subcontinent\\".[80] According to Chris Brewster, India, Pakistan, Bangladesh, Sri Lanka, Nepal and Bhutan constitute the Indian subcontinent; with Afghanistan and Maldives included it is more commonly referred to as South Asia.[81] The geopolitical boundaries of Indian subcontinent, according to Dhavendra Kumar, include \\"India, Pakistan, Bangladesh, Sri Lanka, Nepal, Bhutan and other small islands of the Indian Ocean\\".[82] Maldives, the country consisting of a small archipelago southwest of the peninsula, is considered part of the Indian subcontinent.[83]\\r\\nThe terms \\"Indian subcontinent\\" and \\"South Asia\\" are sometimes used interchangeably.[30][78] The South Asia term is particularly common when scholars or officials seek to differentiate this region from East Asia.[84] According to historians Sugata Bose and Ayesha Jalal, the Indian subcontinent has come to be known as South Asia \\"in more recent and neutral parlance.\\"[85] This \\"neutral\\" notion refers to the concerns of Pakistan and Bangladesh, particularly given the recurring conflicts between India and Pakistan, wherein the dominant placement of \\"India\\" as a prefix before the subcontinent might offend some political sentiments.[27]\\r\\nThere is no globally accepted definition on which countries are a part of South Asia or Indian subcontinent.[18][19][20] While Afghanistan is not considered as a part of the Indian subcontinent, Afghanistan is often included in South Asia.[20] Similarly, Myanmar is included by some scholars in South Asia but not in Indian subcontinent.[27]\\r\\nThe history of core South Asia begins with evidence of human activity of Homo sapiens, as long as 75,000 years ago, or with earlier hominids including Homo erectus from about 500,000 years ago.[86] The Indus Valley Civilization, which spread and flourished in the northwestern part of South Asia from c. 3300 to 1300 BCE in present-day Northern India, Pakistan and Afghanistan, was the first major civilization in South Asia.[87] A sophisticated and technologically advanced urban culture developed in the Mature Harappan period, from 2600 to 1900 BCE.[88]\\r\\nThe earliest prehistoric culture have roots in the mesolithic sites as evidenced by the rock paintings of Bhimbetka rock shelters dating to a period of 30,000 BCE or older,[note 2] as well as neolithic times.[note 3] According to anthropologist Possehl, the Indus Valley Civilization provides a logical, if somewhat arbitrary, starting point for South Asian religions, but these links from the Indus religion to later-day South Asian traditions are subject to scholarly dispute.[89]\\r\\nThe Vedic period, named after the Vedic religion of the Indo-Aryans,[note 4] lasted from c. 1900 to 500 BCE.[91][92] The Indo-Aryans were pastoralists[93] who migrated into north-western India after the collapse of the Indus Valley Civilization,[90][94] Linguistic and archaeological data show a cultural change after 1500 BCE,[90] with the linguistic and religious data clearly showing links with Indo-European languages and religion.[95] By about 1200 BCE, the Vedic culture and agrarian lifestyle was established in the northwest and northern Gangetic plain of South Asia.[93][96][97] Rudimentary state-forms appeared, of which the Kuru-Pa?cla union was the most influential.[98][99] The first recorded state-level society in South Asia existed around 1000 BCE.[93] In this period, states Samuel, emerged the Brahmana and Aranyaka layers of Vedic texts, which merged into the earliest Upanishads.[100] These texts began to ask the meaning of a ritual, adding increasing levels of philosophical and metaphysical speculation,[100] or \\"Hindu synthesis\\".[101]\\r\\nIncreasing urbanisation of India between 800 and 400 BCE, and possibly the spread of urban diseases, contributed to the rise of ascetic movements and of new ideas which challenged the orthodox Brahmanism.[102] These ideas led to Sramana movements, of which Mahavira (c. 549ÿ477 BCE), proponent of Jainism, and Buddha (c. 563-483), founder of Buddhism, were the most prominent icons.[103]\\r\\nThe Greek army led by Alexander the Great stayed in the Hindu Kush region of South Asia for several years and then later moved into the Indus valley region. Later, the Maurya Empire extended over much of South Asia in the 3rd century BCE. Buddhism spread beyond the Indian subcontinent, through northwest into Central Asia. The Bamiyan Buddhas of Afghanistan and the edicts of A?oka suggest that the Buddhist monks spread Buddhism (Dharma) in eastern provinces of the Seleucid Empire, and possibly even farther into West Asia.[104][105][106] The Theravada school spread south from India in the 3rd century BCE, to Sri Lanka, later to Southeast Asia.[107] Buddhism, by the last centuries of the 1st millennium BCE, was prominent in the Himalayan region, Gandhara, Hindu Kush region and Bactria.[108][109][110]\\r\\nFrom about 500 BCE through about 300 CE, the Vedic-Brahmanic synthesis or \\"Hindu synthesis\\" continued.[101] Classical Hindu and Sramanic (particularly Buddhist) ideas spread within Indian subcontinent, as well outside South Asia.[111][112][113] The Gupta Empire ruled over a large part of the subcontinent between 4th and 7th centuries, a period that saw the construction of major temples, monasteries and universities such as the Nalanda.[114][115][116] During this era, and through the 10th century, numerous cave monasteries and temples such as the Ajanta Caves, Badami cave temples and Ellora Caves were built in South Asia.[117][118][119]\\r\\nIslam came as a political power in the fringe of South Asia in 8th century CE when the Arab general Muhammad bin Qasim conquered Sindh and Multan in southern Punjab in modern-day Pakistan.[120] By 962 CE, Hindu and Buddhist kingdoms in South Asia were under a wave of raids from Muslim armies from Central Asia.[121] Among them was Mahmud of Ghazni, who raided and plundered kingdoms in north India from east of the Indus river to west of Yamuna river seventeen times between 997 and 1030.[122] Mahmud of Ghazni raided the treasuries but retracted each time, only extending Islamic rule into western Punjab.[123][124]\\r\\nThe wave of raids on north Indian and western Indian kingdoms by Muslim warlords continued after Mahmud of Ghazni, plundering and looting these kingdoms.[125] The raids did not establish or extend permanent boundaries of their Islamic kingdoms. The Ghurid Sultan Mu'izz al-Din Muhammad began a systematic war of expansion into north India in 1173.[126] He sought to carve out a principality for himself by expanding the Islamic world.[122][127] Mu'izz sought a Sunni Islamic kingdom of his own extending east of the Indus river, and he thus laid the foundation for the Muslim kingdom that became the Delhi Sultanate.[122] Some historians chronicle the Delhi Sultanate from 1192 due to the presence and geographical claims of Mu'izz al-Din in South Asia by that time.[128] The Delhi Sultanate covered varying parts of South Asia, and was ruled by a series of dynasties, called Mamluk, Khalji, Tughlaq, Sayyid and Lodi dynasties. Muhammad bin Tughlaq came to power in 1325, launched a war of expansion and the Delhi Sultanate reached it largest geographical reach over the Indian subcontinent during his 26-year rule.[129] A Sunni Sultan, Muhammad bin Tughlaq persecuted non-Muslims such as Hindus, as well as non-Sunni Muslims such as Shia and Mahdi sects.[130][131][132]\\r\\nRevolts against the Delhi Sultanate sprang up in many parts of South Asia during the 14th century. After the death of Muhammad bin Tughlaq, the Bengal Sultanate came to power in 1352 CE, as the Delhi Sultanate began disintegrating. The Bengal Sultanate remained in power through the early 16th century. It was reconquered by the armies of the Mughal Empire. The state religion of the Bengal Sultanate was Islam, and the region under its rule, a region that ultimately emerged as the modern nation of Bangladesh, saw a growth of a syncretic form of Islam.[133][134] In the Deccan region, the Hindu kingdom Vijayanagara Empire came to power in 1336 and remained in power through the 16th century, after which it too was reconquered and absorbed into the Mughal Empire.[135][136]\\r\\nAbout 1526, the Punjab governor Dawlat Khan Lodؐ reached out to the Mughal Babur and invited him to attack Delhi Sultanate. Babur defeated and killed Ibrahim Lodi in the Battle of Panipat in 1526. The death of Ibrahim Lodi ended the Delhi Sultanate, and the Mughal Empire replaced it.[137]\\r\\nThe modern history period of South Asia, that is 16th-century onwards, witnessed the start of the Central Asian dynasty named the Mughals, with Turkish-Mongol roots and Sunni Islam theology. The first ruler was Babur, whose empire extended the northwest and Indo-Gangetic Plain regions of South Asia. The Deccan and northeastern region of the South Asia was largely under Hindu kings such as those of Vijayanagara Empire and Ahom kingdom,[138] with some regions such as parts of modern Telangana and Andhra Pradesh under local Sultanates such as the Shia Islamic rulers of Golconda Sultanate.[139]\\r\\nThe Mughal Empire continued its wars of expansion after Babur's death. With the fall of Rajput kingdoms and Vijayanagara, its boundaries reached all of west, as well as the Marathi and Kannada speaking regions of the Deccan peninsula. The Mughal Empire was marked by a period of artistic exchanges and a Central Asian and South Asian architecture synthesis, with remarkable buildings such as the Taj Mahal.[140] It also marked an extended period of religious persecution.[141] Two of the religious leaders of Sikhism, Guru Arjan and Guru Tegh Bahadur were arrested under orders of the Mughal emperors, asked to convert to Islam, and executed when they refused.[142][143][144] Religious taxes on non-Muslims called jizya were imposed. Buddhist, Hindu and Sikh temples were desecrated. However, not all Muslim rulers persecuted non-Muslims. Akbar, a Mughal ruler for example, sought religious tolerance and abolished jizya.[145] After his death, the persecution of non-Muslims in South Asia returned.[146] The persecution and religious violence in South Asia peaked during Aurangzeb era, with him issuing orders in 1669, to all his governors of provinces to \\"destroy with a willing hand the schools and temples of the infidels, and that they were strictly enjoined to put an entire stop to the teaching and practice of idolatrous forms of worship\\".[147][148] In Aurangzeb's time, almost all of South Asia was claimed by the Mughal Empire. However, this claim was violently challenged in various regions of South Asia, particularly by the Sikh Guru Gobind Singh in the northwest,[149] and by Shivaji in the Deccan regions.[150]\\r\\nMaritime trading between South Asia and European merchants began after the Portuguese explorer Vasco de Gama returned to Europe. After the death of Aurangzeb and the collapse of the Mughal Empire, the region came under the rule of many small Islamic sultanates and Hindu kingdoms. British, French, Portuguese colonial interests struck treaties with these rulers, and established their trading ports. In the northwest South Asia, a large region was consolidated into the Sikh Empire by Ranjit Singh.[151][page?needed][152] After his death, the British Empire expanded their interests till the Hindu Kush region. In the east, the Bengal region was split into Muslim East Bengal and Hindu West Bengal, by the colonial British empire, in early 1900s, a split that was reversed. However, after the World War II, at the eve of India's independence, the region was split again into East Pakistan and West Bengal. East Pakistan became Bangladesh in 1971.[153][154]\\r\\nAccording to Saul Cohen, early colonial era strategists treated South Asia with East Asia, but in reality the South Asia region excluding Afghanistan is a distinct geopolitical region separated from other nearby geostrategic realms, one that is geographically diverse.[155] The region is home to a variety of geographical features, such as glaciers, rainforests, valleys, deserts, and grasslands that are typical of much larger continents. It is surrounded by three water bodies?ÿ the Bay of Bengal, the Indian Ocean and the Arabian Sea?ÿ and has acutely varied climate zones. The tip of the Indian Peninsula had the highest quality pearls.[156]\\r\\nThe boundaries of South Asia vary based on how the region is defined. South Asia's northern, eastern, and western boundaries vary based on definitions used, while the Indian Ocean is the southern periphery. Most of this region rests on the Indian Plate and is isolated from the rest of Asia by mountain barriers.[157][158] Much of the region consists of a peninsula in south-central Asia, rather resembling a diamond which is delineated by the Himalayas on the north, the Hindu Kush in the west, and the Arakanese in the east,[159] and which extends southward into the Indian Ocean with the Arabian Sea to the southwest and the Bay of Bengal to the southeast.[30][32]\\r\\nAccording to Robert M. Cutler ÿ a scholar of Political Science at Carleton University,[160] the terms South Asia, Southwest Asia and Central Asia are distinct, but the confusion and disagreements have arisen due to the geopolitical movement to enlarge these regions into Greater South Asia, Greater Southwest Asia and Greater Central Asia. The frontier of Greater South Asia, states Cutler, between 2001ÿ2006 has been geopolitically extended to eastern Iran and western Afghanistan in the west, and in the north to northeastern Iran, northern Afghanistan, and southern Uzbekistan.[160]\\r\\nMost of this region is a subcontinent resting on the Indian Plate, the northerly portion of the Indo-Australian Plate, separated from the rest of the Eurasian Plate. The Indian Plate includes most of South Asia, forming a land mass which extends from the Himalayas into a portion of the basin under the Indian Ocean, including parts of South China and Eastern Indonesia, as well as Kunlun and Karakoram ranges,[161][162][163][page?needed] and extending up to but not including Ladakh, Kohistan, the Hindu Kush range and Balochistan.[164][165][166] It may be noted that geophysically the Yarlung Tsangpo River in Tibet is situated at the outside of the border of the Subcontinental structure, while the Pamir Mountains in Tajikistan are situated inside that border.[167]\\r\\nIt was once a small continent before colliding with the Eurasian Plate about 50ÿ55 million years ago and giving birth to the Himalayan range and the Tibetan plateau. It is the peninsular region south of the Himalayas and Kuen Lun mountain ranges and east of the Indus River and the Iranian Plateau, extending southward into the Indian Ocean between the Arabian Sea (to the southwest) and the Bay of Bengal (to the southeast).\\r\\nThe climate of this vast region varies considerably from area to area from tropical monsoon in the south to temperate in the north. The variety is influenced by not only the altitude, but also by factors such as proximity to the sea coast and the seasonal impact of the monsoons. Southern parts are mostly hot in summers and receive rain during monsoon periods. The northern belt of Indo-Gangetic plains also is hot in summer, but cooler in winter. The mountainous north is colder and receives snowfall at higher altitudes of Himalayan ranges.\\r\\nAs the Himalayas block the north-Asian bitter cold winds, the temperatures are considerably moderate in the plains down below. For most part, the climate of the region is called the Monsoon climate, which keeps the region humid during summer and dry during winter, and favours the cultivation of jute, tea, rice, and various vegetables in this region.\\r\\nSouth Asia is largely divided into four broad climate zones:[169]\\r\\nMaximum relative humidity of over 80% has been recorded in Khasi and Jaintia Hills and Sri Lanka, while the area adjustment to Pakistan and western India records lower than 20%ÿ30%.[169] Climate of South Asia is largely characterized by monsoons. South Asia depends critically on monsoon rainfall.[170] Two monsoon systems exist in the region:[171]\\r\\nThe warmest period of the year precedes the monsoon season (March to mid June). In the summer the low pressures are centered over the Indus-Gangetic Plain and high wind from the Indian Ocean blows towards the center. The monsoons are second coolest season of the year because of high humidity and cloud covering. But, at the beginning of June the jetstreams vanish above the Tibetan Plateau, low pressure over the Indus Valley deepens and the Intertropical Convergence Zone (ITCZ) moves in. The change is violent. Moderately vigorous monsoon depressions form in the Bay of Bengal and make landfall from June to September.[169]\\r\\n(2017)[180]\\r\\n(2016)[181]\\r\\nThis list includes dependent territories within their sovereign states (including uninhabited territories), but does not include claims on Antarctica. EEZ+TIA is exclusive economic zone (EEZ) plus total internal area (TIA) which includes land and internal waters.\\r\\nThe population of South Asia is about 1.749 billion which makes it the most populated region in the world.[182] It is socially very mixed, consisting of many language groups and religions, and social practices in one region that are vastly different from those in another.[183]\\r\\nSouth Asia is home to some of the most populated cities in the world. Delhi, Karachi, Mumbai, and Dhaka are four of the world's largest megacities.\\r\\nThere are numerous languages in South Asia. The spoken languages of the region are largely based on geography and shared across religious boundaries, but the written script is sharply divided by religious boundaries. In particular, Muslims of South Asia such as in Afghanistan and Pakistan use the Arabic alphabet and Persian Nastaliq. Till 1971, Muslim Bangladesh (then known as East Pakistan) too mandated only the Nastaliq script, but thereafter has adopted regional scripts and particularly Bengali. Non-Muslims of South Asia, and some Muslims in India, on the other hand use their traditional ancient heritage scripts such as those derived from Brahmi script for Indo-European languages and non-Brahmi scripts for Dravidian languages and others.[187]\\r\\nThe Nagari script has been the primus inter pares of the traditional South Asian scripts.[188] The Devanagari script is used for over 120 South Asian languages,[189] including Hindi,[190] Marathi, Nepali, Pali, Konkani, Bodo, Sindhi and Maithili among other languages and dialects, making it one of the most used and adopted writing systems in the world.[191] The Devanagari script is also used for classical Sanskrit texts.[189]\\r\\nThe largest spoken language in this region is Hindi, followed by Bengali, Tamil, Telugu, Marathi, Gujarati and Punjabi.[187] In the modern era, new syncretic languages developed in the region such as Urdu that is used by Muslim community of northern Indian subcontinent (particularly Pakistan and northern states of India).[192] The Punjabi language spans three religions: Islam, Hinduism and Sikhism. The spoken language is similar, but it is written in three scripts. The Sikh use Gurmukhi alphabet, Muslim Punjabis in Pakistan use the Nastaliq script, while Hindu Punjabis in India use the Gurmukhi or Ngarؐ script. The Gurmukhi and Nagari scripts are distinct but close in their structure, but the Persian Nastaliq script is very different.[193]\\r\\nEnglish, with British spelling, is commonly used in urban areas and is a major economic lingua franca of South Asia.[194]\\r\\nIn 2010, South Asia had the world's largest population of Hindus, Jains and Sikhs,[12] about 510 million Muslims,[12] as well as over 25 million Buddhists and 35 million Christians.[14] Hindus make up about 68 percent or about 1 billion and Muslims at 31 percent or 510 million of the overall South Asia population,[195][196] while Buddhists, Jains, Christians and Sikhs constitute most of the rest. The Hindus, Buddhists, Jains, Sikhs and Christians are concentrated in India, Nepal, Sri Lanka and Bhutan, while the Muslims are concentrated in Afghanistan (99%), Bangladesh (90%), Pakistan (96%) and Maldives (100%).[12]\\r\\nIndian religions are the religions that originated in the Indian subcontinent; namely Hinduism, Jainism, Buddhism and Sikhism.[197] The Indian religions are distinct yet share terminology, concepts, goals and ideas, and from the Indian subcontinent spread into East Asia and southeast Asia.[197] Early Christianity and Islam were introduced into coastal regions of South Asia by merchants who settled among the local populations. Later Sindh, Balochistan, and parts of the Punjab region saw conquest by the Arab caliphates along with an influx of Muslims from Persia and Central Asia, which resulted in spread of both Shia and Sunni Islam in parts of northwestern region of South Asia. Subsequently, under the influence of Muslim rulers of the Islamic sultanates and the Mughal Empire, Islam spread in South Asia.[198][199]\\r\\nIndia is the largest and fastest growing economy in the region (US$2.180 trillion) and makes up almost 82% of the South Asian economy; it is the world's 7th largest in nominal terms and 3rd largest by purchasing power adjusted exchange rates (US$8.020 trillion).[210] India is the only member of powerful G-20 major economies and BRICS from the region. It is the fastest growing major economy in the world and one of the world's fastest registering a growth of 7.3% in FY 2014ÿ15. Pakistan has the next largest economy($304.3 billion) and the 5th highest GDP per capita in the region,[211] followed by Bangladesh and then by Sri Lanka which has the 2nd highest per capita and is the 4th largest economy in the region. According to a World Bank report in 2015, driven by a strong expansion in India, coupled with favorable oil prices, from the last quarter of 2014 South Asia become the fastest-growing region in the world[212]\\r\\nThe Major Market stock exchanges in the region are Bombay Stock Exchange (BSE) with market Capitalization of $1.68 trillion (11th largest in the world), National Stock Exchange of India (NSE) with market capitalization of $1.64 trillion (12th largest in the world), and Karachi Stock Exchange with market capitalization of $60 billion.[213]\\r\\nEconomic data is sourced from the International Monetary Fund, current as of April 2017, and is given in US dollars.[210]\\r\\n(2017)[180]\\r\\n(2017)[214]\\r\\n(2017)[215]\\r\\nAccording to WHO, South Asia is home to two out of the three countries in the world still affected by polio, Pakistan and Afghanistan, with 306 & 28 polio cases registered in 2014 respectively.[218] Attempts to eradicate polio have been badly hit by opposition from militants in both countries, who say the program is cover to spy on their operations. Their attacks on immunization teams have claimed 78 lives since December 2012.[219]\\r\\nAccording to the World Bank's 2011 report, based on 2005 ICP PPP, about 24.6% of the South Asian population falls below the international poverty line of $1.25/day.[220] Afghanistan and Bangladesh rank the highest, with 30.6% and 43.3% of their respective populations below the poverty line. Bhutan, Maldives and Sri Lanka have the lowest number of people below the poverty line, with 2.4%, 1.5% and 4.1% respectively. India has lifted the most people in the region above the poverty line between 2008 and 2011, around 140 million. As of 2011, 21.9% of India's population lives below the poverty line, compared to 41.6% in 2005.[221][222]\\r\\nThe World Bank estimates that India is one of the highest ranking countries in the world for the number of children suffering from malnutrition. The prevalence of underweight children in India is among the highest in the world, and is nearly double that of Sub Saharan Africa with dire consequences for mobility, mortality, productivity and economic growth.[223]\\r\\nAccording to the World Bank, 70% of the South Asian population and about 75% of South Asia's poor live in rural areas and most rely on agriculture for their livelihood[224] according to the UN's Food and Agricultural Organisation. In 2015, approximately 281 million people in the region were malnourished. The report says that Nepal reached both the WFS target as well as MDG and is moving towards bringing down the number of undernourished people to less than 5% of the population.[216] Bangladesh reached the MDG target with the National Food Policy framework?ÿ with only 16.5% of the population undernourished. In India, the malnourished comprise just over 15 percent of the population. While the number of malnourished people in neighborhood has shown a decline over the last 25 years, the number of under-nourished in Pakistan displays an upward trend.There were 28.7 million hungry in Pakistan in the 1990s?ÿ a number that has steadily increased to 41.3 million in 2015 with 22% of the population malnourished. Approximately 194.6 million people are undernourished in India, which accounts for the highest number of people suffering from hunger in any single country.[216][225]\\r\\nThe 2006 report stated \\"the low status of women in South Asian countries and their lack of nutritional knowledge are important determinants of high prevalence of underweight children in the region\\". Corruption and the lack of initiative on the part of the government has been one of the major problems associated with nutrition in India. Illiteracy in villages has been found to be one of the major issues that need more government attention. The report mentioned that although there has been a reduction in malnutrition due to the Green Revolution in South Asia, there is concern that South Asia has \\"inadequate feeding and caring practices for young children\\".[226]\\r\\nIndia[227][228][229] and Pakistan[230][231] are the dominant political powers in the region. India is by far the largest country in the area covering around three-fourths the land area of the subcontinent.[citation needed] India has the largest population of around three times the combined population of the 6 other countries in the subcontinent.[232] India is also the world's largest democracy[233] India's annual defence budget for 2013ÿ14 is $39.2 billion[234] which is equal to the whole Pakistan's Federal budget of $39.3 billion for 2014ÿ15.[235]\\r\\nBangladesh is a unitary state and parliamentary democracy.[236] Bangladesh also stands out as one of the few Muslim-majority democracies. It is a moderate and generally secular and tolerant  though sometimes this is getting stretched at the moment  alternative to violent extremism in a very troubled part of the world, said Dan Mozena, the U.S. ambassador to Bangladesh. Although Bangladesh's legal code is secular, more citizens are embracing a conservative version of Islam, with some pushing for sharia law, analysts say. Experts say that the rise in conservatism reflects the influence of foreign-financed Islamic charities and the more austere version of Islam brought home by migrant workers in Persian Gulf countries.[237]\\r\\nDiplomacy among the countries of South Asia has been mainly driven by populist politics, with the centre-stage taken by India-Pakistan conflict ever since their independence in 1947, and then the creation of Bangladesh under tense circumstances in 1971. During the height of Cold war, the elite political leaders of Pakistan aligned with the US, while India played crucial role in forming the Non-Aligned Movement and while maintaining goodwill relations with the USSR.\\r\\nPakistan's governance is one of the most conflicted in the region. The military rule and the unstable government in Pakistan has become a concern for the South Asian region. In Nepal, the governance has struggled to come in the side of democracy and it only showed signs in the recent past, basically in the 21st century, to support the democratic system. The political situation in Sri Lanka has been dominated by an increasingly assertive Sinhalese nationalism, and the emergence of a Tamil separatist movement under LTTE, which was suppressed in May 2009. Myanmar's politics is dominated by a military Junta, which has sidelined the democratic forces led by Aung San Suu Kyi.\\r\\nGovernance Indicators (2015)[240]\\r\\nof violence/terrorism","input":"What seven countries make up the subcontinent of south asia?"},{"output":"in 1970","context":"\\"Big Yellow Taxi\\" is a song written, composed, and originally recorded by Canadian singer-songwriter Joni Mitchell in 1970, and originally released on her album Ladies of the Canyon. It was a hit in her native Canada (No. 14) as well as Australia (No. 6) and the UK (No. 11). It only reached No. 67 in the US in 1970, but was later a bigger hit there for her in a live version released in 1974,[3][4] which peaked at No. 24. Charting versions have also been recorded by The Neighborhood (who had the original top US 40 hit with the track in 1970, peaking at No. 29), Maire Brennan, Amy Grant and Counting Crows.\\r\\n\\r\\n\\r\\nMitchell said this about writing the song to journalist Alan McDougall in the early 1970s:\\r\\nI wrote 'Big Yellow Taxi' on my first trip to Hawaii. I took a taxi to the hotel and when I woke up the next morning, I threw back the curtains and saw these beautiful green mountains in the distance. Then, I looked down and there was a parking lot as far as the eye could see, and it broke my heart... this blight on paradise. That's when I sat down and wrote the song.[5]\\r\\nThe song is known for its environmental concern ÿ \\"They paved paradise to put up a parking lot\\" and \\"Hey farmer, farmer, put away that DDT now\\" ÿ and sentimental sound. The line \\"They took all the trees, and put 'em in a tree museum / And charged the people a dollar and a half just to see 'em\\" refers to Foster Botanical Garden in downtown Honolulu, which is a living museum of tropical plants, some rare and endangered.[6]\\r\\nIn the song's final verse, the political gives way to the personal. Mitchell recounts the departure of her \\"old man\\" in the titular \\"big yellow taxi,\\" which may refer to the old Metro Toronto Police patrol cars, which until 1986 were painted yellow.[7] In many covers the departed one may be interpreted as variously a boyfriend, a husband or a father. The literal interpretation is that he is walking out on the singer by taking a taxi; otherwise it is assumed he is being taken away by the authorities.\\r\\nMitchell's original recording was first released as a single and then, as stated above, included on her 1970 album Ladies of the Canyon. A later live version was released in 1974 (1975 in France and Spain) and reached No. 24 on the U.S. charts. Mitchell's playful closing vocals have made the song one of the most identifiable in her repertoire, still receiving significant airplay in Canada. In 2005, it was voted No. 9 on CBC's list of the top 50 essential Canadian tracks.\\r\\nIn 2007, Joni Mitchell released the album Shine, which includes a newly recorded, rearranged version of the song.\\r\\nThere are various slight alterations of the lyrics from different versions. Joni Mitchell's original version runs:\\r\\nThey took all the trees\\r\\nAnd put them in a tree museum\\r\\nThen they charged the people\\r\\nA dollar and a half just to see 'em\\r\\nwhereas in Amy Grant's version, the people are charged \\"twenty-five bucks,\\" and in Mitchell's own 2007 re-recording, the people are charged \\"an arm and a leg.\\"\\r\\nBob Dylan, instead of singing about the \\"big yellow taxi\\" that \\"took away my old man,\\" sings, \\"A big yellow bulldozer took away the house and land.\\" Similarly, in Mitchell's live version of the song released on Miles of Aisles in 1974, she sings about \\"a big yellow tractor\\" that \\"pushed around my house, pushed around my land.\\" She then repeats the same verse, but with the original lyrics. While Amy Grant retains the taxi, her final reprise of the line about \\"paved paradise\\" reads \\"steam rolled paradise.\\"\\r\\nOn the Counting Crows's 2002 cover version, lead singer Adam Duritz sings \\"Late, last night I heard the screen door sway / and a big yellow taxi took my girl away\\" instead of \\"Late last night I heard the screen door slam / and a big yellow taxi took away my old man.\\"\\r\\nAn animated music video of Joni Mitchell's \\"Big Yellow Taxi\\" was produced by John Wilson of Fine Arts Films as an animated short for the Sonny and Cher television show in the mid-1970s. The only commercial release of this full-length music video was in the Video Gems home video release on VHS titled John Wilson's Mini Musicals, also released as The All Electric Music Movie. The home video also contains an animated music video of Mitchell's song \\"Both Sides, Now\\".\\r\\nIn 1993, Mire Moya Brennan covered the song.\\r\\nIn 1995, Amy Grant released a cover of \\"Big Yellow Taxi\\" to pop and Adult Contemporary radio in the United States and United Kingdom. The song was the fourth pop radio single from her House of Love album (the third in the U.S.). Grant's version featured slightly altered lyrics, which she changed at Joni Mitchell's request.[12] The single peaked at No. 67 on The Billboard Hot 100, No. 18 on the Adult Contemporary chart, and at No. 20 in the U.K. Grant also released a music video for the single, which was aired in the U.S. and U.K. and released to home video on Grant's Greatest Videos 1986-2004 DVD. Grant also performed the song for her 2006 concert album, Time Again... Amy Grant Live.\\r\\nIn 2002, the Counting Crows covered the song, on whose backing vocals Vanessa Carlton was featured, and it was featured on the soundtrack to the film Two Weeks Notice and is the most successful version to date (U.S. Billboard Adult Top 40). The single was certified Gold on 25 October 2004 by the Recording Industry Association of America.[13] Originally, the song was a hidden track on the band's 2002 album Hard Candy, and it did not include Carlton until it was to be featured in the film. New releases of the album included it as a track with her added, as with her in the video, although Counting Crows and Carlton neither appeared in the video together nor recorded together. This song became the band's only Top 20 single in the UK, peaking at No. 13. This version slightly changed Mitchell's original lyrics to describe when the eponymous taxi took \\"my girl\\" away, instead of Mitchell's \\"my old man.\\" The original version of the song without Vanessa was included on the album \\"Nolee Mix\\" which was released to promote the My Scene dolls.\\r\\nMany other artists have covered the song.","input":"When did joni mitchell wrote big yellow taxi?"},{"output":"February 15, 2002","context":"\\r\\n\\r\\nSmith & Wesson (S&W) is an American manufacturer of firearms, ammunition and restraints. The corporate headquarters are in Springfield, Massachusetts. Smith & Wesson was founded in 1852 and after various corporate changes is now a unit of American Outdoor Brands Corporation.\\r\\n\\r\\nHorace Smith and Daniel B. Wesson founded the Smith & Wesson Company in Norwich, Connecticut in 1852 to develop the Volcanic rifle. Smith developed a new Volcanic Cartridge, which he patented in 1854. The Smith & Wesson Company was renamed Volcanic Repeating Arms in 1855, and was purchased by Oliver Winchester. Smith left the company and returned to his native Springfield, Massachusetts, while Wesson stayed on as plant manager with Volcanic Repeating Arms.[5]\\r\\n\\r\\nAs Samuel Colt's patent on the revolver was set to expire in 1856, Wesson began developing a prototype for a cartridge revolver. His research pointed out that a former Colt employee named Rollin White held the patent for a \\"Bored-through\\" cylinder, a component he would need for his invention. Wesson reconnected with Smith and the two partners approached White to manufacture a newly designed revolver-and-cartridge combination.[5]\\r\\n\\r\\nRather than make White a partner in their company, Smith and Wesson paid him a royalty of $0.25 on every revolver that they made. It would become White's responsibility to defend his patent in any court cases which eventually led to his financial ruin, but was very advantageous for the new Smith & Wesson Company.[5]\\r\\n\\r\\nSmith & Wesson's revolvers came into popular demand with the outbreak of the American Civil War as soldiers from all ranks on both sides of the conflict made private purchases of the revolvers for self-defense.[6]\\r\\n\\r\\nThe orders for the Smith & Wesson Model 1 revolver outpaced the factory's production capabilities. In 1860 demand was so great that Smith & Wesson expanded into a new facility and began experimenting with a new cartridge design more suitable than the .22 Short that it had been using.[6]\\r\\n\\r\\nAt the same time, the company's design was being infringed upon by other manufacturers which led to numerous lawsuits filed by Rollin White. In many of these instances part of the restitution came in the form of the offender being forced to stamp \\"Manufactured for Smith & Wesson\\" on the revolvers in question.[6]\\r\\n\\r\\nWhite's vigorous defense of his patent caused a problem for armsmakers in the United States at the time as they could not manufacture cartridge revolvers. At the end of the war the U.S. Government charged White with causing the retardation of arms development in America.[6]\\r\\n\\r\\nDemand for revolvers declined at the close of the Civil War so Smith & Wesson focused on the development of arms suitable for use on the American frontier. In 1870 the company switched focus from pocket sized revolvers to a large frame revolver in heavier calibers (.44 S&W American).  This new design, known as the Smith & Wesson Model 3, was adopted by the US Army as the first cartridge-firing revolver in US service.\\r\\n\\r\\nIn 1899 Smith & Wesson introduced its most widely used revolver, the .38 Military & Police (also known as the Smith & Wesson Model 10).  With over 6 million produced, it became the standard sidearm of American police officers for much of the 20th century.[7]  An additional 1 million of these guns were made for the US Military during World War II.[7]\\r\\n\\r\\nThe post-war periods in the 20th century were times of great innovation for the company.  In 1935 Smith and Wesson released the Smith & Wesson Model 27 which was the first revolver chambered for .357 Magnum.  It was designed as a more powerful handgun for law enforcement officers.  The Model 27 started the \\"Magnum Era\\" of handguns.  The high point was in 1955 when the company created the Smith & Wesson Model 29 in .44 Magnum.  Two decades later the Dirty Harry movies made this gun a cultural icon.[8]\\r\\n\\r\\nIn 1965 the Wesson family sold its controlling interest in Smith and Wesson to Bangor Punta, a large American conglomerate.[9] Over the next decade Bangor Punta diversified the company's civilian sales to include related gun products (such as holsters) as well as offering additional police equipment (such as handcuffs and breathalyzers).[7]  By the late 1970s these profitable moves made Smith and Wesson \\"the envy of the industry\\" according to Business Week.[10]\\r\\n\\r\\nDespite all of these advantages, however, Smith & Wesson's market share began to decline in the 1980s.  As the war on drugs intensified in the United States, police departments all across the country replaced their Smith and Wesson revolvers with European semiautomatics (such as Glock, Sig Sauer and Beretta).[11]  From 1982 to 1986 profits at the company declined by 41 percent [7] Company ownership changed twice during the decade.\\r\\n\\r\\nIn June 1987 Tomkins plc paid $112.5 million to purchase Smith & Wesson.[12]  Tomkins modernized the production equipment and instituted additional testing which significantly increased product quality.[7]  However new gun sales in the United States lagged in the 1990s, some of which was attributed to the Federal Assault Weapons Ban of 1994.  Also there were numerous city and state lawsuits against Smith and Wesson.  After the success of the Tobacco Master Settlement Agreement, municipalities thought they might be able to succeed through tort law against the gun industry as well.[13]  These political and legal challenges provided a lot of risk and uncertainty for Smith and Wesson's future.\\r\\n\\r\\nOn March 17, 2000, Smith & Wesson made an agreement with U.S. President Bill Clinton under which it would implement changes in the design and distribution of its firearms in return for \\"preferred buying program\\" to offset the loss of revenue as a result of anticipated boycott.[14]  The agreement stated all authorized dealers and distributors of Smith & Wesson's products had to abide by a \\"code of conduct\\" to eliminate the sale of firearms to prohibited persons, and dealers had to agree to not allow children under 18 (without an adult present) access to gun shops or sections of stores that contained firearms.[14]\\r\\n\\r\\nAfter an organized campaign by the NRA and NSSF over the issue of smart guns,[15] thousands of retailers and tens of thousands of firearms consumers boycotted Smith & Wesson.[16][17] CEO Ed Shultz, who negotiated the deal, was forced out in September of that year.[18]  By December 2000, the company's stock price was 19 cents per share.[19]  Smith & Wesson dropped its smart gun plans after nearly being driven out of business.[20]\\r\\n\\r\\nOn May 11, 2001, Saf-T-Hammer Corporation acquired Smith & Wesson Corp. from Tomkins plc for US$15?million, a fraction of the US$112?million originally paid by Tomkins.[21] Saf-T-Hammer assumed US$30?million in debt, bringing the total purchase price to US$45?million.[22][23] Saf-T-Hammer, a manufacturer of firearms locks and other safety products, purchased the company with the intention of incorporating its line of security products into all Smith & Wesson firearms in compliance with the 2000 agreement.\\r\\n\\r\\nThe acquisition of Smith & Wesson was chiefly brokered by Saf-T-Hammer President Bob Scott, who had left Smith & Wesson in 1999 because of a disagreement with Tomkins policies. After the purchase, Scott became the president of Smith & Wesson to guide the 157-year-old company back to its former standing in the market.[24]\\r\\n\\r\\nOn February 15, 2002, the name of the newly formed entity was changed to Smith & Wesson Holding Corporation.[25]\\r\\n\\r\\nIn 2006 Smith & Wesson refocused its marketing on big box retailers, according to Smith & Wesson CEO Mike Golden in a 2008 conference call with investors.[26]\\r\\n\\r\\nSmith & Wesson Holding announced in December 2014 that it was paying $130.5 million for Battenfeld Technologies, a Columbia, Missouri-based designer and distributor of hunting and shooting accessories. The company made the acquisition with the eventual intent to merge all its existing Smith & Wesson, M&P and Thompson Center Arms accessories into a single division.[27]\\r\\n\\r\\nIn August 2016 the company bought Crimson Trace, a laser-sight manufacturer, for $95 million and Taylor Brands, a tool and knife maker, for $85 million. In November of that same year the company bought UST Brands, a survival equipment maker, for $32.3 million. On November 7, 2016, Smith & Wesson announced that it would be changing the name of its holding company to American Outdoor Brands Corporation.[28]\\r\\n\\r\\nIn 2017 Smith & Wesson saw a severe contraction in its sales as units shipped to distributors and retailers declined 38.3%. The company was forced to lay off one-fourth of its manufacturing workforce.[29]\\r\\n\\r\\nThe company has come under increased scrutiny due to the frequent use of its firearms in mass shootings such as the 2018 Stoneman Douglas High School shooting, in which 19-year-old Nikolas Jacob Cruz used a Smith & Wesson AR-15 style rifle, the semi-automatic M&P15. The same weapon was used in the 2015 San Bernardino attack and the 2012 Aurora shooting.[30][31][32][33]\\r\\n\\r\\nSmith & Wesson Volcanic, caliber .31, between 1854 and 1855\\r\\n\\r\\nSmith & Wesson Model 1 Second Issue, .22 rimfire\\r\\n\\r\\nSmith & Wesson Army No 2, made 1863, caliber .32 Rimfire\\r\\n\\r\\nSmith & Wesson No. 3, New Model, 44 Russian\\r\\n\\r\\nSmith & Wesson Model 3, Cal. .44, between 1874 and 1878\\r\\n\\r\\nSmith & Wesson .38 Special Model 1899 Military and Police Hand Ejector\\r\\n\\r\\nSmith & Wesson M1917 cal. 45\\r\\n\\r\\nSmith & Wesson Model 10 cal. 38\\r\\n\\r\\nSmith & Wesson has produced revolvers over the years in several standard frame sizes. M refers to the small early Ladysmith frame, I to the small .32 frame, J to the small .38 frame, K to the medium .38 frame, L to medium large, and N to the largest .44 Magnum type frame.[38] In 2003, the even larger X frame was introduced for the .500 S&W Magnum.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nMost Smith & Wesson revolvers have been equipped with an internal locking mechanism since the acquisition by Saf-T-Hammer. The mechanism is relatively unobtrusive, is activated with a special key, and renders the firearm inoperable. While the lock can simply be left disengaged, most gun enthusiasts prefer \\"pre-lock\\" guns.[50][51]\\r\\n\\r\\nIn 1953 the U.S. Army was looking for a pistol to replace the Colt 1911A1.[39]  To obtain a bid from the U.S. Government, Smith & Wesson began working on a design similar to the German Walther P38.[39]  A year later the Army dropped its search and Smith & Wesson introduced its pistol to the civilian shooting market as the Model 39.[39]\\r\\n\\r\\nThe Model 39 would come to be known as a first-generation pistol. Since the Model 39 debuted, Smith & Wesson has continuously developed this design into its third-generation pistols now on the market. The first-generation models use a 2-digit model number, the second generation use 3 digits, and third-generation models use 4 digits.\\r\\n\\r\\nAlong with the myriad smaller configurations, the mid-sized 4516, 457, the Chiefs Special CS45, and the decocker equipped, 4546, 4566 and 4576, and the 45 TSW, the 4553, stll being issued to the West Virginia State Troopers.[53]\\r\\n\\r\\nFor many of the second-generation models, the first digit identified the material used in the frame;[citation needed] thus a first digit of 4 indicated an alloy, a first digit of 5 indicated blued steel, and a first digit of 6 indicated stainless steel.  For most of the third-generation models, the first two digits identified the caliber (except for 59/69 for 9mm), the last two digits were for the action style and the material, respectively.  Action style numbers were typically 0 for the standard double/single-action and 4 for double-action-only.  Material numbers were commonly 3 for aluminium, 4 for blued steel, and 6 for stainless steel.[citation needed]\\r\\n\\r\\nSmith & Wesson introduced the Sigma series of recoil-operated, locked-breech semi-auto pistols in 1994 with the Sigma SW40F, followed by the Sigma SW9F 9?mm, which included a 17-shot magazine.[39] Glock initiated a patent infringement lawsuit against Smith & Wesson. The latter paid an undisclosed amount to settle the case and for the right to continue producing models in the Sigma line.[54] The gun frame is manufactured from polymer, while the slide and barrel use either stainless steel or carbon steel. In 1996, Smith & Wesson updated the Sigma by adding a compact model with a shortened barrel (from 4?1?2 to 4?inches) and again, in 1999, modified the series by changing the grip by adding checkering and adding an integral accessory rail for lights and laser targeting devices.[39]\\r\\n\\r\\nS&W reached an agreement with Walther to produce variations of the P99 line of pistols.[39] Branded as the SW99, the pistol is available in several calibers, including 9?mm, .40 S&W, and .45 ACP, and in both full size and compact variations. Under the terms of the agreement, Walther produced the frames, and Smith & Wesson produced the slide and barrel. The pistol has several cosmetic differences from the original Walther design and strongly resembles a hybrid between the P99 and the Sigma series.[39]\\r\\n\\r\\nIn 2005, Smith & Wesson debuted a new polymer-frame pistol intended for the law enforcement market. Dubbed the M&P (for Military and Police), its name was meant to evoke S&W's history as the firearm of choice for law enforcement agencies through its previous lineup of M&P revolvers. The M&P is a completely new design with no parts interchangeable with any other pistol including the Sigma. The new design not only looks completely different than the Sigma but feels completely different with 3 different back straps supplied with each M&P. Many of the ergonomic study elements that had been incorporated into the Sigma and the SW99 were brought over to the M&P. The improved trigger weight and feel, and unique takedown method (not requiring a dry pull of the trigger) were meant to set the M&P apart from both the Sigma and the popular Glock pistols.\\r\\n\\r\\nThe M&P is available in 9G19mm, .40 S&W, and .357 SIG. Also a .22 LR M&P was developed with Carl Walther and is made in Germany. A .45 ACP model was released in early 2007, after making its debut at the SHOT Show. In addition, compact versions are available in .22LR, 9G19mm, .40 S&W, .357 SIG, and .45 ACP. The .22LR Compact is made by Smith & Wesson in the United States. Subcompact versions are available in 9G19mm, .40 S&W and .45 ACP.\\r\\n\\r\\nSmith & Wesson introduced the SD VE series in 2012 in hopes of remaking and improving the discontinued Smith & Wesson SD. The SD VE design has an improved self-defense trigger and a comfortable, ergonomic, textured grip. The SD VE also features an improved stainless steel barrel and slide that the SD did not include. The Smith & Wesson SD VE is available in 9G19mm and .40 S&W calibers in either a standard-capacity version (16+1-round capacity for SD9 VE and 14+1 for SD40 VE) or in the low-capacity version (10+1-round capacity for both calibers.)\\r\\n\\r\\nIn 2003, Smith & Wesson introduced their variation of the classic M1911 .45 ACP semi-automatic handgun, the SW1911. This firearm retains the M1911's well known dimensions, operation, and feel, while adding a variety of modern touches. Updates to the design include serration at the front of the slide for easier operation and disassembly, a high \\"beaver-tail\\" grip safety, external extractor, lighter weight hammer and trigger, as well as updated internal safeties to prevent accidental discharges if dropped. S&W 1911s are available with black finished carbon steel slides and frames or bead blasted stainless slides and frames. They are available with aluminum frames alloyed with scandium in either natural or black finishes. These updates have resulted in a firearm that is true to the M1911 design, with additions that would normally be considered \\"custom\\", with a price similar to equivalent designs from other manufacturers.\\r\\n\\r\\nSmith & Wesson's Performance Center produces the top of the line hand fitted competition version knowns as the PC 1911. While most 1911s run around 38 to 39 ounces (1,100 to 1,100?g), the PC 1911 is heavier, at approximately 41 ounces (1,200?g). The full-length guide rod adds some weight, and so does the add-on magazine well.\\r\\n\\r\\nDuring the early years of WW2, Smith & Wesson manufactured batches of the Model 1940 Light Rifle under request from the British Government.[55]\\r\\n\\r\\nIn January 2006, Smith & Wesson reentered the rifle market with its M&P15 series of rifles based on the AR-15. Unveiled at SHOT Show 2006, the rifle debuted in two varieties: the M&P15 and the M&P15T. The two are basically the same rifle, chambered in 5.56 NATO, with the T model featuring folding sights and a four-sided accessories rail. These rifles were first produced by Stag Arms but marketed under the Smith & Wesson name.[56] Currently Smith & Wesson makes the lower receiver in-house while the barrel is supplied by Thompson/Center, a S&W company.\\r\\n\\r\\nIn May 2008, Smith & Wesson introduced its first AR-variant rifle in a caliber other than 5.56 NATO. The M&P15R is a standard AR-15 rifle chambered for the 5.45G39mm cartridge.[57] In 2009, it released the M&P15-22, chambered for .22 Long Rifle.[58]\\r\\n\\r\\nSmith & Wesson manufactured a line of bolt-action rifles called the i-Bolt.[citation needed] These synthetic-stock rifles were available in .25-06, .270 Win, or .30-06 caliber.\\r\\n\\r\\nIn 1967 Smith & Wesson produced a 9mm submachine gun, hoping to capitalize on U.S. sales of the Israeli Uzi and HK MP5. It borrowed the magazine of the Carl Gustav M/45 submachine gun (Kulsprutepistol m/45 or Kpist m/45) which had been popular with the U.S. forces in Vietnam as the \\"Swedish K\\") and made a similar side-folding stock. But the rest of the straight blowback weapon had no parts in common with the earlier Swedish gun. The S&W Model 76 submachine gun was made in limited numbers and was primarily used as a police weapon. Because all of them were made prior to 1986, many of them made it into civilian hands in the United States and are commonly used in submachine gun competition.[44]\\r\\n\\r\\nSmith & Wesson bought patents and tooling for a 12 ga. shotgun design from Noble Manufacturing Co. in 1972 and produced it as the Model 916, 916T, and 916A.[59] The guns were plagued by a variety of quality issues, including a recall due to a safety issue with barrels rupturing.[60] The 916 series was discontinued, then later replaced by the Models 3000, based on an improved Remington 870 design, and 1000 intended to compete with the popular Remington Model 1100; both were produced by Howa of Japan.[61] However, with the sale of the company to British Tomkins PLC, Smith & Wesson exited the shotgun market in the mid'80s to return to their \\"core\\" market of handguns.\\r\\n\\r\\nDuring the 1980s, Smith & Wesson released the S&W assault shotgun, which had full automatic capability.\\r\\n\\r\\nIn November 2006, S&W announced that it would reenter the shotgun market with two new lines of shotguns, the Elite series and the 1000 series, unveiled at the 2007 SHOT Show. The 1000 series was discontinued in 2009. Along with the new shotguns, S&W debuted the Heirloom Warranty program, a first of its kind in the firearms industry. The warranty provides both the original buyer and the buyer's chosen heir with a lifetime warranty on all Elite Series shotguns.[62]\\r\\n\\r\\nSmith & Wesson is also a manufacturer of restraints (handcuffs, leg irons, belly chains, prisoner transport chains). Smith & Wesson first manufactured handcuffs for the Peerless handcuff company which obtained the right to produce the first swinging-bow handcuffs patented by George A. Carney in 1912. Peerless did not have the facilities necessary for production so they contracted Smith & Wesson to manufacture the handcuffs for them.[63] When Peerless set up its own production plant, Smith & Wesson continued to produce Peerless-type handcuffs under their own brand.[64]\\r\\n\\r\\nSmith & Wesson markets firearm accessories, safes, apparel, watches, collectibles, knives, axes, tools, air guns, emergency lightbars, and other products under its brand name.[citation needed]\\r\\n\\r\\nJohn Wilson and Roy G. Jinks designed the Smith & Wesson model 6010 Bowie knife in 1971 and the 1973 Texas Ranger Bowie knife. Blackie Collins designed the subsequent model 6020 and 6060 Survival knife in 1974ÿ1979. All of these limited-production and custom knives were made at the Springfield, Mass., United States factory.[citation needed]\\r\\n\\r\\nIn October 2002, Smith & Wesson announced it had entered into a licensing agreement with Cycle Source Group to produce a line of bicycles designed by and for law enforcement. These bicycles had custom configurations and silent hubs.[65][66]\\r\\n\\r\\nSmith & Wesson flashlights are available to the general public. They are designed and produced by PowerTech, Inc, in Collierville, Tennessee.[67]\\r\\n\\r\\nSmith & Wesson has a line of wood pellet grills named after various pistol cartridges, such as .22 Magnum, .38 Special, .44 Magnum, .357 Magnum, and .500 Magnum.[68]\\r\\n\\r\\nSmith & Wesson has entered into a licensing agreement with North Carolina-based Wellco Enterprises to design and distribute a full line of tactical law enforcement footwear.[69]","input":"When did smith and wesson change their name?"},{"output":"Michael J. Harney","context":"","input":"Orange is the new black cast correctional officers?"},{"output":"amusement","context":"A roller coaster is a type of amusement ride that employs a form of elevated railroad track designed with tight turns, steep slopes, and sometimes inversions.[1] People ride along the track in open cars, and the rides are often found in amusement parks and theme parks around the world.[1] LaMarcus Adna Thompson obtained one of the first known patents for a roller coaster design in 1885, related to the Switchback Railway that opened a year earlier at Coney Island.[2][3] The track in a coaster design does not necessarily have to be a complete circuit, as shuttle roller coasters demonstrate. Most roller coasters have multiple cars in which passengers sit and are restrained.[4] Two or more cars hooked together are called a train. Some roller coasters, notably wild mouse roller coasters, run with single cars.\\r\\n\\r\\n\\r\\nThe oldest roller coasters are believed to have originated from the so-called \\"Russian Mountains\\", specially constructed hills of ice located in the area that is now St. Petersburg.[5] Built in the 17th century, the slides were built to a height of between 21 and 24?m (70 and 80 feet), had a 50-degree drop, and were reinforced by wooden supports. Later, in 1784, Catherine the Great is said to have constructed a sledding hill in the gardens of her palace at Oranienbaum in St. Petersburg.[6] The name Russian Mountains to designate a roller coaster is preserved in many languages (e.g. the Spanish monta?a rusa), but the Russian term for roller coasters is ҿҿ ̾ѿ (\\"amerikanskiye gorki\\"), meaning \\"American mountains.\\"\\r\\nThe first modern roller coaster, the Promenades Aeriennes, opened in Parc Beaujon in Paris on July 8, 1817.[7] It featured wheeled cars securely locked to the track, guide rails to keep them on course, and higher speeds.[8] It spawned half a dozen imitators, but their popularity soon declined.\\r\\nHowever, during the Belle Epoque they returned to fashion. In 1887 French entrepreneur Joseph Oller, co-founder of the Moulin Rouge music hall, constructed the Montagnes Russes de Belleville, \\"Russian Mountains of Belleville\\" with two hundred meters of track laid out in a double-eight, later enlarged to four figure-eight-shaped loops.[9]\\r\\nIn 1827, a mining company in Summit Hill, Pennsylvania constructed the Mauch Chunk Switchback Railway, a downhill gravity railroad used to deliver coal to Mauch Chunk, Pennsylvania ÿ now known as Jim Thorpe.[10] By the 1850s, the \\"Gravity Road\\" (as it became known) was selling rides to thrill seekers. Railway companies used similar tracks to provide amusement on days when ridership was low.\\r\\nUsing this idea as a basis, LaMarcus Adna Thompson began work on a gravity Switchback Railway that opened at Coney Island in Brooklyn, New York in 1884.[11] Passengers climbed to the top of a platform and rode a bench-like car down the 600-foot (180?m) track up to the top of another tower where the vehicle was switched to a return track and the passengers took the return trip.[12] This track design was soon replaced with an oval complete circuit.[8] In 1885, Phillip Hinkle introduced the first full-circuit coaster with a lift hill, the Gravity Pleasure Road, which became the most popular attraction at Coney Island.[8] Not to be outdone, in 1886 Thompson patented his design of roller coaster that included dark tunnels with painted scenery. \\"Scenic Railways\\" were soon found in amusement parks across the county.[8]\\r\\nBy 1919, the first underfriction roller coaster had been developed by John Miller.[13] Soon, roller coasters spread to amusement parks all around the world. Perhaps the best known historical roller coaster, Cyclone, was opened at Coney Island in 1927.\\r\\nThe Great Depression marked the end of the golden age of roller coasters, and theme parks in general went into decline. This lasted until 1972, when the instant success of The Racer at Kings Island near Cincinnati began a roller coaster renaissance which has continued to this day.\\r\\nIn 1959, Disneyland introduced a design breakthrough with Matterhorn Bobsleds, the first roller coaster to use a tubular steel track. Unlike wooden coaster rails, tubular steel can be bent in any direction, allowing designers to incorporate loops, corkscrews, and many other maneuvers into their designs. Most modern roller coasters are made of steel, although wooden coasters and hybrids are still being built.\\r\\nThere are several explanations of the name roller coaster. It is said to have originated from an early American design where slides or ramps were fitted with rollers over which a sled would coast.[8] This design was abandoned in favor of fitting the wheels to the sled or other vehicles, but the name endured.\\r\\nAnother explanation is that it originated from a ride located in a roller skating rink in Haverhill, Massachusetts in 1887. A toboggan-like sled was raised to the top of a track which consisted of hundreds of rollers. This Roller Toboggan then took off down gently rolling hills to the floor. The inventors of this ride, Stephen E. Jackman and Byron B. Floyd, claim that they were the first to use the term \\"roller coaster\\".[12]\\r\\nThe term jet coaster is used for roller coasters in Japan, where such amusement park rides are very popular.[14]\\r\\nIn many languages, the name refers to \\"Russian mountains\\". Contrastingly, in Russian, they are called \\"American mountains\\". In Scandinavian languages, the roller coaster is referred as \\"mountain-and-valley railway\\".\\r\\nThe cars on a typical roller coaster are not self-powered. Instead, a standard full circuit coaster is pulled up with a chain or cable along the lift hill to the first peak of the coaster track. The potential energy accumulated by the rise in height is transferred to kinetic energy as the cars race down the first downward slope. Kinetic energy is then converted back into potential energy as the train moves up again to the second peak. This hill is necessarily lower, as some mechanical energy is lost to friction.\\r\\nNot all rides feature a lift hill, however. The train may be set into motion by a launch mechanism such as a flywheel launch, linear induction motors, linear synchronous motors, hydraulic launch, compressed air launch or drive tire. Such launched coasters are capable of reaching higher speeds in a shorter length of track than those featuring a conventional lift hill. Some roller coasters move back and forth along the same section of track; these are known as shuttles and usually run the circuit once with riders moving forwards and then backwards through the same course.\\r\\nA properly designed ride under good conditions will have enough kinetic, or moving, energy to complete the entire course, at the end of which brakes bring the train to a complete stop and it is pushed into the station. A brake run at the end of the circuit is the most common method of bringing the roller coaster ride to a stop. One notable exception is a powered roller coaster. These rides, instead of being powered by gravity, use one or more motors in the cars to propel the trains along the course.\\r\\nIf a continuous-circuit coaster does not have enough kinetic energy to completely travel the course after descending from its highest point (as can happen with high winds or increased friction), the train can valley: that is, roll backwards and forwards along the track, until all kinetic energy has been released. The train will then come to a complete stop in the middle of the track. This, however, works somewhat differently on a launched coaster. When a train launcher does not have enough potential energy to launch the train to the top of an incline, the train is said to roll back.\\r\\nIn 2006, NASA announced that it would build a system using principles similar to those of a roller coaster to help astronauts escape the Ares I launch pad in an emergency[15], although this has since been scrapped along with the rest of the Ares program.\\r\\nMany safety systems are implemented in roller coasters. One of these is the block system. Most large roller coasters have the ability to run two or more trains at once, and the block system prevents these trains from colliding. In this system, the track is divided into several sections, or blocks. Only one train at a time is permitted in each block. At the end of each block, there is a section of track where a train can be stopped if necessary (either by preventing dispatch from the station, closing brakes, or stopping a lift). Sensors at the end of each block detect when a train passes so that the computer running the ride is aware of which blocks are occupied. When the computer detects a train about to travel into an already occupied block, it uses whatever method is available to keep it from entering. The trains are fully automated.\\r\\nThe above can cause a cascade effect when multiple trains become stopped at the end of each block. In order to prevent this problem, ride operators follow set procedures regarding when to release a newly loaded train from the station. One common pattern, used on rides with two trains, is to do the following: hold train #1 (which has just finished the ride) right outside the station, release train #2 (which has loaded while #1 was running), and then allow #1 into the station to unload safely.\\r\\nAnother key to safety is the control of the roller coaster's operating computers: programmable logic controllers (often called PLCs). A PLC detects faults associated with the mechanism and makes decisions to operate roller coaster elements (e.g. lift, track-switches and brakes) based on configured state and operator actions. Periodic maintenance and inspection are required to verify structures and materials are within expected wear tolerances and are in sound working order. Sound operating procedures are also a key to safety.\\r\\nRoller coaster design requires a working knowledge of basic physics to avoid uncomfortable, even potentially fatal, strain to the rider. Ride designers must carefully ensure the accelerations experienced throughout the ride do not subject the human body to more than it can handle. The human body needs time to detect changes in force in order to control muscle tension. Failure to take this into account can result in severe injuries such as whiplash. The accelerations accepted in roller coaster design are generally in the 4-6Gs (40ÿ60 m s?2) range for positive vertical (pushing you into your seat), and 1.5-2Gs (15ÿ20 m s?2) for the negative vertical (flying out of your seat as you crest a hill).[citation needed] This range safely ensures the majority of the population experiences no harmful side effects. Lateral accelerations are generally kept to a minimum by banking curves. The neck's inability to deal with high forces leads to lateral accelerations generally limited to under 1.8Gs.[citation needed] Sudden accelerations in the lateral plane result in a rough ride.\\r\\nDespite safety measures, accidents can, and do, occur.[16] Regulations concerning accident reporting vary from one authority to another. Thus in the US, California requires amusement parks to report any ride-related accident that requires an emergency room visit, while Florida exempts parks whose parent companies employ more than 1000 people from having to report any accidents at all.[citation needed] Rep. Ed Markey of Massachusetts has introduced legislation that would give oversight of rides to the Consumer Product Safety Commission (CPSC).[citation needed]\\r\\nRide accidents can also be caused by riders themselves or ride operators not following safety directions properly, and, in extremely rare cases, riders can be injured by mechanical failures. In recent years, controversy has arisen about the safety of increasingly extreme rides. There have been suggestions that these may be subjecting passengers to translational and rotational accelerations that may be capable of causing brain injuries. In 2003 the Brain Injury Association of America concluded in a report that \\"There is evidence that roller coaster rides pose a health risk to some people some of the time. Equally evident is that the overwhelming majority of riders will suffer no ill effects.\\"[17]\\r\\nA similar report in 2005 linked roller coasters and other thrill rides with potentially triggering abnormal heart conditions that could lead to death.[18] Autopsies have shown that recent deaths at various Disney parks, Anheuser-Busch parks, and Six Flags parks were due to previously undetected heart ailments.\\r\\nStatistically, roller coasters are very safe compared to other activities. The U.S. Consumer Product Safety Commission estimates that 134 park guests required hospitalization in 2001 and that fatalities related to amusement rides average two per year. According to a study commissioned by Six Flags, 319 million people visited parks in 2001. The study concluded that a visitor has a one in one-and-a-half billion chance of being fatally injured, and that the injury rates for children's wagons, golf carts, and folding lawn chairs are higher than for amusement rides.[19]\\r\\nRoller coasters are divided into two main categories: steel roller coasters and wooden roller coasters. Steel coasters have tubular steel tracks, and compared to wooden coasters, they are typically known for offering a smoother ride and their ability to turn riders upside-down. Wooden coasters have flat steel tracks, and are typically renowned for producing \\"air time\\" through the use of negative G-forces when reaching the crest of some hill elements. Newer types of track, such as I-Box and Topper introduced by Rocky Mountain Construction, improve the ride experience on wooden coasters, lower maintenance costs, and add the ability to invert riders.\\r\\nModern roller coasters are constantly evolving to provide a variety of different experiences. More focus is being placed on the position of riders in relation to the overall experience. Traditionally, riders sit facing forward, but newer variations such as stand-up and flying models position the rider in different ways to change the experiences. A flying model, for example, is a suspended roller coaster where the riders lie facing forward and down with their chests and feet strapped in. Other ways of enhancing the experience involve removing the floor beneath passengers riding above the track, as featured in floorless roller coasters. Also new track elements ÿ usually types of inversions ÿ are often introduced to provide entirely new experiences.\\r\\nSeveral height classifications have been used by parks and manufacturers in marketing their roller coasters, as well as enthusiasts within the industry. One classification, the kiddie coaster, is a roller coaster specifically designed for younger riders. Following World War II, parks began pushing for more of them to be built in contrast to the height and age restrictions of standard designs at the time. Companies like Philadelphia Toboggan Company (PTC) developed scaled-down versions of their larger models to accommodate the demand. These typically featured lift hills smaller than 25 feet (7.6?m), and still do today. The rise of kiddie coasters soon led to the development of \\"junior\\" models that had lift hills up to 45 feet (14?m). A notable example of a junior coaster is the Sea Dragon ÿ the oldest operating roller coaster from PTC's legendary designer John Allen ÿ which opened at Wyandot Lake in 1956 near Powell, Ohio.[12]\\r\\nA hypercoaster, occasionally stylized as hyper coaster, is a type of roller coaster with a height or drop of at least 200 feet (61?m). Moonsault Scramble, which debuted at Fuji-Q Highland in 1984, was the first to break this barrier, though the term hypercoaster was first coined by Cedar Point and Arrow Dynamics with the opening of Magnum XL-200 in 1989.[20][21] Hypercoasters have become one of the most predominant types of roller coasters in the world, now led by manufacturers Bolliger & Mabillard and Intamin.\\r\\nA giga coaster is a type of roller coaster with a height or drop of at least 300 feet (91?m).[22] The term was coined during a partnership between Cedar Point and Intamin on the construction of Millennium Force.[23][24] Although Morgan and Bolliger & Mabillard have not used the term giga,[25] both have also produced roller coasters in this class.\\r\\nA strata coaster is a type of roller coaster with a height or drop of at least 400 feet (120?m).[22] As with the other two height classifications, the term strata was first introduced by Cedar Point with the release of Top Thrill Dragster, a 420-foot-tall (130?m) roller coaster that opened in 2003.[28] Another strata coaster, Kingda Ka, opened at Six Flags Great Adventure in 2005 as the tallest roller coaster in the world featuring a height of 456 feet (139?m). Superman: Escape From Krypton exceeded 400 feet (120?m) back when it opened in 1997, but its shuttle coaster design where the trains fail to travel a complete circuit usually prevents the roller coaster from being classified in the same category.[28][29]\\r\\nRiding Fahrenheit, located at Hersheypark in Hershey, Pennsylvania.\\r\\nHypersonic XLC, the world's first production Thrust Air 2000 (now defunct)\\r\\nTop Thrill Dragster at Cedar Point is the first strata coaster ever built.\\r\\nRiding Expedition GeForce at Holiday Park, Germany.\\r\\nRaptor, a steel inverted coaster, is located at Cedar Point in Sandusky, Ohio.\\r\\nNew Texas Giant at Six Flags Over Texas before being refurbished into a hybrid steel-wood coaster.\\r\\nLightning Racer at Hersheypark is a racing, dueling roller coaster made by GCI.\\r\\nThis all-wooden roller coaster, built in 1951, dominates the Linnanm?ki amusement park in Helsinki, Finland.\\r\\nConey Island Cyclone in Brooklyn, New York was built in 1927 and refurbished in 1975.\\r\\nSon of Beast in Kings Island was the only wooden coaster to have a vertical loop. The loop was removed in 2006, and the ride was closed from 2009 until its demolition in 2012.\\r\\nJack Rabbit at Kennywood Park outside of Pittsburgh, Pennsylvania was built in 1920.\\r\\nPhoenix, built in 1947, at Knoebles Grove in Elysburg, Pennsylvania. It was relocated from San Antonio's Playland Park in 1984.\\r\\nOblivion at Alton Towers in Staffordshire, England.\\r\\nGriffon splashing down into a pool at Busch Gardens Williamsburg.\\r\\nGreat Bear is the first steel inverted coaster in Pennsylvania, located at Hersheypark.\\r\\nBehemoth, at Canada's Wonderland, was the highest (70?m or 230?ft) and fastest (124?km/h or 77?mph) coaster in Canada before Leviathan opened.\\r\\n\\"Montu\\", a popular inverted roller coaster at Busch Gardens Tampa Bay\\r\\nBlack Mamba at Phantasialand, Germany\\r\\nEuro-Mir, a spinning roller coaster at Europa-Park in Rust, Germany\\r\\nDragon Khan at PortAventura in Salou (Tarragona), Spain\\r\\nThunderbolt at Kennywood outside of Pittsburgh, Pennsylvania was built in 1968.\\r\\nLeviathan, also at Canada's Wonderland, is the current biggest coaster in Canada (93?m or 306?ft, 148?km/h or 92?mph) and is also made by Bolliger & Mabillard. It is the second biggest coaster before Fury 325 at Carowinds over 91?m (300?ft) made by B&M.\\r\\nKingda Ka is the world's tallest roller coaster and is the second strata coaster in the world after Top Thrill Dragster.\\r\\nExpedition Everest, a roller coaster at Disney's Animal Kingdom in Walt Disney World\\r\\nA small roller coaster at a local festival in ?akovec, Croatia","input":"What is the purpose of a roller coaster?"},{"output":"eight","context":"The GMC Acadia is a mid-size crossover SUV (full-size until 2016) from GMC. The first generation GMC Acadia shared the GM Lambda platform with the Chevrolet Traverse, and Buick Enclave. The Acadia went on sale in 2006 as a 2007 model in the United States and in Canada. The Acadia replaces three of the 7- or 8-seater vehicles on the Pontiac-Buick-GMC dealership network, the mid-size GMC Safari van, the GMC Envoy, and the Pontiac Montana SV6 minivan for the domestic market. As of 2009, the Lambda vehicles have replaced the Buick Rainier, Buick Rendezvous, and the Buick Terraza, and then subsequently the GMC Envoy and the Chevrolet TrailBlazer. A Denali version of the Acadia debuted in 2010 as a 2011 model. In 2016, the second generation Acadia was repositioned as a mid-size crossover utility vehicle (as a 2017 model) in order to compete within the growing mid-size CUV market against the likes of the Ford Edge and Dodge Journey.\\r\\n\\r\\n\\r\\nThe Acadia represents GMC's entry-level truck-like CUV and is the first unibody vehicle from the marque. It is also GMC's first front-wheel drive passenger vehicle and GMC's first crossover utility vehicle (CUV).\\r\\nThe Acadia has seating for eight and either front or all-wheel drive. With a 5,200?lb (2,359?kg) towing capacity, the Acadia slots between the GMC Terrain and the Yukon. The Acadia is the mid-priced Lambda model between the Chevrolet Traverse and Enclave.\\r\\nIn December 2006, all production and sales of the Acadia (and the Saturn Outlook) were temporarily stopped due to the engine mounts not having holes drilled to release accumulated water, as well as an issue with potentially faulty rivets in the load floor just forward of the vehicle's rear hatch. The assembly process was quickly adjusted, and dealership sales of the vehicles had resumed within days of the notice.\\r\\nThe first-generation Acadia will continue to be sold alongside its second-generation replacement as the Acadia Limited. For more information, please see below.\\r\\nThe 2008 Acadia has a 118.9?in (3,020?mm) wheelbase and 67.28?in (1,709?mm) front/rear tracks. The independent front suspension is a MacPherson strut design, with a direct-acting stabilizer bar and aluminum knuckles. The independent rear suspension uses a \\"H\\" Linked design.\\r\\nSteering is power-assisted rack-and-pinion, with an optional variable-effort system. Standard are 18?inch wheels and tires, with optional 19?inch wheels. Four-wheel disc brakes with ABS are standard.\\r\\nThe Acadia uses GM's High Feature LY7 V6 that was introduced in the Cadillac CTS. In the Acadia, the engine produces 275?hp (205?kW) and 258?lb{ft (350?N{m) of torque. The Acadia also uses the new GM-Ford 6-speed automatic transmission.\\r\\nThe 2009 model year engine was the direct injected LLT, producing 288?hp (215?kW) and 270?lb{ft (366?N{m) of torque.\\r\\nThe 2012 Acadia has a 3.6?L V6 engine producing 288?hp (215?kW) and 270?lb{ft (366?N{m) torque.[citation needed]\\r\\nThe 2017 Acadia Limited has a 3.6 L V6 engine producing 310 hp (231 kW) and 271 lb ft (367 Nm) torque[1]\\r\\nThe Acadia features available 3-row, 7 or 8-seater seating. Inside, the Acadia features more contemporary trim than the Outlook, including chrome and satin nickel textures, and a wider variety of two-toned interior colors such as an Ebony and Light Titanium (grey) cloth upholstery, and a choice of either Ebony, Light Titanium (grey) or Brick (brown) leather upholstery. A heads-up display similar to those found in the Cadillac XLR, Cadillac STS, Pontiac Grand Prix, Pontiac Aztek, Pontiac Bonneville, Buick Park Avenue and Chevrolet Corvette is one of the available options exclusive to the Acadia and standard on the Acadia Denali thus far.\\r\\nThe 2008 Acadia features 19?inch cast aluminum wheels and Goodyear Eagle RS-A M+S P255/60R-19 108H tires with a high-pressure compact spare tire standard on the SLT2 model and optional on the SLT1. The SLE model has 18?inch painted aluminum wheels with P255/65R-18 tires. The SLT1 comes standard with 18?inch machined aluminum wheels.\\r\\nThe Denali version of the Acadia arrived at dealerships in 2010 as 2011 model. This upgraded trim is available in FWD and AWD versions in seven- or eight-passenger form and features monotone paint, honeycomb grille, unique front and rear fascias, along with HID headlamps, chrome accents, exhaust tips, and six-spoke 20-inch wheels. Interior upgrades include perforated leather seating and wood trim.\\r\\nThe Acadia Denali joins its SUV siblings, the Terrain Denali, the Yukon Denali and Yukon XL Denali, and marks GMC's first crossover to take the Denali badge.[2]\\r\\nA unibody construction helps lower center of gravity, compared to previous truck-based GM SUVs, reduces the risk of rollover accidents. Much of the Acadia's structure is reinforced with high-strength steel, including a steel cross-car beam welded across the floor between the B-pillars.\\r\\nThe Acadia has six different airbags equipped in the vehicle; two dual-stage front air bags for the driver and front passenger, two seat-mounted side-impact air bags in the first row and two head curtain side impact air bags that cover all three seating rows. Similar to the Volvo XC90, the Acadia will have a detection system on board that will deploy various airbags if a rollover is detected.\\r\\nAdditionally, the Acadia comes standard with the OnStar system.\\r\\nGM unveiled an updated 2013 GMC Acadia at the 2012 Chicago Auto Show, taking the bodyshell of the discontinued Saturn Outlook.[3] The 2013 Acadia received a completely redesigned grille and front fascia, a redesigned rear liftgate, and an improved interior with upgrades to interior quality, as with its other two updated siblings, the Buick Enclave and Chevrolet Traverse.\\r\\nFor the 2014 model year, the Acadia added Forward Collision Alert and Lane Departure Warning as a standard on Denali and as an optional feature on SLT1 and SLT2 trims, while two charging-only USB ports on the rear of the center console for second-row use were added as a standard for all trims.[4]\\r\\nFor the 2015 model year, a heated, leather-wrapped steering wheel became standard on SLT-2 and Denali models, along with a twenty-inch aluminum wheel design. Crimson Red Tintcoat, Dark Sapphire Blue Metallic, and Midnight Amethyst Metallic are added as new exterior color palettes. In addition, dual exhaust became available only on Denali versions, while all other trim levels received single exhaust.[5]\\r\\nThe second generation Acadia made its official debut at the North American International Auto Show on January 12, 2016. The redesigned Acadia went on sale in May 2016 as a 2017 model.[6] The second generation Acadia is built in Spring Hill, Tennessee.\\r\\nThe second generation Acadia was reduced from 200.8 inches to 193.6 inches in length and from 78.9 inches to 75.4 inches in width (losing 700 pounds in the process), as GM repositioned the vehicle to mid-size status while remaining above the Terrain in size (the Terrain itself would relinquish its midsize status after it was repositioned as a second generation smaller compact crossover with the 2018 model), as GM has already announced that the next generation Traverse and Enclave will be the only GM-built full size crossover SUVs in this segment. With the reduction from full-size to mid-size, the Acadia's MSRP was reduced to $29,995 (US).[7] The GMC Acadia now has 5, 6, and 7 passenger configurations arranged respectively like either two front bucket seats and one rear bench seat; two bucket seats up front, two buckets in the second row, and two seats in the rear row; or two front bucket seats, one second row bench seat, and two rear seats in the third row.\\r\\nThe second generation Acadia features seven seats instead of eight, a choice of either a 2.5L or 3.6L engine, a new All Terrain drivetrain designed for off-road environments (joining the FWD and AWD drivetrains), an updated fascia and redesigned lighting.[8] The Acadia is built on the same platform as the Cadillac XT5 and will be joined by the Chinese-built Buick Envision and a yet to be named Chevrolet equivalent (built on the C1XX platform) when GM unveils its lineup for its mid-size CUVs for 2018 and beyond.[9]\\r\\nWith the decrease in size, fuel economy with the 3.6L increases to 18 city, 25 hwy (AWD and FWD) from 17 city 24 hwy (FWD).[10][11] The 2017 Acadia will offer the newly launched AppShop feature.[12]\\r\\nTrim levels for the second generation Acadia were carried over from the previous generation; SL, SLE-1, SLE-2, SLT-1, SLT-2, as well as a Denali trim level, though an All-Terrain Package is available on Acadia SLE and SLT trim levels that will be more rugged in its appearance. The second generation Acadia is also available with Third Row Seat Delete option on SLE and SLT trim levels as part of the All-Terrain Package.\\r\\nThe second generation Acadia introduced a feature to remind drivers to check the rear seats for children to help prevent heatstroke of children accidentally left behind in a vehicle.[13]\\r\\nThe Acadia will be sold in Australia and New Zealand as a Holden badged vehicle and will retain the Acadia name. The Holden Acadia will also be built in Tennessee with RHD specifications and marketed as a full size CUV when it goes on sale in 2018.[14] Australia will be the only market in the world where a right-hand drive version of the Acadia will be sold.\\r\\nStarting with the 2017 model year, the first-generation Acadia is sold alongside its second-generation replacement, but was renamed the Acadia Limited, much like the Chevrolet Cruze Limited, Chevrolet Malibu Limited, and Chevrolet Impala Limited were also sold alongside their replacements, at least for a single model year, as rental and fleet vehicles. However, production remained in Lansing, Michigan as the second generation Acadia moved to Spring Hill, Tennessee, the site of the old Saturn plant. With the second-generation Buick Enclave and Chevrolet Traverse making their debuts in 2017 as 2018 models, GMC ended production on the Acadia Limited on March 31, 2017 in order to start production on the Traverse and Enclave.[15] The Acadia Limited carried a base price tag of $44,775 and continued to offer the same features from the 2016 model.[16]","input":"How many seats does the gmc acadia have?"},{"output":"June 27, 1952","context":"The Immigration and Nationality Act of 1952 (Pub.L. 82ÿ414, 66?Stat.?163, enacted June?27, 1952), also known as the McCarranÿWalter Act, codified under Title 8 of the United States Code (8 U.S.C. ch. 12), governs immigration to and citizenship in the United States. It has been in effect since June 27, 1952. Before this Act, a variety of statutes governed immigration law but were not organized within one body of text.\\r\\n\\r\\n\\r\\nH.R. 5678 was named after its sponsors, Senator Pat McCarran (D-Nevada), and Congressman Francis Walter (D-Pennsylvania).\\r\\nPresident Harry Truman, a Democrat, vetoed the Act because he regarded the bill as \\"un-American\\" and discriminatory. His veto message said:[1][2][3]\\r\\nToday, we are \\"protecting\\" ourselves as we were in 1924, against being flooded by immigrants from Eastern Europe. This is fantastic. ... We do not need to be protected against immigrants from these countriesÿon the contrary we want to stretch out a helping hand, to save those who have managed to flee into Western Europe, to succor those who are brave enough to escape from barbarism, to welcome and restore them against the day when their countries will, as we hope, be free again....These are only a few examples of the absurdity, the cruelty of carrying over into this year of 1952 the isolationist limitations of our 1924 law.\\r\\nIn no other realm of our national life are we so hampered and stultified by the dead hand of the past, as we are in this field of immigration.\\r\\nTruman's veto was overridden by a vote of 278 to 113 in the House and 57 to 26 in the Senate.\\r\\nSpeaking in the Senate on March 2, 1953, McCarran said:[4]\\r\\nI believe that this nation is the last hope of Western civilization and if this oasis of the world shall be overrun, perverted, contaminated or destroyed, then the last flickering light of humanity will be extinguished. I take no issue with those who would praise the contributions which have been made to our society by people of many races, of varied creeds and colors. ... However, we have in the United States today hard-core, indigestible blocs which have not become integrated into the American way of life, but which, on the contrary are its deadly enemies. Today, as never before, untold millions are storming our gates for admission and those gates are cracking under the strain. The solution of the problems of Europe and Asia will not come through a transplanting of those problems en masse to the United States. ... I do not intend to become prophetic, but if the enemies of this legislation succeed in riddling it to pieces, or in amending it beyond recognition, they will have contributed more to promote this nation's downfall than any other group since we achieved our independence as a nation.\\r\\nThe Act abolished racial restrictions found in United States immigration and naturalization statutes going back to the Naturalization Act of 1790. The 1952 Act retained a quota system for nationalities and regions. Eventually, the Act established a preference system which determined which ethnic groups were desirable immigrants and placed great importance on labor qualifications. The Act defined three types of immigrants: immigrants with special skills or relatives of U.S. citizens who were exempt from quotas and who were to be admitted without restrictions; average immigrants whose numbers were not supposed to exceed 270,000 per year; and refugees.\\r\\nIt expanded the definition of the \\"United States\\" for nationality purposes, which already included Puerto Rico and the Virgin Islands, to add Guam. Persons born in these territories on or after December 24, 1952 acquire U.S. citizenship at birth on the same terms as persons born in other parts of the United States.[5]\\r\\nThe McCarran-Walter Act abolished the \\"alien ineligible to citizenship\\" category from US immigration law, which de facto only applied to people of Asian descent. Small, token quotas of about 100 people per country were established for the countries of Asia. However, people of Asian descent but who were citizens of a non-Asian country counted towards the quota of their Asian ancestral country.[6] Overall annual immigration from the Asiatic Barred Zone was also capped at 2000.[7] Passage of the act was strongly lobbied for by the Chinese American Citizens Alliance, Japanese American Citizens League, Filipino Federation of America, and Korean National Association; though as an incremental measure, as those organizations wished to see national origins quotas abolished altogether.[8]\\r\\nMcCarran-Walter Act allowed for people of Asian descent to immigrate and to become citizens, which had been banned by laws like the Chinese Exclusion Act of 1882 and Asian Exclusion Act of 1924. Chinese immigration in particular had been allowed for a decade prior to McCarran-Walter by the Magnuson Act of 1943, which was passed because of America's World War II alliance with China.[9] Japanese Americans and Korean Americans were first allowed to naturalize by the McCarran-Walter Act.[10] Overall changes in the perceptions of Asians were made possible by Cold War politics; the Displaced Persons Act of 1948 allowed anticommunist Chinese American students who feared returning to the Chinese Civil War to stay in the United States; and these provisions would be expanded by the Refugee Relief Act of 1953.[7]\\r\\nThe Act allowed the government to deport immigrants or naturalized citizens engaged in subversive activities and also allowed the barring of suspected subversives from entering the country. It was used to ban members and former members and \\"fellow travelers\\" of the Communist Party from entry into the United States, even those who had not been associated with the party for decades.\\r\\nThe act also allowed the government to prevent polygamists from entering the country. It specifically stated under Title II, chapter 2, \\"GENERAL CLASSES OF ALIENS INELIGIBLE TO RECEIVE VISAS AND EXCLUDED FROM ADMISSION\\", Section 212, sub (a), part (11): \\"Aliens who are polygamists or who practice polygamy or advocate the practice of polygamy\\". If one was a polygamist, advocate of polygamy or your religious belief or ideology advocates the practice of polygamy, they would not be allowed in the United States under this law.\\r\\nA 1962 guideline explained procedures under the Act:[11]\\r\\nThe Immigration and Nationality Act of 1952 requires an alien to apply for a petition for naturalization. This form may be obtained from any office of the Immigration and Naturalization Service, a division of the Department of Justice, or from any court authorized to naturalize aliens.\\r\\nBefore applying, an alien must be at least 18 years old and must have been lawfully admitted to live permanently in the United States. He must have lived in the United States for five years and for the last six months in the state where he seeks to be naturalized. In some cases, he need only have lived three years in the United States. He must be of good moral character and \\"attached to the principles of the Constitution\\". The law states that an alien is not of good moral character if he is a drunkard, has committed adultery, has more than one wife, makes his living by gambling, has lied to the Immigration and Naturalization Service, has been in jail more than 180 days for any reason during his five years in the United States, or is a convicted murderer.\\r\\nThe following list provides examples of those who were excluded from the Act prior to the 1990 amendment. While it has not been substantiated that all of these individuals formally petitioned to become United States Citizens, many were banned from travelling to the US because of anti-American political views and/or criminal records. Among those listed, there are noted communists, socialists, and anti-American sympathizers.[12]\\r\\nParts of the Act remain in place today, but it has been amended many times and was modified substantially by the Immigration and Nationality Services Act of 1965.\\r\\nWhen regulations issued under the authority of the Passport Act of 1926 were challenged in Haig v. Agee, Congress enacted  707(b) of the Foreign Relations Authorization Act, Fiscal Year 1979 (Pub.L. 95ÿ426, 92?Stat.?993, enacted October?7, 1978), amending  215 of the Immigration and Nationality Act making it unlawful to travel abroad without a passport. Until that legislation, under the Travel Control Act of 1918, the president had the authority to require passports for foreign travel only in time of war.\\r\\nSome provisions that excluded certain classes of immigrants based on their political beliefs were revoked by the Immigration Act of 1990, however members of Communist Parties are still banned from becoming citizens of the United States.\\r\\nAfter the September 11, 2001 attacks, President George W. Bush implemented the National Security Entry-Exit Registration System and other border and immigration controls.\\r\\nIn January 2017, President Donald Trump's Executive Order 13769 made reference to the \\"Immigration and Nationality Act\\".[17]","input":"When was the immigration and nationality act passed?"},{"output":"under Trajan","context":"","input":"When did the roman empire reach its greatest extent?"},{"output":"1 June 1967","context":"David Bowie is the self-titled debut studio album by English musician David Bowie, released on 1 June 1967, on Deram Records, the same week as the Beatles' Sgt. Pepper's Lonely Hearts Club Band..\\r\\nIts content bears little overt resemblance to the type of music that later made him famous, such as the folk rock of \\"Space Oddity\\" or the glam rock of The Rise and Fall of Ziggy Stardust and the Spiders from Mars. NME critics Roy Carr and Charles Shaar Murray have said, \\"a listener strictly accustomed to David Bowie in his assorted '70s guises would probably find this debut album either shocking or else simply quaint\\",[2] while biographer David Buckley describes its status in the Bowie discography as \\"the vinyl equivalent of the madwoman in the attic\\".[3]\\r\\n\\r\\n\\r\\nDavid Bowie's influences at this stage of his career included the theatrical tunes of Anthony Newley, music hall numbers by acts like Tommy Steele, some of the more whimsical and 'British' material by Ray Davies of the Kinks, Syd Barrett's slightly cracked nursery rhymes for the early Pink Floyd, and the Edwardian flam shared by such contemporary songs as the Beatles' \\"Being for the Benefit of Mr. Kite!\\"[3] The desire of Bowie's then-manager, Ken Pitt, for his charge to become an 'all-round entertainer' rather than a 'rock star' has also been cited as impacting the songwriter's style at this time, which virtually eschewed any rock 'n' roll trappings.[4] Bowie himself said that his debut album \\"seemed to have its roots all over the place, in rock and vaudeville and music hall. I didn't know if I was Max Miller or Elvis Presley\\".[5]\\r\\nThe album was solely written by Bowie, who also arranged with Dek Fearnley, having reportedly taught themselves the craft using the Observer Book of Music.[3] \\"Rubber Band\\" was a marching tune that employed tuba as the lead instrument. \\"Little Bombardier\\" and \\"Maid of Bond Street\\" were in waltz time, and also made extensive use of brass and strings. \\"Love You till Tuesday\\" and \\"Come and Buy My Toys\\" were among the few songs on the album with a lead (acoustic) guitar, the former heavily augmented by strings. \\"Join the Gang\\" was a rare excursion into contemporary youth culture, an acerbic observation of peer pressure and drug use, which included sitar in its instrumentation as well as a musical quotation of The Spencer Davis Group's recent hit \\"Gimme Some Lovin'.\\" The final track, \\"Please Mr. Gravedigger\\", was \\"a macabre duet for voice and sound effects\\",[2] and has been described as \\"one of pop's genuinely crazy moments\\".[3]\\r\\nDespite the album's incongruity in the Bowie catalogue, some commentators have discerned embryonic themes that inform the artist's more mature work.[2][3] \\"We Are Hungry Men\\" is told by a self-styled \\"messiah\\" whose persona would reappear in different forms in the songs \\"Cygnet Committee\\" (from the album Space Oddity), \\"Saviour Machine\\" (from The Man Who Sold the World) and \\"Oh! You Pretty Things\\" (from Hunky Dory), as well as in the protagonist of The Rise and Fall of Ziggy Stardust and the Spiders from Mars. The track also explicitly referenced subjects like abortion, infanticide and cannibalism. \\"There Is a Happy Land\\" was an early manifestation of Bowie's vision of children as a race apart from their elders, a theme revisited on The Man Who Sold the World, Hunky Dory and Ziggy Stardust. \\"She's Got Medals\\" was a gender-bending tale with gay and lesbian connotations that predated the 'dress cover' of The Man Who Sold the World and the bisexual/androgynous character of Ziggy Stardust.\\r\\nPrior to releasing the album, Deram issued two singles with the same personnel, \\"Rubber Band\\" b/w \\"London Boys\\", in December 1966, and \\"The Laughing Gnome\\" b/w \\"The Gospel According to Tony Day\\", in April 1967. \\"Rubber Band\\" was a different recording to the album track. \\"London Boys\\" has been lauded as Bowie's first mini-masterpiece,[2][6] a melancholy observation of the London Mod scene of the time. \\"The Laughing Gnome\\" was a novelty record featuring high-pitched vocals but the varispeed technique used to create this effect would serve Bowie in more serious fashion on many future songs including \\"After All\\", \\"The Bewlay Brothers\\", \\"Fame\\" and \\"Scream Like a Baby\\". The song became a hit when reissued in 1973, in the wake of Bowie's commercial breakthrough The Rise and Fall of Ziggy Stardust and the Spiders from Mars. Despite it being radically different from his material at the time, the single made No. 6 in the UK charts. A re-recorded version of \\"Love You till Tuesday\\" b/w \\"Did You Ever Have a Dream\\" was released as a single in July 1967.\\r\\nDavid Bowie was released in the UK, in both mono and stereo, on 1 June 1967. It was issued in the US in August 1967, minus \\"We Are Hungry Men\\" and \\"Maid of Bond Street\\". The album - only reaching 125 in the UK album charts - and its associated singles were all commercial failures at the time, and Bowie did not release another record until 1969's David Bowie, two years later. The album's failure cost Bowie his record contract with Deram Records, who dropped him in April 1968. Bowie then made a video called \\"Love You till Tuesday\\" in 1969 to sell himself to a new label. The video recycled many songs from the debut album as well as the then newly written \\"Space Oddity\\". Many recycled songs in the video saw new orchestral arrangements and added vocals from friend John 'Hutch' Hutchinson and (at the time) girlfriend Hermione Farthingale, whom with Bowie formed the folk-rock trio Feathers. All three appear in the \\"Sell Me A Coat\\" portion of the video. The video saw an official public release in 1984. The songs from the debut album and its singles, plus later Deram works, have been recycled in a multitude of compilation albums, including The World of David Bowie (1970), Images 1966ÿ1967 (1973), Another Face (1981), Rock Reflections (1990), and The Deram Anthology 1966ÿ1968 (1997).\\r\\nThe album itself was reissued by Deram on CD in 1988. The booklet reprints the original press release by Kenneth Pitt and a new (1988) essay by John Tracy. In addition, the rear sleeve notes the different versions included. These are \\"Rubber Band\\" (Version 2), \\"When I Live My Dream\\" (Version 1) and \\"Please Mr. Gravedigger\\" (Version 2).\\r\\nIn 2010, the album was released in a deluxe edition by Deram in the UK and Universal Music world-wide. This features both stereo and mono mixes of the album, together with previously unreleased stereo mixes of songs not originally included and, for the first time as an official release, the first BBC radio session. (Top Gear, 18 December 1967)[12]\\r\\nAll tracks written by David Bowie.","input":"When was david bowie's first album released?"},{"output":"Great Dividing Range","context":"\\r\\n\\r\\nThe Blue Mountains are a mountainous region and a mountain range located in New South Wales, Australia. The region borders on Sydney's metropolitan area, its foothills starting about 50 kilometres (31?mi) west of the state capital.[4] The public's understanding of the extent of the Blue Mountains is varied, as it forms only part of an extensive mountainous area associated with the Great Dividing Range. Officially the Blue Mountains region is bounded by the Nepean and Hawkesbury rivers in the east, the Coxs River and Lake Burragorang to the west and south, and the Wolgan and Colo rivers to the north.[5] Geologically, it is situated in the central parts of the Sydney Basin.[6]\\r\\n\\r\\nThe Blue Mountains Range comprises a range of mountains,  plateau escarpments extending off the Great Dividing Range about 4.8 kilometres (3.0?mi) northwest of Wolgan Gap in a generally southeasterly direction for about 96 kilometres (60?mi), terminating at Emu Plains. For about two-thirds of its length it is traversed by the Great Western Highway and the Main Western railway line. Several established towns are situated on its heights, including Katoomba, Blackheath, Mount Victoria, and Springwood. The range forms the watershed between Coxs River to the south and the Grose and Wolgan rivers to the north.[3] The range contains the Explorer Range and the Bell Range.[7]\\r\\n\\r\\nThe Blue Mountains area includes the local government areas of the City of Blue Mountains, the City of Hawkesbury, the City of Lithgow, Wollondilly Shire and Oberon Shire.\\r\\n\\r\\nFollowing European settlement of the Sydney area, the area was named the Carmarthen and Lansdowne Hills by Arthur Phillip in 1788. The Carmarthen Hills were in the north of the region and the Lansdowne Hills were in the south. The name Blue Mountains, however, was preferred[8] and is derived from the blue tinge the range takes on when viewed from a distance. The tinge is believed to be caused by Mie scattering which occurs when incoming light with shorter wavelengths is preferentially scattered by particles within the atmosphere imparting a blue-greyish colour to any distant objects, including mountains and clouds. Volatile terpenoids emitted in large quantities by the abundant eucalyptus trees in the Blue Mountains may cause Mie scattering and thus the blue haze for which the mountains were named.[9]\\r\\n\\r\\nWhen Europeans arrived in Australia, the Blue Mountains had already been inhabited for several millennia by the Gundungurra people, now represented by the Gundungurra Tribal Council Aboriginal Corporation based in Katoomba, and, in the lower Blue Mountains, by the Darug people, now represented by the Darug Tribal Aboriginal Corporation.[10][11]\\r\\n\\r\\nThe Gundungurra creation story of the Blue Mountains tells that Dreamtime creatures Mirigan and Garangatch, half fish and half reptile, fought an epic battle which scarred the landscape into the Jamison Valley.\\r\\n\\r\\nThe Gundungurra Tribal Council is a nonprofit organisation representing the Gundungurra traditional owners, promoting heritage and culture and providing a support for Gundungurra people connecting back to Country.\\r\\n\\r\\nGundungurra Tribal Council Aboriginal Corporation has a registered Native Title Claim since 1995 over their traditional lands, which include the Blue Mountains and surrounding areas.\\r\\n\\r\\nExamples of Aboriginal habitation can be found in many places. In the Red Hands Cave, a rock shelter near Glenbrook, the walls contain hand stencils from adults and children.[12]:170 On the southern side of Queen Elizabeth Drive, at Wentworth Falls, a rocky knoll has a large number of grinding grooves created by rubbing stone implements on the rock to shape and sharpen them. There are also carved images of animal tracks and an occupation cave. The site is known as Kings Tableland Aboriginal Site and dates back 22,000 years.[citation needed]\\r\\n\\r\\nArthur Phillip, the first governor of New South Wales, first glimpsed the extent of the Blue Mountains from a ridge at the site of today's Oakhill College, Castle Hill. He named them the Carmarthen Hills, \\"some forty to sixty miles distant...\\" and he reckoned that the ground was \\"most suitable for government stock\\". This is the location where Gidley King in 1799 established a prison town for political prisoners from Ireland and Scotland.\\r\\n\\r\\nThe first documented use of the name Blue Mountains appears in Captain John Hunters account of Phillips 1789 expedition up the Hawkesbury River. Describing the events of about 5 July, Hunter wrote: \\"We frequently, in some of the reaches which we passed through this day, saw very near us the hills, which we suppose as seen from Port Jackson, and called by the governor the Blue Mountains.\\"[13] During the nineteenth century the name was commonly applied to the portion of the Great Dividing Range from about Goulburn in the south to the Hunter Valley in the north, but in time it came to be associated with a more limited area.[14]\\r\\n\\r\\nThe native Aborigines knew two routes[citation needed] across the mountains: Bilpin Ridge, which is now the location of Bells Line of Road between Richmond and Bell, and the Coxs River, a tributary of the Nepean River. It could be followed upstream to the open plains of the Kanimbla Valley, the type of country that farmers prize.\\r\\n\\r\\nEuropean settlers initially considered that fertile lands lay beyond the mountains, as was China in the belief of many convicts, but that this didn't matter much, since the mountains were impassable.[15] This idea was, to some extent, convenient for local authorities. An \\"insurmountable\\" barrier would deter convicts from trying to escape in that direction.\\r\\n\\r\\nA former convict, John Wilson, may have been the first European to cross the Blue Mountains. It is also believed that Mathew Everingham, 1795,[16] may have also been partly successful based on letters he wrote at the time which came to light in the late 1980s. Wilson arrived with the First Fleet in 1788 and was freed in 1792. He settled in the bush, living with the Aborigines and even functioning as an intermediary between them and the settlers. In 1797 he returned to Sydney, claiming to have explored up to a hundred miles in all directions around Sydney, including across the mountains. His descriptions and observations were generally accurate, and it is possible that he had crossed the mountains via the southern aspect at the Coxs River corridor, guided by the Aborigines.[17]:76ÿ77\\r\\n\\r\\nGovernor Hunter was impressed by Wilson's skills and sent him on an expedition with John Price and others in January 1798. The party crossed the Nepean River and moved southwest towards the present site of Mittagong. There they turned west and found a route along the ridge where today the Wombeyan Caves Road is located. In the process they found a way to go west of the mountains, by going around them instead of across them. In March of the same year, Wilson and Price ventured to the Camden area, and then continued further south until they discovered Thirlmere Lakes, finally almost reaching the present site of Goulburn.\\r\\n\\r\\nIt is possible that the accomplishments of this expedition were suppressed by Hunter, who may not have wanted convicts to know that there was a relatively easy way out of Sydney.[17]:83 Wilson was killed by Aborigines after abducting one of their women for his personal use,[18] but he had accomplished much as an explorer. He was never recognised as the first person to cross the mountains, possibly because his Coxs River journey could not be verified, while his route west of Mittagong may have been the \\"long way around\\" for a colony that had its eyes fixed on the sandstone fortress west of the Nepean.\\r\\n\\r\\nBetween 1798 and 1813, many people explored various parts of the mountains, from the Bilpin Ridge to the southern regions, today the site of the Kanangra-Boyd National Park. Still, they did not find a definite route across the mountains.\\r\\nThe 1813 crossing of the Blue Mountains by Gregory Blaxland, William Lawson and William Charles Wentworth is officially credited as the first successful European crossing.[19] Blaxland set out with Lawson and Wentworth on 11 May 1813 and succeeded in crossing the mountains by 31 May. They ventured as far as to what is now Mount Blaxland, just west of Coxs River.\\r\\n\\r\\nIn November 1813, Macquarie sent the surveyor George Evans on an expedition to confirm the discoveries made by Blaxland and his party. He was also told to see if there existed enough arable land to justify settlement. The issue had become more urgent because the colony was in the grips of a drought.\\r\\n\\r\\nEvans and his party reached the Fish and Macquarie rivers, and the site of Bathurst.[20] On 7 July 1814, construction of a road across the mountains was begun by William Cox. The work was at the behest of Governor Macquarie. 30 convict labourers and 8 guards completed the road on 14 January 1815 after 27 weeks of hard work.[17]:145\\r\\n\\r\\nSince the Blue Mountains are rich in coal and shale, mining for these resources began in Hartley Vale in 1865. J.B. North ran a shale mine in the Jamison Valley in the 19th century,[12]:243 and other operations were set up in several places. Locations for mining activities included the Jamison Valley, the upper Grose Valley, Newnes, Glen Davis and the Asgard Swamp area outside Mount Victoria. Shale mining failed in the long run because it was not financially viable. \\r\\n\\r\\nThe climate varies with elevation. At Katoomba, (1,010?m or 3,314?ft) the summer average maximum temperature is around 22?C with a few days extending into the 30s (80sÿ90s F) although it is quite common to see maximum temperatures stay in the teens when east coast troughs persist. Night-time temperatures are usually in the teens but can drop to single figures at times. During winter, the temperature is typically around 10 to 11?C in the daytime with ?1?C or so on clear nights and 3 to 4?C on cloudy nights. Very occasionally it will get down to ?3?C or slightly lower but usually the coldest air drains into the valleys during calm, clear nights. The Blue Mountains is not known for particularly cold mornings compared to other areas on the Central Tablelands, such as Oberon, Bathurst and Orange. There are two to three snowfalls per year, although settled snow has become less common in recent decades. In the lower mountains, however, the climate is significantly warmer.\\r\\n\\r\\nAnnual rainfall is about 1,050 millimetres (41?in) in the Upper Blue Mountains[21] with many misty days.\\r\\n\\r\\nThe predominant natural vegetation of the higher ridges is eucalyptus forest. Heath-like vegetation is present on plateau edges above cliffs. The sheltered gorges often contain temperate rainforests. There are also many hanging swamps with button grass reeds and thick, deep black soil. Wollemia nobilis, the \\"Wollemi pine\\", a relict of earlier vegetation of Gondwana, is found in remote and isolated valleys of the Wollemi National Park.\\r\\n\\r\\nThe main natural disasters to afflict the area are bush fires and severe storms. In recent years the lower mountains have been subjected to a series of bush fires which have caused great loss of property but relatively little loss of life. The upper mountains had not had a major fire for some decades until December 2002 (the Blackheath Glen Fire) and November 2006 when an extensive blaze in the Grose Valley threatened several communities including Bell and Blackheath (the Lawsons Long Alley Fire). This latest fire burned for almost a month but was extinguished, mainly due to a change in the weather, without loss of human life or property. A program of winter burning seems to have been successful in reducing fires in the upper mountains.\\r\\n\\r\\nThe Blue Mountains Range contains smaller mountain ranges:   the Bell Range near The Bells Line of Road and north of the Grose River; the Explorer Range, south of the Grose River extending west towards Mount Victoria; the Caley Range, Erskine Range, Mount Hay Range, Paterson Range, and the Woodford Range.[22] The major recorded peaks are:[23]\\r\\n\\r\\nThe Blue Mountains are a dissected plateau carved in sandstone bedrock. They are now a series of ridge lines separated by gorges up to 760 metres (2,490?ft) deep. The highest point in the Blue Mountains, as it is now defined, is an unnamed point with an elevation of 1,189?m (3,901?ft) AHD?, located 7 kilometres (4.3?mi) north-east of Lithgow. However, the highest point in the broader region that was once considered to be the Blue Mountains is Mount Bindo, with an elevation 1,362?m (4,469?ft) AHD?.[14] A large part of the Blue Mountains is incorporated into the Greater Blue Mountains Area World Heritage Site, consisting of seven national park areas and a conservation reserve.[28]\\r\\n\\r\\nThe Blue Mountains area is a distinct physiographic section of the larger Hunter-Hawkesbury Sunkland province. This is in turn a part of the larger East Australian Cordillera physiographic division.\\r\\n\\r\\nThe Greater Blue Mountains Area was unanimously listed as a World Heritage Area by UNESCO on 29 November 2000, becoming the fourth area in New South Wales to be listed.[29] The area totals roughly 10,000 square kilometres (3,900?sq?mi), including the Blue Mountains, Kanangra-Boyd, Wollemi, Gardens of Stone, Yengo, Nattai and Thirlmere Lakes National Parks, plus the Jenolan Caves Karst Conservation Reserve.\\r\\n\\r\\nThis site was chosen to be included on the World Heritage list because:\\r\\n\\r\\nThe Greater Blue Mountains Area is inhabited by over 400 different forms of animals. Among them are rare mammal species like spotted-tailed quoll, the koala, the yellow-bellied glider, and long-nosed potoroo. There are also some rare reptiles, like the Blue Mountain water skink.[31] There are also some dingos in the area, which form the top predators and hunt for grey kangaroos.[32]\\r\\n\\r\\nThe Blue Mountains are a popular destination for rock climbers, mountain bikers and hikers as well as canyoning and other adventure sports. These sports are well catered for by guiding companies and equipment stores located mainly in Katoomba.\\r\\n\\r\\nPopular climbing destinations include the Centennial Glen cliffs near Blackheath, Mount Victoria, Mount Piddington and Mount Boyce. Climbing is currently banned on The Three Sisters.[38]\\r\\n\\r\\nMountain biking takes place mainly on the many fire trails that branch away from the main spine of the Great Western Highway, such as Narrow Neck, Anderson's Fire Trail and others.[39][40]\\r\\n\\r\\nLikewise many of the fire trails are popular with day hikers, though many dedicated walking trails exist away from the fire roads.[41]\\r\\n\\r\\nCanyoning in the Blue Mountains is a popular sport and caters for various skill levels. It carries inherent dangers, yet for those with the appropriate skills or those looking to take a guided trip there are many great opportunities to experience a different view of the Blue Mountains.\\r\\n\\r\\nThere are numerous abseiling options available in the Blue Mountains including single and multipitch routes. There are some restrictions though with certain areas being closed for abseiling.[42]\\r\\n\\r\\nCricket is a popular sport in the Blue Mountains, with the Blue Mountains Cattle Dogs representing the district in the Western Zone Premier League, Country Plate and Presidents Cup competitions.[43]","input":"What mountain range is the blue mountains part of?"},{"output":"between the ages of seven and thirteen","context":"First Communion is a ceremony in some Christian traditions during which a person first receives the Eucharist. It is most common in the Latin Church tradition of the Catholic Church, as well as in many parts of the Lutheran Church and Anglican Communion. In churches that celebrate First Communion, it typically occurs between the ages of seven and thirteen, often acting as a rite of passage.\\r\\n\\r\\nCatholics believe this event to be very important, as the Eucharist occupies a central role in Catholic theology and practice.\\r\\n\\r\\nFirst Communion is not celebrated in the Eastern Orthodox churches, the Oriental Orthodox churches, or the Assyrian Church of the East, as they practice infant communion (which often is simultaneously administered with infant baptism and confirmation). Some Anglicans allow infant communion, while others require the previous reception of confirmation, usually during the teenage years.\\r\\n\\r\\nCelebration of this ceremony is typically less elaborate in many Protestant churches. Catholics and some Protestants believe that Christ is truly present in the Eucharist, although only Catholics and some Anglo-Catholics of the Anglican Communion believe this is through transubstantiation.\\r\\n\\r\\nOther denominations have varying understandings, ranging from the Eucharist being a symbolic meal to a meal of remembering Christ's last supper.\\r\\n\\r\\nThe sacrament of First Communion is an important tradition for Catholic families and individuals. For Catholics, Holy Communion is the third of seven sacraments received. It occurs only after receiving Baptism, and once the person has reached the age of reason (usually, around the second grade). First confession (the first sacrament of penance) must precede the reception of the Eucharist. This order of the sacraments is practiced universally by all Latin rite Catholics. In 1910, Pope Pius X issued the decree Quam singulari, which changed the age at which First Communion is taken to 7 years old.  Previously, local standards had been 10 or 12 or even 14 years old.[1] Byzantine Catholics (Eastern Rite) celebrate the sacraments of baptism, confirmation (Chrismation), and Holy Communion on the same day as an infant's baptism.\\r\\n\\r\\nTraditions of celebration surrounding First Communion usually include large family gatherings and parties to celebrate the event. The first communicant wears special clothing. The clothing is often white to symbolize purity, but not in all cultures. Often, a girl wears a fancy dress and a veil attached to a chaplet of flowers or some other hair ornament. In other communities, girls commonly wear dresses passed down to them from sisters or mothers, or even simply their school uniforms with the veil or wreath. Boys may wear a suit and tie, tuxedo, their Sunday best, or national dress, with embroidered arm bands worn on the left arm and occasionally white gloves.\\r\\n\\r\\nIn many Latin American countries, boys wear military-style dress uniforms with gold braid aiguillettes. In Switzerland, both boys and girls wear plain white robes with brown wooden crosses around their necks. In Spain, Germany, Luxembourg, Austria, and Guam, girls are dressed up as little brides, although this has been partly replaced by albs in recent times.\\r\\n\\r\\nIn Scotland, boys traditionally wear kilts and other traditional Scottish dress which accompany the kilt. In the Philippines, First Communion services often occur on or around the Feast of the Immaculate Conception (the country's patron saint), with boys donning either the Barong Tagalog or semi-formal Western dress, and girls a plain white dress and sometimes a veil.\\r\\n\\r\\nGifts of a religious nature are usually given, such as rosaries, prayer books, religious statues, icons, and holy cards. Monetary gifts are also common.[2]\\r\\n\\r\\nMany families have formal professional photographs taken in addition to candid snapshots in order to commemorate the event. Some churches arrange for a professional photographer after the ceremony.\\r\\n\\r\\nDuring the communist era, initiation into the pioneer movement in communist countries that had large Catholic populations was an overt attempt to supplant the Catholic ritual (e.g., the Union of Pioneers of Yugoslavia). In all cases, a child at the age of seven to ten is initiated as a member of a group within which the individuals share certain values and culture.\\r\\n\\r\\nChildren from Argentina in 1920\\r\\n\\r\\nChildren from Holyrood School at England, in 1949\\r\\n\\r\\nAn Indian girl receiving her First Communion\\r\\n\\r\\nThe decorated bread is a gift for the Catholic Church from children who had their first Communion in Poland.\\r\\n\\r\\nCandle as used for First Communion\\r\\n\\r\\n2014 First Communion ceremony in the St. Nicholas Church in ?akovec, Croatia\\r\\n\\r\\nFirst Communion in Mexico City, Mexico","input":"When do you have your first holy communion?"},{"output":"110 by 65 yards","context":"American and Canadian football are gridiron codes of football that are very similar. Both have their origins in rugby football. There are, however, some key differences.\\r\\n\\r\\nRugby football was introduced to North America in Canada by the British Army garrison in Montreal, Quebec, which played a series of games with McGill University.[1]  In 1874, the U.S. Harvard University hosted Canada's McGill University to play the new game derived from rugby football in a home-and-home series. When the Canadians arrived several days early, to take advantage of the trip to see Boston and the surrounding areas, they held daily practices. During this time, the Americans were surprised to see the Canadians kick, chase, and then run with the ball. Picking up and running with the ball violated a basic rule of the American game of the day; when the U.S. captain (Henry Grant) pointed this out to the captain of the Canadian team (David Roger), the reply was simple: Running with the ball is a core part of the Canadian game. When the American asked which game the Canadians played, David replied \\"rugby\\". After some negotiation, it was decided to play a game with half and half Canadian/U.S. rules. Thus, many of the similarities and differences between the Canadian and American games indeed came out of this original series where each home team set the rules. For instance, Harvard, because of a lack of campus space, did not have a full-sized rugby pitch. Their pitch was only 100?yd (91?m) long by 50?yd (46?m) wide with undersized end zones (slightly less than the 53?-yard width of the current regulation-sized field for American football). Because of the reduced field, the Harvard team opted for 11 players per side, four fewer than the regulation 15 of Rugby Union. To generate more offense, Harvard also increased the number of downs from three, as set by McGill, to four. Furthermore, the Harvard players so enjoyed running with the ball, this rule was wholly adopted into all Harvard play following the two games with McGill. While the American team bested the Canadian (3-0 and a following tie game), both countries' flavours of football were forever changed and linked to one another. Both the Canadian and American games still have some things in common with the two varieties of rugby, especially rugby league, and because of the similarities, the National Football League (NFL) had a formal relationship with the Canadian Football League (CFL) between 1997 and 2008.[2]\\r\\n\\r\\nMany, perhaps most, of the rules differences have arisen because of rules changes in American football in the early 20th century which have not been copied by Canadian football. The major Canadian codes never abolished the onside scrimmage kick (see Kicker advancing the ball below) or restricted backfield motion, while the American college football (from whose code all American codes derive) did. Canadian football was later in adopting the hand snap and the forward pass, although one would not suspect the latter from play today. Additionally, Canadian football was slower in removing restrictions on blocking, but caught up by the 1970s so that no significant differences remain today. Similarly, differences in scoring (the Canadian game valuing touchdowns less) opened up from the late 19th century, but were erased by the 1950s. An area in which American football has been more conservative is the retention of the fair catch (see below).\\r\\n\\r\\nIn some regions along the Canada-U.S. border, especially western areas, some high schools from opposite sides of the border regularly play games against one another (typically one or two per team per season). By agreement between the governing bodies involved, the field of the home team is considered a legal field, although it is a different size from one school's normal field. In all but a few cases, the rules of the home team are followed throughout the game.\\r\\n\\r\\nMany CFL players are Americans who grew up playing American football and cannot find a place in the NFL, or who prefer to play in the CFL; strict import quotas restrict the number of non-Canadian players. Furthermore, the classifications of import (non-Canadian) and non-import (Canadian) were highly restrictive and required a player to have been in Canada since childhood to qualify as a non-import (i.e. a player cannot simply become a Canadian citizen and become a non-import, nor can he arrive in Canada during high school or college; both scenarios would still have the player in question classified as an import and counted against the team's maximum); these restrictions were loosened beginning in 2014 so that anyone who had become a Canadian citizen at any time before signing with the league for the first time could qualify as a non-import player. For individuals who played both American and Canadian football professionally, their career statistic totals are considered to be their combined totals from their careers in both the CFL and NFL. Warren Moon, for example, was the all-time professional football leader in passing yards after an illustrious career in both leagues. He was surpassed in 2006 by Damon Allen, who in turn was surpassed by Anthony Calvillo in 2011, both of whose careers were exclusively in the CFL.\\r\\n\\r\\nThere are several important specific differences between the Canadian and American versions of the game of football:\\r\\n\\r\\nThe official playing field in Canadian football is larger than the American, and similar to American fields prior to 1912. The Canadian field of play is 110 by 65 yards (101 by 59.4?m), rather than 100 by 53?1?3 yards (91.4 by 48.8?m) as in American football. The end zones in Canadian football are 10 yards deeper than American football end zones as the CFL uses 20-yard (18.3?m) end zones. Prior to 1986, however, the end zones were 25 yards, with Vancouver's BC Place the first to use the 20 yard-long end zone in 1983. However, beginning in 2016, the new home of the CFL's Toronto Argonauts, BMO Field, utilizes an 18-yard-long end zone.[3] Including the end zones, the Canadian field is almost 34% larger than the American field (87,750 square feet (8,152?m2) for the Canadian field vs 57,600 square feet (5,350?m2) for the American field). Occasionally, however, the Canadian field will have its end zone truncated at the corners so that the field fits in the infield of a running track. The only example in the CFL is the Percival Molson Memorial Stadium, home of the Montreal Alouettes. The goalposts for kicking are placed at the goal line in Canadian football and the end line in the American game. In Canadian rules, the distance between the sideline and hash marks is 24 yards (21.9?m); in American amateur rules, at the high school level, the distance is 17?yards 2?feet 4?inches (16.3?m), virtually sectioning the field into three equal columns. The hash marks are closer together at the American college level, where they are 20 yards (18.3?m) from the sideline, and in the NFL, where they are 23?yd 1?ft 9?in (21.6?m) from the sideline and the distance between them is the same as that between the goalposts.[4]\\r\\n\\r\\nBecause of the larger field, many American football venues are generally unfit for the Canadian game. While there are several American stadia which could accommodate the extra ?17?1?2?feet (nearly 6 yards (5.5?m) per side in width (multi-purpose stadiums, baseball parks converted for football, and some soccer-specific stadiums are particularly good fits), most American stadia would lose between fifteen and eighteen rows of seating in each end zone because the field is 15 yards (13.7?m) longer on each end. In many smaller venues, this would be the entire end zone section, losing seating for at least 3,000 spectators. During the CFL's failed expansion to U.S. cities, Canadian football was either played on fields designed to accommodate both American football and baseball (such as the Baltimore Stallions playing at Memorial Stadium), or in some cases, on a field designed for American football (for instance, the Memphis Mad Dogs and the Birmingham Barracudas of the CFL, playing in the Liberty Bowl and at Legion Field, respectively, played the Canadian game on modified American-sized fields because of the inability of the stadia to adapt to the larger field). The Alamodome, originally built as a multi-purpose dome, proved to best accommodate both Canadian football (the CFL's San Antonio Texans) and American football (Alamo Bowl, Dallas Cowboys training camp, the New Orleans Saints after Hurricane Katrina, the NFLPA Game, the U.S. Army All-American Bowl and the UTSA Roadrunners), although Canadian football is no longer played there. Similarly, Hornet Stadium fairly easily adapted to both the Canadian and American game, as it was built with a running track in which the Canadian field fits with only some cuts to the corners. Hornet Stadium hosts California State University, Sacramento (more often known as Sacramento State), hosted the Sacramento Surge and Sacramento Mountain Lions in American football and hosted the Sacramento Gold Miners in Canadian football.[citation needed]\\r\\n\\r\\nAmerican teams use eleven players, while Canadian teams have twelve players on the field per side. Both games have the same number of offensive players required at the line of scrimmage, so the twelfth player in the Canadian game plays a backfield position on offence, whereas this is usually a defensive back on defence.[5]\\r\\n\\r\\nBecause of this, position designations of the various offensive and defensive lines vary. For example, there is no tight end in most formations in Canadian football (this position was phased out in 1980 in the Canadian game).[6] The typical offensive arrangement in Canadian football is for there to be two slotbacks instead of the American tight end, while on the defensive end of the ball, two defensive halfbacks are used instead of one strong safety in the American game.[citation needed]\\r\\n\\r\\nThe sizes of individual American and Canadian footballs can vary within specified size limitations. Because the ball size specifications for the CFL and NFL are nearly identical, the same ball can fall within the requirements of both.[citation needed]\\r\\n\\r\\nHistorically the CFL ball was slightly larger, both because of slightly bigger specifications, but also because CFL manufacturers tended to make balls at the larger end of the allowed tolerances as opposed to NFL manufacturers who built balls to the smaller end. However, the CFL updated its specifications in 1985.[7]\\r\\n\\r\\nAccording to the current standards the regulation size for a Canadian Football League football is specified as: short circumference from 20?7?8 to 21?1?8 inches (530 to 537?mm); long circumference from 27?3?4 to 28?1?4 inches (705 to 718?mm).[8]\\r\\n\\r\\nThe regulation size for a National Football League football is specified as: short circumference from 21 to 21?1?4 inches (533 to 540?mm); long circumference from 28 to 28?1?2 inches (711 to 724?mm).[9]\\r\\n\\r\\nDespite the fact that the CFL rules allow for a smaller legal ball and the NFL rules allow for a larger legal ball, there is a common misconception among media, fans, and even players that the current CFL ball is bigger. Some professional quarterbacks have stated that they notice a difference in size.[10]\\r\\n\\r\\nAnother difference between NFL and CFL balls is that Canadian balls have two 1 inch (25?mm) complete white stripes around the football 3 inches (76?mm) from the largest diameter of the ball and NFL balls have no stripes at all. College football and high school football both specify the use of stripes, but only on two of the football's four panels (the ones adjacent to the laces).[citation needed]\\r\\n\\r\\nIn American football, a team has four downs to advance the ball ten yards, while in Canadian football the limit is three downs.[11][12]\\r\\n\\r\\nIn both games, the ball is placed at a line of scrimmage, in which a player known as the centre/center performs a snap to start a football play. In Canadian football, the snap is required to go between the centre's legs; there is no such requirement in American football, but it is invariably done this way anyway, so the center is in position to block following the snap. The defensive team must stay a set distance away from the line of scrimmage on their side of the line.[citation needed]\\r\\n\\r\\nIn Canadian football, the distance between the line of scrimmage and the defensive team is a full yard. Because of this one-yard distance, teams will tend to gamble on third and one or third and inches. If a team has possession within one yard of either goal line, the line of scrimmage is moved to the one-yard line.[13]\\r\\n\\r\\nIn American football, the set distance between the offensive and defensive teams is eleven inches?ÿ the length of the ball, creating the illusion of the teams being \\"nose-to-nose\\" against each other.[citation needed]\\r\\n\\r\\nWhile large, relatively immobile offensive line players used to form a line that cannot be easily penetrated by the defense are valued in American football, the extra distance from the defensive team means Canadian football finds value in more nimble athletes.[14]\\r\\n\\r\\nIn American football, if a punt returner sees that, in his judgment, he will be unable to advance the ball after catching it, he may signal for a fair catch by waving his hand in the air, and forgo the attempt to advance. If he makes this signal, the opposing team must allow him to attempt to catch the ball cleanly; if he is interfered with, the team covering the kick will be penalized fifteen yards. In contrast, there is no fair catch rule in Canadian football: instead no player from the kicking team, except the kicker or any player who was behind him when he kicked the ball, may approach within five yards of the ball until it has been touched by an opponent. If they do, a \\"no yards\\" penalty is called against the kicking team. Penalties for \\"no yards\\" calls vary on whether the ball made contact with the ground or not. The penalty is 5 yards if the ball has bounced and 15 if the ball is caught in the air.[15]\\r\\n\\r\\nFurthermore, in American football the receiving team may elect not to play the ball if the prospects for a return are not good and the returner is not certain he can successfully catch the ball on the fly; American players are generally taught not to attempt to touch a bouncing football. If any member of the kicking team touches the ball after the kick is made, without an intervening touch by the member of the receiving team, the receiving team may elect to scrimmage the ball from that spot of first touching, regardless of anything else (other than a penalty) that happens during the rest of the play. If the kicking team gains possession of the ball during the kick before it is touched by the receiving team, the ball is then dead. Often, the ball hits the ground and is surrounded by players from the kicking team, who allow it to roll as far as possible downfield  ÿ without going into the end zone  ÿ before grasping or holding the ball against the ground. (If a punt bounces into the receiving team's end zone, it is dead, and a touchback is awarded.) On the other hand, if the ball touches a member of the receiving team without his gaining possession (a \\"muff\\"), then the ball can be recovered by either team (but cannot be advanced by the kicking team). If the kicking team recovers the ball, they regain possession and are awarded a first down at the spot of the recovery.[citation needed]\\r\\n\\r\\nFollowing a fair catch in American football, the receiving team can elect a free kick (called a fair catch kick) from the spot the ball is received ÿ and if the kick goes through the opposite goal posts a field goal is scored. Fair catch kicks are rarely attempted in the NFL and are usually unsuccessful (The last successful fair catch kick was in 1976). The fair catch kick is not allowed in college football.[citation needed]\\r\\n\\r\\nIn Canadian football, if the receiving team does not play the ball, the kicker, and any teammates behind the kicker at the time of the kick, can attempt to retrieve and advance the ball. This is further explained in the kicker advancing the ball section.[citation needed]\\r\\n\\r\\nIn American football, after all players are set, only one offensive player is allowed to be in motion, and he cannot be moving toward the line of scrimmage while the ball is snapped. The motion player must also be behind the line of scrimmage; players on the line cannot be in motion.[16]\\r\\n\\r\\nIn Canadian football, all offensive backfield players, except the quarterback, may be in motion at the snap  ÿ players in motion may move in any direction as long as they are behind the line of scrimmage at the snap. In addition, the two players on the ends of the line of scrimmage (generally wide receivers) may also be in motion along the line.[13] Many teams encourage this unlimited motion, as it can confuse the defence. It also provides receivers the advantage of a running start, as they can time their runs so that they cross the line of scrimmage at speed when the ball is snapped, allowing them to get downfield faster than receivers in American football, allowing for comparatively longer throws in the same amount of time after the snap or quicker throws for a given distance.[citation needed]\\r\\n\\r\\nIn American football, the offensive team must run a play within 25 seconds of the referee whistling the play in ÿ except in the NCAA (college) and the NFL where teams have 40 seconds from the end of the previous play, or 25 seconds following a penalty or timeout. In Canadian football, teams have 20 seconds regardless of the preceding situation.[17]\\r\\n\\r\\nAmerican football rules allow each team to have three timeouts in each half, and the NFL stops play for a two-minute warning. However, NCAA football has no two-minute warning, but the clock stops on a first down until the ball is ready for play if the play ended in the field of play. In the CFL, each team has two time-outs a game but cannot use both in the last three minutes of the game, while at lower levels of Canadian football each team has two. In Canadian football there is a three-minute rather than a two-minute warning. Also, at all levels of Canadian football, the clock is stopped after every play during the last three minutes of each half. Once the referee has set the ball the clock will restart if the last play ended with a runner tackled in the field of play.[citation needed]\\r\\n\\r\\nTiming rules change drastically after the N-minute warning in both leagues:\\r\\n\\r\\nThese timing differences, combined with the fewer downs available for the Canadian offence to earn a first down, lead to spectacularly different end-games if the team leading the game has the ball. In American football, if the other team is out of time-outs, it is possible to run slightly more than 120 seconds (two minutes) off the clock without gaining a first down. In Canadian football, just over 40 seconds can be run off.[citation needed]\\r\\n\\r\\nCanadian football retains much more liberal rules regarding recovery of the ball by members of the kicking team. On any kick, the kicker and any member of the kicker's team behind the kicker at the time of the kick may recover and advance the ball. On a kickoff, since every member of the kicking team must be behind the ball when it is kicked, this effectively makes all twelve players \\"onside\\" and eligible to recover the kick, once it has gone ten yards downfield. On a punt or missed field goal, usually only the kicker is onside, as no one is behind the kicker. All of the players offside at the time of the kick may neither touch the ball nor be within five yards of the member of the receiving team who fields the kick; violation of this rule is a penalty for no yards. The penalty for no yards is 15 yards if the kick is in flight and 5 yards if it has been grounded.[citation needed]\\r\\n\\r\\nThe American rules are similar for the recovery of kickoffs. Any member of the kicking team may recover the ball once it has touched an opponent or once it has gone ten yards downfield and touched the ground. The ball is dead when recovered, though the kicking team is awarded possession at the spot of recovery.[citation needed]\\r\\n\\r\\nThe American rules differ from the Canadian ones for scrimmage kicks. In American rules to recover a scrimmage kick (punt or missed field goal) and retain possession, the ball must be touched beyond the line of scrimmage by a member of the receiving team (defense). If the ball is touched by the receiving team and then recovered by the kicking team, the kicking team will retain possession and be awarded a first down. If the receiving team has not touched the ball before the kicking team touches it, it is first touching as described above in fair catches and punt returns.[citation needed]\\r\\n\\r\\nAdditionally, members of the kicking team must allow the receiving team the opportunity to catch a scrimmage kick in flight. There is no required distance; the NCAA revoked its rule of a 2-yard halo.[19]\\r\\nOnce the scrimmage kick has touched the ground, the kicking team is free to recover, subject to the first touching rules.\\r\\n\\r\\nIn both codes, a scrimmage kick which is blocked and recovered by the kicking team behind the line of scrimmage is in play. The kicking team may then choose to either attempt another kick or try to advance the ball, however no turnover has taken place on the play (unless a member of the receiving team has control of the ball), and therefore, the kicking team either has to advance the ball to the first down marker, or loses the down, which often results in a turnover on downs.[citation needed]\\r\\n\\r\\nUnder Canadian rules, the defensive line can only hold up or block a receiver within one yard of the scrimmage lines. In the NFL, contact up to five yards from the line is allowed. This allows for more open plays in Canadian football.[citation needed]\\r\\n\\r\\nIn Canadian play, if the ball is fumbled out of bounds, the play ends with possession going to the team to last contact the ball in bounds (after the ball has completely left the possession of the fumbling ball carrier). A loose ball may be kicked forward (dribbled) provided it is then recovered by a player who is onside at the time of said kick. The ball may not, however, be intentionally kicked out of bounds to gain possession, this is then treated as a scrimmage kick out of bounds and possession goes to the opposing team. Incidental contact with the foot does not count as kicking the ball out of bounds. In American play, when a ball is fumbled out of bounds, the last team to have clear possession of the football is awarded possession, unless the ball goes out of the back or side of the end zone.\\r\\n\\r\\nA team may still lose possession after a fumble out of bounds if the fumble occurred on fourth down (third down in Canadian play) and the ball becomes dead short of the line to gain. Because of plays like the Holy Roller, the NFL changed its rule regarding advancing a fumbled ball on offense. If the offensive team fumbles in the last two minutes of either half, or on fourth down at any time, only the player who fumbled is allowed to advance the ball past the point of the fumble. If any other offensive player advances the ball toward the opponent's goal line, the ball is moved back to the spot of the fumble, and the defensive team gains possession on downs unless the ball is recovered behind the spot of the fumble.\\r\\n\\r\\nIn Canadian football any kick that goes into the end zone is a live ball, except for a successful field goal or if the goalposts are hit while the ball is in flight. If the player receiving the kick fails to return it out of the end zone, or (except on a kickoff) if the ball was kicked through the end zone, then the kicking team scores a single point (rouge), and the returning team scrimmages from its 35-yard line or, if the rouge is scored as a result of a missed field goal attempt, the receiving team may choose the last point of scrimmage. If a kickoff goes through the end zone without a player touching it or a kicked ball in flight hits a post without scoring a field goal, there is no score, and the receiving team scrimmages from its 25-yard line. If the kick is returned out of the end zone, the receiving team next scrimmages from the place that was reached (or if they reach the opponents' goal line, they score a touchdown); in the amateur levels of the game, they are given the ball at their 20-yard line if the kick was not returned that far.[20]\\r\\n\\r\\nSingles do not exist in American football; however, only one point is counted when a safety is scored during a conversion attempt, in contrast to the two points scored on other safeties.[citation needed]\\r\\n\\r\\nAmerican football also allows a defending team to advance a missed field goal; however, because of the absence of singles and the goalpost position at the back of the end zone, the return is rarely exercised, except on a blocked kick, or as time expires in the half or in the game (with the most famous recent example being Chris Davis' game-ending return of a missed field goal for the winning touchdown in the 2013 AlabamaÿAuburn game). Most teams instead elect not to attempt a return and assume possession  ÿ at the previous line of scrimmage in the NCAA and at the spot of the kick in the NFL. Since the goalpost is out of bounds, any non-scoring kick that strikes the goalpost is dead, and the receiving team takes over possession from the spot of the kick or their own 20-yard line, whichever is further from the receiving team's goal. Likewise, any kickoff or punt which either is kicked through the end zone, is kicked into the end zone and rolls out of bounds (without being touched by a player), is touched in the end zone by a member of the kicking team (with no member of the receiving team having touched it), or is downed in the end zone by a member of the receiving team, results in a touchback ÿ the receiving team is awarded possession on their own 20-yard line. If a player of the receiving team fields a kickoff or punt in the end zone, he has the option to down it in the end zone (resulting in a touchback) or to try and advance the ball.[citation needed]\\r\\n\\r\\nFollowing a successful field goal, in Canadian rules, the team scored upon has the option of receiving a kickoff, kicking off from its 35-yard line, or scrimmaging at its own 35-yard line (the CFL first instituted this rule in 1975, but eliminated this last option for the 2009 season, but it was reinstated for 2010). In American football, there is a kickoff by the scoring team after every score, with the exception of safeties (see below). The option for the scored-upon team to kick off after a touchdown exists in American amateur football, but it is very rarely exercised.[citation needed]\\r\\n\\r\\nCanadian football retains the open-field kick as a legal play, allowing a kick to be taken from anywhere on the field. The open-field kick may be used as a desperation last play by the offense: realizing they are unable to go the length of the field, they advance part of the way and attempt a drop kick, trying to score a field goal, or recover the ball in the end zone for a touchdown.[21]\\r\\n\\r\\nConversely, the defence, facing a last-second field goal attempt in a tie game or game they lead by one point, will often position its punter and place-kicker in the end zone. If the field goal is missed, they can punt the ball back into the field of play and not concede a single. Multiple such kicks may be attempted on the same play. During the October 29, 2010, Toronto Argonauts game against the Montreal Alouettes, four kicks occurred in one play: after a Montreal missed field goal, the Argonauts punted from the end zone to about the 20-yard line. The ball was caught and immediately punted back to the end zone by Montreal to attempt a single, and finally the Argos punted but failed to kick it out of the end zone, where the Alouettes recovered it for a touchdown.[22] [23]\\r\\n\\r\\nAmerican football only allows free kicks and scrimmage kicks made from behind the line of scrimmage; any kick beyond the line of scrimmage or after change of possession would result in a penalty.  (Some levels of American football allow the rare fair catch kick, which according to the NFL rules is neither a free kick nor scrimmage kick, but sui generis.)[citation needed]\\r\\n\\r\\nIn both American and Canadian football, a safety (or safety touch) awards 2 points to the defending team if the offensive team is brought down in their end zone. In American football, the team giving up the safety must take a free kick from their own 20-yard line. In Canadian football, the team being awarded the 2 points has the option of scrimmaging from their own 35-yard line, kicking the ball off from their own 35-yard line, or having the opposing team kick off the ball from their own 35-yard line. In 2009, the CFL changed the latter option to be a kick-off from their own 25-yard line.[24]\\r\\n\\r\\nIn both games, after a touchdown is scored, the scoring team may then attempt one play for additional points. In Canadian football, this play is called a convert, and in American football, it is formally called a try, although it is more commonly referred to as either a conversion, extra point, or point after touchdown (PAT).   The additional points may be earned through a kick or a play from scrimmage. If done via kick, the scoring team gains one point, and if done from a scrimmage, the scoring team gains two.[citation needed]\\r\\n\\r\\nHowever, the position of the ball for attempts is different in the two games. Point-after-touchdown attempts are snapped from the following points (as of the 2015 season):.[citation needed]\\r\\n\\r\\nBecause the goalposts are on the goal line in Canada and the end line in the United States, a CFL kicker is at the same distance from the goalposts as an NFL kicker. Before the 2015 CFL season, that league used the 5-yard line for all attempts, which meant that the Canadian kicker was closer to the goal posts than an American kicker at any level. Amateur Canadian kickers remain closer to their goal posts than their American counterparts.[citation needed]\\r\\n\\r\\nAccording to the rules of both the NFL and NCAA, on conversion attempts, the ball will automatically be spotted in the middle of the field at the appropriate scrimmage line unless a member of the kicking team expressly asks a referee for an alternative placement. Per the rules, the ball can be placed at another spot between the hash marks (especially for strategic positioning on a 2-point conversion attempt) or at another spot further back from the 2-, 3-, or 15-yard line (not uncommon at lower levels of football, since as the season progresses, conditions may worsen toward the center of the field, especially at the spot from which the PAT is usually kicked; the kicker may thus request a spot where the footing is surer).[citation needed]\\r\\n\\r\\nDuring conversions, the ball is considered live in the CFL, American collegiate football, some high school associations, the now-defunct NFL Europa, and starting with the 2015 season the NFL itself. As such, this allows the defensive team to gain two points on an interception or fumble return should they reach the kicking team's end zone, or (in the CFL) one point should the defensive team make an open-field drop kick through the kicking team's goalposts. Conversely, in other levels of American football and amateur Canadian football, defensive teams cannot score during a try attempt.[citation needed]\\r\\n\\r\\nIn Canadian amateur football, the ball is not dead if a player kneels momentarily to, and does, recover a rolling snap, onside/lateral pass, or opponent's kick, while in American amateur football, such a situation produces a dead ball, unless the player is the holder for a place kick. The holder is allowed to catch the snap or recover a rolling snap while on a knee to hold the kick and may also rise to catch a high snap and immediately return to a knee.[citation needed]\\r\\n\\r\\nAt professional levels in both games, unless it is a clearly willful kneel or slide by a ball carrier to go down, a player must be touched while on the ground, otherwise, the player may stand up and continue to advance the ball. Hitting a player who is kneeling, sliding, or clearly intends to run the ball out of bounds (especially quarterbacks) is generally viewed as unsportsmanlike and is often penalized, and in the most blatant of cases (especially if it happens in the dying seconds of a game), the player may be subject to off-field disciplinary action by their respective league governing body, usually in the form of fines or suspensions.[citation needed]\\r\\n\\r\\nThe procedures to settle games that are tied at the end of regulation vary considerably among football leagues.\\r\\n\\r\\nMost leagues other than the NFL, including the CFL, use a procedure frequently called the \\"Kansas Playoff\\", so named because it was first developed for high school football in that state. The rules are summarized here:[citation needed]\\r\\n\\r\\nThe NFL overtime is a modified sudden-death period of 15 minutes, for playoff games only; since the 2017 season, overtime periods in the preseason and regular season are 10 minutes, as part of an overall effort by the NFL to speed up games and reduce their length. If the team that receives the opening kickoff scores a touchdown, or the defensive team scores a safety, the game ends at that point. If the receiving team scores a field goal, the game continues with the scoring team kicking off, and the scored-upon team having a chance at possession. If that team scores a touchdown, or loses possession, the game ends; if it scores a field goal, overtime continues, with the next score by either team ending the game. In the regular season, if a game remains tied after the 10-minute period, it is declared a tie. In postseason games, there are multiple 15-minute periods until a winner is decided.[citation needed]\\r\\n\\r\\nIn American high school and college football, as well as all Canadian football, receivers need only have one foot in bounds for a catch to count as a reception. NFL play requires two feet in bounds and, up through the 2007 season, an NFL official could also award a catch if it was judged that the receiver would have come down in bounds if he had not been pushed by a defender. This rule was based on a judgment call by the official, and was criticized for being inconsistent. The rule was dropped prior to the 2008 season by the NFL.[25]\\r\\n\\r\\nCFL roster sizes are 46 players (rather than 53 as in the NFL, though only 45 will dress for a game). A CFL team may dress up to 44 players comprising 21 non-imports (essentially, Canadians), 20 imports (almost exclusively Americans), and 3 quarterbacks.[26]\\r\\n\\r\\nThe traditional NFL football season runs from the 2nd week of September until late December or the start of January, with the NFL playoffs occurring in January and February. In contrast, the CFL regular season runs from late June to late October. This is in order to ensure the Grey Cup playoffs can be completed in mid-November, before the harsh Canadian winters set in. This is an important consideration for a sport played in outdoor venues in locations such as Regina, Saskatchewan; Toronto, Ontario; Hamilton, Ontario; Calgary, Alberta; Edmonton, Alberta; Winnipeg, Manitoba and Montreal, Quebec.\\r\\n\\r\\nOfficials' penalty flags used in the CFL are orange in color. In American football, officials typically use yellow penalty flags. Conversely, coaches' challenge flags for replays are yellow in the CFL as opposed to red in the NFL. Further, in the CFL, the referee wears a black cap with white piping, and the other officials wear white caps with black piping. In American leagues, the referee wears a solid white cap, and the other officials wear black with white piping.[26]\\r\\n\\r\\nAdditionally, when announcing penalties, in American football, the penalized team is announced using generic terms (\\"offense\\"/\\"defense\\", for example), but in Canadian football (especially the CFL) the penalized team is announced by their respective city or province.\\r\\n\\r\\nAlthough the rules of Canadian and American football have similarities, the differences have a great effect on how teams play and are managed. Generally, the \\"big play\\" is more important in the Canadian game, and offensive series are more difficult to manage.[citation needed]\\r\\n\\r\\nThe red zone is an unofficial designating the portion of the field between the 20-yard line and the goal line. Due to the goalposts' being on the goal line in Canadian football, teams must avoid hitting the goalposts. Thus most touchdown throws are aimed away from the centre portion of the end zone. In the CFL, the goalposts have the same construction as the NFL posts, with the centre post being about 2 yards deep in the end zone. It is extremely rare for CFL passes to hit any part of the posts. When this occurs, a dead ball results. Occasionally, receivers can use the post to good effect in a 'rub' play to shed a defender. End zone passing becomes even more complicated when the corners of the end zone are truncated, as is the case at stadia where the field is bounded by a running track. However, the offensive team enjoys a counteracting advantage of end zones more than twice the size of those in American football (20 yards with a wider field), significantly expanding the area that must be covered by the defensive team and also allowing the freedom to run some pass patterns not available in American football's red zone. Moreover, the rule requiring only a single foot to be in bounds upon pass reception in Canadian football further stretches the amount of area that the offenses have to work with. NFL offenses generally try a run between tackles when on the one-yard line.  CFL offenses make similar attempts on first down on the one-yard line, but second and third down attempts, if required, can be much more varied than their NFL counterparts.[citation needed]\\r\\n\\r\\nThe frequency of punts is highly dependent upon the success, or lack thereof, of the offense. Punt returns are ubiquitous in Canadian football because the \\"no-yards\\" rule permits virtually every punt to be fielded and returned. Moreover, if the kicking team punts the ball out of bounds in an attempt to forestall a return and the ball goes out of bounds between the two 20-yard lines without touching the ground first, a 10-yard penalty is assessed and the ball advanced from where it left play, or the kicking team is backed up 10 yards and must replay the down.[27]  \\"Shanked\\" punts are therefore extremely costly to the kicking team. Though missed field goals may be returned in both national rule sets, the deeper end zone and goal post positioning make this much more common in Canadian rules. TSN on-air analysts state that they are the single play-from-scrimmage most likely to result in a touchdown. This set of special teams play (field goal return units) are rare in the American game to the point where a returner is not a standard part of a defensive field goal unit and will only be seen in unusual circumstances, with one especially notable example being the famous \\"Kick Six\\" college football game in 2013. Canadian kickoffs rarely result in a touchback, so special teams are more prominent in that area of the game as well. The difference in the games' final minutes procedures make comebacks  and the need for an onside kick 'hands' team, more prominent as well. The rule regarding last touch of the ball before leaving the play of field, rather than American football's last possession rule, makes the onside kick more likely to be successful as well. The most complex coaching job in Canadian football is said[by whom?] to be that of special teams co-ordinator. As many as 40 of a CFL roster of players may have a special teams role because of the wide variety of possible situations. In 2014 and 2015, the Edmonton Eskimos even used their third-string quarterbacks (Pat White in 2014 and Jordan Lynch in 2015[28]) as part of their kick and kick-coverage teams. This is highly unusual, as quarterbacks are generally discouraged from making contact plays. Kick returning was a duty generally handled by a player with another role, such as receiver or defensive back. Henry \\"The Gizmo\\" Williams[29] was the first player designated by telecasts as \\"KR\\" for a kicker returner position as his duties were almost entirely for that role, and referring to him as \\"WR\\" for wide receiver was increasingly seen as anachronistic. By far the greatest kick returner in professional football history, Gizmo Williams had more returns for touchdowns called back for infractions than any other player has ever scored (28  26 punts, 2 kick-offs).  No NFL player has enjoyed similar success and the careers of such specialists (like Devin Hester) come nowhere near to matching the impact on the game that such players have in the CFL.[citation needed]\\r\\n\\r\\nHaving three downs on a much longer and wider field with unlimited backfield motion results in Canadian teams requiring faster, more nimble athletes (comparatively) than their American counterparts. Paradoxically, this makes Canadian defense better at defending rushing plays. Rushing plays tend to be unlikely to produce a full ten yard gain, and if correctly anticipated by the defense, much gain at all. The fewer downs means that an unsuccessful rushing play leaves an offense to have a single play to make comparatively longer first down yardage, so rushing plays are less-favored unless the team on offense is actively managing the clock while maintaining the lead. Pundits often like to claim that a Canadian team that rushes for 100 yards or more per game is likely to win, but the reality is winning teams rush the ball in defense of their leads, and not as a tactic to produce drives that lead to points unless they are markedly superior to their opponents. The larger field generally permits greater YAC (yards after catch) on each individual catch, where the NFL produces passing plays where there are either very few YAC yards (immediate tackle) or huge YAC yards (missed tackle or broken coverage for huge gains.)  For this reason, Canadian teams usually prefer passing over rushing to a greater extent than American, since pass attempts generally tend to gain more yards than rushing. Offensive drives (continuous possession of the ball) tend to be shorter. Long drives of half a quarter or more are common in American football but rare in Canadian.[citation needed]\\r\\nIn theory, an NFL team taking possession on their own one-yard line, using three downs for each first-down conversion and the full 40 second clock could run 27 plays and consume a full 18 minutes of clock time covering the 99 yards. A CFL team doing something similar (two plays per conversion, 20 second clock, average 10 seconds of clock time while the officials reset the ball between plays, 109 yards) would run 24 plays and consume 12 minutes of clock at the most. Typical drive lengths in practice reflect this theoretical difference.[citation needed]\\r\\n\\r\\nOne other notable difference is the propensity of CFL quarterbacks to rush the ball, both by design and as a result of reacting to the defense. Damon Allen[30] (the younger brother of Pro Football Hall of Fame running back Marcus Allen) had 11,920 rushing yards to go along with 72,381 passing yards in his 23-year career and actually sits third overall in career rushing yards. Contrast that will Randall Cunnigham's 4928 yards over 16 seasons.[31] 1000 yard rushing seasons for CFL quarterbacks have occurred,[32] and 400+ yard seasons for playoff bound teams' starting quarterbacks, if they remain healthy for the entire schedule, are not unusual.[citation needed]\\r\\n\\r\\nPerhaps the greatest difference arises due to the virtually unlimited movement allowed in the defensive and offensive backfields on a play from scrimmage in the Canadian game vs. very restricted offensive movement in the American game. Combined with the much larger field size, this difference changes the skillsets required of the athletes. Canadian wide receivers, safeties and cornerbacks are far enough from the point where the ball is snapped that they rarely participate in rush offense or defense. Linebackers can be called upon to successfully defend running backs sent to receive passes. There is therefore a much greater premium placed on athletic speed, with former Edmonton Eskimos GM and former wide receiver Ed Hervey (6?ft 0?in?[183?cm], 195?lb?[88?kg], All-American at USC in the 200 meter) and Malcolm Frank[30] (5?ft 8?in?[173?cm] 170?lb?[77?kg]) being prototypical for the CFL. The offence has many more formation options and starting positions, forcing the defence to anticipate more possibilities. Seven of the 12 men on a CFL offense (typically the five linemen and the wide receivers) must be at the line of scrimmage at the time of the snap, and the other five must be at least one yard behind the line. Only the quarterback and linemen must be motionless at the time of the snap, allowing up to six players to be moving toward or along the line at varying speeds (typically the wide receivers are still or at a walking pace at the snap to ensure they are at the line of scrimmage.)[citation needed]\\r\\n\\r\\nIn both the college and pro games, an offensive team with the lead has more difficulty in running out the clock in the Canadian game. In the Canadian Football League, the clock is stopped while the officials place the ball, and then they whistle the game clock and play clock to begin in the last minutes of a half; whereas in the National Football League the clock remains running while the officials set the ball (dependent upon the result of the previous playpenalty, incomplete pass, out-of-bounds, or tackle inbounds in both leagues) while the play clock of 40 seconds runs down. The game clock only begins again when the play is whistled in, for an inbounds tackle, or at the snap of the ball for the other outcomes in the CFL. A team that is ahead has one fewer opportunity to kill clock time in the Canadian game with three downs, and can only take the play clock time (20 seconds) and the length of the play itself off the clock with each down. After the three-minute warning, a penalty of a loss of down (on first and second down, 10 yards on third down) results for failing to start the new play in time (time count violation). Additionally, if a Canadian team commits a time count violation on third down, the referee has the right to require that it legally start a new play before the play clock expires, and can award possession to the defending team if another time count is committed. If a team that is trailing in the CFL can begin to produce two-and-outs on defense and efficient scoring drives on offense, 14 and even 17 points can be successfully scored in the final three minutes. This comeback proclivity is so pronounced that the CFL uses it for marketing purposes: No Lead Is Safe.[33]","input":"How many yards in a cfl football field?"},{"output":"December 12, 1948","context":"This history of McDonald's is an overview of the original restaurant and of the chain.\\r\\n\\r\\n\\r\\nThe McDonald family moved from Manchester, New Hampshire to Hollywood, California in the late 1930s, where brothers Richard and Maurice McDonald began working as set movers and handymen at Motion-Picture studios.[1] In 1937, their father Patrick McDonald opened \\"The Airdrome\\", a food stand, on Huntington Drive (Route 66) near the Monrovia Airport in the Los Angeles County city of Monrovia, California[2] with hot dogs being one of the first items sold. Hamburgers were later added to the menu at a cost of ten cents with all-you-can-drink orange juice at five cents. In 1940, Maurice and Richard (\\"Mac\\" and \\"Dick\\") moved the entire building 40 miles (64?km) east, to West 14th and 1398 North E Streets in San Bernardino, California. The restaurant was renamed \\"McDonald's Bar-B-Que\\" and had 25 menu items, mostly barbecue.\\r\\nIn October 1948, after the McDonald brothers realized that most of their profits came from selling hamburgers, they closed down their successful carhop drive-in to establish a streamlined system with a simple menu which consisted of only hamburgers, cheeseburgers, potato chips, coffee, soft drinks, and apple pie.[3] After the first year, potato chips and pie were swapped out for french fries and milkshakes. The carhops were eliminated, making the new restaurant a self-service operation. Richard and Maurice took great care in setting up their kitchen like an assembly line to ensure maximum efficiency. The restaurant's name was changed again, this time to simply \\"McDonald's,\\" and reopened on December 12, 1948.\\r\\nIn April 1952, the brothers decided they needed an entirely new building in order to achieve two goals: further efficiency improvements, and a more eye-catching appearance. They collected recommendations for an architect and interviewed at least four, finally choosing Stanley Clark Meston, an architect practicing in nearby Fontana.[1] The brothers and Meston worked together closely in the design of their new building. They achieved the extra efficiencies they needed by, among other things, drawing the actual measurements of every piece of equipment in chalk on a tennis court behind the McDonald house (with Meston's assistant Charles Fish).[4] The new restaurant's design achieved a high level of noticeability thanks to gleaming surfaces of red and white ceramic tile, stainless steel, brightly colored sheet metal, and glass; pulsing red, white, yellow, and green neon; and two 25-foot yellow sheet-metal arches trimmed in neon, called \\"golden arches\\" even at the design stage. A third, smaller arch sign at the roadside hosted a pudgy character in a chef's hat, known as Speedee, striding across the top, trimmed in animated neon. Further marketing techniques were implemented to change McDonald's from a sit down restaurant to a fast food chain. They used such things as turning off the heating to prevent people wanting to stay so long, fixed and angled seating so the customer would sit over their food promoting them to eat faster, spreading the seats further apart so being less of a sociable place to dine in, and giving their customers branded cone shaped cups forcing them to hold their drink whilst eating which would speed up the eating process. Many other companies followed McDonald's strategies to turn their own restaurants into fast food establishments including Burger King, White Castle and Subway.[1]\\r\\nIn late 1953, with only a rendering of Meston's design in hand, the brothers began seeking franchisees.[1] Their first franchisee was Neil Fox, a distributor for General Petroleum Corporation. Fox's stand, the first with Meston's golden arches design, opened in May 1953 at Central Avenue and Indian School Road in Phoenix, Arizona. Their second franchisee was the team of Fox's brother-in-law Roger Williams and Burdette \\"Bud\\" Landon, both of whom also worked for General Petroleum. Williams and Landon opened their stand on August 18, 1953 at 10207 Lakewood Boulevard in Downey, California. The Downey stand has the distinction of being the oldest surviving McDonald's restaurant.[5] The Downey stand was never required to comply with the McDonald's Corporation's remodeling and updating requests over the years because it was franchised not by the McDonald's Corporation, but by the McDonald brothers themselves to Williams and Landon.\\r\\nIn 1954, Ray Kroc, a seller of Prince Castle brand Multimixer milkshake machines, learned that the McDonald brothers were using eight of his machines in their San Bernardino restaurant. His curiosity was piqued, and he went to take a look at the restaurant. He was joined by good friend Charles Lewis who had suggested to Kroc several improvements to the McDonald's burger recipe.\\r\\nBelieving the McDonald's formula was a ticket to success, Kroc suggested they franchise their restaurants throughout the country. The brothers were skeptical, however, that the self-service approach could succeed in colder, rainier climates; furthermore, their thriving business in San Bernardino, and franchises already operating or planned, made them reluctant to risk a national venture.[1] Kroc offered to take the major responsibility for setting up the new franchises elsewhere. He returned to his home outside of Chicago with rights to set up McDonald's restaurants throughout the country, except in a handful of territories in California and Arizona already licensed by the McDonald brothers. The brothers were to receive one-half of one percent of gross sales.[1] Kroc's first McDonald's restaurant opened on April 15, 1955, at 400 North Lee Street in Des Plaines, Illinois, near Chicago. The Des Plaines interior and exterior was painted by master painter Eugene Wright, who owned Wright's Decorating Service. Eugene was asked to come up with a color scheme and he chose yellow and white, with dark brown and red being secondary trim colors. Those colors would go on to become the colors of all McDonald's franchises. (Recognizing its historic and nostalgic value, in 1990 the McDonald's Corporation acquired the stand and rehabilitated it to a modern but nearly original condition, and then built an adjacent museum and gift shop to commemorate the site.)\\r\\n Once the Des Plaines restaurant had become operational, Kroc sought franchisees for his McDonald's chain. The first snag came quickly. In 1956 he discovered that the McDonald brothers had licensed the franchise rights for Cook County, Illinois to the Frejlach Ice Cream Company. Kroc was incensed that the McDonald's had not informed him of this arrangement. He purchased the rights back for $25,000 ($225,000 today), five times what the Frejlacks had originally paid, and pressed forward. McDonald's grew slowly for its first three years. By 1958, there were 34 restaurants. In 1959, however, Kroc opened 68 new restaurants, bringing the total to 102 locations.\\r\\nIn 1960, the McDonald's advertising campaign \\"Look for the Golden Arches\\" gave sales a big boost. Kroc believed that advertising was an investment that would in the end come back many times over, and advertising has always played a key role in the development of the McDonald's Corporation. In 1962, McDonald's introduced its now world-famous Golden Arches logo. A year later, the company sold its millionth hamburger and introduced Ronald McDonald, a red-haired clown designed to appeal to children.\\r\\nIn the early 1960s, McDonald's really began to take off. The growth in U.S. automobile use that came with suburbanization and the interstate highway system contributed heavily to McDonald's success. In 1961 Kroc bought out the McDonald brothers for $2.7 million, aiming at making McDonald's the number one fast-food chain in the country.\\r\\nOn May 3, 1960 Kroc assisted Christopher Boulos in opening a McDonald's franchise in DeKalb, Illinois. By 1965, the McDonald's at 805 W. Lincoln Highway sold over 4 million burgers and 1,000,000 pounds (450,000?kg) of fries. Boulos was the first Greek-American McDonald's franchise operator.[6]\\r\\nIn 1965, McDonald's Corporation went public. Common shares were offered at $22.50 per share. By the end of the first day's trading, the price had risen to $30. A block of 100 shares purchased for $2,250 in 1965 was worth, after 12 stock splits (increasing the number of shares to 74,360), over $5.7 million as of year-end market close on December 31, 2010. In 1980, McDonald's Corporation became one of the 30 companies that make up the Dow Jones Industrial Average.\\r\\nMcDonald's success in the 1960s was in large part due to the company's skillful marketing and flexible response to customer demand. In 1962, the Filet-O-Fish sandwich, billed as \\"the fish that catches people\\", was introduced in McDonald's restaurants.[7] The new item had originally met with disapproval from Kroc, but after its successful test marketing, he eventually agreed to add it. Another item that Kroc had backed a year previously, a burger with a slice of pineapple and a slice of cheese, known as a \\"hulaburger\\", had flopped (both it and the Filet-O-Fish were developed in Catholic neighborhoods where burger sales dropped off markedly on Fridays and during Lent). The market was not quite ready for Kroc's taste; the hulaburger's tenure on the McDonald's menu board was short. In 1968 the now legendary Big Mac made its debut, and in 1969 McDonald's sold its five billionth hamburger. Two years later, as it launched the \\"You Deserve a Break Today\\" advertising campaign, McDonald's restaurants had reached all 50 states.\\r\\nIn 1968, McDonald's opened its 1,000th restaurant, and Fred L. Turner became the company's president and chief administrative officer. Kroc became chairman and remained CEO until 1973. Turner had originally intended to open a McDonald's franchise, but when he had problems with his backers over a location, he went to work as a grillman for Kroc in 1956. As operations vice president, Turner helped new franchisees get their stores up and running. He was constantly looking for new ways to perfect the McDonald's system, experimenting, for example, to determine the maximum number of hamburger patties one could stack in a box without squashing them and pointing out that seconds could be saved if McDonald's used buns that were presliced all the way through and were not stuck together in the package. Such attention to detail was one reason for the company's extraordinary success.\\r\\nBy the late 1960s, many of the candy-striped Golden Arches stores had been modified with enclosed walk-up order areas and limited indoor seating. In June 1969, McDonald's introduced a new \\"mansard roof\\" building design featuring indoor seating. The natural brick and cedar shake look mansards were a response to critics who berated McDonald's architecture as too garish. It became the standard for McDonald's restaurants, and franchise holders were ultimately required to demolish older restaurants and replace them with the new design. The first McDonald's restaurant using the \\"mansard roof\\" design opened that same year in the Chicago suburb of Matteson.\\r\\nMcDonald's spectacular growth continued in the 1970s. Americans were more on-the-go than ever, and fast service was a priority. In 1972, the company passed $1 billion in annual sales. By 1976, McDonald's had served 20 billion hamburgers, and system wide sales exceeded $3 billion.\\r\\nThe company pioneered breakfast fast food with the introduction of the Egg McMuffin in 1972 when market research indicated that a quick breakfast would be welcomed by consumers. Five years later McDonald's added a full breakfast line to the menu, and by 1987 one-fourth of all breakfasts eaten out in the United States came from McDonald's restaurants. In test market locations, such as New York City, McDonald's added a full breakfast line to its menus in 1975.\\r\\nKroc was a firm believer in giving \\"something back into the community where you do business\\". In 1974 McDonald's acted upon that philosophy in an original way by opening the first Ronald McDonald House, in Philadelphia, to provide a \\"home away from home\\" for the families of children in nearby hospitals. Twelve years after this first house opened, 100 similar Ronald McDonald Houses were in operation across the United States.\\r\\nThere was some skepticism in the company's phenomenal growth internationally. When Wally and Hugh Morris approached the corporation in 1974 to bring McDonald's into New Zealand, they were firmly shunned by Kroc, citing a visit to the country and saying \\"There aren't any people... I never met a more dead-than-alive hole in my life.\\" Persistence by the brothers eventually led to their request being granted in May 1975. They managed to negotiate a deal with the corporation by selling New Zealand cheese to the US to offset the high costs of importing plant equipment. The first New Zealand restaurant opened in June 1976 at Porirua, near Wellington, to much more success than the corporation predicted.[8]\\r\\nIn 1975, McDonald's opened its first drive-thru window in Sierra Vista, Arizona, following Wendy's lead. This service gave Americans a fast, convenient way to procure a quick meal. The company's goal was to provide service in 50 seconds or less. Drive-thru sales eventually accounted for more than half of McDonald's systemwide sales. Meantime, the Happy Meal, a combo meal for children featuring a toy, was added to the menu in 1979. A period of aggressive advertising campaigns and price slashing in the early 1980s became known as the \\"burger wars\\". Burger King suggested to customers: \\"have it your way\\"; Wendy's offered itself as the \\"fresh alternative\\" and launched their \\"Where's the beef?\\" campaign. McDonald's sales and market still predominated, however.\\r\\nDuring the 1980s, a period of substantial expansion, McDonald's further diversified its menu to suit changing consumer tastes. The company introduced the McChicken in 1980; it proved to be a sales disappointment, and was replaced with Chicken McNuggets a year later (having originally been invented by Rene Arend in 1979). In 1985, ready-to-eat salads were introduced. Efficiency, combined with an expanded menu, continued to draw customers. McDonald's began to focus on urban centers and introduced new architectural styles.\\r\\nThe first McDonald's Express locations opened in 1991. These are smaller-scale prototypes, usually constructed in prefabricated buildings or urban storefronts, that do not feature certain menu items such as milkshakes and Quarter Pounders.[9]\\r\\nIn 1992 Michael R. Quinlan became president of McDonald's Corporation, and Fred L. Turner became chairman. Quinlan, who took over as CEO in 1987, had started at McDonald's in the mail room in 1963, and gradually worked his way up. In his first year as CEO, the company opened 600 new restaurants.\\r\\nBy 1991, 37 percent of system-wide sales came from restaurants outside the United States. McDonald's opened its first foreign restaurant in British Columbia, Canada, in 1967. By the early 1990s the company had established itself in 58 foreign countries, and operated more than 3,600 restaurants outside the United States, through wholly owned subsidiaries, joint ventures, and franchise agreements. Its strongest foreign markets were Japan, Canada, Germany, Great Britain, Australia, and France.\\r\\nIn the mid-1980s, McDonald's, like other traditional employers of teenagers, was faced with a shortage of labor in the United States. The company met this challenge by being the first to entice retirees back into the workforce. Focusing on off-site training, it opened its Hamburger University in 1961 to train franchisees and corporate decision-makers. By 1990, more than 40,000 people had received \\"Bachelor of Hamburgerology\\" degrees from the 80-acre (320,000?m2) Oak Brook, Illinois, facility. The corporation opened a Hamburger University in Tokyo in 1971, in Munich in 1975, and in London in 1982.\\r\\nBraille menus were first introduced in 1979, and picture menus in 1988. In March 1992, combination Braille and picture menus were reintroduced to accommodate those with vision, speech, or hearing impairments.\\r\\nClamshell grills, which cooked both sides of a hamburger simultaneously, were tested. New locations such as hospitals and military bases were tapped as sites for new restaurants. In response to the increase in microwave oven usage, McDonald's, whose name is the single most advertised brand name in the world, stepped up advertising and promotional expenditures stressing that its taste was superior to quick-packaged foods.\\r\\nThe first McDonald's in Mainland China opened in Dongmen, Shenzhen in October 1990.[10]\\r\\nMcRecycle USA began in 1990 and included a commitment to purchase at least $100 million worth of recycled products annually for use in construction, remodeling, and equipping restaurants. Chairs, table bases, table tops, eating counters, table columns, waste receptacles, corrugated cartons, packaging, and washroom tissue were all made from recycled products. McDonald's worked with the U.S. Environmental Defense Fund to develop a comprehensive solid waste reduction program. Wrapping burgers in paper rather than plastic led to a 90 percent reduction in the wrapping material waste stream.\\r\\nIt took McDonald's 33 years to open its first 10,000 restaurants. The 10,000th unit opened in April 1988. Incredibly, the company reached the 20,000-restaurant mark in only eight more years, in mid-1996. By the end of 1997 the total had surpassed 23,000, and by that time McDonald's was opening 2,000 new restaurants each year, an average of five every day.\\r\\nMuch of the growth of the 1990s came outside the US, with international units increasing from about 3,600 in 1991 to more than 11,000 by 1998. The number of countries with McDonald's outlets nearly doubled from 59 in 1991 to 114 in late 1998. In 1993, a new region was added to the empire when the first McDonald's in the Middle East opened in Tel Aviv, Israel. As the company entered new markets, it showed increasing flexibility with respect to local food preferences and customs. In Israel, for example, the first kosher McDonald's opened in a Jerusalem suburb in 1995. In Arab countries the restaurant chain used \\"Halal\\" menus, which complied with Islamic laws for food preparation. In 1996 McDonald's entered India for the first time, where it offered a Big Mac made with lamb called the Maharaja Mac. That same year the first McSki-Thru opened in Lindvallen, Sweden.\\r\\nOverall, the company derived increasing percentages of its revenue and income from outside the US. In 1992 about two-thirds of systemwide sales came from U.S. McDonald's, but by 1997 that figure was down to about 51 percent. Similarly, the operating income numbers showed a reduction from about 60 percent of sales derived from the US in 1992 to 42.5 percent in 1997.\\r\\nIn the US, the number of units grew from 9,000 in 1991 to 12,500 in 1997, an increase of about 40 percent. Although the additional units increased market share in some markets, a number of franchisees complained that new units were cannibalizing sales from existing ones. Same-store sales for outlets open for more than one year were flat in the mid-1990s, a reflection of both the greater number of units and the mature nature of the U.S. market.\\r\\nThe company made several notable blunders in the US in the 1990s which hurt stateside profits. The McLean Deluxe sandwich, which featured a 91 percent fat-free beef patty, was introduced in 1991, never really caught on, and was dropped from the menu in February 1996 to make room for the Arch Deluxe, itself an underperforming product. The \\"grown-up\\" (and pricey) Arch Deluxe sandwich was launched in May 1996 and the Deluxe Line was launched in September 1996 in a $200 million campaign to gain the business of more adults, but were bombs. The following spring brought a 55-cent Big Mac promotion, which many customers either rejected outright or were confused by because the burgers had to be purchased with full-priced fries and a drink. The promotion embittered still more franchisees, whose complaints led to its withdrawal. In July 1997 McDonald's fired its main ad agency, Leo Burnett, a 15-year McDonald's partner after the nostalgic \\"My McDonald's\\" campaign proved a failure. Several other 1990s-debuted menu items, including fried chicken, pasta, fajitas, and pizza failed as well. A seemingly weakened McDonald's was the object of a Burger King offensive when the rival fast-food maker launched the Big King sandwich, a Big Mac clone. Meanwhile, internal taste tests revealed that customers preferred the fare at Wendy's and Burger King.\\r\\nIn response to these difficulties, McDonald's drastically cut back on its U.S. expansion. In contrast to the 1,130 units opened in 1995, only about 400 new McDonald's were built in 1997. Plans to open hundreds of smaller restaurants in Wal-Marts and gasoline stations were abandoned because test sites did not meet targeted goals. Reacting to complaints from franchisees about poor communication with the corporation and excess bureaucracy, the head of McDonald's U.S.A. (Jack M. Greenberg, who had assumed the position in October 1996) reorganized the unit into five autonomous geographic divisions. The aim was to bring management and decision-making closer to franchisees and customers.\\r\\nOn the marketing side, McDonald's scored big in 1996 and 1997 with a Teenie Beanie Baby promotion in which about 80 million of the toys/collectibles were gobbled up virtually overnight. The chain received some bad publicity, however, when it was discovered that a number of customers purchased Happy Meals just to get the toys and threw the food away. For a similar spring 1998 Teenie Beanie giveaway, the company altered the promotion to allow patrons to buy menu items other than kids' meals. McDonald's also began to benefit from a seven years global marketing alliance signed with Disney/Pixar in 1998. Initial Disney/Pixar movies promoted by McDonald's included A Bug's Life, Monsters, Inc., Finding Nemo and The Incredibles. Perhaps the most important marketing move came in the later months of 1997 when McDonald's named DDB Needham as its new lead ad agency. Needham had been the company's agency in the 1970s and was responsible for the hugely successful \\"You Deserve a Break Today\\" campaign. Late in 1997, McDonald's launched the Needham-designed \\"Did Somebody Say McDonald's?\\" campaign, which appeared to be an improvement over its predecessors.\\r\\nFollowing the difficulties of the early and mid-1990s, several moves in 1998 seemed to indicate a reinvigorated McDonald's. In February the company for the first time took a stake in another fast-food chain when it purchased a minority interest in the 16-unit, Colorado-based Chipotle Mexican Grill chain. The following month came the announcement that McDonald's would improve the taste of several sandwiches and introduce several new menu items. McFlurry desserts, developed by a Canadian franchisee in 1997, proved popular when launched in the United States in the summer of 1998. That same month, McDonald's said that it would overhaul its food preparation system in every U.S. restaurant. The new just-in-time system, dubbed \\"Made for You\\", was in development for a number of years and aimed to deliver to customers \\"fresher, hotter food\\"; enable patrons to receive special-order sandwiches (a perk long offered by rivals Burger King and Wendy's); and allow new menu items to be more easily introduced thanks to the system's enhanced flexibility. The expensive changeover was expected to cost about $25,000 per restaurant, with McDonald's offering to pay for about half of the cost; the company planned to provide about $190 million in financial assistance to its franchisees before implementation was completed by year-end 1999.\\r\\nIn May 1998, Greenberg was named president and CEO of McDonald's Corporation, with Quinlan remaining chairman; at the same time Alan D. Feldman, who had joined the company only four years earlier from Pizza Hut, replaced Greenberg as president of McDonald's U.S.A., an unusual move for a company whose executives typically were long-timers. The following month brought another first, McDonald's first job cuts. The company said it would eliminate 525 employees from its headquarters staff, a cut of about 23 percent. In the second quarter of 1998 McDonald's took a $160 million charge in relation to the cuts. As a result, the company, for the first time since it went public in 1965, recorded a decrease in net income, from $1.64 billion in 1997 to $1.55 billion in 1998.\\r\\nMcDonald's followed up its investment in Chipotle with several more moves beyond the burger business. In March 1999 the company bought Aroma Caf, a UK chain of 23 upscale coffee and sandwich shops. In July of that year McDonald's added Donatos Pizza, a midwestern chain of 143 pizzerias based in Columbus, Ohio. Donatos had revenues of $120 million in 1997. In 1999, McDonald's 25,000th unit opened, Greenberg took on the additional post of chairman, and Jim Cantalupo was named company president. Cantalupo, who had joined the company as controller in 1974 and later became head of McDonald's International, had been vice-chairman, a position he retained. In May 2000 McDonald's completed its largest acquisition yet, buying the bankrupt Boston Market chain for $173.5 million in cash and debt. At the time, there were more than 850 Boston Market outlets, which specialized in home-style meals, with rotisserie chicken the lead menu item. Revenue at Boston Market during 1999 totaled $670 million. McDonald's rounded out its acquisition spree in early 2001 by buying a 33 percent stake in Pret A Manger, an upscale urban-based chain specializing in ready-to-eat sandwiches made on the premises. There were more than 110 Pret shops in the United Kingdom and several more in New York City. Also during 2001, McDonald's sold off Aroma Caf and took its McDonald's Japan affiliate public, selling a minority stake through an initial public offering.\\r\\nAs it was exploring new avenues of growth, even though, McDonald's core burger chain had become plagued by problems. Most prominently, the Made for You system backfired. Although many franchisees believed that it succeeded in improving the quality of the food, it also increased service times and proved labor-intensive. Some franchisees also complained that the actual cost of implementing the system ran much higher than the corporation had estimated, a charge that McDonald's contested. In any case, there was no question that Made for You failed to reverse the chain's sluggish sales. Growth in sales at stores open more than a year (known as same-store sales) fell in both 2000 and 2001. Late in 2001 the company launched a restructuring involving the elimination of about 850 positions, 700 of which were in the US, and several restaurant closings.[citation needed]\\r\\nIn 2000, a McDonald's in Dearborn, Michigan in Greater Detroit was the first one in Michigan and the only one east of the Mississippi River to offer halal food for Muslim customers.[11]\\r\\nThere were further black eyes as well. McDonald's was sued in 2001 after it was revealed that for flavoring purposes a small amount of beef extract was being added to the vegetable oil used to cook the french fries. The company had cooked its fries in beef tallow until 1990, when it began claiming in ads that it used 100 percent vegetable oil. McDonald's soon apologized for any \\"confusion\\" that had been caused by its use of the beef flavoring, and in mid-2002 it reached a settlement in the litigation, agreeing to donate $10 million to Hindu, vegetarian, and other affected groups. Also in 2001, further embarrassment came when 51 people were charged with conspiring to rig McDonald's game promotions over the course of several years. It was revealed that $24 million of winning McDonald's game tickets had been stolen as part of the scam. McDonald's was not implicated in the scheme, which centered on a worker at an outside company that had administered the promotions.\\r\\nMcDonald's also had to increasingly battle its public image as a purveyor of fatty, unhealthy food. Consumers began filing lawsuits contending that years of eating at McDonald's had made them overweight. McDonald's responded by introducing low-calorie menu items and switching to a more healthful cooking oil for its french fries. McDonald's franchises overseas became a favorite target of people and groups expressing anti-American and/or anti-globalization sentiments. In August 1999 a group of protesters led by farmer Jos Bov destroyed a half-built McDonald's restaurant in Millau, France. In 2002 Bov, who gained fame from the incident, served a three-month jail sentence for the act, which he said was in protest against U.S. trade protectionism. McDonald's was also one of three multinational corporations (along with Starbucks Corporation and Nike, Inc.) whose outlets in Seattle were attacked in late 1999 by some of the more aggressive protesters against a World Trade Organization meeting taking place there. In the early 2000s McDonald's pulled out of several countries, including Bolivia and two Middle Eastern nations, at least in part because of the negative regard with which the brand was held in some areas.\\r\\nEarly in 2002, Cantalupo retired after 28 years of service. Sales remained lackluster that year, and in October the company attempted to revive U.S. sales through the introduction of a low-cost Dollar Menu. In December 2002, after this latest initiative to reignite sales growth failed and also after profits fell in seven of the previous eight quarters, Greenberg announced that he would resign at the end of the year. Cantalupo came out of retirement to become chairman and CEO at the beginning of 2003.\\r\\nCantalupo started his tenure by announcing a major restructuring that involved the 2002 quarterly loss, which included the closure of more than 700 restaurants (mostly in the United States and Japan), the elimination of 600 jobs, and charges of $853 million. The charges resulted in a fourth-quarter 2002 loss of $343.8 million, the first quarterly loss in McDonald's 38 years as a public company. The new CEO also shifted away from the company's traditional reliance on growth through the opening of new units to a focus on gaining more sales from existing units. By 2003, with Ray Kroc's McDonald's Corporation nearly 50 and the McDonald's fast food restaurant concept itself old enough to qualify for AARP membership, the brand had perhaps become too familiar and sales figures stalled. Analysts, management, owners, and customers alike recognized that the aged chain required revivification. The question in need of solution was: How should McDonald's reinvent itself without losing its core values and maintain relevance in the marketplace? To that end, several new menu items were successfully launched, including entree salads, McGriddles breakfast sandwiches (which used pancakes in place of bread), and white-meat Chicken McNuggets. Some outlets began test-marketing fruits and vegetables as Happy Meal options. It was quickly determined that focus on customer experience was key in reversing the slippage. Then, a new global marketing campaign was adopted which was designed around the notion of the \\"Rolling Energy\\" phase. Launched on September 29, 2003, the campaign began featuring youthful images, hip music, and pop culture celebrities touting the tagline, \\"I'm lovin' it\\". Next, James R. Cantalupo was called back from retirement to head the corporation and its efforts to recapture golden luster. His plan was to keep things simple with a focus on the basics like customer service, clean restrooms, and reliable appealing food (not unlike Ray Kroc's mantra of QSC and V: Quality, Service, Cleanliness, and Value). In addition to the basics he determined to position the company with a more modern coherent image in order to foster a McDonald's \\"experience\\" for customers. More than an advertising campaign he and his team approved sweeping new architecture for McDonald's restaurants, the first major overhaul since 1969 when the now universally recognized signature double mansard roof became standard. In fact, Mr. Cantalupo personally approved abandonment of the ubiquitous and familiar mansard in favor of what became the \\"Forever Young\\" prototype topped with its swish eyebrow. This was the first global campaign in McDonald's history, as the new slogan was to be used in advertising in more than 100 countries. It also proved to be the first truly successful ad campaign in years; sales began rebounding, helped also by improvements in service. Cantalupo did not live to see the fruits of his labor and he died in 2004 just as his modern vision for McDonald's was getting underway. Nonetheless he had set things into motion causing a paradigm shift for the company resulting in a refreshed image without a dilution of brand identity.\\r\\nIn December 2003, for instance, same-store sales increased 7.3 percent. Same-store sales rose 2.4 percent for the entire year, after falling 2.1 percent in 2002. Also, in that month, McDonald's announced that it would further its focus on its core hamburger business by downsizing its other ventures. The company said that it would sell Donatos back to that chain's founder. In addition, it would discontinue development of non-McDonald's brands outside of the United States. This included Boston Market outlets in Canada and Australia and Donatos units in Germany. McDonald's kept its minority investment in Pret A Manger, but McDonald's Japan was slated to close its Pret units there. These moves would enable the company to concentrate its international efforts on the McDonald's chain, while reducing the non-hamburger brands in the United States to Chipotle and Boston Market, both of which were operating in the black.\\r\\nMcDonald's continued to curtail store openings in 2004 and to concentrate on building business at existing restaurants. Much of the more than $1.5 billion budgeted for capital expenditures in 2004 was slated to be used to remodel existing restaurants. McDonald's also aimed to pay down debt by $400 million to $700 million and to return approximately $1 billion to shareholders through dividends and share repurchases. Cantalupo also set several long-term goals, such as sustaining annual systemwide sales and revenue growth rates of 3 to 5 percent. In a move to both simplify the menu and make its offerings less fattening, McDonald's announced in March 2004 that it would phase out Super Size french fries and soft drinks by the end of the year.\\r\\nIn the 1960s, the 1970s, the 1980s, and the 1990s, \\"no loitering\\" had been McDonald's motto. Ray Kroc had decreed upon the origins of his version of the chain that pay telephones, jukeboxes, and vending machines of any kind were forbidden at McDonald's restaurants. The goal had been to quickly serve customers and not force them to stay in the restaurants any longer than it took them to eat a hamburger. Along that line of thinking, dining areas were designed with minimalist hard plastic tables and chairs which were more often than not bolted in place. Thus customers consumed their fast food in scant comfort without dillydally allowing room for the next hurrying customers.\\r\\nWith the new \\"Forever Young\\" design (adopted in 2006), the first major redesign since 1969, McDonald's turned a new page for itself. New and remodeled restaurants feature dining zones with \\"distinct personalities\\". Most of them offer three sections or zones. A linger zone was designed to accommodate people who were inclined to dawdle and socialize while sitting comfortably on armchairs or sofas using free wifi access. Another zone offers counters and stools for patrons in a hurry who might just grab and go. The third and perhaps most important zone is the one for families or groups where seating arrangements can be reconfigured to meet a variety of needs. Harsh colors and hard plastics have been replaced with custom earth tones and flexible, padded, fabric-covered booth-seating, all in hopes of engaging diners to loiter and perhaps spend more money. In addition to architecture and furnishings, the McDonald's menu has been tweaked to offer a larger variety of what the corporation refers to as more healthy food.\\r\\nMcDonald's franchises are required to follow the directions of the parent company and perhaps more than a few have complained about the Forever Young changes. First, customers needed to recognize the mansard buildings and identify McDonald's with thema new look may initially generate some degree of confusion. The next objection is cost: as of 2008, a newly built swish-brow store was said to cost upwards of $1 million and renovation of an existing unit to meet the new standards as much as $400,000. With a large percentage of sales from drive-in business, franchises could argue that the expensive interior redesign is unwarranted for their bottom lines.\\r\\nIn May 2010, McDonald's redesigned its US website to a sleeker, HTML 5 friendly interface. Along with those changes, McDonald's also introduced new advertising material to its website, including the unveiling of new pictures used exclusively for in-restaurant ads, television commercials, print advertising, and online advertising, which consist of more realistic pictures of its products, which are now up close and face the camera instead of facing left or right.\\r\\nIn July 2011, McDonald's announced that their largest restaurant in the world would be built on the 2012 London Olympics site. The restaurant contains over 1,500 seats and is half the length of an American football field. Over 470 staff were employed serving on average (during the 2012 Olympics) 100,000 portions of fries, 50,000 Big Macs and 30,000 Milkshakes. This restaurant will overshadow the current largest McDonald's in the world in Moscow, Russia.[citation needed]\\r\\nIn January 2012, the company announced revenue for 2011 reached an all-time high of $27 billion, and that 2,400 restaurants would be updated and 1,300 new ones opened worldwide.[12]\\r\\nIn the middle of the decade, the restaurant began to suffer from declining profits.[13] In response, McDonald's began to offer its breakfast menu all day starting in 2015.[14] At first, the launch was unpopular with franchisees who claimed that the changes caused service to slow down.[13] However, the plan paid off with CNBC reporting that the company's fourth quarter earning \\"easily topped analysts' forecasts\\".[15]\\r\\nmagazine ad appears in Life magazine.","input":"When did the first mcdonald's open up?"},{"output":"Durango, Mexico","context":"Chisum is a 1970 Warner Bros. Technicolor Western film starring John Wayne in Panavision.\\r\\nThe large cast also includes Forrest Tucker, Christopher George, Ben Johnson, Glenn Corbett, Geoffrey Deuel, Andrew Prine, Bruce Cabot, Patric Knowles, and Richard Jaeckel.\\r\\nDirected by Andrew V. McLaglen, it was adapted for the screen by Andrew J. Fenady from his short story, \\"Chisum and the Lincoln County Cattle War\\".\\r\\nAlthough this movie is historically inaccurate in many details, it is loosely based on events and characters from the Lincoln County War of 1878 in the New Mexico Territory, which involved historical figures John Chisum, (1824-1884), Pat Garrett (1850-1908), and Billy the Kid (1859-1881), [William H. Bonney] among others.\\r\\n\\r\\n\\r\\nJohn Chisum (John Wayne), a virtuous, patriarchal land baron, locks horns with greedy Lawrence Murphy (Forrest Tucker), who will stop at nothing to get control of the trade and even the law in Lincoln County, New Mexico Territory.\\r\\nChisum is an aging rancher with an eventful past and a paternalistic nature towards his companions and community. Murphy, a malevolent land developer, plans to take control of the county for his own personal gain.\\r\\nThe story begins with Murphy's men tipping off Mexican rustlers who plan to steal Chisum's horses. Chisum and his sidekick Pepper (Ben Johnson) stop the bandits with help from a newcomer to the area, William H. Bonney (Geoffrey Deuel), also known as \\"Billy the Kid\\". A notorious killer, Billy has been given a chance to reform by Chisum's philanthropic Britisher neighbor, rancher Henry Tunstall (Patric Knowles). Billy also falls for Chisum's newly arrived niece, Sallie (Pamela McMyler).\\r\\nMurphy is buying up all the stores in town and using his monopoly to push up the prices. He appoints his own county sheriff and deputies. He also brings in a lawyer, Alexander McSween (Andrew Prine), whose principles lead him to switch sides and seek work with Chisum and Tunstall. The two ranchers set up their own bank and general store in town under McSween's control.\\r\\nChisum's land and cattle remain targets. Murphy's men attempt to steal Chisum's cattle before he can sell them to the United States Army. Chisum's ranch hands are warned by Pat Garrett (Glenn Corbett), a passing buffalo hunter. Garrett agrees to help Chisum and soon befriends Bonney. Together they foil an attack by Murphy's men on the wagons bringing in provisions for the new store.\\r\\nFed up with Murphy's underhanded activities, British rancher Henry Tunstall rides off to Santa Fe to seek the intervention of Territorial Gov. Sam Axtell (Alan Baxter). On the way he is intercepted by Murphy's deputies, who falsely accuse him of cattle rustling and shoot him dead. Chisum and Garrett hunt down the deputies and bring them back to town for trial. Bonney, seeking revenge for the murder of his mentor and skeptical that the men will truly face justice in town, overpowers Garrett by surprise and shoots dead both deputies. Before corrupt Sheriff Brady (Bruce Cabot) can organise a posse, Billy rides into town and kills him, too.\\r\\nMurphy appoints bounty hunter Dan Nodeen (Christopher George) as the new sheriff, giving him orders to hunt down Bonney. Nodeen has a score to settle, as a previous encounter with Bonney has left him with a permanent limp.\\r\\nBilly's plans for revenge are only just beginning. He breaks into McSween's store looking for dynamite to rob Murphy's bank. He is spotted by Nodeen, who surrounds the store with Murphys's men. McSween's wife is allowed to leave. McSween later comes out unarmed but Nodeen shoots him in cold blood.\\r\\nChisum is alerted by McSween's wife (Lynda Day George) and rides into town with his ranch hands. The main street is blocked, so Chisum stampedes his cattle through the barricades. He tracks down Murphy and takes him on in a fist fight which ends with both men falling from a balcony. Murphy ends up impaled on steer horns. With his paymaster dead, Nodeen flees, with Billy in pursuit.\\r\\nThe film ends with Garrett taking over as sheriff and it is implied that is settling down with Sallie. It is learned that famous U.S. Army General Lew Wallace has become governor of the territory. With law and order restored, Chisum can resume his iconic vigil over the Pecos valley.\\r\\nMichael A. Wayne, executive producer, took on the project of making Chisum because he felt the story summed up well his father's political views. The sizeable cast is packed with familiar faces from earlier John Wayne films, as well as friends such as Forrest Tucker.\\r\\nIt was filmed in 1969 in Durango, Mexico.[4][5] The picturesque vistas of the area were captured by cinematographer William H. Clothier.\\r\\nThe film was originally made for 20th Century Fox, but they sold the film to Warner Bros.[6]\\r\\nJohn Wayne was on the set of Chisum when he heard of his nomination for an Academy Award in 1970 for True Grit.\\r\\nThe song \\"The Ballad of John Chisum\\" was narrated by William Conrad, the song \\"Turn Me Around\\" was sung by Merle Haggard.\\r\\nDuring filming, John Mitchum, brother of Robert, introduced John Wayne to his patriotic poetry. Seeing that Wayne was greatly moved by those words, Forrest Tucker suggested that the two collaborate to record some of the poetry, which resulted in a Grammy-nominated spoken-word album, America: Why I Love Her.\\r\\nChisum re-united several actors from Sands of Iwo Jima including John Wayne, John Agar, Forrest Tucker and Richard Jaeckel.\\r\\nReleased in June 1970, the film grossed $6 million at the box office.[3]\\r\\nU.S. President Richard Nixon commented on the film during a press conference in Denver, Colorado, on 3 August 1970. In doing so, he used the film as a context to explain his views on law and order:[7]\\r\\nOver the last weekend I saw a movieÿI don't see too many movies but I try to see them on weekends when I am at the Western White House or in Floridaÿand the movie that I selected, or, as a matter of fact, my daughter Tricia selected it, was \\"Chisum\\" with John Wayne. It was a western. And as I looked at that movie, I said, \\"Well, it was a very good western, John Wayne is a very fine actor and it was: a fine supporting cast. But it was just basically another western, far better than average movies, better than average westerns.\\"\\r\\nI wondered why it is that the western survives year after year after year. A good western will outdraw some of the other subjects. Perhaps one of the reasons, in addition to the excitement, the gun play, and the rest, which perhaps is part of it but they can get that in other kinds of movies but one of the reasons is, perhaps, and this may be a square observationÿis that the good guys come out ahead in the westerns; the bad guys lose.\\r\\nIn the end, as this movie particularly pointed out, even in the old West, the time before New Mexico was a State, there was a time when there was no law. But the law eventually came, and the law was important from the standpoint of not only prosecuting the guilty, but also seeing that those who were guilty had a proper trial.\\r\\nAndrew McLaglen called the film one of his favourites. \\"I wanted Billy the Kid to just be Billy the Kid, a human being, not a bad little boy. Fenady was sort of a scholar about the Lincoln County Cattle War, which was a conflict over water and cattletrading cattleand John Chisum actually became a very powerful landowner. It was an American story.\\"[8]\\r\\nWarner Home Video released Chisum for the first time on Blu-ray on June 7, 2016.","input":"Where was the john wayne movie chisum filmed?"},{"output":"March 18, 1980","context":"Michael William Krzyzewski (/?????fski/ shih-ZHEF-skee;[2] nicknamed \\"Coach K\\"; born February 13, 1947) is an American college basketball coach and former player. Since 1980, he has served as the head men's basketball coach at Duke University, where he has led the Blue Devils to five NCAA Championships, 12 Final Fours, 12 ACC regular season titles, and 14 ACC Tournament championships. Among men's college basketball coaches, only UCLA's John Wooden, with 10, has won more NCAA Championships. Krzyzewski was also the coach of the United States men's national basketball team, which he has led to three gold medals at the 2008 Summer Olympics, 2012 Summer Olympics, and 2016 Summer Olympics. He served as the head coach of the American team that won gold medals at the 2010 and the 2014 FIBA World Cup. He was also an assistant coach for the 1992 \\"Dream Team\\".\\r\\nKrzyzewski was a point guard at Army from 1966 to 1969 under coach Bob Knight. From 1975 to 1980, he was the head basketball coach for his alma mater.[3] He is a two-time inductee into the Naismith Memorial Basketball Hall of Fame, in 2001 for his individual coaching career and in 2010 as part of the collective induction of the \\"Dream Team\\".[4] He was inducted into the College Basketball Hall of Fame in 2006, and the United States Olympic Hall of Fame in 2009 (with the \\"Dream Team\\")[4]\\r\\nOn November 15, 2011, Krzyzewski led Duke to a 74ÿ69 victory over Michigan State at Madison Square Garden to become the coach with the most wins in NCAA Division I men's basketball history. Krzyzewski's 903rd victory set a new record, breaking that held by his former coach, Bob Knight. On January 25, 2015, Duke defeated St. John's, 77ÿ68, again at Madison Square Garden, as Krzyzewski became the first Division I men's basketball coach to reach 1,000 wins.[5]\\r\\n\\r\\n\\r\\nKrzyzewski was born in Chicago, Illinois, the son of Polish American, Catholic parents Emily M. (ne Pituch) and William Krzyzewski.[6][7]\\r\\nRaised as a Catholic, Krzyzewski attended St. Helen Catholic School in Ukrainian Village, Chicago and,[8] later, Archbishop Weber High School in Chicago, a Catholic prep school for boys.[9] He graduated from the United States Military Academy at West Point, New York, in 1969, and played basketball under Bob Knight while training to become an officer in the United States Army. He was captain of the Army basketball team in his senior season, 1968ÿ69, leading his team to the National Invitation Tournament (NIT) at Madison Square Garden in New York City, where West Point finished fourth in the tournament.\\r\\nFrom 1969 to 1974, Krzyzewski served in the United States Army and directed service teams for three years. In 2005, he was presented West Point's Distinguished Graduate Award.[10]\\r\\nHe was discharged from active duty in 1974 and started his coaching career as an assistant on Knight's staff with the Indiana Hoosiers during their historic 1974ÿ75 season. After one year with Indiana, Krzyzewski returned to West Point as head coach of the Army Cadets. He led the Cadets to a 73ÿ59 record and one NIT berth in five seasons.\\r\\nOn March 18, 1980, Krzyzewski was named the head coach at Duke University after five seasons at Army.[11] After a few rebuilding seasons, he and the Blue Devils became a fixture on the national basketball scene with 31 NCAA Tournament berths in the past 32 years and 22 consecutive from 1996 to 2017, which is the second-longest current streak of tournament appearances behind Kansas, which has appeared in the tournament in 27 consecutive seasons. Overall, he has taken his program to postseason play in 31 of his 34 years at Duke and is the most winning active coach in men's NCAA Tournament play with an 86ÿ25 record for a .767 winning percentage. His Duke teams have won 13 ACC Championships, been to 12 Final Fours, and won five NCAA tournament National Championships.\\r\\nOn February 13, 2010, Krzyzewski coached in his 1,000th game as the Duke head coach. On March 20, 2011, Krzyzewski won his 900th game, becoming the second of three Division I men's basketball coaches to reach 900 basketball wins, the other two being Jim Boeheim at Syracuse and his head coach at Army, Bob Knight.[12] On November 15, 2011, Krzyzewski got his 903rd win passing Knight's record for most Division I wins. In an interview of both men on ESPN the previous night, Krzyzewski discussed the leadership skills he learned from Knight and the United States Military Academy. Knight credited Krzyzewski's understanding of himself and his players as keys to his success over the years.[13]\\r\\nOn March 20, 2011, Krzyzewski won his 900th game with the Duke Blue Devils, making him the second head coach to win 900 games with one NCAA Division I men's basketball program.[14]\\r\\nOn January 25, 2015, Krzyzewski won his 1,000th game, when Duke defeated St. John's in Madison Square Garden. He is the first men's coach to win 1,000 NCAA Division I basketball games.\\r\\nOn April 6, 2015, Krzyzewski won his 5th NCAA championship, when Duke defeated Wisconsin in the title game.\\r\\nWinning against Yale in the 2016 NCAA tournament on March 19, Krzyzewski became the all-time winningest coach in the NCAA Division I tournament with 90 total wins.\\r\\nOn November 11, 2017, Krzyzewski won his 1,000th game with the Duke Blue Devils, making him the first head coach to win 1,000 games with one NCAA Division I men's basketball program.[n 1]\\r\\nOn March 17, 2018, Krzyzewski won his 1,099th game in his career, passing Pat Summitt for most wins by a Division I coach, male or female.[15]\\r\\nKrzyzewski has won three consecutive gold medals in the Olympics among several appearances as head coach of the USA men's national team. His other results include winning a silver medal at the 1987 World University Games, a bronze medal at the 1990 FIBA World Championship, a silver medal at the 1990 Goodwill Games, a bronze medal at the 2006 FIBA World Championship, and gold medals at the 2007 FIBA Americas Championship, the 2010 FIBA World Championship, and the 2014 FIBA World Cup.\\r\\nHe was also an assistant coach to the USA teams which won gold medals at the 1984 and 1992 Olympics as well as the 1979 Pan American Games Team and 1992 Tournament of the Americas.\\r\\nIn 2005, he was appointed coach of the national team through the Beijing Olympics. In the 2006 FIBA World Championship, the USA won the bronze medal after losing in the semifinals to Greece and then beating defending Olympic gold medalist Argentina for third place.\\r\\nOn August 24, 2008, Krzyzewski's U.S. team won the gold medal at the 2008 Beijing Olympic Games. \\"The Redeem Team\\" finished the tournament with a perfect 8ÿ0 record. He coached the U.S. team for the 2010 FIBA World Championship and led Team USA to a perfect 9ÿ0 record, defeating host Turkey in the gold medal game, 81ÿ64. His team won a second Olympic gold in London, defeating runners-up Spain, 107ÿ100. Krzyzewski has amassed a total record of 75ÿ1 (.987) as head coach of the USA National Team.[16]\\r\\nIn February 2013, Krzyzewski initially stepped down after seven years of coaching the national team,[17] but Team USA in May announced that he would return as head coach from 2013 through 2016.[18]\\r\\nDuring his long tenure at Duke, Krzyzewski has been given the opportunity to coach in the NBA at least five times. The first time came after the 1990 season when he led the Blue Devils to their third straight Final Four appearance. The Boston Celtics offered a coaching position to Krzyzewski, but he soon declined their offer. The next season, Krzyzewski proceeded to lead the Blue Devils to the first of two straight national championships. In 1994, he was pursued by the Portland Trail Blazers, but again he chose to stay with Duke. In 2004, Krzyzewski was also interviewed by the Los Angeles Lakers following the departure of high-profile coach Phil Jackson. He was given a formal offer from Lakers general manager Mitch Kupchak, reportedly for five years, $40 million and part ownership, but again turned down the NBA. In 2010, the New Jersey Nets were reportedly willing to pay Krzyzewski between $12 million and $15 million per season to coach the Nets. Krzyzewski again declined the offer and stayed at Duke.[19] In 2011, Krzyzewski was offered the vacant coaching position for the Minnesota Timberwolves, but he again declined the offer and chose to stay at Duke.\\r\\nKrzyzewski married his wife, Carol \\"Mickie\\" Marsh, in the Catholic chapel at West Point on the day of his graduation in 1969. They have three daughters and nine grandchildren.[26]\\r\\nKrzyzewski and his family founded the Emily Krzyzewski Center, a non-profit organization in Durham, which was established in 2006 and named in honor of Krzyzewski's mother. The mission is to inspire students from kindergarten to high school to dream big, act with character and purpose, and reach their potential as leaders in their community. The Center's K to College Model serves academically focused students in out-of-school programming designed to help them achieve in school, gain entry to college, and break the cycle of poverty in their families. Krzyzewski and his wife, Mickie, have also been active for years in fundraising and support for the Duke Children's Hospital, Children's Miracle Network, the V Foundation for Cancer Research.[9] In all of those entities they have both served as chairs and/or led major fundraising efforts. In addition, the Krzyzewskis have been major donors to Duke University in supporting a number of areas, including establishing scholarship endowments for students in North and South Carolina as well as a Duke student-athlete every year. He also serves on the board of advisors of the Code of Support Foundation, a nonprofit military services organization.[27]\\r\\nIn 2012, Krzyzewski received the U.S. Basketball Writers Association's Wayman Tisdale Humanitarian Award honoring his civic service and charitable efforts in making a significant positive impact on society.[28]\\r\\n??????National champion?? ??????Postseason invitational champion??\\r\\n??????Conference regular season champion?? ??????Conference regular season and conference tournament champion\\r\\n??????Division regular season champion ??????Division regular season and conference tournament champion\\r\\n??????Conference tournament champion\\r\\n[29]\\r\\nPound sign (#) denotes interim head coach.\\r\\n*Selection later vacated","input":"When did mike she schefsky start coaching at duke?"},{"output":"Western Bloc or Capitalist Bloc","context":"The Western Bloc or Capitalist Bloc during the Cold War refers to the countries allied with the NATO against the Soviet Union and its allies. The latter were referred to as the Eastern Bloc. The governments and press of the Western Bloc were more inclined to refer to themselves as the Free World or the Western world.[citation needed]\\r\\nDuring the Cold War the non-Communist countries in Europe were called \\"Western Europe\\", but within the framework of the modern times it is a purely geographical term.[citation needed]","input":"Who were the capitalists in the cold war?"},{"output":"parts of Greene, Washington, Allegheny, and Westmoreland counties","context":"Pennsylvania's 18th congressional district includes parts of Greene, Washington, Allegheny, and Westmoreland counties. The district is represented by Conor Lamb, who was elected March 13, 2018.[4]\\r\\nThe district is concentrated in the southern suburbs of Pittsburgh. It is predominantly white, although it contains a diverse range of suburbs. It is drawn in such a way that in some locations, neighborhoods and even streets are split between the 18th and the neighboring 12th and 14th districts. In parts of the eastern portion of the district, one side of the street is in the 12th while the other side is in the 18th. In the west, one side of the street is in the 14th while the other side is in the 18th.\\r\\nAlthough there are 35,000 more[5] Democrats in the district than Republicans, the district has trended increasingly Republican since the mid-1990s; most of the district's state legislators are Republicans. The district is home to many large coal mines and the energy industry is an important employer. The western part of the district contains some rural regions of Washington County, as well as the very wealthy suburbs in the northern part of that county, which tends to be more Republican than the part contained in the neighboring 9th District. The district also contains many of Allegheny County's southern suburbs of Pittsburgh, which range from traditionally wealthy areas such as Mount Lebanon and Upper St. Clair to middle-class communities such as Bethel Park and Scott Township and working-class labor towns such as Elizabeth.\\r\\nThe district skews older and has the second-oldest electorate in the state.[6]\\r\\nThe district winds along the eastern suburbs at the edge of Allegheny County, including most of the large suburban commercial center of Monroeville, and in western Westmoreland County. Central Westmoreland County, including the city of Greensburg, is also part of the district. It also contains the rural foothills of the county at the district's eastern end. Westmoreland County has become a major Republican stronghold.\\r\\nThe Supreme Court of Pennsylvania ruled that the district map violated the state constitution and redrew it in February 2018. The 18th and 14th districts will swap names and have their boundaries adjusted for the 2018 elections (after March's special election) and thereafter.[7][8]\\r\\nHistorically, the current district covers much of the area that was the center of the Whiskey Rebellion of the 1790s.\\r\\n\\r\\n\\r\\nCoordinates: 401014N 800139W? / ?40.17056N 80.02750W? / 40.17056; -80.02750","input":"Who is included in the 18th congressional district?"},{"output":"Britain and France declaring war on Germany after it  invaded Poland in September 1939","context":"Among the causes of World War II were Italian fascism in the 1920s, Japanese militarism and invasion of China in the 1930s, and especially the political takeover in 1933 of Germany by Adolf Hitler and his Nazi Party and its aggressive foreign policy. The immediate cause was Britain and France declaring war on Germany after it  invaded Poland in September 1939.\\r\\n\\r\\nProblems arose in Weimar Germany that experienced strong currents of revanchism after the Treaty of Versailles that concluded its defeat in World War I in 1918. Dissatisfactions of treaty provisions included the demilitarization of the Rhineland, the prohibition of unification with Austria (including the Sudetenland) and the loss of German-speaking territories such as Danzig and Eupen-Malmedy despite Wilson's Fourteen Points, the limitations on the Reichswehr making it a token military force, the war-guilt clause, and last but not least the heavy tribute that Germany had to pay in the form of war reparations, which became an unbearable burden after the Great Depression. The most serious internal cause in Germany was the instability of the political system, as large sectors of politically active Germans rejected the legitimacy of the Weimar Republic.\\r\\n\\r\\nAfter his rise and take-over of power in 1933 to a large part based on these grievances, Adolf Hitler and the Nazis heavily promoted them and also ideas of vastly ambitious additional demands based on Nazi ideology, such as uniting all Germans (and further all Germanic peoples) in Europe in a single nation; the acquisition of \\"living space\\" (Lebensraum) for primarily agrarian settlers (Blut und Boden), creating a \\"pull towards the East\\" (Drang nach Osten) where such territories were to be found and colonized; the elimination of Bolshevism; and the hegemony of an \\"Aryan\\"/\\"Nordic\\" so-called Master Race over the \\"sub-humans\\" (Untermenschen) of inferior races, chief among them Slavs and Jews.\\r\\n\\r\\nTensions created by those ideologies and the dissatisfactions of those powers with the interwar international order steadily increased. Italy laid claim on Ethiopia and conquered it in 1935, Japan created a puppet state in Manchuria in 1931 and expanded beyond in China from 1937, and Germany systematically flouted the Versailles treaty, reintroducing conscription in 1935 with the Stresa Front's failure after having secretly started re-armament, remilitarizing the Rhineland in 1936, annexing Austria in March 1938, and the Sudetenland in October 1938.\\r\\n\\r\\nAll those aggressive moves met only feeble and ineffectual policies of appeasement from the League of Nations and the Entente Cordiale, in retrospect symbolized by the \\"peace for our time\\" speech following the Munich Conference, that had allowed the annexation of the Sudeten from interwar Czechoslovakia. When the German Fhrer broke the promise he had made at that conference to respect that country's future territorial integrity in March 1939 by sending troops into Prague, its capital, breaking off Slovakia as a German client state, and absorbing the rest of it as the \\"Protectorate of Bohemia-Moravia\\", Britain and France tried to switch to a policy of deterrence.\\r\\n\\r\\nAs Nazi attentions turned towards resolving the \\"Polish Corridor Question\\" during the summer of 1939, Britain and France committed themselves to an alliance with Poland, threatening Germany with a two-front war. On their side, the Germans assured themselves of the support of the USSR by signing a non-aggression pact with them in August, secretly dividing Eastern Europe into Nazi and Soviet spheres of influence.\\r\\n\\r\\nThe stage was then set for the Danzig crisis to become the immediate trigger of the war in Europe started on 1 September 1939. Following the Fall of France in June 1940, the Vichy regime signed an armistice, which tempted the Empire of Japan to join the Axis powers and invade French Indochina to improve their military situation in their war with China. This provoked the then neutral United States to respond with an embargo. The Japanese leadership, whose goal was Japanese domination of the Asia-Pacific, thought they had no option but to pre-emptively strike at the US Pacific fleet, which they did by attacking Pearl Harbor on 7 December 1941.\\r\\n\\r\\nMeanwhile, the Axis in Europe had brought the Soviet Union into the war as an active belligerent by attacking eastwards in Operation Barbarossa (June 1941).\\r\\n\\r\\nThe internationalist-minded, radical Bolsheviks seized power in Russia in November 1917, with the goal of overthrowing capitalism across the world. They supported communist parties in many countries and helped set up similar regimes in Hungary and Bavaria, Azerbaijan, Armenia and Georgia. By 1920 there was a corridor of anti-communist border states just west of Russia. However, these states feuded among themselves, and such alliances they formed, like the Little Entente, were unstable.[1]\\r\\n\\r\\nItalian and German fascism were in part a reaction to international communist and socialist uprisings, in conjunction with nationalist fears of a Slavic empire. A further factor in Germany was the success of Freikorps (voluntary paramilitary groups of World War I veterans) in crushing the Bolshevik Bavarian Soviet Republic in Munich in 1919. Many of these veterans became early components of the Nazis' SA (\\"Stormtroopers\\"), which would be the party's troops in the street warfare with the communist paramilitary Roter Frontk?mpferbund in the decade before 1933. The street violence would help shift moderate opinion towards the need for Germany to find an anti-communist strongman to restore stability to German life.[2][3]\\r\\n\\r\\nExpansionism is the doctrine of expanding the territorial base (or economic influence) of a country, usually by means of military aggression. In Europe, Italy under Benito Mussolini sought to create a New Roman Empire based around the Mediterranean. It invaded Albania in early 1939, at the start of the war, and later invaded Greece. Italy had also invaded Ethiopia as early as 1935. This provoked angry words and an oil embargo from the League of Nations, which failed.\\r\\n\\r\\nUnder the Nazi regime, Germany began its own program of expansion, seeking to restore the \\"rightful\\" boundaries of historic Germany. As a prelude toward these goals the Rhineland was remilitarized in March 1936.[4]\\r\\n\\r\\nAlso, of importance was the idea of a Greater Germany, supporters hoped to unite the German people under one nation state, which included all territories where Germans lived, regardless of whether they happened to be a minority in a particular territory. After the Treaty of Versailles, a unification between Germany and a newly formed German-Austria, a successor rump state of Austria-Hungary, was prohibited by the Allies despite the majority of Austrian Germans supporting such a union.\\r\\n\\r\\nIn Asia, the Empire of Japan harbored expansionist desires towards Manchuria and the Republic of China.\\r\\n\\r\\nMilitarism is the principle or policy of maintaining a strong military capability to use it aggressively to expand national interests and/or values, with the view that military efficiency is the supreme ideal of a state.[5] A highly militaristic and aggressive national ideology prevailed in Germany, Japan and Italy.[6] This attitude fueled advancements in military technology, subversive propaganda, and ultimately territorial expansion as well. The leaders of countries that have been militarized often feel a need to prove that their armies are important and formidable, and this was often a contributing factor in the start of conflicts in the interwar period such as the Second Italo-Abyssinian War and the Second Sino-Japanese War.[7]\\r\\n\\r\\nDuring the period of the Weimar Republic (1919ÿ1933), the Kapp Putsch, an attempted coup d'tat against the republican government, was launched by disaffected members of the armed forces. After this event, some of the more radical militarists and nationalists were submerged in grief and despair into the NSDAP, while more moderate elements of militarism declined. The result was an influx of militarily-inclined men into the Nazi Party which, when combined with their racial theories, fueled irredentist sentiments and put Germany on a collision course for war with its immediate neighbors. \\r\\n\\r\\nTwo contemporaneous factors in Japan contributed both to the growing power of its military and chaos within its ranks leading up to the Second World War. One was the Cabinet Law, which required the Imperial Japanese Army (IJA) and Imperial Japanese Navy (IJN) to nominate servinet could be formed. This essentially gave the military veto power over the formation of any Cabinet in the ostensibly parliamentary country. Another factor was gekokuj, or institutionalized disobedience by junior officers. It was not uncommon for radical junior officers to press their goals, to the extent of assassinating their seniors. In 1936, this phenomenon resulted in the February 26 Incident, in which junior officers attempted a coup d'tat and killed leading members of the Japanese government. In the 1930s, the Great Depression wrecked Japan's economy and gave radical elements within the Japanese military the chance to force the entire military into working towards the conquest of all of Asia. For example, in 1931 the Kwantung Army (a Japanese military force stationed in Manchuria) staged the Mukden Incident, which sparked the Invasion of Manchuria and its transformation into the Japanese puppet state of Manchukuo.\\r\\n\\r\\nTwentieth-century events marked the culmination of a millennium-long process of intermingling between Germans and Slavs. Over the centuries, many Germans had settled in the east (examples being the Volga Germans invited to Russia by Catherine the Great, and the Ostsiedlung in medieval times). Such migratory patterns created enclaves and blurred ethnic frontiers. The rise of nationalism in the 19th century  made race a centerpiece of political loyalty. The rise of the nation-state had given way to the politics of identity, including Pan-Germanism and Pan-Slavism. Furthermore, Social-Darwinist theories framed the coexistence as a \\"Teuton vs. Slav\\" struggle for domination, land and limited resources.[8] Integrating these ideas into their own world-view, the Nazis believed that the Germans, the \\"Aryan race\\", were the master race and that the Slavs were inferior.[9]  In 1935, When the Nazis came to power, they set out Numenburgs Laws.  These laws were supposed to define the biological traits of Jewishness and anyone that fit the description would not be part of the supreme Aryan Race.[10]\\r\\n\\r\\nThe Treaty of Versailles was neither lenient enough to appease Germany, nor harsh enough to prevent it from becoming the dominant continental power again.[11] Germans largely saw the treaty place the blame, or \\"war guilt\\", on Germany and Austria-Hungary and punish them for their \\"responsibility\\" rather than working out an agreement that would assure long-term peace. The treaty provided for harsh monetary reparations, separated millions of ethnic Germans into neighboring countries, territorial dismemberment, and caused mass ethnic resettlement. In an effort to pay war reparations to Britain and France, the Weimar Republic printed trillions of marks, causing extremely high inflation of the German currency (see Hyperinflation in the Weimar Republic).\\r\\n\\r\\nThe treaty created bitter resentment towards the victors of World War I, who had promised the people of Germany that U.S. President Woodrow Wilson's Fourteen Points would be a guideline for peace; however, the US played a minor role in World War I and Wilson could not convince the Allies to agree to adopt his Fourteen Points. Many Germans felt that the German government had agreed to an armistice based on this understanding, while others felt that the German Revolution of 1918ÿ1919 had been orchestrated by the \\"November criminals\\" who later assumed office in the new Weimar Republic.\\r\\n\\r\\nThe German colonies were taken during the war, and Italy took the southern half of Tyrol after an armistice had been agreed upon. The war in the east ended with the defeat and collapse of Russian Empire, and German troops occupied large parts of Eastern and Central Europe (with varying degree of control), establishing various client states such as a kingdom of Poland and the United Baltic Duchy. After the destructive and indecisive battle of Jutland (1916) and the mutiny of its sailors in 1917, the Kaiserliche Marine spent most of the war in port, only to be turned over to the allies and scuttled at surrender by its own officers. The lack of an obvious military defeat was one of the pillars that held together the Dolchstosslegende (\\"Stab-in-the-back myth\\") and gave the Nazis another propaganda tool at their disposal.\\r\\n\\r\\nFrench security demands, such as reparations, coal payments, and a demilitarized Rhineland, took precedence at the Paris Peace Conference in 1919 and shaped the Treaty of Versailles by severely punishing Germany; however, Austria found the treaty to be unjust which encouraged Hitler's popularity.  Ginsberg argues, \\"France was greatly weakened and, in its weakness and fear of a resurgent Germany, sought to isolate and punish Germany....French revenge would come back to haunt France during the Nazi invasion and occupation twenty years later.\\"[12]\\r\\n\\r\\nAs World War I ended in 1918, France, along with the other victor countries, were in a desperate situation regarding their economies, security, and morale. The Paris Peace Conference of 1919 was their chance to punish Germany for starting the war. The war \\"must be someone's fault ÿ and that's a very natural human reaction\\" analyzed historian Margaret MacMillan.[13] Germany was charged with the sole responsibility of starting World War I. The War Guilt Clause was the first step towards a satisfying revenge for the victor countries, namely France, against Germany. France understood that its position in 1918 was \\"artificial and transitory\\".[14] Thus, Clemenceau, the French leader at the time, worked to gain French security via the Treaty of Versailles.[14]\\r\\n\\r\\nThe two main provisions of the French security agenda were reparations from Germany in the form of money and coal and a detached German Rhineland. The French government printed excess currency, which created inflation, to compensate for the lack of funds in addition to borrowing money from the United States. Reparations from Germany were necessary to stabilize the French economy.[15] France also demanded that Germany give France their coal supply from the Ruhr to compensate for the destruction of French coalmines during the war. Because France feared for its safety as a country, the French demanded an amount of coal that was a \\"technical impossibility\\" for the Germans to pay back.[16] France wanted the German Rhineland demilitarized because that would hinder a German attack. This gave France a physical security barrier between itself and Germany.[17] The inordinate amount of reparations, coal payments, and the principle of a demilitarized Rhineland were viewed by the Germans to be insulting and unreasonable.\\r\\n\\r\\n\\"No postwar German government believed it could accept such a burden on future generations and survive?...\\".[15] Paying reparations is a classic punishment of war but in this instance it was the \\"extreme immoderation\\" that caused German resentment. Germany made its last World War I reparation payment on 3 October 2010,[18] ninety-two years after the end of World War I. Germany also fell behind in their coal payments. They fell behind because of a passive resistance movement against the French.[19] In response, the French invaded the Ruhr, the region filled with German coal, and occupied it. At this point the majority of Germans were enraged with the French and placed the blame for their humiliation on the Weimar Republic. Adolf Hitler, a leader of the Nazi Party, attempted a coup d'tat against the republic to establish a Greater German Reich[20] known as the Beer Hall Putsch in 1923. Although this failed, Hitler gained recognition as a national hero amongst the German population. The demilitarized Rhineland and additional cutbacks on military infuriated the Germans. Although it is logical that France would want the Rhineland to be a neutral zone, the fact that France had the power to make that desire happen merely added onto the resentment of the Germans against the French. In addition, the Treaty of Versailles dissolved the German general staff and possession of navy ships, aircraft, poison gas, tanks, and heavy artillery was made illegal.[17] The humiliation of being bossed around by the victor countries, especially France, and being stripped of their prized military made the Germans resent the Weimar Republic and idolize anyone who stood up to it.[21]\\r\\n\\r\\nOther than a few coal and iron deposits, and a small oil field on Sakhalin Island, Japan lacked strategic mineral resources.  At the start of the 20th century in the Russo-Japanese War, Japan had succeeded in pushing back the East Asian expansion of the Russian Empire in competition for Korea and Manchuria.\\r\\n\\r\\nJapan's goal after 1931 was economic dominance of most of East Asia, often expressed in Pan-Asian terms of \\"Asia for the Asians.\\".[22]  Japan was determined to dominate the China market, which the U.S. and other European powers had been dominating. On October 19, 1939, the American Ambassador to Japan, Joseph C. Grew, in a formal address to the America-Japan Society stated:\\r\\n\\r\\nthe new order in East Asia has appeared to include, among other things, depriving Americans of their long established rights in China, and to this the American people are opposed?... American rights and interests in China are being impaired or destroyed by the policies and actions of the Japanese authorities in China.[23]\\r\\n\\r\\nIn 1937 Japan invaded Manchuria and China proper. Under the guise of the Greater East Asia Co-Prosperity Sphere, with slogans as \\"Asia for the Asians!\\" Japan sought to remove the Western powers' influence in China and replace it with Japanese domination.[24][25]\\r\\n\\r\\nThe ongoing conflict in China led to a deepening conflict with the U.S., where public opinion was alarmed by events such as the Nanking Massacre and growing Japanese power. Lengthy talks were held between the U.S. and Japan. When Japan moved into the southern part of French Indochina, President Roosevelt chose to freeze all Japanese assets in the U.S. The intended consequence of this was the halt of oil shipments from the U.S. to Japan, which had supplied 80 percent of Japanese oil imports. The Netherlands and Britain  followed suit. With oil reserves that would last only a year and a half during peace time (much less during wartime), this ABCD line left Japan two choices: comply with the U.S.-led demand to pull out of China, or seize the oilfields in the East Indies from the Netherlands. The Japan government deemed it unacceptable to retreat from China.[26]\\r\\n\\r\\nThe League of Nations was an international organization founded after World War I to prevent future wars.  It failed.[27] The League's methods included disarmament; preventing war through collective security; settling disputes between countries through negotiation diplomacy; and improving global welfare. The diplomatic philosophy behind the League represented a fundamental shift in thought from the preceding century. The old philosophy of \\"concert of nations\\", growing out of the Congress of Vienna (1815), saw Europe as a shifting map of alliances among nation-states, creating a balance of power maintained by strong armies and secret agreements. Under the new philosophy, the League was a government of governments, with the role of settling disputes between individual nations in an open and legalist forum. The United States never joined, which lessened the power and credibility of the Leaguethe addition of a burgeoning industrial and military world power might have added more force behind the League's demands and requests.\\r\\n\\r\\nThe League lacked an armed force of its own and so depended on the members to enforce its resolutions, uphold economic sanctions that the League ordered, or provide an army when needed for the League to use. However, they were often very reluctant to do so. After numerous notable successes and some early failures in the 1920s, the League ultimately proved incapable of preventing aggression by the Axis powers in the 1930s. The reliance upon unanimous decisions, the lack of an armed force, and the continued self-interest of its leading members meant that this failure was arguably inevitable.[28][29]\\r\\n\\r\\nIn the late 1980s the British historian Richard Overy was involved in a historical dispute with Timothy Mason that mostly played out over the pages of the Past and Present journal over the reasons for the outbreak of World War II in 1939. Mason had contended that a \\"flight into war\\" had been imposed on Adolf Hitler by a structural economic crisis, which confronted Hitler with the choice of making difficult economic decisions or aggression. Overy argued against Mason's thesis, maintaining that though Germany was faced with economic problems in 1939, the extent of these problems cannot explain aggression against Poland and the reasons for the outbreak of war were due to the choices made by the Nazi leadership.\\r\\n\\r\\nMason had argued that the German working-class was always  to the Nazi dictatorship; that in the over-heated German economy of the late 1930s, German workers could force employers to grant higher wages by leaving for another firm that would grant the desired wage increases; that this was a form of political resistance and this resistance forced Adolf Hitler to go to war in 1939.[30] Thus, the outbreak of the Second World War was caused by structural economic problems, a \\"flight into war\\" imposed by a domestic crisis.[30] The key aspects of the crisis were according to Mason, a shaky economic recovery was threatened by a rearmament program that was overwhelming the economy and in which the Nazi regime's nationalist bluster limited its options.[30] In this way, Mason articulated a Primat der Innenpolitik (\\"primacy of domestic politics\\")  view of World War II's origins through the concept of social imperialism.[31] Mason's Primat der Innenpolitik thesis was in marked contrast to the Primat der Au?enpolitik (\\"primacy of foreign politics) usually used to explain World War II.[30] In Mason's opinion, German foreign policy was driven by domestic political considerations, and the launch of World War II in 1939 was best understood as a \\"barbaric variant of social imperialism\\".[32]\\r\\n\\r\\nMason argued that \\"Nazi Germany was always bent at some time upon a major war of expansion.\\"[33] However, Mason argued that the timing of such a war was determined by domestic political pressures, especially as relating to a failing economy, and had nothing to do with what Hitler wanted.[33] In Mason's view in the period between 1936ÿ41, it was the state of the German economy, and not Hitler's 'will' or 'intentions' that was the most important determinate on German decision-making on foreign policy.[34] Mason argued that the Nazi leaders were deeply haunted by the November Revolution of 1918, and was most unwilling to see any fall in working class living standards out of the fear that it might provoke another November Revolution.[34] According to Mason, by 1939, the \\"overheating\\" of the German economy caused by rearmament, the failure of various rearmament plans produced by the shortages of skilled workers, industrial unrest caused by the breakdown of German social policies, and the sharp drop in living standards for the German working class forced Hitler into going to war at a time and place not of his choosing.[35] Mason contended that when faced with the deep socio-economic crisis the Nazi leadership had decided to embark upon a ruthless 'smash and grab' foreign policy of seizing territory in Eastern Europe which could be pitilessly plundered to support living standards in Germany.[36] Mason described German foreign policy as driven by an opportunistic 'next victim' syndrome after the Anschluss, in which the \\"promiscuity of aggressive intentions\\" was nurtured by every successful foreign policy move.[37] In Mason's opinion, the decision to sign the German-Soviet Non-Aggression Pact with the Soviet Union and to attack Poland and the running of the risk of a war with Britain and France were the abandonment by Hitler of his foreign policy program outlined in Mein Kampf forced on him by his need to stop a collapsing German economy by seizing territory abroad to be plundered.[35]\\r\\n\\r\\nFor Overy, the problem with Mason's thesis was that it rested on the assumption that in a way not shown by records, information was passed on to Hitler about the Reich's economic problems.[38] Overy argued that there was a difference between economic pressures induced by the problems of the Four Year Plan and economic motives to seize raw materials, industry and foreign reserves of neighboring states as a way of accelerating the Four Year Plan.[39] Overy asserted that the repressive capacity of the German state as a way of dealing with domestic unhappiness was somewhat downplayed by Mason.[38] Finally, Overy argued that there is considerable evidence that the German state felt they could master the economic problems of rearmament; as one civil servant put it in January 1940 \\"we have already mastered so many difficulties in the past, that here too, if one or other raw material became extremely scarce, ways and means will always yet be found to get out of a fix\\".[40]\\r\\n\\r\\nHitler and his Nazis took full control of Germany in 1933ÿ34 (Machtergreifung), turning it into a dictatorship with a highly hostile outlook toward the Treaty of Versailles and Jews.[41] It solved its unemployment crisis by heavy military spending.[42]\\r\\n\\r\\nHitler's diplomatic tactics were to make seemingly reasonable demands, then threatening war if they were not met; concessions were made, he accepted them and moved onto a new demand.[43] When opponents tried to appease him, he accepted the gains that were offered, then went to the next target. That aggressive strategy worked as Germany pulled out of the League of Nations (1933), rejected the Versailles Treaty and began to re-arm (1935) with the Anglo-German Naval Agreement, won back the Saar (1935), re-militarized the Rhineland (1936), formed an alliance (\\"axis\\") with Mussolini's Italy (1936), sent massive military aid to Franco in the Spanish Civil War (1936ÿ39), seized Austria (1938), took over Czechoslovakia after the British and French appeasement of the Munich Agreement of 1938, formed a peace pact with Stalin's Russia in August 1939, and finally invaded Poland in September 1939.[44]\\r\\n\\r\\nIn violation of the Treaty of Versailles and the spirit of the Locarno Pact and the Stresa Front, Germany re-militarized the Rhineland on March 7, 1936. It moved German troops into the part of western Germany where, according to the Versailles Treaty, they were not allowed. France could not act because of political instability at the time. According to his official Biography, King Edward VIII, who thought the Versailles provision was unjust,[45] ordered the government to stand down.[46]\\r\\n\\r\\nAfter the Stresa Conference and even as a reaction to the Anglo-German Naval Agreement, Italian dictator Benito Mussolini attempted to expand the Italian Empire in Africa by invading the Ethiopian Empire (also known as Abyssinia). The League of Nations declared Italy the aggressor and imposed sanctions on oil sales that proved ineffective. Italy annexed Ethiopia in May 7 and merged Ethiopia, Eritrea, and Somaliland into a single colony known as Italian East Africa.  On June 30, 1936, Emperor Haile Selassie gave a stirring speech before the League of Nations denouncing Italy's actions and criticizing the world community for standing by. He warned that \\"It is us today. It will be you tomorrow\\". As a result of the League's condemnation of Italy, Mussolini declared the country's withdrawal from the organization.[47]\\r\\n\\r\\nBetween 1936 and 1939, Germany and Italy lent support to the Nationalists led by general Francisco Franco in Spain, while the Soviet Union supported the existing democratically elected government, the Spanish Republic, led by Manuel Aza?a. Both sides experimented with new weapons and tactics.  The League of Nations was never involved, and the major powers of the League remained neutral and tried (with little success) to stop arms shipments into Spain. The Nationalists eventually defeated the Republicans in 1939.[48]\\r\\n\\r\\nSpain negotiated with joining the Axis but remained neutral during World War II, and did business with both sides. It also sent a volunteer unit to help the Germans against the USSR. Whilst it was considered in the 1940s and 1950s to be a prelude to World War II and It prefigured the war to some extent (as it changed it into an antifascists contest after 1941), it bore no resemblance to the war that started in 1939 and had no major role in causing it.[49][50]\\r\\n\\r\\nIn 1931 Japan took advantage of China's weakness in the Warlord Era and fabricated the Mukden Incident in 1931 to set up the puppet state of Manchukuo in Manchuria, with Puyi, who had been the last emperor of China, as its emperor. In 1937 the Marco Polo Bridge Incident triggered the Second Sino-Japanese War.\\r\\n\\r\\nThe invasion was launched by the bombing of many cities such as Shanghai, Nanjing and Guangzhou. The latest, which began on 22 and 23 September 1937, called forth widespread protests culminating in a resolution by the Far Eastern Advisory Committee of the League of Nations.  The Imperial Japanese Army captured the Chinese capital city of Nanjing, and committed war crimes in the Nanjing massacre.  The war tied down large numbers of Chinese soldiers, so Japan set up three different Chinese puppet states to enlist some Chinese support.[51]\\r\\n\\r\\nThe Anschluss was the 1938 annexation by threat of force of Austria into Germany. Historically, the Pan-Germanism idea of creating a Greater Germany to include all ethnic Germans into one nation-state was popular for Germans in both Austria and Germany.\\r\\n\\r\\nOne of the Nazi party's points was \\"We demand the unification of all Germans in the Greater Germany on the basis of the people's right to self-determination.\\"\\r\\n\\r\\nThe Stresa Front of 1935 between Britain, France and Italy had guaranteed the independence of Austria, but after the creation of the Rome-Berlin Axis Mussolini was much less interested in upholding its independence.\\r\\n\\r\\nThe Austrian government resisted as long as possible, but had no outside support and finally gave in to Hitler's fiery demands. No fighting occurred as most Austrians were enthusiastic, and Austria was fully absorbed as part of Germany. Outside powers did nothing. Italy had little reason for continued opposition to Germany, and was if anything drawn in closer to the Nazis.[52][53]\\r\\n\\r\\nThe Sudetenland was a predominantly German region inside Czechoslovakia alongside its border with Germany. Its more than 3 million ethnic Germans comprised almost a quarter of the population of Czechoslovakia. In the Treaty of Versailles it was given to the new Czechoslovak state against the wishes of much of the local population. The decision to disregard their right to self determination was based on French intent to weaken Germany. Much of Sudetenland was industrialized.[54]\\r\\n\\r\\nCzechoslovakia had a modern army of 38 divisions, backed by a well-noted armament industry (?koda) as well as military alliances with France and Soviet Union. However its defensive strategy against Germany was based on the mountains of the Sudetenland.\\r\\n\\r\\nHitler pressed for the Sudetenland's incorporation into the Reich, supporting German separatist groups within the Sudeten region. Alleged Czech brutality and persecution under Prague helped to stir up nationalist tendencies, as did the Nazi press. After the Anschluss, all German parties (except German Social-Democratic party) merged with the Sudeten German Party (SdP). Paramilitary activity and extremist violence peaked during this period and the Czechoslovakian government declared martial law in parts of the Sudetenland to maintain order. This only complicated the situation, especially now that Slovakian nationalism was rising, out of suspicion towards Prague and Nazi encouragement. Citing the need to protect the Germans in Czechoslovakia, Germany requested the immediate annexation of the Sudetenland.\\r\\n\\r\\nIn the Munich Agreement of September 30, 1938, British, French and Italian prime ministers appeased Hitler by giving him what he wanted, hoping he would not want any more. The conferring powers allowed Germany to move troops into the region and incorporate it into the Reich \\"for the sake of peace.\\" In exchange for this, Hitler gave his word that Germany would make no further territorial claims in Europe.[55] Czechoslovakia was not allowed to participate in the conference. When the French and British negotiators informed the Czechoslovak representatives about the agreement, and that if Czechoslovakia would not accept it, France and Britain would consider Czechoslovakia to be responsible for war, President Edvard Bene? capitulated. Germany took the Sudetenland unopposed.[56]\\r\\n\\r\\nIn March 1939, breaking the Munich Agreement, German troops invaded Prague, and with the Slovaks declaring independence, the country of Czechoslovakia disappeared. The entire ordeal was the last show of the French and British policy of appeasement.\\r\\n\\r\\nAfter the German occupation of Czechoslovakia, Benito Mussolini feared for Italy becoming a second-rate member of the Axis. Rome delivered Tirana an ultimatum on March 25, 1939, demanding that it accede to Italy's occupation of Albania. King Zog refused to accept money in exchange for countenancing a full Italian takeover and colonization of Albania. On April 7, 1939, Italian troops invaded Albania. Albania was occupied after a 3 days campaign with minimal resistance offered by the Albanian forces.\\r\\n\\r\\nIn 1939, the Japanese attacked west from Manchuria into the Mongolian People's Republic, following the earlier Battle of Lake Khasan in 1938. They were decisively beaten by Soviet units under General Georgy Zhukov. Following this battle, the Soviet Union and Japan were at peace until 1945. Japan looked south to expand its empire, leading to conflict with the United States over the Philippines and control of shipping lanes to the Dutch East Indies. The Soviet Union focused on her western border, but leaving 1 million to 1.5 million troops to guard the frontier with Japan.\\r\\n\\r\\nAfter the final fate of Czechoslovakia proved that the Fhrer's word could not be trusted, Britain and France decided on a change of strategy. They decided any further unilateral German expansion would be met by force. The natural next target for the Third Reich's further expansion was Poland, whose access to the Baltic sea had been carved out of West Prussia by the Versailles treaty, making East Prussia an exclave. The main port of the area, Danzig, had been made a free city-state under Polish influence guaranteed by the League of Nations, a stark reminder to German nationalists of the Napoleonic free city established after the French emperor's crushing victory over Prussia in 1807.\\r\\n\\r\\nAfter taking power, the Nazi government made efforts to establish friendly relations with Poland, resulting in the signing of the ten-year GermanÿPolish Non-Aggression Pact with the Pi?sudski regime in 1934. In 1938, Poland participated in the dismemberment of Czechoslovakia by annexing Zaolzie. In 1939, Hitler claimed extra-territoriality for the Reichsautobahn Berlin-K?nigsberg and a change in Danzig's status, in exchange for promises of territory in Poland's neighbours and a 25-year extension of the non-aggression pact. Poland refused, fearing losing de facto access to the sea, subjugation as a German satellite state or client state, and future further German demands.[57][58] In August 1939, Hitler delivered an ultimatum to Poland on Danzig's status.\\r\\n\\r\\nIn March 1939, Britain and France guaranteed the independence of Poland. Hitler's claims in the summer of 1939 on Danzig and the Polish provoked yet another international crisis. On August 25, Britain signed the Polish-British Common Defence Pact.\\r\\n\\r\\nNominally, the MolotovÿRibbentrop Pact was a non-aggression treaty between Germany and the Soviet Union. It was signed in Moscow on August 23, 1939, by the Soviet foreign minister Vyacheslav Molotov and the German foreign minister Joachim von Ribbentrop.\\r\\n\\r\\nIn 1939, neither Germany nor the Soviet Union were ready to go to war with each other. The Soviet Union had lost territory to Poland in 1920. Although officially labeled a \\"non-aggression treaty\\", the pact included a secret protocol, in which the independent countries of Finland, Estonia, Latvia, Lithuania, Poland and Romania were divided into spheres of interest of the parties. The secret protocol explicitly assumed \\"territorial and political rearrangements\\" in the areas of these countries.\\r\\n\\r\\nSubsequently, all the mentioned countries were invaded, occupied, or forced to cede part of their territory by either the Soviet Union, Germany, or both.\\r\\n\\r\\nBetween 1919 and 1939 Poland pursued a policy of balancing between the Soviet Union and Nazi Germany, seeking non-aggression treaties with both.[59] In early 1939 Germany demanded that Poland join the Anti-Comintern Pact as a satellite state of Germany.[60] Poland, fearing a loss of independence, refused, and Hitler told his generals on 23 May 1939 that the reason for invading Poland was not Danzig: \\"Danzig is not the issue at stake. It's a matter of extending our living space in the East...\\"[61] To deter Hitler, Britain and France announced that an invasion would mean war and tried to convince the Soviet Union to join in this deterrence. The USSR however gained control of the Baltic states and parts of Poland by allying with Germany, which it did through the secret MolotovÿRibbentrop Pact in August 1939. London's attempt at deterrence failed, but Hitler did not expect a wider war. Germany invaded Poland on September 1, 1939, and rejected the British and French demands that it withdraw, resulting in their declaration of war on September 3, 1939, in accordance with the defense treaties with Poland that they had signed and publicly announced.[62][63]\\r\\n\\r\\nGermany attacked the Soviet Union in June 1941. Hitler believed that the Soviet Union could be defeated in a fast-paced and relentless assault that capitalized on the Soviet Union's ill-prepared state, and hoped that success there would bring Britain to the negotiation table, ending the war altogether.\\r\\n\\r\\nUsually, the US government and the American public in general had been supportive of China, condemning the colonialist policies of the European powers and Japan in that country, and promoting a so-called Open Door Policy. Also, many Americans viewed the Japanese as an aggressive or inferior race, or both. The Nationalist Government of Chiang Kai-shek held close relations with the United States, which opposed Japan's invasion of China in 1937 that it considered an illegal violation of the sovereignty of the Republic of China, and offered the Nationalist Government diplomatic, economic, and military assistance during its war against Japan. Diplomatic friction between the US and Japan manifested itself in events like the Panay incident in 1937 and the Allison incident in 1938.\\r\\n\\r\\nReacting to Japanese pressure on French authorities of French Indochina to stop trade with China, the U.S. began restricting trade with Japan in July 1940. The cutoff of all oil shipments in 1941 was decisive, for the U.S., Britain and the Netherlands provided almost all of Japan's oil.[64]\\r\\nIn September 1940, the Japanese invaded Vichy French Indochina and occupied Tonkin in order to prevent China from importing arms and fuel through French Indochina along the Sino-Vietnamese Railway, from the port of Haiphong through Hanoi to Kunming in Yunnan.[65]  This tightening of the blockade of China made a continuation of the drawn-out Battle of South Guangxi unnecessary. The agreement also allowed Japan to station troops in the rest of Indochina, though this did not happen immediately.\\r\\n\\r\\nTaking advantage of the situation, Thailand launched the Franco-Thai War in October 1940. In November 1940, American military aviator Claire Lee Chennault upon observing the dire situation in the air war between China and Japan, set out to organize a volunteer squadron of American fighter pilots to fight alongside the Chinese against Japan, known as the Flying Tigers.[66] US President Franklin D. Roosevelt accepted dispatching them to China in early 1941.[66] However, they only became operational shortly after the attack on Pearl Harbor.\\r\\n\\r\\nJapan stepped in as a mediator for the French-Thai war in May 1941, allowing its ally to occupy bordering provinces in Cambodia and Laos. In July 1941, as operation Barbarossa had neutralized the Soviet threat, the faction of the Japanese military junta supporting the \\"Southern Strategy\\", pushed through the occupation of the rest of French Indochina.\\r\\n\\r\\nThe United States reacted by seeking to bring the Japanese war effort to a complete halt by imposing a full embargo on all trade between the United States to Japan on 1?August 1941, demanding that Japan withdraw all troops from both China and Indochina. Japan was dependent on the United States for 80 percent of its oil, resulting in an economic and military crisis for Japan that could not continue its war effort with China without access to petroleum and oil products.[67]\\r\\n\\r\\nOn 7 December 1941, without any prior declaration of war,[68] the Imperial Japanese Navy attacked Pearl Harbor with the aim of destroying the main American battle fleet at anchor. At the same time, other Japanese forces attacked the U.S.-held Philippines and the British Empire in Malaya, Singapore, and Hong Kong. These attacks led both the USA and the United Kingdom to declare war upon Japan the next day.\\r\\n\\r\\nFour days later the U.S was brought into the European war when on December 11, 1941, Nazi Germany and Fascist Italy declared war on the United States. Hitler chose to declare that the Tripartite Pact required that Germany follow Japan's declaration of war; although American destroyers escorting convoys and German U-boats were already de facto at war in the Battle of the Atlantic. This declaration effectively ended isolationist sentiment in the U.S. and the United States immediately reciprocated, formally entering the war in Europe.[69]","input":"What was the main cause of the second world war?"},{"output":"baritone","context":"Frank Sinatra's musical career began in the swing era in 1935, and ended in 1995. \\r\\n\\r\\nSinatra's vocal style represented a strong departure from the \\"crooning\\" style of his idol, Bing Crosby. Sinatra's generation represented the first generation of children that had grown up in the era of the microphone, and the amplification of sound enabled singers to sing in a much softer, personal and nuanced style. However Sinatra, as he himself once noted, sang more, by which he meant that he introduced a bel canto sound to the tradition begun by Crosby. And, more importantly, he might be said to have brought the Crosby tradition to artistic completion, taking it to levels of intensity and depth of feeling that, because of the displacement of the Crosby ÿ Sinatra tradition by rock and roll and subsequent genres, are unlikely to be achieved again.\\r\\nTwo other great performers of the 1930s and 1940s were significant influences on Sinatra: Billie Holiday and Mabel Mercer. Sinatra regularly heard \\"Lady Day\\" in New York clubs in the 1940s and learned from her the importance of authenticity of emotion. From Mercer he learned the importance of the element of \\"story\\" in a song. For Sinatra a song is a three- to four-minute narrative  sometimes even the story of himself, his own life, his own heartaches, his own feelings of buoyancy  and this is why Ella Fitzgerald could say of him, \\"With Frank, it's always this little guy, telling this ... story.\\" The archetypal examples of the Sinatra song as story could later be found in two selections from his 1958 Capitol album, Frank Sinatra Sings For Only The Lonely: \\"Angel Eyes\\" and \\"One For My Baby (And One More For The Road)\\".\\r\\nSinatra made a point of studying Tommy Dorsey's trombone playing as a means of cultivating a more free-flowing vocal style  he noticed that Dorsey used a tiny airhole at the side of his mouth to sneak breaths when playing. Sinatra would employ a similar technique, and so be able to hold notes for incredibly long durations. In addition to this, Sinatra started to jog and swim underwater to develop his lung capacity  which enabled him to continue a musical phrase through a stanza without pausing, or breaking the note, for breath. Sinatra's legato-style of singing/phrasing took pop singing in new directions when most singers of the 1940s were keen to emulate Bing Crosby.\\r\\nAs happens with many singers, Sinatra suffered at least one period of major vocal difficulty, which he remedied with the help of Metropolitan Opera baritone Robert Merrill.\\r\\nAs a song-stylist, Sinatra's jazz-infused approach to singing seemed to occur with the end of the \\"Big Band\\" era and ushering in of an era that favored the vocalist and made him/her the focus, not the bandleader and his band.\\r\\nSinatra also possessed an outstanding vocal range. According to music critic Henry Pleasants \\"The voice itself was a typical Italian light baritone with a two octave range from G to G, declining, as it darkened in later years, to F to F and with greater potential at the top than he was commonly disposed to exploit. He could and sometimes did depress the larynx and 'cover' as classical singers do, to sustain a full rounded tone in moving up the scale. On his recording 'Day by Day,' for example he gives out with full-voiced, admirably focused D's and E's and even lands a briefly held but confident high G just before the end.\\" [1] His early recordings found him singing in near-tenor range, hitting a high F on \\"All or Nothing At All\\" (1939) or \\"Where's My Bess\\" (1946), whilst being equally adept in the lower register, the low E on his 1962 recording of \\"Ol' Man River\\" being a prime example of such. His phrasing was also impeccable, getting to the heart of a song by emphasizing words and lines in ways that made a song more personal, whilst his ability to hold notes, sing above or behind the beat and rest on a note were hallmarks of a singer fully in command of his instrument.\\r\\nBetween 1946 and 1983 Sinatra conducted seven albums and occasionally conducted live orchestras on stage. His first recordings on which he wielded the baton were instigated by producer Mitch Miller, who approached Columbia boss Maine Sachs to request that Sinatra conduct some of the work of Alec Wilder, later released as Frank Sinatra Conducts The Music Of Alec Wilder. In 1956 Sinatra recorded the first album in the Capitol Records tower, not as a vocalist, but as a conductor on the album Frank Sinatra Conducts Tone Poems of Color. In 1957 and 1959 he conducted albums for Peggy Lee  The Man I Love  and Dean Martin  Sleep Warm  the latter, charting inside Billboard's Top 40. A lesser-known project for his own label, Reprise, entitled Frank Sinatra Conducts Music from Pictures and Plays remains relatively obscure, and it was 20 years before Sinatra conducted in a studio again, for Sylvia Syms on the album Syms by Sinatra, which featured the final arrangements of Don Costa. The following year Sinatra conducted for trumpeter Charles Turner on the album What's New?.\\r\\nSinatra would certainly have been considered a 'pop' singer before the \\"Rock and Roll\\" era, and the epithets Traditional Pop or more specifically Classic Pop have perhaps been coined to describe Sinatra's style. In addition, Sinatra would and did tackle several styles and genres of music throughout his career, with differing degrees of success.\\r\\nThere still exists a debate as to whether Sinatra was a jazz singer. He certainly performed with many of the finest jazz musicians and, in fact, headlined the Newport Jazz Festival and toured with the Red Norvo Quintet. There are very few occasions when Sinatra was recorded scat singing, but minor nuances and slight deviations from the vocal line are a hallmark of the material he recorded, and he was also known for his impeccable jazz timing and phrasing. Indeed, it is impossible to imagine the Sinatra after 1953 without the influence of jazz. It is no accident that he would be Lester Young's ideal singer in the band Young had hoped to lead, nor that Miles Davis identified Sinatra's phrasing as an influence on his own. The list of Sinatra's jazz admirers is long and stellar, including such figures as Count Basie, Stan Getz, Oscar Peterson[2] and Jaco Pastorius[3]\\r\\nThe Philadelphia radio DJ Sid Mark has hosted a Frank Sinatra show (Friday with Frank, Saturday with Sinatra, Sunday with Sinatra) since 1957.\\r\\nSinatra left a vast legacy of recordings, from his very first sides with the Harry James orchestra in 1939, the vast catalogs at Columbia in the 1940s, Capitol in the 1950s, and Reprise from the 1960s onwards, up to his 1994 album Duets II.\\r\\nSome of his best known recorded songs include:\\r\\nThree of his songs made #1 on the Billboard Hot 100 after the advent of the rock and roll era: \\"Learnin' the Blues\\" (1955), \\"Strangers in the Night\\" (1966), and \\"Somethin' Stupid\\" (1967), the last a duet with daughter Nancy.\\r\\nOf all his many albums, Sinatra at the Sands, recorded live in Las Vegas in 1966, with Sinatra in his prime, backed by Count Basie's big band, remains his most popular and is still a big seller. Whether in nightclubs, casinos, arenas, or stadiums, Sinatra was one of the most mesmeric entertainers of the twentieth century, capable of turning the largest venue into a simulacrum of an intimate club. There are, however, few recordings or videos of his concerts. In addition to the Sands performance with Basie, three performances of Sinatra at the very peak of his career were captured: With Red Norvo Quintet: Live In Australia, 1959, Sinatra '57 in Concert, a performance in Seattle with an orchestra conducted by Nelson Riddle and Sinatra & Sextet: Live in Paris, recorded in June 1962.\\r\\nSinatra is also credited with putting out perhaps the first concept album. 1955's In the Wee Small Hours is the prime example: a set of songs specifically recorded for the album, using only ballads, organized around a central mood of late-night isolation and aching lost love (supposedly due to his separation from Ava Gardner), with a now-classic album cover reflecting the theme. Rolling Stone magazine later named In the Wee Small Hours as #100[4] on its list of the 500 best albums of all time, an impressive showing given that its rankings reflect the outlook of the rock generation.\\r\\nThe following year's Songs For Swingin' Lovers took an alternate tack, recording existing pop standards in a hipper, jazzier fashion, revealing an overall exuberance; Rolling Stone placed it #306[5] on the above list.\\r\\nIt was the advent of the long-playing record that opened the door to these famous concept albums of the 1950s, but Sinatra's first efforts in this direction go back to the Columbia years and The Voice, when the 78 rpm disc made \\"album\\" less of a metaphor than it would become with the single-disc LPs of the 1950s. The Voice of Frank Sinatra was released on March 10, 1946  it was re-issued as a 10\\" record in 1958. Four more albums would follow over the next five years, as would a Christmas album and a project in which Sinatra conducted the songs of Alec Wilder.\\r\\nOther notable Sinatra albums include Where Are You? from 1957, which was his first stereo album and his first album recorded with Gordon Jenkins, Frank Sinatra Sings for Only the Lonely (1958), a bleak, introspective album, which Sinatra later claimed was his finest work.\\r\\nThe lavish The Concert Sinatra (1963) offered re-recordings of \\"Ol' Man River\\" and \\"You'll Never Walk Alone\\", backed by a 73-piece orchestra. 1965's September of My Years, according to critic Stephen Holden \\"summed up the punchy sentimentality of a whole generation of American men\\". Francis Albert Sinatra & Antonio Carlos Jobim (1967) was a late foray into bossa nova, with Antonio Carlos Jobim.\\r\\n1973's comeback album Ol' Blue Eyes Is Back was Sinatra's first album after being away from recording for three years; 1980's Trilogy: Past Present Future was an ambitious triple album using three arrangers that attempted to portray the past, present, and future of his career.\\r\\n1981's She Shot Me Down is sometimes considered the last great Sinatra album. A collection of what Sinatra called \\"saloon songs\\", it includes Alec Wilder's \\"A Long Night\\".\\r\\nSpeaking to Robin Douglas-Home in 1961, Sinatra said, with regards to the making of his many concept albums, \\"First I pick the mood for an album, and perhaps pick a title. Or perhaps it might be that I had that title and then picked the mood to fit it... Then I get a shortlist of maybe sixty possible songs and out of these I pick twelve to record. Next comes the pacing of the album, which is vitally important... Once we choose songs that will be in a particular album, I'll sit with Bill Miller, my pianist, and find the proper key. Then I will meet with the orchestrator... Usually we wind up doing it the way the arranger feels it should be done, because he understands more than I do about it...\\"\\r\\nFrank Sinatra holds the unique distinction of singing on the first Billboard #1 single, \\"I'll Never Smile Again\\" (1940)  which sold 900,000 copies  and had the first ever #1 album in the UK, Songs For Swingin' Lovers (July 28, 1956). This same album is also the only album to chart among the UK top twenty singles, peaking at #12 on June 15, 1956. In 1959, the album Come Dance with Me! also entered the UK singles chart peaking at #30 the same week it would start a 30-week run on the album chart, going as high as #2.\\r\\nFrom his first released single in 1940  as the singer with Tommy Dorsey's band  to the 1980 release of \\"Theme from New York, New York\\", Frank Sinatra had 209 hits on Billboard's pop singles charts. Of those, 127 made the Top Twenty, 70 made the Top Ten and 10 reached the #1 position  \\"I'll Never Smile Again\\" (1940), \\"There Are Such Things\\" (1942), \\"In the Blue of the Evening\\" (1943), \\"All or Nothing at All\\" (1944), \\"Oh What It Seemed To Be\\" (1945), \\"Five Minutes More\\" (1946), \\"Mam'selle\\" (1947), \\"Learning the Blues\\" (1955), \\"Strangers in the Night\\" (1966) and \\"Somethin' Stupid\\" (1967).\\r\\nOf Sinatra's 56 Top Twenty albums on Billboard's pop album charts, 42including soundtracks  reached the Top Ten and 6 made the #1 position  The Voice of Frank Sinatra (1946), In the Wee Small Hours  which spent 18 weeks at #2Come Fly With Me (1956), Frank Sinatra Sings for Only the Lonely (1958), Nice 'n' Easy (1960) and Strangers in the Night (1966). He also has the longest time span of charting Top Ten albums on the Billboard album chart, 62 years with The Voice of Frank Sinatra going to #1 in 1946, and Nothing But the Best going to #2 in 2008.\\r\\nSinatra's 1958 album Frank Sinatra Sings for Only the Lonely spent 120 weeks on Billboard's album chart, peaking at #1. His next album, Come Dance with Me! (1959) spent 140 weeks on Billboard, peaking at #2.\\r\\n\\"My Way\\" (1969) is the longest charting U.K. single of all time, with 122 weeks spent on the chart, peaking at number 5. The single re-entered the chart 8 times between 1970 and 1972. A 1995 re-release spent 2 weeks on the chart.\\r\\nIn the UK, 42 Sinatra albums have made the Top Ten. Fifty-four Sinatra albums have made the Top Twenty, the longest charting of those albums being the 1997 compilation My Way: The Very Best of Frank Sinatra, which, to date, has charted for 128 weeks achieving 5 x platinum status. Six of Sinatra's albums reached the #1 position on the UK album chart, with a further five peaking at #2.","input":"What kind of voice did frank sinatra have?"},{"output":"sets money aside for specific spending","context":"An appropriation bill (also known as a running bill or supply bill) is proposed law that authorizes the expenditure of government funds. It is a bill that sets money aside for specific spending.[1] In most democracies, approval of the legislature is necessary for the government to spend money.\\r\\nIn a Westminster parliamentary system, the defeat of an appropriation bill in a parliamentary vote generally necessitates either the resignation of a government or the calling of a general election. One of the more famous examples of the defeat of a supply bill was the 1975 Australian constitutional crisis, when the Senate, which was controlled by the opposition, refused to approve a package of appropriation and loan bills, prompting Governor-General Sir John Kerr to dismiss Prime Minister Gough Whitlam and appoint Malcolm Fraser as caretaker Prime Minister until the next election (where the Fraser government was elected).[2]\\r\\n\\r\\n\\r\\nIn New Zealand, an Appropriation Bill is the formal name for the annual act of Parliament which gives legal effect to the Budget, that is, the Government's taxing and spending policies for the forthcoming year (from 1 July to 30 June). Like other bills, it is enacted, following debate, by the House of Representatives, and assented to by the Governor-General. The main Appropriation Bill is traditionally placed before the House for its first reading in May amid considerable media interest, an event known as the introduction of the Budget. An Appropriation Bill is not sent to a select committee, a lengthy process undergone by most bills during which they are scrutinised in detail by the committee, which also receives public submissions relating to the bill. Instead, an expedited process is followed in which the Appropriation Bill essentially goes directly to its second reading for consideration by the committee of the whole House. Royal assent is granted after the formality of a third reading.\\r\\nThe main Appropriation Bill is formally called an \\"Appropriation (Estimates) Bill\\", or, after assented to, an \\"Appropriation (Estimates) Act\\". Supplementary Budgetary legislation in New Zealand includes an annual \\"Appropriation (Confirmation and Validation) Bill\\", which serves to validate taxation and spending incurred in the previous year which fell outside the previous year's Budget, and \\"Imprest Supply Bills,\\" typically several in a year, which grant interim authority to the Government to tax and spend.\\r\\nBoth Appropriation and Imprest Supply bills fall under the rubric of confidence and supply. A refusal by the House to pass such a Bill conventionally leads to either the resignation of the Government (unlikely, since there is usually no alternative Government immediately available) or to a dissolution of the House and a subsequent general election.\\r\\nUnder the U.S. presidential system, the support of the Congress for the President's appropriations requests is not necessary for the separately elected President to remain in office, but can severely limit the President's ability to govern effectively.\\r\\nIn the United States, there are two types of appropriations. When Congress sets up particular programs, the legislation may itself set up the necessary appropriation mechanism, such as the social security program where payment of benefits are \\"mandatory\\". A mandatory program does not need an additional authorisation in order for spending under the program to occur. An authorization bill can create programs and make known Congress's intended level of spending for programs that also require an appropriation.[3] What distinguishes a mandatory program from a discretionary program is that after Congress enacts a law creating a mandatory program, the program is permitted to spend funds until the program expires based on a provision in law, or until a subsequent law either terminates the program or reauthorizes it. \\"Discretionary\\" programs typically require annual appropriations legislation.\\r\\nAn appropriation bill is used to actually provide money for \\"discretionary\\" programs. Appropriations are generally done on an annual basis, although multi-year appropriations are occasionally passed. According to the United States Constitution (Article I, Section 8, clause 12), Army appropriations cannot be for more than two years at a time. An annual appropriation requires that the funds appropriated be obligated (spent) by the end of the fiscal year of the appropriation. Once the fiscal year ends, no more money can be spent via the prior year's appropriation. A new appropriation for the new fiscal year must be passed in order for continued spending to occur, or passage of a special appropriations bill known as a continuing resolution, which generally permits continued spending for a short period of timeusually at prior year levels. The Anti-Deficiency Act makes void any attempt to spend money for which there is no current appropriation.[4]\\r\\nAccording to the Origination Clause of the United States Constitution, all bills for raising revenue, generally tax bills, must originate in the House of Representatives, similar to the Westminster system requirement that all money bills originate in the lower house. Traditionally, though, appropriation bills also originate in the House of Representatives.[5][6] House appropriations bills begin with \\"H.R.\\", meaning \\"House of Representatives\\". In reference to revenue bills, the Constitution also states that the \\"Senate may propose or concur with Amendments as on other Bills.\\" As with revenue bills, the Senate and House each drafts and considers its own appropriation bill. The Senate then \\"cuts-and-pastes,\\" substituting the language of its version of a particular appropriation bill for the language of the House bill, then agrees to the bill as amended.\\r\\nThe United States House Committee on Appropriations and the United States Senate Committee on Appropriations have jurisdiction over appropriations bills.[7] Both committees have twelve matching subcommittees tasked with working on one of the twelve annual regular appropriations bills. Other Committees and lawmakers in Congress write legislation creating programs and reauthorizing old ones to continue. This legislation is called an authorization bill. In this legislation, they authorize these programs to exist, and they authorize the expenditure of funds on them, but they cannot actually give them the money. That second step, of granting the money, is done in an appropriations bill. The appropriations committees have power because they can decide whether to fund these programs at the maximum level authorized, a lesser amount, or not at all.[8]\\r\\nAppropriations bills in the United States can also come in the format of an omnibus spending bill, a continuing resolution, or a supplemental appropriation bill. If Congress has not enacted the regular appropriations bills by the start of a new fiscal year, it can pass a continuing resolution, which continues the pre-existing appropriations at the same levels as the previous fiscal year (or with minor modifications) for a set amount of time.[7] An omnibus spending bill is simply a combination of multiple appropriations bills into one larger appropriations bill. Supplemental appropriations bills increase funding for activities that were already funded in previous appropriations bills or the provide new funding for unexpected expenses.[9] For example, both the War in Afghanistan and the Iraq War were funded with a variety of supplemental appropriations.[10][11] Supplemental appropriations bills also provide funding for recovering from unexpected natural disasters like Hurricane Sandy (the Disaster Relief Appropriations Act, 2013).\\r\\nAnnual appropriations are divided into 12 separate pieces of legislation:","input":"What action is accomplished by an appropriations bill?"},{"output":"David Michael Draiman","context":"David Michael Draiman (born March 13, 1973) is an American songwriter and the vocalist for the band Disturbed as well as for the band Device. Draiman is known for his distorted voice and percussive singing style. In November 2006, Draiman was voted number 42 on the Hit Paraders \\"Top 100 Metal Vocalists of All Time\\".[1] Draiman has written some of Disturbed's most successful singles, such as \\"Stupify\\", \\"Down with the Sickness\\", \\"Indestructible\\", and \\"Inside the Fire\\".\\r\\nIn October 2011, Disturbed entered a hiatus.[2] Draiman announced in the following year that he was working on an industrial rock/metal project with Geno Lenardo, formerly of Filter, which was later named Device.[3][4] In June 2015, Disturbed released their first single since their hiatus, named \\"The Vengeful One\\". They produced it over a year before, and along with it announced a new album, Immortalized.\\r\\n\\r\\n\\r\\nDraiman was born in Brooklyn, New York on March 13, 1973, the son to Miriam and YJ Draiman. His father, a candidate in the 2017 race for mayor of Los Angeles,[5] is a former real estate developer, small-business owner, and elected member of the Northridge East Neighborhood Council, among other roles.[6] Draiman's brother Benjamin[7] is an ambient/folk rock musician who lives in Israel and performs in Jerusalem.[8][9] Draiman's grandmother also lives in Israel.[8][10][11]\\r\\nHis parents were observant, religious Jews (dati). They intended for Draiman to receive semikhah, and Draiman frequently spent time in Israel during his early life.[8] Draiman attended five Jewish day schools, including Wisconsin Institute for Torah Study in Milwaukee, Wisconsin; Valley Torah High School in Los Angeles, California, where he formed his first band; and Fasman Yeshiva High School in Chicago, Illinois.[8] During his freshman year at Wisconsin Institute for Torah Study he was asked to leave, as he \\"rebelled against the conformity\\" and \\"just wanted to be a normal teenage kid\\", adding that he \\"couldn't really stomach the rigorous religious requirements of the life [there]\\".[8] Of his study at Jewish day schools, Draiman stated that he \\"was a bit resentful\\"; but he later encouraged his family to observe Shabbat, and was trained as a hazzan.[8]\\r\\nDraiman later enrolled at Ida Crown Jewish Academy in Chicago, Illinois, where he graduated from high school in 1991.[8] From there, in 1991ÿ1992, he spent a year after high school studying at the Yeshivas Neveh Zion in Kiryat Ye'arim, on the outskirts of Jerusalem, Israel.[8]\\r\\nAfter returning to the US in 1992, Draiman commenced pre-law studies at Loyola University Chicago.[8] In 1996, he graduated from the University with a Bachelor of Arts in Political Science and Government, Philosophy, and Business Administration.[12][13] Initially considering offers to matriculate and study at law school, Draiman realized that although criminal defense law was the only area of law that interested him, he could not \\"really look at myself in the mirror and say 'I'm going to lie for a living and protect criminals'\\".[8] During his university studies, Draiman also worked as a bank teller and in phone sales.[8]\\r\\nAfter graduating from college, Draiman worked as an administrative assistant in a healthcare facility.[8] After his first year, he earned an administrator's license and commenced running his own healthcare facility.[8] For five years before joining Disturbed and the band's signing with Giant Records, Draiman was a healthcare administrator.[8]\\r\\nDraiman said, \\"the first record I ever bought was Kiss Destroyer. And those classic bands like Black Sabbath were my first loves...I focused on the seminal metal bands like Metallica, Iron Maiden, Pantera and Queensr?che\\".[8]\\r\\nDraiman continues, \\"But I could also appreciate the hair metal bands ÿ When you hear Whitesnake, you cant deny their greatness. Then I went in the direction of punk and new wave, groups like the Sex Pistols, The Ramones, The Misfits and later The Smiths and The Cure ÿ that was my '80s\\".[8]\\r\\n\\"And then when the grunge revolution happened, it was like a wakeup call. Ill never forget getting my first Nirvana, Soundgarden and Alice in Chains records\\".[8]\\r\\nDraiman has cited James Hetfield of Metallica, Rob Halford of Judas Priest, and Bruce Dickinson of Iron Maiden as the three biggest influences on his singing.[citation needed]\\r\\nDraiman is married to former WWE Diva Lena Yada; they have a son, Samuel Bear Isamu Draiman,[14] born in 2013.[15] In politics he said \\"I'm liberal about everything that is issue-based as far as ideology, but I'm also of the opinion of a very small government. I don't agree with the fiscal policies of the Democrats, but I certainly don't agree with the right-wing craziness of the Republicans.\\"[16]\\r\\nFor a more comprehensive list, see Disturbed discography Disturbed\\r\\nDevice\\r\\nGuest appearances\\r\\nAs Producer\\r\\nMetal Hammer Golden Gods Awards\\r\\nLoudwire Music Awards","input":"Who is the lead singer of the group disturbed?"},{"output":"opened on July 22, 1997","context":"Cars: $3.00 USD or $4.00 CAD (to Canada) and $3.50 USD or $4.00 CAD (to US) as of April 22, 2016. Extra Axles: $4.00 CAD or $3.50 USD.\\r\\nThe Blue Water Bridge is a twin-span international bridge across the St. Clair River that links Port Huron, Michigan, United States, and Point Edward, Ontario, Canada. The Blue Water Bridge connects Highway 402 in Ontario with both Interstate 69 (I-69) and Interstate 94 (I-94) in Michigan.\\r\\nThe original span is a cantilever truss bridge with a total length of 6,178?feet?(1,883?m) and a main span of 871?feet?(265?m). The second, newer span is a continuous tied-arch bridge with a total length of 6,109?feet?(1,862?m) and a main span of 922?feet?(281?m).\\r\\nTogether, the two bridges are the second-busiest commercial crossing between the United States and Canada, after the Ambassador Bridge at Detroit-Windsor. They are the fourth-busiest overall international crossing in Ontario in terms of total number of vehicles.[2] Overall, in Canada, they are the third-busiest bridges after the Champlain Bridge, in Montreal, and the Ambassador Bridge. They also provide one of the four shortest routes of land travel between the Northeastern United States (particularly New England) and the central United States. The Blue Water Bridges are jointly owned and maintained by Canada and the United States: Federal Bridge Corporation, a Crown corporation of the Government of Canada, is in charge of the Canadian side,[3] and the Michigan Department of Transportation (MDOT) is in charge of the U.S. side.[4] A toll is charged to cross the bridges, which is used to pay for maintenance and operations.\\r\\n\\r\\n\\r\\nThe first bridge was fully opened to traffic on October 10, 1938.[5] The lead engineer was Polish-born Ralph Modjeski. This bridge originally had two lanes for vehicles as well as sidewalks; the latter were removed in the 1980s to make room for a third lane for automobiles. The third lane for each direction started from the apex of the bridge in order to accommodate long lineups entering each sides' respective border crossings.\\r\\nIn 1928, Maynard D. Smith hired a Pennsylvania-based company named Modjeski and Masters to build what would become the Blue Water Bridge. Ralph Modjeski, a Polish-born engineer who would become known as \\"America's greatest bridge builder\\", served as lead engineer for the project. In developing a design, Modjeski faced obstacles posed by the U.S. Army Corps of Engineers, which sought to ensure that the St. Clair River remain navigable for military and commercial vessels. Bridge construction could not interfere with navigation, bridge builders could not use floating platforms, and the completed span was required to clear the water by 150?ft. Originally, Modjeski proposed a mammoth suspension-style bridge with tall towers and massive cable anchorages; however, because of the 150-ft. vertical clearance requirement for shipping, he opted instead for a cantilevered through-truss design.[citation needed]\\r\\nIn 1935, the Michigan Legislature passed a law (Act 147, Public Acts of 1935) creating a State Bridge Commission to finance the design and erection of the main bridge structure of the Blue Water Bridge. The commission was approved by the United States Congress in August 1935 (Public Law 411 of 1935). The law permitted the commission to sell bonds ($.25 toll for travelers) that would be repaid by the revenue from the tolls collected within 30 years.\\r\\nWhen the bridge and its associated bonds were paid off, Michigan Governor John Swainson used an executive order to cancel the $.25 toll that had been collected. \\"Stoically\\", he effectively cancelled his own father's \\"$6,115-a-year toll-collector's job\\", which John A. C. Swainson held since 1957.[6]\\r\\nIn 1964, the eastern terminus of I-94 was completed at the foot of the Blue Water Bridge on the American side.[7] Traffic volumes steadily increased, spurred by the completion of Highway 402 in 1982 which provided a continuous freeway link to Highway 401 on the Canadian side. In 1984, I-69 was completed to Port Huron which meant that three freeways converged on the three-lane bridge.[7]\\r\\nAs a precursor to the upcoming twinning project, the customs and toll collection booths on both sides were extensively reconfigured in the early 1990s. On the American side, the I-beam girder overpass crossing Pine Grove Avenue was replaced by a much wider embankment, which also added a four-story customs office building in the center. On the Canadian side this necessitated the demolition of the original booths that had been in use since 1938; these were noted for their Art Deco style but they were too low to accommodate semi-trailer trucks which had been directed to the outside.\\r\\nIn 1992, it was determined that traffic on the bridge had exceeded its rated capacity,[8][9] so bridge authorities decided to add a second span in order to accommodate the higher traffic. During the debate over the form of the second span, five possible designs were proposed from 1994ÿ1995. Over half of public opinion had mostly favored a duplicate of the first bridge, while the cable-stayed bridge came in second with around 21%. The Blue Water Bridge Authority had rejected both designs, due to the duplicate creating a false sense of history, while the cable-stayed option was feared to overshadow the existing bridge. Another cost-effective but unpopular design was the parallel truss. The continuous-tied arch design, which was a distant third place in polls, was chosen for two reasons. One was that it blends in with the original span yet stands out on its own, and the other is lower maintenance costs because fewer spans are involved.\\r\\nThe twinning project was a combined effort between Modjeski & Masters (American engineers) and Buckland & Taylor Ltd. (Canadian engineers). During the construction, two temporary masts were erected to assist in the construction of the tied arch; the towers were painted red and lighted, enabling them to be seen from afar.[10] The approaches to the new bridge use box girders, compared to the original which hold up the road deck with trusses.[11]\\r\\nThe second three-lane bridge, just south of the first bridge, opened on July 22, 1997. The first bridge was immediately closed for extensive renovation. During this period, the new span used a three-lane configuration reminiscent of the one employed on the original bridge. A flyover ramp on the U.S. side temporarily diverted westbound traffic from the new bridge to the toll plaza, which was blocked off after the original bridge was rehabilitated.[12] The original span was reopened on November 13, 1999, making the Blue Water Bridge the largest infrastructure-crossing project in North America.[13]\\r\\nIn 2007, in accordance with Federal Identity Program requirements, the Blue Water Bridge Authority agreed on a name for the federal Crown corporate organization: Blue Water Bridge Canada.[14]\\r\\nIn March 2009, the Canadian government announced that CA$13.5 million (US$10.8 million) in funding would be allocated toward upgrading the border crossing facilities at the Blue Water Bridge. The work was scheduled to begin in May 2009.[15]\\r\\nConstruction was also underway in 2011 to widen and improve both Highway 402 on the Canadian side[16] and co-signed Interstate 94/69 on the American side approaching the Blue Water Bridge.[17] The projects, completed the following year, added dedicated lanes separating Blue Water Bridge traffic from local traffic.","input":"When was the second blue water bridge built?"},{"output":"about 480 million years ago","context":"","input":"When did the first insects appear on earth?"},{"output":"April 2, 2009","context":"\\"And in the End...\\" is the 331st and final episode of the American television series ER. The two-hour episode aired on April 2, 2009 and was preceded by a one-hour retrospective special.\\r\\n\\r\\n\\r\\nDr. Tony Gates (John Stamos) treats a teenage girl in a coma with alcohol poisoning after she played drinking games with her friends. Gates calls the police when he discovers that the parents of the girl's friend supplied the alcohol, and has the friend's father arrested. Later, the girl's parents arrive and request she be transferred to Mercy Hospital. Before she can be transferred, the girl finally awakens but just thrashes around and is sent for a new CT to diagnose possible brain damage.\\r\\nDr. Julia Wise (Alexis Bledel), new to County General, treats a homosexual HIV-positive patient who has severe breathing difficulties. It is discovered he has terminal cancer. With the support of his partner, he decides not to seek treatment, as he has already outlived most of his friends who died at the height of the AIDS epidemic in the 1980s.\\r\\nAn elderly patient named Beverly (Jeanette Miller) is brought in by a fire engine with a broken wrist, and comments on Dr. Archie Morris' (Scott Grimes) \\"soft and strong\\" hands. She is later claimed by her daughter, as she had wandered off before her accident. She returns later, having wandered off again but otherwise not further injured.\\r\\nA married couple comes in with the woman going into labor with twins, and John Carter (Noah Wyle) and Simon Brenner (David Lyons) handle the delivery. During the delivery of the second baby, complications set in. It is discovered that the mother has an inverted uterus, and requires an emergency caesarean section. The second baby requires intensive care, and the mother ultimately dies as surgeons attempt to fix the complications.\\r\\nMark Greene's daughter Rachel (Hallee Hirsh) is visiting the hospital as a prospective medical student, and is interviewing for a spot in the teaching program. She is interviewed primarily by Catherine Banfield (Angela Bassett), who confides to Carter afterward that she made it through the first cut. Carter shows her around after her interview, giving her a few pointers she will need when she is accepted into medical school.\\r\\nCarter opens his clinic for the underprivileged, with Peter Benton (Eriq La Salle) and his son Reese (Matthew Watkins), Kerry Weaver (Laura Innes) and Susan Lewis (Sherry Stringfield) among the guests. He named the facility after his late son, Joshua. After the event, he has an awkward reunion with Joshua's mother Kem (Thandie Newton), who is briefly in town. He also has drinks with his old friends plus Elizabeth Corday (Alex Kingston), who brought Rachel to Chicago for her med school interview. Benton and Corday linger together after everyone else go their separate ways.\\r\\nMarjorie Manning (Beverly Polcyn), a previous elderly multiple sclerosis patient suffering sepsis and pulmonary edema, comes in with her husband (Ernest Borgnine). Mr. Manning is initially unwilling to let Marjorie go, but with guidance from Dr. Gates, he finally accepts the inevitable. Before Marjorie dies, her daughter arrives, and confides in Samantha Taggart (Linda Cardellini) her regrets over her relationship with her mother. This leads Samantha to call her own mother.\\r\\nIt turns out it is Samantha's birthday. Her son Alex (Dominic Janes) reveals her present: a vintage Ford Mustang restored by Alex and Tony, who both decided upon bright red as its color over cobalt blue.\\r\\nA young bride and her new mother-in-law (Marilu Henner) both come in, in separate ambulances, with minor injuries sustained in a drunken brawl at their wedding reception, and continue arguing all the way into the treatment rooms. The groom later arrives, and is promptly torn between tending to his mother and his new wife.\\r\\nThe episode ends with the beginning of a disaster protocol: an industrial explosion, with a minimum of eight casualties. Dr. Carter is again pressed into service to assist. Dr. Morris is ordered by Dr. Banfield to triage patients as they arrive. The first patient was thrown 20 feet, and is diagnosed as a possible lacerated spleen or liver, to be sent straight to the OR. The second patient has a compound leg fracture with no circulatory impairment, which Dr. Banfield takes herself for an orthopedic consult. The third patient was electrocuted, and fell into asystole on the way in, declared DOA. The fourth patient has smoke inhalation, relatively minor burns and a pneumothorax, and is set up for a chest tube. The fifth patient had his left arm blown off below the elbow, with nothing left to save; Dr. Gates takes him in to repair the damage. Dr. Morris gives the sixth patient to Dr. Carter: third-degree burns over 90% coverage. As he runs the patient in, Dr. Carter asks Rachel to tag along saying \\"Dr. Greene, you comin'?\\", which she does enthusiastically. As Morris continues to triage patients, the original theme music plays and the point of view pulls back, revealing the entire hospital for the first and only time.\\r\\nThe episode was structured \\"very much like the show's pilot. Like that first episode, it took place over the course of 24 hours, featured a dizzying number of cases  some comical, and some that were dead serious  and showed the life of the ER through eyes both inexperienced ... and jaded.\\"[1] It featured several direct references to the pilot and other early episodes, including: Dr. Archie Morris is woken by veteran nurse Lydia Wright, who had done the same to Dr. Mark Greene in the pilot (the shot is used in several early episodes); Dr. John Carter demonstrating how to start an IV to Rachel Greene exactly as Dr. Peter Benton showed him in the pilot; a comedic scene dealing with a child who had swallowed a rosary, as in the pilot, when a child swallowed a key; and the day marked by cutscenes showing the time at the start of each act as was done in the series' fifth episode, \\"Into That Good Night\\", with both episodes starting and ending at 4:00?AM. A portion of the storyline was inspired by the death of producer John Wells' 17-year-old niece, who died of alcohol poisoning in December 2008.[2]\\r\\nThe episode featured full-length opening credits from the first season (albeit with shots and credits added for both the final regular cast, and five of the past stars that appeared) with music by James Newton Howard, the show's original composer.[1] That theme music also played as the camera pulled out and faded at the end of the last scene of the episode, which showed the entire exterior of County General Hospital for the only time in the history of the series, except in high tone and fully in 1080i HD.\\r\\nDoctors\\r\\nNurses\\r\\nStaff\\r\\nParamedics\\r\\nIn his review in the then-St. Petersburg Times, Eric Deggans called the final episode \\"a welcome reminder of why the show has lasted 15 years on network television  and proof of why it's now time for the show to go.\\" He continued, \\"Thanks to cameo appearances by most key actors whose characters remain alive... the episode had the feel of a friendly class reunion... Once upon a time, the cases that filled Thursday's episode would have felt groundbreaking and fresh... But on Thursday, these contrivances felt like shadows echoing other, better episodes long past... Wells echoed the series most consistent theme by showing old and new doctors uniting to handle yet another emergency at the finale's end. But Thursday's episode also proved that NBC's ER called it quits at just the right time, because TV series  unlike medical institutions  should never go on forever.\\"[3]\\r\\nKen Tucker of Entertainment Weekly said he \\"liked the fact that the surprises were small but effective,\\" while Alan Sepinwall of the Star-Ledger stated, \\"It wasn't an all-time great finale, but it did what it set out to do.\\" In the Baltimore Sun, David Zurawik said, \\"I especially liked the final scene with the ER team suiting up and standing ready to respond in the courtyard to the arriving fleet of emergency vehicles loaded with victims of an industrial explosion. Perhaps, it was one last tweak at the critics who said the pilot looked more like a feature film and would never make it as a TV series  because it was definitely a movie ending.\\"[4] The episode received an A- from The A.V. Club.[5]\\r\\nIn 2010, Time magazine ranked the episode as #10 on its list of the most anticipated TV finales.[6]\\r\\nAccording to Nielsen ratings, the finale gave NBC \\"its best ratings on the night in a long time.\\" The first hour received a 9.6 rating and a 15 share, numbers which improved to an 11.1 rating and 19 share in the final hour.[7] Overall the finale attracted an average of 16.2 million viewers.[8][9]\\r\\nThe series finale of ER scored the highest 18-49 rating for a drama series finale since The X-Files wrapped with a 6.3 on May 19, 2002. In total viewers, ER assembled the biggest overall audience for a drama series finale since Murder, She Wrote concluded with 16.5 million on May 19, 1996.\\r\\nIn Canada, the finale fared even better on a percentage basis with 2.768 million viewers. With ten times less the population, this equates to roughly 27.68 million American viewers.[10]\\r\\nTimeslot 18ÿ49/share 18ÿ34/share viewers (millions)\\r\\n8PM[7]\\r\\n9PM[7]\\r\\n10PM[7]\\r\\nAt the 2009 Primetime Emmy Awards Rod Holcomb won the Emmy for Outstanding Directing for a Drama Series for his work on this episode, while Ernest Borgnine was nominated for the Emmy for Outstanding Guest Actor in a Drama Serires.[11]","input":"When did the last episode of er air?"},{"output":"A Game of Thrones","context":"Jaime Lannister is a fictional character in the A Song of Ice and Fire series of fantasy novels by American author George R. R. Martin, and its television adaptation Game of Thrones. He becomes a prominent point of view character in the novels beginning in A Storm of Swords (2000).\\r\\nIntroduced in A Game of Thrones (1996), Jaime is a knight of the Kingsguard and a member of House Lannister, one of the wealthiest and most powerful families in the fictional kingdom of Westeros. Although he first appears to be unscrupulous and amoral,[clarification needed] he later proves to be far more complex, honorable and sympathetic.\\r\\nJaime is portrayed by Nikolaj Coster-Waldau on the HBO series Game of Thrones. He was nominated for a Saturn Award for Best Supporting Actor on Television, a Critics' Choice Television Award for Best Supporting Actor in a Drama Series and a People Choice Awards Favorite TV Anti-Hero for his performance in the show's third season. He and the rest of the cast were nominated for Screen Actors Guild Awards for Outstanding Performance by an Ensemble in a Drama Series in 2012, 2014, 2015, 2016 and 2017.\\r\\n\\r\\n\\r\\nJaime was born shortly after the birth of his twin sister Cersei. The two were inseparable as children, and at an early age began an incestuous affair that continued into adulthood and even after Cersei's marriage to Robert Baratheon. At the age of fifteen he participated in the battle against the Kingswood Brotherhood, squiring for Lord Sumner Crakehall, and was subsequently knighted by Ser Arthur Dayne. While returning to Casterly Rock, he visited Cersei at King's Landing, who persuaded him to join the Kingsguard in order to be close to her. Jaime was accepted into the Kingsguard, but Tywin resigned as Hand of the King, taking Cersei with him back to Casterly Rock. Jaime quickly realises that Aerys named him to the Kingsguard solely to strip Tywin of his heir. Jaime becomes further disillusioned after witnessing Aerys burn Rickard and Brandon Stark alive and overhearing him raping his sister-wife Rhaella. When Tywin begins sacking King's Landing at the climax of Robert's Rebellion, Jaime realises that Aerys intends to burn King's Landing with wildfire and kills him, almost immediately being discovered by Tywin's men. Robert Baratheon later pardons Jaime for breaking his vows and names him to his own Kingsguard. Although Jaime considers his assassination of Aerys as his greatest act, his motives for killing him remain unknown to the rest of the realm and he is given the insulting moniker \\"Kingslayer\\".\\r\\nSeveral years later, he and Tyrion rescue a young woman, Tysha, from outlaws, and Tyrion secretly marries her. When Tywin finds out he orders Jaime to tell Tyrion that he staged the incident in order for Tyrion to lose his virginity. Though it is unknown if Jaime was aware that Tywin subsequently forced Tyrion to watch Tysha be gang-raped by his guards, Jaime carries a great amount of guilt for his unkindness to Tyrion in his later years.\\r\\nIn A Game of Thrones (1996), Jaime is introduced as one of the Kingsguard, the royal security detail, and the son of wealthy and powerful Tywin Lannister, the former Hand of the King. Jaime's twin is Cersei, the Queen of Westeros by virtue of her marriage to King Robert Baratheon. Perhaps the greatest swordsman in the kingdom, Jaime is sometimes derisively called \\"the Kingslayer\\" because he killed the \\"Mad King\\" Aerys Targaryen in the coup that put Robert on the Iron Throne.[1]\\r\\nEric Dodds of Time described Jaime as \\"handsome, an incomparably skilled fighter and disarmingly witty\\",[2] with The New Yorker calling the Lannisters \\"a crowd of high-cheekboned beauties ... who form a family constellation so twisted, charismatic, and cruel that it rivals Flowers in the Attic for blond dysfunction\\".[3] Lev Grossman wrote for Time that while Jaime and Cersei's younger brother Tyrion is a grotesque dwarf, \\"the rest of the Lannisters are stunted too, but on the inside.\\"[4] The Los Angeles Times called Jaime \\"handsome and unscrupulous\\",[5] though Dodds noted in 2014:\\r\\nSure, he's done some of the most despicable things on a show full of despicable thingsincluding but not limited to fathering children by incest, attempting to murder a boy who discovered said incest, and the cold-blooded murder of one of his own cousinsbut despite all that, the Kingslayer remains one of Game of Thrones' most popular characters.[2]\\r\\nDarren Franich of Entertainment Weekly noted that in the novels, \\"[Jaime is] a vaguely villainous minor character in Game of Thrones, then is basically absent from Clash of Kings, and suddenly he becomes a tragic hero in Storm of Swords.\\"[6] In A Game of Thrones, Jaime is not only carrying on an incestuous affair with his twin sister, but he pushes a young Bran Stark out a high window to his likely death after the boy witnesses them in the act.[7][8] Jaime admits these crimes to Catelyn Stark in A Clash of Kings (1998), and tells her a horrific story of Aerys Targaryen's cruelty.[9] In A Storm of Swords (2000), Jaime initially loathes the female warrior Brienne of Tarth, but both his honor and his reluctant respect for Brienne compel him to lie to their captors to prevent her from being raped.[8][10] He later explains to Brienne that he killed Aerys because the king had planned to incinerate all of King's Landing and its inhabitants rather than let it fall into Robert's hands.[11] When Jaime is released to be sent back to King's Landing in deference to his father, he first saves Brienne, who has been thrown into a bear pit for the mercenaries' amusement.[12] Martin told Rolling Stone in 2014:\\r\\nOne of the things I wanted to explore with Jaime, and with so many of the characters, is the whole issue of redemption. When can we be redeemed? Is redemption even possible? ... When do we forgive people? ... Our society is full of people who have fallen in one way or another ... How many good acts make up for a bad act? ... I don't know the answer, but these are questions worth thinking about. I want there to be a possibility of redemption for us, because we all do terrible things. We should be able to be forgiven. Because if there is no possibility of redemption, what's the answer then?[8]\\r\\nSpecifically addressing Jaime's attempted murder of Bran, Martin said:\\r\\n[What] Jaime did [to Bran] is interesting ... Remember, Jaime isn't just trying to kill Bran because he's an annoying little kid. Bran has seen something that is basically a death sentence for Jaime, for Cersei, and their children ... So I've asked people who do have children, \\"Well, what would you do in Jaime's situation?\\" They say, \\"Well, I'm not a bad guyI wouldn't kill.\\" Are you sure? Never? If Bran tells King Robert, he's going to kill you and your sister-lover, and your three children ... Then many of them hesitate. Probably more people than not would say, \\"Yeah, I would kill someone else's child to save my own child, even if that other child was innocent.\\" These are the difficult decisions people make, and they're worth examining.[8]\\r\\nJaime Lannister accompanies the royal family to Winterfell, where King Robert Baratheon hopes to persuade his old friend Ned Stark to serve as Hand of the King. During the visit, Ned's young son Bran inadvertently spies Jaime and Cersei having sex in a remote tower,[7] at which point Jaime pushes the boy out a window, intending to kill Bran to keep their relationship secret.[8] Bran managed to survive, though crippled, and when an assassin later tries to kill Bran, Catelyn Stark accuses and arrests Tyrion.[1] In revenge, Jaime instigates a brawl with Ned and his men in the streets of King's Landing, killing many on both sides. Ned later discovers that Robert's three children are actually the products of Jaime and Cersei's affair, but is executed by the oldest child, Joffrey Baratheon, upon the latter's ascension as king. Jaime then rides for the Riverlands to aid Tywin in his campaign against the Riverlands, taking command of half the Lannister host. He captures the Riverlands' capital of Riverrun, but his army is waylaid by Robb Stark's army in the Battle of the Whispering Wood. Jaime is taken prisoner and incarcerated in Riverrun. Despite his capture Joffrey names Jaime as commander of his Kingsguard.\\r\\nHillary Busis of Entertainment Weekly called the twist of Jaime and Cersei in the tower \\"lurid and shocking, exactly what I needed to jolt me awake and make me start paying closer attention ... By the end of the chapter  'The things I do for love'  I was totally hooked on Thrones\\".[7] Mikal Gilmore of Rolling Stone noted in 2014 that the moment in which Jaime pushes Bran to his likely death \\"grabs you by the throat\\".[8] Martin commented in the interview:\\r\\nI've had a million people tell me that was the moment that hooked them, where they said, \\"Well, this is just not the same story I read a million times before.\\"[8]\\r\\nTyrion makes several attempts to free Jaime, first by having disguised Lannister guards attempt to break him out and then by offering to swap Arya and Sansa Stark for Jaime. After hearing of the supposed deaths of Bran and Rickon Stark, Catelyn interrogates Jaime. Jaime admits to pushing Bran out the tower window, to his incest with Cersei and to fathering her children. Jaime then mocks Ned for having tarnished his own honour by fathering a bastard, prompting Catelyn to call her bodyguard Brienne of Tarth for her sword.[9][13]\\r\\nJaime is freed by Catelyn and sent to King's Landing to exchange for Sansa and Arya, escorted by Brienne of Tarth and Jaime's cousin Ser Cleos Frey. Cleos is killed by bandits and Jaime and Brienne are captured by the Brave Companions, who were formerly in service to Tywin but have defected to Roose Bolton. Their leader, Vargo Hoat, cuts off Jaime's sword hand in the hope that Tywin will blame Roose and prevent the Boltons defecting to the Lannisters.[10][12] While held captive at Harrenhal, Jaime reveals to Brienne the circumstances surrounding his murder of King Aerys.[11][12] Roose Bolton releases Jaime but keeps Brienne hostage. While returning to King's Landing, Jaime has a dream about Brienne and decides to return to Harrenhal to rescue her from Hoat.\\r\\nJaime returns to King's Landing to discover that Joffrey has been poisoned and Tyrion on trial for the murder, though Jaime refuses to believe Tyrion is guilty. He gifts Brienne a Valyrian steel sword forged from House Stark's ancestral sword Ice and tasks her with finding and protecting the fugitive Sansa Stark. He then forces Varys into helping Tyrion escape, confessing to Tyrion that he owed him a debt for his role in Tysha's fare. Outraged, Tyrion spitefully reveals to Jaime Cersei's affairs during his imprisonment, and lies that he did indeed kill Joffrey, before killing Tywin.[12]\\r\\nCersei orders Jaime to go to Riverrun and dislodge Ser Brynden \\"Blackfish\\" Tully. Before his leaving, Jaime has an armorer forge him a prosthetic hand. He takes the tongueless Ser Ilyn Payne with him to teach him to fight with his left hand, using the lessons to confess to his numerous crimes. During the march, he encounters his cousin Lancel, who confesses to his affair with Cersei. Jaime persuades Edmure Tully to force the Blackfish's surrender by threatening to sack the castle and kill Edmure's child when it is born. Jaime later receives a letter from Cersei, who has been imprisoned by the High Sparrow and is awaiting trial and begs Jaime to be her champion in her trial by combat, but Jaime has the letter burned without reply.[14]\\r\\nJaime travels to Raventree Hall and manages to have Lord Tytos Blackwood surrender, officially ending House Stark's insurrection. In the aftermath, he is approached by Brienne, who claims that Sansa is in danger from Sandor \\"The Hound\\" Clegane.[15] It is unknown how much of this is true, as Brienne was previously seen as a prisoner of a reanimated Catelyn Stark and the anti-Lannister Brotherhood Without Banners.\\r\\nJaime is portrayed by Nikolaj Coster-Waldau in the HBO adaptation Game of Thrones.[5] His casting was announced on August 20, 2009.[16]\\r\\nIn October 2014, Coster-Waldau and several other key cast members, all contracted for six seasons of the series, renegotiated their deals to include a potential seventh season and salary increases for seasons five, six, and seven.[17][18] The Hollywood Reporter called the raises \\"huge\\", noting that the deal would make the performers \\"among the highest-paid actors on cable TV\\".[17] Deadline.com put the number for season five at \\"close to $300,000 an episode\\" for each actor,[18] and The Hollywood Reporter wrote in June 2016 that the performers would each be paid \\"upward of $500,000 per episode\\" for seasons seven and the potential eight.[19] In 2017, He became one of the highest paid actor on television and will earn S2?million per episode for the show.[20][21]\\r\\nMatt Fowler of IGN noted in 2013 that \\"the people who do seem to get redemption arcs on this show are the villains\\".[22] Over the course of the first three seasons, the series has transitioned Jaime from an obvious villain to an antihero of sorts.[23][24][25] Eric Dodds of Time wrote that Jaime had become \\"a complex, bizarrely likable character\\".[2] Andrew Romano of The Daily Beast explained:\\r\\nBut Jaime wasn't a black-and-white baddie for long. In fact, GoT spent the next three seasons transforming him into a pretty sympathetic character. The turning point was when Jaime was captured and chained up by the Starksan ordeal that humbled him, humanized him, and eventually left him without a sword hand, struggling to find a new, post-Kingslayer identity for himself. Sure, Jaime could still slaughter his own cousin to escape captivity. But he could also rescue his sidekick Brienne of Tarth from a bear. And pledge to return the Stark girls to their mother, Catelyn. And refuse to kill his brother Tyrion on Cersei's behalf. And so on. He was a compromised, conflicted assholebut he was basically trying to do the right thing.[26]\\r\\nFowler wrote that Jaime's adventure with Brienne was \\"the best storyline of the season\\" in Season 3, aside from the Red Wedding.[22]\\r\\nJaime's apparent rape of Cersei in the fourth season episode \\"Breaker of Chains\\" created controversy among fans and journalists, who debated the show's depiction of sexual violence against women as well as Jaime's character development.[23][27][28] The showrunners never commented on what their intention with the scene actually was. The cast members involved initially gave only vague comments, but after the fourth season was released on Blu-ray and the showrunners avoided making any comment about the scene in it (it is conspicuously the only episode which has no commentary track), both Coster-Waldau and Headey publicly stated that the scene was never intended to portray rape at all - they were given no instructions to this effect (such as in the script) nor did they play it that way, and apparently the scene was just edited very confusingly.\\r\\nIn the source novel A Storm of Swords, the sex between Jaime and Cersei in the equivalent scene is consensual.[23][26][29][30] Several critics argued that the TV series' change damaged Jaime's redemption arc.[24][25][26][31] Dodds noted that the episode \\"irreparably changes the way we see Jaime Lannister\\".[2] Alyssa Rosenberg of The Washington Post wrote:\\r\\nWhat happens next dramatically complicates the work Game of Thrones has done to make Jaime a more explicable, even sympathetic character, given what we learned of his reasons for killing the king he was sworn to protect. Jaime has experienced profound losses over the last two seasons. His hand and his identity as a fighter have been taken from him. His son has been murdered. His father, a toxic, commanding man has returned to his life. And what Cersei is asking of Jaime is that he remove one of the few remaining things that gives him happiness, the little brother who makes him feel better about his hand, from existence. To assuage her pain and grief, Cersei is asking Jaime to inflict more pain on himself ... But his response is not to stop loving her, not to stop believing that he is victim to the gods. Instead, Jaime rapes his sister, passing that sense of unendurable pain on to her. He must know that this is the worst possible way that he could hurt her. Jaime knew that Robert raped Cersei ... Not only does raping Cersei remind his sister of her repeated, humiliating violation, Jaime is poisoning their own relationship, the thing that had been Cerseis antidote to the miseries of her marriage. It is an exceptionally cruel thing for Jaime to do.[32]\\r\\nCoster-Waldau said, \\"If you look closer there are those moments where shewell, I haven't seen the finished edit, of coursebut we tried to have it where she goes into it then she pulls away, she goes in then she pulls away, but of course he is forcing himself.\\"[33] Later he and his co-star Lena Headey (Cersei) spoke with Entertainment Weekly during the filming of season 5, admitting that they were never directed or intended to film a rape scene.[34] Headey stated:\\r\\nIts that terrible thing as a womantalking about something as horrendous as rape and dismissing it, which Im not. But we never discussed it as that. It was a woman in grief for her dead child, and the father of the childwho happens to be her brotherwho never really acknowledged the children is standing with her. Weve all experienced grief. Theres a moment of wanting to fill a void, and that is often very visceral, physical. That, for me, is where she was at. There was an emotional block, and [her brother] was just a bit of a drug for her.[34]\\r\\nThe Jaime-Cersei scene was subsequently ignored for the rest of Season 4 and the rest of the series. TheMarySue.com concluded that not really referring to the scene again, instead of as a long and developed subplot, trivializes rapeif, in fact, it was ever their actual intention to portray it as a rape scene. In real life a woman would be traumatized by being raped, not act as if nothing had happened immediately afterwards. TheMarySue.com was also critical that even if they didn't intend it as a rape scene, ignoring all questions about the scene and simply hoping they would go away over time was very insensitive to the audience.[35]\\r\\nIn 2016, Christopher Hooton wrote for The Independent:\\r\\nGame of Thrones is full of characters who are very sure of themselves ... Except Jaime Lannister, who was given a considerable amount of screen time this week in order to establish a little more complexity in his character. Thus far in the show his character arc has gone from \\"massive jerk\\", to \\"still a massive jerk but admirable in how he withstands imprisonment\\" to \\"maybe he's starting to redeem himself\\". This third strand had waned a little in season 6 as he returned to Cersei's side, but showed glimpses of returning in episode 8 as he was reunited with Brienne, about the only character who can appeal to his sense of guilt and honour.[36]\\r\\nJaime's storyline in the first season remains, for the most part, identical to his book storyline, with only minor details altered. In the aftermath of Jaime's capture, he confesses to Catelyn that he tried to kill Bran, but refuses to reveal why.\\r\\nRobb brings a captive Jaime with his camp as they march through the Westerlands, as Robb fears Tywin may coerce one of his bannerman into freeing Jaime. At one point Jaime attempts to escape by beating his cousin and fellow inmate Alton Lannister to death and strangling his guard Torrhen Karstark when he comes to investigate; though unsuccessfully, the anger of Torrhen's father Rickard proves fatal for Robb's campaign in the coming months. After Jaime goads Catelyn by mentioning Ned's infidelity, she releases him and has Brienne of Tarth escort him to King's Landing to trade for Sansa and Arya.\\r\\nJaime and Brienne are captured by a squad of Bolton soldiers. Jaime manages to convince them not to rape Brienne, but their leader Locke takes umbrage when Jaime tries to use his status to secure his own release and chops off Jaime's sword hand. The two are taken to Harrenhal, where the former maester Qyburn treats Jaime's wound and Jaime reveals to Brienne why he killed Aerys. Roose Bolton lets Jaime return to King's Landing but insists on keeping Brienne prisoner for abetting treason, though Jaime ultimately returns to rescue Brienne from being killed by a bear for Locke's amusement. The two return to King's Landing and Jaime is reunited with Cersei.\\r\\nTywin gifts Jaime a sword forged from Ice and asks him to resign from the Kingsguard and rule Casterly Rock, disowning him when he refuses. Qyburn fits Jaime with a gilded steel hand and Tyrion arranges for Jaime to have sword lessons with his bodyguard Bronn. Cersei initially refuses to resume their relationship, but ultimately relents and has sex with Jaime in front of Joffrey's body, lying in state. Jaime gifts Brienne Tywin's sword and the services of Tyrion's squire Podrick Payne, asking her to find Arya and Sansa and take them to safety. With Tyrion accused of Joffrey's murder, Jaime convinces Tywin to spare Tyrion in return for leaving the Kingsguard, though Tyrion later chooses trial by combat. Tyrion loses the trial and is sentenced to death, but Jaime releases him from his cell and helps him escape to Essos.\\r\\nCersei guilts Jaime for releasing Tyrion, and Jaime later admits to Bronn that he will kill Tyrion the next time they meet. When a message arrives from the Martells subtly threatening Myrcella as revenge for Prince Oberyn Martell's death in Tyrion's trial by combat, Jaime and Bronn travel to Dorne in secret to retrieve her. As they make their escape they are accosted by Oberyn's bastard daughters, the Sand Snakes, and engage in a fight before all five are arrested by the Water Gardens' palace guards. Doran Martell realises that the message was sent by Oberyn's paramour Ellaria Sand and decides to send Myrcella and his own son Trystane back to King's Landing with Jaime. As the ship sets sail, Myrcella admits to Jaime that she knows and is happy that he is her father. The two share a brief embrace before Myrcella suddenly collapses and dies, having been poisoned by Ellaria.\\r\\nJaime returns to King's Landing with Myrcella's corpse. He orders Trystane to stay on the boat outside the city to protect him from Cersei's wrath, and sends word to Doran naming Ellaria as Myrcella's killer, though Ellaria promptly kills Doran, has Obara and Nymeria kill Trystane and seizes control of Dorne. At Myrcella's funeral, Jaime confronts the religious leader, the High Sparrow, for having forced Cersei to walk naked through the streets of King's Landing as punishment for adultery, but is forced to stand down at the arrival of the Faith Militant. Jaime enlists the Tyrell army to march on the Sept of Baelor to secure the release of Margaery and Loras Tyrell. However, they find that Margaery has seemingly become a follower of the High Sparrow and that Tommen has forged an alliance with the Faith Militant. As punishment for taking up arms against the Faith, Jaime is removed from the Kingsguard. He is sent to Riverrun with Bronn to assist House Frey in ousting Brynden \\"Blackfish\\" Tully and the occupying Tully forces. After a failed parlay, Brienne arrives and beseeches Jaime to end the siege without bloodshed, so the Tully rebels can help Sansa Stark retake Winterfell. Brienne also fails, so Jaime manipulates the captured Edmure Tully into thinking his infant son will be killed if Edmure does not order a surrender. Edmure is released and promptly opens the gates to the Lannisters. Jaime sees Brienne and Podrick fleeing by boat from the castle walls, but only waves a discreet farewell and does not alert his men.\\r\\nAfter traveling to House Frey's fortress The Twins for a feast celebrating their victory, Jaime returns to King's Landing, and is horrified to discover that the Great Sept has been destroyed as a result of Cersei's plotting. He returns to the Red Keep in time to witness Cersei being crowned as Queen of the Seven Kingdoms, immediately realizing that Cersei essentially murdered the Faith Militant, the Tyrells, Grand Maester Pycelle and their uncle Kevan to acquire her new power.\\r\\nDespite his discomfort at the circumstances leading to Cersei's coronation, Jaime remains loyal to his sister, with Daenerys Targaryen and her forces sailing on Westeros. Cersei agrees to marry Euron Greyjoy after the war against Daenerys is won, but continues her relationship with Jaime, no longer attempting to hide their intimacy to their servants. Jaime negotiates with Randyll Tarly to pledge allegiance to the Lannisters; with the help of House Tarly's forces he leads the Lannister army to defeat the Tyrell army at Highgarden, following Olenna Tyrell's defection to Daenerys following Cersei's role in the death of the other Tyrells. In the aftermath of the battle, Jaime allows Olenna to commit suicide by poisoning. Before she dies, Olenna reveals that she was responsible for poisoning Joffrey.\\r\\nAs the Lannister forces return to King's Landing, they are attacked by Daenerys, with her horde of Dothraki and her dragon Drogon. Although the Lannisters suffer heavy casualties, Daenerys is forced to dismount Drogon after Bronn wounds the dragon with a scorpion. Jaime attempts to kill Daenerys but is narrowly saved from being burned alive by Drogon by Bronn's intervention. Jaime and Bronn fall into the Blackwater Rush, and survive the battle by being carried downstream and away from the carnage. Jaime returns to King's Landing, where he warns Cersei that the Lannisters are doomed if Daenerys escalates the conflict further and tells her of Olenna's confession.\\r\\nBronn takes Jaime to the Red Keep cellars under the pretense of training him; however, this is a ruse to allow Tyrion (who is now Hand to Daenerys) to speak with Jaime. Tyrion suggests a ceasefire so that Westeros can fight the threat posed by the White Walkers instead and proposes a meeting between Daenerys and Cersei so Jon Snow can present evidence of their existence. Jaime is apparently convinced by Tyrion and relays the message to Cersei. Cersei is skeptical, but remains adamant that the Lannisters will prevail against any foe. She also reveals that she is pregnant with Jaime's child.\\r\\nAt the meeting, Jon and Daenerys present a wight, convincing the Lannisters of the White Walkers' existence. After some persuasion from Tyrion, Cersei declares that she will order the Lannister forces north to aid in the war against the army of the dead. However, when Jaime begins organising the march north, she reveals to him that she was lying and will not intervene until either of the two enemies ÿ dead and living ÿ are defeated. Outraged by Cersei's shortsightedness and selfishness, Jaime leaves Cersei's side and rides north alone.\\r\\nMatt Roush wrote for TV Guide that Coster-Waldau plays \\"dastardly\\" Jaime \\"with malevolent charisma\\",[37] and Dodds noted that, despite the \\"despicable things\\" he has done, \\"the Kingslayer remains one of Game of Thrones' most popular characters\\".[2] Matthew Gilbert of The Boston Globe wrote:\\r\\nThe most riveting characters are the most self-serving, notably the queen, Cersei ... and her twin brother Jaime Lannister ... with whom she is having an incestuous affair. They have gorgeous, aristocratic features, but they are pure, compelling evil.[38]\\r\\nCoster-Waldau has received several nominations for his portrayal of Jaime, including the Critics' Choice Television Award for Best Supporting Actor in a Drama Series in 2013,[39] the Satellite Award for Best Supporting Actor in a Series, Miniseries or Television Film in 2013,[40] the Gold Derby TV Award for Best Drama Supporting Actor in 2013,[41] the People's Choice Award for Favorite TV Anti-Hero in 2014,[42] the Saturn Award for Best Supporting Actor on Television in 2014,[43] and the Zulu Award for Best Actor in 2017.[44]","input":"When did jaime lannister kill the mad king?"},{"output":"locomotive","context":"In railroading, the pilot (also known as a cowcatcher or cattle catcher) is the device mounted at the front of a locomotive to deflect obstacles on the track that might otherwise derail the train.\\r\\n\\r\\nIn addition to the pilot, small metal bars called life-guards, rail guards or guard irons (UK) are provided immediately in front of the wheels. They knock away smaller obstacles lying directly on the running surface of the railhead. Historically, fenced-off railway systems in Europe relied exclusively on those devices and did not use pilots, but that design is rarely used in modern systems.\\r\\n\\r\\nInstead of a pilot, trams use a device called a fender. Objects lying on the tram track get hit by a sensor bracket, which triggers the lowering of a basket-shaped device to the ground, preventing the overrunning of the obstacles and dragging them along the road surface in front of the wheels.\\r\\n\\r\\nIn snowy areas the pilot also has the function of a snowplow.\\r\\n\\r\\nThe pilot was invented by Charles Babbage in the 19th century, during his period of working for the Liverpool and Manchester Railway.[1] However, Babbage's invention was not built, and it is uncertain whether later users were aware of Babbage's idea.\\r\\n\\r\\nOn a mainline locomotive, the pilot has to successfully deflect an obstacle hit at speed; the ideal is to push it upwards and sideways out of the way.  The locomotive should not lift on impact or the train will follow, and the ideal is for a fairly smooth structure so that the locomotive will not get caught and derailed.\\r\\n\\r\\nThe typical shape is a blunt wedge that is shallowly V-shaped in plan.  In the later days of steam locomotives, the front coupler was designed to swing out of the way also, so it could not get caught up; this was called a drop coupler pilot.\\r\\n\\r\\nEarly on, pilots were normally fabricated of bars mounted on a frame; later on, sheet metal pilots were often used for their additional smoothness, and some cast steel pilots were employed for their mass and smooth shape.  Early diesel locomotives followed the same plan.\\r\\n\\r\\nEarly shunting locomotives often had a pilot with steps on it to allow yard workers to ride on the locomotive; these were called footboard pilots.  In some countries, footboard pilots are outlawed for safety reasons, and have been removed. Modern locomotives often have front and rear platforms with safety rails where workers can ride.\\r\\n\\r\\nMost modern European rail vehicles must have pilots with snowplow function and rail guards by law. The required strength of the system is 30?kN (3,059?kp) in the middle of the track and 50?kN (5,099?kp) near the rails.[2]\\r\\n\\r\\nModern US diesel locomotives have flatter, less wedge-shaped pilots, because a diesel locomotive has the cab near the front, and the crew are vulnerable to impact from obstacles pushed up by the pilot.\\r\\n\\r\\nTo protect the crew and passengers, most modern locomotives and passenger cars have a device known as an anti-climber fitted above the coupler, which is designed to prevent colliding vehicles from travelling up over the frame and through the locomotive cab or passenger car.  Where a pilot is not fitted, a different type of anti-climber may be used. This is to prevent one passenger car from riding up over another, or telescoping in a collision.","input":"What is the front part of a train called?"},{"output":"Hydrogen","context":"The abundance of the chemical elements is a measure of the occurrence of the chemical elements relative to all other elements in a given environment. Abundance is measured in one of three ways: by the mass-fraction (the same as weight fraction); by the mole-fraction (fraction of atoms by numerical count, or sometimes fraction of molecules in gases); or by the volume-fraction. Volume-fraction is a common abundance measure in mixed gases such as planetary atmospheres, and is similar in value to molecular mole-fraction for gas mixtures at relatively low densities and pressures, and ideal gas mixtures. Most abundance values in this article are given as mass-fractions.\\r\\nFor example, the abundance of oxygen in pure water can be measured in two ways: the mass fraction is about 89%, because that is the fraction of water's mass which is oxygen. However, the mole-fraction is 33.3333...% because only 1 atom of 3 in water, H2O, is oxygen. As another example, looking at the mass-fraction abundance of hydrogen and helium in both the Universe as a whole and in the atmospheres of gas-giant planets such as Jupiter, it is 74% for hydrogen and 23ÿ25% for helium; while the (atomic) mole-fraction for hydrogen is 92%, and for helium is 8%, in these environments. Changing the given environment to Jupiter's outer atmosphere, where hydrogen is diatomic while helium is not, changes the molecular mole-fraction (fraction of total gas molecules), as well as the fraction of atmosphere by volume, of hydrogen to about 86%, and of helium to 13%.[Note 1]\\r\\nThe abundance of chemical elements in the universe is dominated by the large amounts of hydrogen and helium which were produced in the Big Bang. Remaining elements, making up only about 2% of the universe, were largely produced by supernovae and certain red giant stars. Lithium, beryllium and boron are rare because although they are produced by nuclear fusion, they are then destroyed by other reactions in the stars.[1][2] The elements from carbon to iron are relatively more common in the universe because of the ease of making them in supernova nucleosynthesis. Elements of higher atomic number than iron (element 26) become progressively more rare in the universe, because they increasingly absorb stellar energy in being produced. Elements with even atomic numbers are generally more common than their neighbors in the periodic table, also due to favorable energetics of formation.\\r\\nThe abundance of elements in the Sun and outer planets is similar to that in the universe. Due to solar heating, the elements of Earth and the inner rocky planets of the Solar System have undergone an additional depletion of volatile hydrogen, helium, neon, nitrogen, and carbon (which volatilizes as methane). The crust, mantle, and core of the Earth show evidence of chemical segregation plus some sequestration by density. Lighter silicates of aluminum are found in the crust, with more magnesium silicate in the mantle, while metallic iron and nickel compose the core. The abundance of elements in specialized environments, such as atmospheres, or oceans, or the human body, are primarily a product of chemical interactions with the medium in which they reside.\\r\\n\\r\\n\\r\\nThe elements ÿ that is, ordinary (baryonic) matter made of protons, neutrons, and electrons, are only a small part of the content of the Universe. Cosmological observations suggest that only 4.6% of the universe's energy (including the mass contributed by energy, E = mc2 ? m = E / c2) comprises the visible baryonic matter that constitutes stars, planets, and living beings. The rest is thought to be made up of dark energy (68%) and dark matter (27%).[4] These are forms of matter and energy believed to exist on the basis of scientific theory and observational deductions, but they have not been directly observed and their nature is not well understood.\\r\\nMost standard (baryonic) matter is found in intergalactic gas, stars, and interstellar clouds, in the form of atoms or ions (plasma), although it can be found in degenerate forms in extreme astrophysical settings, such as the high densities inside white dwarfs and neutron stars.\\r\\nHydrogen is the most abundant element in the Universe; helium is second. However, after this, the rank of abundance does not continue to correspond to the atomic number; oxygen has abundance rank 3, but atomic number 8. All others are substantially less common.\\r\\nThe abundance of the lightest elements is well predicted by the standard cosmological model, since they were mostly produced shortly (i.e., within a few hundred seconds) after the Big Bang, in a process known as Big Bang nucleosynthesis. Heavier elements were mostly produced much later, inside of stars.\\r\\nHydrogen and helium are estimated to make up roughly 74% and 24% of all baryonic matter in the universe respectively. Despite comprising only a very small fraction of the universe, the remaining \\"heavy elements\\" can greatly influence astronomical phenomena. Only about 2% (by mass) of the Milky Way galaxy's disk is composed of heavy elements.\\r\\nThese other elements are generated by stellar processes.[5][6][7] In astronomy, a \\"metal\\" is any element other than hydrogen or helium. This distinction is significant because hydrogen and helium are the only elements that were produced in significant quantities in the Big Bang. Thus, the metallicity of a galaxy or other object is an indication of stellar activity, after the Big Bang.\\r\\nIn general, elements up to iron are made in large stars in the process of becoming supernovae. Iron-56 is particularly common, since it is the most stable element that can easily be made from alpha particles (being a product of decay of radioactive nickel-56, ultimately made from 14 helium nuclei). Elements heavier than iron are made in energy-absorbing processes in large stars, and their abundance in the universe (and on Earth) generally decreases with increasing atomic number.\\r\\nThe following graph (note log scale) shows abundance of elements in the Solar System. The table shows the twelve most common elements in our galaxy (estimated spectroscopically), as measured in parts per million, by mass.[3] Nearby galaxies that have evolved along similar lines have a corresponding enrichment of elements heavier than hydrogen and helium. The more distant galaxies are being viewed as they appeared in the past, so their abundances of elements appear closer to the primordial mixture. Since physical laws and processes are uniform throughout the universe, however, it is expected that these galaxies will likewise have evolved similar abundances of elements.\\r\\nThe abundance of elements is in keeping with their origin from the Big Bang and nucleosynthesis in a number of progenitor supernova stars. Very abundant hydrogen and helium are products of the Big Bang, while the next three elements are rare since they had little time to form in the Big Bang and are not made in stars (they are, however, produced in small quantities by breakup of heavier elements in interstellar dust, as a result of impact by cosmic rays).\\r\\nBeginning with carbon, elements have been produced in stars by buildup from alpha particles (helium nuclei), resulting in an alternatingly larger abundance of elements with even atomic numbers (these are also more stable). The effect of odd-numbered chemical elements generally being more rare in the universe was empirically noticed in 1914, and is known as the Oddo-Harkins rule.\\r\\nLoose correlations have been observed between estimated elemental abundances in the universe and the nuclear binding energy curve. Roughly speaking, the relative stability of various atomic nuclides has exerted a strong influence on the relative abundance of elements formed in the Big Bang, and during the development of the universe thereafter. [9] See the article about nucleosynthesis for the explanation on how certain nuclear fusion processes in stars (such as carbon burning, etc.) create the elements heavier than hydrogen and helium.\\r\\nA further observed peculiarity is the jagged alternation between relative abundance and scarcity of adjacent atomic numbers in the elemental abundance curve, and a similar pattern of energy levels in the nuclear binding energy curve. This alternation is caused by the higher relative binding energy (corresponding to relative stability) of even atomic numbers compared with odd atomic numbers and is explained by the Pauli Exclusion Principle.[10] The semi-empirical mass formula (SEMF), also called Weizs?cker's formula or the Bethe-Weizs?cker mass formula, gives a theoretical explanation of the overall shape of the curve of nuclear binding energy.[11]\\r\\nThe Earth formed from the same cloud of matter that formed the Sun, but the planets acquired different compositions during the formation and evolution of the solar system. In turn, the natural history of the Earth caused parts of this planet to have differing concentrations of the elements.\\r\\nThe mass of the Earth is approximately 5.98G1024?kg. In bulk, by mass, it is composed mostly of iron (32.1%), oxygen (30.1%), silicon (15.1%), magnesium (13.9%), sulfur (2.9%), nickel (1.8%), calcium (1.5%), and aluminium (1.4%); with the remaining 1.2% consisting of trace amounts of other elements.[12]\\r\\nThe bulk composition of the Earth by elemental-mass is roughly similar to the gross composition of the solar system, with the major differences being that Earth is missing a great deal of the volatile elements hydrogen, helium, neon, and nitrogen, as well as carbon which has been lost as volatile hydrocarbons. The remaining elemental composition is roughly typical of the \\"rocky\\" inner planets, which formed in the thermal zone where solar heat drove volatile compounds into space. The Earth retains oxygen as the second-largest component of its mass (and largest atomic-fraction), mainly from this element being retained in silicate minerals which have a very high melting point and low vapor pressure.\\r\\nThe mass-abundance of the nine most abundant elements in the Earth's crust is approximately: oxygen 46%, silicon 28%, aluminum 8.2%, iron 5.6%, calcium 4.2%, sodium 2.5%, magnesium 2.4%, potassium 2.0%, and titanium 0.61%. Other elements occur at less than 0.15%. For a complete list, see abundance of elements in Earth's crust.\\r\\nThe graph at right illustrates the relative atomic-abundance of the chemical elements in Earth's upper continental crust the part that is relatively accessible for measurements and estimation.\\r\\nMany of the elements shown in the graph are classified into (partially overlapping) categories:\\r\\nNote that there are two breaks where the unstable (radioactive) elements technetium (atomic number: 43) and promethium (atomic number: 61) would be. These elements are surrounded by stable elements, yet both have relatively short half lives (~ 4 million years and ~ 18 years respectively). These are thus extremely rare, since any primordial initial fractions of these in pre-Solar System materials have long since decayed and disappeared. These two elements are now only produced naturally through the spontaneous fission of very heavy radioactive elements (for example, uranium, thorium, or the trace amounts of plutonium that exist in uranium ores), or by the interaction of certain other elements with cosmic rays. Both technetium and promethium have been identified spectroscopically in the atmospheres of stars, where they are produced by ongoing nucleosynthetic processes.\\r\\nThere are also breaks in the abundance graph where the six noble gases would be, since they are not chemically bound in the Earth's crust, and they are only generated by decay chains from radioactive elements in the crust, and are therefore extremely rare there.\\r\\nThe eight naturally occurring very rare, highly radioactive elements (polonium, astatine, francium, radium, actinium, protactinium, neptunium, and plutonium) are not included, since any of these elements that were present at the formation of the Earth have decayed away eons ago, and their quantity today is negligible and is only produced from the radioactive decay of uranium and thorium.\\r\\nOxygen and silicon are notably the most common elements in the crust. On Earth and in rocky planets in general, silicon and oxygen are far more common than their cosmic abundance. The reason is that they combine with each other to form silicate minerals. In this way, they are the lightest of all of the two-percent \\"astronomical metals\\" (i.e., non-hydrogen and helium elements) to form a solid that is refractory to the Sun's heat, and thus cannot boil away into space. All elements lighter than oxygen have been removed from the crust in this way.\\r\\n\\"Rare\\" earth elements is a historical misnomer. The persistence of the term reflects unfamiliarity rather than true rarity. The more abundant rare earth elements are similarly concentrated in the crust compared to commonplace industrial metals such as chromium, nickel, copper, zinc, molybdenum, tin, tungsten, or lead. The two least abundant rare earth elements (thulium and lutetium) are nearly 200 times more common than gold. However, in contrast to the ordinary base and precious metals, rare earth elements have very little tendency to become concentrated in exploitable ore deposits. Consequently, most of the world's supply of rare earth elements comes from only a handful of sources. Furthermore, the rare earth metals are all quite chemically similar to each other, and they are thus quite difficult to separate into quantities of the pure elements.\\r\\nDifferences in abundances of individual rare earth elements in the upper continental crust of the Earth represent the superposition of two effects, one nuclear and one geochemical. First, the rare earth elements with even atomic numbers (58Ce, 60Nd, ...) have greater cosmic and terrestrial abundances than the adjacent rare earth elements with odd atomic numbers (57La, 59Pr, ...). Second, the lighter rare earth elements are more incompatible (because they have larger ionic radii) and therefore more strongly concentrated in the continental crust than the heavier rare earth elements. In most rare earth ore deposits, the first four rare earth elements ÿ lanthanum, cerium, praseodymium, and neodymium ÿ constitute 80% to 99% of the total amount of rare earth metal that can be found in the ore.\\r\\nThe mass-abundance of the eight most abundant elements in the Earth's mantle (see main article above) is approximately: oxygen 45%, magnesium 23%, silicon 22%, iron 5.8%, calcium 2.3%, aluminum 2.2%, sodium 0.3%, potassium 0.3%.\\r\\nThe mantle differs in elemental composition from the crust in having a great deal more magnesium and significantly more iron, while having much less aluminum and sodium.\\r\\nDue to mass segregation, the core of the Earth is believed to be primarily composed of iron (88.8%), with smaller amounts of nickel (5.8%), sulfur (4.5%), and less than 1% trace elements.[12]\\r\\nThe most abundant elements in the ocean by proportion of mass in percent are oxygen (85.84), hydrogen (10.82), chlorine (1.94), sodium (1.08), magnesium (0.1292), sulfur (0.091), calcium (0.04), potassium (0.04), bromine (0.0067), carbon (0.0028), and boron (0.00043).\\r\\nThe order of elements by volume-fraction (which is approximately molecular mole-fraction) in the atmosphere is nitrogen (78.1%), oxygen (20.9%),[14] argon (0.96%), followed by (in uncertain order) carbon and hydrogen because water vapor and carbon dioxide, which represent most of these two elements in the air, are variable components. Sulfur, phosphorus, and all other elements are present in significantly lower proportions.\\r\\nAccording to the abundance curve graph (above right), argon, a significant if not major component of the atmosphere, does not appear in the crust at all. This is because the atmosphere has a far smaller mass than the crust, so argon remaining in the crust contributes little to mass-fraction there, while at the same time buildup of argon in the atmosphere has become large enough to be significant.\\r\\nFor a complete list of the abundance of elements in urban soils, see Abundances of the elements (data page)#Urban soils.\\r\\nBy mass, human cells consist of 65ÿ90% water (H2O), and a significant portion of the remainder is composed of carbon-containing organic molecules. Oxygen therefore contributes a majority of a human body's mass, followed by carbon. Almost 99% of the mass of the human body is made up of six elements: oxygen, carbon, hydrogen, nitrogen, calcium, and phosphorus. The next 0.75% is made up of the next five elements: potassium, sulfur, chlorine, sodium, and magnesium. Only 17 elements are known for certain to be necessary to human life, with one additional element (fluorine) thought to be helpful for tooth enamel strength. A few more trace elements may play some role in the health of mammals. Boron and silicon are notably necessary for plants but have uncertain roles in animals. The elements aluminium and silicon, although very common in the earth's crust, are conspicuously rare in the human body.[15]\\r\\nBelow is a periodic table highlighting nutritional elements.[16]","input":"What are the two most common elements in the solar system?"},{"output":"Torosaurus","context":"?T. horridus Marsh, 1889\\r\\n?T. prorsus Marsh, 1890\\r\\nTriceratops is a genus of herbivorous ceratopsid dinosaur that first appeared during the late Maastrichtian stage of the late Cretaceous period, about 68 million years ago (mya) in what is now North America. It is one of the last known non-avian dinosaur genera, and became extinct in the CretaceousÿPaleogene extinction event 66 million years ago.[1] The term Triceratops, which literally means \\"three-horned face\\", is derived from the Greek ?- (tri-) meaning \\"three\\", ?ϫ? (kras) meaning \\"horn\\", and ? (ops) meaning \\"face\\".[2][3] Triceratops has been documented by numerous remains collected since the genus was first described in 1889, including at least one complete individual skeleton.[4] Paleontologist John Scannella observed: \\"It is hard to walk out into the Hell Creek Formation and not stumble upon a Triceratops weathering out of a hillside.\\" Forty-seven complete or partial skulls were discovered in just that area from 2000 to 2010.[5] Specimens representing life stages from hatchling to adult have been found.[6] As the archetypal ceratopsid, Triceratops is one of the most popular dinosaurs, and has been featured in film, postal stamps, and many other types of media.\\r\\nBearing a large bony frill and three horns on its large four-legged body, and possessing similarities with the modern rhinoceros, Triceratops is one of the most recognizable of all dinosaurs and the best known ceratopsid. It shared the landscape with and was probably preyed upon by Tyrannosaurus,[7] though it is less certain that the two did battle in the manner often depicted in traditional museum displays and popular images. The functions of the frills and three distinctive facial horns on its head have long inspired debate. Traditionally, these have been viewed as defensive weapons against predators. More recent theories, noting the presence of blood vessels in the skull bones of ceratopsids, find it more probable that these features were primarily used in identification, courtship and dominance displays, much like the antlers and horns of modern reindeer, mountain goats, or rhinoceros beetles.[8] The theory would find additional support if Torosaurus was found to be the mature form of Triceratops, as this would mean the frill also developed holes (fenestrae) as individuals reached maturity, rendering the structure more useful for display than defense.[9]\\r\\nThe exact placement of the genus Triceratops within the ceratopsid group has been debated by paleontologists. Two species, T. horridus and T. prorsus, are considered valid, although many other species have been named. Research published in 2010 suggested that the contemporaneous Torosaurus, a ceratopsid long regarded as a separate genus, represents Triceratops in its mature form.[9][10] The view was immediately disputed[11][12][13] and examination of more fossil evidence is expected to settle the debate.\\r\\n\\r\\n\\r\\nIndividual Triceratops are estimated to have reached about 7.9 to 9.0?m (25.9ÿ29.5?ft) in length, 2.9 to 3.0?m (9.5 to 9.8?ft) in height,[14][15] and 6.1ÿ12.0?tonnes (13,000ÿ26,000?lb) in weight.[16] The most distinctive feature is their large skull, among the largest of all land animals. The largest known skull (specimen MWC 7584, formerly BYU 12183) is estimated to have been 2.5 metres (8.2?ft) in length when complete,[9] and could reach almost a third of the length of the entire animal.[6] A specimen of T. horridus named Kelsey measured 7.3 metres (24?ft) long with a 1.98 metres (6.5?ft) skull, stood about 2.3 metres (7.5?ft) tall, and was estimated by the Black Hills institute to weight nearly 6 tonnes (5.9 long tons; 6.6 short tons).[17] A Triceratops 8 metres (26?ft) long has been estimated by Gregory S. Paul to have massed 9.3 tonnes (9.2 long tons; 10.3 short tons).[18] It bore a single horn on the snout, above the nostrils, and a pair of horns approximately 1?m (3.3?ft) long, with one above each eye.[19] In 2010, paleontologists revealed a fossil (named \\"Yoshi's Trike,\\" MOR 3027) with 115-centimetre-long (3.77?ft) horn cores, housed and displayed at the Museum of the Rockies in Montana.[20][21] To the rear of the skull was a relatively short, bony frill, adorned with epoccipitals in some specimens. Most other ceratopsids had large fenestrae in their frills, while those of Triceratops were noticeably solid.[22] T. horridus can be distinguished from T. prorsus by having a shallower snout.[18]\\r\\nTriceratops species possessed a sturdy build, with strong limbs, short hands with three hooves each, and short feet with four hooves each.[4] Although certainly quadrupedal, the posture of these dinosaurs has long been the subject of some debate. Originally, it was believed that the front legs of the animal had to be sprawling at angles from the thorax in order to better bear the weight of the head.[8] This stance can be seen in paintings by Charles Knight and Rudolph Zallinger. Ichnological evidence in the form of trackways from horned dinosaurs and recent reconstructions of skeletons (both physical and digital) seem to show that Triceratops and other ceratopsids maintained an upright stance during normal locomotion, with the elbows flexed and slightly bowed out, in an intermediate state between fully upright and fully sprawling (as in the modern rhinoceros).[4][23][24][25]\\r\\nThe hands and forearms of Triceratops retained a fairly primitive structure compared to other quadrupedal dinosaurs such as thyreophorans and many sauropods. In those two groups, the forelimbs of quadrupedal species were usually rotated so that the hands faced forward with palms backward (\\"pronated\\") as the animals walked. Triceratops, like other ceratopsians and the related quadrupedal ornithopods, walked with most of their fingers pointing out and away from the body, the primitive condition for dinosaurs also retained by bipedal forms like the theropods. In Triceratops, the weight of the body was carried by only the first three fingers of the hand, while digits 4 and 5 were vestigial and lacked claws or hooves.[4] The phalangeal formula is 2-3-4-3-1, meaning that the innermost finger of the forelimb has two bones, the next has three, etc.[26]\\r\\nThe first named specimen now attributed to Triceratops is a pair of brow horns attached to a skull roof, found near Denver, Colorado in the spring of 1887.[27] This specimen was sent to Othniel Charles Marsh, who believed that the formation from which it came dated from the Pliocene, and that the bones belonged to a particularly large and unusual bison, which he named Bison alticornis.[27][28] He realized that there were horned dinosaurs by the next year, which saw his publication of the genus Ceratops from fragmentary remains,[29] but he still believed B. alticornis to be a Pliocene mammal. It took a third and much more complete skull to change his mind. The specimen, collected in 1888 by John Bell Hatcher from the Lance Formation of Wyoming, was initially described as another species of Ceratops.[30] After reflection, Marsh changed his mind and gave it the generic name Triceratops, accepting his Bison alticornis as another species of Ceratops[31] (it would later be added to Triceratops[32]). The sturdy nature of the animal's skull has ensured that many examples have been preserved as fossils, allowing variations between species and individuals to be studied. Triceratops remains have subsequently been found in the American states of Montana and South Dakota (in addition to Colorado and Wyoming), and in the provinces of Saskatchewan and Alberta, Canada.\\r\\nAn earlier specimen, also recovered from the Lance Formation, was named Agathaumas sylvestris by Edward Drinker Cope in 1872. Originally identified as a hadrosaur, this specimen consists only of post-cranial remains and is only provisionally considered an example of Triceratops.[33]\\r\\nWithin the first decades after Triceratops was described, various skulls were collected, which varied to a lesser or greater degree from the original Triceratops, named T. horridus by Marsh (from the Latin horridus; \\"rough, rugose\\", suggesting the roughened texture of those bones belonging to the type specimen, later identified as an aged individual). This variation is unsurprising, given that Triceratops skulls are large three-dimensional objects, coming from individuals of different ages and both sexes, and which were subjected to different amounts and directions of pressure during fossilization.[8] Discoverers would name these as separate species (listed below), and came up with several phylogenetic schemes for how they were related to each other.\\r\\nIn the first attempt to understand the many species, Lull found two groups, although he did not say how he distinguished them: one composed of T. horridus, T. prorsus, and T. brevicornus; the other of T. elatus and T. calicornis. Two species (T. serratus and T. flabellatus) stood apart from these groups.[32] By 1933, and his revision of the landmark 1907 Hatcher-Marsh-Lull monograph of all known ceratopsians, he retained his two groups and two unaffiliated species, with a third lineage of T. obtusus and T. hatcheri that was characterized by a very small nasal horn.[34] T. horridus-T. prorsus-T. brevicornus was now thought to be the most conservative lineage, with an increase in skull size and a decrease in nasal horn size, and T. elatus-T. calicornis was defined by large brow horns and small nasal horn.[34][35] C. M. Sternberg made one modification, adding T. eurycephalus and suggesting that it linked the second and third lineages closer together than they were to the T. horridus lineage.[36] This pattern was followed until the major studies of the 1980s and 1990s.\\r\\nWith time, the idea that the differing skulls might be representative of individual variation within one (or two) species gained popularity. In 1986, Ostrom and Wellnhofer published a paper in which they proposed that there was only one species, Triceratops horridus.[37] Part of their rationale was that generally there are only one or two species of any large animal in a region (modern examples being the elephant and the giraffe in modern Africa). To their findings, Lehman added the old Lull-Sternberg lineages combined with maturity and sexual dimorphism, suggesting that the T. horridus-T. prorsus-T. brevicornus lineage was composed of females, the T.calicornis-T.elatus lineage was made up of males, and the T. obtusus-T. hatcheri lineage was of pathologic old males.[38] His reasoning was that males had taller, more erect horns and larger skulls, and females had smaller skulls with shorter, forward-facing horns.\\r\\nThese findings were contested a few years later by Catherine Forster, who reanalyzed Triceratops material more comprehensively and concluded that the remains fell into two species, T. horridus and T. prorsus, although the distinctive skull of T. (\\"Nedoceratops\\") hatcheri differed enough to warrant a separate genus.[39] She found that T. horridus and several other species belonged together, and T. prorsus and T. brevicornus stood alone, and since there were many more specimens in the first group, she suggested that this meant the two groups were two species. It is still possible to interpret the differences as representing a single species with sexual dimorphism.[8][40]\\r\\nIn 2009, John Scannella and Denver Fowler supported the separation of T. prorsus and T. horridus, and noted that the two species are also separated stratigraphically within the Hell Creek Formation, indicating that they did not live together at the same time.[41]\\r\\nSome of the following species are synonyms. The synonymised species is notified between brackets in every case (by means of \\"=T. horridus\\" or \\"=T. prorsus\\"). All the others in the list are considered nomina dubia (\\"dubious names\\") because they are based on remains that are too poor or incomplete to be distinguished from pre-existing Triceratops species.\\r\\nTriceratops is the best known genus of the Ceratopsidae, a family of large North American horned dinosaurs. The exact location of Triceratops among the ceratopsians has been debated over the years. Confusion stemmed mainly from the combination of short, solid frills (similar to that of Centrosaurinae), and the long brow horns (more akin to Ceratopsinae, also known as Chasmosaurinae).[42] In the first overview of horned dinosaurs, R. S. Lull hypothesized two lineages, one of Monoclonius and Centrosaurus leading to Triceratops, the other with Ceratops and Torosaurus, making Triceratops a centrosaurine as the group is understood today.[32] Later revisions supported this view, formally describing the first, short-frilled group as Centrosaurinae (including Triceratops), and the second, long-frilled group as Chasmosaurinae.[34][43]\\r\\nIn 1949, C. M. Sternberg was the first to question this and favoured instead that Triceratops was more closely related to Arrhinoceratops and Chasmosaurus based on skull and horn features, making Triceratops a ceratopsine (chasmosaurine of his usage) genus.[36] He was largely ignored, with John Ostrom,[44] and later David Norman both placing Triceratops within Centrosaurinae.[45]\\r\\nSubsequent discoveries and analyses upheld Sternberg's view on the position of Triceratops, with Lehman defining both subfamilies in 1990 and diagnosing Triceratops as ceratopsine (chasmosaurine of his usage) on the basis of several morphological features. In fact, it fits well into the ceratopsine subfamily, apart from its one feature of a shortened frill.[38] Further research by Peter Dodson, including a 1990 cladistic analysis[46] and a 1993 study using RFTRA (resistant-fit theta-rho analysis),[47] a morphometric technique which systematically measures similarities in skull shape, reinforces Triceratops' placement in the ceratopsine subfamily.\\r\\nThe below cladogram follows Longrich (2014), who named a new species of Pentaceratops, and included nearly all species of chasmosaurine.[48]\\r\\nMercuriceratops\\r\\nJudiceratops\\r\\nChasmosaurus\\r\\nMojoceratops\\r\\nAgujaceratops\\r\\nPentaceratops aquilonius\\r\\nWilliams Fork chasmosaur\\r\\nPentaceratops sternbergii\\r\\nUtahceratops\\r\\nKosmoceratops\\r\\nAnchiceratops\\r\\nAlmond Formation chasmosaur\\r\\nBravoceratops\\r\\nCoahuilaceratops\\r\\nArrhinoceratops\\r\\nTitanoceratops\\r\\nTorosaurus\\r\\nTriceratops\\r\\nFor many years after its discovery, the evolutionary origins of Triceratops remained largely obscure. In 1922, the newly discovered Protoceratops was seen as its ancestor by Henry Fairfield Osborn,[8] but many decades passed before additional findings came to light. Recent years have been fruitful for the discovery of several dinosaurs related to ancestors of Triceratops. Zuniceratops, the earliest known ceratopsian with brow horns, was described in the late 1990s, and Yinlong, the first known Jurassic ceratopsian, in 2005.\\r\\nThese new finds have been vital in illustrating the origins of horned dinosaurs in general, suggesting an Asian origin in the Jurassic, and the appearance of truly horned ceratopsians by the beginning of the late Cretaceous in North America.[49] As Triceratops is increasingly shown to be a member of the long-frilled Ceratopsinae subfamily, a likely ancestor may have resembled Chasmosaurus, which thrived some 5 million years earlier.\\r\\nIn phylogenetic taxonomy, the genus Triceratops has been used as a reference point in the definition of Dinosauria; dinosaurs have been designated as all descendants of the most recent common ancestor of Triceratops and Neornithes (i.e. modern birds).[50] Furthermore, the bird-hipped dinosaurs, Ornithischia, have all been designated dinosaurs with a more recent common ancestor to Triceratops than modern birds.[51]\\r\\nAlthough Triceratops are commonly portrayed as herding animals, there is currently little evidence that they lived in herds. While several other genera of horned dinosaurs are known from bonebeds preserving bones from two to hundreds or thousands of individuals, to date there is only one documented bonebed dominated by Triceratops bones: a site in southeastern Montana with the remains of three juveniles. It may be significant that only juveniles were present.[52] Another, more recent find may reveal that Triceratops lived in small family groups. In 2012, a group of three Triceratops in relatively complete condition, each of varying sizes from a full-grown adult to a small juvenile, were found in Wyoming, near Newcastle. The remains are currently under excavation by paleontologist Peter Larson and a team from the Black Hills Institute. It is believed that the animals were traveling as a family unit, but it remains unknown if the group consists of a mated pair and their offspring, or two females and a juvenile they were caring for. The remains also show signs of predation or scavenging from Tyrannosaurus, particularly on the largest specimen, with the bones of the front limbs showing breakage and puncture wounds from Tyrannosaurus teeth.[53]\\r\\nFor many years, Triceratops finds were known only from solitary individuals.[52] These remains are very common; for example, Bruce Erickson, a paleontologist of the Science Museum of Minnesota, has reported having seen 200?specimens of T. prorsus in the Hell Creek Formation of Montana.[54] Similarly, Barnum Brown claimed to have seen over 500?skulls in the field.[8]:79 Because Triceratops teeth, horn fragments, frill fragments, and other skull fragments are such abundant fossils in the Lancian faunal stage of the late Maastrichtian (late Cretaceous, 66?mya) Period of western North America, it is regarded as among the dominant herbivores of the time, if not the most dominant herbivore. In 1986, Robert Bakker estimated it as making up 5/6ths of the large dinosaur fauna at the end of the Cretaceous.[55] Unlike most animals, skull fossils are far more common than postcranial bones for Triceratops, suggesting that the skull had an unusually high preservation potential.[56]\\r\\nTriceratops was one of the last ceratopsian genera to appear before the CretaceousÿPaleogene extinction event. The related Torosaurus, and the more distantly related diminutive Leptoceratops, were also present, though their remains have been rarely encountered.[8]\\r\\nTriceratops were herbivorous, and because of their low head, their primary food was probably low growth, although they may have been able to knock down taller plants with their horns, beak, and bulk.[49][57] The jaws were tipped with a deep, narrow beak, believed to have been better at grasping and plucking than biting.[44]\\r\\nTriceratops teeth were arranged in groups called batteries, of 36 to 40 tooth columns, in each side of each jaw with 3 to 5 stacked teeth per column, depending on the size of the animal.[49] This gives a range of 432 to 800 teeth, of which only a fraction were in use at any given time (tooth replacement was continuous and occurred throughout the life of the animal).[49] They functioned by shearing in a vertical to near-vertical orientation.[49] The great size and numerous teeth of Triceratops suggests that they ate large volumes of fibrous plant material,[49] with some suggesting palms and cycads,[58][59] and others suggesting ferns, which then grew in prairies.[60]\\r\\nThere has been much speculation over the functions of Triceratops' head adornments. The two main theories have revolved around use in combat, or display in courtship, with the latter thought now to be the most likely primary function.[49]\\r\\nEarly on, Lull postulated that the frills may have served as anchor points for the jaw muscles to aid chewing by allowing increased size and thus power for the muscles.[61] This has been put forward by other authors over the years, but later studies do not find evidence of large muscle attachments on the frill bones.[62]\\r\\nTriceratops were long thought to have possibly used their horns and frills in combat with predators such as Tyrannosaurus, the idea being discussed first by C. H. Sternberg in 1917 and 70 years later by Robert Bakker.[55][63] There is evidence that Tyrannosaurus did have aggressive head-on encounters with Triceratops, based on partially healed tyrannosaur tooth marks on a Triceratops brow horn and squamosal; the bitten horn is also broken, with new bone growth after the break. Which animal was the aggressor is not known.[64] Since the Triceratops wounds healed, it is most likely that the Triceratops survived the encounter and managed to overcome the Tyrannosaurus. Paleontologist Peter Dodson estimates that if Tyrannosaurus attacked a bull Triceratops, the Triceratops had the upper hand and would successfully defend itself by inflicting fatal wounds to the Tyrannosaurus using its sharp horns.[65] Tyrannosaurus is also known to have fed on Triceratops. Evidence for this includes a heavily tooth-scored Triceratops ilium and sacrum.[7]\\r\\nIn addition to combat with predators using horns, Triceratops are classically shown engaging each other in combat with horns locked. While studies show that such activity would be feasible, if unlike that of present-day horned animals,[66] there is disagreement about whether they did so. Although pitting, holes, lesions, and other damage on Triceratops skulls (and the skulls of other ceratopsids) are often attributed to horn damage in combat, a 2006 study finds no evidence for horn thrust injuries causing these forms of damage (for example, there is no evidence of infection or healing). Instead, non-pathological bone resorption, or unknown bone diseases, are suggested as causes.[67] A newer study compared incidence rates of skull lesions and periosteal reaction in Triceratops and Centrosaurus and showed that these were consistent with Triceratops using its horns in combat and the frill being adapted as a protective structure, while lower pathology rates in Centrosaurus may indicate visual rather than physical use of cranial ornamentation, or a form of combat focused on the body rather than the head.[68] The frequency of injury was found to be 14% in Triceratops.[69] The researchers also concluded that the damage found on the specimens in the study was often too localized to be caused by bone disease.[70] Histological examination reveals that the frill of Triceratops is composed of fibrolamellar bone[71] which contains fibroblasts that play a critical role in wound healing, and are capable of rapidly depositing bone during remodeling.[72][73]\\r\\nOne skull, assigned to Triceratops, was observed to have a hole in the jugal which appears to be a puncture wound that was sustained while this individual was still alive. This is supported by signs of healing that are present in the bone around the supposed wound. When examined closely, the hole in the bone has a diameter that is very similar to diameter of the distal end of a Triceratops horn. This, and other apparent healed wounds in the skulls of ceratopsians, has been cited as evidence of non-fatal intraspecific competition in these dinosaurs.[74][75]\\r\\nThe large frill also may have helped to increase body area to regulate body temperature.[76] A similar theory has been proposed regarding the plates of Stegosaurus,[77] although this use alone would not account for the bizarre and extravagant variation seen in different members of the Ceratopsidae.[49] This observation is highly suggestive of what is now believed to be the primary function, display.\\r\\nThe theory of their use in sexual display was first proposed by Davitashvili in 1961 and has gained increasing acceptance since.[38][62][78] Evidence that visual display was important, either in courtship or in other social behavior, can be seen in the fact that horned dinosaurs differ markedly in their adornments, making each species highly distinctive. Also, modern living creatures with such displays of horns and adornments use them in similar behavior.[79] A 2006 study of the smallest Triceratops skull, ascertained to be a juvenile, shows the frill and horns developed at a very early age, predating sexual development and thus probably important for visual communication and species recognition in general.[80] The use of the exaggerated structures in dinosaurs as species identification has been questioned, as no such function exists for structures in modern species.[81]\\r\\nIn 2006, the first extensive ontogenetic study of Triceratops was published in the journal Proceedings of the Royal Society. The study, by John R. Horner and Mark Goodwin, found that individuals of Triceratops could be divided into four general ontogenetic groups, babies, juveniles, subadults, and adults. With a total number of 28 skulls studied, the youngest was only 38?cm (15?in) long. 10 of the 28 skulls could be placed in order in a growth series with one representing each age. Each of the four growth stages were found to have identifying features. Multiple ontogenetic trends were discovered, including the size reduction of the epoccipitals, development and reorientation of postorbital horns, and hollowing out of the horns.[82]\\r\\nTorosaurus is a ceratopsid genus first identified from a pair of skulls in 1891, two years after the identification of Triceratops. The Torosaurus genus resembles Triceratops in geological age, distribution, anatomy and physical size and it has been recognised as a close relative.[83] Its distinguishing features are an elongated skull and the presence of two fenestrae, or holes, in the frill. Paleontologists investigating dinosaur ontogeny (growth and development of individuals over the life span) in the Hell Creek Formation, Montana, US, have recently presented evidence that the two represent a single genus.\\r\\nJohn Scannella, in a paper presented in Bristol, UK at the conference of the Society of Vertebrate Paleontology (25 September 2009) reclassified Torosaurus as especially mature Triceratops individuals, perhaps representing a single sex. Jack Horner, Scannella's mentor at Bozeman Campus, Montana State University, noted that ceratopsian skulls consist of metaplastic bone. A characteristic of metaplastic bone is that it lengthens and shortens over time, extending and resorbing to form new shapes. Significant variety is seen even in those skulls already identified as Triceratops, Horner said, \\"where the horn orientation is backwards in juveniles and forward in adults\\". Approximately 50% of all subadult Triceratops skulls have two thin areas in the frill that correspond with the placement of \\"holes\\" in Torosaurus skulls, suggesting that holes developed to offset the weight that would otherwise have been added as maturing Triceratops individuals grew longer frills.[84] A paper describing these findings in detail was published in July 2010 by Scannella and Horner. It formally argues that Torosaurus and the similar contemporary Nedoceratops are synonymous with Triceratops.[9]\\r\\nThe assertion ignited debate. Andrew Farke had in 2006 stressed that, apart from the frill, no systematic differences could be found between Torosaurus and Triceratops.[83] He nevertheless disputed Scannella's conclusion by arguing in 2011 that the proposed morphological changes required to \\"age\\" a Triceratops into a Torosaurus would be without precedent among ceratopsids. Creatures would require the growth of epoccipitals, reversion of bone texture from adult to immature forms back to adult, and growth of frill holes at a later stage than usual.[11] A study by Nicholas Longrich and Daniel Field analyzed 35 specimens of both Triceratops and Torosaurus. The authors concluded that Triceratops individuals too old to be considered immature forms are represented in the fossil record, as are Torosaurus individuals too young to be considered fully mature adults. The synonymy of Triceratops and Torosaurus cannot be supported, they said, without more convincing intermediate forms than Scannella and Horner initially produced. Scannella's Triceratops specimen with a hole on its frill, they argued, could represent a diseased or malformed individual rather than a transitional stage between an immature Triceratops and mature Torosaurus form.[12][13]\\r\\nGiven the abundance of fossils, particularly of Triceratops, additional field discoveries are expected to settle the debate in time.\\r\\nOpinion has varied on the validity of a separate genus for Nedoceratops. John Scannella and Jack Horner regarded it as an intermediate growth stage between Triceratops and Torosaurus.[9][85] Andrew Farke, in his 2011 redescription of the only known skull, concluded that it was an aged individual of its own valid taxon, Nedoceratops hatcheri.[11] Nicholas Longrich and Daniel Fields also did not consider it a transition between Torosaurus and Triceratops, suggesting that the frill holes were pathological.[13]\\r\\nAs described above, John Scannella had argued in 2010 that Nedoceratops should be considered a synonym of Triceratops.[9] Andrew Farke (2011) maintained that it represents a valid distinct genus.[11] Nick Longrich agreed with Scannella about Nedoceratops and made a further suggestion: that the recently described Ojoceratops was likewise a synonym. The fossils, he argued, are indistinguishable from the T. horridus specimens that were previously attributed to the defunct species T. serratus.\\r\\nLongrich observed that another newly described genus, Tatankaceratops, displayed a strange mix of characteristics already found in adult and juvenile Triceratops. Rather than representing a distinct genus, Tatankaceratops could as easily represent a dwarf Triceratops or a Triceratops individual with a developmental disorder that caused it to stop growing prematurely.[86]\\r\\nTriceratops lived during the Late Cretaceous of North America, its fossils have come from the Evanston Formation, Scollard Formation, Laramie Formation, Lance Formation, Denver Formation, and Hell Creek Formation.[87] These fossil formations date back to the time of the Cretaceous-Paleogene Extinction Event, and has been dated to 66 I 0.07 million years ago.[88] Many animals and plants have been found in these formations, but mostly from the Lance Formation and Hell Creek Formation.[87]\\r\\nTheropods from these formations include genera of tyrannosaurids, ornithomimids, troodontids,[87] avialans,[89] caenagnathids,[90] and dromaeosaurids. Acheroraptor and Dakotaraptor are dromaeosaurids from the Hell Creek Formation. Indeterminate dromaeosaurs are known from other fossil formations. Common teeth previously referred to Dromaeosaurus and Saurornitholestes later were considered to be Acheroraptor.[91] The tyrannosaurids from the formation are Nanotyrannus and Tyrannosaurus, although the former might be a junior synonym of the latter. Among ornithomimids are the genera Struthiomimus as well as Ornithomimus,[87] although an undescribed animal named \\"Orcomimus\\" could be from the formation.[92] Troodontids are only represented by Pectinodon and Paronychodon in the Hell Creek Formation; with a possible species of Troodon from the Lance Formation. One species of coelurosaur is known from Hell Creek and similar formations by a single species, Richardoestesia. Only three oviraptorosaurs are from the Hell Creek Formation, Anzu, Leptorhynchos[90] and a giant species of caenagnathid, very similar to Gigantoraptor, from South Dakota. However, only fossilized foot prints were discovered.[93] The avialans known from the formation are Avisaurus,[87] multiple species of Brodavis,[94] and several other species of hesperornithoforms, as well as several species of true birds including Cimolopteryx.[89]\\r\\nOrnithischians are abundant in the Scollard Lance, Laramie, Lance, Denver, and Hell Creek Formation. The main groups of ornithischians are ankylosaurians, ornithopods, ceratopsians, and pachycephalosaurians. Three ankylosaurians are known, Ankylosaurus, Denversaurus, and possibly a species of Edmontonia or an undescribed genus. Multiple genera of ceratopsians are known from the formation other than Triceratops, the leptoceratopsid Leptoceratops, and the chasmosaurine ceratopsids Torosaurus,[87]Nedoceratops and Tatankaceratops.[95] Ornithopods are common in the Hell Creek Formation, and are known from several species of the ornithopod Thescelosaurus, and the hadrosaurids Edmontosaurus,[87][96] and a possible species of Parasaurolophus. Several pachycephalosaurians have been found in the Hell Creek Formation and in similar formations. Among them are the derived pachycephalosaurids Stygimoloch,[87] Dracorex,[97] Pachycephalosaurus,[87] Sphaerotholus, and an undescribed specimen from North Dakota. The first two might be junior synonyms of Pachycephalosaurus.\\r\\nMammals are plentiful in the Hell Creek Formation. Groups represented include multituberculates, metatherians, and eutherians. The multituberculates represented include Paracimexomys,[98] the cimolomyids Paressonodon,[99] Meniscoessus, Essonodon, Cimolomys, Cimolodon, and Cimexomys; and the neoplagiaulacids Mesodma, and Neoplagiaulax. The alphadontids Alphadon, Protalphodon, and Turgidodon, pediomyids Pediomys,[98] Protolambda, and Leptalestes,[100] the stagodontid Didelphodon,[98] the deltatheridiid Nanocuris, the herpetotheriid Nortedelphys,[99] and the glasbiid Glasbius all represent metatherians of the Hell Creek Formation. A few eutherians are known, being represented by Alostera,[98] Protungulatum,[100] the cimolestids Cimolestes and Batodon, the gypsonictopsid Gypsonictops, and the possible nyctitheriid Paranyctoides.[98]\\r\\nTriceratops (the species are not identified) is the official state fossil of South Dakota,[101] and the official state dinosaur of Wyoming.[102] The distinctive appearance of Triceratops has led to them being frequently depicted in films, computer games and documentaries, such as the 1993 film Jurassic Park and the 1999 BBC television documentary Walking with Dinosaurs. A recurring theme, especially in children's dinosaur books, is a climactic showdown or battle between Triceratops and Tyrannosaurus. In 1942, Charles R. Knight painted a mural incorporating a confrontation between the two dinosaurs in the Field Museum of Natural History for the National Geographic Society, establishing them as enemies in popular thought.[103] Paleontologist Bob Bakker said of the imagined rivalry between Tyrannosaurus and Triceratops, \\"No matchup between predator and prey has ever been more dramatic. It's somehow fitting that those two massive antagonists lived out their co-evolutionary belligerence through the very last days of the very last epoch of the Age of Dinosaurs.\\"[103]","input":"What is the closest relative to the triceratops?"},{"output":"Pete Maravich","context":"In basketball, points are the sum of the score accumulated through free throw or field goal.[1] In National Collegiate Athletic Association?(NCAA) Division?I?basketball, where a player's career is at most four seasons long, it is considered a notable achievement to reach the 1,000-points scored threshold. In even rarer instances, players have reached the 2,000- and 3,000-point plateaus (no player has ever scored 4,000 or more points at the Division?I?level). The top?25 highest scorers in NCAA Division I men's basketball history are listed below. The NCAA did not split into its current divisions format until August 1973.[2] From 1906 to 1955, there were no classifications to the NCAA nor its predecessor, the Intercollegiate Athletic Association of the United States (IAAUS).[2] Then, from 1956 to spring 1973, colleges were classified as either \\"NCAA University Division (Major College)\\" or \\"NCAA College Division (Small College)\\".[2][3]\\r\\nSome of the top 25 scorers in Division I history played before the three-point line was officially instituted in 1986ÿ87. All of the players with a dash through the three-point field goals made column were affected by this rule. Hank Gathers of Loyola Marymount is the only three-point shot era player on this list who did not make a single three-point shot. From the 1986ÿ87 season through the 2007ÿ08 season, the three-point perimeter was marked at 19?ft 9?in (6.02?m) for both men's and women's college basketball.[4] On May 3, 2007, the NCAA men's basketball rules committee passed a measure to extend the distance of the men's three-point line back to 20?ft 9?in (6.32?m); the women's line would remain at the original distance until it was moved to match the men's distance effective in 2011ÿ12.[4]\\r\\nAdditionally, several of the players on this list played during an era when college freshmen were ineligible to compete at the varsity level and were forced to participate on either freshman or junior varsity teams. Since freshman and junior varsity points do not count toward official NCAA records, three playersPete Maravich, Oscar Robertson and Elvin Hayesonly had three seasons to compile their totals. Larry Bird redshirted (sat out) his freshman year, and therefore, like Maravich, Robertson, and Hayes, his totals also cover only three seasons (though in Bird's case, unlike the others, it was his own choice not to play a fourth season at the college level). With the added benefits of a three-point line and a full extra year of varsity eligibility, their already-historical statistics would have been much higher. Maravich, a guard from LSU, not only owns the three highest single season averages in Division?I history, but also the highest career total. Remarkably, he scored 3,667?points (over 400 more than the next closest player) in a mere 83?games. His record is considered nearly unbreakable; the only player who could have potentially overtaken him as the top scorer in Division?I history is Stephen Curry of Davidson, who had scored 2,635?points through his first three seasons of college basketball. However, Curry opted to forgo his final year of NCAA eligibility and moved on to the National Basketball Association?(NBA) following his junior season in 2008ÿ09.\\r\\nFive players on this list are enshrined in the Naismith Memorial Basketball Hall of Fame: Pete Maravich,[5] Oscar Robertson,[6] Elvin Hayes,[7] Larry Bird[8] and David Robinson.[9]\\r\\nThe following list contains current and defunct Division I conferences' all-time scoring leaders. The \\"conference founded\\" column indicates when each respective conference first began intercollegiate athletic competition, not necessarily when they began basketball. For example, the Great West Conference was established as a football-only conference in 2004 but became an all-sports conference in 2008 (with basketball actually beginning in 2009ÿ10).[34] Also note that some of the schools on this list are no longer in the conference in which they are identified. Utah, for instance, is currently a member of the Pac-12 Conference, but when Keith Van Horn set the scoring record they were still a member of the Western Athletic Conference. Similarly, BYU is currently in the West Coast Conference, but their final four seasons in the Mountain West Conference were the years in which Jimmer Fredette played at the school and set that conference's scoring record.","input":"Who is the leading scorer in college basketball?"},{"output":"Temne","context":"Sierra Leone is home to about sixteen ethnic groups, each with its own language. In Sierra Leone, membership of an ethnic group often overlaps with a shared religious identity.\\r\\n\\r\\n\\r\\nThe largest contest within Sierra Leone's political culture centres upon the competition between the ethnic Temne in Sierra Leone's north-west, and the Mende in Sierra Leone's south-east.[1]\\r\\nThe vast majority of the Mende support the Sierra Leone People's Party (SLPP). The majority of the Temne support the All People's Congress (APC).[1]\\r\\nThe largest ethnic group is the Temne at about 36% of Sierra Leone's population. The Temne predominate in the Northern Sierra Leone and the areas around the capital of Sierra Leone.\\r\\nThe vast majority of Temne are Muslims, with a small Christian minority.\\r\\nThe Temne are thought to have come from Futa Jallon, which is in present-day Guinea. Sierra Leone's former president Ernest Bai Koroma is the first ethnic Temne to be elected to the office. The majority of the Temne support the All People's Congress (APC).[1]\\r\\nA major ethnic group are the Mende, at about 33% of Sierra Leone's population. The Mende predominate in the Southern Province and Eastern Sierra Leone (with the exception of Kono District).\\r\\nThe Mende are a Muslim majority group, though with a large Christian minority. The Mende, who are believed to be descendants of the Mane, originally occupied the Liberian hinterland. They began moving into Sierra Leone slowly and peacefully in the eighteenth century.\\r\\nThe vast majority of the Mende support the Sierra Leone People's Party (SLPP).[1]\\r\\nOne of the biggest minority ethnic groups are the Fula at around 3.4% of the population. Descendants of seventeenth- and eighteenth-century Fulani migrant settlers from the Fouta Jallo, Fouta Toro, and Maasina regions of Guinea, Senegal, Mali, and Mauritania. The Sierra Leone Fula people settled in the Western Area region of Sierra Leone more than four hundred years ago as settlers from mainly the Fouta Djallon Kingdom that expanded to northern Sierra Leone (Kabala, Bombali).\\r\\nThe Sierra Leonean Fula are traditionally a nomadic, pastoralist, trading people, herding cattle, goats and sheep across the vast dry hinterlands of their domain, keeping somewhat separate from the local agricultural populations. Many of the large shopping centers in Sierra Leone are owned and run by the Fula community.\\r\\nA significant number of the Sierra Leone Fula population are found in all regions of Sierra Leone as traders, and many live in middle-class homes. Because of their trading, the Fulas are found in nearly all parts of the country.\\r\\nThe third largest ethnic group are the Limba at around 6,4% of the population. The Limba are Indigenous people of Sierra Leone and speak various dialects of a language largely unrelated to other tribal languages in Sierra Leone. They are primarily found in the Northern Province, particularly in Bombali District, Koinadugu and Kambia District. During Sierra Leone's colonial era thousands of Limbas migrated to the capital city of Freetown and its Western Area. As a result, a significant number of Limbas can be found in Freetown and its surrounding Western Area. During the 16th, 17th, and 18th century, many Limba people were shipped to North America as slaves.\\r\\nA significant ethnic group are the Mandingo (also known as Mandinka). They are descendants of traders from Guinea who migrated to Sierra Leone during the late nineteenth to mid-twentieth centuries. The Mandika are predominantly found in the east and the northern part of the country.\\r\\nThey predominate in the large towns, most notably Karina, in Bombali District in the north; Kabala and Falaba in Koinadugu District in the north; and Yengema, Kono District in the east of the country. Like the Fula, the Mandinka are virtually all Muslims. Sierra Leone's third president Ahmad Tejan Kabbah, and Sierra Leone's first Vice President Sorie Ibrahim Koroma were both ethnic Mandingo.\\r\\nNext in proportion are the Kono, who live primarily in Kono District in Eastern Sierra Leone. The Kono are descendants of migrants from Guinea; today their workers are known primarily as diamond miners. The majority of the Kono ethnic group are Christians, though with an influential Muslim minority. Sierra Leone's former Vice-President Alhaji Samuel Sam-Sumana is an ethnic Kono.\\r\\nThe small but significant Krio people (descendants of freed African American, West Indian and Liberated African slaves who settled in Freetown between 1787 and about 1885) make up about 3% of the population. They primarily occupy the capital city of Freetown and its surrounding Western Area. Krio culture reflects the Western culture and ideals within which many of their ancestors originated - they also had close ties with British officials and colonial administration during years of development.\\r\\nThe Krio have traditionally dominated Sierra Leone's judiciacy and Freetown's elected city council. One of the first ethnic groups to become educated according to Western traditions, they have traditionally been appointed to positions in the civil service, beginning during the colonial years. They continue to be influential in the civil service. The vast majority of Krios are Christians, though with a significant Muslim minority.\\r\\nOther minority ethnic groups are the Kuranko, who are related to the Mandingo, and are largely Muslims. The Kuranko are believed to have begun arriving in Sierra Leone from Guinea in about 1600 and settled in the north, particularly in Koinadugu District.\\r\\nThe Kuranko are primarily farmers; leaders among them have traditionally held several senior positions in the Military. Sierra Leone current Finance Minister Kaifala Marah is an ethnic Kuranko.\\r\\nThe Loko in the north are native people of Sierra Leone, believed to have lived in Sierra Leone since the time of European encounter. Like the neighbouring Temne, the Loko are Muslim majority.\\r\\nThe Susu and their related Yalunka are traders; both groups are primarily found in the far north in Kambia and Koinadugu District close to the border with Guinea. The Susu and Yalunka are both descendants of migrants from Guinea. They are virtually all Muslims.\\r\\nThe Kissi live further inland in South-Eastern Sierra Leone. They predominate in the large town of Koindu and its surrounding areas in Kailahun District. The vast majority of Kissi are Christians.\\r\\nThe much smaller Vai are primarily found in Kailahun and Pujehun Districts near the border with Liberia. The Vai are largely Muslim.\\r\\nThe Kru are also primarily found in Kailahun and Pujehun Districts near the border with Liberia. The Kru predominate in the Kroubay neighbourhood in the capital Freetown. The Kru are largely Christian.\\r\\nOn the coast in Bonthe District in the south are the Sherbro. Native to Sierra Leone, they have occupied Sherbro Island since it was founded. The Sherbro are primarily fisherman and farmers, and they are predominantly found in Bonthe District. The Sherbro are virtually all Christians, and their paramount chiefs had a history of intermarriage with British colonists and traders.\\r\\nA small number of Sierra Leoneans are of partial or full Lebanese ancestry, descendants of traders who first came to the nation in the 19th century. They are locally known as Sierra Leonean-Lebanese. The Sierra Leonean-Lebanese community are primarily traders and they mostly live in middle-class households in the urban areas, primarily in Freetown, Bo, Kenema, Koidu Town and Makeni.","input":"What is the largest tribe in sierra leone?"},{"output":"30 miles","context":"Methuen /m??Ĳu??n/ is a city[4] in Essex County, Massachusetts, United States. The population was 47,255 at the 2010 census.[5]\\r\\n\\r\\n\\r\\nMethuen was first settled in 1642 and was officially incorporated in 1726. Methuen was originally part of Haverhill, Massachusetts. In 1724 Stephen Barker and others in the western part of that town petitioned the General Court to grant them permission to form a new town above Hawke's Meadow Brook. Although opposed by their fellow townsmen, the petition was approved the following year (December 8, 1725), and the General Court gave them an act of incorporation under the name of Methuen. The town was named for Sir Paul Methuen, a member of the King's Privy Council and friend of acting Provincial Governor William Dummer. The first town meeting was held March 9, 1726, in the home of a resident. Land was set aside for a meetinghouse, which was erected later in 1726 on what is now Meetinghouse Hill Cemetery.[6]\\r\\nThe residents in the northern part of the new town of Methuen soon petitioned to have their own meetinghouse (a combination of town hall and puritan church), and in 1736 the north parish was set off. Land for a meetinghouse was donated by descendants of the original proprietors of Haverhill, and in 1738 the second Methuen meetinghouse was raised. The structure survives to this day, as the Salem N.H. Historical Society building. In 1741, with the fixing of the Northern boundary of Massachusetts, most of this new north parish was removed from Methuen and placed in New Hampshire. It was incorporated as Salem, New Hampshire in 1750.[7]\\r\\nIndustrial growth in the 1800s influenced Methuen's development. Construction of the Methuen Cotton Mills at the Spicket River falls in the 1820s and the increased manufacture of hats and shoes in small factories along the Spicket spurred the centralization of Methuen's economic, residential and cultural activities within the area around Osgood, Broadway, Hampshire and Pleasant streets. Three wealthy and prominent familiesthe Nevins, the Tenneys and the Searlesplayed a significant role in Methuen's history and development. These families were instrumental in the founding of many of Methuen's landmarks, including the Nevins Memorial Library, the Searles building, Tenney Gatehouse, Nevins Home, Spicket Falls, and the Civil War monument between Pleasant and Charles streets.[8]\\r\\nMethuen is located at 424348N 711046W? / ?42.73000N 71.17944W? / 42.73000; -71.17944 (42.730040, ?71.179352).[9] According to the United States Census Bureau, the city has a total area of 23.1 square miles (59.7?km2), of which 22.2 square miles (57.6?km2) is land and 0.77 square miles (2.0?km2), or 3.42%, is water.[10]\\r\\nMethuen lies alongside the northern banks of the Merrimack River, and is also bisected by the Spicket (originally \\"Spigot\\") River,[11] as well as many brooks and streams. There are several ponds dotting the area as well, and the town is home to a town forest, a bird sanctuary, and a small state park (Tenney State Park). Pine Island, near the southern end of town in the Merrimack River, is also part of the town's land.\\r\\nMethuen lies along the northwestern edge of Essex County, just east of Middlesex County and just south of Rockingham County, New Hampshire. The irregularly-shaped town is bordered by Haverhill to the northeast, North Andover to the east, Lawrence and Andover to the south, Dracut (Middlesex County) to the west, Pelham, New Hampshire (Hillsborough County) to the northwest, and Salem, New Hampshire (Rockingham County) to the north. Methuen is located 30 miles (48?km) north-northwest of Boston and 25 miles (40?km) south-southeast of Manchester, New Hampshire.\\r\\nMethuen lies at the northern end of Interstate 93 in Massachusetts, with three exits providing access. A portion of Interstate 495 crosses through the eastern side of town from Lawrence to Haverhill. Massachusetts Route 213, the \\"Loop Connector\\", provides highway access between the two, lying entirely within town and having five exits of its own. The town is also crossed by Route 28, Route 110, and Route 113, the latter two meeting at a rotary at I-93 Exit 46, one of the more congested intersections along the I-93 corridor. Construction to replace this interchange began in July 2014;[12] it is expected to be complete at some point in 2017 with a partial cloverleaf interchange.[13] I-93 provides the town's only bridge across the Merrimack; there are several crossings in Lawrence, and several in neighboring Haverhill, but none for 7 miles (11?km) upstream from I-93 all the way to the eastern end of Lowell.\\r\\nMethuen is served by the Merrimack Valley Regional Transit Authority's bus service; there is no other mass transportation source within town. The nearest rail station is in South Lawrence, which is part of the Haverhill/Reading Line of the MBTA Commuter Rail, providing service into Boston's North Station. Small plane service can be found at Lawrence Municipal Airport and the Merrimack Valley Seaplane Base, with the nearest national service being at Manchester-Boston Regional Airport, and the nearest international service being at Logan International Airport.\\r\\nAs of the census[25] of 2000, there were 43,789 people, 16,532 households, and 11,539 families residing in the city. The population density was 1,954.7 persons per square mile (754.8/km2). There were 16,885 housing units, at an average density of 753.7 per square mile (291.0/km2). The racial makeup of the city was 89.35% White, 1.35% African American, 0.22% Native American, 2.38% Asian, 0.01% Pacific Islander, 4.87% from other races, and 1.82% from two or more races. Hispanics and Latinos, of any race, were 9.64% of the population (8.4% Dominican, 5.7% Puerto Rican, 0.6% Guatemalan, 0.3% Ecuadorian, 0.3% Mexican, 0.3% Cuban).\\r\\nThere were 16,532 households, of which 33.1% had children under the age of 18 living with them, 53.3% were married couples living together, 12.2% had a female householder with no husband present, and 30.2% were non-families. 25.3% of all households were made up of individuals and 11.6% had someone living alone who was 65 years of age or older. The average household size was 2.62 and the average family size was 3.17.\\r\\nIn the city, the population was spread out with 24.7% under the age of 18, 7.3% from 18 to 24, 31.0% from 25 to 44, 21.6% from 45 to 64, and 15.3% who were 65 years of age or older. The median age was 38 years. For every 100 females there were 91.9 males. For every 100 females age 18 and over, there were 87.0 males.\\r\\nThe median income for a household in the city was $49,627, and the median income for a family was $59,831. Males had a median income of $41,693 versus $31,864 for females. The per capita income for the city was $22,305. About 5.8% of families and 7.4% of the population were below the poverty line, including 9.7% of those under age 18 and 7.7% of those age 65 or over.\\r\\nHistorically, Methuen had a town meeting-selectmen form of government and was known as the Town of Methuen until it adopted a charter replacing its traditional town meeting and selectmen with a council and manager. Even with a form of government that had historically and legally been exclusive to cities, the community, in a gesture of traditionalism, retained the name Town of Methuen in its charter.[4] However, because Massachusetts cities have self-governing powers not available to towns, it became known for legal purposes as \\"The City Known as the Town of Methuen\\". A subsequent charter, which adopted a strong mayor form of government, officially changed the community name to the \\"City of Methuen\\".\\r\\nMethuen's city government consists of a mayor, three Councilors-at-Large, two East District councilors, two Central District councilors, two West District councilors, and six School Committee members.\\r\\nMethuen High School's athletic teams play in the Merrimack Valley Conference. Their big rivals are the Andover Golden Warriors, the Central Catholic Raiders of Lawrence, and the Haverhill Hillies. On Thanksgiving Day, the football team plays fellow Merrimack Valley foe the Dracut Middies. The teams first met in a non-Thanksgiving Day game in 1935 and did not play again until the Thanksgiving series started in 1963. The school colors are blue and white, and their mascot is the Ranger, named after Rogers' Rangers, the precursor of the U.S. Army Rangers, which was founded by town resident Robert Rogers.\\r\\nThe Searles Tenney Nevins Historic District, established by the city in 1992 to preserve the \\"distinctive architecture and rich character of one of Massachusetts' most unique neighborhoods\\", is named after the three Methuen city fathers: David C. Nevins, Edward F. Searles and Charles H. Tenney.\\r\\nFrom the City of Methuen:\\r\\nToday, the trio's collective vision can be seen in mills, housing, schools, mansions, churches, monuments, playgrounds, the library, and the architectural fantasies that resulted from their artistic rivalry. The historic district boundaries were established to include properties and buildings constructed or used by the Searles, Tenney and Nevins families and the people who worked for them.[29]\\r\\nThe historic district is administered by the Methuen Historic District Commission, which protects the district from alterations that might compromise its historic integrity. Bounded within the Searles Tenney Nevins Historic District are the Spicket Falls Historic District and the Pleasant-High Historic District. Both are registered with the National Register of Historic Places, as are many of the other buildings within the area.[30]","input":"How far is methuen ma from boston ma?"},{"output":"adjacent to South Bend, Indiana","context":"","input":"Where is notre dame fighting irish university located?"},{"output":"the curve of the oxygen saturation of haemoglobin","context":"Arterial blood is the oxygenated blood in the circulatory system found in the pulmonary vein, the left chambers of the heart, and in the arteries.[1] It is bright red in color, while venous blood is dark red in color (but looks purple through the translucent skin). It is the contralateral term to venous blood.\\r\\nFramed in the cardiac cycle, often historically accredited to the Wiggers diagram, arterial blood has just passed through the lungs and is ready to boost oxygen to sustain the peripheral organs. The essential difference between venous and arterial blood is the curve of the oxygen saturation of haemoglobin. The difference in the oxygen content of the blood between the arterial blood and the venous blood is known as the arteriovenous oxygen difference.[2]","input":"What is the difference between arterial and venous blood?"},{"output":"5 November 2013","context":"The Mars Orbiter Mission (MOM), also called Mangalyaan (\\"Mars-craft\\", from Sanskrit: ???? mangala, \\"Mars\\" and ??? yna, \\"craft, vehicle\\"),[9][10] is a space probe orbiting Mars since 24 September 2014. It was launched on 5 November 2013 by the Indian Space Research Organisation (ISRO).[11][12][13][14] It is India's first interplanetary mission[15] and ISRO has also become the fourth space agency to reach Mars, after the Soviet space program, NASA, and the European Space Agency.[16][17] It is the first Asian nation to reach Mars orbit, and the first nation in the world to do so in its first attempt.[18][19][20][21]\\r\\nThe Mars Orbiter Mission probe lifted-off from the First Launch Pad at Satish Dhawan Space Centre (Sriharikota Range SHAR), Andhra Pradesh, using a Polar Satellite Launch Vehicle (PSLV) rocket C25 at 09:08?UTC on 5 November 2013.[22] The launch window was approximately 20 days long and started on 28 October 2013.[5] The MOM probe spent about a month in Earth orbit, where it made a series of seven apogee-raising orbital manoeuvres before trans-Mars injection on 30 November 2013 (UTC).[23] After a 298-day transit to Mars, it was successfully inserted into Mars orbit on 24 September 2014.\\r\\nThe mission is a \\"technology demonstrator\\" project to develop the technologies for designing, planning, management, and operations of an interplanetary mission.[24] It carries five instruments that will help advance knowledge about Mars to achieve its secondary, scientific objective.[25] The spacecraft is currently being monitored from the Spacecraft Control Centre at ISRO Telemetry, Tracking and Command Network (ISTRAC) in Bangalore with support from Indian Deep Space Network (IDSN) antennae at Byalalu.[26]\\r\\n\\r\\n\\r\\nOn 23 November 2008, the first public acknowledgement of an unmanned mission to Mars was announced by then-ISRO chairman G. Madhavan Nair.[27] The MOM mission concept began with a feasibility study in 2010 by the Indian Institute of Space Science and Technology after the launch of lunar satellite Chandrayaan-1 in 2008. The government of India approved the project on 3 August 2012,[28] after the Indian Space Research Organisation completed ?125 crore (US$19?million) of required studies for the orbiter.[29] The total project cost may be up to ?454 crore (US$71?million).[11][30] The satellite costs ?153 crore (US$24?million) and the rest of the budget has been attributed to ground stations and relay upgrades that will be used for other ISRO projects.[31]\\r\\nThe space agency had planned the launch on 28 October 2013 but was postponed to 5 November 2013 following the delay in ISRO's spacecraft tracking ships to take up pre-determined positions due to poor weather in the Pacific Ocean.[5] Launch opportunities for a fuel-saving Hohmann transfer orbit occur every 26 months, in this case the next two would be in 2016 and 2018.[32]\\r\\nAssembly of the PSLV-XL launch vehicle, designated C25, started on 5 August 2013.[33] The mounting of the five scientific instruments was completed at Indian Space Research Organisation Satellite Centre, Bangalore, and the finished spacecraft was shipped to Sriharikota on 2 October 2013 for integration to the PSLV-XL launch vehicle.[33] The satellite's development was fast-tracked and completed in a record 15 months.[34] Despite the US federal government shutdown, NASA reaffirmed on 5 October 2013 it would provide communications and navigation support to the mission.[35] During a meeting on 30 September 2014, NASA and ISRO officials signed an agreement to establish a pathway for future joint missions to explore Mars. One of the working group's objectives will be to explore potential coordinated observations and science analysis between the MAVEN orbiter and MOM, as well as other current and future Mars missions.[36]\\r\\nThe total cost of the mission was approximately ?450 Crore (US$73 million),[37][38] making it the least-expensive Mars mission to date.[39] The low cost of the mission was ascribed by K. Radhakrishnan, the chairman of ISRO, to various factors, including a \\"modular approach\\", few ground tests and long (18ÿ20 hour) working days for scientists.[40] BBC's Jonathan Amos mentioned lower worker costs, home-grown technologies, simpler design, and a significantly less complicated payload than NASA's MAVEN.[25]\\r\\nThe primary objective of the mission is to develop the technologies required for designing, planning, management and operations of an interplanetary mission.[24] The secondary objective is to explore Mars' surface features, morphology, mineralogy and Martian atmosphere using indigenous scientific instruments.[41]\\r\\nThe main objectives are to develop the technologies required for designing, planning, management and operations of an interplanetary mission comprising the following major tasks:[42]:42\\r\\nThe scientific objectives deal with the following major aspects:[42]:43\\r\\nThe mission would also provide multiple opportunities to observe the Martian moon Phobos and also offer an opportunity to identify and re-estimate the orbits of asteroids seen during the Martian Transfer Trajectory.[42]:43\\r\\nThe 15?kg (33?lb) scientific payload consists of five instruments:[44][45][46]\\r\\nThe ISRO Telemetry, Tracking and Command Network performed navigation and tracking operations for the launch with ground stations at Sriharikota, Port Blair, Brunei and Biak in Indonesia,[47] and after the spacecraft's apogee became more than 100,000?km, an 18?m (59?ft) and a 32?m (105?ft) diameter antenna of the Indian Deep Space Network were utilised.[48] The 18?m (59?ft) dish antenna was used for communication with the craft until April 2014, after which the larger 32?m (105?ft) antenna was used.[49] NASA's Deep Space Network is providing position data through its three stations located in Canberra, Madrid and Goldstone on the US West Coast during the non-visible period of ISRO's network.[50] The South African National Space Agency's (SANSA) Hartebeesthoek (HBK) ground station is also providing satellite tracking, telemetry and command services.[51]\\r\\nCommunications are handled by two 230-watt TWTAs and two coherent transponders. The antenna array consists of a low-gain antenna, a medium-gain antenna and a high-gain antenna. The high-gain antenna system is based on a single 2.2-metre (7?ft 3?in) reflector illuminated by a feed at S-band. It is used to transmit and receive the telemetry, tracking, commanding and data to and from the Indian Deep Space Network.[2]\\r\\nAs originally conceived, ISRO was to launch MOM on its Geosynchronous Satellite Launch Vehicle (GSLV),[71] but as the GSLV failed twice in 2010 and ISRO was continuing to sort out issues with its cryogenic engine,[72] it was not advisable to wait for the new batch of rockets as that would have delayed the MOM project for at least three years.[73] ISRO opted to switch to the less-powerful Polar Satellite Launch Vehicle (PSLV). Since the PSLV was not powerful enough to place MOM on a direct-to-Mars trajectory, the spacecraft was launched into a highly elliptical Earth orbit and used its own thrusters over multiple perigee burns (to take advantage of the Oberth effect) to place itself on a trans-Mars trajectory.[71]\\r\\nOn 19 October 2013, ISRO chairman K. Radhakrishnan announced that the launch had to be postponed by a week as a result of a delay of a crucial telemetry ship reaching Fiji. The launch was rescheduled for 5 November 2013.[5] ISRO's PSLV-XL placed the satellite into Earth orbit at 09:50?UTC on 5 November 2013,[29] with a perigee of 264.1?km (164.1?mi), an apogee of 23,903.6?km (14,853.0?mi), and inclination of 19.20 degrees,[52] with both the antenna and all three sections of the solar panel arrays deployed.[74] During the first three orbit raising operations, ISRO progressively tested the spacecraft systems.[58]\\r\\nThe orbiter's dry mass is 500?kg (1,100?lb), and it carried 852?kg (1,878?lb) of fuel and oxidiser at launch. Its main engine, which is a derivative of the system used on India's communications satellites, uses the bipropellant combination monomethylhydrazine and dinitrogen tetroxide to achieve the thrust necessary for escape velocity from Earth. It was also used to slow down the probe for Mars orbit insertion and, subsequently, for orbit corrections.\\r\\nSeveral orbit raising operations were conducted from the Spacecraft Control Centre (SCC) at the ISRO Telemetry, Tracking and Command Network (ISTRAC) at Peenya, Bangalore on 6, 7, 8, 10, 12 and 16 November by using the spacecraft's on-board propulsion system and a series of perigee burns. The first three of the five planned orbit raising manoeuvres were completed with nominal results, while the fourth was only partially successful. However, a subsequent supplementary manoeuvre raised the orbit to the intended altitude aimed for in the original fourth manoeuvre. A total of six burns were completed while the spacecraft remained in Earth orbit, with a seventh burn conducted on 30 November to insert MOM into a heliocentric orbit for its transit to Mars.\\r\\nThe first orbit-raising manoeuvre was performed on 6 November 2013 at 19:47?UTC when the spacecraft's 440-newton (99?lbf) liquid engine was fired for 416 seconds. With this engine firing, the spacecraft's apogee was raised to 28,825?km (17,911?mi), with a perigee of 252?km (157?mi).[53]\\r\\nThe second orbit raising manoeuvre was performed on 7 November 2013 at 20:48?UTC, with a burn time of 570.6 seconds resulting in an apogee of 40,186?km (24,970?mi).[54][55]\\r\\nThe third orbit raising manoeuvre was performed on 8 November 2013 at 20:40?UTC, with a burn time of 707 seconds, resulting in an apogee of 71,636?km (44,513?mi).[54][56]\\r\\nThe fourth orbit raising manoeuvre, starting at 20:36?UTC on 10 November 2013, imparted an incremental velocity of 35?m/s (110?ft/s) to the spacecraft instead of the planned 135?m/s (440?ft/s) as a result of underburn by the motor.[57][75] Because of this, the apogee was boosted to 78,276?km (48,638?mi) instead of the planned 100,000?km (62,000?mi).[57] When testing the redundancies built-in for the propulsion system, the flow to the liquid engine stopped, with consequent reduction in incremental velocity. During the fourth orbit burn, the primary and redundant coils of the solenoid flow control valve of 440 newton liquid engine and logic for thrust augmentation by the attitude control thrusters were being tested. When both primary and redundant coils were energised together during the planned modes, the flow to the liquid engine stopped. Operating both the coils simultaneously is not possible for future operations, however they could be operated independently of each other, in sequence.[58]\\r\\nAs a result of the fourth planned burn coming up short, an additional unscheduled burn was performed on 12 November 2013 that increased the apogee to 118,642?km (73,721?mi),[54][58] a slightly higher altitude than originally intended in the fourth manoeuvre.[54][76] The apogee was raised to 192,874?km (119,846?mi) on 15 November 2013, 19:57?UTC in the final orbit raising manoeuvre.[54][76]\\r\\nOn 30 November 2013 at 19:19?UTC, a 23-minute engine firing initiated the transfer of MOM away from Earth orbit and on heliocentric trajectory toward Mars.[23] The probe travelled a distance of 780,000,000 kilometres (480,000,000?mi) to reach Mars.[77]\\r\\nFour trajectory corrections were originally planned, but only three were carried out.[61] The first trajectory correction manoeuvre (TCM) was carried out on 11 December 2013 at 01:00?UTC by firing the 22-newton (4.9?lbf) thrusters for a duration of 40.5 seconds.[54] After this event, MOM was following the designed trajectory so closely that the trajectory correction manoeuvre planned in April 2014 was not required. The second trajectory correction manoeuvre was performed on 11 June 2014 at 11:00?UTC by firing the spacecraft's 22 newton thrusters for a duration of 16 seconds.[78] The third planned trajectory correction manoeuvre was postponed, due to the orbiter's trajectory closely matching the planned trajectory.[79] The third trajectory correction was also a deceleration test 3.9 seconds long on 22 September 2014.[70]\\r\\nThe plan was for an insertion into Mars orbit on 24 September 2014,[7][80] approximately 2 days after the arrival of NASA's MAVEN orbiter.[81] The 440-newton liquid apogee motor was successfully test fired on 22 September at 09:00?UTC for 3.968 seconds, about 41 hours before actual orbit insertion.[80][82][83]\\r\\nAfter these events, the spacecraft performed a reverse manoeuvre to reorient from its deceleration burn and successfully entered Martian orbit.[8][84][4]\\r\\nThe orbit insertion put MOM in a highly elliptical orbit around Mars, with a period of 72 hours 51 minutes 51 seconds, a periapsis of 421.7?km (262.0?mi) and apoapsis of 76,993.6?km (47,841.6?mi).[8] At the end of the orbit insertion, MOM was left with 40?kg (88?lb) of fuel on board, more than the 20?kg (44?lb) necessary for a six-month mission.[85]\\r\\nOn 28 September 2014, MOM controllers published the spacecraft's first global view of Mars. The image was captured by the Mars Colour Camera (MCC).[86]\\r\\nOn 7 October 2014, the ISRO altered MOM's orbit so as to move it behind Mars for Comet Siding Spring's flyby of the planet on 19 October 2014. The spacecraft consumed 1.9?kg (4?lb) of fuel for the manoeuvre. As a result, MOM's apoapsis was reduced to 72,000?km (45,000?mi).[87] After the comet passed by Mars, ISRO reported that MOM remained healthy.[88]\\r\\nOn 4 March 2015, the ISRO reported that MOM's methane sensors were functioning normally and are studying Mars' albedo, the reflectivity of the planet's surface. The Mars Colour Camera was also returning new images of the Martian surface.[89][90]\\r\\nOn 24 March 2015, MOM completed its initial six-month mission in orbit around Mars. ISRO extended the mission by an additional six months; the spacecraft has 37?kg (82?lb) of propellant remaining and all five of its scientific instruments are working properly.[91] The orbiter can reportedly continue orbiting Mars for several years with its remaining propellant.[92]\\r\\nA 17-day communications blackout occurred from 6 to 22 June 2015 while Mars' orbit took it behind the Sun from Earth's view.[42]:52\\r\\nOn 24 September 2015, ISRO released its \\"Mars Atlas\\", a 120-page scientific atlas containing images and data from the Mars Orbiter Mission's first year in orbit.[93]\\r\\nIn March 2016, the first science results of the mission were published in Geophysical Research Letters, presenting measurements obtained by the spacecraft's MENCA instrument of the Martian exosphere.[94][95]\\r\\nOn 17 January 2017, MOM's orbit was altered to avoid the impending eclipse season. With a burn of eight 22?N thrusters for 431 seconds, resulting in a velocity difference of 97.5 metres per second (351?km/h) using 20 kilograms (44?lb) of propellant (leaving 13?kg remaining), eclipses will be avoided until September 2017. The battery is able to handle eclipses of up to 100 minutes.[96]\\r\\nOn 19 May 2017, MOM reached 1,000 days (973 sols) in orbit around Mars. In that time, the spacecraft completed 388 orbits of the planet and relayed more than 715 images back to Earth. ISRO officials stated that it remains in good health.[97]\\r\\nThe Mars Orbiter Mission team won US-based National Space Society's 2015 Space Pioneer Award in the science and engineering category. NSS said the award was given as the Indian agency successfully executed a Mars mission in its first attempt; and the spacecraft is in an elliptical orbit with a high apoapsis where, with its high resolution camera, it is taking full-disk colour imagery of Mars. Very few full disk images have ever been taken in the past, mostly on approach to the planet, as most imaging is done looking straight down in mapping mode. These images will aid planetary scientists.[98][99][100]\\r\\nAn illustration of the Mars Orbiter Mission spacecraft is featured on the reverse of the ?2,000 currency note of India.[101]\\r\\nISRO plans to develop and launch a follow-up mission called Mangalyaan 2 with a greater scientific payload to Mars by 2020.[102] The mission will consist of an orbiter, and will not include a lander or rover as suggested earlier.[103] Mangalyaan 2 will be launched after the Chandrayaan 2 Moon mission scheduled for December 2018.","input":"When was the first indian space craft sent to planet mars by isro?"},{"output":"Victoria Woodhull","context":"The following is a list of female U.S. presidential and vice-presidential nominees and invitees. Nominees are candidates nominated or otherwise selected by political parties for particular offices. Listed as nominees or nomination candidates are those women who achieved ballot access in at least one state (or, before the institution of government-printed ballots, had ballots circulated by their parties). They each may have won the nomination of one of the US political parties (either one of the two major parties or one of the third parties), or made the ballot as an Independent, and in either case must have votes in the election to qualify for this list. Exception is made for those few candidates whose parties lost ballot status for additional runs.\\r\\n\\r\\n\\r\\nIn 1872, Victoria Woodhull ran for president. While many historians and authors agree that Woodhull was the first woman to run for president, some have questioned the legality of her run. They disagree with classifying it as a true candidacy because she was younger than the constitutionally mandated age of 35, but election coverage by contemporary newspapers does not suggest age was a significant issue. The presidential inauguration was in March 1873, and Woodhull's 35th birthday was sixth months later in September. In 1884, Belva Lockwood followed with a run for president. Her running mate was Marietta Stow, who became the first woman to run for vice president.[1]\\r\\nThe first woman considered for a major party presidential candidacy by an incumbent president was Oveta Hobby, by Dwight D. Eisenhower. Eisenhower encouraged Hobby to run in 1960, but she declined.[2] In 1964, Margaret Chase Smith announced her candidacy for the Republican Party nomination, becoming the first female candidate for a major party's nomination. She qualified for the ballot in six state primaries, and came in second in the Illinois primary, receiving 25% of the vote. She became the first woman to have her name placed in nomination for the presidency at a major political party's convention.[3] In 1968, Charlene Mitchell became the first African American woman to run for president, and the first to receive valid votes in a general election. She qualified for the ballot in two states, winning 1,075 votes.[4] In 1972, Shirley Chisholm became the first black candidate for a major party's presidential nomination, and the first woman to run for the Democratic Party's nomination.[5] Tonie Nathan, the Libertarian Party's vice-presidential candidate in 1972, became the first woman to receive an electoral vote, via faithless elector Roger MacBride.[6] In the 1988 presidential election, Lenora Fulani and her running mate Joyce Dattner became the first women to achieve ballot access in all fifty states.[7] Fulani was also the first African American to do so. In the 2008 Democratic presidential primaries, Hillary Clinton became the first woman to win a presidential primary, and the first to be listed as a presidential candidate in every primary and caucus nationwide.[8] Despite losing the nomination in a close race against Barack Obama, Clinton won more votes in 2008 than any primary candidate in American history.\\r\\nIn 2016, Hillary Clinton became the first woman nominated for president by a major party after winning a majority of delegates in the Democratic Party primaries, and was formally nominated at the Democratic National Convention on July 26, 2016.[9][10] As a major party nominee, Clinton became the first woman to participate in a presidential debate, and later the first to carry a state in a general election. Clinton lost the election but became the first woman to win the popular vote.[11]\\r\\nTwo women have won the vice-presidential nominations of major parties, Geraldine Ferraro for the Democratic Party in the 1984, and Sarah Palin for the Republican Party in the 2008.\\r\\nThe Green Party has run a female candidate three times, Cynthia McKinney in 2008 and Jill Stein in 2012 and 2016. Stein received more general election votes than any prior female candidate in 2012.\\r\\nThis list includes female candidates who have run or are currently running for President of the United States in a general election and are sorted by the number of votes they received in the election.\\r\\n???? Major-party nominee\\r\\nThis list includes female candidates for a party nomination for President of the United States who ran in at least one primary or caucus, sorted by the number of votes they received during their run.\\r\\nCandidates who failed to receive their parties' nomination (or who are currently campaigning for their party's nomination).\\r\\nThis list includes female candidates who have run or are currently running for Vice President of the United States and received over 100,000 votes. Note that the vote for Vice President is not separate in the United States and is tied together with whoever their running mate is.[71]\\r\\n???? Indicates major-party nominee","input":"Who was the first woman who ran for president?"},{"output":"a Parliamentary representative democratic constitutional monarchy","context":"The politics of Norway take place in the framework of a Parliamentary representative democratic constitutional monarchy. Executive power is exercised by the Council of State, the cabinet, led by the Prime Minister of Norway. Legislative power is vested in both the government and the legislature, the Storting, elected within a multi-party system. The judiciary is independent of the executive branch and the legislature.\\r\\n\\r\\nThe Economist Intelligence Unit rated Norway as a \\"full democracy\\" in 2016.[1]\\r\\n\\r\\nThe Norwegian constitution, signed by the Eidsvoll assembly on 17 May 1814, transformed Norway from being an absolute monarchy into a constitutional monarchy. The 1814 constitution granted rights such as freedom of speech (100) and rule of law (ҡ?96, 97, 99). Important amendments include:\\r\\n\\r\\nNorway is a constitutional monarchy, where the King has a mainly symbolic power. The Royal House is a branch of the princely family of Glcksburg, originally from Schleswig-Holstein in Germany. The functions of the King, Harald V, are mainly ceremonial, but he has influence as the symbol of national unity. Although the constitution of 1814 grants important executive powers to the King, these are always exercised by the Council of State in the name of the King (King's Council, or cabinet). The King is also High Protector of the Church of Norway (the state church), Grand Master of The Royal Norwegian Order of St. Olav, and symbolically Supreme Commander of the Norwegian armed forces.\\r\\n\\r\\nThe Council of State is formally convened by the reigning monarch. The Council of State consists of a Prime Minister and his/her council, formally appointed by the King. Parliamentarism has evolved since 1884 and entails that the cabinet must not have the parliament against it, and that the appointment by the King is a formality.  The council must have the confidence of the Norwegian legislative body, known as the Storting. In practice, the monarch will ask the leader of a parliamentary block that has a majority in the Storting to form a government. After elections resulting in no clear majority to any party or coalition, the leader of the party most likely to be able to form a government is appointed Prime Minister. Since World War II, most non-Socialist governments have been coalitions, and Labour Party governments have often relied on the support of other parties to retain the necessary parliamentary votes.\\r\\n\\r\\nThe executive branch is divided into the following Ministries:\\r\\n\\r\\nThe Labour Party has been the largest party in Parliament ever since the election of 1927 up to the recent 2013 election. Labour formed their first brief minority government in 1928 which lasted for 18 days only. After the 1936 election the Labour Party formed a new minority government, which had to go into exile 1940ÿ45 because of the German occupation of Norway. After a brief trans-party government following the German capitulation in 1945, Labour gained a majority of the seats in parliament in the first post-war election of 1945.\\r\\n\\r\\nNorway was ruled by Labour governments from 1945 to 1981, except for three periods (1963, 1965ÿ71, and 1972ÿ73). The Labour Party had a single party majority in the Storting from 1945 to 1961. Since then no party has single-handedly formed a majority government, hence minority and coalition governments have been the rule. After the centre-right Willoch government lost its parliamentary majority in the election of 1985, there were no majority governments in Norway until the second Stoltenberg government was formed after the 2005 election.\\r\\n\\r\\nFrom 1981 to 1997, governments alternated between minority Labour governments and Conservative-led centre-right governments. The centre-right governments gained power in 3 out of 4 elections during this period (1981, 1985, 1989), whereas Labour toppled those governments twice between elections (1986, 1990) and stayed in power after one election (1993). Elections take place in September and governments change in October of election years.\\r\\n\\r\\nConservative leader K?re Willoch formed a minority government after the election of 1981. In 1983, midway between elections, this government was expanded to a majority three-party coalition of the Conservatives, the Centre Party and the Christian Democrats. In the election of 1985 the coalition lost its majority but stayed in office until 1986, when it stepped down after losing a parliamentary vote on petrol taxes.\\r\\n\\r\\nLabour leader Gro Harlem Brundtland served three periods as Prime Minister. First briefly from February 1981 until the election the same year, then from May 1986 to the election of 1989, and last from November 1990 until October 1996 when she decided to step out of domestic politics. Brundtland strongly influenced Norwegian politics and society during this period and was nicknamed the \\"national mother\\".\\r\\n\\r\\nAfter the election of 1989 a centre-right coalition was formed with the same three parties as in 1983ÿ1986, this time headed by Conservative leader Jan P. Syse. This coalition governed from 1989 to November 1990 when it collapsed from inside over the issue of Norwegian membership in the European Economic Area.\\r\\n\\r\\nWhen Brundtland resigned in 1996, Labour leader Thorbj?rn Jagland formed a new Labour government that stayed in office until October 1997 when he, after the September 1997 election, declared that his government would step down because the Labour Party failed to win at least 36.9% of the national vote ÿ the percentage Labour had won in the 1993 election.\\r\\n\\r\\nA three-party minority coalition of the Centre, Christian Democratic, and Liberal parties, headed by Christian Democrat Prime Minister Kjell Magne Bondevik, moved into office in October 1997. That government fell in March 2000 over the issue of proposed natural gas plants, opposed by Bondevik due to their impact on climate change.\\r\\n\\r\\nThe Labour Party's Jens Stoltenberg, a Brundtland protg, took over in a minority Labour government but lost power in the September 2001 election when Labour posted its worst performance since World War I.\\r\\n\\r\\nBondevik once again became Prime Minister in 2001, this time as head of a minority coalition of the Conservatives, Christian Democrats and Liberals, a coalition dependent on support from the Progress Party. This coalition government was the first to stay in office for a complete four-year election period since Per Borten's coalition government of 1965ÿ69.\\r\\n\\r\\nA coalition between the Labour Party, Socialist Left Party, and Centre Party, took over from 17 October 2005[update] after the 2005 general election, where this coalition obtained a majority of 87 out of 169 seats in the Storting. Jens Stoltenberg became Prime Minister and formed a cabinet known as Stoltenberg's Second Cabinet.\\r\\n\\r\\nThis was a historical coalition in several aspects. It was the first time the Socialist Left sat in cabinet, the first time the Labour Party sat in a coalition government since the 1945 four-month post-war trans-party government (otherwise in government alone), and the first time the Centre Party sat in government along with socialist parties (otherwise in coalition with conservative and other centre parties).\\r\\n\\r\\nIn the 2009 general election the coalition parties kept the majority in the Storting by winning 86 out of 169 seats.[2] Stoltenberg's second cabinet thus continued. There have been several reshuffles in the cabinet during its existence.\\r\\n\\r\\nIn the 2013 election, the incumbent redÿgreen coalition government obtained 72 seats and lost its majority. The election ended with a victory for the four opposition non-socialist parties, winning a total of 96 seats out of 169 (85 needed for a majority). Following convention, Stoltenberg's government resigned and handed over power in October 2013. The Labour Party, however, remained the largest party in parliament with 30.8% of the popular vote. The Progress Party also lost ground, but nevertheless participates in the new cabinet. Among the smaller parties, the centrist Liberal Party and Christian Peoples Party hold the balance of power. Both campaigned on a change in government. On 30 September the two smaller parties announced that they would support a minority coalition of the Conservative and Progress parties, but they would not take seats in the cabinet themselves. The new Erna Solberg government was re-elected in 2017.\\r\\n\\r\\nNorway has a unicameral Parliament, the Storting (\\"Great Council\\"), with members elected by popular vote for a four-year term (during which it may not be dissolved) by proportional representation in multi-member constituencies. Voting rights are granted in the year a person turns 18.\\r\\n\\r\\nThe Storting currently has 169 members (increased from 165, effective from the elections of 12 September 2005). The members are elected from the 19 counties for 4-year terms according to a system of proportional representation. Until 2009, the Storting divided itself into two chambers, the Odelsting and the Lagting for the sole purpose of voting on legislation. Laws were proposed by the government through a Member of the Council of State or by a member of the Odelsting and decided on by the Odelsting and Lagting, in case of repeated disagreement by the joint Storting. In practice, the Lagting rarely disagreed and mainly just rubber-stamped the Odelsting's decision. In February 2007, the Storting passed a constitutional amendment to repeal the division, which abolished the Lagting for the 2009 general election, thereby establishing a fully unicameral system.[3]\\r\\n\\r\\nElections are to be held every four years on the second Monday of September.\\r\\n\\r\\n\\r\\n\\r\\nThe Norwegian legal system is a mixture of customary law, civil law system, and common law traditions; the Supreme Court renders advisory opinions to legislature when asked; accepts compulsory ICJ jurisdiction, with reservations.\\r\\n\\r\\nThe regular courts include the Supreme Court (H?yesterett) with 18 permanent judges and a president, courts of appeal (court of second instance in most cases), city and county courts (court of first instance in most cases), and conciliation councils (court of first instance in most civil-code cases). Judges attached to the regular courts are appointed by the King in council after nomination by the Ministry of Justice.\\r\\n\\r\\nThe special High Court of the Realm (Riksrett) hears impeachment cases against members of the Government, Parliament, or Supreme Court. Following an amendment to the Norwegian constitution in February 2007, impeachment cases are heard by the five highest ranking Supreme Court justices and six lay members in one of the Supreme Court courtrooms The High Court of the Realm had generally lost most of its significance after 1884, and this institution has been passive ever since 1927. The new system is meant to restore the Riksrett to its earlier significance.\\r\\n\\r\\nImpeachment may be brought against Members of the Council of State, or of the Supreme Court or of the Storting, for criminal offenses which they may have committed in their official capacity. Indictments are raised by the Storting and judged by five Supreme Court justices and six lay judges.\\r\\n\\r\\nThe mainland of Norway is divided into 18 counties (fylker, singular fylke): Akershus, Aust-Agder, Buskerud, Finnmark, Hedmark, Hordaland, M?re og Romsdal, Nordland, Oppland, Oslo, ?stfold, Rogaland, Sogn og Fjordane, Telemark, Troms, Tr?ndelag (created from two separate counties 1. January 2018), Vest-Agder, and Vestfold. In addition are the island group Svalbard and the island Jan Mayen.\\r\\n\\r\\nCounties and municipalities have local autonomy, but this autonomy is circumscribed by national controls. Counties and municipalities are subject to the oversight of a governor (fylkesmann) appointed by the King in the Council of State. One governor exercises authority in both Oslo and the adjacent county of Akershus. Each county has a directly elected county assembly, led by a mayor, which decides upon matters falling within purview of the counties (upper secondary and vocational education, some culture, transport and social services). There is also a governor (sysselmann) on Svalbard, who is under the Ministry of Foreign Affairs and not the Ministry of Local Government and Regional Development as the other counties.\\r\\n\\r\\nThe counties are divided into 428 municipalities (kommuner, singular kommune). The municipalities are led by directly elected assemblies, which elect a board of aldermen and a mayor. Some municipalities, most notably Oslo, have a parliamentary system of government, where the city council elects a city government that is responsible for executive functions. Some municipalities are also divided into municipal districts or city districts (again, Oslo is one of these) responsible for certain welfare and culture services. These districts are also headed by political assemblies, in some cases elected directly by the citizens. The municipalities deal with a wide range of planning issues and welfare services, and are mostly free to engage in activities which are not explicitly restricted by law. Lately, the functions of the counties and municipalities have been the subject of debates, and changes may take place in the near future.\\r\\n\\r\\nNorway has three dependent areas, all in or near Antarctica: Bouvet Island in the South Atlantic Ocean, Queen Maud Land in Antarctica, and Peter I Island off West Antarctica. The Norwegian Act of 27 February 1930 declares these areas are subject to Norwegian sovereignty as dependencies.\\r\\n\\r\\nAn attempt to annex East Greenland ended in defeat at the Hague Tribunal in 1933.\\r\\n\\r\\nAfDB, AsDB, Australia Group, BIS, CBSS, CE, CERN, EAPC, EBRD, ECE, EFTA, ESA, FAO, IADB, IAEA, IBRD, ICAO, ICCt, ICC, ICFTU, ICRM, IDA, IEA, IFAD, IFC, International IDEA, IFRCS, IHO, ILO, IMF, International Maritime Organization, Inmarsat, Intelsat, Interpol, IOC, IOM, ISO, ITU, MINURSO, NAM (guest), NATO, NC, NEA, NIB, NSG, OECD, OPCW, OSCE, PCA, UN, UNCTAD, UNESCO, UNHCR, UNIDO, UNMIBH, UNMIK, UNMOP, UNTSO, UPU, WCO, WEU (associate), WHO, WIPO, WMO, WTO, Zangger Committee.","input":"What kind of political system does norway have?"},{"output":"selective breeding in a controlled environment","context":"Cannabis strains are either pure or hybrid varieties of the Cannabis genus of plants, that encompasses the species C. sativa, C. indica and C. ruderalis.\\r\\nVarieties are developed to intensify specific characteristics of the plant, or to differentiate the strain for the purposes of marketing or to make it more effective as a drug. Variety names are typically chosen by their growers, and often reflect properties of the plant such as taste, color, smell, or the origin of the variety.[citation needed] Cannabis strains commonly refer to those varieties with recreational and medicinal use. These varieties have been cultivated to contain a high percentage of cannabinoids. Several varieties of Cannabis, known as hemp, have a very low cannabinoid content, and are instead grown for their fiber and seed.\\r\\nCannabis indica\\r\\nCannabis ruderalis\\r\\nCannabis sativa\\r\\nRelative size of cannabis types\\r\\nThe two species of the Cannabis genus that are most commonly grown are Cannabis indica and Cannabis sativa.[1] A third species, Cannabis ruderalis is very short and produces only trace amounts of tetrahydrocannabinol (THC), and thus is not commonly grown for industrial, recreational or medicinal use. However, because Cannabis ruderalis flowers independently of the photoperiod and according to age, it has been used to breed autoflowering strains.[2]\\r\\nPure sativas are relatively tall (reaching as high as 4.5 meters), with long internodes and branches, and large, narrow-bladed leaves. Pure indica varieties are shorter and bushier, have wider leaflets. They are often favored by indoor growers for their size. Sativas bloom later than indicas, often taking a month or two longer to mature. The subjective effects of sativas and indicas are said to differ, but the ratio of tetrahydrocannabinol (THC) to cannabidiol (CBD) in most named drug varieties of both types is similar (averaging about 200:1). Unlike most commercially developed strains, indica landraces exhibit plants with varying THC/CBD ratios.[3] Avidekel, a medical marijuana strain developed in Israel, has a very low content of THC but a high content of CBD, limiting its recreational value but maximizing medical effect.[4]\\r\\nThere is an increasing discussion whether the existing paradigm of the difference between species adequately represents the variability found within the Cannabis genus.[5][6][7] There are five chemotaxonomic types of Cannabis: one with high levels of THC, one which is more fibrous and has higher levels of CBD, one that is an intermediate between the two, another one with high levels of cannabigerol (CBG), and the last one almost without cannabinoids.[8]\\r\\n\\r\\nThere has also been a recent movement[citation needed] to characterize strains based on their reported subjective effects. For example, WoahStork has used machine learning algorithms to classify strains into six Distinct Activity groups.[9]\\r\\n\\r\\nIn addition to pure indica, sativa, and ruderalis varieties, hybrid varieties with varying ratios of these three types are common. For example, the White Widow hybrid containing about 60% indica and 40% sativa ancestry. These hybrid varieties exhibit traits from both parental types. There are also commercial crossbred hybrids which contain a mix of both ruderalis, indica and/or sativa genes, and are usually autoflowering varieties. These varieties are bred mostly for the medicinal cannabis market, since they are not very appreciated by recreational cannabis users because ruderalis varieties are lower in THC and impart a slightly unpleasant taste. \\"Lowryder\\" was an early auto-flowering hybrid that retained the flowering behavior of ruderalis plants, while also producing appreciable amounts of THC and CBD. Autoflowering cannabis varieties have the advantage of being discreet due to their small stature. They also require shorter growing periods, as well as having the additional advantage that they do not rely on a change in the photoperiod to determine when to flower.\\r\\nBreeding requires pollinating a female cannabis plant with male pollen. Although this occurs spontaneously and ubiquitously in nature, the intentional creation of new varieties typically involves selective breeding in a controlled environment.\\r\\nWhen cannabis is cultivated for its psychoactive or medicinal properties, male plants will often be separated from females. This prevents the fertilization of the female plants, either to facilitate sinsemilla flowering or to provide more control over which male is chosen. Pollen produced by the male is caught and stored until it is needed.\\r\\nWhen a male plant of one strain pollinates a female of another strain, the seeds will be F1 hybrids of the male and female. These offspring will not be identical to their parents. Instead, they will have characteristics of both parents. Repeated breeding results in certain characteristics appearing with greater regularity.\\r\\nIt is impossible for a hermaphrodite to create any male only plants. A hermaphrodite may create female only seeds and hermaphrodite seeds. Also the female only seeds may carry the hermaphrodite trait.[10]\\r\\nA common technique to stabilize a cannabis variety is called \\"cubing\\".[citation needed] A breeder seeking specific traits in the hybrid offspring (for example, greater resin production or tighter node spacing) will breed hybrid plants most exemplifying these characteristics with a parent plant. The same traits are sought in the new inbred offspring, which are then again bred with the original parent plant. This process is called cubing because it usually repeated across three, or possibly more generations before the variety's genetics are acceptably stable.\\r\\nIn a retail market that is decriminalised such as in The Netherlands, (wholesale production is illegal but prosecutions are not always enforced because of the contradiction of the law that is recognised by the courts[11]), competition puts pressure on breeders to create increasingly attractive varieties to maintain market share. Breeders give their strains distinct and memorable names in order to help differentiate them from their competitors' strains, although they may in fact be very similar.\\r\\nPopular strains are incorporated into new hybrids, which often bear a similar name to their parent. This phenomenon has occurred with Haze and Sour varieties, amongst others.\\r\\nAcapulco Gold is a golden-leafed Cannabis sativa strain originally from the Acapulco area of southwest Mexico.[12][13][14]\\r\\nBedrocan is a medicinal cannabis variety cultivated from a Dutch medical marijuana Cannabis sativa L. strain, having a standardized content of THC (22%) and CBD (1%). It is currently cultivated by Bedrocan Nederland, Bedrocan Canada and Bedrocan ?esk Republika. It was first introduced in 2003 and is dispensed through pharmacies after prescription from a physician.[15]\\r\\nBlue Dream is a hybrid cannabis strain widely used for both medical and recreational purposes first developed in 2003.[16]\\r\\nCharlotte's Web is a high-cannabidiol (CBD), low-tetrahydrocannabinol (THC) Cannabis extract marketed as a dietary supplement under federal law of the United States.[17][18][19] It is produced by the Stanley brothers in Colorado. It does not induce the psychoactive \\"high\\" typically associated with recreational marijuana strains that are high in THC.[20] In September 2014, the Stanleys announced that they would ensure that the product consistently contained less than 0.3% THC.[21] Charlotte's Web gained national attention when it was used to treat Charlotte Figi's epileptic seizures.[22][23] Her story has led to her being described as \\"the girl who is changing medical marijuana laws across America,\\"[24] as well as the \\"most famous example of medicinal hemp use\\".[25] There is little evidence about the safety or efficacy of cannabinoids in the treatment of epilepsy.[26][27]\\r\\nDurban Poison is a pure Cannabis sativa strain that originates from the South African port city of Durban.\\r\\nPurple Kush is a 100% Indica strain of Cannabis. This plant, \\"forms a short squat bush with very dense internodes and large fan leaves, staying in the 60 to 90?cm (2 to 3?ft) height range while grown indoors. Purple Kush's foliage exhibits a classic indica growth pattern: a sturdy bush with dark green hues and sometimes hints of purple toward ripeness.\\"[28][unreliable source?]\\r\\nTom Cruise Purple is a strain of cannabis sold in California by select licensed cannabis clubs. The strain is potent, and is packaged with a picture of the actor Tom Cruise laughing. Tom Cruise Purple is sold by cannabis purveyors in Northern California.[29] Cruise sought out legal advice regarding the product, and considered a lawsuit against its manufacturers.[30][31][32][32][33][34][35][36]\\r\\nSkunk refers to Cannabis strains that are strong-smelling and have been likened to the smell of the spray from a skunk. These strains of cannabis are believed to have originated in the United States prior to development by Dutch growers.[37] Just as with other strains of cannabis, skunk is commonly grown in controlled indoor environments under specialized grow lights, or in a greenhouse when full outdoor conditions are not suitable; skunk strains are hybrids of Cannabis sativa and Cannabis indica.[37]\\r\\nSour Diesel is a Cannabis sativa dominant hybrid strain.\\r\\nThai sometimes called Thai stick is a pure sativa form of cannabis, brought from Thailand.","input":"How do you get different strains of weed?"},{"output":"Olympics 2016 bronze medallist Sakshi Malik","context":"\\r\\n\\r\\nBeti Bachao, Beti Padhao (translation: Save girl child, educate a girl child) is a personal campaign of the Government of India that aims to generate awareness and improve the efficiency of welfare services intended for girls. The scheme was launched with an initial funding of ?100 crore (US$14?million).[1] It mainly targets the clusters in Uttar Pradesh, Haryana, Uttarakhand, Punjab, Bihar and Delhi.[2][3]\\r\\n\\r\\nAccording to census data, the child gender ratio (0ÿ6 years) in India was 927 girls per 1,000 boys in 2001, which dropped to 918[1] girls for every 1,000 boys in 2011. A 2012 UNICEF report ranked India 41st among 195 countries.[citation needed] In the Population Census of 2011 it was revealed that the population ratio of India 2011 is 943 females per 1000 of males. The Sex Ratio 2011 shows an upward trend from the census 2001 data.\\r\\n\\r\\nSpeaking on the occasion of [International Day of the Girl Child] in 2015, the Prime Minister, Narendra Modi had called for the eradication of female foeticide and invited suggestions from the citizens of India via the MyGov.in portal.[4]\\r\\n\\r\\nThe Beti Bachao, Beti Padhao (BBBP) [1] scheme was launched on 22 January 2015 by Modi.[5][6] It aims to address the issue of the declining child sex ratio image (CSR) and is a national initiative jointly run by the Ministry of Women and Child Development, the Ministry of Health and Family Welfare and the Ministry of Human Resource Development. It initially focused multi-sector action in 100 districts throughout the country where there was a low CSR.\\r\\n\\r\\nOn 26 August 2016, Olympics 2016 bronze medallist Sakshi Malik was made brand ambassador for BBBP.[7]\\r\\n\\r\\nThe hashtag #SelfieWithDaughter was promoted on social media in June 2015, which started when Sunil Jaglan the sarpanch of the village Bibipur, Jind in Haryana took a selfie with his daughter Nandini  and posted on Facebook on 9 June 2015.[8] The hashtag garnered worldwide fame.[9]\\r\\n\\r\\nSex-selective abortion or female foeticide has led to a sharp drop in the ratio of girls born in contrast to boy infants in some states in India. Ultrasound technology has made it possible for pregnant women and their families to learn the gender of a foetus early in a pregnancy. Discrimination against girl infants, for several reasons, has combined with the technology to result in a rise in abortions of foetuses identified as female during ultrasonic testing.\\r\\n\\r\\nThe trend was first noticed when results of the 1991 national census were released, and it was confirmed to be a worsening problem when results of the 2001 national census were released. The reduction in the female population of certain Indian states continues to worsen, as results of the 2011 national census have shown. It has been observed that the trend is most pronounced in relatively prosperous regions of India.[10] The dowry system in India is often blamed; the expectation that a large dowry must be provided for daughters in order for them to marry is frequently cited as a major cause for the problem.[11]  Pressure for parents to provide large dowries for their daughters is most intense in prosperous states where high standards of living, and modern consumerism, are more prevalent in Indian society.\\r\\n\\r\\nRates of female foeticide in Madhya Pradesh are increasing; the rate of live births was 932 girls per 1000 boys in 2001, which dropped to 918 by 2011. It is expected that if this trend continues, by 2021 the number of girls will drop below 900 per 1000 boys.[12]\\r\\n\\r\\nStrategies employed to successfully carry out the scheme are:\\r\\n\\r\\nThe Bharatiya Janata Party has formed a National Executive Committee to promote Beti Bachao Beti Padhao (BBBP) across the country. The committee is organising a number of programs to promote \\"Save Girl Child\\" and \\"to Educate Girl Child\\" since January 2015.  Dr. Rajendra Phadke is the National Convener of BBBP Abhiyan.[citation needed]\\r\\n\\r\\nThe Beti Bachao campaign is also supported by the Indian Medical Association.[13]","input":"Who is the brand ambassador of beti bachao?"},{"output":"Botany Bay","context":"","input":"Where did the english first settle in australia?"},{"output":"February 26, 1986","context":"Dragon Ball (Japanese: ´ʾλ, Hepburn: Doragon Bru) is a Japanese media franchise created by Akira Toriyama. The initial manga, written and illustrated by Toriyama, was serialized in Weekly Shnen Jump from 1984 to 1995, with the 519 individual chapters collected into 42 tankbon volumes by its publisher Shueisha. Dragon Ball was initially inspired by the classical Chinese novel Journey to the West. The series follows the adventures of the protagonist, Son Goku, from his childhood through adulthood as he trains in martial arts and explores the world in search of the seven orbs known as the Dragon Balls, which summon a wish-granting dragon when gathered. Along his journey, Goku makes several friends and battles a wide variety of villains, many of whom also seek the Dragon Balls.\\r\\nThe Dragon Ball manga has been adapted into two anime series produced by Toei Animation: Dragon Ball and Dragon Ball Z, which together were broadcast in Japan from 1986 to 1996. Additionally, the studio has developed 19 animated feature films and three television specials, as well as two anime sequel series titled Dragon Ball GT (1996ÿ1997) and Dragon Ball Super (2015ÿpresent). From 2009 to 2015, a revised, faster-paced version of Dragon Ball Z aired in Japan under the title Dragon Ball Kai, in which most of the original version's footage not featured in the manga is removed. Several companies have developed various types of merchandising based on the series leading to a large media franchise that includes films, both animated and live-action, collectible trading card games, numerous action figures, along with several collections of soundtracks and a large number of video games. As of January 2012, the franchise generated $5?billion in merchandise,[1] making Dragon Ball one of the most merchandisable anime based media franchises of all time.\\r\\nSince its release, Dragon Ball has become one of the most successful manga and anime series of all time. The manga's 42 volumes have sold over 156 million copies in Japan and more than 240 million copies worldwide, making it the second best-selling manga series in history. Reviewers have praised the art, characterization, and humor of the story. It is widely regarded as one of the greatest manga series ever made, with many manga artists citing Dragon Ball as a source of inspiration for their own now popular works. The anime, particularly Dragon Ball Z, is also highly popular in various countries and was arguably one of the most influential in boosting the popularity of Japanese animation in Western culture.\\r\\n\\r\\n\\r\\nOn a version of Earth in Universe 7, a monkey-tailed boy named Goku befriends a teenage girl named Bulma, whom he accompanies to find the seven Dragon Balls (´ʾλ, Doragon Bru), which summon the dragon Shenlong to grant the user one wish. The journey leads them to the desert bandit Yamcha, who later becomes an ally; Chi-Chi, whom Goku unknowingly agrees to marry; and Emperor Pilaf, an impish man who seeks the Dragon Balls to fulfill his desire to rule the world. Goku then undergoes rigorous training regimes under the martial arts master Kame-Sen'nin in order to fight in the Tenkaichi Budkai (BH, \\"Strongest Under the Heavens Martial Arts Tournament\\"). A monk named Kuririn becomes his training partner and rival, but they soon become best friends. After the tournament, Goku searches for the Dragon Ball his grandfather left him and almost single-handedly defeats the Red Ribbon Army and their hired assassin Taopaipai. Thereafter Goku reunites with his friends to defeat the fortuneteller Baba Uranai's fighters and have her locate the last Dragon Ball to revive a friend killed by Taopaipai.\\r\\nAt the Tenkaichi Budkai three years later Goku and his allies oppose Kame-Sen'nin's rival and Taopaipai's brother, Tsuru-Sen'nin, and his students Tenshinhan and Chaozu. Kuririn is killed after the tournament and Goku tracks down and is defeated by his killer, Piccolo Daimao. The samurai Yajirobe takes Goku to the hermit Karin, where he receives healing and a power boost. Meanwhile, Piccolo fights Kame-Sen'nin and Chaozu, leading to both their deaths, and uses the Dragon Balls to regain his youth before destroying Shenlong. Goku then kills Piccolo Daimao, who, just before dying, spawns his son/reincarnation Piccolo. Karin then directs Goku to Kami-sama, the original creator of the Dragon Balls, to restore Shenlong and revive his slain friends. Goku trains under Kami for the next three years, once again reuniting with his friends at the Tenkaichi Budkai, where he narrowly wins against Piccolo before leaving with Chi-Chi to keep his promise to marry her.\\r\\nFive years later, Goku is a young adult and father of Gohan, when Raditz arrives on Earth, identifies Goku as his younger brother 'Kakarrot' and reveals to him that they are members of a nearly extinct extraterrestrial race called the Saiyans (??, Saiya-jin), who sent Goku to conquer Earth for them, until he suffered a severe head injury and lost all memory of his mission. Goku refuses to continue the mission, sides with Piccolo, and sacrifices his life to defeat Raditz. In the afterlife Goku trains under the North Kai until he is revived by the Dragon Balls to save the Earth from the invading Nappa and Vegeta. In the battle Yamcha, Chaozu, Tenshinhan, and Piccolo are killed, and the Dragon Balls cease to exist. Kuririn and the galactic tyrant Freeza learn of another set of Dragon Balls on planet Namek (ρGx, Namekku-sei), whereupon Bulma, Gohan, and Kuririn search for them to revive their friends and subsequently the Earth's Dragon Balls, leading to several battles with Freeza's minions and Vegeta, the latter standing alongside the heroes to fight the Ginyu Force, a team of mercenaries. The long battle with Freeza himself comes to a close when Goku transforms into a Super Saiyan (]??, Sp Saiya-jin) of legends and defeats him, with Namek being destroyed in the process.\\r\\nSometime after, Frieza is revealed to have survived after the fight in Namek and have traveled to Earth with his father King Cold, to face Goku. Instead, he is encountered by a mysterious person who came from the future who is none other than Vegeta and Bulma's son, Trunks. Trunks had defeated both Frieza and King Cold, also having the ability to turn to a Super Saiayn. Trunks gave the Z warriors the medicine for the virus that would be fatal to Goku and a warning to train for an upcoming threat that will appear in the near future. Three years later, a group of Androids (?kg, Jinzningen, \\"Artificial Humans\\") created by a member of the former Red Ribbon Army, Doctor Gero, seeking revenge against Goku. During this time, an evil life form called Cell also emerges and, after absorbing two of the Androids to achieve his \\"perfect form,\\" holds his own fighting tournament to challenge the protagonists. After Goku sacrifices his own life to no avail, Gohan avenges his father by defeating Cell.\\r\\nSeven years later, Goku, briefly revived for one day, and his allies are drawn into a fight against Majin Buu. After numerous battles, including destruction and re-creation of the Earth, Goku destroys Boo with a Genki-Dama (a sphere of pure energy drawn from all intelligent beings on Earth) and wishes for him to be reincarnated as a \\"good person.\\" Ten years later, at another Tenkaichi Budkai, Goku meets Boo's human reincarnation, Oob. Leaving their match unfinished, Goku departs with Oob to train him to be Earth's new guardian.\\r\\nSix months after the defeat of Majin Buu, Earth was at peace one again. Goku became a radish farmer, Krillen a father, Gohan married to Videl, and Vegeta spent more time with his family. After thirtyÿnine years of slumber, Lord Beerus, the God of Destruction of the universe, had awoken to a prophecy of a warrior emerging that would rival his power being known as a Super Saiyan God (] ?ހҾ? Sp Saiya-jin Goddo). Beerus along with mentor/servant Whis took course to find Goku and Vegeta to fulfill the prophecy. Goku along with the rest of the sayians Including Gohan and Videl's soon to be born daughter, united to have Goku achieve the Super Saiyan God form through a ritual revealed by Shenron. Goku with his new power faced the God of Destruction with Earth's destruction at stake. Although Goku defeated, Beerus spares Earth and looks forward to Goku being more powerful.\\r\\nA year has past since the encounter with Beerus. Goku and Vegeta had been training under Whis who had taken them as his students to master on controlling their flow of energy, maintaining it from within themselves. During this time, the remaining remnant of Frieza's army had went to earth to use the dragon balls to revive the long dead galactic ruler. Being revived and regenerated from the army's latest technology of regeneration, Frieza had began to plot his revenge on Goku who had defeated him on planet Namek. Learning that he had defeated Majin Buu, one of two beings warned not to fight with from his father; the other being Beerus, Frieza decided to begin intense training to reach new heights of power to face Goku. Frieza's army had arrived on Earth while Goku and Vegeta still training with Whis. Tien, Piccolo, Gohan, Krillin, and Master Roshi had fought the remnant until their return. Once arriving on Earth, Frieza and Goku had fought showing their latest forms and power. Goku achieved a form beyond the Super Saiyan God form being the Super Saiyan God Super Saiyan (shortened Super Sayian Blue). Frieza revealed his new form achieved after training stating he has not done before, calling it his Golden Frieza form.\\r\\nFreiza became a handful with Goku but was bested yet again. However, with his commander Sorbet's help, defeated Goku with a severe shot when he was no longer focused. Vegeta then came in to fight Frieza being able to transform to the form as well. Vegeta outmatched Frieza, showing the flaw of his new form where his body has not been used to the energy that gets consumed. Out of desperation, Frieza had destroyed Earth, killing him and almost everyone, later with Whis reversing time, Goku was able to prevent Frieza from destroying the Earth after being too over confident against him in the first place and destroys him once more.\\r\\nAfter Frieza's return, the God of Destruction by the name of Champa and his attendant, Vados appears who are Beerus and Whis' relatives. Also being the God of Destruction and Angel of Universe 6. Champa arrived to Beerus' home to see whose universe was superior. Eventually, it was settled in a tournament using the Super Dragon Balls, planet-sized versions of the titular artifacts, to grant a wish as the main prize for the two universes to swap their Earths. The universes' fighters were selected by the Gods of Destruction. During the tournament, many surprises and turn of events occurred. Each universe having formidable warriors in their respected universes. Following the elimination of Frost, Vegeta singlehandedly eliminated most of Universe 6's fighters until being defeated by the assassin Hit. The tournament reached its peak with the match up of Goku vs Hit. Each fighter raising hew heights in power with Hit being the winner by forfeit following Goku's failed attempt to lift the tournament rules to fight Hit at his full power.\\r\\nAkira Toriyama loosely modeled Dragon Ball on the classic Chinese novel Journey to the West;[2][3] but also redeveloped it from his 1983 one-shot manga Dragon Boy.[3] He has said that the fighting was influenced from movies by famous martial arts actor Jackie Chan,[4][5] as he wanted to create a story with the basic theme of Journey to the West, but with \\"a little kung fu.\\"[6] Since it was serialized in a shnen magazine, he added the idea of the Dragon Balls to give it a game-like activity of gathering something, without thinking of what the characters would wish for.[6] With Goku being Sun Wukong, Bulma as Xuanzang, Oolong as Zhu Bajie and Yamcha being Sha Wujing, he originally thought it would last about a year or end once the Dragon Balls were collected.[5][7] Toriyama stated that although the stories are purposefully easy to understand, he specifically aimed Dragon Ball at readers older than those of his previous serial Dr. Slump.[8] He also wanted to break from the Western influences common in Dr. Slump, deliberately going for Chinese scenery, referencing Chinese buildings and photographs of China his wife had bought.[9] The island where the Tenkaichi Budkai is held is modeled after Bali, which he, his wife and assistant visited in mid-1985, and for the area around Bobbidi's spaceship he consulted photos of Africa.[9]\\r\\nIt was when the Tenkaichi Budkai martial arts tournament began that Dragon Ball truly became popular, having recalled the races and tournaments in Dr. Slump.[5] Anticipating that readers would expect Goku to win the tournaments, Toriyama had him lose the first two while planning an eventual victory. He said that Muscle Tower in the Red Ribbon Army storyline was inspired by the video game Spartan X, in which enemies tended to appear very fast. He then created Piccolo Daimao as a truly evil villain, and as a result called that arc the most interesting to draw.[5] Once Goku and company had become the strongest on Earth, they turned to extraterrestrial opponents including the Saiyans. Freeza, who forcibly took over planets to resell them, was created around the time of the Japanese economic bubble and was inspired by real estate speculators, whom Toriyama called the \\"worst kind of people.\\"[5] Finding the escalating enemies difficult, he created the Ginyu Force to add more balance to the series. He added time travel next, but said he had a hard time with it, only thinking of what to do that week and having to discuss it with his second editor Yu Kondo.[5] After Cell's death, Toriyama intended for Gohan to replace Goku as the series' protagonist, but felt the character was not suited for the role and changed his mind.[5]\\r\\nGoing against the normal convention that the strongest characters should be the largest in terms of physical size, he designed many of Dragon Ball's most powerful characters with small statures, including the protagonist, Goku.[10] Toriyama later explained that he had Goku grow up as a means to make drawing fight scenes easier, even though his first editor Kazuhiko Torishima was initially against it because it was rare to have the main character of a manga series change drastically.[11] When including fights in the manga, Toriyama had the characters go to uninhabited locations to avoid difficulties in drawing residents and destroyed buildings.[9] Toriyama said that he did not plan the details of the story, resulting in strange occurrences and discrepancies later in the series, including changing the colors of the characters mid-story and few characters having screen tone because he found it difficult to use.[4][6][7][12] Since the completion of Dragon Ball, Toriyama has continued to add to its story, mostly background information on its universe, through guidebooks published by Shueisha.\\r\\nDuring the second half of the series, Toriyama has said that he had become more interested in coming up with the story than actually drawing it, and that the battles became more intense with him simplifying the lines.[4] In 2013, he stated that because Dragon Ball is an action manga the most important aspect is the sense of speed, so he did not draw very elaborate, going so far as to suggest one could say that he was not interested in the art.[11] He also once said that his goal for the series was to tell an \\"unconventional and contradictory\\" story.[10] In 2013, commenting on Dragon Ball's global success, Toriyama said, \\"Frankly, I don't quite understand why it happened. While the manga was being serialized, the only thing I wanted as I kept drawing was to make Japanese boys happy.\\", \\"The role of my manga is to be a work of entertainment through and through. I dare say I don't care even if [my works] have left nothing behind, as long as they have entertained their readers.\\"[13]\\r\\nWritten and illustrated by Akira Toriyama, Dragon Ball was serialized in the manga anthology Weekly Shnen Jump from December 3, 1984 to June 5, 1995,[14][15] when Toriyama grew exhausted and felt he needed a break from drawing. The 519 individual chapters were published into 42 tankbon volumes by Shueisha from September 10, 1985 through August 4, 1995.[16][17][18] Between December 4, 2002 and April 2, 2004, the chapters were re-released in a collection of 34 kanzenban volumes, which included a slightly rewritten ending, new covers, and color artwork from its Weekly Shnen Jump run.[19][20] The February 2013 issue of V Jump, which was released in December 2012, announced that parts of the manga will be fully colored and re-released in 2013.[21] Twenty volumes, beginning from chapter 195 and grouped by story arcs, were released between February 4, 2013 and July 4, 2014.[22][23] Twelve volumes covering the first 194 chapters were published between January 4 and March 4, 2016.[24][25] A sshhen edition that aims to recreate the manga as it was originally serialized in Weekly Shnen Jump with color pages, promotional text, and next chapter previews, was published in eighteen volumes between May 13, 2016 and January 13, 2017.[26][27]\\r\\nAnother manga penned by ishi, the three-chapter Dragon Ball: Episode of Bardock that revolves around Bardock, Goku's father, was published in the monthly magazine V Jump from August and October 2011.[28]\\r\\nThe final chapter of Toriyama's 2013 manga series Jaco the Galactic Patrolman revealed that it is set before Dragon Ball, with several characters making appearances.[29] Jaco's collected volumes contain a bonus Dragon Ball chapter depicting Goku's mother.[30]\\r\\nIn December 2016, a spin-off manga titled Dragon Ball Side Story: The Case of Being Reincarnated as Yamcha began in Shueisha's Shnen Jump+ digital magazine. Written and illustrated by Dragon Garow Lee, it is about a high school boy who after an accident wakes up in the body of Yamcha in the Dragon Ball manga.[31]\\r\\nToriyama also created a short series, Neko Majin (1999ÿ2005), that became a self-parody of Dragon Ball.[32] In 2006, a crossover between Kochira Katsushika-ku Kameari Ken-mae Hashutsujo (or Kochikame) and Dragon Ball by Toriyama and Kochikame author Osamu Akimoto appeared in the Super Kochikame (]?w, Ch Kochikame) manga.[33] That same year, Toriyama teamed up with Eiichiro Oda to create a crossover chapter of Dragon Ball and One Piece titled Cross Epoch.[34]\\r\\nToei Animation produced an anime television series based on the first 194 manga chapters, also titled Dragon Ball. The series premiered in Japan on Fuji Television on February 26, 1986 and ran until April 12, 1989, lasting 153 episodes.[3]\\r\\nInstead of continuing the anime as Dragon Ball, Toei Animation decided to carry on with their adaptation under a new name and asked Akira Toriyama to come up with the title. Dragon Ball Z (´ʾλZ(δ), Doragon Bru Zetto, commonly abbreviated as DBZ) picks up five years after the first series left off and adapts the final 325 chapters of the manga. It premiered in Japan on Fuji Television on April 26, 1989, taking over its predecessor's time slot, and ran for 291 episodes until its conclusion on January 31, 1996.[3]\\r\\nDragon Ball GT (´ʾλGT(Κū), Doragon Bru Jؐ Tؐ, G(rand) T(ouring)[35]) premiered on Fuji TV on February 2, 1996 and ran until November 19, 1997 for 64 episodes.[3] Unlike the first two anime series, it is not based on Akira Toriyama's original Dragon Ball manga,[36] being created by Toei Animation as a sequel to the series or as Toriyama called it, a \\"grand side story of the original Dragon Ball.\\"[35] Toriyama designed the main cast, the spaceship used in the show, the design of three planets, and came up with the title and logo. In addition to this, Toriyama also oversaw production of the series, just as he had for the Dragon Ball and Dragon Ball Z anime.\\r\\nIn February 2009, Dragon Ball Z celebrated its 20th anniversary, with Toei Animation announcing that it would broadcast a re-edited and remastered version under the name Dragon Ball Kai (´ʾλ, Doragon Bru Kai, lit. \\"Dragon Ball Revised\\"). The footage would be re-edited to follow the manga more closely, eliminating scenes and episodes which were not featured in the original manga, resulting in a more faithful adaptation, as well as in a more faster-moving, and focused story.[37] The episodes remastered for HDTV, with rerecording of the vocal tracks by most of the original cast, and featuring updated opening and ending sequences. On April 5, 2009, the series premiered in Japan airing in Fuji TV.[38][39] Dragon Ball Z Kai reduced the episode count to 159 episodes (167 episodes internationally), from the original footage of 291 episodes. Damaged frames were removed, resulting in some minor shots being remade from scratch in order to fix cropping, and others to address continuity issues.[40] The majority of the international versions, including Funimation Entertainment's English dub, are titled Dragon Ball Z Kai.[41][42]\\r\\nOn April 28, 2015, Toei Animation announced Dragon Ball Super (´ʾλ], Doragon Bru Sp), the first all-new Dragon Ball television series to be released in 18 years. It debuted on July 5 and will run as a weekly series at 9:00 am on Fuji TV on Sundays.[43] Masako Nozawa reprised her roles as Goku, Gohan, and Goten. Most of the original cast reprise their roles as well.[44][45] Kouichi Yamadera and Masakazu Morita also reprise their roles, as Beerus and Whis, respectively.[45]\\r\\nThe story of the anime is set four years after the defeat of Majin Buu, when the Earth has become peaceful once again. Akira Toriyama is credited as the original creator, as well for \\"original story & character design concepts.\\"[46] It is also being adapted into a parallel manga.[47]\\r\\nNineteen animated theatrical films based on the Dragon Ball series have been released in Japan. The first three films are based on the original Dragon Ball anime series. The remaining films include fifteen based on Dragon Ball Z and one tenth anniversary special (also based on the first anime series). The first five films were shown at the Toei Manga Festival (?|ϻԻ, Tei Manga Matsuri), while the sixth through seventeenth films were shown at the Toei Anime Fair (?|ʧ߁Gʧ, Toei Anime Fea). They are mostly alternate re-tellings of certain story arcs involving new characters or extra side-stories that do not correlate with the same continuity as the series. Since these movies were originally shown as back-to-back presentations alongside other Toei film productions, they were usually below feature length (around 45ÿ60 minutes each), making them only slightly longer than an episode of the TV series (the sole exception being 1996's The Path to Power, which has a running time of 80 minutes). The newest films, Dragon Ball Z: Battle of Gods (2013) and Dragon Ball Z: Resurrection 'F' (2015), were produced as full-length feature films and were given stand-alone theatrical releases in Japan (as well as limited theatrical releases in the U.S.); these being the first movies to have original creator Akira Toriyama deeply involved in their production.[48][49] As such, they are considered to have a greater sense of continuity with the series than the older films, especially since they have been adapted into the first two-story arcs of the new Dragon Ball Super anime.\\r\\nAn American live-action film titled Dragonball Evolution was produced by 20th Century Fox, after it acquired the feature film rights to the Dragon Ball franchise in March 2002, previous to the film, two unofficial live-action films have been produced decades prior.[50][51] The film was directed by James Wong and produced by Stephen Chow, it was released in the United States on April 10, 2009.[51][52] The film was meant to lead into sequels,[53][54] which were cancelled, after the film released and became universally heralded as one of the worst adaptations of all time, being considered by the fans as being unfaithful to the source material.[55] Franchise creator Akira Toriyama also criticized the film adding he was completely left out of the creative process, despite having himself offered to help, going as far as saying: \\"the result was a movie, I couldn't even call Dragon Ball\\".[56] Years after its release, the writer of the film, Ben Ramsey, released a public apology in which he admitted to have written the film \\"chasing for a payday\\" instead of \\"as a fan of the franchise\\".[57][58]\\r\\nThree television specials based on the series were aired on Fuji TV in Japan. The first, The One True Final Battle ~The Z Warrior Who Challenged Freeza -- Son Goku's Father~, renamed Bardock ÿ The Father of Goku by Funimation, was shown on October 17, 1990. The second special, The Hopeless Resistance!! Gohan and Trunks -- The Two Remaining Super Warriors, renamed The History of Trunks by Funimation, is based on a special chapter of the original manga and aired on March 24, 1993. Goku Side Story! The Four Star Ball is a Badge of Courage, renamed A Hero's Legacy by Funimation, aired on March 26, 1997. A two-part hour-long crossover special between Dragon Ball Z, One Piece and Toriko, referred to as Dream 9 Toriko & One Piece & Dragon Ball Z Super Collaboration Special!! aired on April 7, 2013.[59]\\r\\nThe short film Dragon Ball: Yo! Son Goku and His Friends Return!! was created for the Jump Super Anime Tour,[60] which celebrated Weekly Shnen Jump's 40th anniversary, and debuted on November 24, 2008. A short animated adaptation of Naho ishi's Bardock spinoff manga, Dragon Ball: Episode of Bardock, was shown on December 17ÿ18, 2011 at the Jump Festa 2012 event.[61]\\r\\nA two-episode original video animation (OVA) titled Dragon Ball Z Side Story: Plan to Eradicate the Saiyans was created in 1993 as strategy guides for the Famicom video game of the same name.[62] A remake titled Dragon Ball: Plan to Eradicate the Super Saiyans was created as a bonus feature for the PlayStation 3 and Xbox 360 video game Dragon Ball: Raging Blast 2, which was released on November 11, 2010.[63]\\r\\n\\"Dragon Ball Z: The Real 4D\\" debuted at Universal Studios Japan in the summer of 2016. It features a battle between Goku and Freeza. Unlike most Dragon Ball animation, the attraction is animated with CGI. A second attraction titled \\"Dragon Ball Z: Super Tenkaichi Budokai\\" debuted at Universal Studios Japan in the summer of 2017.\\r\\nThe Dragon Ball franchise has spawned multiple video games across various genres and platforms. Earlier games of the series included a system of card battling and were released for the Famicom following the storyline of the series.[64] Starting with the Super Famicom and Mega Drive, most of the games were from the fighting genre or RPG (Role Playing Game), such as the Super Butoden series.[65] The first Dragon Ball game to be released in the United States was Dragon Ball GT: Final Bout for the PlayStation in 1997.[66] For the PlayStation 2 and PlayStation Portable games the characters were redone in 3D cel-shaded graphics. These games included the Dragon Ball Z: Budokai series and the Dragon Ball Z: Budokai Tenkaichi series.[67][68] Dragon Ball Z: Burst Limit was the first game of the franchise developed for the PlayStation 3 and Xbox 360.[69] Dragon Ball Xenoverse was the first game of the franchise developed for the PlayStation 4 and Xbox One.[70][71] A massively multiplayer online role-playing game called Dragon Ball Online was available in Korea, Hong Kong and Taiwan until the servers were shut down in 2013.[72] A few years later fans started recreating the game. Today, \\"Dragon Ball Online Global\\" is a new, European version of Dragon Ball Online and it is being developed, while open beta server is running.[73]\\r\\nMyriad soundtracks were released in the anime, movies and the games. The music for the first two anime Dragon Ball and Z and its films was composed by Shunsuke Kikuchi, while the music from GT was composed by Akihito Tokunaga and the music from Kai was composed by Kenji Yamamoto and Norihito Sumitomo. For the first anime, the soundtracks released were Dragon Ball: Music Collection in 1985 and Dragon Ball: Complete Song Collection in 1991, although they were reissued in 2007 and 2003, respectively.[74] For the second anime, the soundtrack series released were Dragon Ball Z Hit Song Collection Series. It was produced and released by Columbia Records of Japan from July 21, 1989 to March 20, 1996 the show's entire lifespan. On September 20, 2006 Columbia re-released the Hit Song Collection on their Animex 1300 series.[75][76] Other CDs released are compilations, video games and films soundtracks as well as music from the English versions.[77]\\r\\nThere have been numerous companion books to the Dragon Ball franchise. Chief among these are the Daizenshuu (??) series, comprising seven hardback main volumes and three supplemental softcover volumes, covering the manga and the first two anime series and their theatrical films. The first of these, Dragon Ball: The Complete Illustrations (Daizenshuu volume 1), first published in Japan in 1995, is the only one that was released in English, being printed in 2008 by Viz Media.[78] It contains all 264 colored illustrations Akira Toriyama drew for the Weekly Shnen Jump magazines' covers, bonus giveaways and specials, and all the covers for the 42 tankbon. It also includes an interview with Toriyama on his work process. The remainder have never been released in English, and all are now out of print in Japan. From February 4 to May 9, 2013, condensed versions of the Daizenshuu with some updated information were released as the four-volume Chzensh (]??) series.[21] For Dragon Ball GT, the Dragon Ball GT Perfect Files were released in May and December 1997 by Shueisha's Jump Comics Selection imprint. They include series information, illustration galleries, behind-the-scenes information, and more. They were out of print for many years, but were re-released in April 2006 (accompanying the Japanese DVD release of Dragon Ball GT) and this edition is still in print.[79][80]\\r\\nCoinciding with the 34-volume kanzenban re-release of the manga, and the release of the entire series on DVD for the first time in Japan, four new guidebooks were released in 2003 and 2004. Dragon Ball Landmark and Dragon Ball Forever cover the manga, using volume numbers for story points that reference the kanzenban release,[81][82] while Dragon Ball: Tenkaichi Densetsu (´ʾλ B?wh) and Dragon Ball Z: Son Goku Densetsu (´ʾλZ ?OiZ?wh) cover the Dragon Ball and Dragon Ball Z anime, respectively.[83][84] Much of the material in these books is reused from the earlier Daizenshuu volumes, but they include new textual material including substantial interviews with the creator, cast and production staff of the series. Son Goku Densetsu in particular showcases previously-unpublished design sketches of Goku's father Bardock, drawn by character designer Katsuyoshi Nakatsuru prior to creator Akira Toriyama's revisions that resulted in the final version.\\r\\nFollowing the release of Dragon Ball Kai in Japan, four new guidebooks were released: the two-volume Dragon Ball: Super Exciting Guide (´ʾλ ]ƽ̚ū벼) in 2009, covering the manga,[85][86] and two-volume Dragon Ball: Extreme Battle Collection (´ʾλx?Oj) in 2010, covering the anime series.[87][88] Despite the TV series airing during this time being Kai, the Extreme Battle Collection books reference the earlier Z series in content and episode numbers. These books also include new question-and-answer sessions with Akira Toriyama, revealing a few new details about the world and characters of the series. 2010 also saw the release of a new artbook, Dragon Ball: Anime Illustrations Guide - The Golden Warrior (´ʾλ ʧ߁G̷µĦ ߼?ؿV?); a sort of anime-counterpart to the manga-oriented Complete Illustrations, it showcases anime-original illustrations and includes interviews with the three principal character designers for the anime. Each of the Japanese \\"Dragon Box\\" DVD releases of the series and movies, which were released from 2003 to 2006, as well as the Blu-ray boxed sets of Dragon Ball Kai, released 2009 to 2011, come with a Dragon Book guide that contains details about the content therein. Each also contains a new interview with a member of the cast or staff of the series. These books have been reproduced textually for Funimation's release of the Dragon Ball Z Dragon Box sets from 2009 to 2011.\\r\\nCollectible cards based on the Dragon Ball, Dragon Ball Z, and Dragon Ball GT series have been released by Bandai. These cards feature various scenes from the manga and anime stills, plus exclusive artwork from all three series. Bandai released the first set in the United States in July 2008.[89]\\r\\nDragon Ball is one of the most popular manga series of all time, and it continues to enjoy high readership today. By 2000, more than 126 million copies of its tankbon volumes had been sold in Japan alone.[92] By 2016, this number had grown to pass 156 million in Japan and 240 million worldwide, making it the second best-selling Weekly Shnen Jump manga of all time.[93][94] Dragon Ball is credited as one of the main reasons for the period when manga circulation was at its highest in the mid-1980s and mid-1990s.[95][96] For the 10th anniversary of the Japan Media Arts Festival in 2006, Japanese fans voted Dragon Ball the third greatest manga of all time.[97]\\r\\nIn a survey conducted by Oricon in 2007 among 1,000 people, Son Goku, the main character of the franchise, ranked first place as the \\"Strongest Manga Character of All Time.\\"[98] Goku's journey and his ever-growing strength resulted in the character winning \\"the admiration of young boys everywhere\\".[2] Manga artists, such as One Piece creator Eiichiro Oda and Naruto creator Masashi Kishimoto, have stated that Goku inspired their series' main protagonists as well as series structure.[99][100]\\r\\nManga critic Jason Thompson stated in 2011 that \\"Dragon Ball is by far the most influential shonen manga of the last 30 years, and today, almost every Shonen Jump artist lists it as one of their favorites and lifts from it in various ways.\\"[101] He says the series \\"turns from a gag/adventure manga to an nearly-pure fighting manga\\",[101] and its basic formula of \\"lots of martial arts, lots of training sequences, a few jokes\\" became the model for other shnen series, such as Naruto.[102] Thompson also called Toriyama's art influential and cited it as a reason for the series' popularity.[101] James S. Yadao, author of The Rough Guide to Manga, claims that the first several chapters of Dragon Ball \\"play out much like Saiyuki with Dr. Slump-like humour built in\\" and that Dr. Slump, Toriyama's previous manga, has a clear early influence on the series.[103] He feels the series \\"established its unique identity\\" after the first occasion when Goku's group disbands and he trains under Kame-sen'nin, when the story develops \\"a far more action-packed, sinister tone\\" with \\"wilder\\" battles with aerial and spiritual elements and an increased death count, while humor still makes an occasional appearance.[103] Yadao claims that an art shift occurs when the characters \\"lose the rounded, innocent look that he established in Dr. Slump and gain sharper angles that leap off the page with their energy and intensity.\\"[104]\\r\\nAnimerica felt the series had \\"worldwide appeal\\", using dramatic pacing and over-the-top martial arts action to \\"maintain tension levels and keep a crippler crossface hold on the audience's attention spans\\".[105] In Little Boy: The Art of Japan's Exploding Subculture, Takashi Murakami commented that Dragon Ball's \\"never-ending cyclical narrative moves forward plausibly, seamlessly, and with great finesse.\\"[92] Ridwan Khan from Animefringe.com commented that the manga had a \\"chubby\\" art style, but as the series continued the characters got more refined, leaner, and more muscular. Khan prefers the manga over the slow pacing of the anime counterparts.[106] Allen Divers of Anime News Network praised the story and humor of the manga as being very good at conveying all of the characters' personalities. Divers also called Viz's translation one of the best of all the English editions of the series due to its faithfulness to the original Japanese.[107] D. Aviva Rothschild of Rationalmagic.com remarked the first manga volume as \\"a superior humor title\\". They praised Goku's innocence and Bulma's insistence as one of the funniest parts of the series.[108]\\r\\nThe content of the manga has been controversial in United States. In November 1999, Toys \\"R\\" Us removed Viz's Dragon Ball from their stores nationwide when a Dallas parent complained the series had \\"borderline soft porn\\" after he bought them for his four-year-old son.[109] Commenting on the issue, Susan J. Napier explained it as a difference in culture.[109] After the ban, Viz reluctantly began to censor the series to keep wide distribution.[110] However, in 2001, after releasing three volumes censored, Viz announced Dragon Ball would be uncensored and reprinted due to fan reactions.[110] In October 2009, Wicomico County Public Schools in Maryland banned the Dragon Ball manga from their school district because it \\"depicts nudity, sexual contact between children and sexual innuendo among adults and children.\\"[109]\\r\\nThe anime adaptations have also been very well-received and are better known in the Western world than the manga, with Anime News Network saying, \\"Few anime series have mainstreamed it the way Dragon Ball Z has. To a certain generation of television consumers its characters are as well known as any in the animated realm, and for many it was the first step into the wilderness of anime fandom.\\"[111] In 2000, satellite TV channel Animax together with Brutus, a men's lifestyle magazine, and Tsutaya, Japan's largest video rental chain, conducted a poll among 200,000 fans on the top anime series, with Dragon Ball coming in fourth.[112] TV Asahi conducted two polls in 2005 on the Top 100 Anime, Dragon Ball came in second in the nationwide survey conducted with multiple age-groups and in third in the online poll.[113][114] On several occasions the Dragon Ball anime has topped Japan's DVD sales.[115][116]\\r\\nCarl Kimlinger of Anime News Network summed up Dragon Ball as \\"an action-packed tale told with rare humor and something even rarera genuine sense of adventure.\\"[117] Both Kimlinger and colleague Theron Martin noted Funimation's reputation for drastic alterations of the script, but praised the dub.[117][118] However, some critics and most fans of the Japanese version have been more critical with Funimation's English dub and script of Dragon Ball Z over the years. Jeffrey Harris IGN criticized the voices including how Freeza's appearance combined with the feminine English voice left fans confused about Freeza's gender.[119] Carlos Ross of T.H.E.M. Anime Reviews considered the series' characters to be different from stereotypical stock characters and noted that they undergo much more development.[120] Despite praising Dragon Ball Z for its cast of characters, they criticized it for having long and repetitive fights.[121]\\r\\nDragon Ball Z is well-known, and often criticized, for its long, repetitive, dragged-out fights that span several episodes, with Martin commenting \\"DBZ practically turned drawing out fights into an art form.\\"[122] However, Jason Thompson of io9 explained that this comes from the fact that the anime was being created alongside the manga.[123] Dragon Ball Z was listed as the 78th best animated show in IGN's Top 100 Animated Series,[124] and was also listed as the 50th greatest cartoon in Wizard magazine's Top 100 Greatest Cartoons list.[125]\\r\\nHarris commented that Dragon Ball GT \\"is downright repellent\\", mentioning that the material and characters had lost their novelty and fun. He also criticized the GT character designs of Trunks and Vegeta as being goofy.[119] Zac Bertschy of Anime News Network also gave negative comments about GT, mentioning that the fights from the series were \\"a very simple childish exercise\\" and that many other anime were superior. The plot of Dragon Ball GT has also been criticized for giving a formula that was already used in its predecessors.[126]\\r\\nThe first episode of Dragon Ball Z Kai earned a viewer ratings percentage of 11.3, ahead of One Piece and behind Crayon Shin-chan.[127] Although following episodes had lower ratings, Kai was among the top 10 anime in viewer ratings every week in Japan for most of its run.[128][129]\\r\\nAn unofficial live-action Mandarin Chinese film adaptation of the series, Dragon Ball: The Magic Begins, was released in Taiwan in 1989.[3] In December 1990, the unofficial live-action Korean film Dragon Ball: Ssawora Son Goku, Igyeora Son Goku was released.","input":"When did the first episode of dragonball air?"},{"output":"pulpit","context":"Pulpit is a raised stand for preachers in a Christian church. The origin of the word is the Latin pulpitum (platform or staging).[1] The traditional pulpit is raised well above the surrounding floor for audibility and visibility, accessed by steps, with sides coming to about waist height. From the late medieval period onwards, pulpits have often had a canopy known as the sounding board or abat-voix above and sometimes also behind the speaker, normally in wood. Though sometimes highly decorated, this is not purely decorative, but can have a useful acoustic effect in projecting the preacher's voice to the congregation below. Most pulpits have one or more book-stands for the preacher to rest his or her bible, notes or texts upon.\\r\\nThe pulpit is generally reserved for clergy. This is mandated in the regulations of the Roman Catholic church, and several others (though not always strictly observed). Even in Welsh Nonconformism, this was felt appropriate, and in some chapels a second pulpit was built opposite the main one for lay exhortations, testimonials and other speeches.[2] Many churches have a second, smaller stand called the lectern, which can be used by lay persons, and is often used for all the readings and ordinary announcements. The traditional Catholic location of the pulpit to the side of the chancel or nave has been generally retained by episcopalian and some other Protestant denominations, while in Presbyterian and Evangelical churches the pulpit has often replaced the altar at the centre.\\r\\nEquivalent platforms for speakers are the bema (bima, bimah) of Ancient Greece and Jewish synagogues, and the minbar of Islamic mosques. From the pulpit is often used synecdochically for something which is said with official church authority.\\r\\n\\r\\n\\r\\nIn many Reformed and Evangelical Protestant denominations, the pulpit is at the centre of the front of the church (and any altar or communion table off to one side), while in the Catholic, Lutheran and Anglican traditions the pulpit is placed to one side and the altar or communion table is in the centre. In many Christian churches, there are two speakers' stands at the front of the church. Often, the one on the left (as viewed by the congregation) is called the pulpit. Since the Gospel lesson is often read from the pulpit, the pulpit side of the church is sometimes called the gospel side.\\r\\nIn both Catholic and Protestant churches the pulpit may be located closer to the main congregation in the nave, either on the nave side of the crossing, or at the side of the nave some way down. This is especially the case in large churches, to ensure the preacher can be heard by all the congregation. Fixed seating for the congregation came relatively late in the history of church architecture, so the preacher being behind some of the congregation was less of an issue than later. Fixed seating facing forward in the nave and modern electric amplification has tended to reduce the use of pulpits in the middle of the nave. Outdoor pulpits, usually attached to the exterior of the church, or at a preaching cross, are also found in several denominations.[2] If attached to the outside wall of a church, these may be entered from a doorway in the wall, or by steps outside.\\r\\nThe other speaker's stand, usually on the right (as viewed by the congregation), is known as the lectern. The word lectern comes from the Latin word \\"lectus\\" past participle of legere, meaning \\"to read\\", because the lectern primarily functions as a reading stand. It is typically used by lay people to read the scripture lessons (except for the Gospel lesson), to lead the congregation in prayer, and to make announcements. Because the epistle lesson is usually read from the lectern, the lectern side of the church is sometimes called the epistle side. In other churches, the lectern, from which the Epistle is read, is located to the congregation's left and the pulpit, from which the sermon is delivered, is located on the right (the Gospel being read from either the centre of the chancel or in front of the altar).\\r\\nThough unusual, movable pulpits with wheels were also found in English churches. They were either wheeled into place for each service where they would be used or, as at the hospital church in Shrewsbury, rotated to different positions in the church quarterly in the year, to allow all parts of the congregation a chance to have the best sound.[3] A portable outside pulpit of wood and canvas was used by John Wesley, and a 19th century Anglican vicar devised a folding iron pulpit for using outdoors.[3]\\r\\nThe Ancient Greek bema (?ϫ) means both 'platform' and 'step', and was used for a variety of secular raised speaking platforms in ancient Greece and Rome, and from those times to today for the central raised platform in Jewish synagogues. Modern synagogue bimahs are often similar in form to centrally-placed pulpits in Evangelical churches.\\r\\nThe use of a bema carried over from Judaism into early Christian church architecture. It was originally a raised platform, often large, with a lectern and seats for the clergy, from which lessons from the Scriptures were read and the sermon was delivered. In Western Christianity the bema developed over time into the sanctuary and chancel (or presbytery).\\r\\nThe next development was the ambo, from a Greek word meaning an elevation. This was originally a raised platform from which the Epistle and Gospel would be read, and was an option to be used as a preacher's platform for homilies, though there were others. Saint John Chrysostom (died 407) is recorded as preaching from the ambo, but this was probably uncommon at this date. In cathedrals early bishops seem often to have preached from their chair in the apse, echoing the position of magistrates in the secular basilicas whose general form most large early churches adopted. Often there were two ambos, one to each side, one used more as a platform on which the choir sang; sometimes the gospel was read, chanted or sung from one side and the epistle from the other. The location of the ambo within the church varied, with about the same range of places as modern pulpits. In ancient Syrian churches it was often placed in the centre of the nave (on both axes). Gradually the ambo came to resemble the modern pulpit in both form and function, though early examples in large churches are often large enough to accommodate several people.[4] The steps up to the pulpit almost invariably approach it from the side or behind, and are often curved. The typical design of the Islamic minbar, where a straight flight of steps leads to the front of the pulpit, is very different.\\r\\nThe Ambon of Henry II, an Imperial gift of 1014 to Aachen Cathedral, was originally installed centrally, but later moved to the side. It is richly decorated with sheets of gold, ivory, and gems, probably emulating Justinian's lost pulpit of Hagia Sophia in Constantinople, of which a description by Paul the Silentiary survives.[5] In churches where there is only one speaker's stand at the front of the church, it serves the functions of both lectern and pulpit and may be called the ambo, which is still the official Catholic term for the place the gospel is read from.\\r\\nIn Roman Catholic churches, the stand used for readings and homilies is formally called the ambo. Despite its name, this structure usually more closely resembles a lectern than the ambon of the Eastern Catholic churches. The readings are typically read from an ambo in the sanctuary, and depending on the arrangement of the church, the homily may be delivered from a raised pulpit where there is one.[6] The General Instruction of the Roman Missal (GIRM) specifies:\\r\\n309. The dignity of the word of God requires that in the church there be a suitable place from which it may be proclaimed and toward which the attention of the faithful naturally turns during the Liturgy of the Word. It is appropriate that generally this place be a stationary ambo and not simply a movable lectern. The ambo must be located in keeping with the design of each church in such a way that the ordained ministers and readers may be clearly seen and heard by the faithful. From the ambo only the readings, the Responsorial Psalm, and the Easter Proclamation (Exsultet) are to be proclaimed; likewise it may be used for giving the Homily and for announcing the intentions of the Universal Prayer. The dignity of the ambo requires that only a minister of the word should stand at it. ...[7]\\r\\nIn some Protestant churches, the pulpit is considered the most important piece of furniture in the sanctuary. It is located centrally in relation to the congregation and raised. In such churches it may be where the minister stands for most of the service. In the eighteenth century, two-decker and triple-decker pulpits were often introduced in English-speaking countries. The three levels of lecterns were intended to show the relative importance of the readings delivered there. The bottom tier was for community announcements, the middle was for the gospel, and the top tier was reserved for the delivery of the sermon. A good example of a three-decker pulpit is found in St Andrew's Church, Slaidburn, Lancashire. America's only surviving three-decker pulpit on the centerline of the church is at Trinity Church, Newport, Rhode Island.\\r\\nIn many Evangelical Christian churches, the pulpit stands squarely in the centre of the platform, and is generally the largest piece of church furniture. This is to symbolize the proclamation of the Word of God as the central focus of the weekly service of worship. In more contemporary evangelical churches, the pulpit may be much smaller, if used at all, and is generally carried out after the end of the song service. However, it usually is placed in the centre of the platform as well.\\r\\nTraditional Presbyterian Churches in Scotland and elsewhere often had a central pulpit, that is, the pulpit was located in the centre of the chancel in the position where most churches have the communion table or altar. The table could be situated in front of the pulpit or to the side, and sometimes was not in the chancel area at all.\\r\\nThis declares the Bible to be the foundation of the faith. Furthermore, the \\"Centrality of the Word\\" implies that the reading and preaching of the Bible is the centrepiece of a service of worship, and thus takes priority over the sacraments. The central pulpit is intended to give visual representation of this idea.[8][9]\\r\\nSince the late 19th century, the fashion in the Church of Scotland and most other Presbyterian denominations has been for a return to the pre-Reformation layout. Thus many buildings which once had a central pulpit now have a pulpit to the side. See for example Skene Parish Church or Old West Church, Boston, Massachusetts.\\r\\nThis Presbyterian tradition is historically distinct from the tradition of the ambon in Eastern Christianity.\\r\\nIn modern Eastern Christianity the area directly in front of the Beautiful Gates of the iconostasis from which the Gospel is typically read is called the ambon, and the entire low elevation above the level of the nave in front of the iconostasis is called the soleas. In larger churches, the ambo might be distinguished by three curved steps by which one may reach it from the nave.[10] In addition many Orthodox churches, especially Greek-speaking churches, have pulpits for preaching from, which are similar to those in Western Christianity.\\r\\nIn Eastern Orthodox Church cathedrals there is usually a low platform in the center of the nave called the episcopal ambo where the bishop is vested prior to the Divine Liturgy and where he is enthroned until the Little Entrance. If the bishop is serving in a simple parish church, an episcopal ambo is set temporarily in place.\\r\\nIn addition to the ambo, many major churches in Greece and Cyprus also have a raised pulpit on the left side of the nave, usually attached to a column and raised several feet high. This is reached by a narrow flight of stairs. It is considered an architectural element that is symmetrical to the bishop's throne, which is located in an equivalent position on the right. Pulpit and throne are usually similar in construction, usually made of either sculpted stone or sculpted wood. This pulpit was used mostly for sermons and in order to improve audibility, before the advent of modern public address systems in churches. Nowadays it is used rarely. Tradition dictates that it be used for the reading of the \\"12 Passion Gospels\\" during the Matins of Holy Friday, served late in the evening of Maundy Thursday. This is done to signify that the Passion of Christ is being \\"broadcast\\" for all to know. In the same spirit, a phonetic transcription of the relevant Gospel passages is provided in several common languages (e.g. English, French, Russian, Arabic etc.), so that they may be read from this pulpit at the same time.\\r\\nThe exterior of a wood or stone pulpit may be decorated, especially with carved reliefs, and in the centuries after the Protestant Reformation these were sometimes, especially in Lutheran churches, one of the few areas of the church left with figurative decoration such as scenes from the Life of Christ. Pulpit reliefs were especially important at the start of the Italian Renaissance, including those from the Pisa Baptistry (1260) and Siena Cathedral Pulpit (1265ÿ68) by Nicola Pisano, the Pulpit of Sant' Andrea, Pistoia by Giovanni Pisano (1301), and those by Donatello\\r\\nElements of decoration shared between Catholic and Protestant denominations are the flowers that may be placed in front of the pulpit, and the antependium or \\"pulpit fall\\", a piece of cloth that covers the top of the book-stand in the pulpit and hangs down a short way at the front. It is often of a rich material and decorated with Christian symbols. Flags and banners used by church-related organizations may also stand on the floor around the pulpit.\\r\\nIn the Reformed tradition, though avoiding figurative art, pulpits were increasingly important as a focus for the church, with the sanctuary now comparatively bare and de-emphasized, and were often larger and more elaborately decorated than in medieval churches.[11]\\r\\nThe bookstand of the pulpit (usually in medieval churches) or lectern (common in Anglican churches) may be formed in the shape of an eagle. The eagle symbolizes the gospels, and shows where these were read from at the time the eagle was placed there. When pulpits like those by the Pisani with eagles in stone on them were built the gospel reading was done from the pulpit.\\r\\nThe spread of the sounding board offered artists decorating Catholic Baroque churches a space for spectacular features of various types on top of it. An artistic conceit largely confined to the 18th century Rococo churches of South Germany was to shape the body of the pulpit as a ship, to utilize the old metaphor of the church as a ship. This allowed for fantastical plaster or wood decoration of sails and rigging manned by angels above, and apostles hauling in nets below.\\r\\nExternal gothic pulpit in Saint-L?, France\\r\\nOutdoor pulpit of Giovanni da Capistrano, Vienna Austria\\r\\nSt James's Church, Piccadilly, in the centre of London, 1680s\\r\\nOpen-air pulpit in the forecourt of the Chapel at Scotch College, Melbourne\\r\\nFr. Coughlin's outdoor pulpit at the Shrine of the Little Flower in Royal Oak, Michigan.\\r\\nA modern pulpit on the chancel of a Presbyterian Church in California\\r\\nA modern pulpit in Jakobskirken, Roskilde, Denmark.\\r\\nA pulpit in the chancel of a Methodist church in Ohio\\r\\nAmbon of Henry II (1014), Aachen Cathedral\\r\\nBaroque pulpit of 1613 carved in wood by Paolo Froni Parma Cathedral\\r\\nItalian pulpit of 1150 or older\\r\\nGothic wood, France\\r\\nLate Baroque polychromed wood in a South German pilgrimage church\\r\\nPulpit in Irsee Abbey, Bavaria in the shape of a ship's prow\\r\\nA late 18th century pulpit in a small Roman Catholic church in Spielfeld, Styria, Austria.\\r\\nEn?nger old church in Sweden\\r\\nboat-shaped German Rococo pulpit\\r\\nTerracotta Pieve delle Sante Flora e Lucilla in Santa Fiora, Italy\\r\\nStone pulpit at Chiesa Bartolomeo in Pantano Pistoia Italy\\r\\nPulpit at St. John the Baptist Cathedral, Yaroslavl, Russia (17th century)\\r\\nStone pulpit at Worcester cathedral England\\r\\nWooden pulpit at the Church of the Holy Ghost in Tallinn in Estonia\\r\\nMany of the most elaborate Catholic pulpits are from Baroque Belgium\\r\\nBaroque Church of St. Anne in Krak܇w, Poland\\r\\nA Calvinist 17th century pulpit of the Calvinist Dutch Reformed church in Buren, the Netherlands.\\r\\nGallus chapel in Greifensee ZH, Switzerland\\r\\nBaroque pulpit in the Amiens Cathedral, France\\r\\nSaint-Thibaut Church, Thann, France\\r\\nStrasbourg Cathedral, France\\r\\nOld Ship Church, Hingham, Massachusetts\\r\\nStone with wooden top in the Collgiale Saint-Florent, Niederhaslach, France\\r\\nGothic-revival \\"wine glass\\" pulpit and sounding board from 1872 in St. Matthew's German Evangelical Lutheran Church, Charleston, South Carolina\\r\\nNeo-Byzantine in the Catholic Westminster Cathedral","input":"What is the podium in a catholic church called?"},{"output":"5 October 1962","context":"\\"Love Me Do\\" is the debut single by the English rock band the Beatles, backed by \\"P.S. I Love You\\". When the single was originally released in the United Kingdom on 5 October 1962, it peaked at No. 17; in 1982 it was re-promoted (not re-issued, retaining the same catalogue number) and reached No. 4. In the United States the single was a No. 1 hit in 1964. In 2013, recordings of the song that were released in 1962 entered the public domain in Europe.[4]\\r\\nThe song was written several years before it was recorded, and prior to the existence of the Beatles. The single features John Lennon's prominent harmonica playing and duet vocals by him and Paul McCartney. Three different recorded versions of the song by the Beatles have been released, each with a different drummer.\\r\\n\\r\\n\\r\\n\\"Love Me Do\\" was primarily written by Paul McCartney in 1958ÿ1959 while truant from school at age 16 and later credited to LennonÿMcCartney; [5] John Lennon contributed the middle eight.[5][6][7] Lennon: \\"Paul wrote the main structure of this when he was 16, or even earlier. I think I had something to do with the middle?... 'Love Me Do' is Paul's song. He wrote it when he was a teenager. Let me think. I might have helped on the middle eight, but I couldn't swear to it. I do know he had the song around, in Hamburg, even, way, way before we were songwriters\\". (David Sheff. John Lennon: All We Are Saying).[8] McCartney: \\"'Love Me Do' was completely co-written. It might have been my original idea but some of them really were 50-50s, and I think that one was. It was just Lennon and McCartney sitting down without either of us having a particularly original idea. We loved doing it, it was a very interesting thing to try and learn to do, to become songwriters. I think why we eventually got so strong was we wrote so much through our formative period. 'Love Me Do' was our first hit, which ironically is one of the two songs that we control, because when we first signed to EMI they had a publishing company called Ardmore and Beechwood which took the two songs, 'Love Me Do' and 'P.S. I Love You', and in doing a deal somewhere along the way we were able to get them back\\".[8][9]\\r\\nTheir practice at the time was to scribble songs in a school notebook, dreaming of stardom, always writing \\"Another LennonÿMcCartney Original\\" at the top of the page.[10] 'Love Me Do' is a song based around two simple chords: G7 and C, before moving to D for its middle eight. It begins with Lennon playing a bluesy dry \\"dockside harmonica\\" riff, [11] then features Lennon and McCartney on joint lead vocals, including Everly Brothers-style harmonising during the beseeching \\"please\\" before McCartney sings the unaccompanied vocal line on the song's title phrase. Lennon had previously sung the title sections, but this change in arrangement was made in the studio under the direction of producer George Martin when he realised that the harmonica part encroached on the vocal (Lennon needed to begin playing the harmonica again on the same beat as the \\"do\\" of \\"love me do\\".[12] Although when a similar situation later occurred on the 'Please Please Me' single session, the harmonica was superimposed afterwards using tape-to-tape overdubbing).[13] Described by Ian MacDonald as \\"standing out like a bare brick wall in a suburban sitting-room, 'Love Me Do', [with its] blunt working class northerness, rang the first faint chime of a revolutionary bell\\" compared to the standard tin pan alley productions occupying the charts at the time.[14]\\r\\n'Love Me Do' was recorded by the Beatles on three different occasions with three different drummers at EMI Studios at 3 Abbey Road in London:\\r\\nFirst issues of the single, released on Parlophone in the UK on 5 October 1962, featured the Ringo Starr version, prompting Mark Lewisohn to later write: \\"Clearly, the 11 September version was not regarded as having been a significant improvement after all\\".[17]\\r\\nThe Andy White version of the track was included on The Beatles' debut UK album, Please Please Me, The Beatles' Hits EP, and subsequent album releases on which \\"Love Me Do\\" was included (except as noted below), as well as on the first US single release in April 1964. For the 1976 single re-issue and the 1982 \\"20th Anniversary\\" re-issue, the Andy White version was again used. The Ringo Starr version was included on the albums Rarities (American version) and Past Masters, Volume One. The CD single issued on 2 October 1992 contains both versions.[18] The Pete Best version remained unreleased until 1995, when it was included on the Anthology 1 album.\\r\\nCapitol Records Canada pressed 170 singles which were released on 4 February 1963 with catalog number 72076.[19][20] This pressing was dubbed from the original UK single and featured Ringo on drums.[21]\\r\\n'Love Me Do,' featuring Starr drumming, was also recorded eight times at the BBC and played on the BBC radio programmes Here We Go, Talent Spot, Saturday Club, Side By Side, Pop Go The Beatles and Easy Beat between October 1962 and October 1963. The version of 'Love Me Do' recorded on 10 July 1963 at the BBC and broadcast on the 23 July 1963 Pop Go the Beatles programme can be heard on The Beatles' album Live at the BBC. The Beatles also performed the song live on the 20 February 1963 Parade of the Pops BBC radio broadcast.\\r\\nIn 1969, during the Get Back sessions, The Beatles played the song in a slower, more bluesy form than they had in earlier recordings. This version of 'Love Me Do' is one of many recordings made during these sessions and subsequently appeared on some bootlegs. The song featured no harmonica by Lennon, and McCartney sang the majority of the song in the same vocal style he used for 'Lady Madonna'.\\r\\nOn 4 September 1962, Brian Epstein paid for the Beatlesalong with their new drummer, Ringo Starrto fly down from Liverpool to London.[22] After first checking into their Chelsea hotel, they arrived at EMI Studios early in the afternoon where they set up their equipment in Studio 3 and began rehearsing six songs including: \\"Please Please Me\\", \\"Love Me Do\\" and a song originally composed for Adam Faith by Mitch Murray called \\"How Do You Do It?\\" which George Martin \\"was insisting, in the apparent absence of any stronger original material, would be the group's first single\\".[23][24] Lennon and McCartney had yet to impress Martin with their songwriting ability, and the Beatles had been signed as recording artists on the basis of their charismatic appeal: \\"It wasn't a question of what they could do [as] they hadn't written anything great at that time.\\"[25] \\"But what impressed me most was their personalities. Sparks flew off them when you talked to them.\\"[26] During the course of an evening session that then followed (7:00?pm to 10:00?pm in Studio 2) they recorded \\"How Do You Do It\\" and \\"Love Me Do\\". An attempt at \\"Please Please Me\\" was made, but at this stage it was quite different from its eventual treatment and it was dropped by Martin. This was a disappointment for the group as they had hoped it would be the B-side to \\"Love Me Do\\".[27]\\r\\nThe Beatles were keen to record their own material, something which was almost unheard of at that time, and it is generally accepted that it is to George Martin's credit that they were allowed to float their own ideas. But Martin insisted that unless they could write something as commercial as \\"How Do You Do It?\\" then the Tin Pan Alley practice of having the group record songs by professional songwriters (which was standard procedure then, and is still common today) would be followed.[23] MacDonald points out, however: \\"It's almost certainly true that there was no other producer on either side of the Atlantic then capable of handling the Beatles without damaging themlet alone of cultivating and catering to them with the gracious, open-minded adeptness for which George Martin is universally respected in the British pop industry.\\" Martin rejects however the view that he was the \\"genius\\" behind the group: \\"I was purely an interpreter. The genius was theirs: no doubt about that.\\"[28]\\r\\nIt was on the 4 September session that, according to McCartney, Martin suggested using a harmonica.[6] However, Lennon's harmonica part was present on the Anthology 1 version of the song recorded during the 6 June audition with Pete Best on drums.[29] Also, Martin's own recollection of this is different, saying: \\"I picked up on 'Love Me Do' because of the harmonica sound\\", adding: \\"I loved wailing harmonicait reminded me of the records I used to issue of Sonny Terry and Brownie McGhee. I felt it had a definite appeal.\\"[30]\\r\\nLennon had learned to play a chromatic harmonica that his Uncle George (late husband of his Aunt Mimi) had given to him as a child. But the instrument being used at this time was one stolen by Lennon from a music shop in Arnhem, the Netherlands, in 1960, as the Beatles first journeyed to Hamburg by road.[31][30][32] Lennon would have had this with him at the EMI audition on 6 June as Bruce Channel's \\"Hey Baby\\", with its harmonica intro, and a hit in the UK in March 1962, was one of the thirty three songs the Beatles had prepared (although only four were recorded: \\"Bsame Mucho\\"; \\"Love Me Do\\"; \\"P.S. I Love You\\" and \\"Ask Me Why\\", of which only \\"Bsame Mucho\\" and \\"Love Me Do\\" survive and appear on Anthology 1). Brian Epstein had also booked the American Bruce Channel to top a NEMS Enterprises promotion at New Brighton's Tower Ballroom, in Wallasey on 21 June 1962, just a few weeks after \\"Hey Baby\\" had charted, and placed the Beatles a prestigious second on the bill. Lennon was so impressed that night with Channel's harmonica player, Delbert McClinton,[33] that he later approached him for advice on how to play the instrument.[34] Lennon makes reference also to Frank Ifield's \\"I Remember You\\" and its harmonica intro, a huge number one hit in the UK July 1962, saying: \\"The gimmick was the harmonica. There was a terrible thing called \\"I Remember You\\", and we did those numbers; and we started using it on \\"Love Me Do\\" just for arrangements\\".[35] The harmonica was to become a feature of the Beatles' early hits such as \\"Love Me Do\\", \\"Please Please Me\\" and \\"From Me to You\\" as well as various album tracks. Paul McCartney recalled, \\"John expected to be in jail one day and he'd be the guy who played the harmonica.\\"[10]\\r\\nMartin came very close to issuing \\"How Do You Do It?\\" as the Beatles' first single (it would also re-appear as a contender for their second single)[36] before settling instead on \\"Love Me Do\\", as a mastered version of it was made ready for release and which still exists in EMI's archives.[23] Martin commented later: \\"I looked very hard at 'How Do You Do It?', but in the end I went with 'Love Me Do', it was quite a good record.\\"[23] McCartney would remark: \\"We knew that the peer pressure back in Liverpool would not allow us to do 'How Do You Do It'.\\"[37]\\r\\nMartin then decided that as \\"Love Me Do\\" was going to be the group's debut release it needed to be re-recorded with a different drummer as he was unhappy with the 4 September drum sound.[38] (Abbey Road's Ken Townsend also recalls McCartney being dissatisfied with Starr's timing, due probably to him being under-rehearsed.)[39] Record producers at that time were used to hearing the bass drum \\"lock in\\" with the bass guitar as opposed to the much looser R&B feel that was just beginning to emerge, and so professional show band drummers were often used for recordings. Ron Richards, placed in charge of the 11 September re-recording session in George Martin's absence, booked Andy White whom he had used in the past. Starr was expecting to play, and would have been very disappointed to be dropped for only his second Beatles recording session: Richards remembers \\"He just sat there quietly in the control box next to me. Then I asked him to play maracas on 'P.S. I Love You'. Ringo is lovelyalways easy going\\".[38] Starr recalled: \\"On my first visit in September we just ran through some tracks for George Martin. We even did Please Please Me. I remember that, because while we were recording it I was playing the bass drum with a maraca in one hand and a tambourine in the other. I think it's because of that that George Martin used Andy White, the 'professional', when we went down a week later to record Love Me Do. The guy was previously booked, anyway, because of Pete Best. George didn't want to take any more chances and I was caught in the middle. I was devastated that George Martin had his doubts about me. I came down ready to roll and heard, 'We've got a professional drummer.' He has apologised several times since, has old George, but it was devastatingI hated the bugger for years; I still don't let him off the hook!\\"[40] Paul McCartney: \\"George got his way and Ringo didn't drum on the first single. He only played tambourine. I don't think Ringo ever got over that. He had to go back up to Liverpool and everyone asked, 'How did it go in the Smoke?' We'd say, 'B-side's good,' but Ringo couldn't admit to liking the A-side, not being on it.\\" (From Anthology).[8] \\"Love Me Do\\" was recorded with White playing drums and Starr on tambourine, but whether using a session drummer solved the problem is unclear, as session engineer Norman Smith was to comment: \\"It was a real headache trying to get a [good] drum sound, and when you listen to the record now you can hardly hear the drums at all.\\"[41] Ringo Starr's version was mixed \\"bottom-light\\" to hide Starr's bass drum.[42]\\r\\nEarly pressings of the single (issued with a red Parlophone label) are the 4 September versionminus tambourinewith Starr playing drums. But later pressings of the single (on a black Parlophone label), and the version used for the Please Please Me album, are the 11 September re-record with Andy White on drums and Starr on tambourine. This difference has become fundamental in telling the two recordings of \\"Love Me Do\\" apart. Regarding the editing sessions that then followed all these various takes, Ron Richards remembers the whole thing being a bit fraught, saying: \\"Quite honestly, by the time it came out I was pretty sick of it. I didn't think it would do anything.\\"[43]\\r\\nThere are major discrepancies regarding the White session, and who produced it. In his book Summer of Love, Martin concedes that his version of events differs from some accounts, saying: \\"On the 6 June Beatles session (audition) I decided that Pete Best had to go [and said to Epstein] I don't care what you do with Pete Best; but he's not playing on any more recording sessions: I'm getting a session drummer in.\\"[44] When Starr turned up with the group for their first proper recording session on 4 September, Martin says that he was totally unaware that the Beatles had fired Best; and, not knowing \\"how good bad or indifferent\\" Starr was, was not prepared to \\"waste precious studio time finding out.\\"[44] Martin, therefore, appears to have this as the Andy White session in which Martin was present, and not 11 September. This contradicts Mark Lewisohn's account, as in his book The Complete Beatles Recording Sessions, he has Starr on drums on 4 September[23] and White for the 11 September re-make.[38] Lewisohn also says that Richards was in charge on 11 September, which means, if accurate, that Richards was sole producer of the White version of \\"Love Me Do\\". Martin says, \\"My diary shows that I did not oversee any Beatles recording sessions on 11 Septemberonly the one on 4 September.\\"[44] But, if Lewisohn's account is correct and \\"the 4 September session really hadn't proved good enough to satisfy George Martin\\",[38] it might seem odd that Martin was not then present to oversee the 11 September remake.\\r\\nIn his memoirs, assistant engineer Geoff Emerick supports the Lewisohn version, recounting that Starr played drums at the 4 September session (Emerick's second day at EMI) and that Martin, Smith, and McCartney were all dissatisfied with (the underrehearsed) Starr's timekeeping.[45] Emerick places White firmly at the second session, and describes the reactions of Mal Evans and Starr to the substitution.[46] Emerick also noted that Martin only came in very late for the 11 September session, after work on \\"Love Me Do\\" was complete.[46]\\r\\nAndy White confirms that he was booked by Ron Richards for the 11 September session, not by George Martin, who he says \\"could not make the session, could not get there till the end, so he had Ron Richards handle it\\". White also says that he recognises his own drumming on the released version of \\"Please Please Me\\", recorded that same session with him on drums.[47] White, however was not at the studio for the final recording on 26 November and was only hired for the 11 September session (this run through with White can be heard on Anthology 1).\\r\\nThe song was the fourth of six songs by the Beatles to hit #1 in a one-year period; an all-time record for the US charts. In order, these were \\"I Want to Hold Your Hand\\", \\"She Loves You\\", \\"Can't Buy Me Love\\", \\"Love Me Do\\", \\"A Hard Day's Night\\", and \\"I Feel Fine.\\" It was also the fourth of seven songs written by Lennon-McCartney to hit #1 in 1964 (the remaining song being \\"A World Without Love\\" by Peter and Gordon); That's an all-time record on the US charts for writing the most songs to hit #1 in the same calendar year. (see List of Billboard Hot 100 chart achievements and milestones)\\r\\n*sales figures based on certification alone\\r\\n^shipments figures based on certification alone\\r\\nThe original master tapes of the 4 September version of \\"Love Me Do\\" are not known to exist. Standard procedure at Abbey Road Studios at the time was to erase the original two-track session tape for singles once they had been \\"mixed down\\" to the (usually monaural) master tape used to press records. This was the fate of two Beatles singles (four songs): \\"Love Me Do\\", \\"P.S. I Love You\\", \\"She Loves You\\", and \\"I'll Get You\\". However, at some point the mixdown master tape for this song was also lost, and apparently no backup copies had been made. Thus, for many years the only extant recorded copies were the red label Parlophone 45?rpm vinyl records pressed in 1962. This version was also issued in Canada as Capitol 72076.[59]\\r\\nBy the time the tapes had disappeared, the song's 11 September 1962 remake featuring Andy White had been released. EMI would not have been too concerned about the loss of the 4 September take, therefore, as it was now considered obsolete, and they may not have anticipated ever having any use for it again anyway.\\r\\nAround 1980, a reasonably clean, original 45 single from EMI's archives was used as the \\"best available source\\" for the track's inclusion on the Capitol compilation LP Rarities. A few years later, a new master tape was struck, this time using another, better-sounding 45 supplied by a record collector, and this has served as the official EMI master tape for the original \\"Love Me Do\\" ever since.\\r\\nThe version with Ringo on drums was released in 1980 for Record 1 of The Beatles Box.\\r\\nEMI's planned 2012 release of a 50th anniversary limited-edition replica of the original single was cancelled when it was discovered that the pressings contained the Andy White version instead of the Ringo version as intended.\\r\\nRevised release plans have been announced for a limited-edition replica vinyl version of the Beatles' first single, \\"Love Me Do\\" backed with \\"P.S. I Love You,\\" in honour of the 50th anniversary of its release in the UK. The 7-inch disc originally was scheduled to hit stores on 5 October, the single's actual golden anniversary, but it was reissued (in its corrected version) on 22 October.\\r\\nOn the version released on the British single, Rarities and Past Masters:\\r\\nOn the version released on American single, Please Please Me, The Beatles' Hits, and compilation albums including 1962ÿ1966 (\\"The Red Album\\") and 1:\\r\\nOn the Anthology 1 version:\\r\\nEngineered by Norman Smith.[8][better?source?needed]\\r\\n\\"Love Me Do\\" has been covered by (among others):","input":"When did the beatles release their first song?"},{"output":"flooding","context":"Weather-related fatalities in the United States may be caused by extreme temperatures, such as abnormal heat or cold, flooding, lightning, tornado, hurricane, wind, rip currents, and others. The National Weather Service compiles statistics on weather-related fatalities and publishes reports every year.[1] In 2016, flooding was the number-one cause of weather-related fatalities, but over a 30-year period, on average, extreme heat is the deadliest form of weather.[2]\\r\\n\\r\\nThis table represents a 6-year period and only a select type of recorded weather events.  The data was tabulated by running searches on the specified weather events recorded with at least 1 fatality.  The yearly timeframes were selected to cover January 1 to December 31 of each year for each event type.\\r\\n\\r\\nThis table does not provide a comprehensive total of all weather-related events.  Also, this is not necessarily a weather event severity comparison, but is more of an indicator of frequency of fatal occurrences for some events.  However, the unexpected leader of fatalities comes from the temperature extremes category.  That category includes heat waves as well as cold extremes.\\r\\n\\r\\nBetween 1979 and 2014, the death rate as a direct result of exposure to heat (underlying cause of death) generally hovered around 0.5 to 1 deaths per million people, with spikes in certain years. Overall, a total of more than 9,000 Americans have died from heat-related causes since 1979, according to death certificates.[4]\\r\\n\\r\\nCold weather is deadly too. For example, in the USA, 21 people died in a cold wave in January 2014, which also caused property damage valued at US$2.5 billion[5]","input":"What is the leading cause of weather related deaths?"},{"output":"Battle of Mons","context":"1915\\r\\n1916\\r\\n1917\\r\\n1918\\r\\nAssociated articles\\r\\nThe Battle of Mons was the first major action of the British Expeditionary Force (BEF) in the First World War. It was a subsidiary action of the Battle of the Frontiers, in which the Allies clashed with Germany on the French borders. At Mons, the British Army attempted to hold the line of the MonsÿCond Canal against the advancing German 1st Army. Although the British fought well and inflicted disproportionate casualties on the numerically superior Germans, they were eventually forced to retreat due both to the greater strength of the Germans and the sudden retreat of the French Fifth Army, which exposed the British right flank. Though initially planned as a simple tactical withdrawal and executed in good order, the British retreat from Mons lasted for two weeks and took the BEF to the outskirts of Paris before it counter-attacked in concert with the French, at the Battle of the Marne.\\r\\n\\r\\n\\r\\nBritain declared war on Germany on 4 August 1914 and on 9 August, the BEF began embarking for France.[1] Unlike Continental European armies, the BEF in 1914 was exceedingly small. At the beginning of the war, the German and French armies numbered well over a million men each, divided into eight and five field armies respectively; the BEF had c.?80,000 soldiers in two corps of entirely professional soldiers made up of long-service volunteer soldiers and reservists. The BEF was probably the best trained and most experienced of the European armies of 1914.[2] British training emphasised rapid-fire marksmanship and the average British soldier was able to hit a man-sized target fifteen times a minute, at a range of 300 yards (270?m) with his LeeÿEnfield rifle.[3] This ability to generate a high volume of accurate rifle-fire played an important role in the BEF's battles of 1914.[4]\\r\\nThe Battle of Mons took place as part of the Battle of the Frontiers, in which the advancing German armies clashed with the advancing Allied armies along the Franco-Belgian and Franco-German borders. The BEF was stationed on the left of the Allied line, which stretched from Alsace-Lorraine in the east to Mons and Charleroi in southern Belgium.[5][6] The British position on the French flank meant that it stood in the path of the German 1st Army, the outermost wing of the massive \\"right hook\\" intended by the Schlieffen Plan (a combination of the Aufmarsch I West and Aufmarsch II West deployment plans), to pursue the Allied armies after defeating them on the frontier and force them to abandon northern France and Belgium or risk destruction.[7]\\r\\nThe British reached Mons on 22 August.[8] On that day, the French Fifth Army, located on the right of the BEF, was heavily engaged with the German 2nd and 3rd armies at the Battle of Charleroi. At the request of the Fifth Army commander, General Charles Lanrezac, the BEF commander, Field Marshal Sir John French, agreed to hold the line of the CondÿMonsÿCharleroi Canal for twenty-four hours, to prevent the advancing German 1st Army from threatening the French left flank. The British thus spent the day digging in along the canal.[9]\\r\\nAt the Battle of Mons the BEF had some 80,000 men, comprising the Cavalry Division, an independent cavalry brigade and two corps, each with two infantry divisions.[10] I Corps was commanded by Sir Douglas Haig and was composed of the 1st and 2nd Divisions. II Corps was commanded by Sir Horace Smith-Dorrien and consisted of the 3rd and 5th Divisions.[8] Each division had 18,073 men and 5,592 horses, in three brigades of four battalions. Each division had twenty-four Vickers machine guns ÿ two per battalion ÿ and three field artillery brigades with fifty-four 18-pounder guns, one field howitzer brigade of eighteen 4.5-inch howitzers and a heavy artillery battery of four 60-pounder guns.[11]\\r\\nThe II Corps, on the left of the British line, occupied defensive positions along the MonsÿCond Canal, while I Corps was positioned almost at a right angle away from the canal, along the MonsÿBeaumont road (see map).[12] I Corps was deployed in this manner to protect the right flank of the BEF, in case the French were forced to retreat from their position at Charleroi.[8] I Corps did not line the canal, which meant that it was little involved the battle and the German attack was faced mostly by II Corps.[13] The dominant geographical feature of the battlefield, was a loop in the canal, jutting outwards from Mons towards the village of Nimy. This loop formed a small salient which was difficult to defend and formed the focus of the battle.[14]\\r\\nThe first contact between the two armies occurred on 21 August, when a British bicycle reconnaissance team encountered a German unit near Obourg; and Private John Parr became the first British soldier to be killed in the war.[15] The first substantial action occurred on the morning of 22 August. At 6:30 a.m., the 4th Dragoon Guards laid an ambush for a patrol of German lancers outside the village of Casteau, to the north-east of Mons. When the Germans spotted the trap and fell back, a troop of the dragoons, led by Captain Hornby gave chase, followed by the rest of his squadron, all with drawn sabres. The retreating Germans led the British to a larger force of lancers, whom they promptly charged and Captain Hornby became the first British soldier to kill an enemy in the Great War, fighting on horseback with sword against lance. After a further pursuit of a few miles, the Germans turned and fired upon the British cavalry, at which point the dragoons dismounted and opened fire. Drummer Edward Thomas is reputed to have fired the first shot of the war for the British Army, hitting a German trooper.[16][a]\\r\\nAdvancing towards the British was the German 1st Army, commanded by Alexander von Kluck.[18] The 1st Army was composed of four active corps (II, III, IV, and IX Corps) and three reserve corps (III, IV and IX Reserve corps), although only the active corps took part in the fighting at Mons. German corps had two divisions each, with attendant cavalry and artillery.[18] The 1st Army had the greatest offensive power of the German armies, with a density of c.?18,000 men per 1-mile (1.6?km) of front, or about ten per 1 metre (1.1?yd).[19]\\r\\nLate on 20 August General Karl von Blow, the 2nd Army commander, who had tactical control over the 1st Army while north of the Sambre, held the view that an encounter with the British was unlikely and wished to concentrate on the French units reported between Charleroi and Namur, on the south bank of the Sambre; reconnaissance in the afternoon failed to reveal the strength or intentions of the French. The 2nd Army was ordered to reach a line from Binche, Fontaine-l'Eveque and the Sambre next day to assist the 3rd Army across the Meuse by advancing south of the Sambre on 23 August. The 1st Army was instructed to be ready to cover Brussels and Antwerp to the north and Maubeuge to the south-west. Kluck and the 1st Army staff expected to meet British troops, probably through Lille, which made a wheel to the south premature. Kluck wanted to advance to the south-west to maintain freedom of manuoeuvre and on 21 August, attempted to persuade Blow to allow the 1st Army to continue its manoeuvre. Blow refused and ordered the 1st Army to isolate Maubeuge and support the right flank of the 2nd Army, by advancing to a line from Lessines to Soignies, while the III and IV Reserve corps remained in the north, to protect the rear of the army from Belgian operations southwards from Antwerp.[20]\\r\\nOn 22 August, the 13th Division of the VII Corps, on the right flank of the 2nd Army, encountered British cavalry north of Binche, as the rest of the army to the east began an attack over the Sambre river, against the French Fifth Army. By the evening the bulk of the 1st Army had reached a line from Silly to Thoricourt, Louvignies and Mignault; the III and IV Reserve corps had occupied Brussels and screened Antwerp. Reconnaissance by cavalry and aircraft indicated that the area to the west of the army was free of troops and that British troops were not concentrating around Kortrijk (Courtrai), Lille and Tournai but were thought to be on the left flank of the Fifth Army, from Mons to Maubeuge. Earlier in the day, British cavalry had been reported at Casteau, to the north-east of Mons. A British aeroplane had been seen at Louvain (Leuven) on 20 August and on the afternoon of 22 August, a British aircraft en route from Maubeuge, was shot down by the 5th Division. More reports had reached the IX Corps, that columns were moving from Valenciennes to Mons, which made clear the British deployment but were not passed on to the 1st Army headquarters. Kluck assumed that the subordination of the 1st Army to the 2nd Army had ended, since the passage of the Sambre had been forced. Kluck wished to be certain to envelop the left (west) flank of the opposing forces to the south but was again over-ruled and ordered to advance south, rather than south-west, on 23 August. [21]\\r\\nLate on 22 August, reports arrived that the British had occupied the Canal du Centre crossings from Nimy to Ville-sur-Haine, which revealed the location of British positions, except for their left flank. On 23 August, the 1st Army began to advance north-west of Maubeuge, to a line from Bascles to St. Ghislain and Jemappes. The weather had turned cloudy and rainy, which grounded the 1st Army Flieger-Abteilung all day, despite an improvement in the weather around noon. News that large numbers of troops had been arriving at Tournai by train were received and the advance was suspended, until the reports from Tournai could be checked. The IX Corps divisions advanced in four columns against the Canal du Centre, from the north of Mons to Roeulx and on the left (eastern) flank, met French troops at the canal, which was thought to be the junction of the British and French forces. The corps commander, General von Quast, had ordered an attack for 9:55 a.m. to seize the crossings, before the halt order was received. The two III Corps divisions were close to St. Ghislain and General Ewald von Lochow ordered them to prepare an attack from Tertre to Ghlin. In the IV Corps area, General Sixt von Armin ordered an attack on the canal crossings of Pruwelz and Blaton and ordered the 8th Division to reconnoitre from Tournai to Cond and to keep contact with H?here Kavallerie-Kommando 2 (HKK 2, II Cavalry Corps).[22]\\r\\nAt dawn on 23 August, a German artillery bombardment began on the British lines; throughout the day the Germans concentrated on the British at the salient formed by the loop in the canal.[23] At 9:00 a.m., the first German infantry assault began, with the Germans attempting to force their way across four bridges that crossed the canal at the salient.[24] Four German battalions attacked the Nimy bridge, which was defended by a company of the 4th Battalion, Royal Fusiliers and a machine-gun section led by Lieutenant Maurice Dease. Advancing at first in close column, \\"parade ground formation\\", the Germans made easy targets for the British riflemen, who hit German soldiers at over 1,000 yards (910?m), mowing them down by rifle, machine-gun and artillery fire.[25][26] So heavy was the British rifle fire throughout the battle that some Germans thought they were facing batteries of machine-guns.[27]\\r\\nThe German attack was a costly failure and the Germans switched to an open formation and attacked again. This attack was more successful, as the looser formation made it more difficult for the British to inflict casualties rapidly. The outnumbered defenders were soon hard-pressed to defend the canal crossings, and the Royal Fusiliers at the Nimy and Ghlin bridges only held on with piecemeal reinforcement and the exceptional bravery of two of the battalion machine-gunners.[28] At the Nimy bridge, Dease took control of his machine gun after the rest of the section had been killed or wounded and fired the weapon despite being shot several times. After a fifth wound he was evacuated to the battalion aid station, where he died.[29] Private Sidney Godley took over and covered the Fusilier retreat at the end of the battle, but when it was his time to retreat he disabled the gun by throwing parts into the canal then surrendered.[30] Dease and Godley were awarded the Victoria Cross, the first awards of the First World War.[31]\\r\\nTo the right of the Royal Fusiliers, the 4th Battalion, Middlesex Regiment, and the 1st Battalion, Gordon Highlanders, were equally hard-pressed by the German assault on the salient. Greatly outnumbered, both battalions suffered heavy casualties but with the addition of reinforcements from the Royal Irish Regiment, from the divisional reserve and fire support from the divisional artillery, they managed to hold the bridges.[32] The Germans expanded their attack, assaulting the British defences along the straight reach of the canal to the west of the salient. The Germans used the cover of fir plantations that lined the northern side of the canal and advanced to within a few hundred yards of the canal, to rake the British with machine-gun and rifle fire. The German attack fell particularly heavily on the 1st Battalion, Royal West Kent Regiment and the 2nd Battalion, King's Own Scottish Borderers, which despite many casualties, repulsed the Germans throughout the day.[33]\\r\\nBy the afternoon, the British position in the salient had become untenable; the 4th Middlesex had 15 officers and 353 other ranks killed or wounded.[34] To the east of the British position, units of the German IX Corps had begun to cross the canal in force, threatening the British right flank. At Nimy, Private Oskar Niemeyer had swum across the canal under British fire to operate machinery closing a swing bridge. Although he was killed, his actions re-opened the bridge and allowed the Germans to increase pressure against the 4th Royal Fusiliers.[35][36]\\r\\nAt 3:00 p.m., the British 3rd Division was ordered to retire from the salient, to positions a short distance to the south of Mons and a similar retreat towards evening by the 5th Division to conform. By nightfall II Corps had established a new defensive line running through the villages of Montr?ul, Boussu, Wasmes, Paturages and Frameries. The Germans had built pontoon bridges over the canal and were approaching the British positions in great strength. News had arrived that the French Fifth Army was retreating, dangerously exposing the British right flank and at 2:00 a.m. on 24 August, II Corps was ordered to retreat south-west into France to reach defensible positions along the ValenciennesÿMaubeuge road.[37]\\r\\nThe unexpected order to retreat from prepared defensive lines in the face of the enemy, meant that II Corps was required to fight a number of sharp rearguard actions against the Germans. For the first stage of the withdrawal, Smith-Dorrien detailed the 15th Brigade of the 5th Division, which had not been involved in heavy fighting on 23 August, to act as rearguard. On 24 August they fought various holding actions at Paturages, Frameries and Audregnies. During the engagement at Audregnies the 1st Battalions of the Cheshire and Norfolk Regiments temporarily checked the German advance from Quivrain and Baisieux despite being heavily outnumbered and took resultant very heavy losses, though in return with the support of the 5th Brigade artillery assets they also inflicted heavy casualties on the advancing German regiments. An evening roll call of the Cheshires 1st Battalion, who had not received an issued withdrawal order, indicated that their establishment had been reduced by almost 80 per cent.\\r\\nAt Wasmes, elements of the 5th Division faced a big attack, German artillery began bombarding the village at daybreak, and at 10:00 a.m. infantry of the German III Corps attacked. Advancing in columns, the Germans were immediately met with massed rifle and machine-gun fire and were \\"mown down like grass.\\"[38] For a further two hours, soldiers of the Northumberland Fusiliers, 1st West Kents, 2nd Battalion, King's Own Yorkshire Light Infantry, 2nd Battalion, Duke of Wellington's Regiment, and 1st Battalion, Bedfordshire Regiment, held off German attacks on the village despite many casualties and then retreated in good order to St. Vaast.[39]\\r\\nOn the extreme left of the British line, the 14th and 15th Brigades of the 5th Division were threatened by a German outflanking move and were forced to call for help from the cavalry.[40] The 2nd Cavalry Brigade, along with the 119th Battery RFA and L Battery RHA, were sent to their aid. Dismounting, the cavalry and the two artillery batteries screened the withdrawal of the 14th and 15th Brigades in four hours of intense fighting.[41]\\r\\nOn 23 August, the 18th Division of IX Corps advanced and began to bombard the British defences near Maisires and St. Denis. Part of the 35th Brigade, which contained large numbers of ethnic Danes from Northern Schleswig, got across the canal east of Nimy, with few casualties and reached the railway beyond in the early afternoon but the attack on Nimy was repulsed. The 36th Brigade captured bridges at Obourg against determined resistance, after which the defenders of Nimy gradually withdrew; the bridges to the north were captured at 4:00 p.m. and the town stormed. Quast ordered the 18th Division to take Mons and push south to Cuesmes and Mesvin. Mons was captured unopposed, except for a skirmish on the southern fringe and by dark, the 35th Brigade was in the vicinity of Cuesmes and Hyon. On higher ground to the east of Mons, the defence continued. On the front of the 17th Division, British cavalry withdrew from the canal crossings at Ville-sur-Haine and Thieu and the division advanced to the St. SymphorienÿSt. Ghislain road. At 5:00 p.m., the divisional commander ordered an enveloping attack on the British east of Mons, who were pushed back after a stand on the MonsÿGivry road.[42]\\r\\nBy 11:00 a.m., reports from the IV, III and IX corps revealed that the British were in St. Ghislain and at the canal crossings to the west, as far as the bridge at Pommeroeuil, with no troops east of Cond. Intelligence reports from 22 August, had noted 30,000 troops heading through Dour towards Mons and on 23 August, 40,000 men had been seen on the road to Genlis south of Mons, with more troops arriving at Jemappes. To the north of Binche, the right flanking division of the 2nd Army had been forced back to the south-west by British cavalry. In the early afternoon, the II Cavalry Corps reported that it had occupied the area of ThieltÿKortrykÿTournai during the night and forced back a French brigade, to the south-east of Roubaix. With this report indicating that the right flank was clear of Allied troops, Kluck ordered the III Corps to advance through St. Ghislain and Jemappes on the right of IX Corps and the IV Corps to continue towards Hensis and Thulies; IV Corps was already attacking at the Canal du Centre, the II Corps and the IV Reserve Corps, were following on behind the main part of the army.[43]\\r\\nIII Corps had to advance across meadows, to an obstacle with few crossings, all of which had been destroyed. The 5th Division advanced towards Tertre on the right, which was captured but then the advance on the railway bridge was stopped by small-arms fire, from across the canal. On the left flank, the division advanced towards a bridge north-east of Wasmuel and eventually managed to get across the canal against determined resistance, before turning towards St. Ghislain and Hornu. As dark fell, Wasmuel was occupied and attacks on St. Ghislain were repulsed by machine-gun fire, which prevented troops crossing the canal except at Tertre, where the advance was stopped for the night. The 6th Division was counter-attacked at Ghlin, before advancing towards higher ground south of Jemappes. The British in the village stopped the division with small-arms fire, except for small parties, who found cover west of a path from Ghlin to Jemappes. These isolated parties managed to surprise the defenders at the crossing north of the village, with the support of a few field guns around 5:00 p.m., after which the village was captured. The rest of the division crossed the canal and began a pursuit towards Frameries and Ciply but stopped as dark fell.[43]\\r\\nThe IV Corps arrived in the afternoon, as the 8th Division closed on Hensies and Thulin and the 7th Division advanced towards Ville-Pommeroeuil, where there were two canals blocking the route. The 8th Division encountered the British at the northernmost canal, west of Pommeroeuil and forced back the defenders but then bogged down in front of the second canal, under machine-gun fire from the south bank. The attack was suspended after night fell and the British blew the bridge. The 7th Division forced the British back from a railway embankment and over the canal, to the east of Pommeroeuil but was pushed back from the crossing. Small parties managed to cross by a footbridge built in the dark and protected repair parties at the blown bridge, which allowed troops to get across and dig in 400 metres (440?yd) south of the canal, on either side of the road to Thulin.[44]\\r\\nLate in the day, the II Corps and the IV Reserve Corps rested on their march routes at La Hamaide and Bierghes, after marching 32 and 20 kilometres (20 and 12?mi) respectively, 30 and 45 kilometres (19 and 28?mi) behind the front, too far behind to take part in the battle on 24 August. In the mid-afternoon of 23 August, IV Corps was ordered to rest, as reports from the front suggested that the British defence had been overcome and the 1st Army headquarters wanted to avoid the army converging on Maubeuge, leaving the right (western) flank vulnerable. In the evening, Kluck cancelled the instruction, after reports from IX Corps reporting that its observation aircraft had flown over a column 3 kilometres (1.9?mi) long, moving towards Mons along the Malplaquet road. Two more columns were seen on the MalplaquetÿGenly and the QuevyÿGenly roads, a large force was seen near Asquillies and cavalry was found further east, which showed that most of the BEF was opposite the 1st Army. It was considered vital that the second canal crossings were captured along the line, as had been achieved by the IX and part of III corps. IV Corps was ordered to resume its march and move the left wing towards Thulin but it was already engaged at the canal crossings. The III and IX corps attack during the day, had succeeded against \\"a tough, nearly invisible enemy\\" but the offensive had to continue, because it appeared that only the right flank of the army could get behind the BEF.[45]\\r\\nThe situation remained unclear at the 1st Army headquarters in the evening, because communication with the other right flank armies had been lost and only fighting near Thuin by VII Corps, the right-flank unit of the 2nd Army had been reported. Kluck ordered that the attack was to continue on 24 August, past the west of Maubeuge and that II Corps would catch up behind the right flank of the army. IX Corps was to advance to the east of Bavay, III Corps was to advance to the west of the village, IV Corps was to advance towards Warnies-le-Grand 10 kilometres (6.2?mi) further to the west and the II Cavalry Corps was to head towards Denain, to cut off the British retreat. During the night there were several British counter-attacks but none of the German divisions was forced back over the canal. At dawn the IX Corps resumed its advance and pushed forwards against rearguards until the afternoon, when the corps stopped the advance due to uncertainty about the situation on its left flank and the proximity of Maubeuge. At 4:00 p.m. cavalry reports led Quast to resume the advance, which was slowed by the obstacles of Maubeuge and III Corps congesting the roads.[46]\\r\\nOn the III Corps front to the west, the 6th Division attacked Frameries at dawn, which held out until 10:30 a.m. and then took La Bouverie and Paturages, after which the British began to retreat; the division turned west towards Warquignies and the 5th Division. St. Ghislain had been attacked by the 5th Division behind an artillery barrage, where the 10th Brigade had crossed the canal and taken the village in house-to-house fighting, then reached the south end of Hornu. A defensive line had been established by the British along the DourÿWasmes railway, which stopped the German advance and diverted the 9th Brigade until 5:00 p.m., when the British withdrew. The German infantry were exhausted and stopped the pursuit at Dour and Warquignies. During the day Kluck sent liaison officers to the corps headquarters, stressing that the army should not converge on Maubeuge but pass to the west, ready to envelop the British left (west) flank.[47]\\r\\nThe IV Corps headquarters had ordered its divisions to attack over the canal at dawn but found that the British had blown the bridges and withdrawn. Repairs took until 9:00 a.m. and the 8th Division did not reach Quivrain until noon; the 7th Division reached the railway at Thuin during the morning and then took louges late in the afternoon. As the 8th Division moved on, the vanguard was ambushed by British cavalry before an advance to Valenciennes could begin and then attacked a British rearguard at Baisieux, which then slipped away to Audregnies. The rest of the division skirmished with French Territorials south-west of Baisieux. The IV Corps attack forced back rearguards but inflicted no serious damage, having been slowed by the bridge demolitions at the canals. The cavalry divisions had advanced towards Denain and the J?gerbattalions had defeated troops of the French 88th Territorial Division at Tournai and then reached Marchiennes, after a skirmish with the 83rd Territorial Division near Orchies.[47]\\r\\nGerman air reconnaissance detected British troops on 21 August, advancing from Le Cateau to Maubeuge, and on 22 August from Maubeuge to Mons, as other sources identified halting places, but poor communication and lack of systematic direction of air operations led to the assembly of the BEF from Cond to Binche being unknown to the Germans on 22ÿ23 August.[48] British reconnaissance flights had begun on 19 August with two sorties and two more on 20 August, which reported no sign of German troops. Fog delayed flights on 21 August but in the afternoon German troops were seen near Kortrijk and three villages were reported to be burning. Twelve reconnaissance sorties were flown on 22 August and reported many German troops closing in on the BEF, especially troops on the BrusselsÿNinove road, which indicated an enveloping manoeuvre. One British aircraft was shot down and a British observer became the first British soldier to be wounded while flying. By the evening Sir John French was able to discuss with his commanders the German dispositions near the BEF which had been provided by aircraft observation, the strength of the German forces, that the Sambre had been crossed and that an encircling move by the Germans from Geraardsbergen was possible. During the battle on 23 August, the aircrews flew behind the battlefield looking for troop movements and German artillery batteries.[49]\\r\\nBy nightfall on 24 August, the British had retreated to what was expected to be their new defensive lines, on the ValenciennesÿMaubeuge road. Outnumbered by the German 1st Army and with the French Fifth Army also falling back, the BEF had no choice but to continue to retire ÿ I Corps retreating to Landrecies and II Corps to Le Cateau.[50] The chaos and confusion were graphically illustrated in Landrecies on 25 August, where a senior officer \\"apparently took leave of his senses and began firing his revolver down a street.\\"[51] The Great Retreat continued for two weeks and covered over 250 miles (400?km). The British were closely pursued by the Germans and fought several rearguard actions, including the Battle of Le Cateau on 26 August, the treux rearguard action on 27 August and the Action at Nry on 1 September.[52] Units disappeared and \\"More guns were lost than at any time since the American War of Independence.\\"[53]\\r\\nBoth sides had success at the Battle of Mons: the British had been outnumbered by about 3:1 but managed to withstand the German 1st Army for 48 hours, inflict more casualties on the Germans and then retire in good order. The BEF achieved its main strategic objective, which was to prevent the French Fifth Army from being outflanked.[54][55] The battle was an important moral victory for the British; as their first battle on the continent since the Crimean War, it was a matter of great uncertainty as to how they would perform. In the event, the British soldiers came away from the battle with a clear sense that they had got the upper hand during the fighting at Mons. The Germans appeared to recognise that they had been dealt a sharp blow by an army they had considered inconsequential. The German novelist and infantry officer wrote,\\r\\nThe men all chilled to the bone, almost too exhausted to move and with the depressing consciousness of defeat weighing heavily upon them. A bad defeat, there can be no gainsaying it... we had been badly beaten, and by the English ÿ by the English we had so laughed at a few hours before.[56]Captain Walter Bloem\\r\\nFor the Germans, the Battle of Mons was a tactical repulse and a strategic success. The staff at Kluck's headquarters claimed that the two-day battle had failed to envelop the British, due to the subordination of the army to Blow and the 2nd Army headquarters, which had insisted that the 1st Army keep closer to the western flank, rather than attack to the west of Mons. It was believed that only part of the BEF had been engaged and that there was a main line of defence from Valenciennes to Bavay, which Kluck ordered to be enveloped on 25 August.[57] The 1st Army was delayed by the British and suffered many casualties but crossed the barrier of the MonsÿCond Canal and began its advance into France. The Germans drove the BEF and French armies before them almost to Paris, before being stopped at the Battle of the Marne.[58]\\r\\nJ. E. Edmonds, the British official historian, recorded \\"just over\\" 1,600 British casualties, most in the two battalions of the 8th Brigade which had defended the salient and wrote that German losses \\"must have been very heavy\\", which explained German inertia after dark, when the 8th Brigade was vulnerable, several other gaps existed in the British line and the retirement had begun.[59] John Keegan estimated German losses to have been c.?5,000 men.[60] In 1997, D. Lomas recorded German losses from 3,000 to 5,000 men.[61] In 2009, Herwig recorded 1,600 British casualties and c.?5,000 German casualties, despite the fact that Kluck and Kuhl did not reveal 1st Army casualties.[62] Post-war German records estimated 2,145 dead and missing and 4,932 wounded in the 1st Army from 20ÿ31 August.[63] Using German regimental histories, Terence Zuber gave \\"no more than 2,000\\" German casualties.[64]\\r\\nThe Battle of Mons has attained an almost mythic status. In British historical writing, it has a reputation as an unlikely victory against overwhelming odds, similar to the English victory at the Battle of Agincourt.[54] Mons gained a myth, a miraculous tale that the Angels of Monsangelic warriors sometimes described as phantom longbowmen from Agincourthad saved the British Army by halting the German troops.[65]\\r\\nSoldiers of the BEF who fought at Mons became eligible for a campaign medal, the 1914 Star, often colloquially called the Mons Star, honouring troops who had fought in Belgium or France 5 August ÿ 22 November 1914. On 19 August 1914, Kaiser Wilhelm allegedly issued an Order of the Day which read in part: \\". . . my soldiers to exterminate first the treacherous English; walk over Field Marshal French's contemptible little Army.\\" This led to the British \\"Tommies\\" of the BEF proudly labelling themselves \\"The Old Contemptibles\\". No evidence of the Order of the Day has been found in German archives and the ex-Kaiser denied giving it. According to the controversial book Falsehood in War-Time, an investigation conducted by General Frederick Maurice traced the origins of the Order to the British GHQ, where it had been concocted for propaganda purposes.[66]\\r\\nThe Germans established the St Symphorien Military Cemetery as a memorial to the German and British dead. On a mound in the centre of the cemetery, a grey granite obelisk 7 metres (23?ft) tall was built with a German inscription: \\"In memory of the German and English soldiers who fell in the actions near Mons on the 23rd and 24th August 1914.\\"[67] Originally, 245 German and 188 British soldiers were interred at the cemetery. More British, Canadian and German graves were moved to the cemetery from other burial grounds and more than 500 soldiers were eventually buried in St. Symphorien, of which over 60 were unidentified. Special memorials were erected to five soldiers of the Royal Irish Regiment believed to be buried in unnamed graves. Other special memorials record the names of four British soldiers, buried by the Germans in Obourg Churchyard, whose graves could not be found. St. Symphorien cemetery also contains the graves of the two soldiers believed to be the first (Private John Parr, 4th Battalion, Middlesex Regiment, 21 August 1914) and the last (Private Gordon Price, Canadian Infantry, 11 November 1918) Commonwealth soldiers to be killed during the First World War. A tablet in the cemetery sets out the gift of the land by Jean Houzeau de Lehaie.[68]\\r\\nBooks\\r\\nEncyclopaedias\\r\\nJournals","input":"What was the first british battle of ww1?"},{"output":"basic life support","context":"Allied health professions are health care professions distinct from nursing, medicine, and pharmacy.[1] They work in health care teams to make the health care system function by providing a range of diagnostic, technical, therapeutic and direct patient care and support services that are critical to the other health professionals they work with and the patients they serve.\\r\\n\\r\\n\\r\\nIn September 2012 the organization of International Chief Health Professions Officers (ICHPO) provided an agreed-upon definition of an allied health professional.[citation needed]\\r\\n\\"Allied Health Professions are a distinct group of health professionals who apply their expertise to prevent disease transmission, diagnose, treat and rehabilitate people of all ages and all specialties. Together with a range of technical and support staff they may deliver direct patient care, rehabilitation, treatment, diagnostics and health improvement interventions to restore and maintain optimal physical, sensory, psychological, cognitive and social functions.\\"\\r\\nThe International CHPO group is a network of chief officers with a professional and policy leadership role for allied health professions and supports knowledge exchange and partnership working across the international community ICHPO member countries: Australia, Belgium, Canada, Denmark, England, Hong Kong (SAR), Malaysia, Malta, Namibia, New Zealand, Northern Ireland, Scotland, Singapore, Slovenia, South Africa, Republic of Ireland, and Wales.\\r\\nDepending on the country and local health care system, a limited subset of the following professions (professional areas) may be represented, and may be regulated:\\r\\nThe precise titles, roles and requisites of allied health professionals may vary considerably from country to country.\\r\\nSome allied health professions are more specialized, and so must adhere to national training and education standards and their professional scope of practice. Often they must prove their skills through degrees, diplomas, certified credentials, and continuing education. Other allied health professions require no special training or credentials and are trained for their work by their employer through on-the-job training (which would then exclude them from consideration as an allied health profession in a country like Australia). Many allied health jobs are considered career ladder jobs because of the opportunities for advancement within specific fields.[3]\\r\\nAllied health professions can include the use of many skills. Depending on the profession, these may include basic life support; medical terminology, acronyms and spelling; basics of medical law and ethics; understanding of human relations; interpersonal communication skills; counseling skills; computer literacy; ability to document healthcare information; interviewing skills; and proficiency in word processing; database management and electronic dictation.[4]\\r\\nThe explosion of scientific knowledge that followed World War II brought increasingly sophisticated and complex medical diagnostic and treatment procedures. Increasing public demand for medical services combined with higher health care costs provoked a trend toward expansion of service delivery from treating patients in hospitals to widespread provision of care in physician's private and group practices, ambulatory medical and emergency clinics, and mobile clinics and community-based care. In the developing world, international development assistance led to numerous initiatives for strengthening health workforce capacity to deliver essential health care services. What followed has been an increase in the need for skilled health care delivery personnel worldwide.\\r\\nChanges in the health industry and emphasis on cost-efficient solutions to health care delivery will continue to encourage expansion of the allied health workforce. The World Health Organization estimates there is currently a worldwide shortage of about 2 million allied health professionals (considering all health workers aside from medical and nursing personnel) needed in order to meet global health goals.[5]\\r\\nIn recognition of the growth of the number and diversity of allied health professionals in recent years, the newly adopted 2008 version of the International Standard Classification of Occupations has increased the number of groups dedicated to allied health professions. Depending on the presumed skill level, they may either be identified as \\"health professionals\\" or \\"health associate professionals\\". For example, new categories have been created for delineating \\"paramedical practitioners\\"grouping professions such as clinical officers, clinical associates, physician assistants, Feldshers, and assistant medical officersas well as for community health workers; dietitians and nutritionists; audiologists and speech therapists; and others.[6]\\r\\nProjections in the United States and many other countries have shown an expected long-term shortage of qualified workers to fill many allied health positions. This is primarily due to expansion of the health industry due to demographic changes (a growing and aging population), large numbers of health workers nearing retirement, the industry's need to be cost efficient, and a lack of sufficient investment in training programs to keep pace with these trends.[7][8]\\r\\nStudies have also pointed to the need for increased diversity in the allied health workforce to realize a culturally competent health system in the United States[9] and elsewhere.\\r\\nWorkforce and health care experts anticipate that health services will increasingly be delivered via ambulatory and nursing care settings rather than in hospitals. According to the North American Industry Classification System (NAICS), the health care industry consists of three main sub-sectors, divided by the types of services provided at each facility:[10]\\r\\nIn the US, a larger proportion of the allied health care workforce is already employed in ambulatory settings. In California, nearly half (49.4 percent) of the allied health workforce is employed in ambulatory health care settings, compared with 28.7 percent and 21.9 percent employed in hospital and nursing care, respectively.[11] One source reported allied health professionals making up 60 percent of the total US health workforce.[12]\\r\\nIn the United Kingdom there are 12 distinct professions who are considered allied health professionals; in combination they account for about 6% of the NHS workforce. In 2013 the annual expenditure on services provided by allied health professionals amounted to around ?2 billion, although there is a lack of evidence around the extent to which these services improve the quality of care.[13]\\r\\nAdvancements in medical technology also allow for more services that formerly required expensive hospital stays to be delivered via ambulatory care. For example, in California, research has predicted the total consumption of hospital days per person will decline from 4 days in 2010 to 3.2 days in 2020 to 2.5 days in 2030. In contrast, the number of ambulatory visits per person will increase from 3.2 visits per person in 2010 to 3.6 visits per person in 2020 to 4.2 visits in 2030.[14]\\r\\nIn developing countries, many national human resources for health strategic plans and international development initiatives are focusing on scaling up training of allied health professions, such as HIV/AIDS counsellors, clinical officers and community health workers, in providing essential preventive and treatment services in ambulatory and community-based care settings.[15]\\r\\nWith this growing demand for ambulatory health care, researchers expect to witness a heavier demand for professions that are employed within the ambulatory sector and other non-hospital settingsin other words, allied health.[16]","input":"What skills should an allied health professional have?"},{"output":"Chikwendu Kanu","context":"The Speaker of the Abia State House of Assembly is the political head of the Abia State legislative who serves as the preciding officer of the Abia State House of Assembly.[1] The Speaker is elected by Members of the House with the sole responsibilities of conducting meetings of the House, appointing committees and enforcing the Rules of the House. The current speaker is Chikwendu Kanu, a People's Democratic Party member who was sworn in on 30 December 2016, succeeding Kennedy Njoku.[2]","input":"Who is the current speaker of abia state?"},{"output":"a set of research ethics principles for human experimentation","context":"The Nuremberg Code (German: Nrnberger Kodex) is a set of research ethics principles for human experimentation set as a result of the subsequent Nuremberg trials at the end of the Second World War.\\r\\n\\r\\nThe origin of the Nuremberg Code began in pre-World War II German politics, particularly during the 1930s and 1940s. The pre-war German Medical Association was considered to be a progressive yet democratic association with great concerns for public health, one example being the legislation of compulsory health insurance for German workers. However, starting in the mid-1920s, German physicians, usually proponents of racial hygiene, were accused by the public and the medical society of unethical medical practices. The use of racial hygiene was supported by the German government in order to create an Aryan \\"master race,\\" and to exterminate those who did not fit into their criteria. Racial hygiene extremists merged with National Socialism to promote the use of biology to accomplish their goals of racial purity, a core concept in the Nazi ideology. Physicians were attracted to the scientific ideology and aided in the establishment of National Socialist Physicians' League in 1929 to \\"purify the German medical community of 'Jewish Bolshevism.'\\" Criticism was becoming prevalent; Alfons Stauder, member of the Reich Health Office, claimed that the \\"dubious experiments have no therapeutic purpose,\\" and Fredrich von Muller, physician and the president of the Deutsche Akademie, joined the criticism.[1]\\r\\n\\r\\nIn response to the criticism of unethical human experimentation, the Reich government issued \\"Guidelines for New Therapy and Human Experimentation\\" in Weimar, Germany. The guidelines were based on beneficence and non-maleficence, but also stressed legal doctrine of informed consent. The guidelines clearly distinguished the difference between therapeutic and non-therapeutic research. For therapeutic purposes, the guidelines allowed administration without consent only in dire situations, but for non-therapeutic purposes any administration without consent was strictly forbidden. However, the guidelines from Weimar were negated by Adolf Hitler. By 1942, more than 38,000 German physicians were in the Nazi party, who helped carry out medical programs such as the Sterilization Law.[2]\\r\\n\\r\\nAfter World War II, a series of trials were held to hold members of the Nazi party responsible for a multitude of war crimes. The trials were approved by President Harry Truman in January 1946 and were led exclusively by the United States. They began on December 9, 1946 in Nuremberg, Germany, in what became known as the Nuremberg trials. In one of the trials, which became known as the \\"Doctors' Trial,\\" German physicians responsible for conducting unethical medical procedures on humans during the war were tried. They focused on physicians that conducted inhumane and unethical human experiments in concentration camps, in addition to those who were involved in over 3,500,000 sterilizations of German citizens.[3][4] Several of the accused argued that their experiments differed little from those used before the war, and that there was no law that differentiated between legal and illegal experiments. On August 20, 1947, the judges delivered their verdict against Karl Brandt and 22 others.[5]\\r\\n\\r\\nIn May 1947, while the trials were being held, six points defining legitimate medical research were submitted to the Counsel for War Crimes. Three judges, in response to expert medical advisers for the prosecution, adopted these points and added four additional points. The 10 points constituted the \\"Nuremberg Code,\\" which includes such principles as informed consent and absence of coercion; properly formulated scientific experimentation; and beneficence towards experiment participants. It is thought to have been mainly based on the Hippocratic Oath, which was interpreted as endorsing the experimental approach to medicine while protecting the patient.[6]\\r\\n\\r\\nThe Nuremberg Code was initially ignored, but gained much greater significance about 20 years after it was written. As a result, there were substantial rival claims for the creation of the Code. Some claimed that Harold Sebring, one of the three U.S. judges who presided over the Doctors' Trial, was the author. Leo Alexander, MD and Andrew Ivy, MD, the prosecution's chief medical expert witnesses, were also each identified as authors. In his letter to Maurice H. Pappworth, an English physician and the author of the book Human Guinea Pigs, Andrew Ivy claimed sole authorship of the Code. Leo Alexander, approximately 30 years after the trial, also claimed sole authorship.[7] However, after careful reading of the transcript of the Doctors' Trial, background documents, and the final judgements, it is more accepted that the authorship was shared and the Code grew out of the trial itself.[8]\\r\\n\\r\\nThe Nuremberg Code has not been officially accepted as law by any nation or as official ethics guidelines by any association. In fact, the Code's reference to Hippocratic duty to the individual patient and the need to provide information was not initially favored by the American Medical Association. The Western world initially dismissed the Nuremberg Code as a \\"code for barbarians\\" and not for civilized physicians and investigators. Additionally, the final judgement did not specify whether the Nuremberg Code should be applied to cases such as political prisoners, convicted felons, and healthy volunteers. The lack of clarity, the brutality of the unethical medical experiments, and the uncompromising language of the Nuremberg Code created an image that the Code was designed for singularly egregious transgressions.[9]\\r\\n\\r\\nHowever, the Code is considered to be the most important document in the history of clinical research ethics, which had a massive influence on global human rights. The Nuremberg Code and the related Declaration of Helsinki are the basis for the Code of Federal Regulations Title 45 Part 46,[10][11] which are the regulations issued by the United States Department of Health and Human Services for the ethical treatment of human subjects, and are used in Institutional Review Boards (IRBs). In addition, the idea of informed consent has been universally accepted and now constitutes Article 7 of the United Nations' International Covenant on Civil and Political Rights. It also served as the basis for International Ethical Guidelines for Biomedical Research Involving Human Subjects proposed by the World Health Organization.[7]","input":"What is the purpose of the nuremberg code?"},{"output":"12 December 1991","context":"Abuja (/??bu?d??/)[4] is the capital city of Nigeria located in the centre of the country within the Federal Capital Territory (FCT). It is a planned city and was built mainly in the 1980s,[5] replacing the country's most populous city of Lagos as the capital on 12 December 1991. Abuja's geography is defined by Aso Rock, a 400-metre (1,300?ft) monolith left by water erosion. The Presidential Complex, National Assembly, Supreme Court and much of the city extend to the south of the rock. Zuma Rock, a 792-metre (2,598?ft) monolith, lies just north of the city on the road to Kaduna State.\\r\\nAt the 2006 census, the city of Abuja had a population of 776,298,[6] making it one of the ten most populous cities in Nigeria. According to the United Nations, Abuja grew by 139.7% between 2000 and 2010, making it the fastest growing city in the world.[7] As of 2015[update], the city is still experiencing an annual growth of at least 35%, still retaining its position as the fastest-growing city on the African continent and one of the fastest-growing in the world.[8] Abuja has witnessed a huge influx of people into the city; the growth has led to the emergence of satellite towns, such as Karu Urban Area, Suleja, Gwagwalada, Lugbe, Kuje and smaller settlements towards which the planned city is sprawling. The unofficial metropolitan area of Abuja has a population of well over three million, making it the fourth largest metropolitan area in Nigeria, surpassed only by Lagos, Kano and Ibadan. As at 2016, the metropolitan area of Abuja is estimated at 6 million persons, placing it behind only Lagos, as the most populous metro area.[2]\\r\\nMajor religious sites include the Nigerian National Mosque and the Nigerian National Christian Centre. The city is served by the Nnamdi Azikiwe International Airport. Abuja is known for being one of the few purpose-built capital cities in Africa, as well as being one of the wealthiest.[9]\\r\\n\\r\\n\\r\\n\\"Abuja\\" was in the earlier 20th century the name of the nearby town now called Suleja.\\r\\nThe indigenous inhabitants of Abuja are the Gbagyi (Gwari) as the major language, Bassa, Gwandara, Gade, Ganagana, Koro etc. In light of the ethnic and religious divisions of Nigeria, plans had been devised since Nigeria's independence to have its capital in a place deemed neutral to all major ethnic parties, and also in close proximity to all the regions of Nigeria. The location was eventually designated in the centre of the country in the early 1970s as it signified neutrality and national unity. Another impetus for Abuja came because of Lagos' population boom that made that city overcrowded and conditions squalid. As Lagos was already undergoing rapid economic development, the Nigerian regime felt the need to expand the economy towards the inner part of the country, and hence decided to move its capital to Abuja.[10] The logic used was similar to the way Brazil planned its capital, Braslia. Construction broke ground and was dedicated in the late 1970s but, due to economic and political instability, the initial stages of the city were not complete until the late 1980s.\\r\\nThe master plan for Abuja and the Federal Capital Territory (FCT) was developed by International Planning Associates (IPA), a consortium of three American firms: Planning Research Corporation; Wallace, McHarg, Roberts and Todd;[11] and Archisystems, a division of the Hughes Organization. The master plan for Abuja defined the general structure and major design elements of the city that are visible in its current form. More detailed design of the central areas of the capital, particularly its monumental core, was accomplished by Kenzo Tange, a renowned Japanese architect, with his team of city planners at Kenzo Tange and Urtec company.\\r\\nMost countries relocated their embassies to Abuja, and many maintain their former embassies as consulates in Lagos, the commercial capital of Nigeria. Abuja is the headquarters of the Economic Community of West African States (ECOWAS) and the regional headquarters of OPEC. Abuja and the FCT have experienced huge population growth; it has been reported that some areas around Abuja have been growing at 20% to 30% per year.[12] Squatter settlements and towns have spread rapidly in and outside the city limits.[5][13] Tens of thousands of people have been evicted since former FCT minister Nasir Ahmad el-Rufai started a demolition campaign in 2003.[14]\\r\\nThe FCT's ministers have been as follows:\\r\\nThe Phase 1 area of the city is divided into ten districts known as cadastral zones.[16]\\r\\nThere are also sixteen districts in Phase 2.[16]\\r\\nThere are eleven districts in Phase 3.[16]\\r\\nThere are five suburban districts: Nyanya, Karu, Gwagwalada, Kubwa, and Jukwoyi. Along the Airport Road are clusters of satellite settlements, namely Lugbe, Chika, Kuchigworo and Pyakassa. Other satellite settlements are Idu (the main industrial zone), Mpape, Karimu, Gwagwa, Dei-Dei (housing the International Livestock market and also International Building materials market).\\r\\nAbuja's Central District, also called Central Area, spans from the foot of Aso Rock, across the Three Arms Zone, to the southern base of the inner ring road. It is like the city's spinal cord, dividing it into the northern sector with Maitama and Wuse, and the southern sector with Garki and Asokoro. While each district has its own clearly demarcated commercial and residential sectors, the Central District is the city's principal Business Zone, where practically all parastatals and multinational corporations have their offices. An attractive area in the Central District is the region known as the Three Arms Zone, so called because it houses the administrative offices of the executive, legislative and judicial arms of the federal government. A few of the other sites worth seeing in the area are the federal secretariats alongside Shehu Shagari Way, Aso Hill, the Abuja Plant Nursery, Eagle Square (which has important historic significance, as it was in this grounds that the present democratic dispensation had its origin on 29 May 1999) and the Tomb of the Unknown Soldier across the road facing it. The National Mosque and National Church of Nigeria are opposite each other on either side of Independence Avenue.[17] A well-known government office is the Ministry of Defense, colloquially nicknamed \\"Ship House\\".[18]\\r\\nThe Garki District is the area in the southwest corner of the city, having the Central District to the north and the Asokoro District to the east. The district is subdivided into units called \\"Areas\\". Garki uses a distinctive naming convention of \\"Area\\" to refer to parts of Garki. These are designated as Areas 1 to 11. Garki II is used to differentiate the area from Garki Area 2. Visitors may find this system confusing.\\r\\nGarki is presently the principal business district of Abuja. Numerous buildings of interest are in this area. Some of them include the General Post Office, Abuja International Conference Centre along the busy Herbert Maculay Way, Nicon Luxury Hotel (formally known as Abuja Sofitel Hotel and Le Meridian), Agura Hotel and Old Federal Secretariat Complex Buildings (Area 1). A new five-star hotel, Hawthorn Suites Abuja, is in Garki.\\r\\nArea 2 is mainly used for residential purposes, although a zoological garden as well as Garki Shopping Centre are in Area 2. Several banks and other commercial offices are located along Moshood Abiola Way in Area 7. The headquarters of the Nigerian Armed Forces ÿ Army, Airforce and Navy ÿ are all in the Garki District.\\r\\nThe tallest building in this district is the Radio House, which houses the Federal Ministry of Information and Communications, the Federal Radio Corporation of Nigeria (FRCN) and Voice of Nigeria (VON). The Nigerian Television Authority (NTA) stations and corporate headquarters are in Garki. The Federal Capital Development Authority (FCDA) which oversees and runs the Administration of the Federal Capital Territory has its offices in Garki.\\r\\nThe Office of the Minister of the Federal Capital Territory, Abuja is in Area 10. Other places of note include the Arts and Culture Centre and The Nigerian Police Mobile Force headquarters in Area 10. The Abuja Municipal Area Council, which is the local government administration has its headquarters in Area 10. The new United States Embassy is in the Garki District.\\r\\nWuse District is the northwestern part of the city, with the Maitama District to its north and the Central District to its south. The District is numbered Zones 1ÿ8. The Wuse Market is Abuja's principal market (Zone 5). The second most important post office in the city is here. This district houses the Sheraton Hotel and Towers (Zone 4), Ibro International hotel, the Foreign Affairs Ministry Headquarters (Zone 3) and Nigerian Customs Services Headquarters, Federal Civil Service Commission (Zone 3), Federal Road Safety Commission (FRSC), National Agency for Food and Drugs Administration (NAFDAC) (Zone 7), Wuse General Hospital, and the Nigerian Tourism Development Corporation. Just as Garki District has Garki II, Wuse has Wuse II. This is distinct from Wuse Zone 2.\\r\\nMaitama District is to the north of the city, with the Wuse and Central Districts lying to its southwest and southeast respectively. This area is home to the top bracket sections of society and business, and has the reputation of being very exclusive and very expensive. Interesting buildings include the Transcorp Hilton Hotel, Nigerian Communications Commission Headquarters (NCC), National Universities Commission (NUC), Soil Conservation Complex, and Independent National Electoral Commission (INEC). The British High Commission is located along Aguiyi Ironsi Way, in Maitama. Also, the Maitama District Hospital is another notable building in Maitama. Maitama District is home to many of the European and Asian embassies.\\r\\nAsokoro District, the doyen of the districts, houses all of the state's lodges/guest houses. The ECOWAS secretariat is a focal point of interest. Asokoro is to the east of Garki District and south of Central District. It is one of the most exclusive districts of Abuja and houses virtually all of the federal cabinet ministers; in addition, the Presidential Palace (commonly referred to as the Aso Rock) is in Asokoro District. By virtue of this fact, Asokoro is the most secure area of the city.\\r\\nGwarinpa is the last district in the Abuja Municipal Area Council. It is a 20-kilometre (12?mi) drive from the central district and contains the largest single housing estate in Nigeria, the Gwarinpa Housing Estate. The estate was built by the administration of General Sani Abacha and is the largest of its kind in Africa. It provides residence for the majority of the civil servants in federal ministries and government parastatals. The ECOWAS Court has an official quarters for the President and Members of the Court in Gwarinpa.\\r\\nDurumi District is located southwest of Abuja and is bordered by Garki Districts I and II to the northeast. Its borders are the Oladipo Dia Road to the southwest, the Nnamdi Azikiwe Express Way to the northeast, and Ahmadu Bello Way to the southeast.[19][20]\\r\\nThe American International School of Abuja is located in the Durumi District.[21][22]\\r\\nAbuja under K?ppen climate classification features a tropical wet and dry climate (K?ppen: Aw). The FCT experiences three weather conditions annually. This includes a warm, humid rainy season and a blistering dry season. In between the two, there is a brief interlude of harmattan occasioned by the northeast trade wind, with the main feature of dust haze and dryness.\\r\\nThe rainy season begins from April and ends in October, when daytime temperatures reach 28?C (82.4?F) to 30?C (86.0?F) and nighttime lows hover around 22?C (71.6?F) to 23?C (73.4?F). In the dry season, daytime temperatures can soar as high as 40?C (104.0?F) and nighttime temperatures can dip to 12?C (53.6?F). Even the chilliest nights can be followed by daytime temperatures well above 30?C (86.0?F). The high altitudes and undulating terrain of the FCT act as a moderating influence on the weather of the territory.\\r\\nRainfall in the FCT reflects the territory's location on the windward side of the Jos Plateau and the zone of rising air masses with the city receiving frequent rainfall during the rainy season from April to October every year.[23]\\r\\nThe FCT falls within the Guinean forest-savanna mosaic zone of the West African sub-region. Patches of rain forest, however, occur in the Gwagwa plains, especially in the rugged terrain to the south southeastern parts of the territory, where a landscape of gullies and rough terrain is found. These areas of the Federal Capital Territory (FCT) form one of the few surviving occurrences of the mature forest vegetation in Nigeria.\\r\\nThe Abuja skyline is made up of mostly mid-range and a few tall buildings. Only recently have tall buildings begun to appear. Most of the buildings are modern, reflecting that it is a new city.\\r\\nPlans were made to build skyscrapers such as the Millennium Tower which is partly completed. This structure looms 170 metres (560?ft) above the city. The tower is part of a huge cultural development complex called the Nigeria National Complex including the Nigeria Cultural Centre, a 120,000?m2 (1,300,000?sq?ft) structure dedicated to the art and culture of Nigeria. The Cultural Centre and the Millennium Tower have been designed by the Italian architect Manfredi Nicoletti.\\r\\nLandmarks include the Millennium Tower, the Central Bank of Nigeria headquarters, the Nigerian Presidential Complex, the Ship House, the National Stadium, National Mosque, the National Church, Aso Rock and Zuma Rock.\\r\\nAbuja City Gate\\r\\nZuma rock\\r\\nAbuja National Mosque\\r\\nNnamdi Azikiwe International Airport is the main airport serving Abuja and the surrounding capital region. It was named after Nigeria's first president, Nnamdi Azikiwe. The airport has international and domestic terminals.\\r\\nAbuja is on the route of the planned LagosÿKano Standard Gauge Railway, which has been completed between Abuja and Kaduna. Trains for Kaduna depart from the Idu Railway Station in Abuja. There is a motor park at the train station for passengers traveling to the city centre.[25] A light rail system is now under construction, including a station at Idu.\\r\\nAbuja is home to several parks and green areas with the largest one being Millennium Park. Millennium Park was designed by world-renowned architect Manfredi Nicoletti and was officially opened by the United Kingdom's Elizabeth II in December 2003. Another open area park is located in Lifecamp Gwarimpa; near the residence of the Minister of the Federal Capital Territory. The park is located on a slightly raised hilltop which contains sport facilities like Basketball and Badminton courts another park is the city park, it is located in wuse 2 and is home to numerous outdoor and indoor attractions such as a 4D cinema, astro-turf, lawn tennis court, paintball arena and a variety of restaurants.\\r\\nAbuja has a variety of informal spaces known as \\"Bush Bars\\" that usually, though not always, include a covered area with tables and chairs where people can sit and have drinks (alcoholic and non-alcoholic) and sometimes there are snacks such as suya, grilled catfish, pounded yam, egusi soup and other small items available for purchase and they are located all over Abuja.[26][27]\\r\\nAbuja is served by the Nigerian Postal Service which maintains postal codes, street names and zones.[28] Postal codes and district names can also be found on geocodes.com.[29]\\r\\n Media related to Abuja at Wikimedia Commons","input":"When was fct moved from lagos to abuja?"},{"output":"Villanova","context":"The 1985 NCAA Division I Men's Basketball Tournament involved 64 schools playing in single-elimination play to determine the national champion of men's NCAA Division I college basketball. This was the first year the field was expanded to 64 teams, from 53 in the previous year's tournament. It began on March 14, 1985, and ended with the championship game on April 1 in Lexington, Kentucky. A total of 63 games were played.\\r\\nEighth-seed Villanova, coached by Rollie Massimino, won their first national title with a 66ÿ64 victory in the final game over Georgetown, coached by John Thompson. Ed Pinckney of Villanova was named the tournament's Most Outstanding Player. The game, often cited as \\"The Perfect Game\\", is widely considered among the greatest upsets in college basketball history, and is the second biggest point-spread upset in Championship Game history.[1][2] This Villanova team remains the lowest-seeded team to win the tournament. The Wildcats are also notable as the last Division I men's national champion to date to represent a school that did not sponsor varsity football at the time of its title (Villanova had dropped football after the 1980 season and did not reinstate the sport until the 1985 season, the first after the championship game). The game is also notable as the last played without a shot clock.\\r\\nThis year's Final Four saw an unprecedented and unmatched three teams from the same conference, with Big East members Villanova and Georgetown joined by St. John's. The only \\"interloper\\" in the Big East party was Memphis State, then of the Metro Conference. (Memphis State's 1985 Final Four appearance was vacated due to using ineligible players, as were all of its tournament appearances from 1982ÿ1986.)\\r\\nThis was also the first year that one of the regionals was named \\"Southeast\\", replacing \\"Mideast.\\" This name was used until 1998, when the regional was renamed \\"South.\\" This was also the last tournament until 2010 to feature two private schools in the title game. This tournament was also the last until 2012 to feature no teams in the Sweet 16 from the Mountain or Pacific Time Zones.\\r\\nThis tournament's East Region is the only one in NCAA Tournament history in which the higher-seeded team won every game.\\r\\n\\r\\n\\r\\nLexington became the 21st host city, and Rupp Arena the 23rd host venue, for the Final Four. Lexington is the smallest metropolitan area to host a Final Four, and due to the use of domed football stadiums, it is unlikely to host a Final Four again despite the size of Rupp Arena. The 1985 tournament was the last time an off-campus arena (or, for that matter, any arena) whose primary tenant was a college team was used for a tournament. (The Continental Airlines Arena (in 1996) was the main arena for Seton Hall Pirates, but they were not the primary tenants.) This tournament also marks the last time a domed stadium was not used for any tournament games; before 1985, the four previous tournaments and the 1971 tournament were the only tournaments to include them. The other implication of this is that it was the only tournament between 1984 and 1993 to not feature an NFL stadium. Denver was the only new city or venue host games in 1985. At the time, the city did not host a NCAA Division I institution, making it just one of a handful of host cities all-time to do so. 1985 would be the last time the Providence Civic Center would host the regional rounds; all subsequent tournaments would be early rounds. The tournament would also mark the last time the University of Tulsa's Mabee Center would host games; the tournament would not return to the city until 2011, when the BOK Center hosted.\\r\\n* ÿ Denotes overtime period\\r\\nCBS Sports\\r\\nESPN and NCAA Productions\\r\\nCBS Radio","input":"Who won the 1985 ncaa men's basketball championship?"},{"output":"Bertha Benz","context":"?Bertha Benz?(help{info) (ne Ringer, 3 May 1849 ÿ 5 May 1944) was a German automotive pioneer. She was the wife and business partner of automobile inventor Karl Benz. In 1888, she was the first person to drive an automobile over a long distance.[1] In doing so, she brought the Benz Patent-Motorwagen worldwide attention and got the company its first sales.\\r\\n\\r\\n\\r\\nBertha Ringer was born in 1849 to a wealthy family in Pforzheim, Grand Duchy of Baden.[citation needed]\\r\\nTwo years before her marriage to Karl Benz, she used part of her dowry to invest in his failing iron construction company.[2] As an unmarried woman, she was able to do so; after she married Benz, according to German law, Bertha lost her legal power to act as an investor.[3] On 20 July 1872, Bertha Ringer married Karl Benz. As he moved on to a new manufacturing venture, Benz & Cie, he continued to use her dowry as financial support. He finished his work on his first horseless carriage in December 1885. Although Bertha financed the development process, and would hold patent rights under modern law, as a married woman she was not allowed to apply for the patent.[4]\\r\\nTogether they had five children: Eugen (1873), Richard (1874), Clara (1877), Thilde (1882), and Ellen (1890).[citation needed]\\r\\nIn August 1888, without telling her husband and without permission of the authorities, 39-year-old Bertha Benz drove with her sons Richard and Eugen, thirteen and fifteen years old, in the newly constructed Patent Motorwagen automobilefrom Mannheim to Pforzheimbecoming the first person to drive an automobile over a real distance.[1] Motorized drives before this historic trip were merely very short trial drives, returning to the point of origin, made with mechanical assistants. Following wagon tracks, this pioneering tour had a one-way distance of about 106?km (66?mi).[5][6]\\r\\nAlthough the ostensible purpose of the trip was to visit her mother, Bertha Benz had other motives: to prove to her husbandwho had failed to consider marketing his invention adequatelythat the automobile they both heavily invested in would become a financial success once it was shown to be useful to the general public; and to give her husband the confidence that his constructions had a future.[7]\\r\\nShe left Mannheim around dawn, solving numerous problems along the way.[8] Bertha demonstrated her significant technical capabilities on this journey.[9] With no fuel tank and only a 4.5-litre supply of petrol in the carburetor, she had to find ligroin, the petroleum solvent needed for the car to run. It was only available at apothecary shops, so she stopped in Wiesloch at the city pharmacy to purchase the fuel.[10] At the time petrol and other fuels could only be bought from chemists and so this is how the chemist in Wiesloch became the first fuel station in the world.[9]\\r\\nShe even cleaned a blocked fuel line with her hat pin and used her garter as isolation material.[9] A blacksmith had to help mend a chain at one point. When the wooden brakes began to fail Benz visited a cobbler to install leather, making the world's first pair of brake pads.[2] The thermosiphon system was employed to cool the engine, making water supply a big worry along the trip. The trio added water to their supply every time they stopped.[10] The car's two gears were not enough to surmount uphill inclines and Eugen and Richard often had to push the vehicle.[10] Benz reached Pforzheim somewhat after dusk, notifying her husband of her successful journey by telegram. She drove back to Mannheim several days later.\\r\\nAlong the way, several people were frightened by the automobile. Some even thought that two young boys and a woman on a hissing, thumping horseless carriage could only be the work of the Devil himself.[11] The novel trip received a great deal of publicity, as she had sought. The drive was a key event in the technical development of the automobile. The pioneering couple introduced several improvements after Bertha's experiences. She reported everything that had happened along the way and made important suggestions, such as the introduction of an additional gear for climbing hills and brake linings to improve brake-power. Her trip proved to the burgeoning automotive industry that test drives were essential to their business.[2][10]\\r\\nIn 1886, Benz presented the Patent Motor Car to the world. Within the decade, 25 vehicles had been built. With cutting-edge bicycle constructions, the Model I was the original Patent Motor Car and the world's first automobile.[citation needed]\\r\\nThe Model II was converted to a four-wheeler for test purposes, making it the only one of this model.[citation needed]\\r\\nThe first Patent Motor Car sold in small production runs was the Model III. It had powered rear wheels with a ringed steel and solid rubber, steerable front wheel. Various options were provided for customers to choose from, such as seat arrangements and a folding top.[10]\\r\\nIn 1944, on her 95th birthday, Bertha Benz was honoured with the title Honourable Senator, by the Technical University of Karlsruhe. This is her husband's alma mater and they had awarded an honorary doctorate degree to him in his lifetime. Two days later, Bertha Benz died in her villa in Ladenburg, where the workshop of Karl Benz had been built after they had moved there in 1906 and he established a solely family-held business, Benz and Sons. Karl Benz later wrote the following in his memoirs: \\"Only one person remained with me in the small ship of life when it seemed destined to sink. That was my wife. Bravely and resolutely she set the new sails of hope.\\"[11]\\r\\nIn 2008, the Bertha Benz Memorial Route [12] was officially approved as a route of the industrial heritage of mankind, because it follows Bertha Benz's path during the world's first long-distance journey by automobile in 1888. Now it is possible to follow the 194?km of signs indicating her route from Mannheim via Heidelberg to Pforzheim (Black Forest) and back.\\r\\nThe Bertha Benz Challenge, embedded in the framework of the ceremony of Automobile Summer 2011, the big official German event and birthday party commemorating the invention of the automobile by Karl Benz 125 years ago, took place on Bertha Benz Memorial Route on 10 and 11 September 2011. It was a globally visible signal for new automobile breakthroughs, and was only open for sustainable mobility: future-oriented vehicles with alternative drive systems ÿ hybrid and electric, hydrogen and fuel cell vehicles ÿ and other extremely economical vehicles. The motto is: Bertha Benz Challenge ÿ Sustainable Mobility on the World's Oldest Automobile Road!.[13][14]\\r\\nOn 25 January 2011 Deutsche Welle (DW-TV) broadcast worldwide in its series, Made in Germany, a TV documentary on the invention of the automobile by Karl Benz, highlighting the very important role of his wife, Bertha Benz. The report is not only on the history of the automobile, but takes a look at its future as well, shown by the Bertha Benz Challenge on 10 and 11 September 2011.[13]\\r\\nThe documentary The Car is Born produced by Ulli Kampelmann centered on the first road trip by Bertha Benz.[15]\\r\\nIn 2011, a television movie about the life of Karl and Bertha Benz was made named Karl & Bertha which premiered on 11 May[16] and was aired by Das Erste on 23 May.[17][18] A trailer of the movie[19] and a \\"making of\\" special were released on YouTube.[20]","input":"Who drove the first car in the world?"},{"output":"25 February 1960","context":"Cardiac surgery, or cardiovascular surgery, is surgery on the heart or great vessels performed by cardiac surgeons. It is often used to treat complications of ischemic heart disease (for example, with coronary artery bypass grafting); to correct congenital heart disease; or to treat valvular heart disease from various causes, including endocarditis, rheumatic heart disease, and atherosclerosis. It also includes heart transplantation.\\r\\n\\r\\n\\r\\nThe earliest operations on the pericardium (the sac that surrounds the heart) took place in the 19th century and were performed by Francisco Romero (1801),[1] Dominique Jean Larrey (1810), Henry Dalton (1891), and Daniel Hale Williams (1893).[2] The first surgery on the heart itself was performed by Axel Cappelen on 4 September 1895 at Rikshospitalet in Kristiania, now Oslo. Cappelen ligated a bleeding coronary artery in a 24-year-old man who had been stabbed in the left axilla and was in deep shock upon arrival. Access was through a left thoracotomy. The patient awoke and seemed fine for 24 hours, but became ill with a fever and died three days after the surgery from mediastinitis.[3][4]\\r\\nThe first successful surgery on the heart, without any complications, was performed by Dr. Ludwig Rehn of Frankfurt, Germany, who repaired a stab wound to the right ventricle on 7 September 1896.[5][6]\\r\\nSurgery on the great vessels (e.g., aortic coarctation repair, Blalock-Thomas-Taussig shunt creation, closure of patent ductus arteriosus) became common after the turn of the century. However, operations on the heart valves were unknown until, in 1925, Henry Souttar operated successfully on a young woman with mitral valve stenosis. He made an opening in the appendage of the left atrium and inserted a finger in order to palpate and explore the damaged mitral valve. The patient survived for several years,[7] but Souttar's colleagues considered the procedure unjustified, and he could not continue.[8][9]\\r\\nCardiac surgery changed significantly after World War II. In 1947, Thomas Holmes Sellors (1902ÿ1987) of Middlesex Hospital in London operated on a Tetralogy of Fallot patient with pulmonary stenosis and successfully divided the stenosed pulmonary valve. In 1948, Russell Brock, probably unaware of Sellors's work, used a specially designed dilator in three cases of pulmonary stenosis. Later that year, he designed a punch to resect a stenosed infundibulum, which is often associated with Tetralogy of Fallot. Many thousands of these \\"blind\\" operations were performed until the introduction of cardiopulmonary bypass made direct surgery on valves possible.[8]\\r\\nAlso in 1948, four surgeons carried out successful operations for mitral valve stenosis resulting from rheumatic fever. Horace Smithy (1914ÿ1948) of Charlotte used a valvulotome to remove a portion of a patient's mitral valve,[10] while three other doctorsCharles Bailey (1910ÿ1993) of Hahnemann University Hospital in Philadelphia; Dwight Harken in Boston; and Russell Brock of Guy's Hospital in Londonadopted Souttar's method. All four men began their work independently of one another within a period of a few months. This time, Souttar's technique was widely adopted, with some modifications.[8][9]\\r\\nThe first successful intracardiac correction of a congenital heart defect using hypothermia was performed by Drs. C. Walton Lillehei and F. John Lewis at the University of Minnesota on 2 September 1952. In 1953, Alexander Alexandrovich Vishnevsky conducted the first cardiac surgery under local anesthesia. In 1956, Dr. John Carter Callaghan performed the first documented open heart surgery in Canada.\\r\\nAlfred Blalock, Helen Taussig and Vivien Thomas performed the first successful pediatric cardiac operation at Johns Hopkins Hospital on November, 29 1944, a total repair of Tetralogy of Fallot in a one-year-old girl.[11]\\r\\nIn open heart surgery, the patient's heart is opened and surgery is performed on its internal structures.\\r\\nDr. Wilfred G. Bigelow of the University of Toronto found that such procedures could be performed better in a bloodless and motionless environment. Therefore, during open heart surgery, the heart is temporarily stopped, and the patient is placed on cardiopulmonary bypass, meaning a machine pumps their blood and oxygen. Because the machine cannot function the same way as the heart, surgeons try to minimize the time a patient spends on it.[12]\\r\\nCardiopulmonary bypass was developed after surgeons realized the limitations of hypothermia in cardiac surgery: Complex intracardiac repairs take time, and the patient needs blood flow to the body (particularly to the brain), as well as heart and lung function. In 1953, Dr. John Heysham Gibbon of Jefferson Medical School in Philadelphia reported the first successful use of extracorporeal circulation by means of an oxygenator, but he abandoned the method after subsequent failures. In 1954, Dr. Lillehei performed a series of successful operations with the controlled cross-circulation technique, in which the patient's mother or father was used as a \\"heart-lung machine\\". Dr. John W. Kirklin at the Mayo Clinic was the first to use a Gibbon-type pump-oxygenator.\\r\\nNazih Zuhdi performed the first total intentional hemodilution open heart surgery on Terry Gene Nix, age 7, on 25 February 1960 at Mercy Hospital in Oklahoma City. The operation was a success; however, Nix died three years later.[13] In March 1961, Zuhdi, Carey, and Greer performed open heart surgery on a child, age ?3?1?2, using the total intentional hemodilution machine.\\r\\nIn the early 1990s, surgeons began to perform off-pump coronary artery bypass, done without cardiopulmonary bypass. In these operations, the heart continues beating during surgery, but is stabilized to provide an almost still work area in which to connect a conduit vessel that bypasses a blockage using a technique known as endoscopic vessel harvesting (EVH).\\r\\nIn 1945, the Soviet pathologist Nikolai Sinitsyn successfully transplanted a heart from one frog to another frog and from one dog to another dog.\\r\\nNorman Shumway is widely regarded as the father of human heart transplantation, although the world's first adult heart transplant was performed by a South African cardiac surgeon, Christiaan Barnard, using techniques developed by Shumway and Richard Lower.[14] Barnard performed the first transplant on Louis Washkansky on 3 December 1967 at Groote Schuur Hospital in Cape Town.[14][15] Adrian Kantrowitz performed the first pediatric heart transplant on 6 December 1967 at Maimonides Hospital (now Maimonides Medical Center) in Brooklyn, New York, barely three days later.[14] Shumway performed the first adult heart transplant in the United States on 6 January 1968 at Stanford University Hospital.[14]\\r\\nCoronary artery bypass grafting, also called revascularization, is a common surgical procedure to create an alternative path to deliver blood supply to the heart and body, with the goal of preventing clot formation. This can be done in many ways, and the arteries used can be taken from several areas of the body.[16] Arteries are typically harvested from the chest, arm, or wrist and then attached to a portion of the coronary artery, relieving pressure and limiting clotting factors in that area of the heart.[17]\\r\\nThe procedure is typically performed because of coronary artery disease (CAD), in which a plaque-like substance builds up in the coronary artery, the main pathway carrying oxygen-rich blood to the heart. This can cause a blockage and/or a rupture, which can lead to a heart attack.[17]\\r\\nAs an alternative to open heart surgery, which involves a five- to eight-inch incision in the chest wall, a surgeon may perform an endoscopic procedure by making very small incisions through which a camera and specialized tools are inserted.[18]\\r\\nIn robot-assisted heart surgery, a machine controlled by a cardiac surgeon is used to perform a procedure. The main advantage to this is the size of the incision required: three small holes instead of an incision big enough for the surgeon's hands.[citation needed]\\r\\nAs with any surgical procedure, cardiac surgery requires postoperative precautions to avoid complications. Incision care is needed to avoid infection and minimize scarring. Swelling and loss of appetite are common.[19][20]\\r\\nRecovery from open heart surgery begins with about 48 hours in an intensive care unit, where heart rate, blood pressure, and oxygen levels are closely monitored. Chest tubes are inserted to drain blood around the heart and lungs. After discharge from the hospital, compression socks may be recommended in order to regulate blood flow.[21]\\r\\nThe advancement of cardiac surgery and cardiopulmonary bypass techniques has greatly reduced the mortality rates of these procedures. For instance, repairs of congenital heart defects are currently estimated to have 4ÿ6% mortality rates.[22][23]\\r\\nA major concern with cardiac surgery is neurological damage. Stroke occurs in 2ÿ3% of all people undergoing cardiac surgery, and the rate is higher in patients with other risk factors for stroke.[24] A more subtle complication attributed to cardiopulmonary bypass is postperfusion syndrome, sometimes called \\"pumphead\\". The neurocognitive symptoms of postperfusion syndrome were initially thought to be permanent,[25] but turned out to be transient, with no permanent neurological impairment.[26]\\r\\nIn order to assess the performance of surgical units and individual surgeons, a popular risk model has been created called the EuroSCORE. It takes a number of health factors from a patient and, using precalculated logistic regression coefficients, attempts to quantify the probability that they will survive to discharge. Within the United Kingdom, the EuroSCORE was used to give a breakdown of all cardiothoracic surgery centres and to indicate whether the units and their individuals surgeons performed within an acceptable range. The results are available on the Care Quality Commission website.[27]\\r\\nAnother important source of complications are the neuropsychological and psychopathologic changes following open heart surgery. One example is Skumin syndrome, described by Victor Skumin in 1978, which is a \\"cardioprosthetic psychopathological syndrome\\"[28] associated with mechanical heart valve implants and characterized by irrational fear, anxiety, depression, sleep disorder, and weakness.[29][30]\\r\\nA 2012 Cochrane systematic review found evidence that preoperative physical therapy reduced postoperative pulmonary complications, such as pneumonia and atelectasis, in patients undergoing elective cardiac surgery.[31] In addition, the researchers found that preoperative physical therapy decreased the length of hospital stay by more than three days on average.[31]\\r\\nA 2013 Cochrane review showed that both pharmacological and non-pharmacological prevention reduce the risk of atrial fibrillation after an operation and reduced the length of hospital stays. No difference in mortality could be shown.[32]\\r\\nThere is evidence that quitting smoking at least four weeks before surgery may reduce the risk of postoperative complications.[33]","input":"When was the first open heart bypass surgery performed?"},{"output":"electron","context":"A covalent bond, also called a molecular bond, is a chemical bond that involves the sharing of electron pairs between atoms. These electron pairs are known as shared pairs or bonding pairs, and the stable balance of attractive and repulsive forces between atoms, when they share electrons, is known as covalent bonding.[1][better?source?needed] For many molecules, the sharing of electrons allows each atom to attain the equivalent of a full outer shell, corresponding to a stable electronic configuration.\\r\\nCovalent bonding includes many kinds of interactions, including -bonding, -bonding, metal-to-metal bonding, agostic interactions, bent bonds, and three-center two-electron bonds.[2][3] The term covalent bond dates from 1939.[4] The prefix co- means jointly, associated in action, partnered to a lesser degree, etc.; thus a \\"co-valent bond\\", in essence, means that the atoms share \\"valence\\", such as is discussed in valence bond theory.\\r\\nIn the molecule H\\r\\n2, the hydrogen atoms share the two electrons via covalent bonding.[5] Covalency is greatest between atoms of similar electronegativities. Thus, covalent bonding does not necessarily require that the two atoms be of the same elements, only that they be of comparable electronegativity. Covalent bonding that entails sharing of electrons over more than two atoms is said to be delocalized.\\r\\n\\r\\n\\r\\nThe term covalence in regard to bonding was first used in 1919 by Irving Langmuir in a Journal of the American Chemical Society article entitled \\"The Arrangement of Electrons in Atoms and Molecules\\". Langmuir wrote that \\"we shall denote by the term covalence the number of pairs of electrons that a given atom shares with its neighbors.\\"[6]\\r\\nThe idea of covalent bonding can be traced several years before 1919 to Gilbert N. Lewis, who in 1916 described the sharing of electron pairs between atoms.[7] He introduced the Lewis notation or electron dot notation or Lewis dot structure, in which valence electrons (those in the outer shell) are represented as dots around the atomic symbols. Pairs of electrons located between atoms represent covalent bonds. Multiple pairs represent multiple bonds, such as double bonds and triple bonds. An alternative form of representation, not shown here, has bond-forming electron pairs represented as solid lines.\\r\\nLewis proposed that an atom forms enough covalent bonds to form a full (or closed) outer electron shell. In the diagram of methane shown here, the carbon atom has a valence of four and is, therefore, surrounded by eight electrons (the octet rule), four from the carbon itself and four from the hydrogens bonded to it. Each hydrogen has a valence of one and is surrounded by two electrons (a duet rule) ÿ its own one electron plus one from the carbon. The numbers of electrons correspond to full shells in the quantum theory of the atom; the outer shell of a carbon atom is the n?=?2 shell, which can hold eight electrons, whereas the outer (and only) shell of a hydrogen atom is the n?=?1 shell, which can hold only two.\\r\\nWhile the idea of shared electron pairs provides an effective qualitative picture of covalent bonding, quantum mechanics is needed to understand the nature of these bonds and predict the structures and properties of simple molecules. Walter Heitler and Fritz London are credited with the first successful quantum mechanical explanation of a chemical bond (molecular hydrogen) in 1927.[8] Their work was based on the valence bond model, which assumes that a chemical bond is formed when there is good overlap between the atomic orbitals of participating atoms.\\r\\nAtomic orbitals (except for s orbitals) have specific directional properties leading to different types of covalent bonds. Sigma () bonds are the strongest covalent bonds and are due to head-on overlapping of orbitals on two different atoms. A single bond is usually a  bond. Pi () bonds are weaker and are due to lateral overlap between p (or d) orbitals. A double bond between two given atoms consists of one  and one  bond, and a triple bond is one  and two  bonds.\\r\\nCovalent bonds are also affected by the electronegativity of the connected atoms which determines the chemical polarity of the bond. Two atoms with equal electronegativity will make nonpolar covalent bonds such as HÿH. An unequal relationship creates a polar covalent bond such as with H?Cl. However polarity also requires geometric asymmetry, or else dipoles may cancel out resulting in a non-polar molecule.\\r\\nThere are several types of structures for covalent substances, including individual molecules, molecular structures, macromolecular structures and giant covalent structures. Individual molecules have strong bonds that hold the atoms together, but there are negligible forces of attraction between molecules. Such covalent substances are usually gases, for example, HCl, SO2, CO2, and CH4. In molecular structures, there are weak forces of attraction. Such covalent substances are low-boiling-temperature liquids (such as ethanol), and low-melting-temperature solids (such as iodine and solid CO2). Macromolecular structures have large numbers of atoms linked by covalent bonds in chains, including synthetic polymers such as polyethylene and nylon, and biopolymers such as proteins and starch. Network covalent structures (or giant covalent structures) contain large numbers of atoms linked in sheets (such as graphite), or 3-dimensional structures (such as diamond and quartz). These substances have high melting and boiling points, are frequently brittle, and tend to have high electrical resistivity. Elements that have high electronegativity, and the ability to form three or four electron pair bonds, often form such large macromolecular structures.[9]\\r\\nBonds with one or three electrons can be found in radical species, which have an odd number of electrons. The simplest example of a 1-electron bond is found in the dihydrogen cation, H+\\r\\n2. One-electron bonds often have about half the bond energy of a 2-electron bond, and are therefore called \\"half bonds\\". However, there are exceptions: in the case of dilithium, the bond is actually stronger for the 1-electron Li+\\r\\n2 than for the 2-electron Li2. This exception can be explained in terms of hybridization and inner-shell effects.[10]\\r\\nThe simplest example of three-electron bonding can be found in the helium dimer cation, He+\\r\\n2. It is considered a \\"half bond\\" because it consists of only one shared electron (rather than two); in molecular orbital terms, the third electron is in an anti-bonding orbital which cancels out half of the bond formed by the other two electrons. Another example of a molecule containing a 3-electron bond, in addition to two 2-electron bonds, is nitric oxide, NO. The oxygen molecule, O2 can also be regarded as having two 3-electron bonds and one 2-electron bond, which accounts for its paramagnetism and its formal bond order of 2.[12] Chlorine dioxide and its heavier analogues bromine dioxide and iodine dioxide also contain three-electron bonds.\\r\\nMolecules with odd-electron bonds are usually highly reactive. These types of bond are only stable between atoms with similar electronegativities.[12]\\r\\nThere are situations whereby a single Lewis structure is insufficient to explain the electron configuration in a molecule, hence a superposition of structures are needed. The same two atoms in such molecules can be bonded differently in different structures (a single bond in one, a double bond in another, or even none at all), resulting in a non-integer bond order. The nitrate ion is one such example with three equivalent structures. The bond between the nitrogen and each oxygen is a double bond in one structure and a single bond in the other two, so that the average bond order for each NÿO interaction is 2 + 1 + 1/3 = 4/3.\\r\\n\\r\\nIn organic chemistry, when a molecule with a planar ring obeys Hckel's rule, where the number of  electrons fit the formula 4n?+?2 (where n is an integer), it attains extra stability and symmetry. In benzene, the prototypical aromatic compound, there are 6  bonding electrons (n?=?1, 4n?+?2?=?6). These occupy three delocalized  molecular orbitals (molecular orbital theory) or form conjugate  bonds in two resonance structures that linearly combine (valence bond theory), creating a regular hexagon exhibiting a greater stabilization than the hypothetical 1,3,5-cyclohexatriene.\\r\\nIn the case of heterocyclic aromatics and substituted benzenes, the electronegativity differences between different parts of the ring may dominate the chemical behaviour of aromatic ring bonds, which otherwise are equivalent.\\r\\nCertain molecules such as xenon difluoride and sulfur hexafluoride have higher co-ordination numbers than would be possible due to strictly covalent bonding according to the octet rule. This is explained by the three-center four-electron bond (\\"3cÿ4e\\") model which interprets the molecular wavefunction in terms of non-bonding highest occupied molecular orbitals in molecular orbital theory and ionic-covalent resonance in valence bond theory.\\r\\nIn three-center two-electron bonds (\\"3cÿ2e\\") three atoms share two electrons in bonding. This type of bonding occurs in electron deficient compounds like diborane. Each such bond (2 per molecule in diborane) contains a pair of electrons which connect the boron atoms to each other in a banana shape, with a proton (nucleus of a hydrogen atom) in the middle of the bond, sharing electrons with both boron atoms. In certain cluster compounds, so-called four-center two-electron bonds also have been postulated.\\r\\nAfter the development of quantum mechanics, two basic theories were proposed to provide a quantum description of chemical bonding: valence bond (VB) theory and molecular orbital (MO) theory. A more recent quantum description[13] is given in terms of atomic contributions to the electronic density of states.\\r\\nIn COOP,[14] COHP[15] and BCOOP,[16] evaluation of bond covalency is dependent on the basis set. To overcome this issue, an alternative formulation of the bond covalency can be provided in this way.\\r\\nThe center mass cm(n,l,ml,ms) of an atomic orbital |n,l,ml,ms?, with quantum numbers n, l, ml, ms, for atom A is defined as\\r\\nwhere gA\\r\\n|n,l,ml,ms?(E) is the contribution of the atomic orbital |n,l,ml,ms? of the atom A to the total electronic density of states g(E) of the solid\\r\\nwhere the outer sum runs over all atoms A of the unit cell. The energy window [E0,E1] is chosen in such a way that it encompasses all relevant bands participating in the bond. If the range to select is unclear, it can be identified in practice by examining the molecular orbitals that describe the electron density along the considered bond.\\r\\nThe relative position CnAlA,nBlB of the center mass of |nA,lA? levels of atom A with respect to the center mass of |nB,lB? levels of atom B is given as\\r\\nwhere the contributions of the magnetic and spin quantum numbers are summed. According to this definition, the relative position of the A levels with respect to the B levels is\\r\\nwhere, for simplicity, we may omit the dependence from the principal quantum number n in the notation referring to CnAlA,nBlB.\\r\\nIn this formalism, the greater the value of CA,B, the higher the overlap of the selected atomic bands, and thus the electron density described by those orbitals gives a more covalent AÿB bond. The quantity CA,B is denoted as the covalency of the AÿB bond, which is specified in the same units of the energy E.","input":"What types of elements are involved in covalent bonding?"},{"output":"oral mucosa","context":"The oral mucosa is the mucous membrane lining the inside of the mouth and consists of stratified squamous epithelium termed oral epithelium and an underlying connective tissue termed lamina propria.[1] The oral cavity has sometimes been described as a mirror that reflects the health of the individual.[2] Changes indicative of disease are seen as alterations in the oral mucosa lining the mouth, which can reveal systemic conditions, such as diabetes or vitamin deficiency, or the local effects of chronic tobacco or alcohol use.[3]\\r\\n\\r\\n\\r\\nOral mucosa can be divided into three main categories based on function and histology:\\r\\nOral mucosa consists of two layers, the surface stratified squamous epithelium and the deeper lamina propria. In keratinized oral mucosa, the epithelium consists of four layers:\\r\\nIn nonkeratinised epithelium, the two deep layers (basale and spinosum) remain the same but the outer layers are termed the intermediate and superficial layers.\\r\\nDepending on the region of the mouth, the epithelium may be nonkeratinized or keratinized. Nonkeratinized squamous epithelium covers the soft palate, inner lips, inner cheeks, and the floor of the mouth, and ventral surface of the tongue. Keratinized squamous epithelium is present in the attached gingiva and hard palate as well as areas of the dorsal surface of the tongue.[5][6]\\r\\nKeratinization is the differentiation of keratinocytes in the stratum granulosum into nonvital surface cells or squames to form the stratum corneum. The cells terminally differentiate as they migrate to the surface from the stratum basale where the progenitor cells are located to the superficial surface.\\r\\nUnlike keratinized epithelium, nonkeratinized epithelium normally has no superficial layers showing keratinization. Nonkeratinized epithelium may, however, readily transform into a keratinizing type in response to frictional or chemical trauma, in which case it undergoes hyperkeratinization.This change to hyperkeratinization commonly occurs on the usually nonkeratinized buccal mucosa when the linea alba forms, a white ridge of calloused tissue that extends horizontally at the level where the maxillary and mandibular teeth come together and occlude. Histologically, an excess amount of keratin is noted on the surface of the tissue, and the tissue has all the layers of an orthokeratinized tissue with its granular and keratin layers. In patients who have habits such as clenching or grinding (bruxism) their teeth, a larger area of the buccal mucosa than just the linea alba becomes hyperkeratinized. This larger white, rough, raised lesion needs to be recorded so that changes may be made in the dental treatment plan regarding the patients parafunctional habits.[7][8]\\r\\nEven keratinized tissue can undergo further level of hyperkeratinization; an increase in the amount of keratin is produced as a result of chronic physical trauma to the region. Changes such as hyperkeratinization are reversible if the source of the injury is removed, but it takes time for the keratin to be shed or lost by the tissue. Thus, to check for malignant changes, a baseline biopsy and microscopic study of any whitened tissue may be indicated, especially if in a high-risk cancer category, such with a history of tobacco or alcohol use or are HPV positive. Hyperkeratinized tissue is also associated with the heat from smoking or hot fluids on the hard palate in the form of nicotinic stomatitis.[7]\\r\\nThe lamina propria is a fibrous connective tissue layer that consists of a network of type I and III collagen and elastin fibers in some regions. The main cells of the lamina propria are the fibroblasts, which are responsible for the production of the fibers as well as the extracellular matrix.\\r\\nThe lamina propria, like all forms of connective tissue proper, has two layers: papillary and dense. The papillary layer is the more superficial layer of the lamina propria. It consists of loose connective tissue within the connective tissue papillae, along with blood vessels and nerve tissue. The tissue has an equal amount of fibers, cells, and intercellular substance. The dense layer is the deeper layer of the lamina propria. It consists of dense connective tissue with a large amount of fibers. Between the papillary layer and the deeper layers of the lamina propria is a capillary plexus, which provides nutrition for the all layers of the mucosa and sends capillaries into the connective tissue papillae.[7]\\r\\nA submucosa may or may not be present deep in the dense layer of the lamina propria, depending on the region of the oral cavity. If present, the submucosa usually contains loose connective tissue and may also contain adipose tissue or salivary glands, as well as overlying bone or muscle within the oral cavity.[7]\\r\\nA variable number of Fordyce spots or granules are scattered throughout the nonkeratinized tissue. These are a normal variant, visible as small, yellowish bumps on the surface of the mucosa. They correspond to deposits of sebum from misplaced sebaceous glands in the submucosa that are usually associated with hair follicles.[7]\\r\\nA basal lamina (basement membrane without aid of the microscope) is at the interface between the oral epithelium and lamina propria similar to the epidermis and dermis.[9]","input":"What is the lining of the mouth called?"},{"output":"Suez Canal Company","context":"The Suez Canal (Arabic: ???? ???????? qant as-suws) is an artificial sea-level waterway in Egypt, connecting the Mediterranean Sea to the Red Sea through the Isthmus of Suez. Constructed by the Suez Canal Company between 1859 and 1869, it was officially opened on November 17, 1869. The canal offers watercraft a shorter journey between the North Atlantic and northern Indian Oceans via the Mediterranean and Red seas by avoiding the South Atlantic and southern Indian oceans, in turn reducing the journey by approximately 7,000 kilometres (4,300?mi). It extends from the northern terminus of Port Said to the southern terminus of Port Tewfik at the city of Suez. Its length is 193.30?km (120.11?mi), including its northern and southern access channels. In 2012, 17,225 vessels traversed the canal (47 per day).[1]\\r\\nThe original canal was a single-lane waterway with passing locations in the Ballah Bypass and the Great Bitter Lake.[2] It contains no locks system, with seawater flowing freely through it. In general, the canal north of the Bitter Lakes flows north in winter and south in summer. South of the lakes, the current changes with the tide at Suez.[3]\\r\\nThe canal is owned and maintained by the Suez Canal Authority[4] (SCA) of Egypt. Under the Convention of Constantinople, it may be used \\"in time of war as in time of peace, by every vessel of commerce or of war, without distinction of flag\\".[5]\\r\\nIn August 2014, construction was launched to expand and widen the Ballah Bypass for 35?km (22?mi) to speed the canal's transit time. The expansion was planned to double the capacity of the Suez Canal from 49 to 97 ships a day.[6] At a cost of $8.4 billion, this project was funded with interest-bearing investment certificates issued exclusively to Egyptian entities and individuals. The \\"New Suez Canal\\", as the expansion was dubbed, was opened with great fanfare in a ceremony on 6 August 2015.[7]\\r\\nOn 24 February 2016, the Suez Canal Authority officially opened the new side channel. This side channel, located at the northern side of the east extension of the Suez Canal, serves the East Terminal for berthing and unberthing vessels from the terminal. As the East Container Terminal is located on the Canal itself, before the construction of the new side channel it was not possible to berth or unberth vessels at the terminal while the convoy was running.[8]\\r\\n\\r\\n\\r\\nAncient westÿeast canals were built to facilitate travel from the Nile River to the Red Sea.[9][10][11] One smaller canal is believed to have been constructed under the auspices of Senusret II[12] or Ramesses II.[9][10][11] Another canal, probably incorporating a portion of the first,[9][10] was constructed under the reign of Necho II, but the only fully functional canal was engineered and completed by Darius I.[9][10][11]\\r\\nThe legendary Sesostris (likely either Pharaoh Senusret II or Senusret III of the Twelfth dynasty of Egypt[12][13]) may have started work on an ancient canal joining the Nile with the Red Sea (1897 BC ÿ 1839 BC), when an irrigation channel was constructed around 1850 BC that was navigable during the flood season, leading into a dry river valley east of the Nile River Delta named Wadi Tumelat.[14] (It is said that in ancient times the Red Sea reached northward to the Bitter Lakes[9][10] and Lake Timsah.[15][16])\\r\\nIn his Meteorology, Aristotle wrote:\\r\\nOne of their kings tried to make a canal to it (for it would have been of no little advantage to them for the whole region to have become navigable; Sesostris is said to have been the first of the ancient kings to try), but he found that the sea was higher than the land. So he first, and Darius afterwards, stopped making the canal, lest the sea should mix with the river water and spoil it.[17]\\r\\nStrabo wrote that Sesostris started to build a canal, and Pliny the Elder wrote:\\r\\n165. Next comes the Tyro tribe and, the harbour of the Daneoi, from which Sesostris, king of Egypt, intended to carry a ship-canal to where the Nile flows into what is known as the Delta; this is a distance of over 60 miles. Later the Persian king Darius had the same idea, and yet again Ptolemy II, who made a trench 100 feet wide, 30 feet deep and about 35 miles long, as far as the Bitter Lakes.[18]\\r\\nIn the second half of the 19th century, French cartographers discovered the remnants of an ancient northÿsouth canal past the east side of Lake Timsah and ending near the north end of the Great Bitter Lake.[19] This proved to be the celebrated canal made by the Persian king Darius I, as his stele commemorating its construction was found at the site. (This ancient, second canal may have followed a course along the shoreline of the Red Sea when it once extended north to Lake Timsah.[16][19]) In the 20th century the northward extension of this ancient canal was discovered, extending from Lake Timsah to the Ballah Lakes.[20] This was dated to the Middle Kingdom of Egypt by extrapolating the dates of ancient sites along its course.[20]\\r\\nThe reliefs of the Punt expedition under Hatshepsut, 1470?BC, depict seagoing vessels carrying the expeditionary force returning from Punt. This suggests that a navigable link existed between the Red Sea and the Nile.[21][22] Evidence seems to indicate its existence by the 13th century BC during the time of Ramesses II.[9][23][24][25]\\r\\nRemnants of an ancient westÿeast canal through the ancient Egyptian cities of Bubastis, Pi-Ramesses, and Pithom were discovered by Napoleon Bonaparte and his engineers and cartographers in 1799.[10][26][27][28][29]\\r\\nAccording to the Histories of the Greek historian Herodotus,[30] about 600?BC, Necho II undertook to dig a westÿeast canal through the Wadi Tumilat between Bubastis and Heroopolis,[10] and perhaps continued it to the Heroopolite Gulf and the Red Sea.[9] Regardless, Necho is reported as having never completed his project.[9][10]\\r\\nHerodotus was told that 120,000 men perished in this undertaking, but this figure is doubtless exaggerated.[31] According to Pliny the Elder, Necho's extension to the canal was about 57 English miles,[10] equal to the total distance between Bubastis and the Great Bitter Lake, allowing for winding through valleys.[10] The length that Herodotus tells, of over 1000 stadia (i.e., over 114 miles (183?km)), must be understood to include the entire distance between the Nile and the Red Sea[10] at that time.\\r\\nWith Necho's death, work was discontinued. Herodotus tells that the reason the project was abandoned was because of a warning received from an oracle that others would benefit from its successful completion.[10][32] Necho's war with Nebuchadnezzar II most probably prevented the canal's continuation.\\r\\nNecho's project was completed by Darius I of Persia, who ruled over Ancient Egypt after it had been conquered by his predecessor Cambyses II.[33] It may be that by Darius's time a natural[10] waterway passage which had existed[9] between the Heroopolite Gulf and the Red Sea[34] in the vicinity of the Egyptian town of Shaluf[10] (alt. Chalouf[35] or Shaloof[16]), located just south of the Great Bitter Lake,[10][16] had become so blocked[9] with silt[10] that Darius needed to clear it out so as to allow navigation[10] once again. According to Herodotus, Darius's canal was wide enough that two triremes could pass each other with oars extended, and required four days to traverse. Darius commemorated his achievement with a number of granite stelae that he set up on the Nile bank, including one near Kabret, and a further one a few miles north of Suez. The Darius Inscriptions read:[36]\\r\\nSaith King Darius: I am a Persian. Setting out from Persia, I conquered Egypt. I ordered this canal dug from the river called the Nile that flows in Egypt, to the sea that begins in Persia. When the canal had been dug as I ordered, ships went from Egypt through this canal to Persia, even as I intended.\\r\\nThe canal left the Nile at Bubastis. An inscription[37] on a pillar at Pithom records that in 270 or 269?BC, it was again reopened, by Ptolemy II Philadelphus. In Arsinoe,[10] Ptolemy constructed a navigable lock, with sluices, at the Heroopolite Gulf of the Red Sea,[34] which allowed the passage of vessels but prevented salt water from the Red Sea from mingling with the fresh water in the canal.[38]\\r\\nThe Red Sea is believed by some historians to have gradually receded over the centuries, its coastline slowly moving southward away from Lake Timsah[15][16] and the Great Bitter Lake.[9][10] Coupled with persistent accumulations of Nile silt, maintenance and repair of Ptolemy's canal became increasingly cumbersome over each passing century.\\r\\nTwo hundred years after the construction of Ptolemy's canal, Cleopatra seems to have had no westÿeast waterway passage,[9][10] because the Pelusiac branch of the Nile, which had fed Ptolemy's westÿeast canal, had by that time dwindled, being choked with silt.[9][10]\\r\\nBy the 8th century, a navigable canal existed between Old Cairo and the Red Sea,[9][10] but accounts vary as to who ordered its constructioneither Trajan or 'Amr ibn al-'As, or Omar the Great.[9][10] This canal was reportedly linked to the River Nile at Old Cairo[10] and ended near modern Suez.[9][39] A geography treatise by Dicuil reports a conversation with an English monk, Fidelis, who had sailed on the canal from the Nile to the Red Sea during a pilgrimage to the Holy Land in the first half of the 8th century[40]\\r\\nThe Abbasid Caliph al-Mansur is said to have ordered this canal closed in 767 to prevent supplies from reaching Arabian detractors.[9][10]\\r\\nAl-Hakim bi-Amr Allah is claimed to have repaired the Cairo to Red Sea passageway,[9][10] but only briefly, circa 1000 AD,[9][10] as it soon \\"became choked with sand.\\"[10] However, we are told that parts of this canal still continued to fill in during the Nile's annual inundations.[9][10]\\r\\nThe successful 1488 navigation of southern Africa by Bartolomeu Dias opened a direct maritime trading route to India and the spice islands, and forever changed the balance of Mediterranean trade. One of the most prominent losers in the new order, as former middlemen, was the former spice trading center of Venice.\\r\\nVenetian leaders, driven to desperation, contemplated digging a waterway between the Red Sea and the Nileanticipating the Suez Canal by almost 400 yearsto bring the luxury trade flooding to their doors again. But this remained a dream.\\r\\nDespite entering negotiations with Egypt's ruling Mamelukes, the Venetian plan to build the canal was quickly put to rest by the Ottoman conquest of Egypt in 1517, led by Sultan Selim I[41]\\r\\nDuring the French campaign in Egypt and Syria in late 1798, Napoleon showed an interest in finding the remnants of an ancient waterway passage. This culminated in a cadre of archaeologists, scientists, cartographers and engineers scouring northern Egypt.[42][43] Their findings, recorded in the Description de l'gypte, include detailed maps that depict the discovery of an ancient canal extending northward from the Red Sea and then westward toward the Nile.[42][44]\\r\\nLater, Napoleon, who would become French Emperor in 1804, contemplated the construction of a northÿsouth canal to connect the Mediterranean with the Red Sea. But the plan was abandoned because it wrongly concluded that the waterway would require locks to operate. These would be very expensive and take a long time to construct. This decision was based on an erroneous belief that the Red Sea was 10?m (33?ft) higher than the Mediterranean. The error was the result of using fragmentary survey measurements taken in wartime during Napoleon's Egyptian Expedition.[45] In 1819 the Pacha of Egypt undertook some canal work.[46]\\r\\nHowever, as late as 1861, the unnavigable ancient route discovered by Napoleon from Bubastis to the Red Sea still channeled water in spots as far east as Kassassin.[10]\\r\\nAlthough the alleged difference in sea levels could be problematic for construction, the idea of finding a shorter route to the east remained alive. In 1830, F. R. Chesney submitted a report to the British government that stated that there was no difference in altitude and that the Suez Canal was feasible, but his report received no further attention. Lieutenant Waghorn established his \\"Overland Route\\", which transported post and passengers to India via Egypt. Linant de Bellefonds, a French explorer of Egypt, became chief engineer of Egypt's Public Works. In addition to his normal duties, he surveyed the Isthmus of Suez and made plans for the Suez Canal. French Saint-Simonianists showed an interest in the canal and in 1833, Barthlemy Prosper Enfantin tried to draw Muhammad Ali's attention to the canal but was unsuccessful. Alois Negrelli, the Austrian railroad pioneer, became interested in the idea in 1836. In 1846, Prosper Enfantin's Socit d'tudes du Canal de Suez invited a number of experts, among them Robert Stephenson, Negrelli and Paul-Adrien Bourdaloue to study the feasibility of the Suez Canal (with the assistance of Linant de Bellefonds). Bourdaloue's survey of the isthmus was the first generally accepted evidence that there was no practical difference in altitude between the two seas. Britain, however, feared that a canal open to everyone might interfere with its India trade and therefore preferred a connection by train from Alexandria via Cairo to Suez, which was eventually built by Stephenson.\\r\\nIn 1854 and 1856, Ferdinand de Lesseps obtained a concession from Sa'id Pasha, the Khedive of Egypt and Sudan, to create a company to construct a canal open to ships of all nations. The company was to operate the canal for 99 years from its opening. De Lesseps had used his friendly relationship with Sa'id, which he had developed while he was a French diplomat in the 1830s. As stipulated in the concessions, Ferdinand convened the International Commission for the piercing of the isthmus of Suez (Commission Internationale pour le percement de l'isthme des Suez) consisting of 13 experts from seven countries, among them John Robinson McClean, later President of the Institution of Civil Engineers in London, and again Negrelli, to examine the plans developed by Linant de Bellefonds, and to advise on the feasibility of and the best route for the canal. After surveys and analyses in Egypt and discussions in Paris on various aspects of the canal, where many of Negrelli's ideas prevailed, the commission produced a unanimous report in December 1856 containing a detailed description of the canal complete with plans and profiles.[47] The Suez Canal Company (Compagnie universelle du canal maritime de Suez) came into being on 15 December 1858 and work started on the shore of the future Port Said on 25 April 1859.\\r\\nThe excavation took some 10 years using forced labour (corve) of Egyptian workers during the first years. Some sources estimate that over 30,000 people were working on the canal at any given period, that more than 1.5 million people from various countries were employed, and that thousands of labourers died, many of them from cholera and similar epidemics.[48][49]\\r\\nThe British government had opposed the project from the outset to its completion. As one of the diplomatic moves against the canal, it disapproved of the use of \\"slave labour\\" of forced workers. The British Empire was the major global naval force and officially condemned the forced work and sent armed Bedouins to start a revolt among workers. Involuntary labour on the project ceased, and the viceroy condemned the corve, halting the project.[50]\\r\\nAngered by the British opportunism, de Lesseps sent a letter to the British government remarking on the British lack of remorse a few years earlier when forced workers died in similar conditions building the British railway in Egypt.\\r\\nInitially international opinion was skeptical and Suez Canal Company shares did not sell well overseas. Britain, Austria, and Russia did not buy a significant number of shares.[51] All French shares were quickly sold in France. A contemporary British skeptic claimed \\"One thing is sure... our local merchant community doesn't pay practical attention at all to this grand work, and it is legitimate to doubt that the canal's receipts... could ever be sufficient to recover its maintenance fee. It will never become a large ship's accessible way in any case.\\"[52]\\r\\nThe canal opened under French control on 17 November 1869. Although numerous technical, political, and financial problems had been overcome, the final cost was more than double the original estimate. The opening was performed by Khedive Isma'il Pasha of Egypt and Sudan, and at Ismail's invitation French Empress Eugenie in the Imperial yacht L'Aigle piloted by Napolon Coste, who was bestowed by the Khedive the Ottoman Order of the Medjidie.\\r\\nThe first ship to follow L'Aigle through the canal was the British P&O liner Delta.[53][54] Although L'Aigle was officially the first vessel through the canal, HMS Newport, captained by George Nares, passed through it first. On the night before the canal was due to open, Captain Nares navigated his vessel, in total darkness and without lights, through the mass of waiting ships until it was in front of L'Aigle. When dawn broke, the French were horrified to find that the Royal Navy was first in line and that it would be impossible to pass them. Nares received both an official reprimand and an unofficial vote of thanks from the Admiralty for his actions in promoting British interests and for demonstrating such superb seamanship.[55] An Anchor Line ship, the S.S. Dido, became the first to pass through the Canal from South to North[56][57].\\r\\nAfter the opening, the Suez Canal Company was in financial difficulties. The remaining works were completed only in 1871, and traffic was below expectations in the first two years. De Lesseps therefore tried to increase revenues by interpreting the kind of net ton referred to in the second concession (tonneau de capacit) as meaning a ship's cargo capacity and not only the theoretical net tonnage of the \\"Moorsom System\\" introduced in Britain by the Merchant Shipping Act in 1854. The ensuing commercial and diplomatic activities resulted in the International Commission of Constantinople establishing a specific kind of net tonnage and settling the question of tariffs in its protocol of 18 December 1873.[58] This was the origin of the Suez Canal Net Tonnage and the Suez Canal Special Tonnage Certificate, both of which are still in use today.\\r\\nThe canal had an immediate and dramatic effect on world trade. Combined with the American transcontinental railroad completed six months earlier, it allowed the world to be circled in record time. It played an important role in increasing European colonization of Africa. The construction of the canal was one of the reasons for the Panic of 1873, because goods from the Far East were carried in sailing vessels around the Cape of Good Hope and were stored in British warehouses. As sailing vessels were not adaptable for use through the canal, because the prevailing winds of the Mediterranean blow from west to east, British entrep?t trade suffered.[59] External debts forced Said Pasha's successor, Isma'il Pasha, to sell his country's share in the canal for S4,000,000 (about S87.2 million in 2016) to the United Kingdom in 1875, but French shareholders still held the majority. Prime Minister Benjamin Disraeli was accused by William Ewart Gladstone of undermining Britain's constitutional system, because he had not referred to, or obtained consent from Parliament when purchasing the shares with funding from the Rothschilds.[60]\\r\\nThe Convention of Constantinople in 1888 declared the canal a neutral zone under the protection of the British, who had occupied Egypt and Sudan at the request of Khedive Tewfiq to suppress the Urabi Revolt against his rule. The revolt went on from 1879 to 1882. As a result of British involvement on the side of Khedive Tewfiq, Britain gained control of the canal in 1882. The British defended the strategically important passage against a major Ottoman attack in 1915, during the First World War.[61] Under the Anglo-Egyptian Treaty of 1936, the UK retained control over the canal. The canal was again strategically important in the 1939ÿ1945 Second World War, and Italo-German attempts to capture it were repulsed during the North Africa Campaign, during which the canal was closed to Axis shipping. In 1951 Egypt repudiated the treaty and in October 1954 the UK agreed to remove its troops. Withdrawal was completed on 18 July 1956.\\r\\nBecause of Egyptian overtures towards the Soviet Union, the United Kingdom and the United States withdrew their pledge to support the construction of the Aswan Dam. Egyptian President Gamal Abdel Nasser responded by nationalizing the canal on 26 July 1956[62] and transferring it to the Suez Canal Authority, intending to finance the dam project using revenue from the canal. On the same day that the canal was nationalized Nasser also closed the Straits of Tiran to all Israeli ships.[63] This led to the Suez Crisis in which the UK, France, and Israel invaded Egypt. According to the pre-agreed war plans under the Protocol of Svres, the Israelis invaded the Sinai Peninsula, forcing Egypt to engage them militarily, and allowing the Anglo-French partnership to declare the resultant fighting a threat to stability in the Middle East and enter the war - officially to divide the two forces but in reality to regain the Canal and bring down the Nasser regime.\\r\\nTo save the British from what he thought was a disastrous action and to stop the war from a possible escalation, Canadian Secretary of State for External Affairs Lester B. Pearson proposed the creation of the first United Nations peacekeeping force to ensure access to the canal for all and an Israeli withdrawal from the Sinai Peninsula. On 4 November 1956, a majority at the United Nations voted for Pearson's peacekeeping resolution, which mandated the UN peacekeepers to stay in Sinai unless both Egypt and Israel agreed to their withdrawal. The United States backed this proposal by putting pressure on the British government through the selling of sterling, which would cause it to depreciate. Britain then called a ceasefire, and later agreed to withdraw its troops by the end of the year. Pearson was later awarded the Nobel Peace Prize. As a result of damage and ships sunk under orders from Nasser the canal was closed until April 1957, when it was cleared with UN assistance.[64] A UN force (UNEF) was established to maintain the free navigability of the canal, and peace in the Sinai Peninsula.\\r\\nAccording to the historian Abd aI-Azim Ramadan, Nasser's decision to nationalize the Suez Canal was his alone, made without political or military consultation. The events leading up to the nationalization of the Suez Canal Company, as other events during Nassers rule, showed Nassers inclination to solitary decision making. Ramadan considered Nasser to be far from a rational, responsible leader.[65]\\r\\nIn May 1967, Nasser ordered the UN peacekeeping forces out of Sinai, including the Suez Canal area. Israel objected to the closing of the Straits of Tiran to Israeli shipping. The canal had been closed to Israeli shipping since 1949, except for a short period in 1951ÿ1952.\\r\\nAfter the 1967 Six Day War, Israeli forces occupied the Sinai peninsula, including the entire east bank of the Suez Canal. Unwilling to allow the Israelis to use the canal, Egypt immediately imposed a blockade which closed the canal to all shipping until 5 June 1975. As a result, 15 cargo ships, known as the \\"Yellow Fleet\\", were trapped in the canal for over eight years.\\r\\nIn 1973, during the Yom Kippur War, the canal was the scene of a major crossing by the Egyptian army into Israeli-occupied Sinai and a counter-crossing by the Israeli army to Egypt. Much wreckage from this conflict remains visible along the canal's edges.[citation needed] After the Yom Kippur War the United States initiated Operation Nimbus Moon. The amphibious assault ship USS Inchon (LPH-12) was sent to the Canal, carrying 12 RH-53D minesweeping helicopters of HM-12. These partly cleared the canal between May and December 1974. She was relieved by the LST USS Barnstable County (LST1197). The British Royal Navy initiated Operation Rheostat and Task Group 65.2 provided for Operation Rheostat One[66] (six months in 1974), the minehunters HMS Maxton, HMS Bossington, and HMS Wilton, the Fleet Clearance Diving Team (FCDT)[67] and HMS Abdiel, a practice minelayer/MCMV support ship; and for Operation Rheostat Two[68] (six months in 1975) the minehunters HMS Hubberston and HMS Sheraton, and HMS Abdiel. When the Canal Clearance Operations were completed, the canal and its lakes were considered 99% clear of mines. The canal was then reopened by Egyptian President Anwar Sadat aboard an Egyptian destroyer, which led the first convoy northbound to Port Said in 1975.[69] At his side stood the Iranian Crown Prince Reza Pahlavi, delegated to represent his father, Mohammed Reza Pahlavi, the Shah of Iran. The cruiser USS Little Rock was the only American naval ship in the convoy.[70]\\r\\nThe UNEF mandate expired in 1979. Despite the efforts of the United States, Israel, Egypt, and others to obtain an extension of the UN role in observing the peace between Israel and Egypt, as called for under the EgyptÿIsrael Peace Treaty of 1979, the mandate could not be extended because of the veto by the Soviet Union in the UN Security Council, at the request of Syria. Accordingly, negotiations for a new observer force in the Sinai produced the Multinational Force and Observers (MFO), stationed in Sinai in 1981 in coordination with a phased Israeli withdrawal. It is there under agreements between the United States, Israel, Egypt, and other nations.[71]\\r\\nIn the summer of 2014, months after taking office as President of Egypt, Abdel Fattah el-Sisi ordered the expansion of the Ballah Bypass from 61 metres wide to 312 metres wide for 35 kilometers. The project was called the New Suez Canal, as it would allow ships to transit the canal in both directions simultaneously.[72][73] The project cost more than $8 billion and was completed within one year. Sisi declared the expanded channel open for business in a ceremony on 6 August 2015.[74]\\r\\nPresidents of the Suez Canal Company (1858ÿ1956):\\r\\nChairmen of the Suez Canal Authority (1956ÿpresent):\\r\\nWhen built, the canal was 164?km (102?mi) long and 8?m (26?ft) deep. After several enlargements, it is 193.30?km (120.11?mi) long, 24?m (79?ft) deep and 205 metres (673?ft) wide.[75] It consists of the northern access channel of 22?km (14?mi), the canal itself of 162.25?km (100.82?mi) and the southern access channel of 9?km (5.6?mi).[76]\\r\\nThe so-called New Suez Canal, functional since 6 August 2015,[77] currently has a new parallel canal in the middle part, with its length over 35?km (22?mi). The current parameters of the Suez Canal, including both individual canals of the parallel section are: depth 23 to 24 metres (75 to 79?ft) and width at least 205 to 225 metres (673 to 738?ft) (that width measured at 11 metres (36?ft) of depth).[78]\\r\\n\\r\\nThe canal allows passage of ships up to 20?m (66?ft) draft or 240,000 deadweight tons and up to a height of 68?m (223?ft) above water level and a maximum beam of 77.5?m (254?ft) under certain conditions.[80][81] The canal can handle more traffic and larger ships than the Panama Canal, as Suezmax dimensions are greater than both Panamax and New Panamax. Some supertankers are too large to traverse the canal. Others can offload part of their cargo onto a canal-owned boat to reduce their draft, transit, and reload at the other end of the canal.\\r\\nThe canal has no locks because of the flat terrain, and the minor sea level difference between each end is inconsequential for shipping. As the canal has no sea surge gates, the ports at the ends would be subject to the sudden impact of tsunamis from the Mediterranean Sea and Red Sea, according to a 2012 article in the Journal of Coastal Research.[82]\\r\\nThere is one shipping lane with passing areas in Ballah-Bypass near El Qantara and in the Great Bitter Lake. On a typical day, three convoys transit the canal, two southbound and one northbound. The passage takes between 11 and 16 hours at a speed of around 8 knots (15?km/h; 9?mph). The low speed helps prevent erosion of the banks by ships' wakes.\\r\\nBy 1955, about two-thirds of Europe's oil passed through the canal. Around 8% of world sea trade is carried via the canal. In 2008, 21,415 vessels passed through the canal and the receipts totaled $5.381 billion,[80] with an average cost per ship of $251,000.\\r\\nNew Rules of Navigation came into force on 1 January 2008, passed by the board of directors of the Suez Canal Authority (SCA) to organise vessels' transit. The most important amendments include allowing vessels with 62-foot (19?m) draught to pass, increasing the allowed breadth from 32 metres (105?ft) to 40 metres (130?ft) (following improvement operations), and imposing a fine on vessels using divers from outside the SCA inside the canal boundaries without permission. The amendments allow vessels loaded with dangerous cargo (such as radioactive or flammable materials) to pass if they conform with the latest amendments provided by international conventions.\\r\\nThe SCA has the right to determine the number of tugs required to assist warships traversing the canal, to achieve the highest degree of safety during transit.[83]\\r\\nAs of July 2015, the canal was too narrow for free two-way traffic, so ships pass in convoys and they use bypasses. The by-passes are 78?km (48?mi) out of 193?km (120?mi) (40%). From north to south, they are: Port Said by-pass (entrances) 36.5?km (23?mi), Ballah by-pass & anchorage, 9?km (6?mi), Timsah by-pass 5?km (3?mi), and the Deversoir by-pass (northern end of the Great Bitter Lake) 27.5?km (17?mi). The bypasses were completed in 1980.\\r\\nTypically, it takes a ship 12 to 16 hours to transit the canal. The canal's 24-hour capacity is about 76 standard ships.[84]\\r\\nIn August 2014, Egypt chose a consortium that includes the Egyptian army and global engineering firm Dar Al-Handasah to develop an international industrial and logistics hub in the Suez Canal area,[85] and began the construction of a new canal section from km 60 to km 95 combined with expansion and deep digging of the other 37?km of the canal.[86] This will allow navigation in both directions simultaneously in the 72?km long central section of the canal. These extensions were formally opened on 6 August 2015 by President Al-Sisi.[6][87][88]\\r\\nSince the canal does not cater to unregulated two-way traffic, all ships transit in convoys on regular times, scheduled on a 24-hour basis. Each day, a single northbound convoy starts at 04:00 from Suez. At dulka lane sections, the convoy uses the eastern route.[89][90][91] Synchronised with this convoy's passage is the southbound convoy. It starts at 03:30 from Port Said and so passes the Northbound convoy in the two-lane section[clarification needed].\\r\\nFrom north to south, the crossings are:\\r\\nA railway on the west bank runs parallel to the canal for its entire length.\\r\\nSix new tunnels for cars and trains are also planned across the canal.[94] Currently the Ahmed Hamdi is the only tunnel connecting Suez to the Sinai.\\r\\nThe main alternative is around Cape Agulhas, the southernmost point of Africa, commonly referred as the Cape of Good Hope route. This was the only sea route before the canal was constructed, and when the canal was closed. It is still the only route for ships that are too large for the canal. In the early 21st century, the Suez Canal has suffered from diminished traffic due to piracy in Somalia, with many shipping companies choosing to take the long route instead.[95][96] Between 2008 and 2010, it is estimated that the canal lost 10% of traffic due to the threat of piracy, and another 10% due to the financial crisis. An oil tanker going from Saudi Arabia to the United States has 2,700?mi (4,345?km) longer to go when taking the route south of Africa rather than the canal.[97]\\r\\nBefore the canal's opening in 1869, goods were sometimes offloaded from ships and carried overland between the Mediterranean and the Red Sea.[98]\\r\\nIn recent years, the shrinking Arctic sea ice has made the Northern Sea Route feasible for commercial cargo ships between Europe and East Asia during a six-to-eight-week window in the summer months, shortening the voyage by thousands of miles compared to that through the Suez Canal. According to polar climate researchers, as the extent of the Arctic summer ice pack recedes the route will become passable without the help of icebreakers for a greater period each summer.[99][100][101]\\r\\nThe Bremen-based Beluga Group claimed in 2009 to be the first Western company to attempt using the Northern Sea Route without assistance from icebreakers, cutting 4000 nautical miles off the journey between Ulsan, Korea and Rotterdam, the Netherlands.[102]\\r\\nIsrael has declared that it will construct a railroad through the Negev desert to compete with the canal, with construction partly financed by China.[103]\\r\\nThe opening of the canal created the first salt-water passage between the Mediterranean and the Red Sea. Although the Red Sea is about 1.2?m (4?ft) higher than the eastern Mediterranean,[104] the current between the Mediterranean and the middle of the canal at the Bitter Lakes flows north in winter and south in summer. The current south of the Bitter Lakes is tidal, varying with the tide at Suez.[3] The Bitter Lakes, which were hypersaline natural lakes, blocked the migration of Red Sea species into the Mediterranean for many decades, but as the salinity of the lakes gradually equalised with that of the Red Sea the barrier to migration was removed, and plants and animals from the Red Sea have begun to colonise the eastern Mediterranean. The Red Sea is generally saltier and more nutrient-poor than the Atlantic, so the Red Sea species have advantages over Atlantic species in the less salty and nutrient-rich eastern Mediterranean. Accordingly, most Red Sea species invade the Mediterranean biota, and only few do the opposite. This migratory phenomenon is called Lessepsian migration (after Ferdinand de Lesseps) or \\"Erythrean invasion\\". Also impacting the eastern Mediterranean, starting in 1968, was the operation of Aswan High Dam across the Nile. While providing for increased human development, the project reduced the inflow of freshwater and ended all natural nutrient-rich silt entering the eastern Mediterranean at the Nile Delta. This provided less natural dilution of Mediterranean salinity and ended the higher levels of natural turbidity, additionally making conditions more like those in the Red Sea.[citation needed]\\r\\nInvasive species originated from the Red Sea and introduced into the Mediterranean by the canal have become a major component of the Mediterranean ecosystem and have serious impacts on the ecology, endangering many local and endemic species. About 300 species from the Red Sea have been identified in the Mediterranean, and there are probably others yet unidentified. The Egyptian government's intent to enlarge the canal has raised concerns from marine biologists, fearing that this will worsen the invasion of Red Sea species.[105]\\r\\nConstruction of the canal was preceded by cutting a small fresh-water canal called Sweet Water Canal from the Nile delta along Wadi Tumilat to the future canal, with a southern branch to Suez and a northern branch to Port Said. Completed in 1863, these brought fresh water to a previously arid area, initially for canal construction, and subsequently facilitating growth of agriculture and settlements along the canal.[106]\\r\\nCoordinates: 304218N 322039E? / ?30.70500N 32.34417E? / 30.70500; 32.34417","input":"When was suez canal built and by whom?"},{"output":"John Rutledge","context":"Justices of the Supreme Court of the United States are nominated by the President and are then confirmed by the Senate. Presidential administrations are listed below with any unsuccessful Supreme Court nomineesthat is, individuals who were nominated and who either declined their own nomination, failed the confirmation vote in the Senate, or whose nomination was withdrawn by the president.\\r\\n153 people have been nominated to the U.S. Supreme Court. Thirty nominees (including one nominated for promotion) have been unsuccessful on at least the first try.\\r\\n\\r\\n\\r\\nGeorge Washington nominated six inaugural candidates to the Supreme Court in 1789, including lawyer and statesman John Jay as Chief Justice, which Jay accepted. Washington officially nominated Jay on September 24, 1789, the same day he signed the Judiciary Act of 1789 (which created the position of Chief Justice) into law and he was confirmed on September 26, 1789. All of the other candidates accepted and were appointed as associate justices except for Robert H. Harrison, who declined to serve. The seat remained empty until the confirmation of James Iredell in 1790.[1]\\r\\nWashington nominated William Paterson for the Supreme Court on February 27, 1793.[2] The nomination was withdrawn by the President the following day. Washington had realized that since the law establishing the positions within the Supreme Court had been passed during Paterson's term as a Senator (a post he had resigned in November 1790 after being elected Governor of New Jersey) the nomination was a violation of Article I, Section 6 of the Constitution. Washington re-nominated Paterson to the Court on March 4, 1793, after Paterson's term as Senator had expired, and Paterson was confirmed by the Senate.[2]\\r\\nJohn Jay resigned as Chief Justice on June 29, 1795, after being elected Governor of New York. The subsequent nomination of John Rutledge as Chief Justice was rejected by a vote of 10ÿ14 on December 15, 1795. Rutledge's strident and vocal opposition to the Jay Treaty may have been the main reason for his rejection. Because he had been a recess appointment, Rutledge served as Chief Justice for one term.[3] Washington nominated Associate Justice William Cushing to replace him as Chief Justice, but Cushing declined the role.[1] Washington then successfully appointed Oliver Ellsworth to serve as the next Chief Justice. [1]\\r\\nAfter Oliver Ellsworth decided to resign from the position of Chief Justice, President John Adams sought to replace Ellsworth with John Jay, who had been the first Chief Justice. Jay was formally nominated, but turned down the position. Adams then successfully nominated his Secretary of State, John Marshall.[4]\\r\\nWhen William Cushing died, James Madison nominated Levi Lincoln Sr. on January 2, 1811. Lincoln declined the nomination.[5] Alexander Wolcott was then nominated, but was rejected by a vote of 9ÿ24 on February 13, 1811.[5] After John Quincy Adams declined a nomination, Madison was finally successful in filling the seat with his appointment of Joseph Story.[1]\\r\\nJohn Quincy Adams nominated John J. Crittenden on December 18, 1828. The Senate postponed the vote on his confirmation, by a vote of 23ÿ17, on February 12, 1829. The Senate did not explicitly vote to \\"postpone indefinitely\\", but the resolution did have that effect.[6] President Andrew Jackson instead filled the position with John McLean.[1]\\r\\nAndrew Jackson nominated Roger B. Taney on January 15, 1835, to be an Associate Justice. A resolution was passed by a Senate vote of 24ÿ21 on March 3, 1835, to postpone the nomination indefinitely.[7] Jackson nominated Taney again on December 28, 1835. After the political composition of the Senate changed the next year, Taney was confirmed as Chief Justice March 15, 1836.[7]\\r\\nIn 1837, Jackson nominated William Smith and John Catron to newly created seats. Both were confirmed, but Smith declined to serve. Later that year, President Van Buren appointed John McKinley to fill the vacancy.[1]\\r\\nJohn Tyler experienced difficulty in obtaining approval of his nominees due to his lack of political support in the Senate. Tyler took office in 1841 after the death of Whig President William Henry Harrison. Tyler had been Harrison's running mate in the 1840 election, but Tyler clashed with the Congressional Whigs over issues such as the national bank, and these clashes extended to judicial nominees.[8]\\r\\nJohn C. Spencer was nominated on January 9, 1844, and his nomination was defeated by a vote of 21ÿ26 on January 31, 1844. Reuben H. Walworth was nominated on March 13, 1844, and a resolution to table the nomination passed on a 27ÿ20 vote on June 15, 1844. The nomination was withdrawn from the Senate on June 17, 1844. Edward King was nominated on June 5, 1844. A resolution to table the nomination passed by a vote of 29ÿ18 on June 15, 1844. No other action was taken on this nomination.[8]\\r\\nThe same day that Walworth's nomination was withdrawn, Spencer was re-submitted, but there is no record of debate and a letter from the President withdrawing the nomination was received on the same day. Walworth was then re-nominated later that same day, but the motion to act on the nomination in the Senate was objected to, and no further action was taken.[8]\\r\\nWalworth and King were re-nominated on December 10, 1844, but both nominations were tabled on January 21, 1845. Walworth's nomination was withdrawn on February 6, 1845, and King's two days later. John M. Read was nominated on February 8, 1845, and there was a motion to consider the nomination in the Senate on January 21, 1845, but the motion was unsuccessful and no other action was taken.[8]\\r\\nAfter Henry Baldwin's death in 1844, James K. Polk nominated James Buchanan, who declined the nomination.[9] Polk then nominated George W. Woodward, but the Senate rejected him by a vote of 20ÿ29.[9] Baldwin was finally replaced by Robert Cooper Grier in 1846.[1]\\r\\nMillard Fillmore made three nominations to replace John McKinley, nominating Edward A. Bradford, George Edmund Badger, and William C. Micou, but the Senate did not take action on any of the nominees. President Franklin Pierce filled the vacancy with John Archibald Campbell.[1]\\r\\nJames Buchanan nominated Jeremiah S. Black to the court in 1861 to replace Peter Vivian Daniel. The Senate voted 25ÿ26 against confirming him.[10] President Lincoln filled the seat with Samuel Freeman Miller in 1862.[1]\\r\\nAndrew Johnson took office after the death of Republican President Abraham Lincoln in 1865. Johnson, a former Democrat, had been Lincoln's running mate on the National Union ticket of 1864, but Johnson disagreed with Congressional Republicans on several issues, including judicial nominees.\\r\\nTwo justices died in office during Johnson's administration, James Moore Wayne and John Catron. Congress, however, passed the Judicial Circuits Act of 1866, which provided for a gradual elimination of seats until only seven were left.[11] Chief Justice Salmon P. Chase had urged for this reduction in the hopes that it would result in an increase of the justices' salaries, which, ironically, did not happen until Congress restored the size of the court to nine members in 1871. Johnson had nominated Henry Stanbery to be an Associate Justice, but due to the reduction of seats, this nomination was nullified.[12]\\r\\nUlysses S. Grant nominated Ebenezer R. Hoar to a new seat on the court. The Senate rejected this nomination by a vote of 24ÿ33.[13]:54 Grant successfully nominated Joseph Bradley for the seat.[1]\\r\\nGrant also nominated Edwin M. Stanton, former Attorney General and Secretary of War to the court.[13]:79 The nomination was eventually confirmed, but Stanton died before he was commissioned.[14] Grant then successfully nominated William Strong.[1]\\r\\nGrant nominated George Henry Williams to be Chief Justice of the United States in 1873, but he later withdrew from consideration.[15] Prior to withdrawal of consideration, the Senate Judiciary Committee declined to recommend confirmation to the entire Senate.[16] Grant then nominated Caleb Cushing for Chief Justice on January 9, 1874, but despite Cushing's great learning and eminence at the bar, his anti-war record and the feeling of distrust experienced by many members of the U.S. Senate on account of his inconsistency, aroused such vigorous opposition that his nomination was withdrawn on January 13, 1874.[17] Grant was successful with his third nomination of Morrison Waite.[1]\\r\\nEarly in 1881, President Rutherford B. Hayes nominated Thomas Stanley Matthews for the position of Associate Justice. Matthews was a controversial nominee due to his close ties to the railroad industry,[18] and as the nomination came near the end of Hayes's term, the Senate did not act on it. However, upon succeeding Hayes, incoming President James A. Garfield (who, like Hayes, was a Republican) renominated Matthews, and the Senate confirmed him by a vote of 24 to 23, the narrowest confirmation for a successful U.S. Supreme Court nominee in history. He served on the Court until his death in 1889.[19][20]\\r\\nIn 1882, Chester A. Arthur nominated Roscoe Conkling to serve as an Associate Justice after Ward Hunt resigned.[21] Conkling was confirmed,[22] and then declined the position.[23]\\r\\nAfter Conkling declined, Arthur nominated George F. Edmunds, who twice declined to serve.[24] Arthur then nominated Samuel Blatchford, who was confirmed and accepted.[25]\\r\\nAssociate Justice Samuel Blatchford died in 1893, during the second term of Grover Cleveland.[26] This seat was traditionally held by a New Yorker. Cleveland's first two nominees were not confirmed by the Senate; the nomination of William Hornblower was rejected by the Senate by a vote of 24ÿ30 on January 15, 1894.[1] Cleveland's follow-up nominee Wheeler Hazard Peckham, was also rejected by the Senate, 32ÿ41, on February 16, 1894.[1]\\r\\nBy the tradition of Senatorial courtesy, other Senators generally deferred to a nominee's home state senators when evaluating a presidential nomination.[27] The Senators from New York were Edward Murphy, Jr. and David B. Hill;[28] Hill objected to Cleveland's nominations, and most other Senators supported Hill.[29] Hill was a rival of Cleveland's who had lost the Democratic nomination for President to him in 1892.[30]\\r\\nCleveland finally overcame Hill's opposition by nominating Edward Douglass White of Louisiana.[31] White was a sitting Senator, and Senatorial courtesy dictated that the Senate not reject one of its own.[32] White's nomination was approved; he served as an Associate Justice until 1910,[33] and as Chief Justice until his death in 1921.[34]\\r\\nPresident Warren G. Harding nominated Pierce Butler to the Supreme Court in 1922, but the Senate refused to consider his nomination, in part due to Butler's advocacy for railroad interests. However, Harding re-submitted the nomination later in the year, and Butler was confirmed in a 61ÿ8 vote.[18]\\r\\nOn May 7, 1930, Herbert Hoover's nomination of Appellate Judge John J. Parker for the Supreme Court was rejected by a vote of 39ÿ41.[35] Parker was nominated to replace Edward Terry Sanford. The American Federation of Labor opposed Parker for his rulings that were favorable towards yellow dog contracts and the NAACP opposed Parker due to concerns about Parker's racial views.[36] Hoover attempted to appeal to Southern Democratic Senators to vote for Parker, who was from North Carolina, but Hoover was unable to win enough Democratic votes to make up for Republican defections.[36] Hoover's second nominee, Owen J. Roberts, was confirmed by the Senate.[1]\\r\\nPresident Dwight D. Eisenhower nominated John Marshall Harlan II in 1954, but his nomination was not reported out of the judiciary committee, in part due to opposition to his purported \\"ultra-liberal\\" views. Eisenhower re-nominated Harlan in 1955, and the Senate confirmed him in a 71ÿ11 vote.[18]\\r\\nLyndon B. Johnson nominated Abe Fortas, then an associate justice, for Chief Justice. Fortas would have succeeded Earl Warren, who had decided to retire. Controversy ensued regarding Fortas's extrajudicial activities, and at Fortas's request, Johnson withdrew the nomination prior to a vote of the full Senate.[37] Fortas's nomination was also opposed by many senators who opposed the rulings of the Warren Court.[18] President Nixon instead filled the vacancy caused by Warren's retirement with Warren Burger.\\r\\nWhen Johnson nominated Fortas, he also nominated Homer Thornberry to fill Fortas' seat. Since Fortas withdrew his name from the Chief Justice nomination, but maintained his seat as an Associate Justice (with Earl Warren continuing as Chief Justice), the nomination of Thornberry was void. He was never voted on by the Senate.[38]\\r\\nWhen Abe Fortas resigned in 1969 because of a scandal separate from his Chief Justice bid, Richard Nixon nominated Clement Haynsworth, a Southern jurist. His nomination was rejected by the Senate by a vote of 45ÿ55 on November 21, 1969,[39] due to concerns about Haynsworth's civil rights record and perceived ethical lapses.[18] In response, Nixon nominated G. Harrold Carswell, a Southerner with a history of supporting segregation and opposing women's rights. The Senate rejected his nomination 45 to 51 on April 8, 1970, following much pressure from the Civil Rights and Feminist movements.[40] Nixon's third nominee for the Fortas vacancy was Harry Blackmun, who was confirmed by the Senate with no opposition on May 17, 1970.\\r\\nNixon was soon faced with two more Supreme Court vacancies when John Harlan and Hugo Black retired in the fall of 1971. Nixon considered nominating Arkansas lawyer Hershel Friday and California intermediate appellate judge Mildred Lillie to the high court. By tradition at the time, potential Supreme Court nominees were first disclosed to the American Bar Association's standing committee on the federal judiciary. When it became apparent that this 12-member committee would find that both were unqualified, Nixon passed over Friday and Lillie,[41] and nominated Lewis Powell and William Rehnquist. Powell was confirmed by an 89ÿ1 vote, and Rehnquist was confirmed 68ÿ22.[1]\\r\\nWhen Lewis Powell retired in July 1987, Ronald Reagan nominated Robert Bork. Bork was a member of the Court of Appeals for the District of Columbia at the time and known as a proponent of constitutional originalism. Bork lost confirmation by a Senate vote of 42 to 58, largely due to Bork's extreme conservative opinions on constitutional issues and his role in the Nixon Saturday Night Massacre.[42]\\r\\nReagan then announced his intention to nominate Douglas H. Ginsburg to the court. Before Ginsburg could be officially nominated, he withdrew himself from consideration under heavy pressure after revealing that he had smoked marijuana with his students while a professor at Harvard Law School.[43] Reagan then nominated Anthony Kennedy, who was confirmed by a Senate vote of 97ÿ0.[1]\\r\\nIn October 2005, George W. Bush nominated Harriet Miers, a corporate attorney from Texas who had served as Bush's private attorney and as White House Counsel, as an Associate Justice to replace retiring Justice Sandra Day O'Connor. Miers was widely perceived as unqualified for the position, and it later emerged that she had allowed her law license to lapse for a time. The nomination was immediately attacked by politicians and commentators from across the political spectrum. At Miers' request, Bush withdrew the nomination on October 27, ostensibly to avoid violating executive privilege by disclosing details of her work at the White House.[44] Four days later, Bush nominated Samuel Alito to the seat. Alito was confirmed by a vote of 58ÿ42 on January 31, 2006.[1]\\r\\nIn February 2016, Associate Justice Antonin Scalia died. The following month, President Barack Obama nominated D.C. Circuit Judge Merrick Garland to replace Scalia. However, the Senate was controlled by the Republican Party, which argued that the next president should instead appoint Scalia's successor.[45] Senate Republicans refused to hold hearings on Garland, and Garland's nomination remained before the Senate longer than any other Supreme Court nomination.[46] Garland's nomination expired with the end of the 114th United States Congress.[47]\\r\\nThe vacancy caused by Scalia's death remained unfilled for 422 days, making it just the second Supreme Court vacancy since the end of the Civil War to remain unfilled for more than one year.[48] On January 31, 2017, President Donald Trump nominated federal appeals court Judge Neil Gorsuch to replace Scalia, who, after being confirmed by a vote of 54ÿ45, almost wholly on party lines, was sworn in on April 10, 2017.\\r\\nNotes:","input":"Who was the first rejected supreme court nomination?"},{"output":"white albacore tuna","context":"Chicken of the Sea is an El Segundo, California-based provider of packaged seafood, formerly US-owned and now owned by the Thai Union Group in Samut Sakhon, Thailand. The brand sells tuna, salmon, clams, crab, shrimp, mackerel, oysters, kipper snacks and sardines in cans, pouches, and cups, under the brand names \\"Chicken of the Sea\\", \\"Genova\\", and \\"Ace of Diamonds\\".[1]\\r\\n\\r\\nThe company was founded in 1914 when Frank Van Camp and his son bought the California Tuna Canning Company and changed its name to the Van Camp Seafood Company. The phrase \\"Chicken of the Sea\\", first devised as a way to describe the taste, was so successful that soon it also became the company name.\\r\\n\\r\\nIn 1963, Van Camp Seafood Company was purchased by Ralston Purina. In 1988, Ralston sold its Van Camp division to an Indonesian corporation, P.T. Mantrust, which had financial problems, and the primary creditor, Prudential Life Insurance Company, became the majority owner.\\r\\n\\r\\nIn 1997 the company was purchased by the investment group Tri-Union Seafoods LLC, made up of three partners: \\r\\n\\r\\nThe new owners changed the name of Van Camp Seafood Company to Chicken of the Sea International. In 2000, Tri-Marine International Inc and Edmund A. Gann sold their 50 percent interest in Chicken of the Sea to Thai Union International, Inc., leaving Thai Union the sole owner of the company. Chicken of the Sea International and Tri-Union International LLC merged into one company, still called Chicken of the Sea International.\\r\\n\\r\\nWith the 2003 acquisition of Empress International, an importer of frozen shrimp and other shellfish, Chicken of the Sea's total annual sales climbed to US$600 million. In 2006, Thai Union formed a new division, Chicken of the Sea Frozen Foods, to focus on sales of premium quality fresh and frozen seafood products. This division grew quickly, enhancing Chicken of the Sea's brand awareness and distribution in the food service and retail industries.\\r\\n\\r\\nIn August 2015, Chicken of the Sea was sued, accused of colluding with Bumble Bee Foods and StarKist to fix prices.[2]\\r\\n\\r\\nChicken of the Sea was in talks to merge with Bumble Bee, but it was called off on 3 December 2015, after the US Justice Department expressed \\"serious concerns\\".[1][3]\\r\\n\\r\\nIn May 2018, The company moved its headquarters from San Diego to El Segundo, California.[4]\\r\\n\\r\\nThe company's official explanation for the name of their product: In the \\"old days\\", fishermen referred to white albacore tuna as \\"chicken of the sea\\". It was called this because the white color of its flesh and mild flavor reminded them of chicken. The founder of the company thought this would be a unique name for a brand of tuna, and the Chicken of the Sea brand is now widely known in the Americas.[citation needed]\\r\\n\\r\\nTheir advertising mascot, a blonde mermaid with a golden scepter, was introduced in the 1950s and soon became a familiar product icon. In her book, The Longest Trek: My Tour of the Galaxy, Grace Lee Whitney is credited as being the original Chicken of the Sea Mermaid. One of the photographs in the book documents this. (She is mostly known for playing Yeoman Janice Rand in the first season of the original Star Trek TV series).[5]\\r\\n\\r\\nJessica Simpson famously ate a can of Chicken of the Sea tuna on her TV show Newlyweds: Nick and Jessica, in which she asked her then-husband Nick Lachey: \\"Is this chicken, what I have, or is this fish? I know it's tuna, but it says Chicken... by the Sea [sic].\\"[citation needed]","input":"What animal is the chicken of the sea?"},{"output":"2 hours, 52 minutes, 59 seconds","context":"","input":"How long was the flight from london to new york on concorde?"},{"output":"health maintenance organization (HMO) style plan","context":"Tricare (styled TRICARE), formerly known as the Civilian Health and Medical Program of the Uniformed Services (CHAMPUS), is a health care program of the United States Department of Defense Military Health System.[1] Tricare provides civilian health benefits for U.S Armed Forces military personnel, military retirees, and their dependents, including some members of the Reserve Component. Tricare is the civilian care component of the Military Health System, although historically it also included health care delivered in the military medical treatment facilities.\\r\\n\\r\\nThe Tricare program is managed by the Defense Health Agency (DHA). Before 1 October 2013, it was managed by the Tricare Management Activity (TMA) under the authority of the Assistant Secretary of Defense (Health Affairs). On that date, TMA was disestablished and Tricare responsibility was transferred to the newly established DHA.[2]\\r\\n\\r\\nHistorically, health care for military personnel and their dependents was provided in military medical facilities as promised by the military, and through a referral system, by civilian medical personnel where military physicians were not available in a certain specialty, or when and where overcrowding of a military medical facility occurred. \\r\\n\\r\\nActive duty military personnel always have priority for care in military medical facilities.  After World War II and the Korean War, especially with the growth in the standing forces of the U.S. military due to the Cold War, access to care in military facilities became increasingly unavailable for military retirees and the dependents of both active duty and retired military personnel due to resource constraints and growing demands on the system.  It was at this time that the concept of \\"space-available basis\\" for military retirees and military dependents was first noted. To address this problem, Congress passed the Dependents Medical Care Act of 1956 and the Military Medical Benefits Amendments of 1966. These acts allowed the Secretary of Defense to contract with civilian health care providers. This civilian health care program became known as the Civilian Health and Medical Program of the Uniformed Services (CHAMPUS) in 1966.[3]\\r\\n\\r\\nIn the late 1980s, because of escalating costs, paperwork demands, and general beneficiary dissatisfaction, DoD initiated a series of demonstration projects. Under a program known as the CHAMPUS Reform Initiative (CRI), a contractor provided both health care and administrative-related services, including claims processing. The CRI project was one of the first to introduce managed care features to the CHAMPUS program. Beneficiaries under CRI were offered three choices ÿ a health maintenance organization-like option called Tricare (CHAMPUS) Prime that required enrollment and offered enhanced benefits and low-cost shares, a preferred provider organization-like option called Tricare (CHAMPUS) Extra that required use of network providers in exchange for lower cost shares, and the standard CHAMPUS option that continued the freedom of choice in selecting providers but required higher cost shares and deductibles known as Tricare Standard.\\r\\n\\r\\nAlthough DODs initial intent under CRI was to award three competitively bid contracts covering six states, only one bid, made by Foundation Health Corporation (now Health Net) covering California and Hawaii, was received. Foundation delivered services under this contract between August 1988 and January 1994.\\r\\n\\r\\nIn late 1993, in response to requirements in the DOD Appropriation Act for Fiscal Year 1994, the DoD announced plans for implementing a nationwide managed care program for the MHS that would be completely implemented by May 1997. Under this program, known as Tricare, the United States was divided into 12 health care regions. An administrative organization, the lead agent, was designated for each region and coordinated the health care needs of all military treatment facilities in the region. Under Tricare, seven managed care support contracts were awarded covering DODs 12 health care regions.[4]\\r\\n\\r\\nSince then, Tricare has undergone several restructuring initiatives, including re-alignment of contract regions, Base Realignment and Closure, and the addition of \\"Tricare for Life\\" benefits in 2001 for those who are Medicare-eligible, and \\"Tricare Reserve Select\\" in 2005.\\r\\n\\r\\nTricare Standard provides a similar benefit to the original CHAMPUS program and is available to retirees from the Active Component, retirees from the Reserve Component age 60 or older, and their eligible family members. Tricare Standard is also available to Reservist and their family under the Tricare Reserve Select Component. Under Tricare Standard, beneficiaries can use any civilian health care provider that is payable under Tricare regulations. The beneficiary is responsible for payment of an annual deductible and coinsurance, and may be responsible for certain other out-of-pocket expenses. There is no enrollment fee for Tricare Standard.\\r\\n\\r\\nTricare Standard beneficiaries can elect to use the Tricare Extra option by using a civilian health care provider from within the regional contractor's provider network. In this way, Tricare Extra represents a preferred provider organization (PPO). When using Tricare Extra, the beneficiary's coinsurance amount is reduced by at least five percentage points. There is no fee for use of the Tricare Extra benefit other than the coinsurance.\\r\\n\\r\\nTricare Prime is a health maintenance organization (HMO) style plan available to active duty personnel, retirees from the Active Component, retirees from the Reserve Component age 60 or older, and their eligible family members. Under Tricare Prime, beneficiaries must choose a primary care physician and obtain referrals and authorizations for specialty care. In return for these restrictions, beneficiaries are responsible only for small copayments for each visit (retirees and their families only). There is an annual enrollment fee for Tricare Prime for military retirees and their family members. There is no enrollment fee for active duty military and their family members. The majority of Tricare PRIME enrollees must exclusively use the MTF (Military Treatment Facility) to receive their care, as long as the MTF has capacity. If the MTF does not have capacity, the commander of the MTF notifies the region's contractor and the contractor's provider network is used to supplement the MTF's capacity. If the MTF regains capacity, the MTF reserves the right to move the beneficiaries back to receiving their care at the MTF in a process known as \\"recapture.\\"\\r\\n\\r\\nUS Family Health Plan, a Tricare Prime-sponsored health plan option, is made available by nonprofit health care providers in the Northeast U.S., Southeast Texas/Southwest Louisiana, and the Puget Sound region of Washington state.\\r\\n\\r\\nTricare Reserve Select is a premium-based health plan that active status qualified National Guard and Reserve members may purchase. The classification is sometimes referred to as Tricare Reserve Component (RC). It requires a monthly premium and offers coverage similar to Tricare Standard and Extra for the military member and eligible family members. It has a partial premium cost sharing arrangement with DoD similar to civilian private or public sector employer plans, although typically at a lower cost than civilian plans. The program coverage is available worldwide to Selected Reserve (SELRES) members of both the Title 10 USC Federal Reserve Components (Army Reserve, Navy Reserve, Air Force Reserve, Marine Corps Reserve), Title 14 USC Federal Reserve Component (Coast Guard Reserve) and the Title 32 National Guard (Army National Guard and Air National Guard) in a drill pay (also known as \\"paid\\") status. As of February 2008, retired Reserve Component personnel under the age of 60, actively drilling Individual Ready Reserve (IRR) personnel in a non-paid status, or actively drilling Volunteer Training Unit (VTU) personnel in a non-paid status do not qualify for TRS. IRR and VTU members are eligible for reinstatement under TRS is they return to a SELRES status. Reserve Component personnel who are also Federal civil servants (to include Army Reserve Technicians and Air Reserve Technicians (ART) in the Army Reserve, Army National Guard, Air Force Reserve and Air National Guard) and eligible for the Federal Employee Health Benefit Program (FEHBP) are also excluded from TRS. Retired Reserve Component personnel and eligible dependent family members become eligible Tricare Standard, Tricare Extra or Tricare Prime on the service member's 60th birthday in the same manner as Active Component retirees and their eligible dependents are eligible immediately upon retirement from active service. Qualification questions should be referred to Tricare.\\r\\n\\r\\nTricare Reserve Retired is a premium-based health plan that qualified retired members of the National Guard and Reserve under the age of 60 may purchase for themselves and eligible family members. Established in 2008 and opened for enrollment in 2010, it is similar to Tricare Reserve Select (TRS), but differs in that there is no premium cost-sharing with DoD as there is with TRS. As such, retired Reserve Component members who elect to purchase TRR must pay the full cost (100%) of the calculated premium plus an additional administrative fee. Payments could range as high as $900.00 a month. Although open to all eligible retired Reserve Component personnel under the age of 60, the program's principal focus is often perceived as being focused on recent Reserve Component retirees who are self-employed or otherwise ineligible for civilian employer provided/subsidized health insurance, especially those who were mobilized for full-time active duty service subsequent to 11 September 2001 in support of Operations Enduring Freedom, Enduring Freedom, Iraqi Freedom, New Dawn and/or Noble Eagle. Retired Reserve Component personnel who elect to participate in TRR will exit TRR when the service member reaches age 60 and he/she and their eligible dependent family members become eligible for the same Tricare Standard, Tricare Extra or Tricare Prime options as Active Component retirees and, in the case of Tricare Prime, at the same cost as Active Component retirees. Qualification questions should be referred to Tricare.\\r\\n\\r\\nTricare for Life was first incorporated as part of the then-seven regional Managed Care Support Contracts of Tricare in May 2001. The benefit was enacted by Congress in response to growing complaints from beneficiaries that as Medicare out of pocket costs increased a benefit was needed to pay these costs in lieu of Tricare retirees being required to purchase Medicare Supplemental Coverage to pay for prescriptions, physician and hospital dispensed drugs, cost shares and deductibles. Before Tricare for Life, Tricare beneficiaries immediately lost Tricare coverage upon attaining Medicare eligibility at age 65, placing them at the same level of coverage as U.S. citizens who had never served full 20 to 30-plus year careers in the armed forces. This included becoming Medicare eligible due to disability. Tricare for Life is designed to pay patient liability after Medicare payments. There is no enrollment necessary for Tricare for Life and to be eligible, members must be Tricare and Medicare Eligible and have purchased Medicare Part B coverage. An exception to the requirement for Part B coverage exists when the beneficiary that is Medicare eligible is the spouse of an Active Duty Service Member. In some instances Tricare for Life is primary payer when the services are normally a Tricare benefit but not covered by Medicare. This includes drug charges, when Medicare benefit limits are attained and services performed outside the United States or in a Veterans Affairs facility where Medicare does not pay. TFL does not pay patient liability for services that are not a Tricare benefit even though they may be paid by Medicare, such as chiropractic benefits. The policy limitations applying to Tricare also apply to TFL and must therefore be deemed medically necessary and skilled care. Custodial care therefore is not covered. In 2004 the Tricare for Life benefit was transferred from the individual regional Tricare contractors. Medical claims are processed by the national Tricare Dual Eligible Fiscal Intermediary Contractor (TDEFIC-Wisconsin Physicians Service Insurance Corporation). Pharmacy claims are processed by the Tricare Pharmacy Contractor (Express Scripts) and Overseas TFL claims are processed by the Tricare Overseas Program Contractor (as of September 2010 this will be International SOS using Wisconsin Physicians Service as their Fiscal Intermediary partner).\\r\\n\\r\\nTricare Young Adult (TYA) is a premium-based health care plan available for purchase by qualified dependents who have aged out of Tricare at age 21, or age 23 for full-time college students. Dependents are eligible if they are unmarried, not eligible for either Tricare coverage or their own employer-sponsored health care coverage, and their sponsor is Tricare eligible.[5]\\r\\n\\r\\nEligible dependents have the option to purchase Tricare Standard/Extra health coverage on a month-to-month basis. Purchased coverage includes medical and pharmacy benefits but does not include dental. A premium-based Tricare Prime benefit was to be available later in 2011.[citation needed]\\r\\n\\r\\nThe signing of the National Defense Authorization Act in January 2011 aligned Tricare with the provisions of the 2010 Patient Protection and Affordable Care Act, and led to the creation of TYA.[6] Enrollment began on 1 May 2011.[7]\\r\\n\\r\\nIn addition to the Tricare options listed above, military retirees can opt for a Tricare supplement plan. Tricare supplement was designed to help military retirees and their families save money on unexpected out of pocket expenses that are not covered by Tricare. Out of pocket costs that may be alleviated by Tricare supplement can include specialists, surgeries, and hospital stays. The Tricare supplement plan can be beneficial for Tricare standard beneficiaries who are covered under a 75/25 plan that does not cover all expenses in the event of an unexpected illness or accident. [8]\\r\\n\\r\\nThe ultimate responsible organization for administration of Tricare is the U.S. Department of Defense Military Health System, which organized the Tricare Management Activity (TMA). The Tricare Management Activity contracts with several large health insurance corporations to provide claims processing, customer service and other administrative functions to the Tricare program.\\r\\n\\r\\nCurrently, there are three regional Managed Care Support Contractors (MCSCs), a Medicare/Tricare Dual Eligible Fiscal Intermediary Contractor (TDEFIC), and a Tricare Pharmacy contractor, who administers both Mail Order Pharmacy (TMOP) and Retail Pharmacy (TRRx) programs. In addition several administrative contractors provide quality management, auditing, and statistical services.\\r\\n\\r\\nTMA contracts and administers a couple of dental programs.\\r\\n\\r\\nIn 2004, Tricare Management Activity re-aligned the previous twelve regions into three large regions, known as Tricare North, Tricare South, and Tricare West. Services in these regions are provided by:\\r\\n\\r\\nAll medical claims are processed (\\"adjudicated\\") by the following claims processing sub-contractors:\\r\\n\\r\\nIn 2009, the Tricare Overseas Program contract consolidated the following:\\r\\n\\r\\nIn October 2009 International SOS Assistance, Inc. was awarded the contract to provide comprehensive health care support services to the Department of Defense Tricare Overseas Program.\\r\\n\\r\\nEffective January 1, 2018 the Tricare regions were consolidated into two large regions, Tricare East and Tricare West. Services in these regions are provided by:","input":"What type of health coverage plan is tricare prime?"},{"output":"October 17, 2017","context":"The 2017ÿ18 NBA season will be the 72nd season of the National Basketball Association (NBA). The regular season will begin on October 17, 2017, earlier than previous seasons to reduce the number of \\"back-to-back\\" games teams are scheduled to play,[1] with the 2017 runners-up Cleveland Cavaliers hosting a game against the Boston Celtics at Quicken Loans Arena in Cleveland, Ohio.[2] Christmas games will be played on December 25. The 2018 NBA All-Star Game will be played on February 18, 2018, at the Staples Center in Los Angeles, California. The regular season will end on April 11, 2018 and the playoffs will begin on April 14, 2018.\\r\\n\\r\\n\\r\\nFree agency negotiations began on July 1. Players began signing on July 6, after the July moratorium ended.\\r\\nFor the first time there will be no coaching changes entering training camp.[8]\\r\\nThe preseason began on September 30 and will end on October 13.\\r\\nThe Golden State Warriors and Minnesota Timberwolves played two preseason games in China, at Shenzhen on October 5 and Shanghai on October 8.[9] Minnesota won the first game 111ÿ97, while Golden State emerged victorious in the second one, 142ÿ110.\\r\\nThe regular season will begin on October 17, 2017. Christmas Day games will be played on December 25, 2017.[2] The regular season will end on April 11, 2018.\\r\\nOn August 9, 2017, the NBA announced that the Brooklyn Nets would play two regular season games at Mexico City Arena in Mexico City, Mexico. On December 7, 2017, the Nets would play against the Oklahoma City Thunder and on December 9, 2017, they would match up against the Miami Heat.[10]\\r\\nOn August 10, 2017, the NBA announced that the Philadelphia 76ers would play the Boston Celtics at The O2 Arena in London, United Kingdom on January 11, 2018.[11]\\r\\nThis will be the second season of the current nine-year contracts with ABC, ESPN, TNT, and NBA TV.\\r\\nAdvertisements begin to appear on league uniforms starting this season. As of October 10, 2017, seventeen teams have confirmed to sign uniform sponsorship deals:","input":"When is the first nba regular season game?"},{"output":"9,843","context":"","input":"How many police officers are there in the lapd?"},{"output":"ended on April 11, 2018","context":"The 2017ÿ18 NBA season is the 72nd season of the National Basketball Association (NBA). The regular season began on October 17, 2017, earlier than previous seasons to reduce the number of \\"back-to-back\\" games teams are scheduled to play,[1] with the 2017 Eastern Conference champion (and Finals runnerÿup) Cleveland Cavaliers hosting a game against the Boston Celtics at Quicken Loans Arena in Cleveland, Ohio[2] Christmas games were played on December 25, 2017. The 2018 NBA All-Star Game was played on February 18, 2018, at the Staples Center in Los Angeles, California. LeBron James of the Cleveland Cavaliers was named the All-Star Game Most Valuable Player. The regular season ended on April 11, 2018 and the playoffs began on April 14, 2018.[3]\\r\\n\\r\\nFree agency negotiations began on July 1. Players began signing on July 6, after the July moratorium ended.\\r\\n\\r\\nFor the first time since the inauguration of the BAA, there would be no coaching changes going from the previous season to entering the regular season.[11] In fact, the 536 days from Dave Joerger being fired as coach of the Memphis Grizzlies to the day the Phoenix Suns fired Earl Watson as head coach would be the longest period in NBA history without any coaching changes occur. The last time no head coaches were hired in the off-season occurred during the 1975ÿ76 season.[12]\\r\\n\\r\\nThe preseason began on September 30 and ended on October 13.\\r\\n\\r\\nThe Golden State Warriors and Minnesota Timberwolves played two preseason games in China, at Shenzhen on October 5 and Shanghai on October 8.[17] Minnesota won the first game 111ÿ97, while Golden State emerged victorious in the second one, 142ÿ110.\\r\\n\\r\\nThe regular season began on October 17, 2017 and ended on April 11, 2018.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nNotes\\r\\n\\r\\nOn August 9, 2017, the NBA announced that the Brooklyn Nets would play two regular season games at Mexico City Arena in Mexico City, Mexico. On December 7, 2017, the Nets played against the Oklahoma City Thunder and on December 9, 2017, they matched up against the Miami Heat.[18] The Nets prevailed 100ÿ95 over the Thunder in their first game, but lost to the Heat 101ÿ89 in their second game.\\r\\n\\r\\nOn August 10, 2017, the NBA announced that the Philadelphia 76ers would play the Boston Celtics at The O2 Arena in London, United Kingdom on January 11, 2018.[19] The Celtics won the game 114?103.\\r\\n\\r\\nThe 2018 NBA Playoffs began on Saturday, April 14, 2018, ESPN will air the 2018 Eastern Conference Finals and TNT will air the 2018 Western Conference Finals. The season will end with the 2018 NBA Finals which will begin on May 31, 2018 on ABC.\\r\\n\\r\\n\\r\\n* Division winner\\r\\nBold Series winner\\r\\nItalic Team with home-court advantage\\r\\n\\r\\n\\r\\n\\r\\nThe following players were named the Eastern and Western Conference Players of the Week.\\r\\n\\r\\nThe following players were named the Eastern and Western Conference Players of the Month.\\r\\n\\r\\nThe following players were named the Eastern and Western Conference Rookies of the Month.\\r\\n\\r\\nThe following coaches were named the Eastern and Western Conference Coaches of the Month.\\r\\n\\r\\nThis is the second season of the current nine-year contracts with ABC, ESPN, TNT, and NBA TV.\\r\\n\\r\\nThe Chicago Bulls experienced a mid-season change in their broadcast partner due to the Cumulus Media Chapter 11 bankruptcy filing nulling and voiding many broadcasting agreements. The team's games moved from Cumulus's WLS to Entercom's WSCR on February 3, 2018.[83]\\r\\n\\r\\nAdvertisements begin to appear on league uniforms starting this season. With this, the NBA became the first of the \\"Big Four\\" North American professional sports league to use corporate sponsorship patches on its jerseys.[95] As of March 6, 2018, there are 21 NBA teams that had signed uniform sponsorship contracts:\\r\\n\\r\\nNine who do not: Chicago Bulls, Houston Rockets, Indiana Pacers, Memphis Grizzlies, Oklahoma City Thunder, Phoenix Suns, Portland Trail Blazers, San Antonio Spurs and Washington Wizards.","input":"What is the last day of the nba regular season?"},{"output":"Mandalay Bay hotel","context":"24 guns in total, including:[1]\\r\\nThe 2017 Las Vegas shooting occurred on the night of Sunday, October 1, 2017 when a gunman opened fire on a crowd of concertgoers at the Route 91 Harvest music festival on the Las Vegas Strip in Nevada, leaving 58 people dead and 851 injured. Between 10:05 and 10:15?p.m. PDT, 64-year-old Stephen Paddock of Mesquite, Nevada, fired more than 1,100 rounds from his suite on the 32nd floor of the nearby Mandalay Bay hotel. About an hour after he fired his last shot into the crowd, he was found dead in his room from a self-inflicted gunshot wound. His motive remains unknown.\\r\\nThe incident is the deadliest mass shooting committed by an individual in the United States. It reignited the debate about gun laws in the U.S., with attention focused on bump fire stocks, which Paddock used to fire semi-automatic rifles at a rate similar to that of a fully automatic weapon.\\r\\n\\r\\n\\r\\nThe Las Vegas Strip is a stretch of Las Vegas Boulevard immediately south of the city of Las Vegas in Clark County, Nevada. The Strip is known for its concentration of casinos and resort hotels, including the 43-story Mandalay Bay southwest of its intersection with Mandalay Bay Road, in the unincorporated town of Paradise.[2]\\r\\nLas Vegas Village, a 15-acre (6.1-hectare) lot used for outdoor performances, is located diagonally across the intersection to the northeast.[2][3] From 2014 onward, the venue hosted the annual Route 91 Harvest country music festival. The 2017 festival ran from September 29 to October 1, with over 22,000 attendees on the final day.[3][4][a]\\r\\nStephen Paddock was a 64-year-old former auditor and real estate businessman who had been living 80 miles (130?km) northeast of Las Vegas in a retirement community in Mesquite, Nevada.[5] Paddock was twice divorced, had a long-term girlfriend, and had no known children.[6] He was a son of Benjamin Paddock, a bank robber who was on the FBI's most-wanted list between 1969 and 1977.[6] Paddock's only recorded interactions with law enforcement were traffic citations.[7]\\r\\nPaddock was a high-stakes gambler who placed bets at a high enough level to earn valuable compsfree benefits such as rooms and meals.[8] He was a familiar figure to casino hosts in Las Vegas, but was not well known among other high-stakes gamblers because he mostly played video poker.[8] He reportedly kept to himself and was a heavy drinker.[9] Paddock had lost a significant amount of his wealth over the previous two years,[10] but had paid off all gambling debts before the shooting.[11]\\r\\nPaddock may have considered attacking other events. He had researched large-scale venues in cities such as Boston since at least May 2017,[11] and had reserved a room overlooking the August 2017 Lollapalooza festival in Chicago, but did not use it.[12] According to his girlfriend, she and Paddock were at the Mandalay Bay during an earlier stay a month before the attack when he repeatedly cased out Las Vegas Village from different windows in their room.[13] From September 17, Paddock stayed at The Ogden in Downtown Las Vegas, which overlooked the open-air Life Is Beautiful festival that ran from September 22 to September 24.[13][4] His internet search terms from mid-September included \\"swat weapons\\", \\"ballistics chart 308\\", \\"SWAT Las Vegas\\", and \\"do police use explosives\\".[13]\\r\\nPaddock arrived at the Mandalay Bay on September 25, 2017. He was booked into Room 32-135, a complimentary room on the 32nd floor.[4][8] Four days later, he also checked into the directly-connected Room 32-134. Both suites overlook the site of the concert at Las Vegas Village.[4][14][b] Between September 25 and October 1the day of the shootinghe stockpiled an arsenal of weapons, associated equipment and ammunition that included fourteen AR-15 rifles (twelve of which had bump stocks and 100-round magazines), eight AR-10 rifles, a bolt-action rifle, and a revolver. Often with the help of hotel bellhops, he brought five suitcases to his room on September 25, seven on the 26th, two on the 28th, six on the 30th, and two on October 1.[15][13][4] Cell phone records also show that he also made multiple visits to his home in Mesquite. On September 30, he placed \\"Do not disturb\\" signs on the doors of both rooms.[15] Paddock spent much of his time at the Mandalay Bay before the shooting gambling, often at night. He interacted with Mandalay Bay employees more than ten times during his stay, including twice on the day of the shooting; an MGM Resorts International spokesperson said they were all \\"normal in nature\\".[16]\\r\\nThe mass shooting occurred between 10:05 and 10:15?p.m. PDT on October 1, 2017, which was the third and final night of the festival. When the shooting began, country music singer Jason Aldean was giving the closing performance.[17]\\r\\nShortly before 10:00?p.m., hotel security guard Jesus Campos was sent to the 32nd floor to investigate an open-door alert. He attempted to open a door that provided immediate access to the floor, but found that it would not open. After Campos entered the floor, he discovered an L-shaped bracket screwed into the door and door frame, which was responsible for barring the door from opening. After reporting the discovery to his dispatch center, he heard the sound of rapid drilling coming from Room 32-135 and went to investigate the matter. At approximately 10:05, he was hit in the right thigh by one of about 35 bullets that Paddock fired through the door of his suite. After Campos was hit, he took cover in the alcove between Rooms 32-122 and 32-124 and immediately informed the hotel by radio and cellphone that he had been shot, though he believed he had been shot with a BB or pellet gun. At the same time, maintenance worker Stephen Schuck was on the same floor to fix the door that Campos had reported as being barricaded. Campos, who was already injured, encountered Schuck and told him to take cover. Schuck contacted hotel dispatchers over his radio, informed them of the ongoing shooting, and told them to call the police.[4][18][19][20][21] Neither Las Vegas Metropolitan Police Department nor MGM Resorts International, the Mandalay Bay's owner, have confirmed when information about the initial shooting was relayed to the police.[22][23][24][25]\\r\\nAfter Paddock used a hammer to break two of the windows in both of his suites,[4] he began shooting through them at 10:05?p.m.[26] He ultimately fired more than 1,100 rifle rounds[27] approximately 490 yards (450?m) into the festival audience.[28][29][30][c] He initially started out with a few single gunshots before firing in prolonged bursts.[4] Many people in the crowd initially mistook the gunfire for fireworks.[31] During the shooting, a security fence hindered concertgoers from fleeing the 15-acre concrete lot.[32] The gunfire continued, with some momentary pauses, over the span of ten minutes and ended by 10:15?p.m.[33][34]\\r\\nIn addition to shooting at the concertgoers, Paddock fired eight bullets at a large jet fuel tank at McCarran International Airport 2,000 feet (600?m) away.[4] Two of those bullets struck the exterior of the tank, with one bullet penetrating the tank. The fuel did not explode because jet fuel is mostly kerosene, which is unlikely to ignite when struck by a bullet.[35]\\r\\nDuring the shooting, police officers were initially confused whether the shots were coming from the Mandalay Bay, the nearby Luxor hotel, or the festival grounds.[32] There were also multiple false reports of additional shooters at other hotels on the Strip.[36] Officers eventually spotted multiple flashes of gunfire in the middle of the northern side of Mandalay Bay and responded to the hotel. At 10:12?p.m., two officers on the 31st floor reported the sounds of gunfire on the floor above them.[32] When officers arrived on the 32nd floor at 10:17?p.m. and encountered Campos a minute later, he directed them to Paddock's room and helped others evacuate. Campos was then directed to seek medical attention for himself.[19][21]\\r\\nBetween 10:26 and 10:30?p.m., eight additional officers arrived at the 32nd floor; some of those officers manually breached through the door Paddock had screwed shut with the bracket. The gunfire had ceased, and the police moved systematically down the hallway, searching and clearing each room, using a master key that was provided by Campos. At 10:55?p.m., the officers finished evacuating guests. At 11:20?p.m., police breached Room 32-135 with explosives.[4][32][34][37] Paddock was found dead on the floor from a self-inflicted gunshot wound to the head.[38][39] Police then breached Room 32-134; while entering the hotel suite, an officer accidentally fired a three-round burst from his weapon, but the bullets did not hit anyone.[4][40] At 11:27?p.m., officers announced over the police radio that a suspect was down.[34][41]\\r\\nMuch of Las Vegas Boulevard was closed while police SWAT teams combed the venue and neighboring businesses. McCarran International Airport, adjacent to the shooting site, was shut down for several hours.[42] Approximately 300 people entered the airport property as they fled to safety from the shooting.[31] This prompted officials to shut down all four runways. More than 25 flights were rerouted to ensure that no aircraft would be hit by gunfire,[36] while other flights were canceled before airfield operations resumed at 12:40?a.m. on October 2.[43] At approximately 2:45?p.m. PDT on October 2, a state of emergency was declared in Clark County.[44][45]\\r\\nEarly on October 2, Sheriff Lombardo identified the suspect as Stephen Paddock.[46]\\r\\nFifty-eight people were shot to death at the music festival; Paddock's suicide was the only death at the Mandalay Bay Hotel.[47][48][49] The fatalities included 36 women and 22 men.[47] The oldest was 67, the youngest 20.[50] Six were from Nevada, 35 from California, 13 from other states, and four from Canada.[51] The Clark County Coroner's Office determined that all 58 victims died as a result of gunshot wounds.[52] Thirty one of the victims were pronounced dead at the scene, while the rest were pronounced dead at hospitals.[11]\\r\\nAn additional 851 people were injured, 422 of them with gunshot wounds.[24][53] In the aftermath, many victims were transported to area hospitals, which included University Medical Center of Southern Nevada, Sunrise Hospital & Medical Center, and at least one of the six hospitals of Valley Health System.[54][55][56] Sunrise Hospital treated the largest portion of the wounded: 199 patients,[57] 150 of whom arrived within a timespan of about 40 minutes.[58] For reasons that are unclear, most patients were transported to Sunrise, a Level II trauma center. University Medical Center, a nearby Level I trauma center, treated 104 patients.[59] Six victims sought medical treatment in Southern California; UC Irvine Medical Center treated four and Loma Linda University Medical Center treated two.[60]\\r\\nThe incident is the deadliest mass shooting committed by an individual in the United States, exceeding the death toll of the 2016 Orlando nightclub shooting, where 49 people were shot and killed.[47][61][62]\\r\\nOn the morning after the shooting, lines to donate blood in Las Vegas stretched for blocks. Wait times were as much as six hours or more.[63] Millions of dollars have also been raised to help victims and their families.[64]\\r\\nNevada Governor Brian Sandoval called the shooting \\"a tragic and heinous act of violence that has shaken the Nevada family\\".[65] Jason Aldean, who was singing when the shooting started, posted his condolences on Instagram and noted all of those working with him at the show had survived the attack.[66]\\r\\nAt a press conference, President Donald Trump described Paddock as \\"a very very sick individual\\", and \\"a demented man, [with] a lot of problems\\". He added, \\"the police department has done such an incredible job, and we'll be talking about gun laws as time goes by\\".[67][68] A White House official talking points memo, distributed to Trump allies, opposed tightening gun control since \\"new laws won't stop a mad man\\", but \\"will curtail the freedoms of law abiding citizens\\".[69] On October 2, Trump issued a proclamation to honor the victims and their families.[70][71] On October 4, Trump visited the shooting victims and first responders.[72]\\r\\nA unity prayer walk and ceremony was held in Las Vegas on October 7 in honor of the dead. Speakers at the ceremony included Vice President Mike Pence and Las Vegas Mayor Carolyn Goodman.[73] On the evening of October 15, thousands participated in a commemorative 3 mile walk between Circus Circus and Mandalay Bay.[74]\\r\\nThe annual Rock 'n' Roll Las Vegas Marathon took place on November 12 and was the largest event to be held in the city since the shooting. The event received a massive amount of security, which included 350 officers, counter-sniper surveillance posts, and a number of barriers composed of dump trucks, buses, and other large vehicles.[75]\\r\\nThe expansion Vegas Golden Knights of the NHL held a tribute to the victims and honoured response personnel before their inaugural home game on October 10.[76] Later during the season, the number 58 became the first number in team history to be retired, chosen for the 58 deaths during the shooting.[77]\\r\\nThe future of the Las Vegas Village site remains undetermined.[78]\\r\\nThe shooting prompted support in the U.S. Congress for assault weapons legislation that would ban bump fire stocks. The National Rifle Association (NRA) came out in favor of administrative bump fire stock regulations.[47] Many Congressional Democrats and some Republicans expressed their support of a prohibition of bump fire stocks.[79] Senator Dianne Feinstein introduced a Senate bill that gained 39 Democratic co-sponsors. Two bipartisan bills have been introduced in the House of Representatives. As of November, no Congressional action has been taken. House leaders said the issue of bump fire stock regulation should be decided by the Bureau of Alcohol, Tobacco, and Firearms, which originally approved gun-stocks.[80] On November 6, Massachusetts became the first state to ban the sale, possession, or use of the devices.[81]\\r\\nNine days after the shooting, eighteen Democratic U.S. Senators introduced a bill, the Keep Americans Safe Act, which, if signed into law, would ban gun magazines that hold more than ten rounds of ammunition.[82]\\r\\nStock prices of firearms manufacturers rose the day after the shooting, as has happened after similar incidents. Investors expect gun sales will increase over concerns that such an event could lead to more stringent gun-control legislation and a rush of customers wishing to defend themselves against future attacks.[83][84]\\r\\nAccording to authorities with the Clark County Commissioner, the name \\"1 October\\" was declared the official title for investigations into the mass shooting.[85]\\r\\nInvestigators found hidden surveillance cameras that were placed inside and outside the hotel room, presumably so Paddock could monitor the arrival of others.[86] The cameras were not in record mode.[87] Police said a handwritten note found in the room indicated Paddock had been calculating the distance, wind, and trajectory from his 32nd floor hotel suite to the concertgoers he was targeting on the festival lot.[88][89]\\r\\nAt a press conference on October 4, Clark County Sheriff Joe Lombardo stated there was evidencewhich he declined to discussthat Paddock intended to escape the scene, and that he may have had assistance from an accomplice.[90] Investigators searched Paddock's room and found a \\"bulletproof vest\\" and breathing apparatus, which were survival gear that Paddock never used.[91]\\r\\nThere have been several changes in the official account and timeline of Paddock's shooting of hotel security guard Campos. Police officials described these adjustments as \\"minute changes\\" that are common in complex investigations.[19]\\r\\nIn their first statement about the incident, police officials inaccurately reported that Campos arrived on the scene after Paddock began firing into the crowd. In a second statement, police officials reported, again inaccurately, that Campos was shot six minutes before Paddock began firing into the crowd. That report had been based on a 9:59?p.m. notation in a hotel security log, which in a third statement was determined to have been the time when Campos encountered the barricaded door.[18][20]\\r\\nSheriff Lombardo dismissed allegations that the changing timeline was the result of some kind of conspiracy between the police department, the FBI, and MGM Resorts International saying, \\"Nobody is attempting to hide anything in reference to this investigation. The dynamics and the size of this investigation requires us to go through voluminous amounts of information in order to draw an accurate picture.\\"[18]\\r\\nAlthough the investigation remains ongoing, the Las Vegas Metropolitan Police Department released a preliminary report on the event on January 18, 2018.[4]\\r\\nPolice speculate that Paddock acted alone and have not determined his motive. No links have been identified to any hate groups, terrorist groups or ideologies, and he did not record a reason for his actions.[92]\\r\\nOn February 2, 2018, Douglas Haig, an Arizonan ammunition dealer, was charged in a Nevada federal court with \\"conspiracy to manufacture and sell armor-piercing ammunition without a license\\" after his fingerprints were discovered on unfired armor-piercing ammunition inside Paddock's suite.[93]\\r\\nTwenty-four firearms, a large quantity of ammunition, and numerous high-capacity magazines capable of holding up to 100 rounds apiece were found in the suite.[1][94][95] Fourteen of the firearms were .223-caliber AR-15-type semi-automatic rifles: three manufactured by Colt, two by Daniel Defense, two by FN Herstal, two by LWRC International, two by POF-USA, one with a .223 Wylde chamber by Christensen Arms, one made-to-order by LMT, and one by Noveske. The others were eight .308-caliber AR-10-type rifles, one .308-caliber Ruger American bolt-action rifle, and one .38-caliber Smith & Wesson Model 342 revolver.[1][96][97][98] The AR-15 rifles were fitted with vertical forward grips and bump fire stocks,[1][96] the latter of which allowed for recoil to actuate their triggers at a rate of 90 rounds in 10 seconds.[99] The AR-10 rifles were equipped with various telescopic sights and mounted on bipods.[1][100][101] The Bureau of Alcohol, Tobacco, Firearms and Explosives determined that the firearms found in his hotel room, along with more guns found in his homes, had been legally purchased in Nevada, California, Texas, and Utah.[102] In the month preceding the shooting, he had attempted to purchase tracer ammunition, but the gun dealer he approached did not have the item in stock.[103] He bought tracer ammunition from a private seller at a Phoenix, Arizona gun show.[104]\\r\\nDuring subsequent investigations, ammonium nitrate (often used in improvised explosive devices) was found in the trunk of his Hyundai Tucson SUV, along with 1,600 rounds of ammunition and 50 pounds (23?kg) of Tannerite, a binary explosive used to make explosive targets for gun ranges.[105][106] Undersheriff Kevin McMahill said that while Paddock had \\"nefarious intent\\" with the material, he did not appear to have assembled an explosive device.[103][107]\\r\\nIn the hours and days after the shooting, false information and fake news about the shooter's identity and motive went viral on social media:\\r\\nGoogle and Facebook were criticized for displaying such false news stories in some of their search results.[111][118][119] The two technology companies were said to have failed in their responsibility of keeping false stories from reaching the public.[120] Facebook later said its algorithms were designed to detect and remove false stories, but failed to work adequately in this instance.[118]\\r\\nIn the aftermath of the shooting, some media outlets reported that YouTube search results for information about the shooting returned links to conspiracy videos. YouTube stated that it had tweaked its search algorithm to promote news sources which it considered more authoritative.[121][122] Some experts have stated that the removal of this content ironically fuels conspiracy theories by making a cover-up seem evident.[123]\\r\\nSurvivors of the shooting have been accused of being paid actors, with some having received death threats on social media.[124]\\r\\nConspiracy theorist Alex Jones stated that \\"Vegas is as phony as a three dollar bill or as Obama's birth certificate.\\"[125] Conspiracy theorists claim that there were multiple shooters and that details of the massacre are being covered up for the sake of promoting gun control laws.[123]","input":"What hotel was the shooting at in vegas?"},{"output":"mortgage","context":"A mortgage loan, or simply mortgage, is used either by purchasers of real property to raise funds to buy real estate, or alternatively by existing property owners to raise funds for any purpose, while putting a lien on the property being mortgaged. The loan is \\"secured\\" on the borrower's property through a process known as mortgage origination. This means that a legal mechanism is put into place which allows the lender to take possession and sell the secured property (\\"foreclosure\\" or \\"repossession\\") to pay off the loan in the event the borrower defaults on the loan or otherwise fails to abide by its terms. The word mortgage is derived from a \\"Law French\\" term used by English lawyers in the Middle Ages meaning \\"death pledge\\" and refers to the pledge ending (dying) when either the obligation is fulfilled or the property is taken through foreclosure.[1]\\r\\nA mortgage can also be described as \\"a borrower giving consideration in the form of a collateral for a benefit (loan)\\".\\r\\n\\r\\nMortgage borrowers can be individuals mortgaging their home or they can be businesses mortgaging commercial property (for example, their own business premises, residential property let to tenants, or an investment portfolio). The lender will typically be a financial institution, such as a bank, credit union or building society, depending on the country concerned, and the loan arrangements can be made either directly or indirectly through intermediaries. Features of mortgage loans such as the size of the loan, maturity of the loan, interest rate, method of paying off the loan, and other characteristics can vary considerably. The lender's rights over the secured property take priority over the borrower's other creditors, which means that if the borrower becomes bankrupt or insolvent, the other creditors will only be repaid the debts owed to them from a sale of the secured property if the mortgage lender is repaid in full first.\\r\\n\\r\\nIn many jurisdictions, it is normal for home purchases to be funded by a mortgage loan. Few individuals have enough savings or liquid funds to enable them to purchase property outright. In countries where the demand for home ownership is highest, strong domestic markets for mortgages have developed. Mortgages can either be funded through the banking sector (that is, through short-term deposits) or through the capital markets through a process called \\"securitization\\", which converts pools of mortgages into fungible bonds that can be sold to investors in small denominations.[2]\\r\\n\\r\\nAccording to Anglo-American property law, a mortgage occurs when an owner (usually of a fee simple interest in realty) pledges his or her interest (right to the property) as security or collateral for a loan.  Therefore, a mortgage is an encumbrance (limitation) on the right to the property just as an easement would be, but because most mortgages occur as a condition for new loan money, the word mortgage has become the generic term for a loan secured by such real property.\\r\\nAs with other types of loans, mortgages have an interest rate and are scheduled to amortize over a set period of time, typically 30 years.  All types of real property can be, and usually are, secured with a mortgage and bear an interest rate that is supposed to reflect the lender's risk.\\r\\n\\r\\nMortgage lending is the primary mechanism used in many countries to finance private ownership of residential and commercial property (see commercial mortgages). Although the terminology and precise forms will differ from country to country, the basic components tend to be similar:\\r\\n\\r\\nMany other specific characteristics are common to many markets, but the above are the essential features. Governments usually regulate many aspects of mortgage lending, either directly (through legal requirements, for example) or indirectly (through regulation of the participants or the financial markets, such as the banking industry), and often through state intervention (direct lending by the government, direcct lending by state-owned banks, or sponsorship of various entities). Other aspects that define a specific mortgage market may be regional, historical, or driven by specific characteristics of the legal or financial system.\\r\\n\\r\\nMortgage loans are generally structured as long-term loans, the periodic payments for which are similar to an annuity and calculated according to the time value of money formulae. The most basic arrangement would require a fixed monthly payment over a period of ten to thirty years, depending on local conditions. Over this period the principal component of the loan (the original loan) would be slowly paid down through amortization. In practice, many variants are possible and common worldwide and within each country.\\r\\n\\r\\nLenders provide funds against property to earn interest income, and generally borrow these funds themselves (for example, by taking deposits or issuing bonds). The price at which the lenders borrow money therefore affects the cost of borrowing. Lenders may also, in many countries, sell the mortgage loan to other parties who are interested in receiving the stream of cash payments from the borrower, often in the form of a security (by means of a securitization).\\r\\n\\r\\nMortgage lending will also take into account the (perceived) riskiness of the mortgage loan, that is, the likelihood that the funds will be repaid (usually considered a function of the creditworthiness of the borrower); that if they are not repaid, the lender will be able to foreclose on the real estate assets; and the financial, interest rate risk and time delays that may be involved in certain circumstances.\\r\\n\\r\\nOnce the mortgage application enters into the final steps, the loan application is moved to a Mortgage Underwriter. The Underwriter verifies the financial information that the applicant has provided to the lender. Verification will be made for the applicants credit history and the value of the home being purchased.[4] An appraisal may be ordered. The financial and employment information of the applicant will also be verified. The underwriting may take a few days to a few weeks. Sometimes the underwriting process takes so long that the provided financial statements need to be resubmitted so they are current.[5] It is advisable to maintain the same employment and not to use or open new credit during the underwriting process. Any changes made in the applicants credit, employment, or financial information can result in the loan being denied.\\r\\n\\r\\nThere are many types of mortgages used worldwide, but several factors broadly define the characteristics of the mortgage. All of these may be subject to local regulation and legal requirements.\\r\\n\\r\\nThe two basic types of amortized loans are the fixed rate mortgage (FRM) and adjustable-rate mortgage (ARM) (also known as a floating rate or variable rate mortgage). In some countries, such as the United States, fixed rate mortgages are the norm, but floating rate mortgages are relatively common. Combinations of fixed and floating rate mortgages are also common, whereby a mortgage loan will have a fixed rate for some period, for example the first five years, and vary after the end of that period.\\r\\n\\r\\n\\r\\nThe charge to the borrower depends upon the credit risk in addition to the interest rate risk. The mortgage origination and underwriting process involves checking credit scores, debt-to-income, downpayments, and assets. Jumbo mortgages and subprime lending are not supported by government guarantees and face higher interest rates. Other innovations described below can affect the rates as well.\\r\\nUpon making a mortgage loan for the purchase of a property, lenders usually require that the borrower make a down payment; that is, contribute a portion of the cost of the property. This down payment may be expressed as a portion of the value of the property (see below for a definition of this term). The loan to value ratio (or LTV) is the size of the loan against the value of the property. Therefore, a mortgage loan in which the purchaser has made a down payment of 20% has a loan to value ratio of 80%. For loans made against properties that the borrower already owns, the loan to value ratio will be imputed against the estimated value of the property.\\r\\n\\r\\nThe loan to value ratio is considered an important indicator of the riskiness of a mortgage loan: the higher the LTV, the higher the risk that the value of the property (in case of foreclosure) will be insufficient to cover the remaining principal of the loan.\\r\\n\\r\\nSince the value of the property is an important factor in understanding the risk of the loan, determining the value is a key factor in mortgage lending. The value may be determined in various ways, but the most common are:\\r\\n\\r\\nIn most countries, a number of more or less standard measures of creditworthiness may be used. Common measures include payment to income (mortgage payments as a percentage of gross or net income); debt to income (all debt payments, including mortgage payments, as a percentage of income); and various net worth measures. In many countries, credit scores are used in lieu of or to supplement these measures. There will also be requirements for documentation of the creditworthiness, such as income tax returns, pay stubs, etc. the specifics will vary from location to location.\\r\\n\\r\\nSome lenders may also require a potential borrower have one or more months of \\"reserve assets\\" available. In other words, the borrower may be required to show the availability of enough assets to pay for the housing costs (including mortgage, taxes, etc.) for a period of time in the event of the job loss or other loss of income.\\r\\n\\r\\nMany countries have lower requirements for certain borrowers, or \\"no-doc\\" / \\"low-doc\\" lending standards that may be acceptable under certain circumstances.\\r\\n\\r\\nMany countries have a notion of standard or conforming mortgages that define a perceived acceptable level of risk, which may be formal or informal, and may be reinforced by laws, government intervention, or market practice. For example, a standard mortgage may be considered to be one with no more than 70ÿ80% LTV and no more than one-third of gross income going to mortgage debt.\\r\\n\\r\\nA standard or conforming mortgage is a key concept as it often defines whether or not the mortgage can be easily sold or securitized, or, if non-standard, may affect the price at which it may be sold. In the United States, a conforming mortgage is one which meets the established rules and procedures of the two major government-sponsored entities in the housing finance market (including some legal requirements). In contrast, lenders who decide to make nonconforming loans are exercising a higher risk tolerance and do so knowing that they face more challenge in reselling the loan. Many countries have similar concepts or agencies that define what are \\"standard\\" mortgages. Regulated lenders (such as banks) may be subject to limits or higher-risk weightings for non-standard mortgages. For example, banks and mortgage brokerages in Canada face restrictions on lending more than 80% of the property value; beyond this level, mortgage insurance is generally required.[6]\\r\\n\\r\\nIn some countries with currencies that tend to depreciate, foreign currency mortgages are common, enabling lenders to lend in a stable foreign currency, whilst the borrower takes on the currency risk that the currency will depreciate and they will therefore need to convert higher amounts of the domestic currency to repay the loan.\\r\\n\\r\\nIn addition to the two standard means of setting the cost of a mortgage loan (fixed at a set interest rate for the term, or variable relative to market interest rates), there are variations in how that cost is paid, and how the loan itself is repaid. Repayment depends on locality, tax laws and prevailing culture. There are also various mortgage repayment structures to suit different types of borrower.\\r\\n\\r\\nThe most common way to repay a secured mortgage loan is to make regular payments toward the principal and interest over a set term.[citation needed]  This is commonly referred to as (self) amortization in the U.S. and as a repayment mortgage in the UK. A mortgage is a form of annuity (from the perspective of the lender), and the calculation of the periodic payments is based on the time value of money formulas. Certain details may be specific to different locations: interest may be calculated on the basis of a 360-day year, for example; interest may be compounded daily, yearly, or semi-annually; prepayment penalties may apply; and other factors. There may be legal restrictions on certain matters, and consumer protection laws may specify or prohibit certain practices.\\r\\n\\r\\nDepending on the size of the loan and the prevailing practice in the country the term may be short (10 years) or long (50 years plus).  In the UK and U.S., 25 to 30 years is the usual maximum term (although shorter periods, such as 15-year mortgage loans, are common).  Mortgage payments, which are typically made monthly, contain a repayment of the principal and an interest element.  The amount going toward the principal in each payment varies throughout the term of the mortgage.  In the early years the repayments are mostly interest.  Towards the end of the mortgage, payments are mostly for principal. In this way the payment amount determined at outset is calculated to ensure the loan is repaid at a specified date in the future.  This gives borrowers assurance that by maintaining repayment the loan will be cleared at a specified date, if the interest rate does not change.  Some lenders and 3rd parties offer a bi-weekly mortgage payment program designed to accelerate the payoff of the loan.\\r\\n\\r\\nAn amortization schedule is typically worked out taking the principal left at the end of each month, multiplying by the monthly rate and then subtracting the monthly payment. This is typically generated by an amortization calculator using the following formula:\\r\\n\\r\\nwhere:\\r\\n\\r\\nThe main alternative to a principal and interest mortgage is an interest-only mortgage, where the principal is not repaid throughout the term.  This type of mortgage is common in the UK, especially when associated with a regular investment plan.  With this arrangement regular contributions are made to a separate investment plan designed to build up a lump sum to repay the mortgage at maturity.  This type of arrangement is called an investment-backed mortgage or is often related to the type of plan used: endowment mortgage if an endowment policy is used, similarly a Personal Equity Plan (PEP) mortgage, Individual Savings Account (ISA) mortgage or pension mortgage.  Historically, investment-backed mortgages offered various tax advantages over repayment mortgages, although this is no longer the case in the UK.  Investment-backed mortgages are seen as higher risk as they are dependent on the investment making sufficient return to clear the debt.\\r\\n\\r\\nUntil recently[when?] it was not uncommon for interest only mortgages to be arranged without a repayment vehicle, with the borrower gambling that the property market will rise sufficiently for the loan to be repaid by trading down at retirement (or when rent on the property and inflation combine to surpass the interest rate)[citation needed].\\r\\n\\r\\nRecent Financial Services Authority guidelines to UK lenders regarding interest-only mortgages has tightened the criteria on new lending on an interest-only basis. The problem for many people has been the fact that no repayment vehicle had been implemented, or the vehicle itself (e.g. endowment/ISA policy) performed poorly and therefore insufficient funds were available to repay balance at the end of the term.\\r\\n\\r\\nMoving forward, the FSA under the Mortgage Market Review (MMR) have stated there must be strict criteria on the repayment vehicle being used. As such the likes of Nationwide and other lenders have pulled out of the interest-only market.\\r\\n\\r\\nA resurgence in the equity release market has been the introduction of interest-only lifetime mortgages. Where an interest-only mortgage has a fixed term, an interest-only lifetime mortgage will continue for the rest of the mortgagors life. These schemes have proved of interest to people who do like the roll-up effect (compounding) of interest on traditional equity release schemes.\\r\\nThey have also proved beneficial to people who had an interest-only mortgage with no repayment vehicle and now need to settle the loan. These people can now effectively remortgage onto an interest-only lifetime mortgage to maintain continuity.\\r\\n\\r\\nInterest-only lifetime mortgage schemes are offered by two lenders currently ÿ Stonehaven & more2life. They work by having the options of paying the interest on a monthly basis. By paying off the interest means the balance will remain level for the rest of their life. This market is set to increase as more retirees require finance in retirement.\\r\\n\\r\\nFor older borrowers (typically in retirement), it may be possible to arrange a mortgage where neither the principal nor interest is repaid.  The interest is rolled up with the principal, increasing the debt each year.\\r\\n\\r\\nThese arrangements are variously called reverse mortgages, lifetime mortgages or equity release mortgages (referring to home equity), depending on the country.  The loans are typically not repaid until the borrowers are deceased, hence the age restriction.\\r\\n\\r\\nThrough the Federal Housing Administration, the U.S. government insures reverse mortgages via a program called the HECM (Home Equity Conversion Mortgage). Unlike standard mortgages (where the entire loan amount is typically disbursed at the time of loan closing) the HECM program allows the homeowner to receive funds in a variety of ways: as a one time lump sum payment; as a monthly tenure payment which continues until the borrower dies or moves out of the house permanently; as a monthly payment over a defined period of time; or as a credit line.[7]\\r\\n\\r\\nFor further details, see equity release.\\r\\n\\r\\nIn the U.S. a partial amortization or balloon loan is one where the amount of monthly payments due are calculated (amortized) over a certain term, but the outstanding balance on the principal is due at some point short of that term.  In the UK, a partial repayment mortgage is quite common, especially where the original mortgage was investment-backed.\\r\\n\\r\\nGraduated payment mortgage loan have increasing costs over time and are geared to young borrowers who expect wage increases over time. Balloon payment mortgages have only partial amortization, meaning that amount of monthly payments due are calculated (amortized) over a certain term, but the outstanding principal balance is due at some point short of that term, and at the end of the term a balloon payment is due. When interest rates are high relative to the rate on an existing seller's loan, the buyer can consider assuming the seller's mortgage.[8] A wraparound mortgage is a form of seller financing that can make it easier for a seller to sell a property. A biweekly mortgage has payments made every two weeks instead of monthly.\\r\\n\\r\\nBudget loans include taxes and insurance in the mortgage payment;[9] package loans add the costs of furnishings and other personal property to the mortgage. Buydown mortgages allow the seller or lender to pay something similar to points to reduce interest rate and encourage buyers.[10] Homeowners can also take out equity loans in which they receive cash for a mortgage debt on their house. Shared appreciation mortgages are a form of equity release. In the US, foreign nationals due to their unique situation face Foreign National mortgage conditions.\\r\\n\\r\\nFlexible mortgages allow for more freedom by the borrower to skip payments or prepay. Offset mortgages allow deposits to be counted against the mortgage loan. In the UK there is also the endowment mortgage where the borrowers pay interest while the principal is paid with a life insurance policy.\\r\\n\\r\\nCommercial mortgages typically have different interest rates, risks, and contracts than personal loans. Participation mortgages allow multiple investors to share in a loan. Builders may take out blanket loans which cover several properties at once. Bridge loans may be used as temporary financing pending a longer-term loan. Hard money loans provide financing in exchange for the mortgaging of real estate collateral.\\r\\n\\r\\nIn most jurisdictions, a lender may foreclose the mortgaged property if certain conditions occur ÿ principally, non-payment of the mortgage loan. Subject to local legal requirements, the property may then be sold. Any amounts received from the sale (net of costs) are applied to the original debt. In some jurisdictions, mortgage loans are non-recourse loans: if the funds recouped from sale of the mortgaged property are insufficient to cover the outstanding debt, the lender may not have recourse to the borrower after foreclosure. In other jurisdictions, the borrower remains responsible for any remaining debt.\\r\\n\\r\\nIn virtually all jurisdictions, specific procedures for foreclosure and sale of the mortgaged property apply, and may be tightly regulated by the relevant government. There are strict or judicial foreclosures and non-judicial foreclosures, also known as power of sale foreclosures. In some jurisdictions, foreclosure and sale can occur quite rapidly, while in others, foreclosure may take many months or even years. In many countries, the ability of lenders to foreclose is extremely limited, and mortgage market development has been notably slower.\\r\\n\\r\\nA study issued by the UN Economic Commission for Europe compared German, US, and Danish mortgage systems. The German Bausparkassen have reported nominal interest rates of approximately 6 per cent per annum in the last 40 years (as of 2004). In addition, they charge administration and service fees (about 1.5 per cent of the loan amount). However, in the United States, the average interest rates for fixed-rate mortgages in the housing market started in the tens and twenties in the 1980s and have (as of 2004) reached about 6 per cent per annum. However, gross borrowing costs are substantially higher than the nominal interest rate and amounted for the last 30 years to 10.46 per cent. In Denmark, similar to the United States mortgage market, interest rates have fallen to 6 per cent per annum. A risk and administration fee amounts to 0.5 per cent of the outstanding debt. In addition, an acquisition fee is charged which amounts to one per cent of the principal.[11]\\r\\n\\r\\nThe mortgage industry of the United States is a major financial sector. The federal government created several programs, or government sponsored entities, to foster mortgage lending, construction and encourage home ownership. These programs include the Government National Mortgage Association (known as Ginnie Mae), the Federal National Mortgage Association (known as Fannie Mae) and the Federal Home Loan Mortgage Corporation (known as Freddie Mac).[2]\\r\\n\\r\\nThe US mortgage sector has been the center of major financial crises over the last century. Unsound lending practices resulted in the National Mortgage Crisis of the 1930s, the savings and loan crisis of the 1980s and 1990s and the subprime mortgage crisis of 2007 which led to the 2010 foreclosure crisis.[2]\\r\\n\\r\\nIn the United States, the mortgage loan involves two separate documents: the mortgage note (a promissory note) and the security interest evidenced by the \\"mortgage\\" document; generally, the two are assigned together, but if they are split traditionally the holder of the note and not the mortgage has the right to foreclose.[12] For example, Fannie Mae promulgates a standard form contract Multistate Fixed-Rate Note 3200[13] and also separate security instrument mortgage forms which vary by state.[14]\\r\\n\\r\\nIn Canada, the Canada Mortgage and Housing Corporation (CMHC) is the country's national housing agency, providing mortgage loan insurance, mortgage-backed securities, housing policy and programs, and housing research to Canadians.[15] It was created by the federal government in 1946 to address the country's post-war housing shortage, and to help Canadians achieve their homeownership goals.\\r\\n\\r\\nThe most common mortgage in Canada is the five-year fixed-rate closed mortgage, as opposed to the U.S. where the most common type is the 30-year fixed-rate open mortgage.[16] Throughout the financial crisis and the ensuing recession, Canadas mortgage market continued to function well, partly due to the residential mortgage market's policy framework, which includes an effective regulatory and supervisory regime that applies to most lenders. Since the crisis however, the low interest rate environment that as arisen has contributed to a significant increases in mortgage debt in the country.[17]\\r\\n\\r\\nIn April 2014, the Office of the Superintendent of Financial Institutions (OSFI) released guidelines for mortgage insurance providers aimed at tightening standards around underwriting and risk management. In a statement, the OSFI has stated that the guideline will provide clarity about best practices in respect of residential mortgage insurance underwriting, which contribute to a stable financial system. This comes after several years of federal government scrutiny over the CMHC, with former Finance Minister Jim Flaherty musing publicly as far back as 2012 about privatizing the Crown corporation.[18]\\r\\n\\r\\nIn an attempt to cool down the real estate prices in Canada, Ottawa introduced a mortgage stress test effective 17 October, 2016. Under the stress test every home buyer with less than 20% down payment (high ratio) undergo a test where borrowers affordability is judged based on mortgage rate of 4.64% with 25 years amortization if they want to get a mortgage from any federally regulated lender.This stress test has lowered the maximum mortgage approved amount by almost 20% for all borrowers in Canada. Maximum amortization on home mortgages has been reduced back to 30 years instead of 35.\\r\\n\\r\\nThe mortgage industry of the United Kingdom has traditionally been dominated by building societies, but from the 1970s the share of the new mortgage loans market held by building societies has declined substantially. Between 1977 and 1987, the share fell from 96% to 66% while that of banks and other institutions rose from 3% to 36%. There are currently over 200 significant separate financial organizations supplying mortgage loans to house buyers in Britain. The major lenders include building societies, banks, specialized mortgage corporations, insurance companies, and pension funds.\\r\\n\\r\\nIn the UK variable-rate mortgages are more common than in the United States.[19][20] This is in part because mortgage loan financing relies less on fixed income securitized assets (such as mortgage-backed securities) than in the United States, Denmark, and Germany, and more on retail savings deposits like Australia and Spain.[19][20] Thus, lenders prefer variable-rate mortgages to fixed rate ones and whole-of-term fixed rate mortgages are generally not available. Nevertheless, in recent years fixing the rate of the mortgage for short periods has become popular and the initial two, three, five and, occasionally, ten years of a mortgage can be fixed.[21] From 2007 to the beginning of 2013 between 50% and 83% of new mortgages had initial periods fixed in this way.[22]\\r\\n\\r\\nHome ownership rates are comparable to the United States, but overall default rates are lower.[19] Prepayment penalties during a fixed rate period are common, whilst the United States has discouraged their use.[19] Like other European countries and the rest of the world, but unlike most of the United States, mortgages loans are usually not nonrecourse debt, meaning debtors are liable for any loan deficiencies after foreclosure.[19][23]\\r\\n\\r\\nThe customer-facing aspects of the residential mortgage sector are regulated by the Financial Conduct Authority (FCA), and lenders' financial probity is overseen by a separate regulator, the Prudential Regulation Authority (PRA) which is part of the Bank of England. The FCA and PRA were established in 2013 with the aim of responding to criticism of regulatory failings highlighted by the financial crisis of 2007ÿ2008 and its aftermath.[24][25][26]\\r\\n\\r\\nIn most of Western Europe (except Denmark, the Netherlands and Germany), variable-rate mortgages are more common, unlike the fixed-rate mortgage common in the United States.[19][20] Much of Europe has home ownership rates comparable to the United States, but overall default rates are lower in Europe than in the United States.[19] Mortgage loan financing relies less on securitizing mortgages and more on formal government guarantees backed by covered bonds (such as the Pfandbriefe) and deposits, except Denmark and Germany where asset-backed securities are also common.[19][20] Prepayment penalties are still common, whilst the United States has discouraged their use.[19] Unlike much of the United States, mortgage loans are usually not nonrecourse debt.[19]\\r\\n\\r\\nWithin the European Union, covered bonds market volume (covered bonds outstanding) amounted to about EUR 2 trillion at year-end 2007 with Germany, Denmark, Spain, and France each having outstandings above 200,000 EUR million.[27] Pfandbrief-like securities have been introduced in more than 25 European countriesand in recent years also in the U.S. and other countries outside Europeeach with their own unique law and regulations.[11]\\r\\n\\r\\nOn July 28, 2008, US Treasury Secretary Henry Paulson announced that, along with four large U.S. banks, the Treasury would attempt to kick start a market for these securities in the United States, primarily to provide an alternative form of mortgage-backed securities.[28] Similarly, in the UK \\"the Government is inviting views on options for a UK framework to deliver more affordable long-term fixed-rate mortgages, including the lessons to be learned from international markets and institutions\\".[29]\\r\\n\\r\\nGeorge Soros's October 10, 2008 Wall Street Journal editorial promoted the Danish mortgage market model.[30]\\r\\n\\r\\nMortgages in Malaysia can be categorised into 2 different groups: conventional home loan and Islamic home loan. Under the conventional home loan, banks normally charge a fixed interest rate, a variable interest rate, or both. These interest rates are tied to a base rate (individual bank's benchmark rate).\\r\\n\\r\\nFor Islamic home financing, it follows the Sharia Law and comes in 2 common types: Bai Bithaman Ajil (BBA) or Musharakah Mutanaqisah (MM). Bai' Bithaman Ajil is when the bank buys the property at current market price and sells it back to you at a much higher price. Musharakah Mutanaqisah is when the bank buys the property together with you. You will then slowly buy the bank's portion of the property through rental (whereby a portion of the rental goes to paying for the purchase of a part of the bank's share in the property until the property comes to your complete ownership).\\r\\n\\r\\nIslamic Sharia law prohibits the payment or receipt of interest, meaning that Muslims cannot use conventional mortgages.  However, real estate is far too expensive for most people to buy outright using cash: Islamic mortgages solve this problem by having the property change hands twice. In one variation, the bank will buy the house outright and then act as a landlord. The homebuyer, in addition to paying rent, will pay a contribution towards the purchase of the property. When the last payment is made, the property changes hands.[clarification needed]\\r\\n\\r\\nTypically, this may lead to a higher final price for the buyers. This is because in some countries (such as the United Kingdom and India) there is a stamp duty which is a tax charged by the government on a change of ownership. Because ownership changes twice in an Islamic mortgage, a stamp tax may be charged twice. Many other jurisdictions have similar transaction taxes on change of ownership which may be levied.  In the United Kingdom, the dual application of stamp duty in such transactions was removed in the Finance Act 2003 in order to facilitate Islamic mortgages.[31]\\r\\n\\r\\nAn alternative scheme involves the bank reselling the property according to an installment plan, at a price higher than the original price.\\r\\n\\r\\nBoth of these methods compensate the lender as if they were charging interest, but the loans are structured in a way that in name they are not, and the lender shares the financial risks involved in the transaction with the homebuyer.[citation needed]\\r\\n\\r\\nBali, Indonesia is an exception to the rule of most home purchase being funded by a mortgage. Instead, most properties there are paid with cash due to the lack of available mortgages.[32]\\r\\n\\r\\nMortgage insurance is an insurance policy designed to protect the mortgagee (lender) from any default by the mortgagor (borrower). It is used commonly in loans with a loan-to-value ratio over 80%, and employed in the event of foreclosure and repossession.\\r\\n\\r\\nThis policy is typically paid for by the borrower as a component to final nominal (note) rate, or in one lump sum up front, or as a separate and itemized component of monthly mortgage payment. In the last case, mortgage insurance can be dropped when the lender informs the borrower, or its subsequent assigns, that the property has appreciated, the loan has been paid down, or any combination of both to relegate the loan-to-value under 80%.\\r\\n\\r\\nIn the event of repossession, banks, investors, etc. must resort to selling the property to recoup their original investment (the money lent) and are able to dispose of hard assets (such as real estate) more quickly by reductions in price. Therefore, the mortgage insurance acts as a hedge should the repossessing authority recover less than full and fair market value for any hard asset.","input":"What is the typical term for a mortgage loan?"},{"output":"New Zealand dollar","context":"?New Zealand\\r\\nThe New Zealand dollar (sign: $; code: NZD) (Mori: Tra o Aotearoa) is the currency and legal tender of New Zealand, the Cook Islands, Niue, the Ross Dependency, Tokelau, and a British territory, the Pitcairn Islands.[1] Within New Zealand, it is almost always abbreviated with the dollar sign ($), with \\"NZ$\\" sometimes used to distinguish it from other dollar-denominated currencies. In the context of currency trading, it is often informally called the \\"Kiwi\\",[2] since New Zealand is commonly associated with the indigenous bird and the one-dollar coin depicts a kiwi.\\r\\nIntroduced in 1967, the dollar is subdivided into 100 cents. Altogether there are ten denominationsfive coins and five banknoteswith the smallest being the 10-cent coin. Formerly there were lower denominations, but those were discontinued due to inflation and production costs.\\r\\nThe New Zealand dollar is consistently one of the 10 most-traded currencies in the world, being approximately 2.0% of global foreign exchange market daily turnover in 2013.[3]\\r\\n\\r\\n\\r\\nPrior to the introduction of the New Zealand dollar in 1967, the New Zealand pound was the currency of New Zealand, which had been distinct from the pound sterling since 1933. The pound used the Ssd system, in which the pound was divided into 20 shillings and one shilling was divided into 12 pence, which by the 1950s was considered complicated and cumbersome.\\r\\nSwitching to decimal currency had been proposed in New Zealand since the 1930s, although only in the 1950s did any plans come to fruition. In 1957, a committee was set up by the Government to investigate decimal currency. The idea fell on fertile ground, and in 1963, the Government decided to decimalise New Zealand currency.[4] The Decimal Currency Act was passed in 1964, setting the date of transition to 10 July 1967.[5] Words such as \\"fern\\", \\"kiwi\\" and \\"zeal\\" were proposed to avoid confusion with the word \\"dollar\\", which many people at the time associated with the United States dollar.[6][7] In the end, the word \\"dollar\\" was chosen anyway, and an anthropomorphic dollar note cartoon character called \\"Mr. Dollar\\" became the symbol of transition in a huge publicity campaign.[8]\\r\\nOn Monday 10 July 1967 (\\"Decimal Currency Day\\"), the New Zealand dollar was introduced to replace the pound at a rate of two dollars to one pound (one dollar to ten shillings, ten cents to one shilling, ?5?6 cent to a penny).[9] Some 27 million new banknotes were printed and 165 million new coins were minted for the changeover.[6]\\r\\nThe New Zealand dollar was initially pegged to the US dollar at US$1.43 = NZ$1. This rate changed on 21 November of the same year to US$1.12 = NZ$1 after the devaluation of the British pound (see Bretton Woods system), although New Zealand devalued more than the UK.[10]\\r\\nIn 1971 the US devalued its dollar relative to gold, leading New Zealand on 23 December to peg its dollar at US$1.216 with a 4.5% fluctuation range, keeping the same gold value. From 9 July 1973 to 4 March 1985 the dollar's value was determined from a trade-weighted basket of currencies.\\r\\nThe NZ$ was floated on 4 March 1985 at the initial rate of US$0.4444. Since then the dollar's value has been determined by the financial markets, and has been in the range of about US$0.39 to 0.88.\\r\\nThe dollar's post-float low was US$0.3922 on 22 November 2000, and it reached a post-float high on 9 July 2014 of US$0.8821. Much of this medium-term variation in the exchange rate has been attributed to differences in interest rates.[citation needed]\\r\\nThe New Zealand dollar's value is often strongly affected by currency trading,[citation needed] and is among the 10 most-traded currencies.[11]\\r\\nOn 11 June 2007 the Reserve Bank sold an unknown worth of New Zealand dollars for nine billion USD in an attempt to drive down its value. This is the first intervention in the markets by the Bank since the float in 1985.\\r\\nTwo suspected interventions followed, but they were not as successful as the first: the first appeared to be initially effective, with the dollar dropping to approximately US$0.7490 from near US$0.7620. However, within little more than a month it had risen to new post-float highs, reaching US$0.8103 on 23 July 2007.\\r\\nAfter reaching its post-float record high in early 2008, the value of the NZ$ plummeted throughout much of the 2nd half of 2008 and the first quarter of 2009 as a response to the global economic downturn and flight by investors away from \\"riskier\\" currencies such as the NZ$. The NZ$ bottomed out at approximately US$0.50 on 6 March 2009.[12] However, it rebounded strongly as the year progressed, reaching the US$0.75 range by November 2009.[12]\\r\\nBy late 2012, the dollar was holding above 80 US cents, occasionally reaching 85c, prompting calls from the Green Party for quantitative easing.[13][14] Unions also called on the Government and the Reserve Bank to take action, but as of February 2013 both had declined.[15]\\r\\nAs of early June, 2017, the NZD was trading at approximately US$0.71.[16]\\r\\nOn the introduction of the dollar, coins came in denominations of 1c, 2c, 5c, 10c, 20c, and 50c. The 1c and 2c coins were bronze, the others were cupro-nickel.[17] To ease transition, the 5c, 10c and 20c were the same size as the sixpence, shilling and florin that they respectively replaced. Until 1970 the 10c coin bore the additional legend \\"One Shilling\\". The obverse designs of all the coins featured Arnold Machin's portrait of Queen Elizabeth II, with the legend ELIZABETH II NEW ZEALAND [date]. The reverse sides of coins introduced in 1967 did not follow the designs that were originally intended for them. Those modern art and sculpture themed designs were leaked to a newspaper and met a very negative public reaction. The final releases were given more conservative designs in line with public expectations.\\r\\nIn 1986, New Zealand adopted Raphael Maklouf's new portrait of the Queen. The 1c and 2c coins were last minted for circulation in 1987, with collector coins being made for 1988. The coins were demonetised on 30 April 1990.[17] The lack of 1c and 2c coins meant that cash transactions were normally rounded to the nearest 5c (10c from 2006), a process known as Swedish rounding.\\r\\nOn 11 February 1991, aluminium-bronze $1 and $2 coins were introduced to replace existing $1 and $2 notes.[17] In 1999, Ian Rank-Broadley's portrait of the Queen was introduced and the legend rearranged to read NEW ZEALAND ELIZABETH II.\\r\\nOn 11 November 2004 the Reserve Bank announced that it proposed to take the 5c coin out of circulation and to make the 50c, 20c and 10c coins smaller and use plated steel to make them lighter. After a three-month public submission period that ended on 4 February 2005, the Reserve Bank announced on 31 March that it would go ahead with the proposed changes. The changeover period started on 31 July 2006, with the old coins usable until 31 October 2006.[17] The old 50c, 20c, 10c and 5c pieces are now no longer legal tender, but are still redeemable at the Reserve Bank. Prior to the change over these coins were similar, save for the legend and reverse artwork, to international (mainly Commonwealth) coins of the same British-derived sizes, which led to coins from other currencies, particularly older coins, being accepted by vending machines and many retailers.\\r\\nIn 1967, notes were introduced in denominations of $1, $2, $5, $10, $20 and $100, with all except the $5 replacing their pound predecessors. The original series of dollar notes featured on the obverse a portrait of Queen Elizabeth II wearing Queen Alexandra's Kokoshnik tiara, King George's VI festoon necklace, and Queen Mary's floret earrings, while the reverse featured native birds and plants.[19] The notes were changed slightly in 1981 due to a change of printer (from De La Rue to Bradbury, Wilkinson & Co.) - the most noticeable difference being the portrait based upon a photograph by Peter Grugeon, in which Queen Elizabeth II is wearing Grand Duchess Vladimir's tiara and Queen Victoria's golden jubilee necklace.[19] The $50 note was added in 1983 to fill the long gap between the $20 and the $100 notes. $1 and $2 notes were discontinued in 1991 after being replaced with coins.\\r\\nA new series of notes was introduced in 1992. The obverse of each note featured a notable New Zealander, while the reverse featured a native New Zealand bird and New Zealand scenery. In 1999, polymer notes replaced the paper notes. The designs remained much the same, but were changed slightly to accommodate new security features, with the most obvious changes being the two transparent windows.\\r\\nNew banknotes are being printed in New Zealand at the moment. The new notes are the same sizes and denominations as the older banknotes, and they will continue to be made of the same flexible plastic material. The themes of the notes remain the same, with the same respected New Zealanders, the Queen, and flora and fauna remaining central to the designs. The $5 and $10 notes were released in October 2015, with the $20, $50 and $100 notes set to release in April 2016. The old New Zealand banknotes and the new 'Brighter Money' banknotes can be used interchangeably for the time being.[20]\\r\\nSince the older banknotes were first issued in 1999, security features and the technology for designing and printing banknotes have all advanced considerably. And while counterfeiting rates in New Zealand are low compared to the rest of the world, the New Zealand public and government agree that it is in the best interest to \\"stay one step ahead of the game\\", hence the new notes.[20]\\r\\nNew Zealand $5 Note (sixth-issue)\\r\\nNew Zealand $10 Note (sixth-issue)\\r\\nNew Zealand $20 Note (sixth-issue)\\r\\nNew Zealand $50 Note (sixth-issue)\\r\\nNew Zealand $100 Note (sixth-issue)\\r\\nNew Zealands five-dollar note has been named the banknote of the year for 2015, a clear winner among nearly 40 eligible designs from a record 20 countries. The Reserve Bank of New Zealand released the new $5 and $10 notes in October as part of its Brighter Money range. The $5 shows mountaineer Sir Edmund Hillary facing the South Islands Aoraki/Mount Cook, and, on the other side, a rare yellow-eyed penguin and local flora.\\r\\nWith the breakdown of the Bretton Woods system in 1971, both Australia and New Zealand converted the mostly-fixed foreign exchange regimes to a moving peg against the US dollar.\\r\\nIn September 1974, Australia moved to a peg against a basket of currencies called the trade weighted index (TWI) in an effort to reduce fluctuations associated with its peg to the US dollar. The peg to the TWI was changed to a moving peg in November 1976, causing the actual value of the peg to be periodically adjusted.\\r\\nSince the late 1990s, and certainly since the end of the Cold War the US dollar has had less and less overall influence over the value of both the NZ$ and A$ against other currencies.[citation needed]\\r\\nThe New Zealand dollar contributes greatly to the total global exchange market - far in excess of New Zealand's relative share of population or global GDP.\\r\\nAccording to the Bank for International Settlements, the New Zealand dollar's share of global foreign exchange market daily turnover in 2016 was 2.1% (up from 1.6% in 2010) giving it a rank of 11th.[3] Trading in the currency has climbed steadily since the same survey in 1998 when the NZD's ranking was 17th and the share of turnover was just 0.2%.","input":"What kind of currency is used in new zealand?"},{"output":"Denali","context":"The following sortable table comprises the 200 highest mountain peaks of greater North America[1] with at least 500 meters (1640 feet) of topographic prominence.[2]\\r\\n\\r\\nThe summit of a mountain or hill may be measured in three principal ways:\\r\\n\\r\\nIn greater North America, only Denali exceeds 6000 meters (19,685 feet) elevation.  Three major summits exceed 5500 meters (18,045 feet), 11 exceed 5000 meters (16,404 feet), 21 exceed 4500 meters (14,764 feet), 124 exceed 4000 meters (13,123 feet), 277 exceed 3500 meters (11,483 feet), and 401 exceed 3000 meters' (9843 feet) elevation.\\r\\n\\r\\nOf these 200 highest major summits of North America, 160 are located in the United States, 30 in Canada, 11 in Mxico, six in Guatemala, and one in Costa Rica.  Seven of these peaks lie on the Canada-United States border and one lies on the Mxico-Guatemala border.\\r\\n\\r\\n1. Denali in Alaska is the highest summit of the United States and North America.\\r\\n\\r\\n2. Mount Logan in Yukon is the highest summit of Canada.\\r\\n\\r\\n3. Pico de Orizaba is the highest summit of Mxico.\\r\\n\\r\\n4. Mount Saint Elias is the second-highest summit of both Canada and the United States.\\r\\n\\r\\n5. Popocatpetl is the second-highest summit of Mxico.\\r\\n\\r\\n6. Mount Foraker is the second-highest major summit of the Alaska Range.\\r\\n\\r\\n7. Mount Lucania in Yukon is the highest summit of the northern Saint Elias Mountains.\\r\\n\\r\\n8. Iztaccihuatl is the third-highest summit of Mxico.\\r\\n\\r\\n9. King Peak in Yukon is the fourth-highest summit of Canada.\\r\\n\\r\\n10. Mount Bona in Alaska is the highest volcano in the United States.\\r\\n\\r\\n11. Mount Steele in Yukon is the fifth-highest summit of Canada.\\r\\n\\r\\n12. Mount Blackburn in Alaska is the highest summit of the Wrangell Mountains.\\r\\n\\r\\n13. Mount Sanford in Alaska is the third-highest volcano in the United States.\\r\\n\\r\\n17. Nevado de Toluca is the fourth-highest summit of Mxico.\\r\\n\\r\\n18. Mount Fairweather on the Alaska border is the highest summit of British Columbia.\\r\\n\\r\\n24. Mount Whitney is the highest summit of the Sierra Nevada and California.\\r\\n\\r\\n27. Mount Elbert is the highest summit of Colorado and the Rocky Mountains.\\r\\n\\r\\n30. Mount Rainier is the highest summit of Washington and the Cascade Range.\\r\\n\\r\\n39.  Grays Peak in Colorado is the highest point on the Continental Divide in North America.\\r\\n\\r\\n48. Mount Shasta in California is the highest summit of the southern Cascade Range.\\r\\n\\r\\n53. Pikes Peak in Colorado was the inspiration for America the Beautiful.\\r\\n\\r\\n60.  Nevado de Colima is the highest summit of Jalisco.\\r\\n\\r\\n69. Volcn Tajumulco is the highest summit in Guatemala and all of Central America.\\r\\n\\r\\n72. Mount Hayes is the highest summit of the eastern Alaska Range.\\r\\n\\r\\n75.  Cofre de Perote is the highest summit of Veracruz.\\r\\n\\r\\n76. Gannett Peak is the highest summit of the Wind River Range and Wyoming.\\r\\n\\r\\n78. Grand Teton in Wyoming is the highest summit of the Teton Range.\\r\\n\\r\\n93. Kings Peak is the highest summit of the Uinta Range and Utah.\\r\\n\\r\\n102.  Volcn Tacan is the second-highest summit of Central America.\\r\\n\\r\\n118. Wheeler Peak is the highest summit of New Mexico.\\r\\n\\r\\n128.  Wheeler Peak in Nevada is the highest summit of the Snake Range.\\r\\n\\r\\n135. Mount Robson in British Columbia is the highest summit of the Canadian Rockies.\\r\\n\\r\\n143. Granite Peak is the highest summit of the Beartooth Range and Montana.\\r\\n\\r\\n151. Borah Peak is the highest summit of the Lost River Range and Idaho.\\r\\n\\r\\n155. Humphreys Peak is the highest summit of the San Francisco Peaks and Arizona.\\r\\n\\r\\n171. Chirrip܇ Grande is the highest summit of Costa Rica.\\r\\n\\r\\n198. Mount Columbia on the British Columbia border is the highest summit of Alberta.\\r\\n\\r\\nCoordinates: 630408N 1510023W? / ?63.0690N 151.0063W? / 63.0690; -151.0063? (Denali)","input":"What is the tallest mountain in the north america?"},{"output":"Tiwanaku","context":"The Inca Empire (Quechua: Tawantinsuyu, lit.?\\"The Four Regions\\"[2]), also known as the Incan Empire and the Inka Empire, was the largest empire in pre-Columbian America,[3] and possibly the largest empire in the world in the early 16th century.[4] Its political and administrative structure \\"was the most sophisticated found among native peoples\\" in the Americas.[5] The administrative, political and military center of the empire was located in Cusco in modern-day Peru. The Inca civilization arose from the highlands of Peru sometime in the early 13th century. Its last stronghold was conquered by the Spanish in 1572.\\r\\nFrom 1438 to 1533, the Incas incorporated a large portion of western South America, centered on the Andean Mountains, using conquest and peaceful assimilation, among other methods. At its largest, the empire joined Peru, large parts of modern Ecuador, western and south central Bolivia, northwest Argentina, north and central Chile and a small part of southwest Colombia into a state comparable to the historical empires of Eurasia. Its official language was Quechua.[6] Many local forms of worship persisted in the empire, most of them concerning local sacred Huacas, but the Inca leadership encouraged the sun worship of Inti ÿ their sun god ÿ and imposed its sovereignty above other cults such as that of Pachamama.[7] The Incas considered their king, the Sapa Inca, to be the \\"son of the sun.\\"[8]\\r\\nThe Inca Empire was unique in that it lacked many features associated with civilization in the Old World. In the words of one scholar, \\"The Incas lacked the use of wheeled vehicles. They lacked animals to ride and draft animals that could pull wagons and plows... [They] lacked the knowledge of iron and steel... Above all, they lacked a system of writing... Despite these supposed handicaps, the Incas were still able to construct one of the greatest imperial states in human history\\".[9] Notable features of the Inca Empire include its monumental architecture, especially stonework, extensive road network reaching all corners of the empire, finely-woven textiles, use of knotted strings (quipu) for record keeping and communication, agricultural innovations in a difficult environment, and the organization and management fostered or imposed on its people and their labor.\\r\\nThe Incan economy has been described in contradictory ways by scholars: as \\"feudal, slave, socialist (here one may choose between socialist paradise or socialist tyranny)\\".[10] The Inca empire functioned largely without money and without markets. Instead, exchange of goods and services was based on reciprocity between individuals and among individuals, groups, and Inca rulers. \\"Taxes\\" consisted of a labour obligation of a person to the Empire. The Inca rulers (who theoretically owned all the means of production) reciprocated by granting access to land and goods and providing food and drink in celebratory feasts for their subjects.[11]\\r\\n\\r\\n\\r\\nThe Inca referred to their empire as Tawantinsuyu,[2] \\"the four suyu\\". In Quechua, tawa is four and -ntin is a suffix naming a group, so that a tawantin is a quartet, a group of four things taken together, in this case representing the four suyu (\\"regions\\" or \\"provinces\\") whose corners met at the capital. The four suyu were: Chinchaysuyu (north), Antisuyu (east; the Amazon jungle), Qullasuyu (south) and Kuntisuyu (west). The name Tawantinsuyu was, therefore, a descriptive term indicating a union of provinces. The Spanish transliterated the name as Tahuatinsuyo or Tahuatinsuyu.\\r\\nThe term Inka means \\"ruler\\" or \\"lord\\" in Quechua and was used to refer to the ruling class or the ruling family.[12] The Incas were a very small percentage of the total population of the empire, probably numbering only 15,000 to 40,000, but ruling a population of around 10 million persons.[13] The Spanish adopted the term (transliterated as Inca in Spanish) as an ethnic term referring to all subjects of the empire rather than simply the ruling class. As such the name Imperio inca (\\"Inca Empire\\") referred to the nation that they encountered and subsequently conquered.\\r\\nThe Inca Empire was the last chapter of thousands of years of Andean civilization. Andean civilization was one of five civilizations in the world deemed by scholars to be \\"pristine\\", that is indigenous and not derivative from other civilizations.[14]\\r\\nThe Inca Empire was preceded by two large-scale empires in the Andes: the Tiwanaku (c. 300ÿ1100 AD), based around Lake Titicaca and the Wari or Huari (c. 600ÿ1100 AD) centered near the present-day city of Ayacucho. The Wari occupied the Cuzco area for about 400 years. Thus, many of the characteristics of the Inca Empire derived from earlier multi-ethnic and expansive Andean cultures.[15]\\r\\nCarl Troll has argued that the development of the Inca state in the central Andes was aided by conditions that allows for the elaboration of the staple food chu?o. Chu?o, which can be stored for long periods, is made of potato dried at the freezing temperatures that are common at nighttime in the southern Peruvian highlands. Such link between the Inca state and chu?o may be questioned as potatoes and other crops such as maize can also be dried with only sunlight.[16] Troll did also argue that llamas, the Inca's pack animal, can be found in its largest numbers in this very same region.[16] It is worth considering the maximum extent of the Inca Empire roughly coincided with the greatest distribution of llamas and alpacas in Pre-Hispanic America.[17] The link between the Andean biomes of puna and pramo, pastoralism and the Inca state is a matter of research.[18] As a third point Troll pointed out irrigation technology as advantageous to the Inca state-building.[18] While Troll theorized environmental influences on the Inca Empire he opposed environmental determinism arguing that culture lay at the core of the Inca civilization.[18]\\r\\nThe Inca people were a pastoral tribe in the Cusco area around the 12th century. Incan oral history tells an origin story of three caves. The center cave at Tampu T'uqu (Tambo Tocco) was named Qhapaq T'uqu (\\"principal niche\\", also spelled Capac Tocco). The other caves were Maras T'uqu (Maras Tocco) and Sutiq T'uqu (Sutic Tocco).[19] Four brothers and four sisters stepped out of the middle cave. They were: Ayar Manco, Ayar Cachi, Ayar Awqa (Ayar Auca) and Ayar Uchu; and Mama Ocllo, Mama Raua, Mama Huaco and Mama Qura (Mama Cora). Out of the side caves came the people who were to be the ancestors of all the Inca clans.\\r\\nAyar Manco carried a magic staff made of the finest gold. Where this staff landed, the people would live. They traveled for a long time. On the way, Ayar Cachi boasted about his strength and power. His siblings tricked him into returning to the cave to get a sacred llama. When he went into the cave, they trapped him inside to get rid of him.\\r\\nAyar Uchu decided to stay on the top of the cave to look over the Inca people. The minute he proclaimed that, he turned to stone. They built a shrine around the stone and it became a sacred object. Ayar Auca grew tired of all this and decided to travel alone. Only Ayar Manco and his four sisters remained.\\r\\nFinally, they reached Cusco. The staff sank into the ground. Before they arrived, Mama Ocllo had already borne Ayar Manco a child, Sinchi Roca. The people who were already living in Cusco fought hard to keep their land, but Mama Huaca was a good fighter. When the enemy attacked, she threw her bolas (several stones tied together that spun through the air when thrown) at a soldier (gualla) and killed him instantly. The other people became afraid and ran away.\\r\\nAfter that, Ayar Manco became known as Manco Cpac, the founder of the Inca. It is said that he and his sisters built the first Inca homes in the valley with their own hands. When the time came, Manco Cpac turned to stone like his brothers before him. His son, Sinchi Roca, became the second emperor of the Inca.[20]\\r\\nUnder the leadership of Manco Cpac, the Inca formed the small city-state Kingdom of Cusco (Quechua Qusqu', Qosqo). In 1438, they began a far-reaching expansion under the command of Sapa Inca (paramount leader) Pachacuti-Cusi Yupanqui, whose name literally meant \\"earth-shaker\\". The name of Pachacuti was given to him after he conquered the Tribe of Chancas (modern Apurmac). During his reign, he and his son Tupac Yupanqui brought much of the Andes mountains (roughly modern Peru and Ecuador) under Inca control.[21]\\r\\nPachacuti reorganized the kingdom of Cusco into the Tahuantinsuyu, which consisted of a central government with the Inca at its head and four provincial governments with strong leaders: Chinchasuyu (NW), Antisuyu (NE), Kuntisuyu (SW) and Qullasuyu (SE).[22] Pachacuti is thought to have built Machu Picchu, either as a family home or summer retreat, although it may have been an agricultural station.[23]\\r\\nPachacuti sent spies to regions he wanted in his empire and they brought to him reports on political organization, military strength and wealth. He then sent messages to their leaders extolling the benefits of joining his empire, offering them presents of luxury goods such as high quality textiles and promising that they would be materially richer as his subjects.\\r\\nMost accepted the rule of the Inca as a fait accompli and acquiesced peacefully. Refusal to accept Inca rule resulted in military conquest. Following conquest the local rulers were executed. The ruler's children were brought to Cusco to learn about Inca administration systems, then return to rule their native lands. This allowed the Inca to indoctrinate them into the Inca nobility and, with luck, marry their daughters into families at various corners of the empire.\\r\\nTraditionally the son of the Inca ruler led the army. Pachacuti's son T~pac Inca Yupanqui began conquests to the north in 1463 and continued them as Inca ruler after Pachacuti's death in 1471. T~pac Inca's most important conquest was the Kingdom of Chimor, the Inca's only serious rival for the Peruvian coast. T~pac Inca's empire stretched north into modern-day Ecuador and Colombia.\\r\\nT~pac Inca's son Huayna Cpac added a small portion of land to the north in modern-day Ecuador and in parts of Peru. At its height, the Inca Empire included Peru and Bolivia, most of what is now Ecuador and a large portion of what is today Chile, north of the Maule River. The advance south halted after the Battle of the Maule where they met determined resistance from the Mapuche. The empire's push into the Amazon Basin near the Chinchipe River was stopped by the Shuar in 1527.[24] The empire extended into corners of Argentina and Colombia. However, most of the southern portion of the Inca empire, the portion denominated as Qullasuyu, was located in the Altiplano.\\r\\nThe Inca Empire was an amalgamation of languages, cultures and peoples. The components of the empire were not all uniformly loyal, nor were the local cultures all fully integrated. The Inca empire as a whole had an economy based on exchange and taxation of luxury goods and labour. The following quote describes a method of taxation:\\r\\nFor as is well known to all, not a single village of the highlands or the plains failed to pay the tribute levied on it by those who were in charge of these matters. There were even provinces where, when the natives alleged that they were unable to pay their tribute, the Inca ordered that each inhabitant should be obliged to turn in every four months a large quill full of live lice, which was the Inca's way of teaching and accustoming them to pay tribute.[25]\\r\\nSpanish conquistadors led by Francisco Pizarro and his brothers explored south from what is today Panama, reaching Inca territory by 1526.[26] It was clear that they had reached a wealthy land with prospects of great treasure, and after another expedition in 1529 Pizarro traveled to Spain and received royal approval to conquer the region and be its viceroy. This approval was received as detailed in the following quote: \\"In July 1529 the queen of Spain signed a charter allowing Pizarro to conquer the Incas. Pizarro was named governor and captain of all conquests in Peru, or New Castile, as the Spanish now called the land.\\"[27]\\r\\nWhen they returned to Peru in 1532, a war of succession between the sons of Sapa Inca Huayna Capac, Huscar and Atahualpa, and unrest among newly conquered territories weakened the empire. Perhaps more importantly, smallpox, influenza, typhus and measles had spread from Central America.\\r\\nThe forces led by Pizarro consisted of 168 men, one cannon, and 27 horses. Conquistadors ported lances, arquebuses, steel armor and long swords. In contrast, the Inca used weapons made out of wood, stone, copper and bronze, putting them at significant technological disadvantage. In addition, due to the absence of horses in the Americas, the Inca did not develop tactics to fight cavalry. However, the Inca were still effective warriors, being able to successfully fight the Mapuche, which later would strategically defeat the Spanish as they expanded further south.\\r\\nThe first engagement between the Inca and the Spanish was the Battle of Pun, near present-day Guayaquil, Ecuador, on the Pacific Coast; Pizarro then founded the city of Piura in July 1532. Hernando de Soto was sent inland to explore the interior and returned with an invitation to meet the Inca, Atahualpa, who had defeated his brother in the civil war and was resting at Cajamarca with his army of 80,000 troops, that were at the moment armed only with hunting tools (knives and lassos for hunting llamas).\\r\\nPizarro and some of his men, most notably a friar named Vincente de Valverde, met with the Inca, who had brought only a small retinue. The Inca offered them ceremonial chicha in a golden cup, which the Spanish rejected. The Spanish interpreter, Friar Vincente, read the \\"Requerimiento\\" that demanded that he and his empire accept the rule of King Charles I of Spain and convert to Christianity. Atahualpa dismissed the message and asked them to leave. After this, the Spanish began their attack against the mostly unarmed Inca, captured Atahualpa as hostage, and forced the Inca to collaborate.\\r\\nAtahualpa offered the Spaniards enough gold to fill the room he was imprisoned in and twice that amount of silver. The Inca fulfilled this ransom, but Pizarro deceived them, refusing to release the Inca afterwards. During Atahualpa's imprisonment Huscar was assassinated elsewhere. The Spaniards maintained that this was at Atahualpa's orders; this was used as one of the charges against Atahualpa when the Spaniards finally executed him, in August 1533.[28]\\r\\nAlthough \\"defeat\\" often implies an unwanted loss in battle, much of the Inca elite \\"actually welcomed the Spanish invaders as liberators and willingly settled down with them to share rule of Andean farmers and miners.\\"[29]\\r\\nThe Spanish installed Atahualpa's brother Manco Inca Yupanqui in power; for some time Manco cooperated with the Spanish while they fought to put down resistance in the north. Meanwhile, an associate of Pizarro, Diego de Almagro, attempted to claim Cusco. Manco tried to use this intra-Spanish feud to his advantage, recapturing Cusco in 1536, but the Spanish retook the city afterwards. Manco Inca then retreated to the mountains of Vilcabamba and established the small Neo-Inca State, where he and his successors ruled for another 36 years, sometimes raiding the Spanish or inciting revolts against them. In 1572 the last Inca stronghold was conquered and the last ruler, T~pac Amaru, Manco's son, was captured and executed.[30] This ended resistance to the Spanish conquest under the political authority of the Inca state.\\r\\nAfter the fall of the Inca Empire many aspects of Inca culture were systematically destroyed, including their sophisticated farming system, known as the vertical archipelago model of agriculture.[31] Spanish colonial officials used the Inca mita corve labor system for colonial aims, sometimes brutally. One member of each family was forced to work in the gold and silver mines, the foremost of which was the titanic silver mine at Potos. When a family member died, which would usually happen within a year or two, the family was required to send a replacement.[citation needed]\\r\\nThe effects of smallpox on the Inca empire were even more devastating. Beginning in Colombia, smallpox spread rapidly before the Spanish invaders first arrived in the empire. The spread was probably aided by the efficient Inca road system. Smallpox was only the first epidemic.[32] Other diseases, including a probable Typhus outbreak in 1546, influenza and smallpox together in 1558, smallpox again in 1589, diphtheria in 1614, and measles in 1618, all ravaged the Inca people.\\r\\nThe number of people inhabiting Tawantinsuyu at its peak is uncertain, with estimates ranging from 4ÿ37 million. Most population estimates are in the range of 6 to 14 million. In spite of the fact that the Inca kept excellent census records using their quipus, knowledge of how to read them was lost as almost all fell into disuse and disintegrated over time or were destroyed by the Spaniards.[33]\\r\\nThe main form of communication and record-keeping in the empire were quipus, ceramics, textiles and various dialects of Quechua, the language the Incas imposed upon the peoples within the empire. While Quechua had been spoken in the Andean region, including central Peru, for several centuries prior to the expansion of the Inca civilization, the dialect of Quechua the Incas imposed was an adaptation from the Kingdom of Cusco (an early form of \\"Southern Quechua\\" originally named Qhapaq Runasimi, or 'the great language of the people'), or what some historians define as the Cusco dialect.[34][35]\\r\\nThe language imposed by the Incas diverted from its original phonetics as some societies formed their own regional varieties. The diversity of Quechua at that point and even today does not come directly from the Incas, who were just a part of the reason for Quechua's diversity. The civilizations within the empire that had previously spoken Quechua kept their own variety distinct from the Quechua the Incas spread. Although these dialects of Quechua had a similar linguistic structure, they differed according to the region in which they were spoken.[35]\\r\\nAlthough many of the societies within the empire spoke or learned to speak Quechua, others continued to speak their original languages, such as Aymara, which remains in use in contemporary Bolivia, where it is the primary indigenous language and in various regions surrounding Bolivia. The linguistic body of the Inca Empire was thus varied. The Inca's impact outlasted their empire, as the Spanish continued the use of Quechua.[35]\\r\\nThe Incas were not known to develop a written form of communication; however, they visually recorded narratives through paintings on vases and cups (qirus).[36] These paintings are usually accompanied by geometric patterns known as toqapu, which are also found in textiles. Researchers have speculated that toqapu patterns could have served as a form of written communication (e.g.: heraldry, or glyphs), however this remains unclear.[37]\\r\\nIn the Incan Empire, the age of marriage differed for men and women; men typically married at the age of 20, while women usually got married around 4 years earlier at the age of 16.[38] Men who were highly ranked in society could have multiple wives, but those lower in the ranks could only take a single wife.[39] Marriages were typically within classes and resembled a more business-like agreement. Once married, the women were expected to cook, collect food and watch over the children and livestock.[38] Girls and mothers would also work around the house to keep it orderly to please the public inspectors.[40] These duties remained the same even after wives became pregnant and with the added responsibility of praying and making offerings to Kanopa, who was the god of pregnancy.[38] It was typical for marriages to begin on a trial basis with both men and women having a say in the longevity of the marriage. If the man felt that it wouldnt work out or if the woman wanted to return to her parents home the marriage would end. Once the marriage was final, the only way the two could be divorced was if they did not have a child together.[38]\\r\\nThe Inca called newborn infants wawa, a term that they also used for newborn animals. This term was used for all newborn beings without regard to their biological sex. Babies were not given human social status until they reached two or three years of age due to the high infant mortality rates. It was at this time that a ceremony was held called rutuchikuy in which the infant was given its first haircut, name and introduced to the extended family. Also in this ceremony, children advanced from the description of wawa to warma, a gender neutral term for a child who has not developed the language skill set. By the time children reached the age of seven, they had completed gender specific tasks and were referred to as gender specific terms, Thaski for girls and maqta for boys.[41]\\r\\nInca myths were transmitted orally until early Spanish colonists recorded them; however, some scholars claim that they were recorded on quipus, Andean knotted string records.[42]\\r\\nThe Inca believed in reincarnation.[43] After death, the passage to the next world was fraught with difficulties. The spirit of the dead, camaquen, would need to follow a long road and during the trip the assistance of a black dog that could see in the dark was required. Most Incas imagined the after world to be like an earthly paradise with flower-covered fields and snow-capped mountains.\\r\\nIt was important to the Inca that they not die as a result of burning or that the body of the deceased not be incinerated. Burning would cause their vital force to disappear and threaten their passage to the after world. Those who obeyed the Inca moral code ÿ ama suwa, ama llulla, ama quella (do not steal, do not lie, do not be lazy) ÿ \\"went to live in the Sun's warmth while others spent their eternal days in the cold earth\\".[44] The Inca nobility practiced cranial deformation.[45] They wrapped tight cloth straps around the heads of newborns to shape their soft skulls into a more conical form, thus distinguishing the nobility from other social classes.\\r\\nThe Incas made human sacrifices. As many as 4,000 servants, court officials, favorites and concubines were killed upon the death of the Inca Huayna Capac in 1527.[46] The Incas performed child sacrifices around important events, such as the death of the Sapa Inca or during a famine. These sacrifices were known as qhapaq hucha.[47]\\r\\nThe Incas were polytheists who attempted to please many gods. These included:\\r\\nThe Inca Empire employed central planning. The Inca Empire traded with outside regions, although they did not operate a substantial internal market economy. While axe-monies were used along the northern coast, presumably by the provincial mindale trading class,[48] most households in the empire lived in a traditional economy in which households were required to pay taxes, usually in the form of the mit'a corve labor, and military obligations,[49] though barter (or trueque) was present in some areas.[50] In return, the state provided security, food in times of hardship through the supply of emergency resources, agricultural projects (e.g. aqueducts and terraces) to increase productivity and occasional feasts. The economy rested on the material foundations of the vertical archipelago, a system of ecological complementarity in accessing resources[51] and the cultural foundation of ayni, or reciprocal exchange.[52][53]\\r\\nThe Sapa Inca was conceptualized as divine and was effectively head of the state religion. The Willaq Umu (or Chief Priest) was second to the emperor. Local religious traditions continued and in some cases such as the Oracle at Pachacamac on the Peruvian coast, were officially venerated. Following Pachacuti, the Sapa Inca claimed descent from Inti, who placed a high value on imperial blood; by the end of the empire, it was common to incestuously wed brother and sister. He was \\"son of the sun,\\" and his people the intip churin, or \\"children of the sun,\\" and both his right to rule and mission to conquer derived from his holy ancestor. The Sapa Inca also presided over ideologically important festivals, notably during the Inti Raymi, or \\"warriors' cultivation,\\" attended by soldiers, mummified rulers, nobles, clerics and the general population of Cusco beginning on the June solstice and culminating nine days later with the ritual breaking of the earth using a foot plow by the Inca. Moreover, Cusco was considered cosmologically central, loaded as it was with huacas and radiating ceque lines and geographic center of the Four Quarters; Inca Garcilaso de la Vega called it \\"the navel of the universe\\".[54][55][56][57]\\r\\nThe Inca Empire was a federalist system consisting of a central government with the Inca at its head and four quarters, or suyu: Chinchay Suyu (NW), Anti Suyu (NE), Kunti Suyu (SW) and Qulla Suyu (SE). The four corners of these quarters met at the center, Cusco. These suyu were likely created around 1460 during the reign of Pachacuti before the empire reached its largest territorial extent. At the time the suyu were established they were roughly of equal size and only later changed their proportions as the empire expanded north and south along the Andes.[60]\\r\\nCusco was likely not organized as a wamani, or province. Rather, it was probably somewhat akin to a modern federal district, like Washington, D.C. or Mexico City. The city sat at the center of the four suyu and served as the preeminent center of politics and religion. While Cusco was essentially governed by the Sapa Inca, his relatives and the royal panaqa lineages, each suyu was governed by an Apu, a term of esteem used for men of high status and for venerated mountains. Both Cusco as a district and the four suyu as administrative regions were grouped into upper hanan and lower hurin divisions. As the Inca did not have written records, it is impossible to exhaustively list the constituent wamani. However, colonial records allow us to reconstruct a partial list. There were likely more than 86 wamani, with more than 48 in the highlands and more than 38 on the coast.[61][62][63]\\r\\nThe most populous suyu was Chinchaysuyu, which encompassed the former Chimu empire and much of the northern Andes. At its largest extent, it extended through much of modern Ecuador and into modern Colombia.\\r\\nThe largest suyu by area was Qullasuyu, named after the Aymara-speaking Qulla people. It encompassed the Bolivian Altiplano and much of the southern Andes, reaching Argentina and as far south as the Maipo or Maule river in Central Chile.[58] Historian Jos Bengoa singled out Quillota as likely being the foremost Inca settlement in Chile.[64]\\r\\nThe second smallest suyu, Antisuyu, was northwest of Cusco in the high Andes. Its name is the root of the word \\"Andes.\\"[65]\\r\\nKuntisuyu was the smallest suyu, located along the southern coast of modern Peru, extending into the highlands towards Cusco.[66]\\r\\nThe Inca state had no separate judiciary or codified laws. Customs, expectations and traditional local power holders governed behavior. The state had legal force, such as through tokoyrikoq (lit. \\"he who sees all\\"), or inspectors. The highest such inspector, typically a blood relative to the Sapa Inca, acted independently of the conventional hierarchy, providing a point of view for the Sapa Inca free of bureaucratic influence.[67]\\r\\nThe Inca had three moral precepts that governed their behavior:\\r\\nColonial sources are not entirely clear or in agreement about Inca government structure, such as exact duties and functions of government positions. But the basic structure can be broadly described. The top was the Sapa Inca. Below that may have been the Willaq Umu, literally the \\"priest who recounts\\", the High Priest of the Sun.[68] However, beneath the Sapa Inca also sat the Inkap rantin, who was a confidant and assistant to the Sapa Inca, perhaps similar to a Prime Minister.[69] Starting with Topa Inca Yupanqui, a \\"Council of the Realm\\" was composed of 16 nobles: 2 from hanan Cusco; 2 from hurin Cusco; 4 from Chinchaysuyu; 2 from Cuntisuyu; 4 from Collasuyu; and 2 from Antisuyu. This weighting of representation balanced the hanan and hurin divisions of the empire, both within Cusco and within the Quarters (hanan suyukuna and hurin suyukuna).[70]\\r\\nWhile provincial bureaucracy and government varied greatly, the basic organization was decimal. Taxpayers ÿ male heads of household of a certain age range ÿ were organized into corve labor units (often doubling as military units) that formed the state's muscle as part of mit'a service. Each unit of more than 100 tax-payers were headed by a kuraka, while smaller units were headed by a kamayuq, a lower, non-hereditary status. However, while kuraka status was hereditary and typically served for life, the position of a kuraka in the hierarchy was subject to change based on the privileges of superiors in the hierarchy; a pachaka kuraka could be appointed to the position by a waranqa kuraka. Furthermore, one kuraka in each decimal level could serve as the head of one of the nine groups at a lower level, so that a pachaka kuraka might also be a waranqa kuraka, in effect directly responsible for one unit of 100 tax-payers and less directly responsible for nine other such units.[71][72][73]\\r\\nFrancisco Pizarro\\r\\nArchitecture was the most important of the Incan arts, with textiles reflecting architectural motifs. The most notable example is Machu Picchu, which was constructed by Inca engineers. The prime Inca structures were made of stone blocks that fit together so well that a knife could not be fitted through the stonework. These constructs have survived for centuries, with no use of mortar to sustain them.\\r\\nThis process was first used on a large scale by the Pucara (ca. 300 BCÿAD 300) peoples to the south in Lake Titicaca and later in the city of Tiwanaku (ca. AD 400ÿ1100) in present-day Bolivia. The rocks were sculpted to fit together exactly by repeatedly lowering a rock onto another and carving away any sections on the lower rock where the dust was compressed. The tight fit and the concavity on the lower rocks made them extraordinarily stable, despite the ongoing challenge of earthquakes and volcanic activity.\\r\\nPhysical measures used by the Inca were based on human body parts. Units included fingers, the distance from thumb to forefinger, palms, cubits and wingspans. The most basic distance unit was thatkiy or thatki, or one pace. The next largest unit was reported by Cobo to be the topo or tupu, measuring 6,000 thatkiys, or about 7.7?km (4.8?mi); careful study has shown that a range of 4.0 to 6.3?km (2.5 to 3.9?mi) is likely. Next was the wamani, composed of 30 topos (roughly 232?km or 144?mi). To measure area, 25 by 50 wingspans were used, reckoned in topos (roughly 3,280?km2 or 1,270?sq?mi). It seems likely that distance was often interpreted as one day's walk; the distance between tambo way-stations varies widely in terms of distance, but far less in terms of time to walk that distance.[76][77]\\r\\nInca calendars were strongly tied to astronomy. Inca astronomers understood equinoxes, solstices and zenith passages, along with the Venus cycle. They could not, however, predict eclipses. The Inca calendar was essentially lunisolar, as two calendars were maintained in parallel, one solar and one lunar. As 12 lunar months fall 11 days short of a full 365-day solar year, those in charge of the calendar had to adjust every winter solstice. Each lunar month was marked with festivals and rituals.[78] Apparently, the days of the week were not named and days were not grouped into weeks. Similarly, months were not grouped into seasons. Time during a day was not measured in hours or minutes, but in terms of how far the sun had travelled or in how long it had taken to perform a task.[79]\\r\\nThe sophistication of Inca administration, calendrics and engineering required facility with numbers. Numerical information was stored in the knots of quipu strings, allowing for compact storage of large numbers.[80][81] These numbers were stored in base-10 digits, the same base used by the Quechua language[82] and in administrative and military units.[72] These numbers, stored in quipu, could be calculated on yupanas, grids with squares of positionally varying mathematical values, perhaps functioning as an abacus.[83] Calculation was facilitated by moving piles of tokens, seeds or pebbles between compartments of the yupana. It is likely that Inca mathematics at least allowed division of integers into integers or fractions and multiplication of integers and fractions.[84]\\r\\nAccording to mid-17th-century Jesuit chronicler Bernab Cobo,[85] the Inca designated officials to perform accounting-related tasks. These officials were called quipo camayos. Study of khipu sample VA 42527 (Museum fr V?lkerkunde, Berlin)[86] revealed that the numbers arranged in calendrically significant patterns were used for agricultural purposes in the \\"farm account books\\" kept by the khipukamayuq (accountant or warehouse keeper) to facilitate the closing of accounting books.[87]\\r\\nCeramics were painted using the polychrome technique portraying numerous motifs including animals, birds, waves, felines (popular in the Chavin culture) and geometric patterns found in the Nazca style of ceramics. In a culture without a written language, ceramics portrayed the basic scenes of everyday life, including the smelting of metals, relationships and scenes of tribal warfare. The most distinctive Inca ceramic objects are the Cusco bottles or \\"aryballos\\".[88] Many of these pieces are on display in Lima in the Larco Archaeological Museum and the National Museum of Archaeology, Anthropology and History.\\r\\nAlmost all of the gold and silver work of the Incan empire was melted down by the conquistadors.\\r\\nThe Inca recorded information on assemblages of knotted strings, known as Quipu, although they can no longer be decoded. Originally it was thought that Quipu were used only as mnemonic devices or to record numerical data. Quipus are also believed to record history and literature.[89]\\r\\nThe Inca made many discoveries in medicine.[90] They performed successful skull surgery, by cutting holes in the skull to alleviate fluid buildup and inflammation caused by head wounds. Many skull surgeries performed by Inca surgeons were successful. Survival rates were 80ÿ90%, compared to about 30% before Inca times.[91]\\r\\nThe Incas revered the coca plant as sacred/magical. Its leaves were used in moderate amounts to lessen hunger and pain during work, but were mostly used for religious and health purposes.[92] The Spaniards took advantage of the effects of chewing coca leaves.[92] The Chasqui, messengers who ran throughout the empire to deliver messages, chewed coca leaves for extra energy. Coca leaves were also used as an anaesthetic during surgeries.\\r\\nThe Inca army was the most powerful at that time, because they could turn an ordinary villager or farmer into a soldier. Every able bodied male Inca of fighting age had to take part in war in some capacity at least once and to prepare for warfare again when needed. By the time the empire reached its largest size, every section of the empire contributed in setting up an army for war.\\r\\nThe Incas had no iron or steel and their weapons were not much more effective than those of their opponents. They went into battle with drums beating and trumpets blowing. Their armor included:[citation needed]\\r\\nThe Inca weaponry included:\\r\\nRoads allowed quick movement (on foot) for the Inca army and shelters called tambo and storage silos called qullqas were built one day's travelling distance from each other, so that an army on campaign could always be fed and rested. This can be seen in names of ruins such as Ollantay Tambo, or My Lord's Storehouse. These were set up so the Inca and his entourage would always have supplies (and possibly shelter) ready as they traveled.\\r\\nChronicles and references from the 16th and 17th centuries support the idea of a banner. However, it represented the Inca (emperor), not the empire.\\r\\nFrancisco L܇pez de Jerez[96] wrote in 1534:\\r\\n...?todos venan repartidos en sus escuadras con sus banderas y capitanes que los mandan, con tanto concierto como turcos.\\r\\n(...?all of them came distributed into squads, with their flags and captains commanding them, as well-ordered as Turks.)\\r\\nChronicler Bernab Cobo wrote:\\r\\nThe royal standard or banner was a small square flag, ten or twelve spans around, made of cotton or wool cloth, placed on the end of a long staff, stretched and stiff such that it did not wave in the air and on it each king painted his arms and emblems, for each one chose different ones, though the sign of the Incas was the rainbow and two parallel snakes along the width with the tassel as a crown, which each king used to add for a badge or blazon those preferred, like a lion, an eagle and other figures.\\r\\n(...?el gui܇n o estandarte real era una banderilla cuadrada y peque?a, de diez o doce palmos de ruedo, hecha de lienzo de algod܇n o de lana, iba puesta en el remate de una asta larga, tendida y tiesa, sin que ondease al aire, y en ella pintaba cada rey sus armas y divisas, porque cada uno las escoga diferentes, aunque las generales de los Incas eran el arco celeste y dos culebras tendidas a lo largo paralelas con la borda que le serva de corona, a las cuales sola a?adir por divisa y blas܇n cada rey las que le pareca, como un le܇n, un guila y otras figuras.)\\r\\n-Bernab Cobo, Historia del Nuevo Mundo (1653)\\r\\nGuaman Poma's 1615 book, El primer nueva cor܇nica y buen gobierno, shows numerous line drawings of Inca flags.[97] In his 1847 book A History of the Conquest of Peru, \\"William H. Prescott ... says that in the Inca army each company had its particular banner and that the imperial standard, high above all, displayed the glittering device of the rainbow, the armorial ensign of the Incas.\\"[98] A 1917 world flags book says the Inca \\"heir-apparent ... was entitled to display the royal standard of the rainbow in his military campaigns.\\"[99]\\r\\nIn modern times the rainbow flag has been wrongly associated with the Tawantinsuyu and displayed as a symbol of Inca heritage by some groups in Peru and Bolivia. The city of Cusco also flies the Rainbow Flag, but as an official flag of the city. The Peruvian president Alejandro Toledo (2001ÿ2006) flew the Rainbow Flag in Lima's presidential palace. However, according to Peruvian historiography, the Inca Empire never had a flag. Peruvian historian Mara Rostworowski said, \\"I bet my life, the Inca never had that flag, it never existed, no chronicler mentioned it\\".[100] Also, to the Peruvian newspaper El Comercio, the flag dates to the first decades of the 20th century,[101] and even the Congress of the Republic of Peru has determined that flag is a fake by citing the conclusion of National Academy of Peruvian History:\\r\\n\\"The official use of the wrongly called 'Tawantinsuyu flag' is a mistake. In the Pre-Hispanic Andean World there did not exist the concept of a flag, it did not belong to their historic context\\".[101]\\r\\nNational Academy of Peruvian History\\r\\nIncas were able to adapt to their high-altitude living through successful acclimatization, which is characterized by increasing oxygen supply to the blood tissues. For the native Inca living in the Andean highlands, this was achieved through the development of a larger lung capacity, and an increase in red blood cell counts, hemoglobin concentration, and capillary beds.[102]\\r\\nCompared to other humans, the Incas had slower heart rates, almost one-third larger lung capacity, about 2 L (4 pints) more blood volume and double the amount of hemoglobin, which transfers oxygen from the lungs to the rest of the body. While the Conquistadors may have been slightly taller, the Inca had the advantage of coping with the extraordinary altitude.","input":"What two important civilizations preceded the rise of the inca?"},{"output":"44 days","context":"Twilight is a 2008 American romantic fantasy film based on Stephenie Meyer's popular novel of the same name. Directed by Catherine Hardwicke, the film stars Kristen Stewart and Robert Pattinson. It is the first film in The Twilight Saga film series. This film focuses on the development of the relationship between Bella Swan (a teenage girl) and Edward Cullen (a vampire), and the subsequent efforts of Edward and his family to keep Bella safe from a coven of evil vampires.\\r\\nThe project was in development for approximately three years at Paramount Pictures, during which time a screen adaptation that differed significantly from the novel was written. Summit Entertainment acquired the rights to the novel after three years of the project's stagnant development. Melissa Rosenberg wrote a new adaptation of the novel shortly before the 2007ÿ2008 Writers Guild of America strike and sought to be faithful to the novel's storyline. Principal photography took 44 days[4] and completed on May 2, 2008;[5] the film was primarily shot in Oregon.[6]\\r\\nTwilight was theatrically released on November 21, 2008; it grossed over US$393 million worldwide.[3] It was released on DVD March 21, 2009 and became the most purchased DVD of the year.[7] The soundtrack was released on November 4, 2008.[8] Following the film's success, New Moon and Eclipse, the next two novels in the series, were produced as films the following year.\\r\\n\\r\\n\\r\\nBella Swan, a seventeen-year-old outcast, moves to Forks, a small town located by Washington's Olympic Peninsula, to live with her father, Charlie, who is the police chief of town. Her mother, Rene, is remarried to a minor league baseball player, and they travel often to attend games. At her new high school, Bella makes several new friends, including Jacob Black, but she is also intrigued by the mysterious and aloof Cullen siblings. Bella sits next to Edward Cullen in biology class on her first day of school, but he seems to be repulsed by her. After a week of absence from school, Edward returns to school, and begins socializing with Bella normally. A few days later, Bella is nearly struck by a van in the school parking lot. Edward saves her by instantaneously covering a distance of over thirty feet, and putting himself between Bella and the van, stopping it with only his hand, and making a conspicuous dent on the van. He subsequently refuses to explain his actions to Bella, and warns her against befriending him.\\r\\nAfter much research, Bella concludes that Edward is seemingly human, but has mysterious powers resembling to a vampire. He eventually confirms this, but says he and the other Cullens only consume animal blood. The pair fall in love, and Edward introduces Bella to his vampire family. Carlisle Cullen, the family patriarch, is a doctor working at the hospital in Forks. His wife is Esme. Alice, Jasper, Emmett, and Rosalie, are their informally adopted children. Edward and Bella's relationship is soon put in jeopardy, when three nomadic vampiresJames, Victoria, and Laurentarrive at Forks. James, a tracker vampire with incredible hunting instincts, is instantly intrigued by Edward's protectiveness over a human, which incites him to hunt Bella for sport. Edward and the other Cullens put their lives on the line in an effort to protect Bella, but James tracks her to Phoenix, where she is hiding with Jasper and Alice. James lures Bella into a trap by falsely claiming that he is holding her mother hostage. James attacks Bella by biting her wrist, infecting her with vampire venom. After a ferocious battle, Edward subdues James just as the other members of the Cullen family arrive. Alice, Emmett, and Jasper kill James, decapitating and burning him, as Edward removes the venom from Bella's wrist, preventing her from turning into a vampire. In the aftermath of the battle, Bella has suffered a broken leg, and ends up in the hospital, but her mother comes in to visit. Upon returning to Forks, Edward accompanies Bella to the high school prom, where he refuses to grant her request that he would transform her into a vampire. As the couple dance, they are unaware that James' mate, Victoria, is secretly watching, plotting revenge for her lover's death.\\r\\nStephenie Meyer's paranormal romance novel Twilight was originally optioned by Paramount Pictures' MTV Films in April 2004, but the screenplay that was subsequently developed was substantially different from its source material.[2][21] When Summit Entertainment reinvented itself as a full-service studio in April 2007, it began development of a film adaptation anew,[22] having picked up the rights from Paramount (who coincidentally had made an unrelated film with the same title in 1998) in a turnaround.[23] The company perceived the film as an opportunity to launch a franchise based on the success of Meyer's book and its sequels.[12][24] Catherine Hardwicke was hired to direct the film and Melissa Rosenberg was hired to write the script in mid-2007.[25]\\r\\nRosenberg developed an outline by the end of August, and collaborated with Hardwicke on writing the screenplay during the following month. Rosenberg said Hardwicke \\"was a great sounding board and had all sorts of brilliant ideas.... I'd finish off scenes and send them to her, and get back her notes.\\"[26] Due to the impending Writers Guild of America strike, Rosenberg worked full-time to finish the screenplay before October 31.[26] In adapting the novel, she \\"had to condense a great deal.\\" Some characters from the novel were not featured in the screenplay, whereas some characters were combined into others.[27] \\"[O]ur intent all along was to stay true to the book\\", Rosenberg explained, \\"and it has to do less with adapting it word for word and more with making sure the characters' arcs and emotional journeys are the same.\\"[28] Hardwicke suggested the use of voice over to convey Bella's internal dialogue[26]  since the novel is told from her point of view  and she sketched some of the storyboards during pre-production.[29]\\r\\nThe filmmakers behind Twilight worked to create a film that was as faithful to the novel as they thought possible when converting the story to another medium, with producer Greg Mooradian saying, \\"It's very important to distinguish that we're making a separate piece of art that obviously is going to remain very, very faithful to the book.... But at the same time, we have a separate responsibility to make the best movie you can make.\\"[30] To ensure a faithful adaptation, Meyer was kept very involved in the production process, having been invited to visit the set during filming and even asked to give notes on the script and on a rough cut of the film.[31] Of this process, she said, \\"It was a really pleasant exchange [between me and the filmmakers] from the beginning, which I think is not very typical. They were really interested in my ideas\\",[32] and, \\"...they kept me in the loop and with the script, they let me see it and said, 'What are your thoughts?'... They let me have input on it and I think they took 90 percent of what I said and just incorporated it right in to the script.\\"[31] Meyer fought for one line in particular, one of the most well-known from the book about \\"the lion and the lamb\\", to be kept verbatim in the film: \\"I actually think the way Melissa [Rosenberg] wrote it sounded better for the movie [...] but the problem is that line is actually tattooed on peoples' bodies [...] But I said, 'You know, if you take that one and change it, that's a potential backlash situation.'\\"[31] Meyer was even invited to create a written list of things that could not be changed for the film, such as giving the vampires fangs or killing characters who do not die in the book, that the studio agreed to follow.[31][32] The consensus among critics is that the filmmakers succeeded in making a film that is very faithful to its source material,[33][34] with one reviewer stating that, with a few exceptions, \\"Twilight the movie is unerringly faithful to the source without being hamstrung by it.\\"[35]\\r\\nHowever, as is most often the case with film adaptations, differences do exist between the film and source material. Certain scenes from the book were cut from the film, such as a biology room scene where Bella's class does blood typing. Hardwicke explains, \\"Well [the book is] almost 500 pagesyou do have to do the sweetened condensed milk version of that.... We already have two scenes in biology: the first time they're in there and then the second time when they connect. For a film, when you condense, you don't want to keep going back to the same setting over and over. So that's not in there.\\"[36] The settings of certain conversations in the book were also changed to make the scenes more \\"visually dynamic\\" on-screen, such as Bella's revelation that she knows Edward is a vampirethis happens in a meadow in the film instead of in Edward's car as in the novel.[36] A biology field trip scene is added to the film to condense the moments of Bella's frustration at trying to explain how Edward saved her from being crushed by a van.[30] The villainous vampires are introduced earlier in the film than in the novel. Rosenberg said that \\"you don't really see James and the other villains until to the last quarter of the book, which really won't work for a movie. You need that ominous tension right off the bat. We needed to see them and that impending danger from the start. And so I had to create back story for them, what they were up to, to flesh them out a bit as characters.\\"[26] Rosenberg also combined some of the human high school students, with Lauren Mallory and Jessica Stanley in the novel becoming the character of Jessica in the film, and a \\"compilation of a couple of different human characters\\" becoming Eric Yorkie.[27] About these variances from the book, Mooradian stated, \\"I think we did a really judicious job of distilling [the book]. Our greatest critic, Stephenie Meyer, loves the screenplay, and that tells me that we made all the right choices in terms of what to keep and what to lose. Invariably, you're going to lose bits and pieces that certain members of the audience are going to desperately want to see, but there's just a reality that we're not making 'Twilight: The Book' the movie.\\"[30]\\r\\nKristen Stewart was on the set of Adventureland when Hardwicke visited her for an informal screen test that \\"captivated\\" the director.[2] Hardwicke had trouble finding an actor otherworldly enough to play vampire Edward Cullen. Then she got a call about a guy in London. \\"I looked at a couple pictures and was like, Im not sure,?\\" Hardwicke says. \\"He had been fired from his last job, he was unemployed, he was in debt.\\" Pattinson flew to Los Angeles on his own dime to read with Stewart.[37] Shiloh Fernandez, Jackson Rathbone, Ben Barnes, and Robert Pattinson were the final four up for the role of Edward.[38] Hardwicke did not initially choose Robert Pattinson for the role of Edward Cullen, but after an audition at her home with Stewart, he was selected.[2] Hardwicke said, \\"Kristen was like, Its got to be Rob! She felt connected to him from the first moment. That electricity, or love at first sight, or whatever it is.\\" Hardwicke gave him the part, but he had to make a promise. \\"Youve got to realize that Kristen is 17 years old,\\" Hardwicke told him, \\"Shes underage. Youve got to focus, dude, or youre going to be arrested. I made him swear on a stack of Bibles.\\"[37] Pattinson was unfamiliar with the novel series prior to his screen test but read the books later on.[39] Meyer allowed him to view a manuscript of the unfinished Midnight Sun, which chronicles the events in Twilight from Edward's point of view.[40] Fan reaction to Pattinson's casting as Edward was initially negative; Rachelle Lefvre remarked that \\"[e]very woman had their own Edward [that] they had to let go of before they could open up to [him], which they did.\\"[39] Meyer was \\"excited\\" and \\"ecstatic\\" in response to the casting of the two main characters.[41] She had expressed interest in having Emily Browning and Henry Cavill cast as Bella and Edward, respectively, prior to pre-production.[42]\\r\\nPeter Facinelli was not originally cast as Carlisle Cullen. \\"[Hardwicke] liked me, but there was another actor that the studio was pushing for\\", Facinelli said.[11] For unknown reasons, that actor was not able to play the part and Facinelli was selected in his place.[11] The choice of Ashley Greene to portray Alice Cullen was the subject of fan criticism due to Greene being 7 inches (18?cm) taller than her character as described in the novel. Meyer had also stated that Rachael Leigh Cook resembled her vision of Alice.[43] Nikki Reed had previously worked with Hardwicke on Thirteen, which they wrote together, and Lords of Dogtown. Reed commented, \\"I don't want to say it's a coincidence, because we do work well together, and we have a great history. I think we make good work, but it's more that the people that hire [Hardwicke] to direct a film of theirs [have] most likely seen her other work.\\"[44]\\r\\nKellan Lutz was in Africa shooting the HBO miniseries Generation Kill when the auditions for the character of Emmett Cullen were conducted. The role had already been cast by the time that production ended in December 2007, but the actor who had been selected \\"fell through\\"; Lutz subsequently auditioned and was flown to Oregon, where Hardwicke personally chose him.[45] Rachelle Lefvre was interested in pursuing a role in the film because Hardwicke was attached to the project as director; there was also \\"the potential to explore a character, hopefully, over three films\\"; and she wanted to portray a vampire.[46] She \\"thought that vampires were basically the best metaphor for human anxiety and questions about being alive.\\"[46] Christian Serratos initially auditioned for Jessica Stanley, but she \\"fell totally in love with Angela\\" after reading the novels and successfully took advantage of a later opportunity to audition for Angela Weber.[47] The role of Jessica Stanley went to Anna Kendrick, who got the part after two mix-and-match auditions with various actors.[48]\\r\\nOn a bed in Catherine Hardwicke's house is where Pattinson kissed Stewart for the first time for the Twilight screen test. \\"That bed made Pattinson who he is right now,\\" says Reed. Thats also where Hardwicke auditioned Evan Rachel Wood, when she had her get into her bed with Nikki Reed for the film Thirteen. When asked about her lair, Hardwicke says, \\"MTV came and did an episode in my house filming the bed. Its legendary.\\"[37] Principal photography took 44 days,[4] after more than a week of rehearsals,[49] and completed on May 2, 2008.[5] Similar to her directorial debut Thirteen, Hardwicke opted for an extensive use of hand-held cinematography to make the film \\"feel real\\".[11][50] Meyer visited the production set three times and was consulted on different aspects of the story;[51] she also has a brief cameo in the film.[52] Cast members who portrayed vampires avoided sunlight to make their skin pale, though makeup was also applied for that effect, and wore contact lenses: \\"We did the golden color because the Cullens have those golden eyes. And then, when we're hungry, we have to pop the black ones in,\\" Facinelli explained.[11] They also participated in rehearsals with a dance choreographer and observed the physicality of different panthera to make their bodily movements more elegant.[11][43][53]\\r\\nScenes were filmed primarily in Portland, Oregon.[6] Stunt work was done mainly by the cast.[54] The fight sequence between Gigandet and Pattinson's characters in a ballet studio, which was filmed during the first week of production, involved a substantial amount of wire work because the vampires in the story have superhuman strength and speed.[53] Gigandet incorporated mixed martial arts fighting moves in this sequence, which involved chicken and honey as substitutes for flesh.[55] Bella, the protagonist, is unconscious during these events, and since the novel is told from her point of view, such action sequences are illustrative and unique to the film.[39] Pattinson noted that maintaining one's center of gravity is difficult when doing wire work \\"because you have to really fight against it as well as letting it do what it needs to do.\\"[39] Lefvre found the experience disorienting since forward motion was out of her control.[39]\\r\\nInstead of shooting at Forks High School itself, scenes taking place at the school were filmed at Kalama High School[56] and Madison High School.[57] Other scenes were filmed in St. Helens,[58] and Hardwicke conducted some reshooting in Pasadena, California, in August.[4][59] Twilight was originally scheduled to be theatrically released in the United States on December 12, 2008, but its release date was changed to November 21 after Harry Potter and the Half-Blood Prince was rescheduled for an opening in July 2009.[60] Two teaser trailers, as well as some additional scenes, were released for the film, as well as a final trailer, which was released on October 9.[61][62] A 15-minute excerpt of Twilight was presented during the International Rome Film Festival in Italy.[63] The film received a rating of PG-13 from the Motion Picture Association of America for \\"some violence and a scene of sensuality\\".[64]\\r\\nThe score for Twilight was composed by Carter Burwell,[65][66] with the rest of the soundtrack chosen by music supervisor Alexandra Patsavas.[67] Meyer was consulted on the soundtrack, which includes music by Muse and Linkin Park, bands she listened to while writing the novels.[68][69] The original soundtrack was released on November 4, 2008, by Chop Shop Records in conjunction with Atlantic Records.[8] It debuted at number 1 on the Billboard 200.[70]\\r\\nTwilight grossed over $7 million in ticket sales from midnight showings alone on November 21, 2008.[71] The film is fifth overall on Fandango's list of top advance ticket sales, outranked only by its sequel the following year, Star Wars: Episode III ÿ Revenge of the Sith (2005), The Dark Knight (2008), and Harry Potter and the Half-Blood Prince (2009).[71] It grossed $35.7 million on its opening day.[72] For its opening weekend in the United States and Canada, Twilight accumulated $69.6 million from 3,419 theaters at an average of $20,368 per theater.[73] The film grossed $192,769,854 in the United States and Canada, and $199,846,771 in international territories for a total of $392,616,625.[3] Its opening weekend gross was the highest ever of a female-directed film, surpassing that of Deep Impact (1998).[74]\\r\\nTwilight received mixed reviews from critics. Based on 209 reviews collected by Rotten Tomatoes, the film has a rating of 48%, with a weighted average score of 5.4/10.[75] The site's critical consensus reads: \\"Having lost much of its bite transitioning to the big screen, Twilight will please its devoted fans, but do little for the uninitiated.\\"[75] On Metacritic, which assigns a weighted mean rating out of 100 reviews from film critics, it has an average score of 56 from the 37 reviews.[76] New York Press critic Armond White called the film \\"a genuine pop classic\\",[77] and praised Hardwicke for turning \\"Meyer's book series into a Bront?-esque vision.\\"[78] Roger Ebert gave the film two-and-a-half stars out of four and wrote, \\"I saw it at a sneak preview. Last time I saw a movie in that same theater, the audience welcomed it as an opportunity to catch up on gossip, texting, and laughing at private jokes. This time the audience was rapt with attention\\".[79] In his review for the Los Angeles Times, Kenneth Turan wrote, \\"Twilight is unabashedly a romance. All the story's inherent silliness aside, it is intent on conveying the magic of meeting that one special person you've been waiting for. Maybe it is possible to be 13 and female for a few hours after all\\".[80] USA Today gave the film two out of four stars and Claudia Puig wrote, \\"Meyer is said to have been involved in the production of Twilight, but her novel was substantially more absorbing than the unintentionally funny and quickly forgettable film\\".[81] Entertainment Weekly gave the film a \\"B\\" rating and Owen Gleiberman praised Hardwicke's direction: \\"She has reconjured Meyer's novel as a cloudburst mood piece filled with stormy skies, rippling hormones, and understated visual effects\\".[82]\\r\\nThe film was released on DVD in North America on March 21, 2009, through midnight release parties, and sold over 3 million units in its first day.[83] It was released on April 6, 2009 in the UK.[84][85] Bonus features include about 10 to 12 extended or deleted scenes, montages and music videos, behind-the-scenes interviews, a \\"making-of\\" segment, and commentary featuring Hardwicke, Stewart, and Pattinson.[86][87] The Blu-ray disc edition of the film was released on March 21, 2009, in select locations, but was made more widely available at further retailers on May 5, 2009.[88] As of July 2012, the film has sold 11,242,519 units, earning $201,190,019.[89]\\r\\nThe film and the next two installments of the Twilight Saga will be rereleased as a triple feature with extended cuts on January 13, 2015.\\r\\nA movie trivia video game developed by Screenlife and published by Konami for the Wii, Nintendo DS, PC and iPhone was released alongside the second film.\\r\\nSince its release, Twilight has received numerous nominations and awards. In January 2009, Carter Burwell was nominated for Film Composer of the Year by the International Film Music Critics Association.[90] Robert Pattinson won Bravo TV's A-List Award for A-List Breakout.[91] At the 2009 MTV Movie Awards, Pattinson, who was nominated alongside Taylor Lautner, also won an award for Male Breakthrough Performance, \\"Decode\\" was nominated for Best Song from a Movie, Twilight won an award for Best Movie, Kristen Stewart won for Best Female performance, Stewart and Pattinson were awarded Best Kiss, and Pattinson and Cam Gigandet won an award for Best Fight.[92] Christian Serratos won a Young Artist Award for Best Performance in a Feature Film: Supporting Young Actress.[93] For the 2009 Teen Choice Awards, held on August 9, the film and its actors received a combined total of 12 nominations, nine of which the film won.[94] At the 2009 Scream Awards, the film was nominated for nine awards, four of which it won.[95] The film won two ALMA Awards for makeup and hairstyling.[96] It also won the Public Choice Award at the World Soundtrack Awards, where Carter Burwell was also nominated for Composer of the Year.[97] Catherine Hardwicke received a Young Hollywood Award for her directing.[98] In addition, the film was nominated for Best Fantasy Film at the 35th Saturn Awards[99] and two Grammy Awards.[100]\\r\\nMTV reported in February 2008 that Summit Entertainment intended to create a series of at least three films based on Meyer's books.[9] The studio had optioned New Moon, the second book in the series, by October 2008,[101] and confirmed their plans to make a film based on it November 22, 2008.[102][103] Because Catherine Hardwicke had wanted more preparation time than Summit's schedule for the production and release of the sequel would provide,[104][105] Chris Weitz was selected to direct it in December 2008.[106][107]","input":"How long did it take to make the movie twilight?"},{"output":"August 5, 1940","context":"","input":"When did latvia became part of the soviet union?"},{"output":"to protect the innocent from any form of religious persecution","context":"\\r\\n\\r\\nKhalsa (Punjabi: ?????? \\"the pure\\") refers to both a special group of initiated Sikh warriors, as well as a community that considers Sikhism as its faith.[1][2] The Khalsa tradition was initiated in 1699 by the last living Guru of Sikhism, Guru Gobind Singh. Its formation was a key event in the history of Sikhism.[2] The founding of Khalsa is celebrated by Sikhs during the festival of Vaisakhi.[3][4][5]\\r\\n\\r\\nGuru Gobind Singh started the Khalsa tradition after his father had been beheaded for resisting the religious persecution of non-Muslims (mainly Kashmiri Hindus) during the rule of the Mughal Emperor Aurangzeb.[6][7][8] Guru Gobind Singh created and initiated the Khalsa as a warrior with a duty to protect the innocent from any form of religious persecution.[9] The Khalsa redefined the Sikh tradition from the start. It formulated an initiation ceremony (amrit pahul, nectar ceremony) and rules of conduct for the Khalsa warriors. It created a new institution for the temporal leadership of the Sikhs, replacing the masands system maintained by the earlier Gurus of Sikhism. Additionally, the Khalsa provided a political and religious vision for the Sikh community.[1][10][11]:127\\r\\n\\r\\nUpon initiation, a Khalsa Sikh was given the titles of Singh (male) and Kaur (female). The rules of life, included behavioral code (Rahit, such as no tobacco, no alcohol, no meat), and a dress code (Five Ks).[11]:121ÿ126 In contrast to the Khalsa Sikh, a Sahajdhari Sikh is one who reveres the teachings of Sikh gurus, but has not undergone the initiation. Sahajdhari Sikhs do not accept some or all elements of the dress and behavioral codes of the Khalsa Sikhs.[12] The Khalsa has been predominantly a male institution in Sikh history, with Khalsa authority with the male leaders. In the contemporary era, it has become open to women but its authority remains with Sikh men.[1][13]\\r\\n\\r\\n\\"Khalsa\\", according to McLeod, is derived from the Arabic or Persian word \\"Khalisa\\" which means \\"pure\\".[14][15]\\r\\n\\r\\nSikhism emerged in the northwestern part of Indian subcontinent (now parts of Pakistan and India). During the Mughal Empire rule, according to Eleanor Nesbitt, khalsa originally meant the land that was possessed directly by the emperor, which was different from jagir land granted to lords in exchange for a promise of loyalty and annual tribute to the emperor.[16] Prior to Guru Gobind Singh, the religious organization was organized through the masands or agents. The masands would collect revenue from rural regions for the Sikh cause, much like jagirs would for the Islamic emperor.[16][17] The khalsa, in Sikhism, came to mean pure loyalty to the Guru, and not to the intermediary masands who were increasingly becoming corrupt, states Nesbitt.[16][18]\\r\\n\\r\\nThe Sikhs faced religious persecution during the Mughal Empire rule. Guru Arjan Dev, the fifth Guru, was arrested and executed by Emperor Jahangir in 1606.[19] The following Guru, Guru Hargobind formally militarised the Sikhs and emphasised the complementary nature of the temporal power and spiritual power.[20] In 1675, Guru Tegh Bahadur, the ninth Guru of the Sikhs and the father of Guru Gobind Singh was executed by the Mughal emperor Aurangzeb for resisting religious persecution of non-Muslims, and for refusing to convert to Islam.[6][7][8]\\r\\n\\r\\nIn 1699, the tenth Guru of Sikhism, Guru Gobind Singh asked Sikhs to gather at Anandpur Sahib on 30 March 1699, the day of Vaisakhi (the annual harvest festival). Guru Gobind Singh addressed the congregation from the entryway of a tent pitched on a hill (now called Kesgarh Sahib). He drew his sword, according to the Sikh tradition, and then asked for a volunteer from those who gathered, someone willing to sacrifice his head. One came forward, whom he took inside a tent. The Guru returned to the crowd without the volunteer, but with a bloody sword.[21] He asked for another volunteer, and repeated the same process of returning from the tent without anyone and with a bloodied sword four more times. After the fifth volunteer went with him into the tent, the Guru returned with all five volunteers, all safe. He called them the Panj Pyare and the first Khalsa in the Sikh tradition.[21] These five volunteers were?: Daya Ram (Bhai Daya Singh), Dharam Das (Bhai Dharam Singh), Himmat Rai (Bhai Himmat Singh), Mohkam Chand (Bhai Mohkam Singh), and Sahib Chand (Bhai Sahib Singh).\\r\\n\\r\\nGuru Gobind Singh then mixed water and sugar into an iron bowl, stirring it with a double-edged sword to prepare what he called Amrit (\\"nectar\\"). He then administered this to the Panj Pyare, accompanied with recitations from the Adi Granth, thus founding the khande ka pahul (baptization ceremony) of a Khalsa ÿ a warrior community.[21][22] The Guru also gave them a new surname \\"Singh\\" (lion). After the first five Khalsa had been baptized, the Guru asked the five to baptize him as a Khalsa. This made the Guru the sixth Khalsa, and his name changed from Guru Gobind Rai to Guru Gobind Singh.[21]\\r\\n\\r\\nHe introduced ideas that indirectly challenged the discriminatory taxes imposed by Islamic authorities. For example, Aurangzeb had imposed taxes on non-Muslims that were collected from the Sikhs as well, for example the jizya (poll tax on non-Muslims), pilgrim tax and Bhaddar tax ÿ the last being a tax to be paid by anyone following the Hindu ritual of shaving the head after the death of a loved one and cremation.[23] Guru Gobind Singh declared that Khalsa do not need to continue this practice, because Bhaddar is not dharam, but a bharam (illusion).[23][24] Not shaving the head also meant not having to pay the taxes by Sikhs who lived in Delhi and other parts of the Mughal Empire.[23] However, the new code of conduct also led to internal disagreements between Sikhs in the 18th century, particularly between the Nanakpanthi and the Khalsa.[23]\\r\\n\\r\\nGuru Gobind Singh had deep respect for the Khalsa, and stated that there is no difference between the True Guru and the sangat (panth).[25] Before his founding of the Khalsa, the Sikh movement had used the Sanskrit word Sisya (literally, disciple or student), but the favored term thereafter became Khalsa.[26] Additionally, prior to the Khalsa, the Sikh congregations across India had a system of Masands appointed by the Sikh Gurus. The Masands led the local Sikh communities, local temples, collected wealth and donations for the Sikh cause.[26] Guru Gobind Singh concluded that the Masands system had become corrupt, he abolished them and introduced a more centralized system with the help of Khalsa that was under his direct supervision.[26] These developments created two groups of Sikhs, those who initiated as Khalsa, and others who remained Sikhs but did not undertake the initiation.[26] The Khalsa Sikhs saw themselves as a separate religious entity, while the Nanak-panthi Sikhs retained their different perspective.[27][28]\\r\\n\\r\\nThe Khalsa warrior community tradition started by Guru Gobind Singh has contributed to modern scholarly debate on pluralism within Sikhism. His tradition has survived into the modern times, with initiated Sikh referred to as Khalsa Sikh, while those who do not get baptized referred to as Sahajdhari Sikhs.[29][30][31]\\r\\n\\r\\nAn inscription naming the five members of the Khalsa Panth, at Takht Keshgarh Sahib, the birthplace of Khalsa on Baisakh 1, 1756 Vikram Samvat.\\r\\n\\r\\nThe creation of the Khalsa; initiated by Guru Gobind Singh, the tenth Sikh Guru.\\r\\n\\r\\nGuru Gobind Singh initiated the Five K's tradition of the Khalsa,[32][33]\\r\\n\\r\\nHe also announced a code of discipline for Khalsa warriors. Tobacco, eating meat slaughtered according to Muslim ritual and sex with Muslims were forbidden.[32][34] The Khalsas also agreed to never interact with those who followed rivals or their successors.[32] The co-initiation of men and women from different castes into the ranks of Khalsa also institutionalized the principle of equality in Sikhism regardless of one's caste or gender.[34] According to Owen and Sambhi, Guru Gobind Singh's significance to the Sikh tradition has been very important, as he institutionalized the Khalsa, resisted the ongoing persecution by the Mughal Empire, and continued \\"the defense of Sikhism and Hinduism against the Muslim assault of Aurangzeb\\".[9]\\r\\n\\r\\nThe four prohibitions[35] or mandatory restrictions of the Khalsa or life of khalsa at time of Guru Gobind Singh Ji are:\\r\\n\\r\\nA Khalsa who breaks any code of conduct is no longer a Khalsa, is excommunicated from the Khalsa Panth and must go 'pesh' (get baptised again). Guru Gobind Singh also gave the Khalsa 52 hukams or 52 specific additional guidelines while living in Nanded in 1708.[36]\\r\\n\\r\\nA Khalsa is enjoined to be honest, treat everyone as equal, meditate on God, maintain his fidelity, resist tyranny and religious persecution of oneself and others.[citation needed]\\r\\n\\r\\nOne of the duties of the Khalsa is to practice arms. This has been deemed necessary due to the rising persecution from the rulers. Before joining the Khalsa, most of the people were from professions like farming, pottery, masonry, carpenters, Labanas, etc.\\r\\n\\r\\nGuru Gobind Singh in Oct, 1708 deputed his disciple Banda Singh Bahadur to lead the Khalsa in an uprising against the Mughals. Banda Singh Bahadur first established a Sikh kingdom and then brought in the Land reforms in the form of breaking up large estates and distributing the land to peasants. He and his comrades were eventually defeated and executed, but he became an icon among the Sikhs. After a long exile the Khalsa regrouped under Nawab Kapur Singh, who gathered local Khalsa leaders and created Dal Khalsa, a coalition army. The Dal Khalsa fought against the Mughals and the Afghans, eventually resulting in the establishment of a number of small republics called misls (autonomous confederacies) and later in the formation of the Sikh Empire.\\r\\n\\r\\nAfter the fall of the Mughal empire and the later establishment of the Sikh Empire in the Punjab, the Khalsa was converted into a strong, multireligious and multinational fighting force, modernised according to European principles: the Sikh Khalsa Army which had a huge role in the expansion of the empire. Led by generals like: Maharaja Ranjit Singh himself, Misr Diwan Chand and Hari Singh Nalwa. It successfully defeated all its adversaries, including the Afghan tribals and army, Hill Chiefs, Misldars, Chinese, Tibetan and Gurkhas. By the time of death of Maharaja Ranjit Singh in 1839, the whole army of Sikh Empire was assessed at 120,000 men, with 250 artillery pieces. The irregular levies were included.[37]\\r\\n\\r\\nThe official name of the state (Sikh Empire) of Sikhs was \\"Sarkar-i-Khalsa\\": Government of the Khalsa. The boundaries of this state stretched from Tibet to Afghanistan and from Kashmir to Sutlej in the south and included regions of Punjab, Khyber Pakhtunkhwa, Kashmir, Ladakh, etc. The \\"Sarkar-i-Khalsa\\" was dissolved during two wars fought against the British between 1846 and 1849.[citation needed]\\r\\n\\r\\nInitiation into the Khalsa is referred to as Amrit Sanchar (water of immortality life-cycle rite) or Khande di Pahul (Initiation with the double edged sword).[38] Anyone from any previous religion, age, or knowledge group can take Amrit (Amrit Chhakh) when they are convinced that they are ready.[39] This baptism is done by the Panj Pyare in front of the Guru Granth Sahib. The devotee must arrive to the place of baptism, usually a Gurdwara, in the morning after bathing completely including having washed their hair and must be wearing the 5 articles of the Khalsa uniform.[40] After baptism, the new Singh or Kaur must abide by the four restrictions or must get re-baptised if they break any of them. Jasjpit Singh in Lucinda Mosher book describes taking Amrit as a huge commitment, \\"You are making a commitment to God, to God's creation, to yourself ÿ and you're giving up yourself. It is like giving up your own ego and accepting God into your life ÿ and accepting yourself as one with the entire creation.\\"[41]\\r\\n\\r\\nWith the creation of Khalsa, Guru Gobind Singh had abolished all existing social divisions as was fundamental in the teachings of Sri Guru Nanak Dev.[42] In their new order, the former lowest of the low would stand with the former highest; all would become one and drink from the same vessel.[43] All previous beliefs relating to family, occupation, customs and ceremonies were declared useless by the Guru. This caused discomfort to the conservative followers of the Guru and they protested. Many departed from the ceremony, but the Guru declared that the low castes should be raised and would dwell next to him.[43]\\r\\n\\r\\nThe newswriter of the Mughal government, Ghulam Mohyiuddin, reporting to the emperor wrote:[44][45]\\r\\n\\r\\nSri Gur Sobha (18th century) by Sainapati (Saina Singh) contains two sections (adhyays) on the controversies that arose, when Guru Gobind Singh's disciples in Delhi heard the news of his new order.[46] Much of the controversy stated in Sri Gur Sobha revolves around bhaddar, the ritual shaving of head after death of a close relative, which was discouraged by Guru Gobind Singh. According to Sainapti, while creating the Khalsa, Guru Gobind Singh said that bhaddar is bharam (illusion), and not dharam.[46]\\r\\n\\r\\nTensions developed between the Punjabi Khatri disciples of the Guru in Delhi, and members of the newly formed Khalsa. A prominent Khatri disciple was expelled from the place of worship (dharmasala) for refusing to join the Khalsa. Another disciple was expelled for eating with him, starting a chain of further expulsions.[46] The expelled disciples convened a community gathering, at which two wealthy Khatris demanded that the Khalsa produce a written order from the Guru that a new mandatory code of conduct had been promulgated. A Khatri family that refused to follow the bhaddar ritual was boycotted by the Khatri community.[46] The Khatri council (panch) closed the bazaar to pressure the Khalsa. The Khalsa petitioned the state officials to intervene, who forced reopening of the shops. Later, peace was established between the two groups in a sangat (congregation). However, hostility between some Khatris and the Khalsa persisted in the later years.[46]\\r\\n\\r\\nToday, the Khalsa is respected by the entire gamut of Sikhs; however, not all Sikhs are Amritdharis[21] The issue of Khalsa code of conduct has led to several controversies. In the early 1950s, a serious split occurred in the Canadian Sikh community, when the Khalsa Diwan Society in Vancouver, British Columbia elected a clean-shaven Sikh to serve on its management committee.[47] Although most of the early Sikh immigrants to Canada were non-Khalsa, and a majority of the members of the society were clean-shaven non-Khalsa Sikhs, a faction objected to the election of a non-Khalsa to the management committee. The factions in Vancouver and Victoria, British Columbia broke away from the Khalsa Diwan Society, and established their own gurdwara society called Akali Singh.[47]\\r\\n\\r\\nIn the United Kingdom there have been tensions between the Khalsa Sikhs and the non-Khalsa Sikhs. Many Sikhs in Britain have insisted on their right of not conforming to the Khalsa norms, while maintaining that they are truly Sikh. On the other hand, some of the Khalsa Sikhs think of the non-Khalsa Sikhs as having abandoned the Sikh faith altogether.[48]\\r\\n\\r\\nEach year the Khalsa display their military skills around the world at a festival called Hola Mohalla. During Hola Mohalla military exercises are performed alongside mock battles followed by kirtan and valour poetry competitions. The Khalsa also lead the Sikhs in the annual Vaisakhi parade.[49]","input":"What was the original purpose of the khalsa?"},{"output":"north-east Atlantic Ocean and Baltic Sea","context":"Carcinus maenas is a common littoral crab. It is known by different names around the world. In the British Isles, it is generally referred to as the shore crab, or green shore crab. In North America and South Africa, it bears the name green crab or European green crab. In Australia and New Zealand, it is referred to as either the European green crab or European shore crab.\\r\\nC.?maenas is a widespread invasive species, listed among the 100 \\"world's worst alien invasive species\\".[2] It is native to the north-east Atlantic Ocean and Baltic Sea, but has colonised similar habitats in Australia, South Africa, South America and both Atlantic and Pacific coasts of North America. It grows to a carapace width of 90 millimetres (3.5?in), and feeds on a variety of molluscs, worms and small crustaceans, potentially impacting a number of fisheries. Its successful dispersion has occurred via a variety of mechanisms, such as on ships' hulls, sea planes, packing materials, and bivalves moved for aquaculture.\\r\\n\\r\\n\\r\\nC.?maenas has a carapace up to 60 millimetres (2.4?in) long and 90?mm (3.5?in) wide,[3] but can be larger outside its native range, reaching 101?mm (4.0?in) wide in British Columbia.[4] The carapace has five short teeth along the rim behind each eye, and three undulations between the eyes. The undulations, which protrude beyond the eyes, are the simplest means of distinguishing C.?maenas from the closely related C.?aestuarii, which can also be an invasive species. In C.?aestuarii, the carapace lacks any bumps and extends forward beyond the eyes. Another characteristic for distinguishing the two species is the form of the first and second pleopods (collectively the gonopods), which are straight and parallel in C.?aestuarii, but curve outwards in C.?maenas.[3]\\r\\nThe colour of C.?maenas varies greatly, from green to brown, grey or red. This variation has a genetic component, but is largely due to local environmental factors.[5] In particular, individuals which delay moulting become redÿcoloured rather than green. Red individuals are stronger and more aggressive, but are less tolerant of environmental stresses, such as low salinity or hypoxia.[6] Juvenile crabs on average display greater patterning than adults.[7]\\r\\nC.?maenas is native to European and North African coasts as far as the Baltic Sea in the east, and Iceland and Central Norway in the north, and is one of the most common crabs throughout much of its range. In the Mediterranean Sea, it is replaced by the closely related species Carcinus aestuarii.\\r\\nC.?maenas was first observed on the east coast of North America in Massachusetts in 1817, and may now be found from South Carolina northwards;[8] by 2007, this species had extended its range northwards to Placentia Bay, Newfoundland.[9] In 1989, the species was found in San Francisco Bay, California, on the Pacific coast of the United States. Until 1993, it was not able to extend its range, but reached Oregon in 1997, the state of Washington in 1998 and British Columbia in 1999,[10][11] thus extending its range by 750 kilometres (470?mi) in ten years.[12] By 2003, C.?maenas had extended to South America with specimens discovered in Patagonia.[13]\\r\\nIn Australia, C.?maenas was first reported \\"in the late 1800s\\"[14] in Port Phillip Bay, Victoria, although the species was probably introduced as early as the 1850s.[15] It has since spread along the south-eastern and south-western seaboards, reaching New South Wales in 1971, South Australia in 1976 and Tasmania in 1993. One specimen was found in Western Australia in 1965, but there have been no further discoveries in the area since.[14]\\r\\nC.?maenas first reached South Africa in 1983, in the Table Docks area near Cape Town.[16] Since then, it has spread at least as far as Saldanha Bay in the north and Camps Bay in the south, over 100 kilometres (62?mi) apart.\\r\\nThere have been appearances of C.?maenas recorded in Brazil, Panama, Hawaii, Madagascar, the Red Sea, Pakistan, Sri Lanka and Myanmar; however, these have not resulted in invasions, but remain isolated findings. Japan has been invaded by a related crab, either C.?aestuarii or a hybrid of C.?aestuarii and C.?maenas.[17]\\r\\nIt is believed, based on the ecological conditions, that C.?maenas could eventually extend its range to colonise the Pacific coast of North America from Baja California to Alaska.[10] Similar ecological conditions are to be found on many of the world's coasts, with the only large potential area not to have been invaded yet being New Zealand; the New Zealand government has taken action, including the release of a Marine Pest Guide[18] in an effort to prevent colonisation by C.?maenas.\\r\\nC.?maenas can live in all types of protected and semi-protected marine and estuarine habitats, including habitats with mud, sand, or rock substrates, submerged aquatic vegetation, and emergent marsh, although soft bottoms are preferred. C.?maenas is euryhaline, meaning that it can tolerate a wide range of salinities (from 4 to 52?R), and survive in temperatures of 0 to 30?C (32 to 86?F).[19] The wide salinity range allows C.?maenas to survive in the lower salinities found in estuaries, and the wide temperature range allows it to survive in extremely cold climates beneath the ice in Winter. A molecular biological study using the COI gene found genetic differentiation between the North Sea and the Bay of Biscay, and even more strongly between the populations in Iceland and the Faroe Islands and those elsewhere. This suggests that C.?maenas is unable to cross deeper water.[20]\\r\\nFemales can produce up to 185,000 eggs, and larvae develop offshore in several stages before their final moult to juvenile crabs in the intertidal zone.[21] Young crabs live among seaweeds and seagrasses, such as Posidonia oceanica, until they reach adulthood.[22]\\r\\nC.?maenas has the ability to disperse by a variety of mechanisms,[21] including ballast water, ships' hulls, packing materials (seaweeds) used to ship live marine organisms, bivalves moved for aquaculture, rafting, migration of crab larvae on ocean currents, and the movement of submerged aquatic vegetation for coastal zone management initiatives. Thresher et al.[14] found C.?maenas dispersed in Australia mainly by rare long-distance events, possibly caused by human actions.\\r\\nC.?maenas is a predator, feeding on many organisms, particularly bivalve molluscs (such as clams, oysters, and mussels), polychaetes and small crustaceans.[23] They are primarily nocturnal, although activity also depends on the tide, and crabs can be active at any time of day.[24] In California, preferential predation of C.?maenas on native clams (Nutricola spp.) resulted in the decline of the native clams and an increase of a previously introduced clam (the amethyst gem clam, Gemma gemma).[25] C.?maenas has been implicated in the destruction of the soft-shell clam (Mya arenaria) fisheries on the east coast of the United States and Canada, and the reduction of populations of other commercially important bivalves (such as scallops, Argopecten irradians, and northern quahogs, Mercenaria mercenaria).[21] The prey of C.?maenas includes the young of bivalves[26] and fish, although the effect of its predation on winter flounder, Pseudopleuronectes americanus is minimal.[27] C.?maenas can, however, have substantial negative impacts on local commercial and recreational fisheries, by preying on the young of species, such as oysters and the Dungeness crab, or competing with them for resources.[28]\\r\\nDue to its potentially harmful effects on ecosystems, various efforts have been made to control introduced populations of C.?maenas around the world. In Edgartown, Massachusetts, a bounty was levied in 1995 for catching C.?maenas, to protect local shellfish, and 10?tons were caught.[29]\\r\\nThere is evidence that the native blue crab in eastern North America, Callinectes sapidus, is able to control populations of C.?maenas; numbers of the two species are negatively correlated, and C.?maenas is not found in the Chesapeake Bay, where Callinectes sapidus is most frequent.[30] On the west coast of North America, C.?maenas appears to be limited to upper estuarine habitats, in part because of predation by native rock crabs (Romaleon antennarium and Cancer productus) and competition for shelter with a native shore crab, Hemigrapsus oregonensis.[31] Host specificity testing has recently been conducted on Sacculina carcini, a parasitic barnacle, as a potential biological control agent of C.?maenas.[32] In the laboratory, Sacculina settled on, infected, and killed native California crabs, including the Dungeness crab, Metacarcinus magister (formerly Cancer magister), and the shore crabs Hemigrapsus nudus, Hemigrapsus oregonensis and Pachygrapsus crassipes. Dungeness crabs were the most vulnerable of the tested native species to settlement and infection by the parasite. Although Sacculina did not mature in any of the native crabs, developing reproductive sacs were observed inside a few Metacarcinus magister and Hemigrapsus oregonensis. Any potential benefits of using Sacculina to control C.?maenas on the west coast of North America would need to be weighed against these potential non-target impacts.[32]\\r\\nResearchers at the University of Maine have actively been developing value-added green crab products, with the goals of driving business interest, stimulating a commercial green crab fishery, and alleviating predation effects.[33] Specifically, one study evaluated the consumer acceptability of empanadas (fried, stuffed pastries) which contained varying amounts of green crab mince meat.[34] It was reported that the empanadas were rated between \\"like slightly\\" and \\"like moderately\\" for overall acceptability by a consumer panel (n=87). Furthermore, about two-thirds of the panelists would \\"probably\\" or \\"definitely\\" buy the empanadas if available locally. Additionally, the same researchers developed a patty product made from green crab mince meat using restructuring additives (transglutaminase, dried egg white, isolated soy protein).[35] It was reported that although a successful green crab patty was developed, the restructuring additives may have had greater functionality in a raw crab meat system, as opposed to the fully cooked mince that was utilized in the present study. The results from both studies are considered promising, especially considering that these were initial rounds of green crab product development.\\r\\nLegal Sea Foods, an East Coast of the United States restaurant chain has arranged to purchase green crabs in season from local fishermen, in the first known commercial use for the crabs in the US food industry. Legal plans to test crab and shrimp etouffee, crab risotto with spring vegetables, and crab minestrone, all made with green crab stock, in their test kitchen during the winter of 2015 and offer the dishes in their other restaurant locations during the crab season.[36]\\r\\nIn its native range, the green crab is mostly used as an ingredient in soups and sauces.\\r\\nC.?maenas is fished on a small scale in the north-east Atlantic Ocean, with approximately 1200?tonnes being caught annually, mostly in France and the United Kingdom. In the northwest Atlantic, C.?maenas was the subject of fishery in the 1960s, and again since 1996, with up to 86 tonnes being caught annually.[37]\\r\\nCarcinus maenas was first given a binomial name, Cancer maenas, by Carl Linnaeus in his 1758 10th edition of Systema Naturae.[38] An earlier description was published by Georg Eberhard Rumphius in his 1705 work De Amboinsche Rariteitkamer, calling the species Cancer marinus sulcatus, but this predates the starting point for zoological nomenclature.[38] A number of later synonyms have also been published:[38]\\r\\nThe lectotype chosen for the species came from Marstrand, Sweden, but it is assumed to have been lost.[38] In 1814, writing for The Edinburgh Encyclopaedia, William Elford Leach erected a new genus, Carcinus to hold this species alone (making it the type species of the genus, by monotypy).[38] In 1847, Nardo described a distinct subspecies occurring in the Mediterranean Sea, which is now recognised as a distinct species, Carcinus aestuarii.[1]","input":"Where is the european green crab native to?"},{"output":"Egypt","context":"The Sinai Peninsula or simply Sinai (/?sa?na?/;[1][2] Arabic: ?????? Sؐn??; Egyptian Arabic: ????? Sؐna, IPA:?[?si?n?]; Hebrew: ??????? Sinai; Coptic: ????) is a peninsula in Egypt, the only part of the country located in Asia. It is situated between the Mediterranean Sea to the north and the Red Sea to the south, and is a land bridge between Asia and Africa. Sinai has a land area of about 60,000?km2 (23,000?sq?mi) and a population of approximately 1,400,000 people. Administratively, the Sinai Peninsula is divided into two governorates: the South Sinai Governorate and the North Sinai Governorate. Three other governorates span the Suez Canal, crossing into African Egypt: Suez Governorate on the southern end of the Suez Canal, Ismailia Governorate in the center, and Port Said Governorate in the north.\\r\\nThe Sinai Peninsula has been a part of Egypt from the First Dynasty of ancient Egypt (c.?3100 BC). This comes in stark contrast to the region north of it, the Levant (present-day territories of Syria, Lebanon, Jordan, Israel and Palestine), which, due largely to its strategic geopolitical location and cultural convergences, has historically been the centre of conflict between Egypt and various states of Mesopotamia and Asia Minor. In periods of foreign occupation, the Sinai was, like the rest of Egypt, also occupied and controlled by foreign empires, in more recent history the Ottoman Empire (1517ÿ1867) and the United Kingdom (1882ÿ1956). Israel invaded and occupied Sinai during the Suez Crisis (known in Egypt as the Tripartite Aggression due to the simultaneous coordinated attack by the UK, France and Israel) of 1956, and during the Six-Day War of 1967. On 6 October 1973, Egypt launched the Yom Kippur War to retake the peninsula, which was the site of fierce fighting between Egyptian and Israeli forces. By 1982, as a result of the IsraelÿEgypt Peace Treaty of 1979, Israel had withdrawn from all of the Sinai Peninsula except the contentious territory of Taba, which was returned after a ruling by a commission of arbitration in 1989.\\r\\nToday, Sinai has become a tourist destination due to its natural setting, rich coral reefs, and biblical history. Mount Sinai is one of the most religiously significant places in the Abrahamic faiths.\\r\\n\\r\\n\\r\\nThe name Sinai may have been derived from the ancient moon-god Sin[3] or from the Hebrew word Seneh (Hebrew: ???????? Senneh)[4] The peninsula acquired the name due to the assumption that a mountain near Saint Catherine's Monastery is the Biblical Mount Sinai. However this assumption is contested.[5]\\r\\nIn addition to its formal name, Egyptians also refer to it as Ar? ul-Fairz (??? ??????? \\"the land of turquoise\\"). The ancient Egyptians called it Ta Mefkat, or \\"land of turquoise\\".[6]\\r\\nSinai is triangular in shape, with northern shore lying on the southern Mediterranean Sea, and southwest and southeast shores on Gulf of Suez and Gulf of Aqaba of the Red Sea. It is linked to the African continent by the Isthmus of Suez, 125 kilometres (78?mi) wide strip of land, containing the Suez Canal. The eastern isthmus, linking it to the Asian mainland, is around 200 kilometres (120?mi) wide. The peninsula's eastern shore separates the Arabian plate from the African plate.[7]\\r\\nThe southernmost tip is the Ras Muhammad National Park.\\r\\nMost of the Sinai Peninsula is divided among the two governorates of Egypt: South Sinai (Ganub Sina) and North Sinai (Shamal Sina).[8] Together, they comprise around 60,000 square kilometres (23,000 sq mi) and have a population (January 2013) of 597,000. Three more governates span the Suez Canal, crossing into African Egypt: Suez (el-Sewais) is on the southern end of the Suez Canal, Ismailia (el-Isma'ileyyah) in the centre, and Port Said in the north.\\r\\nThe largest city of Sinai is Arish, capital of the North Sinai, with around 160,000 residents. Other larger settlements include Sharm el-Sheikh and El-Tor, on the southern coast. Inland Sinai is arid, mountainous and sparsely populated, the largest settlements being Saint Catherine and Nekhel.[8]\\r\\nSinai is one of the coldest provinces in Egypt because of its high altitudes and mountainous topographies. Winter temperatures in some of Sinai's cities and towns reach ?16?C (3?F).[citation needed]\\r\\nSinai was called Mafkat or \\"country of turquoise\\" by the ancient Egyptians [10][11] From the time of the First Dynasty or before, the Egyptians mined turquoise in Sinai at two locations, now called by their Egyptian Arabic names Wadi Magharah and Serabit El Khadim. The mines were worked intermittently and on a seasonal basis for thousands of years. Modern attempts to exploit the deposits have been unprofitable. These may be the first historically attested mines.[citation needed]\\r\\nAt the end of the time of Darius I, the Great (521ÿ486 BCE) Sinai was part of the Persian province of Abar-Nahra, which means \\"beyond the river [Euphrates].[12]\\r\\nCambyses successfully managed the crossing of the hostile Sinai Desert, traditionally Egypt's first and strongest line of defence, and brought the Egyptians under Psamtik III, son and successor of Ahmose, to battle at Pelusium. The Egyptians lost and retired to Memphis; the city fell to the Persian control and the Pharaoh was carried off in captivity to Susa in mainland Persia.\\r\\nAfter the death of the last Nabatean king, Rabbel II Soter, in 106,[13] the Roman emperor Trajan faced practically no resistance and conquered the kingdom on 22 March 106. With this conquest, the Roman Empire went on to control all shores of the Mediterranean Sea. The Sinai Peninsula became part of the Roman province of Arabia Petraea.[14]\\r\\nSaint Catherine's Monastery on the foot of Mount Sinai was constructed by order of the Emperor Justinian between 527 and 565. Most of the Sinai Peninsula became part of the province of Palaestina Salutaris in the 6th century.\\r\\nDuring the Crusades it was under control of Fatimid Caliphate. Later, Saladin abolished the Fatimid Caliphate in Egypt and took this region under his control too. It was the military route from Cairo to Damascus during the Crusades.[citation needed]\\r\\nThe peninsula was governed as part of Egypt under the Mamluk Sultanate of Egypt from 1260 until 1517, when the Ottoman Sultan, Selim the Grim, defeated the Egyptians at the Battles of Marj Dabiq and al-Raydaniyya, and incorporated Egypt into the Ottoman Empire. From then until 1906, Sinai was administered by the Ottoman provincial government of the Pashalik of Egypt, even following the establishment of the Muhammad Ali Dynasty's rule over the rest of Egypt in 1805.\\r\\nIn 1906, the Ottoman Porte formally transferred administration of Sinai to the Egyptian government, which essentially meant that it fell under the control of the United Kingdom, who had occupied and largely controlled Egypt since 1882. The border imposed by the British runs in an almost straight line from Rafah on the Mediterranean shore to Taba on the Gulf of Aqaba. This line has served as the eastern border of Egypt ever since.\\r\\nAt the beginning of the 1948 ArabÿIsraeli War, Egyptian forces entered the former British Mandate of Palestine from Sinai to support Palestinian and other Arab forces against the newly declared State of Israel. For a period during the war, Israeli forces entered the north-eastern corner of Sinai.[15] With the exception of Palestine's Gaza Strip, which came under the administration of the All-Palestine Government,[16] the western frontier of the former Mandate of Palestine became the EgyptianÿIsraeli frontier under the 1949 Armistice Agreement. In 1958, the Gaza Strip came under direct Egyptian military administration, though it was governed separately from Sinai, and was never annexed by Egypt. The Egyptian government maintained that Egyptian administration would be terminated upon the end of the conflict with Israel.\\r\\nIn 1956, Egypt nationalised the Suez Canal,[17] a waterway marking the boundary between Egyptian territory in Africa and the Sinai Peninsula. Thereafter, Israeli ships were prohibited from using the Canal,[18] owing to the state of war between the two states. Egypt also prohibited ships from using Egyptian territorial waters on the eastern side of the peninsula to travel to and from Israel, effectively imposing a blockade on the Israeli port of Eilat. Subsequently, in what is known in Egypt as the Tripartite Aggression, Israeli forces, aided by Britain, and France (which sought to reverse the nationalisation and regain control over the Suez Canal), invaded Sinai and occupied much of the peninsula within a few days. Several months later Israel withdrew its forces from Sinai, following strong pressure from the United States and the Soviet Union. Thereafter, the United Nations Emergency Force (UNEF) was stationed in Sinai to prevent any further conflict in the Sinai.\\r\\nIn 1967, Egypt reinforced its military presence in Sinai and on 16 May ordered the UNEF out of Sinai with immediate effect.[19] Secretary-General U Thant eventually complied and ordered the withdrawal without Security Council authorisation. In the course of the Six-Day War that broke out shortly thereafter, Israel captured the entire Sinai Peninsula, and Gaza Strip from Egypt, the West Bank (including East Jerusalem) from Jordan (which it had ruled since 1949), and the Golan Heights from Syria. The Suez Canal, the east bank of which was now occupied by Israel, was closed. Israel commenced efforts at large scale Israeli settlement in the peninsula.\\r\\nFollowing the Israeli conquest of Sinai, Egypt launched the War of Attrition (1967ÿ70) aimed at forcing Israel to withdraw from Egyptian territory. The war saw protracted conflict in the Suez Canal Zone, ranging from limited to large scale combat. Israeli shelling of the cities of Port Said, Ismailia, and Suez on the west bank of the canal, led to high civilian casualties (including the virtual destruction of Suez), and contributed to the flight of 700,000[20] Egyptian internal refugees. Ultimately, the war concluded in 1970 with no change in the front line.[21]\\r\\nOn 6 October 1973, Egypt commenced Operation Badr to retake the Sinai, while Syria launched a simultaneous operation to retake the Golan Heights,[citation needed] thereby beginning the Yom Kippur War (known in Egypt as the October War). Egyptian engineering forces built pontoon bridges to cross the Suez Canal, and stormed the Bar-Lev Line, Israel's defensive line along the canal. Though the Egyptians maintained control of most of the east bank of the Canal, in the later stages of the war, the Israeli military crossed the southern section of Canal, cutting off the Egyptian 3rd Army, and occupied a section of the west bank. The war ended following a mutually agreed-upon ceasefire. After the war, as part of the subsequent Sinai Disengagement Agreements, Israel withdrew from the Canal, with Egypt agreeing to permit passage of Israeli ships. The canal was reopened in 1975, with President Sadat leading the first convoy through the canal aboard an Egyptian destroyer.\\r\\nIn 1979, Egypt and Israel signed a peace treaty in which Israel agreed to withdraw from the entirety of Sinai. Israel subsequently withdrew in several stages, ending in 1982. The Israeli pull-out involved dismantling almost all Israeli settlements, including the settlement of Yamit in north-eastern Sinai. The exception was the coastal city of Sharm el-Sheikh, which the Israelis had founded as Ofira during the period of their occupation. The Treaty allows monitoring of Sinai by the Multinational Force and Observers, and limits the number of Egyptian military forces in the peninsula.\\r\\nSince the early 2000s, Sinai has been the site of several terror attacks against tourists, the majority of whom are Egyptian. Investigations have shown that these were mainly motivated by a resentment of the poverty faced by many Bedouin in the area. Attacking the tourist industry was viewed as a method of damaging the industry so that the government would pay more attention to their situation.[22] (See 2004 Sinai bombings, 2005 Sharm El Sheikh bombings and 2006 Dahab bombings). Since the 2011 Egyptian Revolution, unrest has become more prevalent in the area including the 2012 Egyptian-Israeli border attack in which 16 Egyptian soldiers were killed by militants. (See Sinai insurgency).\\r\\nAlso on the rise are kidnappings of refugees. According to Meron Estifanos, Eritrean refugees are often kidnapped by Bedouin in the northern Sinai, tortured, raped, and only released after receiving a large ransom.[23][24]\\r\\nUnder President el-Sisi, Egypt has implemented a rigorous policy of controlling the border to the Gaza Strip, including the dismantling of tunnels between Gaza and Sinai.[25]\\r\\nThe two governorates of North and South Sinai have a total population of 597,000 (January 2013). This figure rises to 1,400,000 by including Western Sinai, the parts of the Port Said, Ismailia and Suez Governorates lying east of the Suez Canal. Port Said alone has a population of roughly 500,000 people (January 2013). Portions of the populations of Ismailia and Suez live in west Sinai, while the rest live on the western side of the Suez Canal.\\r\\nPopulation of Sinai has largely consisted of desert-dwelling Bedouins with their colourful traditional costumes and significant culture.[26] Large numbers of Egyptians from the Nile Valley and Delta moved to the area to work in tourism, but development adversely affected the native Bedouin population.[citation needed] In order to help alleviate their problems, various NGOs began to operate in the region, including the Makhad Trust, a UK charity that assists the Bedouin in developing a sustainable income while protecting Sinai's natural environment, heritage and culture.[citation needed]\\r\\nSince the IsraeliÿEgyptian peace treaty, Sinai's scenic spots (including coral reefs offshore) and religious structures have become important to the tourism industry. The most popular tourist destination in Sinai are Mount Sinai (Jabal Musa) and St Catherine's Monastery, which is considered to be the oldest working Christian monastery in the world, and the beach resorts of Sharm el-Sheikh, Dahab, Nuweiba and Taba. Most tourists arrive at Sharm el-Sheikh International Airport, through Eilat, Israel and the Taba Border Crossing, by road from Cairo or by ferry from Aqaba in Jordan.[citation needed]\\r\\nCoordinates: 2930N 3350E? / ?29.500N 33.833E? / 29.500; 33.833","input":"What nation is in possession of the sinai peninsula today?"},{"output":"Harry Potter and the Deathly Hallows","context":"Harry James Potter is the title character and protagonist of J. K. Rowling's Harry Potter series. The majority of the books' plot covers seven years in the life of the orphan Potter, who, on his eleventh birthday, learns he is a wizard. Thus, he attends Hogwarts School of Witchcraft and Wizardry to practise magic under the guidance of the kindly headmaster Albus Dumbledore and other school professors. Harry also discovers that he is already famous throughout the novel's magical community, and that his fate is tied with that of Lord Voldemort, the internationally feared Dark Wizard and murderer of his parents, Lily and James.\\r\\n\\r\\n\\r\\nAccording to Rowling, the idea for both the Harry Potter books and its eponymous character came while waiting for a delayed train from Manchester, England to London in 1990. She stated that her idea for \\"this scrawny, black-haired, bespectacled boy who didn't know he was a wizard became more and more real to me\\".[1] While developing the ideas for her book, she also decided to make Harry an orphan who attended a boarding school called Hogwarts. She explained in a 1999 interview with The Guardian: \\"Harry had to be an orphanso that he's a free agent, with no fear of letting down his parents, disappointing them?... Hogwarts has to be a boarding schoolhalf the important stuff happens at night! Then there's the security. Having a child of my own reinforces my belief that children above all want security, and that's what Hogwarts offers Harry.\\"[2]\\r\\nHer own mother's death on 30 December 1990 inspired Rowling to write Harry as a boy longing for his dead parents, his anguish becoming \\"much deeper, much more real\\" than in earlier drafts because she related to it herself.[1] In a 2000 interview with The Guardian, Rowling also established that the character of Wart in T. H. White's novel The Once and Future King is \\"Harry's spiritual ancestor.\\"[3] Finally, she established Harry's birth date as 31 July, the same as her own. However, she maintained that Harry was not directly based on any real-life person: \\"he came just out of a part of me\\".[4]\\r\\nRowling has also maintained that Harry is a suitable real-life role model for children. \\"The advantage of a fictional hero or heroine is that you can know them better than you can know a living hero, many of whom you would never meet [...] if people like Harry and identify with him, I am pleased, because I think he is very likeable.\\"[5]\\r\\nHarry first appears in Harry Potter and the Philosopher's Stone (published in the United States as Harry Potter and the Sorcerer's Stone). Starting in 1981,[6] when Harry was just one year old, his parents, James and Lily, were murdered by the most powerful Dark Wizard, Lord Voldemort (subsequently called \\"You-Know-Who\\" and \\"He-Who-Must-Not-Be-Named\\" by those too superstitious to use his actual name). He attempted to kill Harry too, but was unsuccessful and only left a lightning bolt shaped scar on Harry's forehead. Voldemort's body was destroyed, but his soul was not. Harry later learns that the reason why he survived was because his mother sacrificed herself for him, and her love was something that Voldemort could not destroy.\\r\\nAccording to Rowling, fleshing out this back story was a matter of reverse planning: \\"The basic idea [is that] Harry ... didn't know he was a wizard ... and so then I kind of worked backwards from that position to find out how that could be, that he wouldn't know what he was... That's... When he was one year old, the most evil wizard for hundreds and hundreds of years attempted to kill him. He killed Harry's parents, and then he tried to kill Harryhe tried to curse him.... Harry has to find out, before we find out. And for some mysterious reason, the curse didn't work on Harry. So he's left with this lightning-bolt shaped scar on his forehead, and the curse rebounded upon the evil wizard who has been in hiding ever since\\".[7]\\r\\nAs a result, Harry is written as an orphan living with his only remaining family, the Dursleys, who are neglectful and abusive. On his eleventh birthday, Harry learns he is a wizard when Rubeus Hagrid arrives to tell him that he is to attend Hogwarts School of Witchcraft and Wizardry. There he learns about the wizarding world, his parents, and his connection to the Dark Lord. When he is sorted into Gryffindor House, he becomes fast friends with classmates Ron Weasley and Hermione Granger, and foils Voldemort's attempt to steal the Philosopher's Stone. He also forms a rivalry with characters Draco Malfoy, a classmate from an elitist wizarding family, and the cold, condescending Potions master, Severus Snape, Draco's mentor and the head of Slytherin House. Both feuds continue throughout the series and are settled at the series's end (Draco's in the West End play Harry Potter and the Cursed Child and Snape's on his deathbed in Harry Potter and the Deathly Hallows. In a 1999 interview, Rowling stated that Draco is based on several prototypical schoolyard bullies she encountered[8] and Snape on a sadistic teacher of hers who abused his power.[8]\\r\\nRowling has stated that the \\"Mirror of Erised\\" chapter in Harry Potter and the Philosopher's Stone is her favourite; the mirror reflects Harry's deepest desire, namely to see his dead parents and family.[1] Her favourite funny scene is when Harry inadvertently sets a boa constrictor free from the zoo in the horrified Dursleys' presence.[8]\\r\\nIn the second book, Harry Potter and the Chamber of Secrets, Rowling pits Harry against Tom Riddle, Lord Voldemort's \\"memory\\" within a secret diary which has possessed Ron's younger sister Ginny. When Muggle-born students are suddenly being Petrified, many suspect that Harry may be behind the attacks, further alienating him from his peers. Furthermore, Harry begins to doubt his worthiness for House of Gryffindor, particularly considering he discovers he shares Lord Voldemort's ability to communicate with snakes via Parseltongue. In the climax, Ginny disappears. To rescue her, Harry battles Riddle and the monster he controls that is hidden in the Chamber of Secrets. To defeat the monster, Harry summons the Sword of Godric Gryffindor from the Sorting Hat supplied by Dumbledore's pet phoenix, Fawkes. In doing so, Dumbledore later restores Harry's self-esteem by explaining that feat is clear proof of his worthiness of his present house. In the third book, Harry Potter and the Prisoner of Azkaban, Rowling uses a time travel premise. Harry learns that his parents were betrayed to Voldemort by their friend Peter Pettigrew, who framed Harry's godfather Sirius Black for the crimes, condemning him to Azkaban, the wizard prison. When Sirius escapes to find Harry, Harry and Hermione use a Time Turner to save him and a hippogriff named Buckbeak. When Pettigrew escapes, an innocent Sirius becomes a hunted fugitive once again. Harry learns how to create a Patronus which takes the form of a stag, the same as his late father's.\\r\\nIn the previous books, Harry is written as a child, but Rowling states that in the fourth novel, Harry Potter and the Goblet of Fire, \\"Harry's horizons are literally and metaphorically widening as he grows older.\\"[9] Harry's developing maturity becomes apparent when he becomes romantically interested in Cho Chang, a student in Ravenclaw house. Tension mounts, however, when Harry is mysteriously chosen by the Goblet of Fire to compete in the dangerous Triwizard Tournament, even though another Hogwarts champion, Cedric Diggory, has already been selected.\\r\\nVoldemort uses the Tournament for an elaborate scheme to lure Harry into a deadly trap. During the Tournament's final challenge, Harry and Cedric are transported to a graveyard, using a portkey, where Cedric is killed by Peter Pettigrew, and Voldemort, aided by Pettigrew, uses Harry's blood in a gruesome ritual to resurrect his body. When Harry duels Voldemort, their wands' magical streams connect, forcing the spirit echoes of Voldemort's victims, including Cedric and James and Lily Potter, to be expelled from his wand. The spirits briefly protect Harry as he escapes to Hogwarts with Cedric's body. For Rowling, this scene is important because it shows Harry's bravery, and by retrieving Cedric's corpse, he demonstrates selflessness and compassion. Says Rowling, \\"He wants to save Cedric's parents additional pain.\\" She added that preventing Cedric's body from falling into Voldemort's hands is based on the classic scene in the Iliad where Achilles retrieves the body of his best friend Patroclus from the hands of Hector. Rowling also mentioned that book four rounds off an era in Harry's life, and the remaining three books are another,[9] \\"He's no longer protected. He's been very protected until now. But he's very young to have that experience. Most of us don't get that until a bit later in life. He's only just coming up to 15 and that's it now.\\"[10]\\r\\nIn the fifth book, Harry Potter and the Order of the Phoenix, the Ministry of Magic has been waging a smear campaign against Harry and Dumbledore, disputing their claims that Voldemort has returned. Harry is made to look like an attention-seeking liar, and Dumbledore a trouble-maker. A new character is introduced when the Ministry of Magic appoints Dolores Umbridge as the latest Hogwarts' Defence Against the Dark Arts instructor (and Ministry spy). Because the paranoid Ministry suspects that Dumbledore is building a wizard army to overthrow them, Umbridge refuses to teach students real defensive magic. She gradually gains more power, eventually ousting Dumbledore and seizing control of the school. As a result, Harry's increasingly angry and erratic behaviour nearly estranges him from Ron and Hermione.\\r\\nRowling says she put Harry through extreme emotional stress to show his emotional vulnerability and humanitya contrast to his nemesis, Voldemort. \\"[Harry is] a very human hero, and this is, obviously, a contrast, between him, as a very human hero, and Voldemort, who has deliberately dehumanised himself. And Harry, therefore, did have to reach a point where he did almost break down, and say he didn't want to play any more, he didn't want to be the hero any more?ÿ and he'd lost too much. And he didn't want to lose anything else. So that?ÿ Phoenix was the point at which I decided he would have his breakdown.\\"[11]\\r\\nAt Hermione's urging, Harry forms a secret student organisation called Dumbledore's Army to teach more meaningful defence against the dark arts as Professor Umbridge is making them read off a textbook. Their plan is thwarted, however, when a Dumbledore's Army member, Marietta Edgecombe, betrays them and informs Umbridge about the D.A., causing Dumbledore to be ousted as Headmaster. Harry suffers another emotional blow, when his beloved godfather, Sirius, is killed during a duel with Sirius' cousin, the Death Eater Bellatrix Lestrange, at the Department of Mysteries, but Harry ultimately defeats Voldemort's plan to steal an important prophecy. Rowling stated: \\"And now he [Harry] will rise from the ashes strengthened.\\"[11] A side plot of Order of the Phoenix involves Harry's romance with Cho Chang, but the relationship quickly unravels. Says Rowling: \\"They were never going to be happy, it was better that it ended early!\\"[12]\\r\\nIn the sixth book, Harry Potter and the Half-Blood Prince Harry enters a tumultuous puberty that, Rowling says, is based on her and her younger sister's own difficult teenage years.[13] Rowling also made an intimate statement about Harry's personal life: \\"Because of the demands of the adventure that Harry is following, he has had less sexual experience than boys of his age might have had.\\"[14] This inexperience with romance was a factor in Harry's failed relationship with Cho. Now his thoughts concern Ginny, and a vital plot point in the last chapter includes Harry ending their budding romance to protect her from Voldemort.\\r\\nA new character appears when former Hogwarts Potions master Horace Slughorn replaces Snape, who assumes the Defence Against the Dark Arts post. Harry suddenly excels in Potions, using an old textbook once belonging to a talented student known only as \\"The Half-Blood Prince.\\" The book contains many handwritten notes, revisions, and new spells; Hermione, however, believes Harry's use of it is cheating. Through private meetings with Dumbledore, Harry learns about Voldemort's orphaned youth, his rise to power, and how he splintered his soul into Horcruxes to achieve immortality. Two Horcruxes have been destroyedthe diary and a ring; and Harry and Dumbledore locate another, although it is a fake. When Death Eaters invade Hogwarts, Snape kills Dumbledore. As Snape escapes, he proclaims that he is the Half-Blood Prince (being the son of a muggle father and the pure-blood Eileen Prince). It now falls upon Harry to find and destroy Voldemort's remaining Horcruxes and to avenge Dumbledore's death. In a 2005 interview, Rowling stated that [after the events in the sixth book] Harry has, \\"taken the view that they are now at war. He does become more battle-hardened. He's now ready to go out fighting. And he's after revenge [against Voldemort and Snape].\\"[15]\\r\\nThis book also focuses on the mysterious activities of Harry's rival Draco Malfoy. Voldemort has coerced a frightened Malfoy into attempting to kill Dumbledore. During a duel in Moaning Myrtle's bathroom, Harry uses the Half-Blood Prince's spell, Sectumsempra, on Malfoy, who suffers near-fatal injuries as a result. Harry is horrified by what he has done and also comes to feel sympathy for Draco, after learning he was forced to do Voldemort's bidding under the threat of his and his parents' deaths.\\r\\nIn Harry Potter and the Deathly Hallows, Harry, Ron, and Hermione leave Hogwarts to complete Dumbledore's task: to search for and destroy Voldemort's remaining four Horcruxes, then find and kill the Dark Lord. The three pit themselves against Voldemort's newly formed totalitarian police state, an action that tests Harry's courage and moral character. Voldemort's seizure of the Ministry of Magic leads to discriminatory and genocidal policies against Muggle-borns, fuelled by propaganda and fear. According to J. K. Rowling, telling scenes are when Harry uses Cruciatus Curse and Imperius Curse, unforgivable curses for torture and mind-control, on Voldemort's servants, and also when he casts Sectumsempra on Draco Malfoy during the bathroom fight in the sixth book. Each time shows a \\"flawed and mortal\\" side to Harry. However, she explains, \\"He is also in an extreme situation and attempting to defend somebody very good against a violent and murderous opponent.\\"[16]\\r\\nHarry experiences occasional disturbing visions of Draco being forced to perform the Death Eaters' bidding and feels \\"...sickened...by the use to which Draco was now being put by Voldemort,\\" again showing his compassion for an enemy.\\r\\nEach Horcrux Harry must defeat cannot be destroyed easily. They must be destroyed with basilisk venom, Godric Gryffindor's sword, or some other destructive substance. In Book Two, Harry destroys the first horcrux, Tom Riddle's diary, with a basilisk fang, and in Book Six Dumbledore destroys the ring with Gryffindor's sword. Ron destroys Slytherin's locket with the sword, Hermione destroys Hufflepuff's cup with a basilisk fang, and Crabbe destroys Ravenclaw's diadem with Fiendfyre (cursed flame). Neville kills the snake Nagini with the sword, and Voldemort destroys the final accidental Horcrux: a fragment of soul embedded in Harry's scar.\\r\\nHarry comes to recognise that his own single-mindedness makes him predictable to his enemies and often clouds his perceptions. When Voldemort kills Snape later in the story, Harry realises that Snape was not the traitorous murderer he believed him to be, but a tragic antihero who was loyal to Dumbledore. In Chapter 33 ('The Prince's Tale') Snape's memories reveal that he loved Harry's mother Lily, but their friendship ended over his association with future Death Eaters and his \\"blood purity\\" beliefs. When Voldemort murdered the Potters, a grieving Snape vowed to protect Lily's child, although he loathed young Harry for being James Potter's son. The memories also reveal that Snape did not murder Dumbledore, but carried out Dumbledore's prearranged plan. Dumbledore, dying from a slow-spreading curse, wanted to protect Snape's position within the Death Eaters and to spare Draco from completing Voldemort's task of murdering him.\\r\\nTo defeat Harry, Voldemort steals the most powerful wand ever created, the Elder Wand, from Dumbledore's tomb and twice casts the Killing Curse on Harry with it. The first attempt merely stuns Harry into a deathlike state; the murder attempt fails because Voldemort used Harry's blood in his resurrection during book four. The protection that his mother gave Harry with her sacrifice tethers Harry to life, as long as his blood and her sacrifice run in the veins of Voldemort. In the chapter \\"King's Cross,\\" Dumbledore's spirit talks to Harry whilst in this deathlike state. Dumbledore informs Harry that when Voldemort disembodied himself during his failed attempt to kill Harry as a baby, Harry became an unintentional Horcrux; Harry could not kill Voldemort while the Dark Lord's soul shard remained within Harry's body. The piece of Voldemort's soul within Harry was destroyed through Voldemort's first killing curse with the Elder Wand because Harry willingly faced death, which cast a sacrificial protection on the defenders of Hogwarts.\\r\\nIn the book's climax, Voldemort's second Killing Curse hurled at Harry also fails and rebounds upon Voldemort, finally killing him. The spell fails because Harry, not Voldemort, had become the Elder Wand's true master and the wand could not harm its own master. Harry has each of the Hallows (the Invisibility Cloak, the Resurrection Stone, and the Elder Wand) at some point in the story but never unites them. However, J. K. Rowling said the difference between Harry and Voldemort is that Harry willingly accepts mortality, making him stronger than his nemesis. \\"The real master of Death accepts that he must die, and that there are much worse things in the world of the living.\\" At the very end, Harry decides to leave the Elder Wand in Dumbledore's tomb and the Resurrection Stone hidden in the forest, but he keeps the Invisibility Cloak because it had belonged to his father.[16]\\r\\nIn the epilogue of Deathly Hallows, which is set 19 years after Voldemort's death, Harry and Ginny are a couple and have three children: James Sirius Potter, who has already been at Hogwarts for at least one year, Albus Severus Potter, who is starting his first year there, and Lily Luna Potter, who is two years away from her first year at the school.\\r\\nAccording to Rowling, after Voldemort's defeat, Harry joins the \\"reshuffled\\" Auror Department under Kingsley Shacklebolt's mentoring, and ends up eventually rising to become Head of said department in 2007.[17] Rowling said that his old rival Draco has a grudging gratitude towards Harry for saving his life in the final battle, but the two are not friends.[16]\\r\\nIn the eight Harry Potter films screened from 2001 to 2011, Harry Potter has been portrayed by British actor Daniel Radcliffe. Radcliffe was asked to audition for the role of Harry in 2000 by producer David Heyman, while in attendance at a play titled Stones in His Pockets in London.[18][19] The role has been highly lucrative for Radcliffe; as of 2015, he had an estimated wealth of $110 million and was paid a collective $53 million for the last two films.[20]\\r\\nIn a 2007 interview with MTV, Radcliffe stated that, for him, Harry is a classic coming of age character: \\"That's what the films are about for me: a loss of innocence, going from being a young kid in awe of the world around him, to someone who is more battle-hardened by the end of it.\\"[21] He also said that for him, important factors in Harry's psyche are his survivor's guilt in regard to his dead parents and his lingering loneliness. Because of this, Radcliffe talked to a bereavement counsellor to help him prepare for the role.[21] Radcliffe was quoted as saying that he wished for Harry to die in the books, but he clarified that he \\"can't imagine any other way they can be concluded.\\"[21] After reading the last book, where Harry and his friends do indeed survive and have children, Radcliffe stated he was glad about the ending and lauded Rowling for the conclusion of the story.[22] Radcliffe stated that the most repeated question he has been asked is how Harry Potter has influenced his own life, to which he regularly answers it has been \\"fine,\\" and that he did not feel pigeonholed by the role, but rather sees it as a huge privilege to portray Harry.[23]\\r\\nRadcliffe's Harry was named the 36th greatest movie character of all time by Empire.[24]\\r\\nThroughout the series, Harry is described as having his father's perpetually untidy black hair, his mother's bright green eyes, and a lightning bolt-shaped scar on his forehead. He is further described as \\"small and skinny for his age\\" with \\"a thin face\\" and \\"knobbly knees\\", and he wears round eyeglasses. In the first book, his scar is described as \\"the only thing Harry liked about his own appearance\\". When asked about the meaning behind Harry's lightning bolt scar, Rowling said, \\"I wanted him to be physically marked by what he has been through. It was an outward expression of what he has been through inside... It is almost like being the chosen one or the cursed one, in a sense.\\" Rowling has also stated that Harry inherited his parents' good looks.[25] In the later part of the series Harry grows taller, and by the seventh book is said to be 'almost' the height of his father, and 'tall' by other characters.[26]\\r\\nRowling explained that Harry's image came to her when she first thought up Harry Potter, seeing him as a \\"scrawny, black-haired, bespectacled boy\\".[1] She also mentioned that she thinks Harry's glasses are the clue to his vulnerability.[27]\\r\\nAccording to Rowling, Harry is strongly guided by his own conscience, and has a keen feeling of what is right and wrong. Having \\"very limited access to truly caring adults\\", Rowling said, Harry \\"is forced to make his own decisions from an early age on.\\"[28] He \\"does make mistakes\\", she conceded, but in the end, he does what his conscience tells him to do. According to Rowling, one of Harry's pivotal scenes came in the fourth book when he protects his dead schoolmate Cedric Diggory's body from Voldemort, because it shows he is brave and selfless.[9]\\r\\nRowling has stated that Harry's character flaws include anger and impulsiveness; however, Harry is also innately honourable.[16][29] \\"He's not a cruel boy. He's competitive, and he's a fighter. He doesn't just lie down and take abuse. But he does have native integrity, which makes him a hero to me. He's a normal boy but with those qualities most of us really admire.\\"[30] For the most part, Harry shows humility and modesty, often downplaying his achievements; though he uses a litany of his adventures as examples of his maturity early in the fifth book. However, these very same accomplishments are later employed to explain why he should lead Dumbledore's Army, at which point he asserts them as having just been luck, and denies that they make him worthy of authority. After the seventh book, Rowling commented that Harry has the ultimate character strength, which not even Voldemort possesses: the acceptance of the inevitability of death.\\r\\nThroughout the series, Harry Potter is described as a gifted wizard apprentice. He has a particular talent for flying, which manifests itself in Harry Potter and the Philosopher's Stone the first time he tries it, and gets him a place on a Quidditch team one year before the normal minimum joining age. He captains it in his sixth year. In his fourth year (Harry Potter and the Goblet of Fire), Harry is able to confront a dragon on his broomstick.\\r\\nHarry is also gifted in Defence Against the Dark Arts, in which he becomes proficient due to his repeated encounters with Voldemort and various monsters. In his third year, Harry becomes able to cast the very advanced Patronus Charm, and by his fifth year he has become so talented at the subject that he is able to teach his fellow students in Dumbledore's Army, some even older than him how to defend themselves against Dark Magic. At the end of that year, he achieves an 'Outstanding' Defence Against the Dark Arts O.W.L., something that not even Hermione achieved. He is a skilled duellist, the only one of the six Dumbledore's Army members to be neither injured nor incapacitated during the battle with Death Eaters in the Department of Mysteries in Harry Potter and the Order of the Phoenix. He also fends off numerous Death Eaters during his flight to the Burrow at the beginning of Harry Potter and the Deathly Hallows.\\r\\nHarry also had the unusual ability to speak and understand \\"Parseltongue\\", a language associated with Dark Magic. This, it transpires, is because he harbours a piece of Voldemort's soul. He loses this ability after the part of Voldemort's soul inside him is destroyed at the end of The Deathly Hallows.\\r\\nHarry's parents left behind a somewhat large pile of wizard's gold, used as currency in the world of magic, in a vault in the wizarding bank, Gringotts. After Sirius' death later in the series, all of his remaining possessions are also passed along to Harry, including Number Twelve, Grimmauld Place, and Sirius's vast amount of gold were transferred into Harry's account at Gringotts. Rowling noted that \\"Harry's money never really is that important in the books, except that he can afford his books and uniforms and so on.\\"[31]\\r\\nAmong the school items Harry purchases in Diagon Alley after discovering his gold inheritance is his first wand, an 11-inch-long holly and phoenix feather model that he learns is the twin of Voldemort's wand, as the feathers that both wands contain as their cores both comes from Fawkes, the phoenix that Dumbledore keeps as a pet in his office until his death in Half-Blood Prince.[32] Harry's wand is broken in Deathly Hallows. For a time, he borrows Hermione's wand, and later steals Draco's. With his defeat of Voldemort at the end of the series, he comes into the possession of the Elder Wand, but uses it only to repair his holly wand, before returning it to Dumbledore's tomb, from which Voldemort had stolen it. In the film version of Deathly Hallows Part 2, Harry destroys the Elder Wand.\\r\\nHarry also inherits indirectly two of his father's prized possessions. One is the Marauder's Map, given to him by interim owners Fred and George Weasley, which endows Harry with comprehensive knowledge of Hogwarts' facilities, grounds, and occupants. The other is his father's Invisibility Cloak, given to him by Dumbledore, which eventually proves Harry's descent from the Peverell family. Harry uses these tools both to aid in excursions at school and to protect those he cares about; the Invisibility Cloak, in particular, can hide two full-grown people. If three fully-grown people hide under the cloak their feet will be visible. When Harry reaches his age of maturity at seventeen, Molly Weasley gives him a pocket watch which had once belonged to her brother Fabian Prewett, as it is traditional to give a boy a watch when he turns seventeen.\\r\\nThroughout the majority of the books, Harry also has a pet owl named Hedwig, used to deliver and receive messages and packages. Hedwig is killed in the seventh book, about which Rowling says: \\"The loss of Hedwig represented a loss of innocence and security. She has been almost like a cuddly toy to Harry at times. I know that death upset a lot of people!\\"[16] As a Quidditch player, Harry has owned two high-quality brooms. The first, a Nimbus Two Thousand, was procured for him by Professor Minerva Mcgonagall when Harry was added to Gryffindor's Quidditch team despite being a first-year student. This broom was destroyed by the Whomping Willow during a match in Harry's third year. It was replaced by a Firebolt, an even faster (and more expensive) broom, purchased for Harry by Sirius; however, as Sirius was believed to be trying to murder Harry at the time, the broom was subjected to stringent security inspections before Harry was allowed to ride it. Harry used it throughout his Hogwarts career until it, along with Hedwig, was lost during the July escape from Privet Drive in the final book.\\r\\nHarry also owns a mokeskin pouch, or small 'bag' that is used for storing items, which no one but the owner can get out. He receives this from Hagrid as a 17th birthday present. Harry uses the pouch throughout the course of Deathly Hallows to keep several sentimental (yet, as he himself admits, otherwise worthless) objects such as the Marauder's Map, a shard of the magical mirror given to him by his god-father Sirius, the fake Horcrux locket that had belonged to Sirius's brother R.A.B (Regulus Arcturus Black), the Snitch bequeathed to him by Dumbledore, containing the Resurrection Stone that had previously been set into Voldemort's grandfather Marvolo Gaunt's signet ring, which Harry discovers is actually the second Hallow, a letter from his mother to Sirius with part of a photo (of him and his father, James), and eventually, his own broken wand (which Harry later repairs with the Elder Wand).\\r\\nIn the novels, Harry is the only child of James and Lily Potter, orphaned as an infant. Rowling made Harry an orphan from the early drafts of her first book. She felt an orphan would be the most interesting character to write about.[2] However, after her mother's death, Rowling wrote Harry as a child longing to see his dead parents again, incorporating her own anguish into him. Harry is categorised as a \\"half-blood\\" wizard in the series, because although both his parents were magical, Lily was \\"Muggle-born\\", and James was a pure-blood.\\r\\nHarry's aunt and uncle kept the truth about his parents' deaths from Harry, telling him that they had died in a car crash.[1] James Potter is a descendant of Ignotus Peverell, the third of the three original owners of the Deathly Hallows, and thus so is Harry, a realisation he makes during the course of the final book. The lineage continues at the end of the saga through his three children with Ginny: James Sirius Potter, Albus Severus Potter and Lily Luna Potter.\\r\\nIn an original piece published on the Pottermore website in September 2015, Rowling described the history of the Potter family in greater detail, beginning with the 12th-century wizard Linfred of Stinchcombe, \\"a locally well-beloved and eccentric man, whose nickname, 'the Potterer', became corrupted in time to 'Potter'\\". Lindred was the inventor of a number of remedies that evolved into potions still used in the modern day, including Skele-Gro and Pepperup Potion. These successful products garnered Linfred the earnings that formed the basis of the family's wealth, which grew with the work of successive generations. Linfred's oldest son, Hardwin, married a beautiful young witch from Godric's Hollow named Iolanthe Peverell, the granddaughter of Ignotus Peverell, who continued the tradition of passing down Ignotus' Invisibility Cloak through the generations. Two of Harry Potter's ancestors have sat on the Wizengamot: Ralston Potter and Henry Potter. Ralston was a member from 1612-1652, and an ardent supporter of the Statute of Secrecy. Henry Potter, known as \\"Harry\\" to his closest loved ones, was a direct descendant of Hardwin and Iolanthe, and a paternal great-grandfather of Harry Potter. Henry served on the Wizengamot from 1913 - 1921, and caused a minor controversy when he publicly condemned then Minister for Magic, Archer Evermonde, for prohibiting the magical community from helping Muggles waging the First World War. Henry's son, Fleamont Potter, who was given his grandmother's surname as his given name in order to grant the dying wish of Henry's mother to continue her family name, garnered a reputation for his duels at Hogwarts, which were provoked when others mocked him for his name. Fleamont quadrupled the family gold by creating magical Sleekeazy's Hair Potion, selling his company at a vast profit when he retired. Fleamont and his wife, Euphemia, had given up hope of having a child when she became pregnant with their son, James, who would go on to marry Lily Evans and bear a son of their own, Harry Potter. Fleamont and Euphemia lived to see James and Lily marry, but they would never meet their famous grandson, as they both died of dragon pox, stemming from their advanced age.[33][34]\\r\\nIn 2002, Harry Potter was voted No. 85 among the \\"100 Best Fictional Characters\\" by Book magazine[35] and also voted the 35th \\"Worst Briton\\" in Channel 4's \\"100 Worst Britons We Love to Hate\\" programme.[36] Entertainment Weekly ranked Harry Potter number two on its 2010 \\"100 Greatest Characters of the Last 20 Years\\" list, saying \\"Long after we've turned the last page and watched the last end credit, Harry still feels like someone we know. And that's the most magical thing about him.\\"[37] UGO Networks listed Harry as one of their best heroes of all time, who said that \\"Harry is a hero to the often oppressed and downtrodden young fan boys and girls out there, who finally have an icon that is respected and revered by those who might otherwise look down on robe-wearing and wand waving as dork fodder\\".[38] Harry Potter was also ranked number thirty-six on Empire's 2008 list of \\"100 Greatest Movie Characters of All Time\\".[39] IGN said that Harry Potter was their favourite Harry Potter character, calling him a \\"sympathetic figure\\" and saying in response to his fights against Voldemort that \\"everybody loves an underdog story of good vs. evil\\".[40] Despite being the main character, Watchmojo.com ranked him #2 on their \\"Top 10 Harry Potter Characters\\" list in 2014 (Severus Snape was ranked #1 on the list).\\r\\nAccording to halloweenonline.com, Harry Potter sets were the fifth-best selling Halloween costume of 2005.[41] In addition, wizard rock bands like Harry and the Potters and others regularly dress up in the style of Harry Potter, sporting painted forehead scars, black wigs, and round bottle top glasses. Wizard rock is a musical movement dating from 2002 that consists of at least 200 bands made up of young musicians, playing songs about Harry Potter.[42][43] The movement started in Massachusetts with the band Harry and the Potters, who cosplay as Harry during live performances.[44][45]\\r\\nIn April 2009, a group of University of Michigan students (StarKid Productions: Darren Criss, Joey Richter.) performed Harry Potter: The Musical, a two-act musical parody that featured major elements from all seven books and an original score. They posted the entire musical on their YouTube channel but removed it in late June, to edit some more mature elements from the videos. The musical, re-titled A Very Potter Musical, was reposted on 5 July 2009, starring Darren Criss as Harry Potter. A sequel was premiered at the 2010 HPEF Harry Potter Conference Infinitus, and released on YouTube on 22 July at 8?pm EST. The sequel was called A Very Potter Sequel and featured the Death Eaters using the Time-Turner to go back in time to Harry's first year in Hogwarts.[46] Harry Potter is spoofed in the Barry Trotter series by American writer Michael Gerber, where a \\"Barry Trotter\\" appears as the eponymous antihero. On his homepage, Gerber describes Trotter as an unpleasant character who \\"drinks too much, eats like a pig, sleeps until noon, and owes everybody money.\\"[47] The author stated \\"[s]ince I really liked Rowling's books [] I felt obligated to try to write a spoof worthy of the originals\\".[48]","input":"How old was harry potter when he left hogwarts?"},{"output":"Construction of the World Trade Center's North Tower began in August 1968","context":"The construction of New York City's first World Trade Center complex was conceived as an urban renewal project, spearheaded by David Rockefeller, to help revitalize Lower Manhattan. The project was developed by the Port Authority of New York and New Jersey, which hired architect Minoru Yamasaki who came up with the specific idea for twin towers. After extensive negotiations, the New Jersey and New York state governments, which oversee the Port Authority, agreed to support the World Trade Center project, which was built at the site of Radio Row in the Lower West Side of Manhattan, New York City. To make the agreement acceptable to New Jersey, the Port Authority agreed to take over the bankrupt Hudson & Manhattan Railroad, which brought commuters from New Jersey to the Lower Manhattan site and, upon the Port Authority's takeover of the railroad, was renamed PATH.\\r\\nThe towers were designed as framed tube structures, which provided tenants with open floor plans, uninterrupted by columns or walls. This was accomplished using numerous closely spaced perimeter columns to provide much of the strength to the structure, along with gravity load shared with the core columns. The elevator system, which made use of sky lobbies and a system of express and local elevators, allowed substantial floor space to be freed up for use as office space by making the structural core smaller. The design and construction of the World Trade Center, most centrally its twin towers, involved many other innovative techniques, such as the slurry wall for digging the foundation, and wind tunnel experiments.\\r\\nConstruction of the World Trade Center's North Tower began in August 1968, and the South Tower in 1969. Extensive use of prefabricated components helped to speed up the construction process. The first tenants moved into the North Tower in December 1970 and into the South Tower in January 1972. Four other low-level buildings were constructed as part of the World Trade Center in the 1970s, and the seventh building, 7 World Trade Center, was constructed in the mid-1980s.\\r\\n\\r\\n\\r\\nIn 1942, Austin J. Tobin became the Executive Director of the Port Authority, beginning a 30-year career during which he oversaw the planning and development of the World Trade Center.[1] The concept of establishing a \\"world trade center\\" was conceived during the postÿWorld War II period, when the United States thrived economically and international trade was increasing. In 1946, the New York State Legislature passed a bill that called for a \\"world trade center\\" to be established.[2] The World Trade Corporation was founded, and a board was appointed by New York Governor Thomas E. Dewey to develop plans for the project.[2] Architect John Eberson and his son Drew devised a plan that included 21 buildings over a ten-block area, at an estimated cost of $150 million.[3] In 1949, the World Trade Corporation was dissolved by the New York State Legislature, and plans for a \\"world trade center\\" were put on hold.[4]\\r\\nDuring the post-war period, economic growth was concentrated in Midtown Manhattan, in part stimulated by the Rockefeller Center, which was developed in the 1930s. Meanwhile, Lower Manhattan was left out of the economic boom. One exception was the construction of One Chase Manhattan Plaza in the Financial District by David Rockefeller, who led urban renewal efforts in Lower Manhattan.[5] In 1958, Rockefeller established the Downtown-Lower Manhattan Association (DLMA), which commissioned Skidmore, Owings and Merrill to draw up plans for revitalizing Lower Manhattan. The plans, made public in late June 1960 called for a World Trade Center to be built on a 13-acre (53,000?m2) site along the East River, from Old Slip to Fulton Street and between Water Street and South Street.[6][7] The complex would include a 900-foot (275?m) long exhibition hall, and a 50ÿ70 story building, with some of its upper floors used as a hotel.[8] Other amenities would include a theater, shops, and restaurants.[9] The plan also called for a new securities exchange building, which the Downtown-Lower Manhattan Association hoped would house the New York Stock Exchange.[7]\\r\\nDavid Rockefeller suggested that the Port Authority would be a logical choice for taking on the project,[7] and argued that the Trade Center would provide great benefits in facilitating and increasing volume of international commerce coming through the Port of New York.[9] Given the importance of New York City in global commerce, Port Authority director Austin J. Tobin remarked that the proposed project should be the World Trade Center, and not just a \\"world trade center.\\"[10] After a year-long review of the proposal, the Port Authority formally backed the project on March 11, 1961.[11]\\r\\nThe States of New York and New Jersey also needed to approve the project, given their control and oversight role of the Port Authority. Objections to the plan came from New Jersey Governor Robert B. Meyner, who resented that New York would be getting this $335 million project.[5] Meanwhile, ridership on New Jersey's Hudson and Manhattan Railroad (H&M) had declined substantially from a high of 113?million riders in 1927 to 26?million in 1958, after new automobile tunnels and bridges opened across the Hudson River.[12] Toward the end of 1961, negotiations with outgoing New Jersey Governor Meyner regarding the World Trade Center project reached a stalemate. In December 1961, Tobin met with newly elected New Jersey Governor Richard J. Hughes, and made a proposal to shift the World Trade Center project to a west side site where the Hudson Terminal was located.[13] In acquiring the Hudson & Manhattan Railroad, the Port Authority would also acquire the Hudson Terminal and other buildings which were deemed obsolete.[13] On January 22, 1962, the two states reached an agreement to allow the Port Authority to take over the railroad and to build the World Trade Center on Manhattan's lower west side.[14] The shift in location for the World Trade Center to a site more convenient to New Jersey, together with Port Authority acquisition of the H&M Railroad, brought New Jersey to agreement in support of the World Trade Center project.\\r\\nEven once the agreement between the states of New Jersey, New York, and the Port Authority was finalized, the World Trade Center plan faced continued controversy. The site for the World Trade Center was the location of Radio Row, which was home to hundreds of commercial and industrial tenants, property owners, small businesses, and approximately 100 residents.[5] The World Trade Center plans involved evicting these business owners, some of whom fiercely protested the forced relocation.[5] In June 1962, a group representing approximately 325 shops and 1,000 other affected small businesses filed an injunction, challenging the Port Authority's power of eminent domain.[15] The dispute with local business owners worked its way through the court system, up to the New York State Court of Appeals, which in April 1963 upheld the Port Authority's right of eminent domain, saying that the project had a \\"public purpose.\\"[16][17] On November 12, 1963, the United States Supreme Court refused to accept the case.[18][19] Under the state law, the Port Authority was required to assist business owners in relocating, though many business owners regarded what the Port Authority offered as inadequate.[18][20] Questions continued while the World Trade Center was constructed, as to whether the Port Authority really ought to take on the project, described by some as a \\"mistaken social priority.\\"[21]\\r\\nBy 1964, by which time the intended scale of the Yamasaki designed scheme had been made public with plans for the twin 110-story towers, private real estate developers and members of the Real Estate Board of New York also expressed concerns about this much \\"subsidized\\" office space going on the open market, competing with the private sector when there was already a glut of vacancies.[5][22] An especially vocal critic was Lawrence A. Wien, owner of the Empire State Building, which would lose its title of tallest building in the world.[5][23] Wien organized a group of builders into a group called the \\"Committee for a Reasonable World Trade Center\\" to demand that the project be scaled down.[24]\\r\\nIn January 1964, the Port Authority inked a deal with the State of New York to locate government offices at the World Trade Center.[25] The Port Authority began signing commercial tenants in the spring and summer of 1964, including several banks.[26] In 1965, the Port Authority signed the United States Customs Service as a tenant.[27]\\r\\nA final obstacle for the Port Authority was getting approval from New York City Mayor John Lindsay and the New York City Council, who raised concerns about the limited extent that the Port Authority involved the city in the negotiations and deliberations. Negotiations between The City of New York and the Port Authority were centered on tax issues. A final agreement was made on August 3, 1966, that the Port Authority would make annual payments to the City, in lieu of taxes, for the portion of the World Trade Center leased to private tenants.[28] In subsequent years, the payments would rise as the real estate tax rate increased.[29]\\r\\nOn September 20, 1962, the Port Authority announced the selection of Minoru Yamasaki as lead architect, and Emery Roth & Sons as associate architects.[30] Originally, Yamasaki submitted to the Port Authority a concept incorporating twin towers, but with each building only 80 stories tall. Yamasaki remarked that the \\"obvious alternative, a group of several large buildings, would have looked like a housing project.\\"[31] Yamasaki's design for the World Trade Center was unveiled to the public on January 18, 1964, with an eight-foot model.[31] The towers had a square plan, approximately 207?feet (63?m) in dimension on each side.[32] The buildings were designed with narrow office windows, only 18?inches (45?cm) wide, which reflected on Yamasaki's fear of heights and desire to make building occupants feel secure.[33] Yamasaki's design called for the building facades to be sheathed in aluminum-alloy.[34]\\r\\nTo meet the Port Authority's requirement to build 10?million square feet (930,000?m2) of office space, the buildings would each need to be 110 stories tall. A major limiting factor in building heights is elevators; the taller the building, the more elevators are needed to service the building, requiring more space-consuming elevator banks.[35] Yamasaki and the engineers decided to use a new system that included sky lobbies, which are floors where people can switch from a large-capacity express elevator, which goes only to the sky lobbies, to a local elevator that goes to each floor in a section (the local elevators can be stacked within the same elevator shaft). Located on the 44th and 78th floors of each tower, the sky lobbies enabled the elevators to be used efficiently, while also increasing the amount of usable space on each floor from 62 to 75 percent by reducing the number of required elevator shafts.[36] The World Trade Center towers were the second supertall buildings to use sky lobbies, after the John Hancock Center in Chicago.[37] This system was inspired by the New York City Subway system, whose lines include local stations where local trains stop and express stations where all trains stop.[38]\\r\\nYamasaki, who had previously designed Saudi Arabia's Dhahran International Airport with the Saudi Binladin Group, incorporated features of Arabic architecture into the design of the World Trade Center. The plaza was modelled after Mecca, incorporating features such as a vast delineated square, a fountain, and a radial circular pattern. Yamasaki described the plaza as \\"a mecca, a great relief from the narrow streets and sidewalks of the Wall Street area.\\"[39] He also incorporated other features of Arabic architecture into the building design, including pointed arches, interweaving tracery of prefabricated concrete, a minaret like flight tower, and arabesque patterns.[40]\\r\\nThe World Trade Center design brought criticism of its aesthetics from the American Institute of Architects and other groups.[34][41] Lewis Mumford, author of The City in History and other works on urban planning, criticized the project and described it and other new skyscrapers as \\"just glass-and-metal filing cabinets.\\"[42] Television broadcasters raised concerns that the World Trade Center twin towers would cause interference in television reception for viewers in the New York City area.[43] In response to these concerns, the Port Authority offered to provide new television transmission facilities at the World Trade Center.[44] The Linnaean Society of the American Museum of Natural History also opposed the Trade Center project, citing hazards the buildings would impose on migrating birds.[45]\\r\\nThe structural engineering firm Worthington, Skilling, Helle & Jackson worked to implement Yamasaki's design, developing the tube-frame structural system used in the buildings. The Port Authority's Engineering Department served as foundation engineers, Joseph R. Loring & Associates as electrical engineers, and Jaros, Baum & Bolles (JB&B) as mechanical engineers. Tishman Realty & Construction Company was the general contractor on the World Trade Center project. Guy F. Tozzoli, director of the World Trade Department at the Port Authority, and the Port Authority's Chief Engineer, Rino M. Monti, oversaw the project.[46]\\r\\nAs an interstate agency, the Port Authority was not subject to local laws and regulations of the City of New York, including building codes. Nonetheless, the Port Authority required architects and structural engineers to follow the New York City building codes. At the time when the World Trade Center was planned, new building codes were being devised to replace the 1938 version that was still in place. The structural engineers ended up following draft versions of the new 1968 building codes, which incorporated \\"advanced techniques\\" in building design.[47]\\r\\nThe World Trade Center towers included many structural engineering innovations in skyscraper design and construction, which allowed the buildings to reach new heights and become the tallest in the world. Traditionally, skyscrapers used a skeleton of columns distributed throughout the interior to support building loads, with interior columns disrupting the floor space. The tube-frame concept, earlier introduced by Fazlur Khan, was a major innovation, allowing open floor plans and more space to rent. The buildings used high-strength, load-bearing perimeter steel columns called Vierendeel trusses that were spaced closely together to form a strong, rigid wall structure. There were 60 perimeter columns, narrowly spaced, on each side of the buildings. In all, the perimeter walls of the towers were 210 feet (64?m) on each side, and the corners were beveled. The perimeter columns were designed to provide support for virtually all lateral loads (such as wind loads) and to share the gravity loads with the core columns.[48] Structural analysis of major portions of the World Trade Center were computed on an IBM 1620.[49]\\r\\nThe perimeter structure was constructed with extensive use of prefabricated modular pieces, which consisted of three columns, three stories tall, connected by spandrel plates. The perimeter columns had a square cross section, 14?inches (36?cm) on a side, and were constructed of welded steel plate.[50] The thickness of the plates and grade of structural steel varied over the height of the tower, ranging from 36,000 to 100,000?pounds per square inch[51] (260 to 670?MPa). The strength of the steel and thickness of the steel plates decreased with height because they were required to support lesser amounts of building mass on higher floors.[50] The tube-frame design required 40 percent less structural steel than conventional building designs.[52] From the 7th floor to the ground level, and down to the foundation, the columns were spaced 10?feet (3?m) apart.[53] All columns were placed on bedrock, which, unlike that in Midtown Manhattan, where the bedrock is shallow, is at 65ÿ85?feet (20ÿ26?m) below the surface.[54]\\r\\nThe spandrel plates were welded to the columns to create the modular pieces off-site at the fabrication shop.[55] The modular pieces were typically 52?inches (1.3?m) deep, and extended for two full floors and half of two more floors.[50] Adjacent modules were bolted together, with the splices occurring at mid-span of the columns and spandrels. The spandrel plates were located at each floor, transmitting shear stress between columns, allowing them to work together in resisting lateral loads. The joints between modules were staggered vertically, so the column splices between adjacent modules were not at the same floor.[50]\\r\\nThe building's core housed the elevator and utility shafts, restrooms, three stairwells, and other support spaces. The core of each tower was a rectangular area 87 by 135?feet (27 by 41?m), and contained 47 steel columns running from the bedrock to the top of the tower.[50] The columns tapered after the 66th floor, and consisted of welded box-sections at lower floors and rolled wide-flange sections at upper floors. The structural core in 1 WTC was oriented with the long axis east to west, while that of 2 WTC was oriented north to south. All elevators were located in the core. Each building had three stairwells, also in the core, except on the mechanical floors where the two outside stairwells temporarily left the core in order to avoid the express elevator machine rooms, and then rejoined the core by means of a transfer corridor.[56] It was this arrangement that allowed Stairwell A of 2 WTC to remain passable after the aircraft impact on September 11, 2001.[57]\\r\\nThe large, column-free space between the perimeter and core was bridged by prefabricated floor trusses. The floors supported their own weight, as well as live loads, provided lateral stability to the exterior walls, and distributed wind loads among the exterior walls. The floors consisted of 4-inch (10?cm) thick lightweight concrete slabs laid on a fluted steel deck with shear connections for composite action.[50] A grid of lightweight bridging trusses and main trusses supported the floors. The trusses had a span of 60 feet (18?m) in the long-span areas and 35 feet (11?m) in the short span area.[50] The trusses connected to the perimeter at alternate columns, and were on 6-foot-8-inch (2.03?m) centers. The top chords of the trusses were bolted to seats welded to the spandrels on the exterior side and a channel welded to the core columns on the interior side. The floors were connected to the perimeter spandrel plates with viscoelastic dampers, which helped reduce the amount of sway felt by building occupants.[50]\\r\\nHat trusses (or \\"outrigger truss\\") located from the 107th floor to the top of the buildings were designed to support a tall communication antenna on top of each building.[50] Only 1 WTC (north tower) actually had an antenna fitted, which was added in 1978.[58] The truss system consisted of six trusses along the long axis of the core and four along the short axis. This truss system allowed some load redistribution between the perimeter and core columns and supported the transmission tower.\\r\\nThe tube frame design using steel core and perimeter columns protected with sprayed-on fire resistant material created a relatively lightweight structure that would sway more in response to the wind, compared to traditional structures such as the Empire State Building that have thick, heavy masonry for fireproofing of steel structural elements.[59] During the design process, wind tunnel tests were done at Colorado State University and at the National Physical Laboratory in the United Kingdom to establish design wind pressures that the World Trade Center towers could be subjected to and structural response to those forces.[60] Experiments were also done to evaluate how much sway occupants could tolerate. Subjects were recruited for \\"free eye exams,\\" while the real purpose of the experiment was to subject them to simulated building sway and find out how much they could comfortably tolerate.[61] Many subjects did not respond well, experiencing dizziness and other ill effects. One of the chief engineers Leslie Robertson worked with Canadian engineer Alan G. Davenport to develop viscoelastic dampers to absorb some of the sway. These viscoelastic dampers, used throughout the structures at the joints between floor trusses and perimeter columns, along with some other structural modifications reduced the building sway to an acceptable level.[62]\\r\\nThe structural engineers on the project also considered the possibility that an aircraft could crash into the building. In July 1945, a B-25 bomber that was lost in the fog had crashed into the 78th and 79th floors of the Empire State Building. A year later, another airplane crashed into the 40 Wall Street building, and there was another close call at the Empire State Building.[63] In designing the World Trade Center, Leslie Robertson considered the scenario of the impact of a jet airliner, the Boeing 707, which might be lost in the fog, seeking to land at JFK or at Newark airports.[64] The National Institute of Standards and Technology (NIST) found a three-page white paper that mentioned another aircraft impact analysis, involving impact of a jet at 600?mph (970?km/h), was indeed considered, but NIST could not locate the documentary evidence of the aircraft impact analysis.[65]\\r\\nSprayed-fire resistant materials (SFRMs) were used to protect some structural steel elements in the towers, including all floor trusses and beams.[56] Gypsum wallboard in combination with SFRMs, or in some cases gypsum wallboard alone, was used to protect core columns.[56] Vermiculite plaster was used on the interior-side and SFRMs on the other three sides of the perimeter columns for fire protection.[56] The 1968 New York City building codes were more lenient in some aspects of fire protection, such as allowing three exit stairwells in the World Trade Center towers, instead of six as required under older building codes.[66]\\r\\nIn April 1970, the New York City Department of Air Resources ordered contractors building the World Trade Center to stop the spraying of asbestos as an insulating material.[67]\\r\\nMore fireproofing was added after a fire in February 1975 that spread to six floors before being extinguished.[68] After the 1993 bombing, inspections found fireproofing to be deficient. The Port Authority was in the process of replacing it, but replacement had been completed on only 18 floors in WTC 1, including all the floors affected by the aircraft impact and fires on September 11,[69] and on 13 floors in WTC 2, although only three of these floors (77, 78, and 85) were directly affected by the aircraft impact.[70][71]\\r\\nThe 1968 New York City building codes did not require sprinklers for high-rise buildings, except for underground spaces. In accordance with building codes, sprinklers were originally installed only in the underground parking structures of the World Trade Center.[72] Following a major fire in February 1975, the Port Authority decided to start installing sprinklers throughout the buildings. By 1993, nearly all of 2 WTC and 85 percent of 1 WTC had sprinklers installed,[73] and the entire complex was retrofitted by 2001.[74]\\r\\nIn March 1965, the Port Authority began acquiring property at the World Trade Center site.[75] The Ajax Wrecking and Lumber Corporation was hired for the demolition work, which began on March 21, 1966 to clear the site for construction of the World Trade Center.[76]\\r\\nGroundbreaking was on August 5, 1966, marking the beginning of construction of the World Trade Center's foundations.[77] The site of the World Trade Center was located on landfill, with the bedrock located 65 feet (20?m) below grade.[78] In order to construct the World Trade Center, it was necessary to build \\"The Bathtub\\", with the slurry wall along the West Street side of the site, to keep water from the Hudson River out. This method was used in place of conventional dewatering methods because lowering the groundwater table would cause large settlements of nearby buildings not built on deep foundations.[79] The slurry method involves digging a trench, and as excavation proceeds, filling the space with a \\"slurry\\" mixture, composed of bentonite which plugs holes and keeps water out. When the trench was dug out, a steel cage was inserted, with concrete poured in, forcing the \\"slurry\\" out. The \\"slurry\\" method was devised by Port Authority chief engineer John M. Kyle Jr. Towards the end of 1966, work began on building the slurry wall, led by Montreal-based Icanda, a subsidiary of an Italian engineering firm, Impresa Costruzioni Opere Specializzate (I.C.O.S.).[80] It took fourteen months for the slurry wall to be completed, which was necessary before excavation of material from the interior of the site could begin.[80] The original Hudson Tubes, which carried PATH trains into Hudson Terminal, remained in service as elevated tunnels until 1971 when a new PATH station was built.[81]\\r\\nConstruction work began on the North Tower in August 1968 with construction beginning on the South Tower by January 1969.[82] In January 1967, $74 million in contracts were awarded to the Pacific Car and Foundry Company, Laclede Steel Company, Granite City Steel Company, and Karl Koch Erecting Company to supply steel for the project.[83] The Port Authority chose to use many different steel suppliers, bidding on smaller portions of steel, rather than buy larger amounts from a single source such as Bethlehem Steel or U.S. Steel as a cost-saving measure.[84] Karl Koch was also hired to do all the work of erecting the steel, and a contract for work on the aluminum facade was awarded to the Aluminum Company of America.[83] Tishman Realty & Construction was hired in February 1967 to oversee construction of the project.[85]\\r\\nExtensive use of prefabricated parts for the perimeter framing and floor truss systems helped speed up the construction process and reduce costs, while providing greater quality control.[52] Steel components were freighted into a Penn Central yard in Jersey City. From there, they were brought in early morning hours through the Holland Tunnel to the construction site, and lifted into place by a crane.[86] Larger pieces were brought to the construction site by tugboats.[87] A special type of crane, suitable for constructing such tall buildings, that used hydraulics to lift components and provided its own power was used in construction of the World Trade Center. The Favco Standard 2700 Crane, manufactured by Favelle Mort Ltd. of New South Wales, Australia was informally called a \\"kangaroo crane.\\"[88]\\r\\nIn 1970, tugboat workers went on strike, halting the transport of material to the construction site.[89] The Port Authority attempted other means of transporting material, including via helicopter. When this method was tried, the helicopter lost its load of steel into the Kill Van Kull.[90] Some other mishaps occurred during the construction process, including disruption of telephone service in Lower Manhattan when telephone cables were crushed by pile drivers.[91] On March 16, 1970, an explosion injured six workers when a truck hit a propane tank.[92] In all, 60 workers were killed in construction accidents while the World Trade Center was being built.[93]\\r\\nThe topping out ceremony of 1 WTC (North Tower) took place on December 23, 1970, with 2 WTC's ceremony (South Tower) occurring later on July 19, 1971.[82] The first tenants moved into the North Tower on December 15, 1970,[94] and into the South Tower in January 1972.[95] The buildings were dedicated on April 4, 1973; Tobin, who had resigned the year before, was absent from the ceremonies.[96]\\r\\nBuilding the World Trade Center involved excavating 1,200,000 cubic yards (920,000?m3) of material.[97] Rather than transporting this material at great costs out to sea or to landfills in New Jersey, the fill material was used to expand the Manhattan shoreline across West Street.[97] Work to demolish the piers began on January 5, 1967, including Pier 7 to Pier 11 which were all constructed around 1910.[98] The demolition work moved forward, despite conflicts between David Rockefeller, Governor Nelson Rockefeller, and Mayor John Lindsay regarding plans for Battery Park City.[99] Landfill material from the World Trade Center was used to add land, and a cellular cofferdam was constructed to retain the material.[78] The result was a 700-foot (210?m) extension into the Hudson River, running six blocks or 1,484 feet (452?m).[97] This land was a \\"gift\\" to New York City, allowing more tax-generating developments in Battery Park City.[100]\\r\\nThe original estimates put forth by the Port Authority had the costs for construction of the World Trade Center at $350 millionan optimistic figure.[101] In December 1966, the Port Authority announced increased cost estimates, bringing the estimated total to $575 million.[102] This announcement brought criticism of the project from private real estate developers, The New York Times, and others in New York City.[103] The critics charged that the Port Authority figure was an unrealistically low estimate, and they estimated the project would end up costing $750 million.[104] When the World Trade Center twin towers were completed, the total costs to the Port Authority had reached $900 million.[105] The project was financed through tax-exempt bonds issued by the Port Authority.[106]\\r\\nThe World Trade Center complex included four other smaller buildings constructed during the 1970s. 3 World Trade Center was a 22-story building, which was home to the Marriott World Trade Center. It was designed by Skidmore, Owings and Merrill in 1978ÿ79.[107] 4 World Trade Center, 5 World Trade Center, and 6 World Trade Center were all 8ÿ9 story buildings that were designed by the same team as the Twin Towers, including Minoru Yamasaki; Emery Roth & Sons; and Skilling, Helle, Christiansen, Robertson.[108] 7 World Trade Center was built in the mid-1980s, just north of the main World Trade Center site. The 47-story building was designed by Emery, Roth & Sons, and constructed on top of a Con Edison power substation.[109]\\r\\nOver time, numerous structural modifications were made to suit the needs of tenants in the Twin Towers. Modifications were made in accordance with the Port Authority's Tenant Alteration Review Manual and were reviewed by the Port Authority to ensure the changes did not compromise structural integrity of the buildings. In many instances, openings were cut in the floors to accommodate new stairways to connect tenant floors. Some steel beams in the core were reinforced and strengthened to accommodate heavy live loads, such as large amounts of heavy files that tenants had on their floors.[110]\\r\\nRepairs to structural elements on the lower levels of 1 WTC were made following the 1993 bombing. The greatest damage occurred on levels B1 and B2, with significant structural damage also on level B3.[111] Primary structural columns were not damaged, but secondary steel members experienced some damage.[112] Floors that were blown out needed to be repaired to restore the structural support they provided to columns.[113] The slurry wall was in peril following the bombing and loss of the floor slabs which provided lateral support to counteract pressure from Hudson River water on the other side.[114] The refrigeration plant on sublevel B5, which provided air conditioning to the entire World Trade Center complex, was heavily damaged and replaced with a temporary system for the summer of 1993.[114] The fire alarm system for the entire complex needed to be replaced, after critical wiring and signaling in the original system was destroyed in the 1993 bombing. Installation of the new system took years to complete, and replacement of some components was still underway in September 2001.[115]\\r\\nCoordinates: 404242N 740049W? / ?40.71167N 74.01361W? / 40.71167; -74.01361","input":"When was the first world trade center built?"},{"output":"April 14, 2015","context":"Handwritten is the debut studio album by Canadian singer Shawn Mendes, and was released on April 14, 2015 by Island. It debuted at number one on the US Billboard 200 with first-week sales of 119,000 album-equivalent units, of which 106,000 copies were pure album sales. The album includes \\"Stitches\\", which reached top 5 on the US Billboard Hot 100 and number one on the UK Singles Chart for 2 weeks, while its reissue features the US top 20 single \\"I Know What You Did Last Summer\\".\\r\\n\\r\\nAfter signing a record deal with Island Records in June 2014, Mendes released his debut single \\"Life of the Party\\" which peaked at number 24 on the US Billboard Hot 100.[2] Following the single release, an EP titled The Shawn Mendes EP was released on July 28, 2014, and sold 48,000 copies in its first week. Mendes announced his debut album title and artwork on January 27, 2015 and it was made available to pre-order on February 2, 2015.\\r\\n\\r\\nHandwritten was reissued in a revisited edition on November 20, 2015. It includes five live recordings tracks from the Greek Theatre and four brand new songs.[3]\\r\\n\\r\\n\\"Life of the Party\\", was released as the lead single from Mendes debut EP The Shawn Mendes EP on 25 June 2014. The song serves as the first single from Handwritten as well. A one-shot lyric video for the song showing Mendes in the George Street diner premiered on Vevo on June 30, 2014. The official music video for the song premiered on March 10, 2015.\\r\\n\\r\\n\\"Something Big\\", was released as the album's second single on November 7, 2014. The official music video for the song premiered on November 11, 2014 on Vevo, and it is Mendes' first official music video.\\r\\n\\r\\n\\"Stitches\\", was released as the album's third and final single on May 5, 2015.[4] The song debuted on the Billboard Hot 100 chart of June 13, 2015, at number 89 and became his first top 10, peaking at number 4. Peaking at No. 1 in the UK Singles Chart being Mendes' only number 1 in the UK to date.\\r\\n\\r\\n\\"I Know What You Did Last Summer\\", was released as a single from a revisited edition of the album on November 18, 2015. The song is a collaboration with Cuban-American singer Camila Cabello. It has peaked at number 20 on the Billboard Hot 100 chart.[5] In the UK the song peaked at number 42. \\r\\n\\r\\nBefore the album release, Mendes released five songs through pre-order on iTunes. The first song \\"A Little Too Much\\", was released on February 2, the same day the album pre-order started. A music video for the song premiered on the singer's Vevo on February 4. Mendes announced through Instagram that the second promotional song would be \\"Never Be Alone\\" and it was made available on February 16. An official music video for the song was released on February 25. On March 16 the singer released \\"Stitches\\", the third pre-order single, which official music video premiered on March 18. An acoustic version of \\"Life of the Party\\", featured in the deluxe version of the album, was released as the fourth pre-order single on March 30 at midnight, the same day as it was made possible to hear every song preview on iTunes.[6] \\"Kid in Love\\", was made available as the fifth and final pre-order track on April 6 at midnight,[7] just a week before the album release, while a music video for the acoustic version of \\"Life of the Party\\", was posted on Shawn Mendes' Vevo on April 10. \\"Aftertaste\\" also gained a music video, which was posted on April 17.\\r\\n\\r\\nThe album debuted at number one on the US Billboard 200, selling 119,000 album-equivalent units in its first week, of which 106,000 copies were pure album sales. He became the youngest artist since Justin Bieber to have a No. 1 album on the Billboard 200 album chart in nearly five years. Bieber was 16 years and 2 months old when his album My World 2.0 spent its fourth and final week at the top. Mendes was 16 years and 8 months old.[8] In its second week, the album descended to number seventeen with 20,000 equivalent album units, of which 12,000 copies were pure album sales, an 89% pure album sales decrease (from 106,000 copies), surpassing Madonna's MDNA (2012) for the biggest second-week percentage drop for a number-one debuting album in the Nielsen SoundScan era.[9][10] As of August 2016, Handwritten has sold 391,000 copies in the United States.[11]\\r\\n\\r\\nIn his native Canada, the album debuted at number one with first-week sales of 14,000 copies, according to Nielsen SoundScan.[12] In its second week, the album descended to number six.[13]\\r\\n\\r\\nHandwritten has received mixed reviews from music critics. At Metacritic, which assigns a \\"weighted average\\" rating out of 100 from selected independent ratings and reviews from mainstream critics, the album received a Metascore of 58, based on 5 reviews.[24]\\r\\n\\r\\nBillboard's editor Carl Wilson stated \\"confident hooks and big dynamics make the syrup go down smoothly\\", talking about the singles \\"Life of the Party\\" and \\"Stitches,\\" and the blue-eyed-soul track \\"I Don't Even Know Your Name\\", noting the latter influenced by Justin Timberlake and Bruno Mars.[16] Writing for The Guardian, Caroline Sullivan rated the album three-out-of-five stars and claims: \\"At 16, Mendes isnt the artist hell be at 25, but hes made a persuasive start ÿ the adult-pop big league could yet be his.\\"[18] Nick Murray from Rolling Stone described Mendes as a \\"nice-guy guitar strummer more influenced by the light acoustic pop of Ed Sheeran.\\"[22]\\r\\n\\r\\nCredits for Handwritten adapted from AllMusic.[29]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n*sales figures based on certification alone^shipments figures based on certification alone","input":"When did shawn mendes first album come out?"},{"output":"Mir Ghazanfar Ali Khan","context":"The Governor of Gilgit-Baltistan is the appointed Head of State of the provincial government in Gilgit-Baltistan, Pakistan. The governor is designated by the Prime Minister and is normally regarded a ceremonial post. However, throughout the history of Pakistan, the powers of the provincial governors were vastly increased, every time the provincial assemblies were dissolved and the administrative role came under direct control of the governors.\\r\\nThe seat of governor in Gilgit-Baltistan was started on 16 September 2009.\\r\\nThe current Governor is Mir Ghazanfar Ali Khan, who took over on 20 November 2015.\\r\\nFollowing is the list of governors after Gilgit-Baltistan was given the status of province on August 29, 2009, the federal cabinet had approved the Gilgit-Baltistan Empowerment and Self-Governance Order 2009.","input":"Who is the current governor of gilgit baltistan?"},{"output":"made a significant impact within their community","context":"Ghigau (Cherokee: ???) or Agigaue (Cherokee:?????) is a Cherokee prestigious title meaning \\"beloved woman\\" or \\"war woman\\".[1][2]\\r\\nThe title was a recognition of great honor for women who made a significant impact within their community or exhibited great heroism on the battlefield. When a woman was bestowed as a Ghigau she was given great honor and responsibility. The role has changed in Cherokee culture, but the Eastern Band of Cherokee Indians still have Beloved Women today.\\r\\n\\r\\n\\r\\nThe Ghigua title was given to extraordinary women by the Cherokee clans, and the title of great honor and responsibility was held for life. The Cherokees believed that the Great Spirit frequently spoke through the Ghigau. The Ghigau headed the Council of Women and held a voting seat in the Council of Chiefs. She was given the responsibility of prisoners and would decide their fate.[1][2]\\r\\nThere are other similar words. For instance, the word Adageyudi (Cherokee:?????) means \\"beloved\\"[3] or \\"beloved woman\\".[4] An even greater title would have been Chigau (Cherokee: ???) meaning \\"greatly beloved woman\\".[2]\\r\\nNancy Ward, whose Cherokee name was Nanyehi, was a notable Ghigau who was born in the Cherokee town of Chota thought to be the daughter of a Cherokee woman named Tame Doe of the Wolf Clan. Tame Doe's brother was Attakullakulla. [5]\\r\\nIn 1755, the Cherokee fought against the Muscogee Creeks. During the battle, Nanyehi's first husband of four years, Kingfisher, was killed. She was just 18 at the time, and victoriously led and fought in the battle against the Creeks. Her bravery and leadership resulted in her being bestowed with the title of Ghigau.[6][7]\\r\\nNanyehi became aware of a planned attack against the white colonists during the Revolutionary War by Dragging Canoe, her cousin. She warned the colonists of the upcoming battle, which resulted in her being identified as a patriot for the Society of the Sons of the American Revolution and the Daughters of the American Revolution.[6][7]\\r\\nOn September 11, 1808, in council Broom's town, the ancient law of blood revenge was abolished by the Cherokee national government. The Cherokee, once ruled by clan loyalty, were moving toward a republican form of government. There was no longer a place in Cherokee government for a Ghigau. Ward was named Beloved Woman and bestowed with the honor and responsibility of being a Ghigau.[6][7]\\r\\nThe Seneca myth speaks of two Cherokee Ghigau deciding the fate of a Seneca man in \\"A warrior cared for by wolves\\". \\"Among the Cherokees there were two women who were looked upon as the head women of the tribe. Each woman had two snakes tattooed on her lips--the upper jaws of the snakes were on the woman's upper lip, and opposite each other, the lower jaws on the lower lip in the same way. When the woman opened her mouth, the snakes seemed to open theirs. These women said, \\"This is the way to torment him; tie him near a fire and burn the soles of his feet till they are blistered, then let the water out of the blisters, put kernels of corn inside the skin, and chase him with clubs till he dies.\\"[8]","input":"What role did the beloved woman have in the cherokee tribe?"},{"output":"Brian McGuire Cashman","context":"Brian McGuire Cashman (born July 3, 1967) is an American baseball executive for the New York Yankees of Major League Baseball. He has served as the General Manager and Senior Vice President of the Yankees since 1998. During Cashman's tenure as general manager, the Yankees have won six American League pennants and four World Series championships.\\r\\nCashman began working with the Yankees organization in 1986 as an intern while still in college. He was named assistant general manager in 1992, helping to run the team while owner George Steinbrenner was suspended from baseball. He succeeded Bob Watson as the team's general manager in 1998.\\r\\n\\r\\n\\r\\nCashman was born in Rockville Centre, New York, and raised in Washingtonville, New York. He was raised in an Irish Catholic family,[1][2] as the middle of five children born to Nancy and John Cashman.[3] He became a baseball fan at a young age, attending a summer camp hosted by Bucky Dent before starting high school. He grew up as a fan of the Los Angeles Dodgers.[4] While visiting his grandmother in Florida, he served as a batboy for the Dodgers during spring training in 1982, with the help of former Dodger Ralph Branca, a family friend.[2][5]\\r\\nThe Cashman family moved to Lexington, Kentucky, where his father managed Castleton Farm, raising standardbreds for harness racing.[2] Cashman described moving out of Washingtonville before starting high school as \\"the best thing to happen to [him], to get out of there.\\"[6]\\r\\nCashman attended Lexington Catholic High School before moving to the Washington, D.C., metropolitan area. He attended Georgetown Preparatory School in North Bethesda, Maryland, graduating in 1985. Cashman played baseball and junior varsity basketball at both schools, and added football in his senior year.[7] Brian was classmates with Supreme Court Justice Neil Gorsuch[8]\\r\\nThe Catholic University of America offered Cashman the opportunity to play college baseball for the Catholic Cardinals, competing in the National Collegiate Athletic Association's Division III, guaranteeing him playing time as a freshman.[3] He was a four-year starter at second base and the team's leadoff hitter. He set a school record for most hits in a season, which has since been broken. He earned a bachelor's degree with a major in history in 1989.[3][2]\\r\\nGeorge Steinbrenner, the owner of the New York Yankees, met John Cashman when he managed Pompano Park in Pompano Beach, Florida, and the two became friends.[3] Through another family friend, John helped Brian obtain a position with the Yankees organization as an intern in 1986.[9][10] He worked in the minor league scouting department in the day, and worked security at night.[2] After Cashman graduated from Catholic, the Yankees offered him a position as a baseball operations assistant, which he accepted.[3][9]\\r\\nSteinbrenner was banned from baseball in July 1990 for hiring a gambler to investigate Dave Winfield. Gene Michael, then the Yankees' General Manager, took over daily operations of the Yankees, and Cashman played a role in assisting him.[3] He was promoted to assistant farm director that year,[9] and to major league administrator in 1991.[11] Michael named Cashman an Assistant General Manager in 1992.[3] He remained in the role after Bob Watson succeeded Michael as general manager in 1995.[12] The Yankees won the 1996 World Series.[13]\\r\\nIn February 1998, Watson resigned from the Yankees, and Cashman was named Senior Vice-President and General Manager.[11] He agreed to a one-year contract for $300,000, and became the second-youngest general manager in MLB history.[3][14][a] The Yankees won 114 games during the 1998 season, and won the 1998 World Series. In 1999, Cashman traded fan favorite David Wells to the Toronto Blue Jays to acquire Roger Clemens. The next year, he acquired David Justice, who won the American League Championship Series (ALCS) Most Valuable Player Award for his play in the 2000 ALCS. The Yankees won the 2000 World Series, making Cashman the first General Manager to win World Series titles in his first three years. In 2004, Cashman arranged the trade of Alfonso Soriano for Alex Rodriguez.[3]\\r\\nDespite the team's success, Cashman considered leaving the Yankees in 2005 due to conflicts with Steinbrenner and organizational disputes between team officials in New York City and Tampa, Florida.[15] The Washington Nationals were rumored to be interested in hiring Cashman, which would have brought him back to the city where he attended school. Instead, Cashman agreed to a new contract with the Yankees following the conclusion of the 2005 season which gave him more authority in personnel decisions and paid him an average of $1.3 million more over the following three years.[16]\\r\\nWith the increased authority, Cashman created a department of professional scouting, and tabbed Billy Eppler as its director.[17] Later, Eppler would move on to become the General Manager of the Los Angeles Angels of Anaheim.[18] On September 30, 2008, Cashman signed a three-year contract to stay with the Yankees through the 2011 season.[19] Following the 2008 season, when the Yankees failed to make the playoffs, Cashman signed CC Sabathia, A. J. Burnett, and Mark Teixeira to long-term free agent contracts and traded for Nick Swisher. These four players played a significant role in the 2009 Yankees season,[20] culminating with a victory in the 2009 World Series.\\r\\nThe Yankees went on to make the playoffs again following the 2010 season, but lost to the Texas Rangers in the 2010 American League Championship Series. Following the 2010 season, Cashman held a hard line with Derek Jeter during contract negotiations, reportedly telling Jeter that he would prefer to have Troy Tulowitzki as the Yankees' starting shortstop,[21] though a deal was eventually made for three years and $45 million.\\r\\nYankees ownership agreed to sign Rafael Soriano in January 2011 without Cashman's approval. Cashman stated at Soriano's introductory press conference that he disagreed with the deal.[22] The Yankees re-signed Cashman to a three-year contract in November 2011.[23]\\r\\nDuring 2013, Alex Rodriguez composed a tweet saying that he had been cleared to play by his doctors after his hip surgery. Cashman claimed that the doctors did not give such authority to clear Rodriguez to play after seeking a second opinion with them, and that Rodriguez should \\"shut the fuck up\\".[3][24] Cashman wanted to trade Robinson Can܇ during the 2013 season, reasoning that they would be unable to resign him in the next offseason. Ownership prevented Cashman from exploring a trade.[25]\\r\\nAfter the 2013 season, the Yankees signed Masahiro Tanaka, Jacoby Ellsbury, Brian McCann, and Carlos Beltrn to contracts that totaled $438?million. However, the Yankees missed the playoffs for the second consecutive year. On October 10, 2014, the Yankees signed Cashman to another three-year deal through the 2017 season.[26] That offseason, Cashman prioritized restructuring the Yankees roster with younger players. He replaced the retired Jeter with Didi Gregorius and acquired Nathan Eovaldi,[27] both of whom improved during the season.[28] In the 2016 season, he traded Carlos Beltrn to the Texas Rangers, Andrew Miller to the Cleveland Indians, and Aroldis Chapman to the eventual World Series Champion Chicago Cubs to bolster the Yankees farm system.\\r\\nIn 2017, the Yankees youth movement carried them to the postseason again, with rookie outfielder Aaron Judge and catcher Gary Sanchez helping a young Yankees team reach the postseason once more. The Yankees defeated the Minnesota Twins in the 2017 American League Wild Card Game, and then went on to defeat the Cleveland Indians in the 2017 American League Division Series. Making their first appearance in the American League Championship Series since 2012, the Yankees lost to the Houston Astros in seven games, far outpacing the expectations of many analysts.\\r\\nCashman was named to Crain's New York Business 40 under 40 list for 1999.[29] The Boston chapter of the Baseball Writers' Association of America selected Cashman as their MLB Executive of the Year for 2009.[30] In 2010, Cashman was inducted into the Irish American Baseball Hall of Fame.[31]\\r\\nCashman was also involved in the developing of the video game MLB Front Office Manager.[32]\\r\\nCashman lives in Darien, Connecticut.[33] He and his wife, Mary, had two children, Grace and Theodore.[34] Mary filed for divorce in February 2012; they had been reportedly separated for a year. The day prior, prosecutors charged a woman with stalking Cashman in an attempt to extort money regarding an extramarital affair.[35] Cashman is a Kentucky Wildcats and New Jersey Devils fan.[7][36]\\r\\nCashman has referred to himself as an \\"adrenaline junkie\\".[3] In December 2010, Cashman rappelled from a 350-foot (110?m) building in Stamford, Connecticut, as part of an annual Stamford Christmas celebration.[37] He jumped from an airplane with members of the United States Army Parachute Team to raise awareness for the Wounded Warrior Project, and broke his right fibula and dislocated his right ankle in the process.[38] In November 2014, Cashman slept on a New York City sidewalk to raise awareness on behalf of homeless youth.[39]","input":"Who is the general manager of the yankees?"},{"output":"by the start of WWI","context":"","input":"When were the first trenches dug in ww1?"},{"output":"the 2nd century","context":"Original sin, also called ancestral sin,[1] is the Christian doctrine of humanity's state of sin resulting from the fall of man, stemming from Adam and Eve's rebellion in Eden, namely the sin of disobedience in consuming from the tree of knowledge of good and evil.[2] This condition has been characterized in many ways, ranging from something as insignificant as a slight deficiency, or a tendency toward sin yet without collective guilt, referred to as a \\"sin nature\\", to something as drastic as total depravity or automatic guilt of all humans through collective guilt.[3]\\r\\nThe concept of original sin was first alluded to in the 2nd century by Irenaeus, Bishop of Lyon in his controversy with certain dualist Gnostics. Other church fathers such as Augustine also developed the doctrine,[2] seeing it as based on the New Testament teaching of Paul the Apostle (Romans 5:12ÿ21 and 1 Corinthians 15:22) and the Old Testament verse of Psalms 51:5.[4][5][6][7][8] Tertullian, Cyprian, Ambrose and Ambrosiaster considered that humanity shares in Adam's sin, transmitted by human generation. Augustine's formulation of original sin was popular among Protestant reformers, such as Martin Luther and John Calvin, who equated original sin with concupiscence (or \\"hurtful desire\\"), affirming that it persisted even after baptism and completely destroyed freedom.[2] The Jansenist movement, which the Catholic Church declared to be heretical, also maintained that original sin destroyed freedom of will.[9]\\r\\n\\r\\n\\r\\nThe doctrine of ancestral fault (? ??϶ϫ progonikon hamartema), i.e. the sins of the forefathers leading to punishment of their descendants, was presented as a tradition of immemorial antiquity in ancient Greek religion by Celsus in his True Doctrine, a polemic attacking Christianity. Celsus is quoted as attributing to \\"a priest of Apollo or of Zeus\\" the saying that \\"the mills of the gods grind slowly, even to children's children, and to those who are born after them\\".[10] The idea of divine justice taking the form of collective punishment is also ubiquitous in the Hebrew Bible.[11]\\r\\nSt Paul's idea of redemption hinged upon the contrast between the sin of Adam and the death and resurrection of Jesus. \\"Therefore, just as sin entered the world through one man, and death through sin, and in this way death came to all people, because all sinned.\\"[12] \\"For as in Adam all die, so in Christ all will be made alive.\\"[13] Up till then the transgression in the Garden of Eden had not been given great significance. As the Jesus scholar, Geza Vermes has said:\\r\\nPaul believed that Adam's transgression in a mysterious way affected the nature of the human race. The primeval sin, a Pauline creation with no biblical or post-biblical Jewish precedent, was irreparable by ordinary human effort.[14]\\r\\nThe formalized Christian doctrine of original sin was first developed in the 2nd century by Irenaeus, the Bishop of Lyon, in his struggle against Gnosticism.[2] Irenaeus contrasted their doctrine with the view that the Fall was a step in the wrong direction by Adam, with whom, Irenaeus believed, his descendants had some solidarity or identity.[15] Irenaeus believed that Adam's sin had grave consequences for humanity, that it is the source of human sinfulness, mortality and enslavement to sin, and that all human beings participate in his sin and share his guilt.[16]\\r\\nThe Greek Fathers emphasized the cosmic dimension of the Fall, namely that since Adam human beings are born into a fallen world, but held fast to belief that man, though fallen, is free.[2] They thus did not teach that human beings are deprived of free will and involved in total depravity, which is one understanding of original sin.[17][18] During this period the doctrines of human depravity and the inherently sinful nature of human flesh were taught by Gnostics, and orthodox Christian writers took great pains to counter them.[19][20] Christian apologists insisted that God's future judgment of humanity implied humanity must have the ability to live righteously.[21][22]\\r\\nHistorian Robin Lane Fox argues that the foundation of the doctrine of original sin as accepted by the Church was ultimately based on a mistranslation of Paul the Apostle's Epistle to the Romans (Romans 5:12ÿ21) by Augustine, in his On the Grace of Christ, and on Original Sin\\".[23]\\r\\nAugustine of Hippo (354ÿ430) taught that Adam's sin[24] is transmitted by concupiscence, or \\"hurtful desire\\",[25][26] resulting in humanity becoming a massa damnata (mass of perdition, condemned crowd), with much enfeebled, though not destroyed, freedom of will.[2] When Adam sinned, human nature was thenceforth transformed. Adam and Eve, via sexual reproduction, recreated human nature. Their descendants now live in sin, in the form of concupiscence, a term Augustine used in a metaphysical, not a psychological sense.[27] Augustine insisted that concupiscence was not a being but a bad quality, the privation of good or a wound.[28] He admitted that sexual concupiscence (libido) might have been present in the perfect human nature in paradise, and that only later it became disobedient to human will as a result of the first couple's disobedience to God's will in the original sin.[29] In Augustine's view (termed \\"Realism\\"), all of humanity was really present in Adam when he sinned, and therefore all have sinned. Original sin, according to Augustine, consists of the guilt of Adam which all humans inherit. As sinners, humans are utterly depraved in nature, lack the freedom to do good, and cannot respond to the will of God without divine grace. Justo Gonzalez interprets Augustine's teaching that grace is irresistible, results in conversion, and leads to perseverance.[30]\\r\\nAugustine articulated his explanation in reaction to Pelagianism, which insisted that humans have of themselves, without the necessary help of God's grace, the ability to lead a morally good life, and thus denied both the importance of baptism and the teaching that God is the giver of all that is good. Pelagius claimed that the influence of Adam on other humans was merely that of bad example. Augustine held that the effects of Adam's sin are transmitted to his descendants not by example but by the very fact of generation from that ancestor. A wounded nature comes to the soul and body of the new person from his/her parents, who experience libido (or concupiscence). Augustine's view was that human procreation was the way the transmission was being effected. He did not blame, however, the sexual passion itself, but the spiritual concupiscence present in human nature, soul and body, even after baptismal regeneration.[31] Christian parents transmit their wounded nature to children, because they give them birth, not the \\"re-birth\\".[32] Augustine used Ciceronian Stoic concept of passions, to interpret St. Paul's doctrine of universal sin and redemption. In that view, also sexual desire itself as well as other bodily passions were consequence of the original sin, in which pure affections were wounded by vice and became disobedient to human reason and will. As long as they carry a threat to the dominion of reason over the soul they constitute moral evil, but since they do not presuppose consent, one cannot call them sins. Humanity will be liberated from passions, and pure affections will be restored only when all sin has been washed away and ended, that is in the resurrection of the dead.[33][34]\\r\\nAugustine believed that the only definitive destinations of souls are heaven and hell. He concluded that unbaptized infants go to hell as a consequence of original sin.[35][36] The Latin Church Fathers who followed Augustine adopted his position, which became a point of reference for Latin theologians in the Middle Ages.[37] In the later medieval period, some theologians continued to hold Augustine's view, others held that unbaptized infants suffered no pain at all: unaware of being deprived of the beatific vision, they enjoyed a state of natural, not supernatural happiness. Starting around 1300, unbaptized infants were often said to inhabit the \\"limbo of infants\\".[38] The Catechism of the Catholic Church, 1261 declares: \\"As regards children who have died without Baptism, the Church can only entrust them to the mercy of God, as she does in her funeral rites for them. Indeed, the great mercy of God who desires that all men should be saved, and Jesus' tenderness toward children which caused him to say: 'Let the children come to me, do not hinder them',[39] allow us to hope that there is a way of salvation for children who have died without Baptism. All the more urgent is the Church's call not to prevent little children coming to Christ through the gift of holy Baptism.\\" But the theory of Limbo, while it \\"never entered into the dogmatic definitions of the Magisterium ... remains ... a possible theological hypothesis\\".[40]\\r\\nIn the works of John Cassian (c. 360 ÿ 435), Conference XIII recounts how the wise monk Chaeremon, of whom he is writing, responded to puzzlement caused by his own statement that \\"man even though he strive with all his might for a good result, yet cannot become master of what is good unless he has acquired it simply by the gift of Divine bounty and not by the efforts of his own toil\\" (chapter 1). In chapter 11, Cassian presents Chaeremon as speaking of the cases of Paul the persecutor and Matthew the publican as difficulties for those who say \\"the beginning of free will is in our own power\\", and the cases of Zaccheus and the good thief on the cross as difficulties for those who say \\"the beginning of our free will is always due to the inspiration of the grace of God\\", and as concluding: \\"These two then; viz., the grace of God and free will seem opposed to each other, but really are in harmony, and we gather from the system of goodness that we ought to have both alike, lest if we withdraw one of them from man, we may seem to have broken the rule of the Church's faith: for when God sees us inclined to will what is good, He meets, guides, and strengthens us: for 'At the voice of thy cry, as soon as He shall hear, He will answer thee'; and: 'Call upon Me', He says, 'in the day of tribulation and I will deliver thee, and thou shalt glorify Me'. And again, if He finds that we are unwilling or have grown cold, He stirs our hearts with salutary exhortations, by which a good will is either renewed or formed in us.\\"[41]\\r\\nCassian did not accept the idea of total depravity, on which Martin Luther was to insist.[42] He taught that human nature is fallen or depraved, but not totally. Augustine Casiday states that, at the same time, Cassian \\"baldly asserts that God's grace, not human free will, is responsible for 'everything which pertains to salvation' ÿ even faith\\".[43] Cassian pointed out that people still have moral freedom and one has the option to choose to follow God. Colm Luibhid says that, according to Cassian, there are cases where the soul makes the first little turn,[44] but in Cassian's view, according to Casiday, any sparks of goodwill that may exist, not directly caused by God, are totally inadequate and only direct divine intervention ensures spiritual progress;[45] and Lauren Pristas says that \\"for Cassian, salvation is, from beginning to end, the effect of God's grace\\".[46]\\r\\nOpposition to Augustine's ideas about original sin, which he had developed in reaction to Pelagianism, arose rapidly.[47] After a long and bitter struggle the general principles of Augustine's teaching were confirmed within Western Christianity by many councils, especially the Second Council of Orange in 529.[2] However, while the Church condemned Pelagius, it did not endorse Augustine entirely[48] and, while Augustine's authority was accepted, he was interpreted in the light of writers such as Cassian.[49] Some of the followers of Augustine identified original sin with concupiscence[50] in the psychological sense, but this identification was challenged by the 11th-century Saint Anselm of Canterbury, who defined original sin as \\"privation of the righteousness that every man ought to possess\\", thus separating it from concupiscence. In the 12th century the identification of original sin with concupiscence was supported by Peter Lombard and others,[citation needed] but was rejected by the leading theologians in the next century, chief of whom was Thomas Aquinas. He distinguished the supernatural gifts of Adam before the Fall from what was merely natural, and said that it was the former that were lost, privileges that enabled man to keep his inferior powers in submission to reason and directed to his supernatural end. Even after the fall, man thus kept his natural abilities of reason, will and passions. Rigorous Augustine-inspired views persisted among the Franciscans, though the most prominent Franciscan theologians, such as Duns Scotus and William of Ockham, eliminated the element of concupiscence.\\r\\nMartin Luther (1483ÿ1546) asserted that humans inherit Adamic guilt and are in a state of sin from the moment of conception. The second article in Lutheranism's Augsburg Confession presents its doctrine of original sin in summary form:\\r\\nIt is also taught among us that since the fall of Adam all men who are born according to the course of nature are conceived and born in sin. That is, all men are full of evil lust and inclinations from their mothers' wombs and are unable by nature to have true fear of God and true faith in God. Moreover, this inborn sickness and hereditary sin is truly sin and condemns to the eternal wrath of God all those who are not born again through Baptism and the Holy Spirit. Rejected in this connection are the Pelagians and others who deny that original sin is sin, for they hold that natural man is made righteous by his own powers, thus disparaging the sufferings and merit of Christ.[51]\\r\\nLuther, however, also agreed with the Roman Catholic doctrine of the Immaculate Conception (that Mary was conceived free from original sin) by saying:\\r\\n[Mary] is full of grace, proclaimed to be entirely without sin. God's grace fills her with everything good and makes her devoid of all evil. God is with her, meaning that all she did or left undone is divine and the action of God in her. Moreover, God guarded and protected her from all that might be hurtful to her.[52]\\r\\nProtestant Reformer John Calvin (1509ÿ1564) developed a systematic theology of Augustinian Protestantism by interpretation of Augustine of Hippo's notion of original sin. Calvin believed that humans inherit Adamic guilt and are in a state of sin from the moment of conception. This inherently sinful nature (the basis for the Calvinistic doctrine of \\"total depravity\\") results in a complete alienation from God and the total inability of humans to achieve reconciliation with God based on their own abilities. Not only do individuals inherit a sinful nature due to Adam's fall, but since he was the federal head and representative of the human race, all whom he represented inherit the guilt of his sin by imputation. Redemption by Jesus Christ is the only remedy.\\r\\nJohn Calvin defined original sin in his Institutes of the Christian Religion as follows:\\r\\nOriginal sin, therefore, seems to be a hereditary depravity and corruption of our nature, diffused into all parts of the soul, which first makes us liable to God's wrath, then also brings forth in us those works which Scripture calls \\"works of the flesh\\" (Gal 5:19). And that is properly what Paul often calls sin. The works that come forth from it ÿ such as adulteries, fornications, thefts, hatreds, murders, carousings ÿ he accordingly calls \\"fruits of sin\\" (Gal 5:19ÿ21), although they are also commonly called \\"sins\\" in Scripture, and even by Paul himself.[53]\\r\\nThe Council of Trent (1545ÿ1563), while not pronouncing on points disputed among Catholic theologians, condemned the teaching that in baptism the whole of what belongs to the essence of sin is not taken away, but is only cancelled or not imputed, and declared the concupiscence that remains after baptism not truly and properly \\"sin\\" in the baptized, but only to be called sin in the sense that it is of sin and inclines to sin.[54]\\r\\nIn 1567, soon after the close of the Council of Trent, Pope Pius V went beyond Trent by sanctioning Aquinas's distinction between nature and supernature in Adam's state before the Fall, condemned the identification of original sin with concupiscence, and approved the view that the unbaptized could have right use of will.[2]\\r\\nThe Catechism of the Catholic Church says:\\r\\nBy his sin Adam, as the first man, lost the original holiness and justice he had received from God, not only for himself but for all humans.\\r\\nAdam and Eve transmitted to their descendants human nature wounded by their own first sin and hence deprived of original holiness and justice; this deprivation is called \\"original sin\\".\\r\\nAs a result of original sin, human nature is weakened in its powers, subject to ignorance, suffering and the domination of death, and inclined to sin (this inclination is called \\"concupiscence\\").[55]\\r\\nThe Catholic Church teaches that every human person born on this earth is made in the image of God.[56][57] Within man \\"is both the powerful surge toward the good because we are made in the image of God, and the darker impulses toward evil because of the effects of Original Sin\\".[58] Furthermore, it explicitly denies that we inherit guilt from anyone, maintaining that instead we inherit our fallen nature. In this it differs from the Calvinist/Protestant position that each person actually inherits Adam's guilt, and teaches instead that \\"original sin does not have the character of a personal fault in any of Adam's descendants ... but the consequences for nature, weakened and inclined to evil, persist in man\\".[59] \\"In other words, human beings do not bear any 'original guilt' from Adam and Eve's particular sin.\\"[60]\\r\\nThe Church has always held baptism to be \\"for the remission of sins\\", and, as mentioned in Catechism of the Catholic Church, 403, infants too have traditionally been baptized, though not guilty of any actual personal sin. The sin that through baptism was remitted for them could only be original sin, with which they were connected by the very fact of being a human. The first comprehensive theological explanation of this practice of baptizing infants, guilty of no actual personal sin, was given by Saint Augustine of Hippo, not all of whose ideas on original sin have been adopted by the Catholic Church. Indeed, the Church has condemned the interpretation of some of his ideas by certain leaders of the Protestant Reformation.\\r\\nThe Catechism of the Catholic Church explains that in \\"yielding to the tempter, Adam and Eve committed a personal sin, but this sin affected the human nature that they would then transmit in a fallen state ... original sin is called \\"sin\\" only in an analogical sense: it is a sin \\"contracted\\" and not \\"committed\\"a state and not an act\\" (Catechism of the Catholic Church, 404). This \\"state of deprivation of the original holiness and justice ... transmitted to the descendants of Adam along with human nature\\" (Compendium of the Catechism of the Catholic Church, 76) involves no personal responsibility or personal guilt on their part (cf. Catechism of the Catholic Church, 405). Personal responsibility and guilt were Adam's, who because of his sin, was unable to pass on to his descendants a human nature with the holiness with which it would otherwise have been endowed, in this way implicating them in his sin. The doctrine of original sin thus does not impute the sin of the father to his children, but merely states that they inherit from him a \\"human nature deprived of original holiness and justice\\", which is \\"transmitted by propagation to all mankind\\".[61]\\r\\nIn the theology of the Catholic Church, original sin is regarded as the general condition of sinfulness, that is the absence of holiness and perfect charity into which humans are born, distinct from the actual sins that a person commits. This teaching explicitly states that \\"original sin does not have the character of a personal fault in any of Adam's descendants\\".[59] In other words, human beings do not bear any \\"original guilt\\" from Adam's particular sin, which is his alone. The prevailing view, also held in Eastern Orthodoxy, is that human beings bear no guilt for the sin of Adam. The Catholic Church teaches: \\"By our first parents' sin, the devil has acquired a certain domination over man, even though man remains free.\\"[62]\\r\\nThe Catholic doctrine of the Immaculate Conception of Mary is that Mary was conceived free from original sin: \\"the most Blessed Virgin Mary was, from the first moment of her conception, by a singular grace and privilege of almighty God and by virtue of the merits of Jesus Christ, Savior of the human race, preserved immune from all stain of original sin\\".[63] The doctrine sees her as an exception to the general rule that human beings are not immune from the reality of original sin.\\r\\nSoon after the Second Vatican Council, biblical theologian Herbert Haag raised the question: Is original sin in Scripture?[64] According to his exegesis, Genesis 2:25 indicates that Adam and Eve were created from the beginning naked of the divine grace, an originary grace that, then, they would never have had and even less would have lost due to the subsequent events narrated. On the other hand, while supporting a continuity in the Bible about the absence of preternatural gifts (Latin: dona praeternaturalia)[65] with regard to the ophitic event, Haag never makes any reference to the discontinuity of the loss of access to the tree of life.\\r\\nThe Eastern Orthodox version of original sin is the view that sin originates with the Devil, \\"for the devil sinneth from the beginning (1 John iii. 8)\\".[66] They acknowledge that the introduction of ancestral sin[67][better?source?needed] into the human race affected the subsequent environment for humanity (see also traducianism). However, they never accepted Augustine of Hippo's notions of original sin and hereditary guilt.[68][better?source?needed]\\r\\nOrthodox Churches accept the teachings of John Cassian, as do Catholic Churches eastern and western,[42] in rejecting the doctrine of total depravity, by teaching that human nature is \\"fallen\\", that is, depraved, but not totally. Augustine Casiday states that Cassian \\"baldly asserts that God's grace, not human free will, is responsible for 'everything which pertains to salvation' ÿ even faith\\".[43] Cassian points out that people still have moral freedom and one has the option to choose to follow God. Colm Luibhid says that, according to Cassian, there are cases where the soul makes the first little turn,[44] while Augustine Casiday says that, in Cassian's view, any sparks of goodwill that may exist, not directly caused by God, are totally inadequate and only direct divine intervention ensures spiritual progress.[45] and Lauren Pristas says that \\"for Cassian, salvation is, from beginning to end, the effect of God's grace\\".[46]\\r\\nEastern Orthodoxy accepts the doctrine of ancestral sin: \\"Original sin is hereditary. It did not remain only Adam and Eve's. As life passes from them to all of their descendants, so does original sin.\\"[69] \\"As from an infected source there naturally flows an infected stream, so from a father infected with sin, and consequently mortal, there naturally proceeds a posterity infected like him with sin, and like him mortal.\\"[70]\\r\\nThe Orthodox Church in America makes clear the distinction between \\"fallen nature\\" and \\"fallen man\\" and this is affirmed in the early teaching of the Church whose role it is to act as the catalyst that leads to true or inner redemption. Every human person born on this earth bears the image of God undistorted within themselves.[71] In the Orthodox Christian understanding, they explicitly deny that humanity inherited guilt from anyone. Rather, they maintain that we inherit our fallen nature. While humanity does bear the consequences of the original, or first, sin, humanity does not bear the personal guilt associated with this sin. Adam and Eve are guilty of their willful action; we bear the consequences, chief of which is death.\\"[72]\\r\\nThe view of the Eastern Orthodox Church varies on whether Mary is free of all actual sin or concupiscence. Some Patristic sources imply that she was cleansed from sin at the Annunciation, while the liturgical references are unanimous that she is all-holy from the time of her conception.[73][74]\\r\\nThe original formularies of the Church of England also continue in the Reformation understanding of original sin. In the Thirty-Nine Articles, Article IX \\"Of Original or Birth-sin\\" states:\\r\\nOriginal Sin standeth not in the following of Adam, (as the Pelagians do vainly talk); but it is the fault and corruption of the Nature of every man, that naturally is ingendered of the offspring of Adam; whereby man is very far gone from original righteousness, and is of his own nature inclined to evil, so that the flesh lusteth always contrary to the spirit; and therefore in every person born into this world, it deserveth God's wrath and damnation. And this infection of nature doth remain, yea in them that are regenerated; whereby the lust of the flesh, called in the Greek, ϫ ϫϰ?, which some do expound the wisdom, some sensuality, some the affection, some the desire, of the flesh, is not subject to the Law of God. And although there is no condemnation for them that believe and are baptized, yet the Apostle doth confess, that concupiscence and lust hath of itself the nature of sin.[75]\\r\\nHowever, more recent doctrinal statements (e.g. the 1938 report Doctrine in the Church of England) permit a greater variety of understandings of this doctrine. The 1938 report summarizes:\\r\\nMan is by nature capable of communion with God, and only through such communion can he become what he was created to be. \\"Original sin\\" stands for the fact that from a time apparently prior to any responsible act of choice man is lacking in this communion, and if left to his own resources and to the influence of his natural environment cannot attain to his destiny as a child of God.[76]\\r\\nThe Methodist Church upholds Article VII in the Articles of Religion in the Book of Discipline of the United Methodist Church:\\r\\nOriginal sin standeth not in the following of Adam (as the Pelagians do vainly talk), but it is the corruption of the nature of every man, that naturally is engendered of the offspring of Adam, whereby man is very far gone from original righteousness, and of his own nature inclined to evil, and that continually.[77]\\r\\nSeventh-day Adventists believe that humans are inherently sinful due to the fall of Adam,[78] but they do not totally accept the Augustinian/Calvinistic understanding of original sin, taught in terms of original guilt, but hold more to what could be termed the \\"total depravity\\" tradition.[79] Seventh-day Adventists have historically preached a doctrine of inherited weakness, but not a doctrine of inherited guilt.[80] According to Augustine and Calvin, humanity inherits not only Adam's depraved nature but also the actual guilt of his transgression, and Adventists look more toward the Wesleyan model.[81]\\r\\nIn part, the Adventist position on original sin reads:\\r\\nThe nature of the penalty for original sin, i.e., Adam's sin, is to be seen as literal, physical, temporal, or actual death ÿ the opposite of life, i.e., the cessation of being. By no stretch of the scriptural facts can death be spiritualised as depravity. God did not punish Adam by making him a sinner. That was Adams own doing. All die the first death because of Adams sin regardless of their moral character ÿ children included.[81]\\r\\nEarly Adventists Pioneers (such as George Storrs and Uriah Smith) tended to de-emphasise the morally corrupt nature inherited from Adam, while stressing the importance of actual, personal sins committed by the individual. They thought of the \\"sinful nature\\" in terms of physical mortality rather than moral depravity.[81] Traditionally, Adventists look at sin in terms of willful transgressions, and that Christ triumphed over sin. Adventism believes that Christ is both our Substitute and our Example.[82] They base their belief on texts such as \\"Whosoever committeth sin transgresseth also the law: for sin is the transgression of the law.\\" (1 John 3:4)[83]\\r\\nThough believing in the concept of inherited sin from Adam, there is no dogmatic Adventist position on original sin. Related articles dealing with the subject are publicly available on the General Conference of the Seventh-day Adventist Churchs official website on theological doctrine, the Biblical Research Institute.[84]\\r\\nAccording to the theology of the Christian Congregation of Jehovah's Witnesses, all humans are born sinners, because of inheriting sin, corruption, and death from Adam. They teach that Adam was originally created perfect and sinless, but with free will; that the Devil, who was originally a perfect angel, but later developed feelings of pride and self-importance, seduced Eve, and then through her, persuaded Adam to disobey God, and to obey the Devil instead, rebelling against God's sovereignty, thereby making themselves sinners, and because of that, transmitting a sinful nature to all of their future offspring.[85][86] Instead of destroying the Devil right away, as well as destroying the disobedient couple, God decided to test the loyalty of the rest of humankind, and to prove that man cannot be independent of God successfully, that man is lost without God's laws and standards, and can never bring peace to the earth, and that Satan was a deceiver, murderer, and liar.[87]\\r\\nWitnesses believe that all men possess \\"inherited sin\\" from the \\"one man\\" Adam and they teach that verses such as Romans 5:12-22, Psalm 51:5, Job 14:4, and 1st Corinthians 15:22 show that man is born corrupt, and dies because of inherited sin and imperfection, that inherited sin is the reason and cause for sickness and suffering, made worse by the Devil's wicked influence. They believe Jesus is the \\"second Adam\\", being the sinless Son of God and the Messiah, and that he came to undo Adamic sin; and that salvation and everlasting life can only be obtained through faith and obedience to the second Adam.[85][86][87][88][89][90] They believe that \\"sin\\" is \\"missing the mark\\" of God's standard of perfection, and that everyone is born a sinner, due to being the offspring of sinner Adam.[91]\\r\\nThe Book of Mormon, a text sacred to Mormonism, explains that the opportunity to live here in a world where we can learn good and bad is a gift from God, and not a punishment for Adam's and Eve's choice.[92] As Mormon founder Joseph Smith taught, humans had an essentially godlike nature, and were not only holy in a premortal state, but had the potential to progress eternally to become like God.[93] He wrote as one of his church's Articles of Faith, \\"We believe that men will be punished for their own sins, and not for Adams transgression.\\"[94] Later Mormons took this creed as a rejection of the doctrine of original sin and any notion of inherited sinfulness.[93] Thus, while modern Mormons will agree that the fall of Adam brought consequences to the world, including the possibility of sin, they generally reject the idea that any culpability is automatically transmitted to Adam and Eve's offspring.[95] Children under the age of eight are regarded as free of all sin and therefore do not require baptism.[96] Children who die prior to age eight are believed to be saved in the highest degree of heaven.[97]\\r\\nIn Swedenborgianism, exegesis of the first 11 chapters of Genesis from The First Church, has a view that Adam is not an individual person. Rather, he is a symbolic representation of the \\"Most Ancient Church\\", having a more direct contact with heaven than all other successive churches.[98][99] Swedenborg's view of original sin is referred to as hereditary evil, which passes from generation to generation.[100] It cannot be completely abolished by an individual man, but can be tempered when someone reforms their own life,[101] and are thus held accountable only for their own sins.[102]\\r\\nMost Quakers (also known as the Religious Society of Friends), including the founder of Quakerism, George Fox, believe in the doctrine of Inward light, a doctrine which states that there is \\"that of God in everyone\\".[103] This has led to a common belief among many liberal and universalist Quakers affiliated with the Friends General Conference and Britain Yearly Meeting, based on the ideas of Quaker Rufus Jones among others, that rather than being burdened by original sin, human beings are inherently good, and the doctrine of universal reconciliation, that is, that all people will eventually be saved and reconciled with God.\\r\\nHowever, this rejection of the doctrine of original sin or the necessity of salvation is not something that most conservative or evangelical Quakers affiliated with Friends United Meeting or Evangelical Friends Church International tend to agree with. Although the more conservative and evangelical Quakers also believe in the doctrine of inward light, they interpret it in a manner consistent with the doctrine of original sin, namely, that people may or may not listen to the voice of God within them and be saved, and people who do not listen will not be saved, thus there cannot be any universal reconciliation under the more conservative Quaker theology. Many more liberal Quaker meetings have no official creed and allow individual Quakers to have a wide variety of different beliefs, although following the recommendations of one's yearly meeting's Faith and Practice book is strongly recommended even though not mandatory.\\r\\nAlso, not all local Quaker meetings are entirely liberal or conservative and some have members adhering to different Quaker traditions, and a Quaker meeting can be affiliated with both liberal and conservative umbrella organizations. For instance, New York Yearly Meeting, which is mostly liberal, is affiliated with both the liberal Friends General Conference as well as the conservative Friends United Meeting, even though they adhere much more to the liberal teachings of Friends General Conference than the conservative teachings of Friends United Meeting.[104][105] Thus, at any given Quaker meeting, individual Quakers may have differing views on a wide variety of theological issues including original sin, with more conservative Quaker meetings having everyone believe in original sin while at more liberal Quaker meetings there is typically a variety of beliefs and usually a majority of people would not believe in original sin but a minority would believe in original sin.\\r\\nThe doctrine of \\"inherited sin\\" is not found in most of mainstream Judaism. Although some in Orthodox Judaism place blame on Adam for overall corruption of the world, and though there were some Jewish teachers in Babylon[106] who believed that death was a punishment brought upon humanity on account of Adam's sin, that is not the dominant view in most of Judaism today. Modern Judaism generally teaches that humans are born sin-free and untainted, and choose to sin later and bring suffering to themselves.[107][108]\\r\\nJewish theologians are divided in regard to the cause of what is called \\"original sin\\". Some teach that it was due to Adam's yielding to temptation in eating of the forbidden fruit and has been inherited by his descendants; the majority of chazalic opinions, however, do not hold Adam responsible for the sins of humanity,[109] teaching that, in Genesis 8:21 and 6:5-8, God recognized that Adam did not willfully sin. However, Adam is recognized by some[106] as having brought death into the world by his disobedience. Because of his sin, his descendants will live a mortal life, which will end in death of their bodies.[110]\\r\\nThe concept of inherited sin does not exist in Islam.[111][112] Islam teaches that Adam and Eve sinned, but then sought forgiveness and thus were forgiven by God.[113]\\r\\nThe Qur'an says that after Adam and Eve sinned, they asked for repentance and it was granted:\\r\\nThey said: \\"Our Lord, we have wronged ourselves souls. If You forgive us not and bestow not upon us Your mercy, we shall certainly be of the losers.\\r\\nThus did Adam disobey his Lord, so he went astray. Then his Lord chose him, and turned to him with forgiveness, and gave him guidance.\\r\\nThe Qur'an further says about individual responsibility:\\r\\nThat no burdened person (with sins) shall bear the burden (sins) of another. And that man can have nothing but what he does (of good and bad). And that his deeds will be seen, Then he will be recompensed with a full and the best [fair] recompense.","input":"When did the concept of original sin begin?"},{"output":"1.42 million boepd","context":"The oil and gas industry in the United Kingdom produced 1.42 million boepd[1] in 2014, of which 59%[1] was oil/liquids. In 2013 the UK consumed 1.508 million bpd of oil and 2.735tcf of gas,[2] so is now an importer of hydrocarbons having been a significant exporter in the 1980s and 1990s.\\r\\n\\r\\n98% of production comes from offshore fields[3] and the services industry in Aberdeen has been a leader in developing technology for hydrocarbon extraction offshore. Historically most gas came from Morecambe Bay and the Southern North Sea off East Anglia, but both areas are now in decline. Oil comes mainly from the North Sea Central Graben close to the median line with Norway in two main clusters - around the Forties oilfield east of Aberdeen and the Brent oilfield east of Shetland. There have been recent discoveries in challenging conditions west of Shetland. As of  2012[update] there were 15,729 kilometres (9,774?mi)of pipelines linking 113 oil installations and 189 gas installations.[4] The only major onshore field is Wytch Farm in Dorset but there are a handful oil wells scattered across England. There is significant shale potential in the Weald and in the Bowland Shale under Lancashire & Yorkshire, but only a few wells have been drilled amongst considerable protests.\\r\\n\\r\\nThe UK's strengths in financial services have led it to play a leading role in energy trading through markets such as ICE Futures (formerly the International Petroleum Exchange). The price of Brent Crude from the British North Sea remains the major benchmark for the international oil trade, and the National Balancing Point market is the benchmark for most of the gas traded across Europe.[5] The difficult offshore conditions make the UK a high-cost producer; in 2014 the average development cost was $20.40/boe and the operating cost was $27.80/boe for a total of $48.20/boe.[1] In 2014 the industry spent S1.1bn on exploration, S14.8bn on capital investment and S9.6bn on operating costs.[1] Fields developed since 1993 are taxed through an additional corporation tax on profits, in 2014 the industry generated S2.8bn in direct taxes.[1]\\r\\n\\r\\nAfter the Scottish shale oil industry reached its peak in the 19th century, the British government became increasingly concerned to find secure sources of fuel oil for the Royal Navy. This led to a nationwide search for onshore oil during the First World War and a modest discovery of oil at Hardstoft in Derbyshire.\\r\\n\\r\\nIn 1934, the countrys oil resources were nationalised by the Petroleum Production Act, and a fresh attempt was made to find oil on the UK mainland. The outbreak of World War II accelerated this search and led to a number of wells being drilled, primarily around Eakring in the East Midlands.\\r\\n\\r\\nIn the 1950s, the focus turned to southern England where oil was discovered in the Triassic Sherwood Sands formation at 5200 feet, followed by the development of the Wych Farm oilfield. The link between onshore and offshore oil in the North Sea was made after the discovery of the Groningen gas field in The Netherlands in 1959.[6]\\r\\n\\r\\nSince 1965, 3,970[7]  exploration and appraisal wells have been drilled offshore on the UKCS (United Kingdom Continental Shelf). In 2014, 104 new wells and 54 sidetracks were drilled.[1]\\r\\n\\r\\nOver four decades since the 1960s, the industry has spent S58 billion (2008 money)[8] on exploration drilling. In 2008, S1.4[8] billion  was spent finding new oil and gas reserves.\\r\\n\\r\\nIn 2008, 300ÿ400 million barrels (48,000,000ÿ64,000,000?m3) of oil and gas equivalent (boe) were discovered. The average size of the oil and gas fields discovered between 2000 and 2008 was 26 million boe,[9] compared with an average of 248 million boe in the ten years from 1966.[9]\\r\\n\\r\\nIn 2008, the UK was the 14th largest oil and gas producer in the world (10th largest gas producer and 19th largest oil producer).[10]  In Europe the UK is second only to Norway in oil and gas production.\\r\\n\\r\\nOil and gas production from the UK sector of the North Sea peaked in 1999, but the UK remains a substantial producer today. Over the last four decades, 39 billion boe  have been extracted on the UKCS.[11] In 2008, the combined production of oil and gas was 1 billion boe (549 million barrels (87,300,000?m3) of oil and 68 billion cubic metres of gas). This represented a fall of 5% compared with 2007 (6% oil and 3% gas), a slight improvement on the decline rate in 2002-2007 which averaged 7.5% per annum.[12]\\r\\n\\r\\nAs of 2008, just over three quarters of the UK's primary energy demand was met by oil and gas. In 2008, oil produced on the UKCS satisfied almost all domestic consumption (97%) while gas produced in the UK met about three quarters of demand.[7] In 2020, it is estimated that 70% of primary energy consumed in the UK will still come from oil and gas, even upon achievement of the Government's target to source 20% of energy from renewable sources.[12] This will be a combination of oil and gas produced domestically and imports. The UKCS has the potential to satisfy 40% of the UK's oil and gas demand in 2020, if investment is sustained.[12]\\r\\n\\r\\nOver the last four decades, a total of S210 billion (2008 money)[12]  has been invested in developing new resources. In 2008, this figure was S4.8 billion,[7] a 20% decrease since 2006. An additional S147 billion (2008 money)[7] has been spent on producing the oil and gas and in 2008, operating costs were S6.8 billion, an increase on 2007.\\r\\n\\r\\nOil and gas production from the UKCS has contributed S271 billion (2008 money) in tax revenues over the last forty years.[13] In 2008, tax rates on UKCS production ranged from 50 ÿ 75%, depending on the field. The industry paid S12.9 billion[13] in corporate taxes in 2008-9, the largest since the mid-1980s, because of high oil and gas prices. This represented 28% of total corporation tax paid in the UK.[13] It is expected that tax revenues from production will fall to S6.9 billion in 2009-10[13] based on an oil price of $47 per barrel, providing 20% of total corporation taxes. In addition to production taxes, the supply chain contributes another S5-6 billion per year in corporation and payroll taxes.[12]\\r\\n\\r\\nIn 2008, some 450,000 jobs  throughout the United Kingdom were supported by the servicing of activity on the UKCS and in the export of oil and gas related goods and services around the world.[12] The exploration for and extraction of oil and gas from the UKCS accounted for around 350,000 of these;  this comprised 34,000 directly employed by oil and gas companies and their major contractors, plus 230,000 within the wider supply chain. Another 89,000 jobs were supported by the economic activity induced by employees' spending.  In addition, a thriving exports business is estimated to support a further 100,000 jobs.[12] In January 2013 an Industry Job Site www.oilandgaspeople.com predicted that over 50,000 new jobs would be created within the industry that year as new technology makes marginal fields more viable.\\r\\n\\r\\nWhilst the oil and gas industry provides work across the whole of the UK, Scotland benefits the most, with around 195,000 jobs, or 44% of the total.  21% of the workforce is from South East England, 15% from the North of England, and 12% from the East of England . Each Sbillion spent on the UKCS supports approximately 20,000 jobs.[12]\\r\\n\\r\\nSet up in 1996, First Point Assessment Limited (FPAL)[14] is the key tool used by oil and gas companies to identify and select current and potential suppliers when awarding contracts or purchase orders. The organisation operates as a neutral, industry-steered organisation, improving efficiency in the oil and gas supply chain. FPAL currently matches the needs of over 70 purchasing organisations with the capabilities of over 2,400 suppliers.\\r\\n\\r\\nJobs in the UK oil and gas industry are highly skilled and well rewarded. 2008 salaries averaged circa S50,000 a year  across a broad sample of supply chain companies, with the Exchequer benefiting by S19,500 per head in payroll taxes.[12]\\r\\n\\r\\nSet up in 2007, OPITO [15]  supports the efforts and resources that employers throughout the UK are currently investing in workforce development to ensure that the UKCS remains at the forefront of offshore expertise and technology. The organisation allows the industry to consolidate and improve its work in generating and developing the talent needed to sustain the long-term future of the UKCS and export learning internationally. The Academy works with schools, colleges and universities on a shared agenda of encouraging greater uptake of mathematics, science and engineering subjects. The organisation also supports the development of safety, technical and leadership skills within the industry in response to identified need. Training standards and quality assurance on training delivery both here and around the world are also being advanced through the Academy.\\r\\n\\r\\nThe operating environment in the waters around the UK is harsh and demanding. To overcome the challenges of recovering oil and gas from increasingly difficult reservoirs and deeper waters, the North Sea has developed a position at the forefront of offshore engineering, particularly in subsea technology. Many new oil and gas fields in the UK are small, technically complex and economically marginal. Often recovery from these fields is achieved by subsea developments tied back to existing installations and infrastructure, over varying distances measured in tens of kilometres.  Innovative technology is also a critical component in the recovery of reserves from high pressure, high temperature (HPHT), heavy crude oil and deep water fields.\\r\\n\\r\\nUK exports of oil-related goods and services have been estimated at more than 0 billion a year in value.[12]  This amount is a reflection of how well established the UK's supply chain is internationally. The competence of its people and the quality of its technology, particularly subset, are very much in demand in oil and gas provinces around the world.\\r\\n\\r\\nThe Industry's Technology Facilitator (ITF)[16]  identifies needs and facilitates the development of new technology to meet those needs through joint industry projects with up to 100% funding available for promising solutions. Since its creation ten years ago, ITF has helped oil and gas producers, service companies and technology developers to work collaboratively, developing 137 technology projects.\\r\\n\\r\\nSet up 1997, Step Change in Safety[17] is the UK based cross-industry partnership with the remit to make the UK the safest oil and gas exploration and production province in the world. Its initial aim was to reduce the injury rate by 50%, which was achieved in 2003. Step Change in Safety's work is now focused in three areas: recognising hazards and reducing risk, personal ownership for safety and asset integrity. Communication between Step Change in Safety and the industry is through elected safety representatives, offshore installation managers and supervisors, safety professionals and company focal points. These individuals are consulted on what needs to be done and are charged with ensuring that the Step Change programme is implemented.\\r\\n\\r\\nThe Health & Safety Executive (HSE)[18] is the UK offshore oil and gas industry regulator and is organised into a number of directorates. The Hazardous Installations Directorate (HID) is the operational arm responsible for major hazards. A dedicated Offshore Division within HID is responsible for the enforcement of regulations in the offshore oil and gas industry.\\r\\n\\r\\nHSE publishes fatal, major and over-3-day injuries as well as dangerous occurrences under the Reporting of Injuries, Diseases and Dangerous Occurrences Regulations (RIDDOR) 1996 . RIDDOR does not apply to events that are reportable under the Air Navigation (Investigation of Air Accidents involving Civil and Military Aircraft or Installations) Regulations 1986;[19] The Civil Aviation (Investigation of Air Accidents) Regulations 1989; and The Merchant Shipping Act 1988, and Orders and Regulations made or to be made there under ÿ therefore, industry-related aviation and marine accidents which are covered by any of the above regulations are not included in the RIDDOR-derived statistics. In 2007/8 and 2008/9, there were no fatalities, compared with two in 2006/7 and 2005/6. During 2008/9, 30 major injuries were reported compared with 44 in 2007/8. This resulted in a combined fatal and major injury rate of 106 per 100,000 workers, down from 156 and 146 in 2007/8 and 2006/7 respectively. The number of over-3-day injuries has fallen this year by 5% to 140, representing an over-3-day injury rate of 496 per 100,000 workers.\\r\\n\\r\\nAsset integrity is the ability of an oil and gas asset to perform its required function effectively and efficiently whilst protecting health, safety and the environment. Asset integrity management is the means of ensuring that the people, systems, processes and resources that deliver integrity are in place, in use and will perform when required over the whole lifecycle of the asset. In 2004, the HSE highlighted the infrastructure on which work was required to maintain integrity and S4 billion  was subsequently spent in the area of asset maintenance.[20] In 2009, the HSE confirmed that the key issues identified earlier had been resolved.  Specific initiatives now encourage industry wide engagement and continued investment in asset integrity. On a global level, Asset Integrity continues to be ranked amongst the oil and gas industry's biggest challenges and main focus areas, In spite of this, recent research by Oil and Gas Fundamentals has indicated that understanding of the subject inside the industry is still not where it needs to be.[21]\\r\\n\\r\\nAberdeen is the busiest heliport in the UK with 47,000 flights in 2008 transporting workers to and from offshore installations on the UKCS.[22] Between 1977 and the end of 2006, just over 56 million passengers were transported by helicopter from all UK heliports to and from offshore installations on the UKCS. More than 6.5 million sectors were flown, taking nearly 3 million flying hours. During this time, seven fatal helicopter accidents claimed the lives of 94 offshore workers and flight crew.[23] Government data for the period 1995 to 2004 show that with the exception of rail, the yearly passenger casualty rate for offshore helicopter travel is much better than most forms of land-based passenger transport and of a similar order to travelling by car.[23] Offshore helicopter passengers are equipped for their journey with survival suits and other aids and undergo survival training.\\r\\n\\r\\nThe Fisheries Legacy Trust Company's  (FLTC)[24] main function is to help keep fishermen safe in UK waters. It does this by building a trust fund (based on payments from oil and gas producers) which can be used to maintain comprehensive, up-to-date information on all seabed hazards related to oil and gas activities for as long as they remain, and to make this data available for use by fishing vessel plotters found on board in wheelhouses all around the UK coastline.\\r\\n\\r\\nThe industry's vision which guides the environmental management process is to understand and manage environmental risks to achieve demonstrable no harm levels by 2020.\\r\\n\\r\\nUK oil and gas installations participate in the European Union Emissions Trading Scheme (EU ETS) which aims to reduce emissions of carbon dioxide and combat the threat of climate change. Carbon dioxide is released into the atmosphere in three ways during production operations: combustion of fuel for power generation, flaring (a process used to burn off unusable waste gas or flammable gas and liquids for safety reasons) and direct process emissions. Over the years, carbon dioxide emissions in tonnes have steadily decreased with a 10% reduction in 2007 compared with 2000.[23] In 2007, 17 million tonnes of carbon dioxide were emitted.\\r\\n\\r\\nOpen flares for well tests are not permitted in the UK. Release of unburned gas is also not permitted by the Environment Agency/SEPA.[25] The low temperature of combustion in open flaring, and incomplete mixing of oxygen means that carbon in methane may not be burned, leading to a sooty smoke, and potential VOC/BTEX contamination. Radon gas exists in very low concentrations in shale gas and in North Sea gas, but the levels predicted fall below any level of concern. (300 microseiverts p.a.)\\r\\n\\r\\nIn exploration wells, where flow rates are expected to be 10 tonnes of gas per day, testing is licensed by the Environment Agency to 30 days, extendable to 90 days. Enclosed burners are available that will ensure low levels of light pollution, little noise, and 99+% combustion and destruction of VOCs/BTEX, at around 800 C.[26]\\r\\n\\r\\nWell testing is used to estimate productivity of the well. In testing a production well, the test can be made by flowing into the production pipeline. This means that no gases would be lost, and flaring would not be necessary. This is known as a 'green completion'.\\r\\n\\r\\nDischarges into the sea can occur either through accidental release (e.g. oil spill) or in the course of normal operations. In 2007, 59 tonnes of oil in total[23] was accidentally released into the marine environment, which, in open sea, will have a negligible environmental impact.\\r\\n\\r\\nTypes of waste generated offshore vary and include drill cuttings and powder, recovered oil, crude contaminated material, chemicals, drums, containers, sludges, tank washings, scrap metal and segregated recyclables. The majority of wastes produced offshore are transferred onshore where the main routes of disposal are landfill, incineration, recycling and reuse. Drill cuttings are also re-injected into wells offshore.\\r\\n\\r\\n39 billion barrels (6.2G109?m3) of oil and gas have been produced on the UKCS and up to 25 billion barrels (4.0G109?m3)  are left.[12] Therefore, the UK could still be producing significant amounts of oil and gas for decades to come. It is estimated that in 2020, UK production could still meet 40%[12] of the nation's demand for oil and gas.\\r\\n\\r\\nThe principal legislation for decommissioning offshore infrastructure when production ceases is OSPAR Decision 98/3 on Disposal of Disused Offshore Installations. Under OSPAR legislation, only installations that fulfil certain criteria (on the grounds of safety and/or technical limitations) are eligible for derogation (that is, leaving the structure, or part of, in place on the seabed). All other installations must be totally removed from the seabed. During the next two decades, the industry will begin to decommission many of the installations that have been producing oil and gas for the past forty years. There are approximately 470 installations to be decommissioned, including very large ones with concrete sub-structures, small, large and very large steel platforms, and subsea and floating equipment, the vast majority of which will have to be totally removed to the shore for dismantling and disposal.  Some 10,000 kilometres of pipelines, 15 onshore terminals and around 5,000 wells are also part of the infrastructure planned to be gradually phased out, although some, or parts, of the onshore terminals will remain because they are import points for gas pipelines from Norway and the Netherlands. Decommissioning is a complex process, representing a considerable challenge on many fronts and encompassing technical, economic, environmental, health and safety issues. Expenditure is therefore projected to be S19 billion by 2030, rising to S23 billion by 2040, for existing facilities. New facilities could add another S2-3 billion to the decommissioning cost, raising the total to circa S25 billion.[12]\\r\\n\\r\\nExports\\r\\n\\r\\nThe export of oilfield goods and services developed by the UK over forty years are in demand around the world. In 2008, approximately S5 billion[12] was earned through such exports. As energy demand around the world grows, so too will the need for technology and expertise required to satisfy it.\\r\\n\\r\\nTransfer to other industries\\r\\n\\r\\nMarine technology, skills and expertise pioneered in oil and gas are important in the design, installation and maintenance of offshore wind turbines and hence have found roles in the continuing evolution of renewable energy.  The industry has led the way in the development of drilling, remotely operated vehicles (ROVs) and geophysical technology.  All three areas of expertise are used by scientists and engineers elsewhere, whether examining Antarctic ice core samples, raising sunken ship wrecks or studying the plate tectonics of the ocean floor.\\r\\n\\r\\nCarbon Capture and Storage (CCS)\\r\\n\\r\\nTo prevent carbon dioxide building up in the atmosphere it has been theorised that it can be captured and stored, however no working model actually exists.  It is proposed to do CCS by combining three distinct processes: capturing the carbon dioxide at a power station or other major industrial plant, transporting it by pipeline or by tanker, and then storing it in geological formations. Some of the best natural repositories are depleted oil and gas fields, such as those in the North Sea. The oil and gas industry's knowledge of undersea geology, reservoir management and pipeline transport will play an important role in making this technology work effectively.","input":"How many barrels of oil does the uk produce?"},{"output":"Calypso Deep in the Ionian Sea.","context":"The?Mediterranean Sea is a sea connected to the Atlantic Ocean, surrounded by the Mediterranean Basin and almost completely enclosed by land: on the north by Southern Europe and Anatolia, on the south by North Africa, and on the east by the Levant. Although the sea is sometimes considered a part of the Atlantic Ocean, it is usually identified as a separate body of water. Geological evidence indicates that around 5.9 million years ago, the Mediterranean was cut off from the Atlantic and was partly or completely desiccated over a period of some 600,000 years before being refilled by the Zanclean flood about 5.3 million years ago.\\r\\nThe name Mediterranean is derived from the Latin mediterraneus, meaning \\"inland\\" or \\"in the middle of land\\" (from medius, \\"middle\\" and terra, \\"land\\"). It covers an approximate area of 2.5 million km2 (965,000?sq?mi), but its connection to the Atlantic (the Strait of Gibraltar) is only 14?km (8.7?mi) wide. The Strait of Gibraltar is a narrow strait that connects the Atlantic Ocean to the Mediterranean Sea and separates Gibraltar and Spain in Europe from Morocco in Africa. In oceanography, it is sometimes called the Eurafrican Mediterranean Sea or the European Mediterranean Sea to distinguish it from mediterranean seas elsewhere.[3][4]\\r\\nThe Mediterranean Sea has an average depth of 1,500?m (4,900?ft) and the deepest recorded point is 5,267?m (17,280?ft) in the Calypso Deep in the Ionian Sea. The sea is bordered on the north by Europe, the east by Asia, and in the south by Africa. It is located between latitudes 30 and 46 N and longitudes 6 W and 36 E. Its west-east length, from the Strait of Gibraltar to the Gulf of Iskenderun, on the southwestern coast of Turkey, is approximately 4,000?km (2,500 miles). The sea's average north-south length, from Croatias southern shore to Libya, is approximately 800?km (500 miles). The Mediterranean Sea, including the Sea of Marmara (connected by the Dardanelles to the Aegean Sea), has a surface area of approximately 2,510,000 square km (970,000 square miles).[5]\\r\\nThe sea was an important route for merchants and travellers of ancient times that allowed for trade and cultural exchange between emergent peoples of the region. The history of the Mediterranean region is crucial to understanding the origins and development of many modern societies.\\r\\nThe countries with coastlines on the Mediterranean Sea are Albania, Algeria, Bosnia and Herzegovina, Croatia, Cyprus, Egypt, France, Greece, Israel, Italy, Lebanon, Libya, Malta, Morocco, Monaco, Montenegro, Slovenia, Spain, Syria, Tunisia and Turkey. In addition, the Gaza Strip and the British Overseas Territories of Gibraltar and Akrotiri and Dhekelia have coastlines on the sea.\\r\\n\\r\\n\\r\\nThe term Mediterranean derives from the Latin word mediterraneus, meaning \\"amid the earth (note: earth in the sense \\"soil\\", not Planet Earth)\\" or \\"between land\\" (medi-; adj. medius, -um -a \\"middle, between\\" + terra f., \\"land, earth\\"): as it is between the continents of Africa, Asia and Europe. The Ancient Greek name Mesogeios (՚?ۚ?), is similarly from ?, \\"between\\" + ۻ, \\"land, earth\\").[6] It can be compared with the Ancient Greek name Mesopotamia (՚ϫ?ϫ), meaning \\"between rivers\\".\\r\\nThe Mediterranean Sea has historically had several names. For example, the Carthaginians called it the \\"Syrian Sea\\" and latter Romans commonly called it Mare Nostrum (\\"Our Sea\\"), and occasionally Mare Internum[7] and in Greek as the \\"Mare Magnum\\", meaning \\"Great Sea\\".[8]\\r\\nIn ancient Syrian texts, Phoenician epics and in the Hebrew Bible, it was primarily known as the \\"Great Sea\\" (?????? ?????????, HaYam HaGadol, Numbers 34:6,7; Joshua 1:4, 9:1, 15:47; Ezekiel 47:10,15,20), or simply \\"The Sea\\" (1 Kings 5:9; comp. 1 Macc. 14:34, 15:11); however, it has also been called the \\"Hinder Sea\\" (?????? ??????????), due to its location on the west coast of Greater Syria or the Holy Land, and therefore behind a person facing the east, sometimes translated as \\"Western Sea\\", (Deut. 11:24; Joel 2:20). Another name was the \\"Sea of the Philistines\\" (??? ?????????????, Exod. 23:31), from the people inhabiting a large portion of its shores near the Israelites.\\r\\nIn Modern Hebrew, it has been called HaYam HaTikhon (?????? ??????????), \\"the Middle Sea\\", reflecting the Sea's name in ancient Greek (Mesogeios), Latin Mare internum (Inner Sea) or Mare Nostrum (Our Sea), and modern languages in both Europe and the Middle East (Mediterranean, etc.).[8]\\r\\nSimilarly, in Modern Arabic, it is known as al-Ba?r [al-Abya?] al-Mutawassi? (????? [??????] ???????), \\"the [White] Middle Sea\\", while in Islamic and older Arabic literature, it was referenced as Ba?r al-Rm (??? ?????), or \\"the Roman/Byzantine Sea.\\"[8]\\r\\nIn Ottoman Turkish, it has also been called Bahr-i Sefid, meaning the \\"Pure White Sea\\".\\r\\nIn Turkish, it is known as Akdeniz,[9] meaning \\"the White Sea\\", to distinguish it from the Black Sea.[8]\\r\\nSeveral ancient civilisations were located around the Mediterranean shores, and were greatly influenced by their proximity to the sea. It provided routes for trade, colonisation, and war, as well as food (from fishing and the gathering of other seafood) for numerous communities throughout the ages.[10]\\r\\nDue to the shared climate, geology, and access to the sea, cultures centered on the Mediterranean tended to have some extent of intertwined culture and history.\\r\\nTwo of the most notable Mediterranean civilisations in classical antiquity were the Greek city states and the Phoenicians, both of which extensively colonised the coastlines of the Mediterranean. Later, when Augustus founded the Roman Empire, the Romans referred to the Mediterranean as Mare Nostrum (\\"Our Sea\\").\\r\\nDarius I of Persia, who conquered Ancient Egypt, built a canal linking the Mediterranean to the Red Sea. Darius's canal was wide enough for two triremes to pass each other with oars extended, and required four days to traverse.[11]\\r\\nThe Western Roman Empire collapsed around AD 476. Temporarily the east was again dominant as Roman power lived on in the Byzantine Empire formed in the 4th century from the eastern half of the Roman empire. Another power arose in the 7th century, and with it the religion of Islam, which soon swept across from the east; at its greatest extent, the Arab Empire controlled 75% of the Mediterranean region and left a lasting footprint on its eastern and southern shores.\\r\\nEurope started to revive, however, as more organised and centralised states began to form in the later Middle Ages after the Renaissance of the 12th century.\\r\\nOttoman power based in Anatolia continued to grow, and in 1453 extinguished the Byzantine Empire with the Conquest of Constantinople. Ottomans gained control of much of the sea in the 16th century and maintained naval bases in southern France, Algeria and Tunisia. Barbarossa, the famous Ottoman captain is a symbol of this domination with the victory of the Battle of Preveza (1538). The Battle of Djerba (1560) marked the apex of Ottoman naval domination in the Mediterranean. As the naval prowess of the European powers increased, they confronted Ottoman expansion in the region when the Battle of Lepanto (1571) checked the power of the Ottoman Navy. This was the last naval battle to be fought primarily between galleys.\\r\\nThe Barbary pirates of North Africa preyed on Christian shipping and coastlines in the Western Mediterranean Sea.[12] According to Robert Davis, from the 16th to 19th centuries, pirates captured 1 million to 1.25 million Europeans as slaves.[13]\\r\\nThe development of oceanic shipping began to affect the entire Mediterranean. Once, most trade between Western Europe and the East had passed through the region, but after the 1490s the development of a sea route to the Indian Ocean allowed the importation of A sian spices and other goods through the Atlantic ports of western Europe.[14][15][16]\\r\\nIn 2013 the Maltese president described the Mediterranean sea as a \\"cemetery\\" due to the large amounts of migrants who drowned there after their boats capsized.[17] European Parliament president Martin Schulz said in 2014 that Europe's migration policy \\"turned the Mediterranean into a graveyard\\", referring to the number of drowned refugees in the region as a direct result of the policies.[18] An Azerbaijani official described the sea as \\"a burial ground ... where people die\\".[19]\\r\\nFollowing the 2013 Lampedusa migrant shipwreck, the Italian government decided to strengthen the national system for the patrolling of the Mediterranean Sea by authorising \\"Operation Mare Nostrum\\", a military and humanitarian mission in order to rescue the migrants and arrest the traffickers of immigrants. In 2015, more than one million migrants crossed the Mediterranean Sea into Europe.[20]\\r\\nThe Mediterranean Sea is connected to the Atlantic Ocean by the Strait of Gibraltar (known in Homer's writings as the \\"Pillars of Hercules\\") in the west and to the Sea of Marmara and the Black Sea, by the Dardanelles and the Bosporus respectively, in the east. The Sea of Marmara (Dardanelles) is often considered a part of the Mediterranean Sea, whereas the Black Sea is generally not. The 163?km (101?mi) long artificial Suez Canal in the southeast connects the Mediterranean Sea to the Red Sea.[8]\\r\\nLarge islands in the Mediterranean include Cyprus, Crete, Euboea, Rhodes, Lesbos, Chios, Kefalonia, Corfu, Limnos, Samos, Naxos and Andros in the Eastern Mediterranean; Sicily, Cres, Krk, Bra?, Hvar, Pag, Kor?ula and Malta in the central Mediterranean; and Sardinia, Corsica, Ibiza, Majorca and Menorca (the Balearic Islands) in the Western Mediterranean.\\r\\nThe typical Mediterranean climate has hot, humid, and dry summers and mild, rainy winters. Crops of the region include olives, grapes, oranges, tangerines, and cork.\\r\\nThe International Hydrographic Organization defines the limits of the Mediterranean Sea as follows:[21]\\r\\nStretching from the Strait of Gibraltar in the west to the entrances to the Dardanelles and the Suez Canal in the east, the Mediterranean Sea is bounded by the coasts of Europe, Africa and Asia, and is divided into two deep basins:\\r\\nBeing nearly landlocked affects conditions in the Mediterranean Sea: for instance, tides are very limited as a result of the narrow connection with the Atlantic Ocean. The Mediterranean is characterised and immediately recognised by its deep blue colour.\\r\\nEvaporation greatly exceeds precipitation and river runoff in the Mediterranean, a fact that is central to the water circulation within the basin.[22] Evaporation is especially high in its eastern half, causing the water level to decrease and salinity to increase eastward.[23] The salinity at 5 m depth is 3.8%.[24]\\r\\nThe pressure gradient pushes relatively cool, low-salinity water from the Atlantic across the basin; it warms and becomes saltier as it travels east, then sinks in the region of the Levant and circulates westward, to spill over the Strait of Gibraltar.[25] Thus, seawater flow is eastward in the Strait's surface waters, and westward below; once in the Atlantic, this chemically distinct Mediterranean Intermediate Water can persist thousands of kilometres away from its source.[26]\\r\\nThe temperature of the water in the deepest part of the Mediterranean Sea is 13.2?C (55.8?F).[24]\\r\\nThe following countries have a coastline on the Mediterranean Sea:\\r\\nSeveral other territories also border the Mediterranean Sea (from west to east): The British overseas territory of Gibraltar, the Spanish autonomous cities of Ceuta and Melilla and nearby islands, the Sovereign Base Areas on Cyprus, and the Gaza Strip.\\r\\nMajor cities (municipalities) with populations larger than 200,000 people bordering the Mediterranean Sea are:\\r\\nAccording to the International Hydrographic Organization (IHO), the Mediterranean Sea is subdivided into a number of smaller waterbodies, each with their own designation (from west to east):[21]\\r\\nAlthough not recognised by the IHO treaties, there are some other seas whose names have been in common use from the ancient times, or in the present:\\r\\nMany of these smaller seas feature in local myth and folklore and derive their names from these associations.\\r\\nIn addition to the seas, a number of gulfs and straits are also recognised:\\r\\nThe geologic history of the Mediterranean Sea is complex. Underlain by oceanic crust, the sea basin was once thought to be a tectonic remnant of the ancient Tethys Ocean; it is now known to be a structurally younger basin, called the Neotethys, which was first formed by the convergence of the African and Eurasian plates during the Late Triassic and Early Jurassic. Because it is a near-landlocked body of water in a normally dry climate, the Mediterranean is subject to intensive evaporation and the precipitation of evaporites. The Messinian salinity crisis started about six million years ago (mya) when the Mediterranean became landlocked, and then essentially dried up. There are salt deposits accumulated on the bottom of the basin of more than a million cubic kilometresin some places more than three kilometres thick.[42][43]\\r\\nScientists estimate that the sea was last filled about 5.3 million years ago (mya) in less than two years by the Zanclean flood. Water poured in from the Atlantic Ocean through a newly breached gateway now called the Strait of Gibraltar at an estimated rate of about three orders of magnitude (one thousand times) larger than the current flow of the Amazon River.[44]\\r\\nThe Mediterranean Sea has an average depth of 1,500?m (4,900?ft) and the deepest recorded point is 5,267?m (17,280?ft) in the Calypso Deep in the Ionian Sea. The coastline extends for 46,000?km (29,000?mi). A shallow submarine ridge (the Strait of Sicily) between the island of Sicily and the coast of Tunisia divides the sea in two main subregions: the Western Mediterranean, with an area of about 850 thousand?km2 (330 thousand?mi2); and the Eastern Mediterranean, of about 1.65 million?km2 (640 thousand?mi2). A characteristic of the coastal Mediterranean are submarine karst springs or vruljas, which discharge pressurised groundwater into the coastal seawater from below the surface; the discharge water is usually fresh, and sometimes may be thermal.[45][46]\\r\\nThe Mediterranean basin and sea system was established by the ancient African-Arabian continent colliding with the Eurasian continent. As Africa-Arabia drifted northward, it closed over the ancient Tethys Ocean which had earlier separated the two supercontinents Laurasia and Gondwana. At about that time in the middle Jurassic period a much smaller sea basin, dubbed the Neotethys, was formed shortly before the Tethys Ocean closed at its western (Arabian) end. The broad line of collisions pushed up a very long system of mountains from the Pyrenees in Spain to the Zagros Mountains in Iran in an episode of mountain-building tectonics known as the Alpine orogeny. The Neotethys grew larger during the episodes of collisions (and associated foldings and subductions) that occurred during the Oligocene and Miocene epochs (34 to 5.33 mya); see animation: Africa-Arabia colliding with Eurasia. Accordingly, the Mediterranean basin consists of several stretched tectonic plates in subduction which are the foundation of the Eastern part of the Mediterranean Sea. Various zones of subduction harbour and form the deepest and most majestic oceanic ridges, east of the Ionian Sea and south of the Aegean. The Central Indian Ridge runs East of the Mediterranean Sea South-East across the in-between of Africa and the Arabian Peninsula into the Indian Ocean. Nevertheless, while man-made geopolitical turmoil and chaos have governed the coastlines of many various Mediterranean nations throughout the courses of ancient, modern, present and foreseeable history, the Plate tectonic status of nations bordering the Mediterranean Sea will find sharing the same geological concerns and fate.\\r\\nDuring Mesozoic and Cenozoic times, as the northwest corner of Africa converged on Iberia, it lifted the Betic-Rif mountain belts across southern Iberia and northwest Africa. There the development of the intramontane Betic and Rif basins led to creating two roughly-parallel marine gateways between the Atlantic Ocean and the Mediterranean Sea. Dubbed the Betic and Rifian corridors, they progressively closed during middle and late Miocene times; perhaps several times.[47] During late Miocene times the closure of the Betic Corridor triggered the so-called \\"Messinian salinity crisis\\" (MSC), when the Mediterranean almost entirely dried out. The time of beginning of the MSC was recently estimated astronomically at 5.96 mya, and it persisted for some 630,000 years until about 5.3 mya;[48] see Animation: Messinian salinity crisis, at right.\\r\\nAfter the initial drawdown and re-flooding there followed more episodesthe total number is debatedof sea drawdowns and re-floodings for the duration of the MSC. It ended when the Atlantic Ocean last re-flooded the basincreating the Strait of Gibraltar and causing the Zanclean floodat the end of the Miocene (5.33 mya). Some research has suggested that a desiccation-flooding-desiccation cycle may have repeated several times, which could explain several events of large amounts of salt deposition.[49][50] Recent studies, however, show that repeated desiccation and re-flooding is unlikely from a geodynamic point of view. [51][52]\\r\\nThe present-day Atlantic gateway, i.e. the Strait of Gibraltar, originated in the early Pliocene via the Zanclean Flood. As mentioned, two other gateways preceded Gibraltar: the Betic Corridor across southern Spain and the Rifian Corridor across northern Morocco. The former gateway closed about six (6) mya, causing the Messinian salinity crisis (MSC); the latter or possibly both gateways closed during the earlier Tortonian times, causing a \\"Tortonian salinity crisis\\" (from 11.6 to 7.2 mya), which occurred well before the MSC and lasted much longer. Both \\"crises\\" resulted in broad connections of the mainlands of Africa and Europe, which thereby normalised migrations of flora and faunaespecially large mammals including primatesbetween the two continents. The Vallesian crisis indicates a typical extinction and replacement of mammal species in Europe during Tortonian times following climatic upheaval and overland migrations of new species;[53] see Animation: Messinian salinity crisis (and mammal migrations), at right.\\r\\nThe near-completely enclosed configuration of the Mediterranean basin has enabled the oceanic gateways to dominate seawater circulation and the environmental evolution of the sea and basin. Circulation patterns are also affected by several other factorsincluding climate, bathymetry, and water chemistry and temperaturewhich are interactive and can induce precipitation of evaporites. Deposits of evaporites accumulated earlier in the nearby Carpathian foredeep during the Middle Miocene, and the adjacent Red Sea Basin (during the Late Miocene), and in the whole Mediterranean basin (during the MSC and the Messinian age). Diatomites are regularly found underneath the evaporite deposits, suggesting a connection between their geneses.\\r\\nToday, evaporation of surface seawater (output) is more than the supply (input) of fresh water by precipitation and coastal drainage systems, causing the salinity of the Mediterranean to be much higher than that of the Atlanticso much so that the saltier Mediterranean waters sink below the waters incoming from the Atlantic, causing a two-layer flow across the Gibraltar strait: that is, an outflow submarine current of warm saline Mediterranean water, counterbalanced by an inflow surface current of less saline cold oceanic water from the Atlantic. Herman S?rgel's Atlantropa project proposal in the 1920s proposed a hydroelectric dam to be built across the Strait of Gibraltar, using the inflow current to provide a large amount of hydroelectric energy. The underlying energy grid was as well intended to support a political union between Europe and, at least, the Marghreb part of Africa (compare Eurafrika for the later impact and Desertec for a later project with some parallels in the planned grid).[54]\\r\\nThe end of the Miocene also marked a change in the climate of the Mediterranean basin. Fossil evidence from that period reveals that the larger basin had a humid subtropical climate with rainfall in the summer supporting laurel forests. The shift to a \\"Mediterranean climate\\" occurred largely within the last three million years (the late Pliocene epoch) as summer rainfall decreased. The subtropical laurel forests retreated; and even as they persisted on the islands of Macaronesia off the Atlantic coast of Iberia and North Africa, the present Mediterranean vegetation evolved, dominated by coniferous trees and sclerophyllous trees and shrubs with small, hard, waxy leaves that prevent moisture loss in the dry summers. Much of these forests and shrublands have been altered beyond recognition by thousands of years of human habitation. There are now very few relatively intact natural areas in what was once a heavily wooded region.\\r\\nBecause of its latitudinal position and its land-locked configuration, the Mediterranean is especially sensitive to astronomically induced climatic variations, which are well documented in its sedimentary record. Since the Mediterranean is involved in the deposition of eolian dust from the Sahara during dry periods, whereas riverine detrital input prevails during wet ones, the Mediterranean marine sapropel-bearing sequences provide high-resolution climatic information. These data have been employed in reconstructing astronomically calibrated time scales for the last 9 Ma of the Earth's history, helping to constrain the time of past geomagnetic reversals.[55] Furthermore, the exceptional accuracy of these paleoclimatic records has improved our knowledge of the Earth's orbital variations in the past.\\r\\nAs a result of the drying of the sea during the Messinian salinity crisis,[56] the marine biota of the Mediterranean are derived primarily from the Atlantic Ocean. The North Atlantic is considerably colder and more nutrient-rich than the Mediterranean, and the marine life of the Mediterranean has had to adapt to its differing conditions in the five million years since the basin was reflooded.\\r\\nThe Alboran Sea is a transition zone between the two seas, containing a mix of Mediterranean and Atlantic species. The Alboran Sea has the largest population of bottlenose dolphins in the Western Mediterranean, is home to the last population of harbour porpoises in the Mediterranean, and is the most important feeding grounds for loggerhead sea turtles in Europe. The Alboran sea also hosts important commercial fisheries, including sardines and swordfish. The Mediterranean monk seals live in the Aegean Sea in Greece. In 2003, the World Wildlife Fund raised concerns about the widespread drift net fishing endangering populations of dolphins, turtles, and other marine animals such as the ogre cancer.\\r\\nFor 4,000 years, human activity has transformed most parts of Mediterranean Europe, and the \\"humanisation of the landscape\\" overlapped with the appearance of the present Mediterranean climate.[57] The image of a simplistic, environmental determinist notion of a Mediterranean Paradise on Earth in antiquity, which was destroyed by later civilisations dates back to at least the 18th century and was for centuries fashionable in archaeological and historical circles. Based on a broad variety of methods, e.g. historical documents, analysis of trade relations, floodplain sediments, pollen, tree-ring and further archaeometric analyses and population studies, Alfred Thomas Grove and Oliver Rackham's work on \\"The Nature of Mediterranean Europe\\" challenges this common wisdom of a Mediterranean Europe as a \\"Lost Eden\\", a formerly fertile and forested region, that had been progressively degraded and desertified by human mismanagement.[57] The belief stems more from the failure of the recent landscape to measure up to the imaginary past of the classics as idealised by artists, poets and scientists of the early modern Enlightenment.[57]\\r\\nThe historical evolution of climate, vegetation and landscape in southern Europe from prehistoric times to the present is much more complex and underwent various changes. For example, some of the deforestation had already taken place before the Roman age. While in the Roman age large enterprises as the Latifundiums took effective care of forests and agriculture, the largest depopulation effects came with the end of the empire. Some[who?] assume that the major deforestation took place in modern times? the later usage patterns were also quite different e.g. in southern and northern Italy. Also, the climate has usually been unstable and showing various ancient and modern \\"Little Ice Ages\\",[58] and plant cover accommodated to various extremes and became resilient with regard to various patterns of human activity.[57]\\r\\nHumanisation was therefore not the cause of climate change but followed it.[57] The wide ecological diversity typical of Mediterranean Europe is predominantly based on human behavior, as it is and has been closely related human usage patterns.[57] The diversity range was enhanced by the widespread exchange and interaction of the longstanding and highly diverse local agriculture, intense transport and trade relations, and the interaction with settlements, pasture and other land use. The greatest human-induced changes, however, came after World War II, respectively in line with the '1950s-syndrome'[59] as rural populations throughout the region abandoned traditional subsistence economies. Grove and Rackham suggest that the locals left the traditional agricultural patterns towards taking a role as scenery-setting agents for the then much more important (tourism) travellers. This resulted in more monotonous, large-scale formations.[57] Among further current important threats to Mediterranean landscapes are overdevelopment of coastal areas, abandonment of mountains and, as mentioned, the loss of variety via the reduction of traditional agricultural occupations.[57]\\r\\nThe region has a variety of geological hazards which have closely interacted with human activity and land use patterns. Among others, in the eastern Mediterranean, the Thera eruption, dated to the 17th or 16th century BC, caused a large tsunami that some experts hypothesise devastated the Minoan civilisation on the nearby island of Crete, further leading some to believe that this may have been the catastrophe that inspired the Atlantis legend.[60] Mount Vesuvius is the only active volcano on the European mainland, while others as Mount Etna and Stromboli are to be found on neighbouring islands. The region around Vesuvius including the Phlegraean Fields Caldera west of Naples are quite active[61] and constitute the most densely populated volcanic region in the world and eruptive event may occur within decades.[62]\\r\\nVesuvius itself is regarded as quite dangerous due to a tendency towards explosive (Plinian) eruptions.[63] It is best known for its eruption in AD 79 that led to the burying and destruction of the Roman cities of Pompeii and Herculaneum.\\r\\nThe large experience of member states and regional authorities has led to exchange on the international level with cooperation of NGOs, states, regional and municipality authorities and private persons.[64] The GreekÿTurkish earthquake diplomacy is a quite positive example of natural hazards leading to improved relations of traditional rivals in the region after earthquakes in ?zmir and Athens 1999. The European Union Solidarity Fund (EUSF) was set up to respond to major natural disasters and express European solidarity to disaster-stricken regions within all of Europe.[65] The largest amount of fund requests in the EU is being directed to forest fires, followed by floodings and earthquakes. Forest fires are, whether man made or natural, an often recurring and dangerous hazard in the Mediterranean region.[64] Also, tsunamis are an often underestimated hazard in the region. For example, the 1908 Messina earthquake and tsunami took more than 123,000 lives in Sicily and Calabria and is among the most deadly natural disasters in modern Europe.\\r\\nUnlike the vast multidirectional Ocean currents in open Oceans within their respective Oceanic zones; biodiversity in the Mediterranean Sea is that of a stable one due to the subtle but strong locked nature of currents which affects favorably, even the smallest macroscopic type of Volcanic Life Form. The stable Marine ecosystem of the Mediterranean Sea and sea temperature provides a nourishing environment for life in the deep sea to flourish while assuring a balanced Aquatic ecosystem excluded from any external deep oceanic factors.\\r\\nThe opening of the Suez Canal in 1869 created the first salt-water passage between the Mediterranean and Red Sea. The Red Sea is higher than the Eastern Mediterranean, so the canal serves as a tidal strait that pours Red Sea water into the Mediterranean. The Bitter Lakes, which are hyper-saline natural lakes that form part of the canal, blocked the migration of Red Sea species into the Mediterranean for many decades, but as the salinity of the lakes gradually equalised with that of the Red Sea, the barrier to migration was removed, and plants and animals from the Red Sea have begun to colonise the Eastern Mediterranean. The Red Sea is generally saltier and more nutrient-poor than the Atlantic, so the Red Sea species have advantages over Atlantic species in the salty and nutrient-poor Eastern Mediterranean. Accordingly, Red Sea species invade the Mediterranean biota, and not vice versa; this phenomenon is known as the Lessepsian migration (after Ferdinand de Lesseps, the French engineer) or Erythrean invasion. The construction of the Aswan High Dam across the Nile River in the 1960s reduced the inflow of freshwater and nutrient-rich silt from the Nile into the Eastern Mediterranean, making conditions there even more like the Red Sea and worsening the impact of the invasive species.\\r\\nInvasive species have become a major component of the Mediterranean ecosystem and have serious impacts on the Mediterranean ecology, endangering many local and endemic Mediterranean species. A first look at some groups of exotic species show that more than 70% of the non-indigenous decapods and about 63% of the exotic fishes occurring in the Mediterranean are of Indo Pacific origin,[66] introduced into the Mediterranean through the Suez Canal. This makes the Canal as the first pathway of arrival of \\"alien\\" species into the Mediterranean. The impacts of some lessepsian species have proven to be considerable mainly in the Levantine basin of the Mediterranean, where they are replacing native species and becoming a \\"familiar sight\\".\\r\\nAccording to the International Union for Conservation of Nature definition, as well as Convention on Biological Diversity (CBD) and Ramsar Convention terminologies, they are alien species, as they are non-native (non-indigenous) to the Mediterranean Sea, and they are outside their normal area of distribution which is the Indo-Pacific region. When these species succeed in establishing populations in the Mediterranean Sea, compete with and begin to replace native species they are \\"Alien Invasive Species\\", as they are an agent of change and a threat to the native biodiversity. In the context of CBD, \\"introduction\\" refers to the movement by human agency, indirect or direct, of an alien species outside of its natural range (past or present). The Suez Canal, being an artificial (man made) canal, is a human agency. Lessepsian migrants are therefore \\"introduced\\" species (indirect, and unintentional). Whatever wording is chosen, they represent a threat to the native Mediterranean biodiversity, because they are non-indigenous to this sea. In recent years, the Egyptian government's announcement of its intentions to deepen and widen the canal have raised concerns from marine biologists, fearing that such an act will only worsen the invasion of Red Sea species into the Mediterranean, facilitating the crossing of the canal for yet additional species.[67]\\r\\nIn recent decades, the arrival of exotic species from the tropical Atlantic has become a noticeable feature. Whether this reflects an expansion of the natural area of these species that now enter the Mediterranean through the Gibraltar strait, because of a warming trend of the water caused by global warming; or an extension of the maritime traffic; or is simply the result of a more intense scientific investigation, is still an open question. While not as intense as the \\"lessepsian\\" movement, the process may be scientific interest and may therefore warrant increased levels of monitoring.[citation needed]\\r\\nBy 2100 the overall level of the Mediterranean could rise between 3 to 61?cm (1.2 to 24.0?in) as a result of the effects of climate change.[68] This could have adverse effects on populations across the Mediterranean:\\r\\nCoastal ecosystems also appear to be threatened by sea level rise, especially enclosed seas such as the Baltic, the Mediterranean and the Black Sea. These seas have only small and primarily east-west movement corridors, which may restrict northward displacement of organisms in these areas.[71] Sea level rise for the next century (2100) could be between 30?cm (12?in) and 100?cm (39?in) and temperature shifts of a mere 0.05ÿ0.1?C in the deep sea are sufficient to induce significant changes in species richness and functional diversity.[72]\\r\\nPollution in this region has been extremely high in recent years.[when?] The United Nations Environment Programme has estimated that 650,000,000?t (720,000,000 short tons) of sewage, 129,000?t (142,000 short tons) of mineral oil, 60,000?t (66,000 short tons) of mercury, 3,800?t (4,200 short tons) of lead and 36,000?t (40,000 short tons) of phosphates are dumped into the Mediterranean each year.[73] The Barcelona Convention aims to 'reduce pollution in the Mediterranean Sea and protect and improve the marine environment in the area, thereby contributing to its sustainable development.'[74] Many marine species have been almost wiped out because of the sea's pollution. One of them is the Mediterranean monk seal which is considered to be among the world's most endangered marine mammals.[75]\\r\\nThe Mediterranean is also plagued by marine debris. A 1994 study of the seabed using trawl nets around the coasts of Spain, France and Italy reported a particularly high mean concentration of debris; an average of 1,935 items per km2. Plastic debris accounted for 76%, of which 94% was plastic bags.[76]\\r\\nSome of the world's busiest shipping routes are in the Mediterranean Sea. It is estimated that approximately 220,000 merchant vessels of more than 100 tonnes cross the Mediterranean Sea each yearabout one third of the world's total merchant shipping. These ships often carry hazardous cargo, which if lost would result in severe damage to the marine environment.\\r\\nThe discharge of chemical tank washings and oily wastes also represent a significant source of marine pollution. The Mediterranean Sea constitutes 0.7% of the global water surface and yet receives 17% of global marine oil pollution. It is estimated that every year between 100,000?t (98,000 long tons) and 150,000?t (150,000 long tons) of crude oil are deliberately released into the sea from shipping activities.\\r\\nApproximately 370,000,000?t (360,000,000 long tons) of oil are transported annually in the Mediterranean Sea (more than 20% of the world total), with around 250ÿ300 oil tankers crossing the sea every day. Accidental oil spills happen frequently with an average of 10 spills per year. A major oil spill could occur at any time in any part of the Mediterranean.[72]\\r\\nThe Mediterranean Sea is arguably among the most culturally diverse block basin sea regions in the world, with a unique combination of pleasant climate, beautiful coastline, rich history and various cultures. The Mediterranean region is the most popular tourist destination in the worldattracting approximately one third of the world's international tourists.[citation needed]\\r\\nTourism is one of the most important sources of income for many Mediterranean countries regardless of the man-made geopolitical conflicts that harbour coastal nations. In that regard, authorities around the Mediterranean have made it a point to extinguish rising man-made chaotic zones that would affect the economies, societies in neighboring coastal countries, let alone shipping routes. Naval and rescue components in the Mediterranean Sea are considered one of the very best due to the quick intercooperation of various Naval Fleets within proximity of each other. Unlike the vast open Oceans, the closed nature of the Mediterranean Sea provides a much more adaptable naval initiative among the coastal countries to provide effective naval and rescue missions, considered the safest and regardless of any man-made or natural disaster.\\r\\nTourism also supports small communities in coastal areas and islands by providing alternative sources of income far from urban centers. However, tourism has also played major role in the degradation of the coastal and marine environment. Rapid development has been encouraged by Mediterranean governments to support the large numbers of tourists visiting the region each year. But this has caused serious disturbance to marine habitats such as erosion and pollution in many places along the Mediterranean coasts.\\r\\nTourism often concentrates in areas of high natural wealth, causing a serious threat to the habitats of endangered Mediterranean species such as sea turtles and monk seals. Reductions in natural wealth may reduce incentives for tourists to visit.[72]\\r\\nFish stock levels in the Mediterranean Sea are alarmingly low. The European Environment Agency says that more than 65% of all fish stocks in the region are outside safe biological limits and the United Nations Food and Agriculture Organisation, that some of the most important fisheriessuch as albacore and bluefin tuna, hake, marlin, swordfish, red mullet and sea breamare threatened.[date?missing]\\r\\nThere are clear indications that catch size and quality have declined, often dramatically, and in many areas larger and longer-lived species have disappeared entirely from commercial catches.\\r\\nLarge open water fish like tuna have been a shared fisheries resource for thousands of years but the stocks are now dangerously low. In 1999, Greenpeace published a report revealing that the amount of bluefin tuna in the Mediterranean had decreased by over 80% in the previous 20 years and government scientists warn that without immediate action the stock will collapse.\\r\\nAquaculture is expanding rapidlyoften without proper environmental assessmentand currently accounts for 30% of the fish protein consumed worldwide. The industry claims that farmed seafood lessens the pressure on wild fish stocks, yet many of the farmed species are carnivorous, consuming up to five times their weight in wild fish.\\r\\nMediterranean coastal areas are already over exposed to human influence, with pristine areas becoming ever scarcer. The aquaculture sector adds to this pressure, requiring areas of high water quality to set up farms. The installation of fish farms close to vulnerable and important habitats such as seagrass meadows is particularly concerning.\\r\\nBeach of Hammamet, Tunisia\\r\\nThe beach of la Courtade in the ?les d'Hyres, France\\r\\nSardinia's south coast, Italy\\r\\nPretty Bay, Malta\\r\\nPanoramic view of Piran, Slovenia\\r\\nPanoramic view of Cavtat, Croatia\\r\\nView of Neum, Bosnia and Herzegovina\\r\\nA view of Sveti Stefan, Montenegro\\r\\nKsamil Islands, Albania\\r\\nNavagio, Greece\\r\\nMarmaris, Turquoise Coast, Turkey\\r\\nPaphos, Cyprus\\r\\nBurj Islam Beach, Latakia, Syria\\r\\nA view of Raouch off the coast of Beirut, Lebanon\\r\\nA view of Haifa, Israel\\r\\nCoast of Alexandria, view From Bibliotheca Alexandrina, Egypt\\r\\nOld city of Ibiza Town, Spain\\r\\nLes Aiguades near Bja?a, Algeria\\r\\nEl Jebha, a port town in Morocco\\r\\nEuropa Point, Gibraltar\\r\\nPanoramic view of La Condamine, Monaco\\r\\nSunset at the Deir al-Balah beach, Gaza Strip","input":"What is the deepest point in the mediterranean sea?"},{"output":"10,873","context":"Camp Verde (Yavapai: ?Matthi:wa; Western Apache: Gamb~dih[4]) is a town in Yavapai County, Arizona, United States. According to the 2010 census, the population of the city is 10,873.[2]\\r\\nThe town hosts an annual corn festival in July, sponsored and organized by Hauser and Hauser Farms. Other annual festivals include Fort Verde Days (October); the Pecan, Wine and Antiques Festival (February); and the Crawdad Festival (June).[citation needed]\\r\\n\\r\\n\\r\\nThe 42.6?sq?mi (110?km2) town is intersected by I-17, extending 8 miles (13?km) to the West and 10 miles (16?km) to the East of the interstate. Three freeway exits provide local access: Exits 285, 287, and 289. The Town's Historic Downtown is approximately 1-mile (1.6?km) from I-17 and contains a grocery store, physician facilities, shopping, dining, historical museum, Fort Verde State Historic Park, chamber of commerce/visitor center and town offices. Camp Verde is located at 34340N 1115122W? / ?34.56667N 111.85611W? / 34.56667; -111.85611 (34.566713, -111.856194).[5]\\r\\nAccording to the United States Census Bureau, the town has a total area of 42.6 square miles (110?km2), of which, 42.6 square miles (110?km2) of it is land and 0.02% is water. It is in the Verde River valley. To the southwest lie the Black Hills mountain range. Camp Verde is surrounded by Prescott National Forest. The Mogollon Rim is just north of the town and forms the southwestern edge of the large, geologically ancient Colorado Plateau.\\r\\nAs of the census[7] of 2000, there were 9,451 people, 2,611 households, and 2,538 families residing in the town. The population density was 222.0 people per square mile (85.7/km2). There were 3,969 housing units at an average density of 93.2 per square mile (36.0/km2). The racial makeup of the town was 85.05% White, 0.35% Black or African American, 7.31% Native American, 0.22% Asian, 0.14% Pacific Islander, 4.70% from other races, and 2.23% from two or more races. 10.94% of the population were Hispanic or Latino of any race.\\r\\nThere were 2,611 households out of which 27.4% had children under the age of 18 living with them, 55.9% were married couples living together, 9.8% had a female householder with no husband present, and 29.7% were non-families. 24.3% of all households were made up of individuals and 11.8% had someone living alone who was 65 years of age or older. The average household size was 2.52 and the average family size was 2.97.\\r\\nIn the town, the population was spread out with 24.0% under the age of 18, 7.2% from 18 to 24, 23.0% from 25 to 44, 25.3% from 45 to 64, and 20.5% who were 65 years of age or older. The median age was 42 years. For every 100 females there were 101.5 males. For every 100 females age 18 and over, there were 100.2 males.\\r\\nThe median income for a household in the town was $31,868, and the median income for a family was $37,049. Males had a median income of $30,104 versus $20,306 for females. The per capita income for the town was $15,072. About 9.5% of families and 14.0% of the population were below the poverty line, including 21.2% of those under age 18 and 6.1% of those age 65 or over.\\r\\nTourist attractions include the nearby Montezuma Castle National Monument located in Verde Valley. In the town is Fort Verde State Historic Park, and Out of Africa Wildlife Park. The Cliff Castle Casino, operated by the Yavapai-Apache Nation Indian tribe, is an important gambling destination for north and central Arizona. Fort Verde State Historic Park is located in Camp Verdes Historic Downtown approximately 1-mile (1.6?km) from all three Camp Verde exits.\\r\\nCamp Verde Unified School District serves the community.\\r\\nThe Marvel Comics superhero characters James and John Proudstar are from a reservation in Camp Verde.\\r\\nThe 1977 horror movie, Kingdom of the Spiders, was filmed in Camp Verde.\\r\\nIn the 2011 film Paul, Simon Pegg and Nick Frost plan to visit Camp Verde as a UFO hot spot along with Rachel, Nevada, Area 51, Apache Junction, AZ and Roswell, New Mexico.\\r\\nIn Cable #7, Camp Verde is a bunker headquarters of the X-Force.","input":"What is the population of camp verde az?"},{"output":"the Netherlands","context":"The birth control movement in the United States was a social reform campaign beginning in 1914 that aimed to increase the availability of contraception in the U.S. through education and legalization.  The movement began in 1914 when a group of political radicals in New York City, led by Emma Goldman, Mary Dennett, and Margaret Sanger, became concerned about the hardships that childbirth and self-induced abortions brought to low-income women. Sanger, in particular, simultaneously sought to connect birth control to the organized eugenics movement,  regularly appealing to the authority of eugenic scientists  Karl Pearson, Charles Davenport, and others in her Birth Control Review from the early 1920s [1] Such figures sought to prevent population segments they deemed genetically 'undesirable' from reproducing. While seeking legitimacy for the birth control movement partly through the approval of organized eugenics, Sanger and other activists also worked on the political front. Since contraception was considered to be obscene at the time, the activists targeted the Comstock laws, which prohibited distribution of any \\"obscene, lewd, and/or lascivious\\" materials through the mail. Hoping to provoke a favorable legal decision, Sanger deliberately broke the law by distributing The Woman Rebel, a newsletter containing a discussion of contraception. In 1916, Sanger opened the first birth control clinic in the United States, but the clinic was immediately shut down by police, and Sanger was sentenced to 30 days in jail.\\r\\n\\r\\nA major turning point for the movement came during World War I, when many U.S. servicemen were diagnosed with venereal diseases. The government's response included an anti-venereal disease campaign that framed sexual intercourse and contraception as issues of public health and legitimate topics of scientific research. This was the first time a U.S. government institution had engaged in a sustained, public discussion of sexual matters; as a consequence, contraception transformed from an issue of morals to an issue of public health.\\r\\n\\r\\nEncouraged by the public's changing attitudes towards birth control, Sanger opened a second birth control clinic in 1923, but this time there were no arrests or controversy. Throughout the 1920s, public discussion of contraception became more commonplace, and the term \\"birth control\\" became firmly established in the nation's vernacular. The widespread availability of contraception signaled a transition from the stricter sexual mores of the Victorian era to a more sexually permissive society.\\r\\n\\r\\nLegal victories in the 1930s continued to weaken anti-contraception laws. The court victories motivated the American Medical Association in 1937 to adopt contraception as a core component of medical school curricula, but the medical community was slow to accept this new responsibility, and women continued to rely on unsafe and ineffective contraceptive advice from ill-informed sources. In 1942, the Planned Parenthood Federation of America was formed, creating a nationwide network of birth control clinics. After World War II, the movement to legalize birth control came to a gradual conclusion, as birth control was fully embraced by the medical profession, and the remaining anti-contraception laws were no longer enforced.\\r\\n\\r\\nThe practice of birth control was common throughout the U.S. prior to 1914, when the movement to legalize contraception began. Longstanding techniques included the rhythm method, withdrawal, diaphragms, contraceptive sponges, condoms, prolonged breastfeeding, and spermicides.[2] Use of contraceptives increased throughout the nineteenth century, contributing to a 50 percent drop in the fertility rate in the United States between 1800 and 1900, particularly in urban regions.[3] The only known survey conducted during the nineteenth century of American women's contraceptive habits was performed by Clelia Mosher from 1892 to 1912.[4] The survey was based on a small sample of upper-class women, and shows that most of the women used contraception (primarily douching, but also withdrawal, rhythm, condoms and pessaries) and that they viewed sex as a pleasurable act that could be undertaken without the goal of procreation.[5]\\r\\n\\r\\nAlthough contraceptives were relatively common in middle-class and upper-class society, the topic was rarely discussed in public.[6] The first book published in the United States which ventured to discuss contraception was Moral Physiology; or, A Brief and Plain Treatise on the Population Question, published by Robert Dale Owen in 1831.[7] The book suggested that family planning was a laudable effort, and that sexual gratification?ÿ without the goal of reproduction?ÿ was not immoral.[8] Owen recommended withdrawal, but he also discussed sponges and condoms.[9] That book was followed by Fruits of Philosophy: The Private Companion of Young Married People, written in 1832 by Charles Knowlton, which recommended douching.[10] Knowlton was prosecuted in Massachusetts on obscenity charges, and served three months in prison.[11]\\r\\n\\r\\nBirth control practices were generally adopted earlier in Europe than in the United States. Knowlton's book was reprinted in 1877 in England by Charles Bradlaugh and Annie Besant, with the goal of challenging Britain's obscenity laws.[12] They were arrested (and later acquitted) but the publicity of their trial contributed to the formation, in 1877, of the Malthusian League?ÿ the world's first birth control advocacy group?ÿ which sought to limit population growth to avoid Thomas Malthus's dire predictions of exponential population growth leading to worldwide poverty and famine.[13] By 1930, similar societies had been established in nearly all European countries, and birth control began to find acceptance in most Western European countries, except Catholic Ireland, Spain, and France.[14] As the birth control societies spread across Europe, so did birth control clinics. The first birth control clinic in the world was established in the Netherlands in 1882, run by the Netherlands' first female physician, Aletta Jacobs.[15] The first birth control clinic in England was established in 1921 by Marie Stopes, in London.[16]\\r\\n\\r\\nContraception was legal in the United States throughout most of the 19th century, but in the 1870s a social purity movement grew in strength, aimed at outlawing vice in general, and prostitution and obscenity in particular.[17] Composed primarily of Protestant moral reformers and middle-class women, the Victorian-era campaign also attacked contraception, which was viewed as an immoral practice that promoted prostitution and venereal disease.[18] Anthony Comstock, a postal inspector and leader in the purity movement, successfully lobbied for the passage of the 1873 Comstock Act, a federal law prohibiting mailing of \\"any article or thing designed or intended for the prevention of conception or procuring of abortion\\" as well as any form of contraceptive information.[19] Many states also passed similar state laws (collectively known as the Comstock laws), sometimes extending the federal law by outlawing the use of contraceptives, as well as their distribution. Comstock was proud of the fact that he was personally responsible for thousands of arrests and the destruction of hundreds of tons of books and pamphlets.[20]\\r\\n\\r\\nComstock and his allies also took aim at the libertarians and utopians who comprised the free love movement?ÿ an initiative to promote sexual freedom, equality for women, and abolition of marriage.[21] The free love proponents were the only group to actively oppose the Comstock laws in the 19th century, setting the stage for the birth control movement.[22]\\r\\n\\r\\nThe efforts of the free love movement were not successful and, at the beginning of the 20th century, federal and state governments began to enforce the Comstock laws more rigorously.[22] In response, contraception went underground, but it was not extinguished. The number of publications on the topic dwindled, and advertisements, if they were found at all, used euphemisms such as \\"marital aids\\" or \\"hygienic devices\\". Drug stores continued to sell condoms as \\"rubber goods\\" and cervical caps as \\"womb supporters\\".[23]\\r\\n\\r\\nAt the turn of the century, an energetic movement arose, centered in Greenwich Village, that sought to overturn bans on free speech.[24] Supported by radicals, feminists, anarchists, and atheists such as Ezra Heywood, Moses Harman, D. M. Bennett, and Emma Goldman, these activists regularly battled anti-obscenity laws and, later, the government's effort to suppress speech critical of involvement in World War I.[25] Prior to 1914, the free speech movement focused on politics, and rarely addressed contraception.[26]\\r\\n\\r\\nGoldman's circle of radicals, socialists, and bohemians was joined in 1912 by a nurse, Margaret Sanger, whose mother had been through 18 pregnancies in 22 years, and died at age 50 of tuberculosis and cervical cancer.[27] In 1913, Sanger worked in New York's Lower East Side, often with poor women who were suffering due to frequent childbirth and self-induced abortions.[28] After one particularly tragic medical case, Sanger wrote: \\"I threw my nursing bag in the corner and announced?... that I would never take another case until I had made it possible for working women in America to have the knowledge to control birth.\\"[29] Sanger visited public libraries, searching for information on contraception, but nothing was available.[30] She became outraged that working-class women could not obtain contraception, yet upper-class women who had access to private physicians could.[31]\\r\\n\\r\\nUnder the influence of Goldman and the Free Speech League, Sanger became determined to challenge the Comstock laws that outlawed the dissemination of contraceptive information.[32] With that goal in mind, in 1914 she launched The Woman Rebel, an eight-page monthly newsletter which promoted contraception using the slogan \\"No Gods, No Masters\\",[33] and proclaimed that each woman should be \\"the absolute mistress of her own body.\\"[34] Sanger coined the term birth control, which first appeared in the pages of Rebel, as a more candid alternative to euphemisms such as family limitation.[35]\\r\\n\\r\\nSanger's goal of challenging the law was fulfilled when she was indicted in August 1914, but the prosecutors focused their attention on articles Sanger had written on assassination and marriage, rather than contraception.[36] Afraid that she might be sent to prison without an opportunity to argue for birth control in court, she fled to England to escape arrest.[37]\\r\\n\\r\\nWhile Sanger was in Europe, her husband continued her work, which led to his arrest after he distributed a copy of a birth control pamphlet to an undercover postal worker.[38] The arrest and his 30-day jail sentence prompted several mainstream publications, including Harper's Weekly and the New-York Tribune, to publish articles about the birth control controversy.[39] Emma Goldman and Ben Reitman toured the country, speaking in support of the Sangers, and distributing copies of Sanger's pamphlet Family Limitation.[40] Sanger's exile and her husband's arrest propelled the birth control movement into the forefront of American news.[41]\\r\\n\\r\\nIn the spring of 1915 supporters of the Sangers?ÿ led by Mary Dennett?ÿ formed the National Birth Control League (NBCL), which was the first American birth control organization.[42] Throughout 1915, smaller regional organizations were formed in San Francisco, Portland, Oregon, Seattle, and Los Angeles.[43]\\r\\n\\r\\nSanger returned to the United States in October 1915. She planned to open a birth control clinic modeled on the world's first such clinic, which she had visited in Amsterdam. She first had to fight the charges outstanding against her.[44] Noted attorney Clarence Darrow offered to defend Sanger free of charge but, bowing to public pressure, the government dropped the charges early in 1916.[45] No longer under the threat of jail, Sanger embarked on a successful cross-country speaking tour, which catapulted her into the leadership of the U.S. birth control movement.[46] Other leading figures, such as William J. Robinson and Mary Dennett, chose to work in the background, or turned their attention to other causes.[46] Later in 1916, Sanger traveled to Boston to lend her support to the Massachusetts Birth Control League and to jailed birth control activist Van Kleeck Allison.[47]\\r\\n\\r\\nDuring Sanger's 1916 speaking tour, she promoted birth control clinics based on the Dutch model she had observed during her 1914 trip to Europe. Although she inspired many local communities to create birth control leagues, no clinics were established.[48] Sanger therefore resolved to create a birth control clinic in New York that would provide free contraceptive services to women.[49] New York state law prohibited the distribution of contraceptives or even contraceptive information, but Sanger hoped to exploit a provision in the law which permitted doctors to prescribe contraceptives for the prevention of disease.[50] On October 16, 1916, she, partnering with Fania Mindell and Ethel Byrne, opened the Brownsville clinic in Brooklyn. The clinic was an immediate success, with over 100 women visiting on the first day.[51] A few days after opening, an undercover policewoman purchased a cervical cap at the clinic, and Sanger was arrested. Refusing to walk, Sanger and a co-worker were dragged out of the clinic by police officers.[52] The clinic was shut down, and it was not until 1923 that another birth control clinic was opened in the United States.[53]\\r\\n\\r\\nSanger's trial began in January 1917. She was supported by a large number of wealthy and influential women who came together to form the Committee of One Hundred, which was devoted to raising funds for Sanger and the NBCL.[54] The committee also started publishing the monthly journal Birth Control Review, and established a network of connections to powerful politicians, activists, and press figures.[55] Despite the strong support, Sanger was convicted; the judge offered a lenient sentence if she promised not to break the law again, but Sanger replied \\"I cannot respect the law as it exists today.\\"[56] She served a sentence of 30 days in jail.[56]\\r\\n\\r\\nIn protest to her arrest as well, Byrne was sentenced to 30 days in jail at Blackwells Island Prison and responded to her situation with a hunger strike protest. With no signs of ending her demonstration anytime soon, Byrne was force fed by prison guards. Weakened and ill, Byrne refused to end her hunger strike at the cost of securing early release from prison. However, Sanger accepted the plea bargain on her sisters behalf, agreeing that Byrne would be released early from prison if she ended her birth control activism. Horrified, Byrnes relationship quickly eroded with her sister and, both forcefully and willingly, she left the birth control movement. Due to the drama of Byrnes demonstration, the birth control movement became a headline news story in which the organizations purpose was distributed across the country.[57]\\r\\n\\r\\nOther activists were also pushing for progress. Emma Goldman was arrested in 1916 for circulating birth control information,[58] and Abraham Jacobi unsuccessfully tried to persuade the New York medical community to push for a change in law to permit physicians to dispense contraceptive information.[59]\\r\\n\\r\\nThe publicity from Sanger's trial and Byrne's hunger strike generated immense enthusiasm for the cause, and by the end of 1917 there were over 30 birth control organizations in the United States.[61] Sanger was always astute about public relations, and she seized on the publicity of the trial to advance her causes.[62] After her trial, she emerged as the movement's most visible leader.[63] Other leaders, such as William J. Robinson, Mary Dennett, and Blanche Ames Ames, could not match Sanger's charisma, charm and fervor.[64]\\r\\n\\r\\nThe movement was evolving from radical, working-class roots into a campaign backed by society women and liberal professionals.[65] Sanger and her fellow advocates began to tone down their radical rhetoric and instead emphasized the socioeconomic benefits of birth control, a policy which led to increasing acceptance by mainstream Americans.[66] Media coverage increased, and several silent motion pictures produced in the 1910s featured birth control as a theme (including Birth Control, produced by Sanger and starring herself).[67]\\r\\n\\r\\nOpposition to birth control remained strong: state legislatures refused to legalize contraception or the distribution of contraceptive information;[68] religious leaders spoke out, attacking women who would choose \\"ease and fashion\\" over motherhood;[69] and eugenicists were worried that birth control would exacerbate the birth rate differential between \\"old stock\\" white Americans and \\"coloreds\\" or immigrants.[70]\\r\\n\\r\\n\\r\\nSanger formed the New York Woman's Publishing Company (NYWPC) in 1918 and, under its auspices, became the publisher for the Birth Control Review.[71] British suffragette activist Kitty Marion, standing on New York street corners, sold the Review at 20 cents per copy, enduring death threats, heckling, spitting, physical abuse, and police harassment. Over the course of the following ten years, Marion was arrested nine times for her birth control advocacy.[72]\\r\\nSanger appealed her 1917 conviction and won a mixed victory in 1918 in a unanimous decision by the New York Court of Appeals written by Judge Frederick E. Crane. The court's opinion upheld her conviction, but indicated that the courts would be willing to permit contraception if prescribed by doctors.[73] This decision was only applicable within New York, where it opened the door for birth control clinics, under physician supervision, to be established.[74] Sanger herself did not immediately take advantage of the opportunity, wrongly expecting that the medical profession would lead the way; instead she focused on writing and lecturing.[75]\\r\\n\\r\\nThe birth control movement received an unexpected boost during World War I, as a result of a crisis the U.S. military experienced when many of its soldiers were diagnosed with syphilis or gonorrhea. The military undertook an extensive education campaign, focusing on abstinence, but also offering some contraceptive guidance.[76] The military, under pressure from purity advocates, did not distribute condoms, or even endorse their use, making the U.S. the only military force in World War I that did not supply condoms to its troops. When U.S. soldiers were in Europe, they found rubber condoms readily available, and when they returned to America, they continued to use condoms as their preferred method of birth control.[77]\\r\\n\\r\\nThe military's anti-venereal disease campaign marked a major turning point for the movement: it was the first time a government institution had engaged in a sustained, public discussion of sexual matters.[78] The government's public discourse changed sex from a secret topic into a legitimate topic of scientific research, and it transformed contraception from an issue of morals to an issue of public health.[79]\\r\\n\\r\\nIn 1917, advocate Emma Goldman was arrested for protesting World War 1 and American military conscription. Goldmans commitment to free speech on topics such as socialism, anarchism, birth control, labor/union rights, and free love eventually cost her American citizenship and the right to live in the United States. Due to her commitment to socialist welfare and anti-capitalism, Goldman was associated with communism which led to her expulsion from the country during the First Red Scare.[80] While World War 1 led to a breakthrough on American acceptance of birth control relating to public health, anti-communist WW1 propaganda sacrificed one of the birth control movement's most dedicated members.\\r\\n\\r\\nWhile an important birth control activist and leader, Mary Dennett advocated for a wide variety of organizations. Starting as a field secretary for the Massachusetts Womens Suffrage Association, she worked her way up to win an elected seat as a corresponding secretary for the National American Womens Suffrage Association. Dennett headed the literary department, undertaking assignments such as distributing pamphlets and leaflets. Following disillusionment with the NAWSAs organizational structure, Dennett, as described above, helped found the National Birth Control League. The NBCL took a strong stance against militant protest strategies and instead focused attention on legislation changes at both the state and federal level.[81] During World War I, Mary Dennett focused her efforts on the peace movement, but she returned to the birth control movement in 1918.[82] She continued to lead the NBCL, and collaborated with Sanger's NYWPC. In 1919, Dennett published a widely distributed educational pamphlet, The Sex Side of Life, which treated sex as a natural and enjoyable act.[83] However, in the same year, frustrated with the NBCL's chronic lack of funding, Dennett broke away and formed the Voluntary Parenthood League (VPL).[84] Both Dennett and Sanger proposed legislative changes that would legalize birth control, but they took different approaches: Sanger endorsed contraception but only under a physician's supervision; Dennett pushed for unrestricted access to contraception.[85] Sanger, a proponent of diaphragms, was concerned that unrestricted access would result in ill-fitting diaphragms and would lead to medical quackery.[86] Dennett was concerned that requiring women to get prescriptions from physicians would prevent poor women from receiving contraception, and she was concerned about a shortage of physicians trained in birth control.[85] Both legislative initiatives failed, partly because some legislators felt that fear of pregnancy was the only thing that kept women chaste.[87] In the early 1920s, Sanger's leadership position in the movement solidified because she gave frequent public lectures, and because she took steps to exclude Dennett from meetings and events.[88]\\r\\n\\r\\nAmerican Birth Control League founding statement[89]\\r\\n\\r\\nAlthough Sanger was busy publishing the Birth Control Review during the years 1919ÿ1920, she was not formally affiliated with either of the major birth control organizations (NBCL or VPL) during that time. In 1921 she became convinced that she needed to associate with a formal body to earn the support of professional societies and the scientific community. Rather than join an existing organization, she considered creating a new one.[90] As a first step, she organized the First American Birth Control Conference, held in November 1921 in New York City. On the final night of the conference, as Sanger prepared to give a speech in the crowded Town Hall theater, police raided the meeting and arrested her for disorderly conduct. From the stage she shouted: \\"we have a right to hold [this meeting] under the Constitution?... let them club us if they want to.\\"[91]  She was soon released.[91] The following day it was revealed that Patrick Joseph Hayes, the Archbishop of New York, had pressured the police to shut down the meeting.[92] The Town Hall raid was a turning point for the movement: opposition from the government and medical community faded, and the Catholic Church emerged as its most vocal opponent.[93] After the conference, Sanger and her supporters established the American Birth Control League (ABCL).[94]\\r\\n\\r\\nFour years after the New York Court of Appeals opened the doors for physicians to prescribe contraceptives, Sanger opened a second birth control clinic, which she staffed with physicians to make it legal under that court ruling (the first clinic had employed nurses).[95] This second clinic, the Clinical Research Bureau (CRB), opened on January 2, 1923.[96] To avoid police harassment the clinic's existence was not publicized, its primary mission was stated to be conducting scientific research, and it only provided services to married women.[97] The existence of the clinic was finally announced to the public in December 1923, but this time there were no arrests or controversy. This convinced activists that, after ten years of struggle, birth control had finally become widely accepted in the United States.[98] The CRB was the first legal birth control clinic in the United States, and quickly grew into the world's leading contraceptive research center.[98]\\r\\n\\r\\nFollowing the successful opening of the CRB in 1923, public discussion of contraception became more commonplace, and the term \\"birth control\\" became firmly established in the nation's vernacular.[99] Of the hundreds of references to birth control in magazines and newspapers of the 1920s, more than two-thirds were favorable.[100] The availability of contraception signaled the end of the stricter morality of the Victorian era, and ushered in the emergence of a more sexually permissive society.[100] Other factors that contributed to the new sexual norms included increased mobility brought by the automobile, anonymous urban lifestyles, and post-war euphoria.[100] Sociologists who surveyed women in Muncie, Indiana in 1925 found that all the upper class women approved of birth control, and more than 80 percent of the working class women approved.[101] The birth rate in America declined 20 percent between 1920 and 1930, primarily due to increased use of birth control.[102]\\r\\n\\r\\nAlthough clinics became more common in the late 1920s, the movement still faced significant challenges:  Large sectors of the medical community were still resistant to birth control;  birth control advocates were blacklisted by the radio industry; and state and federal laws?ÿ though generally not enforced?ÿ still outlawed contraception.[103]\\r\\n\\r\\nThe most significant opponent to birth control was the Catholic Church, which mobilized opposition in many venues during the 1920s.[104] Catholics persuaded the Syracuse city council to ban Sanger from giving a speech in 1924; the National Catholic Welfare Conference lobbied against birth control; the Knights of Columbus boycotted hotels that hosted birth control events; the Catholic police commissioner of Albany prevented Sanger from speaking there; the Catholic mayor of Boston, James Curley, blocked Sanger from speaking in public; and several newsreel companies, succumbing to pressure from Catholics, refused to cover stories related to birth control.[105] The ABCL turned some of the boycotted speaking events to their advantage by inviting the press, and the resultant news coverage often generated public sympathy for their cause.[106] However, Catholic lobbying was particularly effective in the legislative arena, where their arguments?ÿ that contraception was unnatural, harmful, and indecent?ÿ impeded several initiatives, including an attempt in 1924 by Mary Dennett to overturn federal anti-contraception laws.[107]\\r\\n\\r\\nDozens of birth control clinics opened across the United States during the 1920s, but not without incident.[108] In 1929, New York police raided a clinic in New York and arrested two doctors and three nurses for distributing contraceptive information that was unrelated to the prevention of disease.[109] The ABCL achieved a major victory in the trial, when the judge ruled that use of contraceptives to space births farther apart was a legitimate medical treatment that benefited the health of the mother.[110] The trial, in which many prominent physicians served as witnesses for the defense, brought a large segment of the medical community onto the side of birth control advocates.[111]\\r\\n\\r\\nBefore the advent of the birth control movement, eugenics had become very popular in Europe and the U.S., and the subject was widely discussed in articles, movies, and lectures.[112] Eugenicists had mixed feelings about birth control: they worried that it would exacerbate the birth rate differential between \\"superior\\" and \\"inferior\\" races, but they also recognized its value as a tool to \\"racial betterment\\".[113] Leaders of the birth control movement never considered eugenics to be their primary goal, focusing instead on free speech and women's rights, but around 1920 they started to make common cause with eugenicists, hoping to broaden the support base of the birth control movement.[114] Eugenics buttressed the birth control movement's aims by correlating excessive births with increased poverty, crime and disease.[115] Sanger published two books in the early 1920s that endorsed eugenics: Woman and the New Race and The Pivot of Civilization.[116] Sanger and other advocates endorsed negative eugenics (discouraging procreation of \\"inferior\\" persons), but did not advocate euthanasia or positive eugenics (encouraging procreation of \\"superior\\" persons).[117]  However, many eugenicists refused to support the birth control movement because of Sanger's insistence that a woman's primary duty was to herself, not to the state.[118]\\r\\n\\r\\nLike many white Americans in the U.S. in the 1930s, some leaders of the birth control movement believed that lighter-skinned races were superior to darker-skinned races.[119] They assumed that African Americans were intellectually backward, would be relatively incompetent in managing their own health, and would require special supervision from whites.[120] The dominance of whites in the movement's leadership and medical staff resulted in accusations of racism from blacks and suspicions that \\"race suicide\\" would be a consequence of large scale adoption of birth control.[121] These suspicions were misinterpreted by some of the white birth control advocates as lack of interest in contraception.[122]\\r\\n\\r\\nIn spite of these suspicions, many African-American leaders supported efforts to supply birth control to the African-American community. In 1929, James H. Hubert, a black social worker and leader of New York's Urban League, asked Sanger to open a clinic in Harlem.[124] Sanger secured funding from the Julius Rosenwald Fund and opened the clinic, staffed with African-American doctors, in 1930.[125] The clinic was guided by a 15-member advisory board consisting of African-American doctors, nurses, clergy, journalists, and social workers.[126] It was publicized in the African-American press and African-American churches, and received the approval of W. E. B. Du Bois, co-founder of the National Association for the Advancement of Colored People (NAACP).[126] In the early 1940s, the Birth Control Federation of America (BCFA) initiated a program called the Negro Project, managed by its Division of Negro Service (DNS).[127] As with the Harlem clinic, the primary aim of the DNS and its programs was to improve maternal and infant health.[128] Based on her work at the Harlem clinic, Sanger suggested to the DNS that African Americans were more likely to take advice from a doctor of their own race, but other leaders prevailed and insisted that whites be employed in the outreach efforts.[129] The discriminatory actions and statements by the movement's leaders during the 1920s and 1930s have led to continuing allegations that the movement was racist.[130]\\r\\n\\r\\nTwo important legal decisions in the 1930s helped increase the accessibility of contraceptives. In 1930, two condom manufacturers sued each other in the Youngs Rubber case, and the judge ruled that contraceptive manufacturing was a legitimate business enterprise. He went further, and declared that the federal law prohibiting the mailing of condoms was not legally sound.[131] Sanger precipitated a second legal breakthrough when she ordered a diaphragm from Japan in 1932, hoping to provoke a decisive battle in the courts.[132] The diaphragm was confiscated by the U.S. government, and Sanger's subsequent legal challenge led to the 1936 One Package legal ruling by Judge Augustus Hand. His decision overturned an important provision of the anti-contraception laws that prohibited physicians from obtaining contraceptives.[133] This court victory motivated the American Medical Association in 1937 to finally adopt contraception as a normal medical service and a core component of medical school curricula.[134] However, the medical community was slow to accept this new responsibility, and women continued to rely on unsafe and ineffective contraceptive advice from ill-informed sources until the 1960s.[135]\\r\\n\\r\\nBy 1938, over 400 contraceptive manufacturers were in business, over 600 brands of female contraceptives were available, and industry revenues exceeded $250?million per year.[136] Condoms were sold in vending machines in some public restrooms, and men spent twice as much on condoms as on shaving.[137] Although condoms had become commonplace in the 1930s, feminists in the movement felt that birth control should be the woman's prerogative, and they continued to push for development of a contraceptive that was under the woman's control, a campaign which ultimately led to the birth control pill decades later.[138] To increase the availability of high-quality contraceptives, birth control advocates established the HollandÿRantos company to manufacture contraceptives?ÿ primarily diaphragms, which were Sanger's recommended method.[139] By the 1930s, the diaphragm with spermicidal jelly had become the most commonly prescribed form of contraception;[140] in 1938, female contraceptives accounted for 85 percent of annual contraceptive sales.[141]\\r\\n\\r\\nThe 1936 One Package court battle brought together two birth control organizations?ÿ the ABCL and the Birth Control Clinical Research Bureau (formerly the CRB)?ÿ who had joined forces to craft the successful defense effort.[142] Leaders of both groups viewed this as an auspicious time to merge the two organizations, so, in 1937, the Birth Control Council of America, under the leadership of Sanger, was formed to effect a consolidation.[143] The effort eventually led to the merger of the two organizations in 1939 as the Birth Control Federation of America (BCFA).[144] Although Sanger continued in the role of president, she no longer wielded the same power as she had in the early years of the movement, and, in 1942, more conservative forces within the organization changed the name to Planned Parenthood Federation of America, a name Sanger objected to because she considered it too euphemistic.[145] After World War II, the leadership of Planned Parenthood de-emphasized radical feminism and shifted focus to more moderate themes such as family planning and population policy.[146]\\r\\n\\r\\nThe movement to legalize birth control came to a gradual conclusion around the time Planned Parenthood was formed.[147] In 1942, there were over 400 birth control organizations in America, contraception was fully embraced by the medical profession, and the anti-contraception Comstock laws (which still remained on the books) were rarely enforced.[148]\\r\\n\\r\\nAfter World War II advocacy for reproductive rights transitioned into a new era which focused on abortion, public funding, and insurance coverage.[149]\\r\\n\\r\\nBirth control advocacy also took on a global aspect as organizations around the world began to collaborate. In 1946, Sanger helped found the International Committee on Planned Parenthood, which evolved into the International Planned Parenthood Federation and soon became the world's largest non-governmental international family planning organization.[150] In 1952, John D. Rockefeller III founded the influential Population Council.[151] Fear of global overpopulation became a major issue in the 1960s, generating concerns about pollution, food shortages, and quality of life, leading to well-funded birth control campaigns around the world.[152] The 1994 International Conference on Population and Development and the 1995 Fourth World Conference on Women addressed birth control and influenced human rights declarations which asserted women's rights to control their own bodies.[153]\\r\\n\\r\\nIn the early 1950s in the United States, philanthropist Katharine McCormick provided funding for biologist Gregory Pincus to develop the birth control pill, which was approved by the Food and Drug Administration (FDA) in 1960.[154] The pill became very popular and had a major impact on society and culture. It contributed to a sharp increase in college attendance and graduation rates for women.[155]  New forms of intrauterine devices were introduced in the 1960s, increasing popularity of long acting reversible contraceptives.[156]\\r\\n\\r\\nIn 1965, the Supreme Court ruled in Griswold v. Connecticut that it was unconstitutional for the government to prohibit married couples from using birth control.\\r\\n\\r\\nIn 1967 activist Bill Baird was arrested for distributing a contraceptive foam and a condom to a student during a lecture on birth control and abortion at Boston University. Baird's appeal of his conviction resulted in the United States Supreme Court case Eisenstadt v. Baird (1972), which extended the Griswold holding to unmarried couples, and thereby legalized birth control for all Americans.[157]\\r\\n\\r\\nIn 1970, Congress finally removed references to contraception from federal anti-obscenity laws[158]; and in 1973, the Roe v. Wade decision legalized abortion during the first trimester of pregnancy.[159]\\r\\n\\r\\nAlso in 1970, Title X of the Public Health Service Act was enacted as part of the war on poverty, to make family planning and preventive health services available to low-income and the uninsured.[160]  Without publicly funded family planning services, according to the Guttmacher Institute, the number of unintended pregnancies and abortions in the United States would be nearly two-thirds higher; the number of unintended pregnancies among poor women would nearly double.[161]  According to the United States Department of Health and Human Services, publicly funded family planning saves nearly $4 in Medicaid expenses for every $1 spent on services.[162]\\r\\n\\r\\nIn 1982, European drug manufacturers developed mifepristone, which was initially utilized as a contraceptive, but is now generally prescribed with a prostoglandin to induce abortion in pregnancies up to the fourth month of gestation.[163] To avoid consumer boycotts organized by anti-abortion organizations, the manufacturer donated the U.S. manufacturing rights to Danco Laboratories, a company formed by pro-choice advocates, with the sole purpose of distributing mifepristone in the U.S, and thus immune to the effects of boycotts.[164]\\r\\n\\r\\nIn 1997, the FDA approved a prescription emergency contraception pill (known as the morning-after pill), which became available over the counter in 2006.[165] In 2010, ulipristal acetate, a more effective emergency contraceptive was approved for use up to five days after unprotected sexual intercourse.[166]  Fifty to sixty percent of abortion patients became pregnant in circumstances in which emergency contraceptives could have been used.[167]  These emergency contraceptives, including Plan B and EllaOne, proved to be  another battleground in the war over reproductive rights.[168] Opponents of emergency contraception consider it a form of abortion, because it may interfere with the ability of a fertilized embryo to implant in the uterus; while proponents contend that it is not abortion, because the absence of implantation means that pregnancy never commenced.[169]\\r\\n\\r\\nIn 2000, the Equal Employment Opportunity Commission ruled that companies that provided insurance for prescription drugs to their employees but excluded birth control were violating the Civil Rights Act of 1964.[170]\\r\\n\\r\\nPresident Obama signed the Patient Protection and Affordable Care Act (ACA) on 23 March 2010. As of 1 August 2011, female contraception was added to a list of preventive services covered by the ACA that would be provided without patient co-payment. The federal mandate applied to all new health insurance plans in all states from 1 August 2012.[171][172] Grandfathered plans did not have to comply unless they changed substantially.[173] To be grandfathered, a group plan must have existed or an individual plan must have been sold before President Obama signed the law; otherwise they were required to comply with the new law.[174] The Guttmacher Institute noted that even before the federal mandate was implemented, twenty-eight states had their own mandates that required health insurance to cover the prescription contraceptives, but the federal mandate innovated by forbidding insurance companies from charging part of the cost to the patient.[175]\\r\\n\\r\\nBurwell v. Hobby Lobby, 573 U.S. ___ (2014), is a landmark decision[176][177] by the United States Supreme Court allowing closely held for-profit corporations to be exempt from a law its owners religiously object to if there is a less restrictive means of furthering the law's interest. It is the first time that the court has recognized a for-profit corporation's claim of religious belief,[178] but it is limited to closely held corporations.[a] The decision is an interpretation of the Religious Freedom Restoration Act (RFRA) and does not address whether such corporations are protected by the free-exercise of religion clause of the First Amendment of the Constitution. For such companies, the Court's majority directly struck down the contraceptive mandate  under the Affordable Care Act (ACA) by a 5ÿ4 vote.[179] The court said that the mandate was not the least restrictive way to ensure access to contraceptive care, noting that a less restrictive alternative was being provided for religious non-profits, until the Court issued an injunction 3 days later, effectively ending said alternative, replacing it with a government-sponsored alternative for any female employees of closely held corporations that do not wish to provide birth control.[180]\\r\\n\\r\\nZubik v. Burwell was a case before the United States Supreme Court on whether religious institutions other than churches should be exempt from the contraceptive mandate. Churches were already exempt.[181] On May 16, 2016, the U.S. Supreme Court issued a per curiam ruling in Zubik v. Burwell that vacated the decisions of the Circuit Courts of Appeals and remanded the case \\"to the respective United States Courts of Appeals for the Third, Fifth, Tenth, and D.C. Circuits\\" for reconsideration in light of the \\"positions asserted by the parties in their supplemental briefs\\".[182] Because the Petitioners agreed that \\"their religious exercise is not infringed where they 'need to do nothing more than contract for a plan that does not include coverage for some or all forms of contraception'\\", the Court held that the parties should be given an opportunity to clarify and refine how this approach would work in practice and to \\"resolve any outstanding issues\\".[183] The Supreme Court expressed \\"no view on the merits of the cases.\\"[184]  In a concurring opinion, Justice Sotomeyer, joined by Justice Ginsburg noted that in earlier cases \\"some lower courts have ignored those instructions\\" and cautioned lower courts not to read any signals in the Supreme Court's actions in this case.[185]\\r\\n\\r\\nIn 2017, the Trump administration issued a ruling letting insurers and employers refuse to provide birth control if doing so went against their \\"religious beliefs\\" or \\"moral convictions\\".[186] However, later that same year federal judge Wendy Beetlestone issued an injunction temporarily stopping the enforcement of the Trump administration ruling.[187]","input":"Where did world's first 'birth control clinic' open?"},{"output":"Christianity","context":"Religion in the United States is characterized by a diversity of religious beliefs and practices. Various religious faiths have flourished within the United States. A majority of Americans report that religion plays a very important role in their lives, a proportion unique among developed countries.[1]\\r\\nHistorically, the United States has always been marked by religious pluralism and diversity, beginning with various native beliefs of the pre-colonial time. In colonial times, Anglicans, Catholics and mainline Protestants, as well as Jews, arrived from Europe. Eastern Orthodoxy has been present since the Russian colonization of Alaska. Various dissenting Protestants, who left the Church of England, greatly diversified the religious landscape. The Great Awakenings gave birth to multiple Evangelical Protestant denominations; membership in Methodist and Baptist churches increased drastically in the Second Great Awakening. In the 18th century, deism found support among American upper classes and thinkers. The Episcopal Church, splitting from the Church of England, came into being in the American Revolution. New Protestant branches like Adventism emerged; Restorationists and other Christians like the Jehovah's Witnesses, the Latter Day Saint movement, Churches of Christ and Church of Christ, Scientist, as well as Unitarian and Universalist communities all spread in the 19th century. Pentecostalism emerged in the early 20th century as a result of the Azusa Street Revival. Scientology emerged in the 1950s. Unitarian Universalism resulted from the merge of Unitarian and Universalist churches in the 20th century. Beginning in 1990s, the religious share of Christians is decreasing due to secularization, while Buddhism, Hinduism, Islam, and other religions are spreading. Protestantism, historically dominant, ceased to be the religious category of the majority in the early 2010s.\\r\\nThe majority of U.S. adults self-identify as Christians, while close to a quarter claim no religious affiliation.[2] According to a 2014 study by the Pew Research Center, 70.6% of the adult population identified themselves as Christians, with 46.5% professing attendance at a variety of churches that could be considered Protestant, and 20.8% professing Catholic beliefs. The same study says that other religions (including Judaism, Buddhism, Hinduism, and Islam) collectively make up about 6% of the population. According to a 2012 survey by the Pew forum, 36% of U.S. adults state that they attend services nearly every week or more.[3] According to a 2016 Gallup poll, Mississippi (with 63% of its adult population described as very religious, saying that religion is important to them and attending religious services almost every week) is the most religious state in the country, while New Hampshire (with only 20% of its adult population described as very religious) is the least religious state.[4]\\r\\nFrom early colonial days, when some English and German settlers came in search of religious freedom, America has been profoundly influenced by religion.[5] That influence continues in American culture, social life, and politics.[6] Several of the original Thirteen Colonies were established by settlers who wished to practice their own religion within a community of like-minded people: the Massachusetts Bay Colony was established by English Puritans (Congregationalists), Pennsylvania by British Quakers, Maryland by English Catholics, and Virginia by English Anglicans. Despite these, and as a result of intervening religious strife and preference in England[7] the Plantation Act 1740 would set official policy for new immigrants coming to British America until the American Revolution.\\r\\nThe text of the First Amendment to the country's Constitution states that \\"Congress shall make no law respecting an establishment of religion, or prohibiting the free exercise thereof; or abridging the freedom of speech, or of the press; or the right of the people peaceably to assemble, and to petition the Government for a redress of grievances.\\" It guarantees the free exercise of religion while also preventing the government from establishing a state religion. However the states were not bound by the provision and as late as the 1830s Massachusetts provided tax money to local Congregational churches.[8] The Supreme Court since the 1940s has interpreted the Fourteenth Amendment as applying the First Amendment to the state and local governments.\\r\\nPresident John Adams and a unanimous Senate endorsed the Treaty of Tripoli in 1797 that stated: \\"the Government of the United States of America is not, in any sense, founded on the Christian religion.\\"[9]\\r\\nGoing forward from its foundation, the United States has been called a Protestant nation by a variety of sources.[10][11][12][13]\\r\\nAccording to a 2002 survey by the Pew Research Center, nearly 6 in 10 Americans said that religion plays an important role in their lives, compared to 33% in Great Britain, 27% in Italy, 21% in Germany, 12% in Japan, and 11% in France. The survey report stated that the results showed America having a greater similarity to developing nations (where higher percentages say that religion plays an important role) than to other wealthy nations, where religion plays a minor role.[1]\\r\\nIn 1963, 90% of U.S. adults claimed to be Christian while only 2% professed no religious identity.[14] In 2014, close to 70% identify as Christian while close to 23% claim no religious identity.[2]\\r\\nThe United States federal government was the first national government to have no official state-endorsed religion.[15] However, some states had established religions in some form until the 1830s.\\r\\nModeling the provisions concerning religion within the Virginia Statute for Religious Freedom, the framers of the Constitution rejected any religious test for office, and the First Amendment specifically denied the federal government any power to enact any law respecting either an establishment of religion or prohibiting its free exercise, thus protecting any religious organization, institution, or denomination from government interference. The decision was mainly influenced by European Rationalist and Protestant ideals, but was also a consequence of the pragmatic concerns of minority religious groups and small states that did not want to be under the power or influence of a national religion that did not represent them.[16]\\r\\nThe most popular religion in the U.S. is Christianity, comprising the majority of the population (70.6% of adults in 2014).[2] According to the Association of Statisticians of American Religious Bodies newsletter published March 2017, based on data from 2010, Christians were the largest religious population in all 3,143 counties in the country.[17] Roughly 46.5% of Americans are Protestants, 20.8% are Catholics, 1.6% are Mormons (the name commonly used to refer to members of The Church of Jesus Christ of Latter-day Saints), and 1.7% have affiliations with various other Christian denominations.[2] Christianity was introduced during the period of European colonization.\\r\\nAccording to a 2012 review by the National Council of Churches, the five largest denominations are:[18]\\r\\nThe Southern Baptist Convention, with over 16 million adherents, is the largest of more than 200[19] distinctly named Protestant denominations.[20] In 2007, members of evangelical churches comprised 26% of the American population, while another 18% belonged to mainline Protestant churches, and 7% belonged to historically black churches.[21]\\r\\nA 2015 study estimates some 450,000 Christian believers from a Muslim background in the country, most of them belonging to some form of Protestantism.[22] In 2010 there were approximately 180,000 Arab Americans and about 130,000 Iranian Americans who converted from Islam to Christianity. Dudley Woodbury, a Fulbright scholar of Islam, estimates that 20,000 Muslims convert to Christianity annually in the United States.[23]\\r\\nHistorians agree that members of mainline Protestant denominations have played leadership roles in many aspects of American life, including politics, business, science, the arts, and education. They founded most of the country's leading institutes of higher education.[24] According to Harriet Zuckerman, 72% of American Nobel Prize Laureates between 1901 and 1972, have identified from Protestant background.[25]\\r\\nEpiscopalians[26] and Presbyterians[27] tend to be considerably wealthier and better educated than most other religious groups, and numbers of the most wealthy and affluent American families as the Vanderbilts[26] and Astors,[26] Rockefeller,[28] Du Pont, Roosevelt, Forbes, Whitneys,[26] Morgans[26] and Harrimans are Mainline Protestant families,[26] though those affiliated with Judaism are the wealthiest religious group in the United States[29][30]and those affiliated with Catholicism, owing to sheer size, have the largest number of adherents of all groups in the top income bracket.[31]\\r\\nSome of the first colleges and universities in America, including Harvard,[32] Yale,[33] Princeton,[34] Columbia,[35] Dartmouth,[36] Williams, Bowdoin, Middlebury,[37] and Amherst, all were founded by mainline Protestant denominations. By the 1920s most had weakened or dropped their formal connection with a denomination. James Hunter argues that:\\r\\nBeginning around 1600 European settlers introduced Anglican and Puritans religion, as well as Baptist, Presbyterian, Lutheran, Quaker, and Moravian denominations.[39]\\r\\nBeginning in the 16th century, the Spanish (and later the French and English) introduced Catholicism. From the 19th century to the present, Catholics came to the US in large numbers due to immigration of Italians, Hispanics, Portuguese, French, Polish, Irish, Highland Scots, Dutch, Flemish, Hungarians, Germans, Lebanese (Maronite), and other ethnic groups.\\r\\nEastern Orthodoxy was brought to America by Greek, Ukrainian, Armenian, and other immigrant groups.[40][41]\\r\\nSeveral Christian groups were founded in America during the Great Awakenings. Interdenominational evangelicalism and Pentecostalism emerged; new Protestant denominations such as Adventism; non-denominational movements such as the Restoration Movement (which over time separated into the Churches of Christ, the Christian churches and churches of Christ, and the Christian Church (Disciples of Christ)); Jehovah's Witnesses (called \\"Bible Students\\" in the latter part of the 19th century); and The Church of Jesus Christ of Latter-day Saints (Mormonism).\\r\\nThe strength of various sects varies greatly in different regions of the country, with rural parts of the South having many evangelicals but very few Catholics (except Louisiana and the Gulf Coast, and from among the Hispanic community, both of which consist mainly of Catholics), while urbanized areas of the north Atlantic states and Great Lakes, as well as many industrial and mining towns, are heavily Catholic, though still quite mixed, especially due to the heavily Protestant African-American communities. In 1990, nearly 72% of the population of Utah was Mormon, as well as 26% of neighboring Idaho.[42] Lutheranism is most prominent in the Upper Midwest, with North Dakota having the highest percentage of Lutherans (35% according to a 2001 survey).[43]\\r\\nThe largest religion, Christianity, has proportionately diminished since 1990. While the absolute number of Christians rose from 1990 to 2008, the percentage of Christians dropped from 86% to 76%.[44] A nationwide telephone interview of 1,002 adults conducted by The Barna Group found that 70% of American adults believe that God is \\"the all-powerful, all-knowing creator of the universe who still rules it today\\", and that 9% of all American adults and 0.5% young adults hold to what the survey defined as a \\"biblical worldview\\".[45]\\r\\nEpiscopalian, Presbyterian, Eastern Orthodox and United Church of Christ members[46] have the highest number of graduate and post-graduate degrees per capita of all Christian denominations in the United States,[47][48] as well as the most high-income earners.[49][50]However, owing to the sheer size or demographic head count of Catholics, more individual Catholics have graduate degrees and are in the highest income brackets than have or are individuals of any other religious community.[51]\\r\\nAfter Christianity, Judaism is the next largest religious affiliation in the US, though this identification is not necessarily indicative of religious beliefs or practices.[44] There are between 5.3 and 6.6 million Jews. A significant number of people identify themselves as American Jews on ethnic and cultural grounds, rather than religious ones. For example, 19% of self-identified American Jews do not believe God exists.[52] The 2001 ARIS study projected from its sample that there are about 5.3?million adults in the American Jewish population: 2.83?million adults (1.4% of the U.S. adult population) are estimated to be adherents of Judaism; 1.08?million are estimated to be adherents of no religion; and 1.36?million are estimated to be adherents of a religion other than Judaism.[53] ARIS 2008 estimated about 2.68 million adults (1.2%) in the country identify Judaism as their faith.[44]\\r\\nJews have been present in what is now the US since the 17th century, and specifically allowed since the British colonial Plantation Act 1740. Although small Western European communities initially developed and grew, large-scale immigration did not take place until the late 19th century, largely as a result of persecutions in parts of Eastern Europe. The Jewish community in the United States is composed predominantly of Ashkenazi Jews whose ancestors emigrated from Central and Eastern Europe. There are, however, small numbers of older (and some recently arrived) communities of Sephardi Jews with roots tracing back to 15th century Iberia (Spain, Portugal, and North Africa). There are also Mizrahi Jews (from the Middle East, Caucasia and Central Asia), as well as much smaller numbers of Ethiopian Jews, Indian Jews, Kaifeng Jews and others from various smaller Jewish ethnic divisions. Approximately 25% of the Jewish American population lives in New York City.[54]\\r\\nAccording to the Association of Statisticians of American Religious Bodies newsletter published March, 2017, based on data from 2010, Jews were the largest minority religion in 231 counties out of the 3143 counties in the country.[17] According to a 2014 survey conducted by the Pew Forum on Religion and Public life, 1.7% of adults in the U.S. identify Judaism as their religion. Among those surveyed, 44% said they were Reform Jews, 22% said they were Conservative Jews, and 14% said they were Orthodox Jews.[2][55] According to the 1990 National Jewish Population Survey, 38% of Jews were affiliated with the Reform tradition, 35% were Conservative, 6% were Orthodox, 1% were Reconstructionists, 10% linked themselves to some other tradition, and 10% said they are \\"just Jewish\\".[56]\\r\\nThe Pew Research Center report on American Judaism released in October 2013 revealed that 22% of Jewish Americans say they have \\"no religion\\" and the majority of respondents do not see religion as the primary constituent of Jewish identity. 62% believe Jewish identity is based primarily in ancestry and culture, only 15% in religion. Among Jews who gave Judaism as their religion, 55% based Jewish identity on ancestry and culture, and 66% did not view belief in God as essential to Judaism.[57]\\r\\nA 2009 study estimated the Jewish population (including both those who define themselves as Jewish by religion and those who define themselves as Jewish in cultural or ethnic terms) to be between 6.0 and 6.4 million.[58] According to a study done in 2000 there were an estimated 6.14 million Jewish people in the country, about 2% of the population.[59]\\r\\nAccording to the 2001 National Jewish Population Survey, 4.3?million American Jewish adults have some sort of strong connection to the Jewish community, whether religious or cultural.[60] Jewishness is generally considered an ethnic identity as well as a religious one. Among the 4.3?million American Jews described as \\"strongly connected\\" to Judaism, over 80% have some sort of active engagement with Judaism, ranging from attendance at daily prayer services on one end of the spectrum to attending Passover Seders or lighting Hanukkah candles on the other. The survey also discovered that Jews in the Northeast and Midwest are generally more observant than Jews in the South or West. Reflecting a trend also observed among other religious groups, Jews in the Northwestern United States are typically the least observant of tradition.[citation needed]\\r\\nThe Jewish American community has higher household incomes than average, and is one of the best educated religious communities in the United States.[46]\\r\\nIslam is the third largest faith in the United States, after Christianity and Judaism, representing 0.9% of the population.[2][61] According to the Association of Statisticians of American Religious Bodies newsletter published March, 2017, based on data from 2010, Muslims were the largest minority religion in 392 counties out of the 3143 counties in the country.[17] Islam in America effectively began with the arrival of African slaves. It is estimated that about 10% of African slaves transported to the United States were Muslim.[62] Most, however, became Christians, and the United States did not have a significant Muslim population until the arrival of immigrants from Arab and East Asian Muslim areas.[63] According to some experts,[64] Islam later gained a higher profile through the Nation of Islam, a religious group that appealed to black Americans after the 1940s; its prominent converts included Malcolm X and Muhammad Ali.[65][66] The first Muslim elected in Congress was Keith Ellison in 2006,[67] followed by Andr Carson in 2008.[68]\\r\\nResearch indicates that Muslims in the United States are generally more assimilated and prosperous than their counterparts in Europe.[69][70][71] Like other subcultural and religious communities, the Islamic community has generated its own political organizations and charity organizations.\\r\\nThe United States has perhaps the second largest Bah' community in the world. First mention of the faith in the U.S. was at the inaugural Parliament of World Religions, which was held at the Columbian Exposition in Chicago in 1893. In 1894, Ibrahim George Kheiralla, a Syrian Bah' immigrant, established a community in the U.S. He later left the main group and founded a rival movement.[72] According to the Association of Statisticians of American Religious Bodies newsletter published March, 2017, based on data from 2010, Bah's were the largest minority religion in 80 counties out of the 3143 counties in the country.[17]\\r\\nRastafarians began migrating to the United States in the 1950s, '60s and '70s from the religion's 1930s birthplace, Jamaica.[73][74] Marcus Garvey, who is considered a prophet by many Rastafarians,[75][76] rose to prominence and cultivated many of his ideas in the United States.\\r\\nBuddhism entered the US during the 19th century with the arrival of the first immigrants from East Asia. The first Buddhist temple was established in San Francisco in 1853 by Chinese Americans.\\r\\nDuring the late 19th century Buddhist missionaries from Japan came to the US. During the same time period, US intellectuals started to take interest in Buddhism.\\r\\nThe first prominent US citizen to publicly convert to Buddhism was Henry Steel Olcott in 1880. An event that contributed to the strengthening of Buddhism in the US was the Parliament of the World's Religions in 1893, which was attended by many Buddhist delegates sent from India, China, Japan, Vietnam, Thailand and Sri Lanka.\\r\\nThe early 20th century was characterized by a continuation of tendencies that had their roots in the 19th century. The second half, by contrast, saw the emergence of new approaches, and the move of Buddhism into the mainstream and making itself a mass and social religious phenomenon.[77][78]\\r\\nEstimates of the number of Buddhists in the United States vary between 0.5%[44] and 0.9%,[79] with 0.7% reported by both the CIA[55] and Pew.[80] According to the Association of Statisticians of American Religious Bodies newsletter published March, 2017, based on data from 2010, Buddhists were the largest minority religion in 186 counties out of the 3143 counties in the country.[17]\\r\\nHinduism is the fourth largest faith in the United States, representing 0.7% of the population.[2] The first time Hinduism entered the U.S. is not clearly identifiable. However, large groups of Hindus have immigrated from India and other Asian countries since the enactment of the Immigration and Nationality Act of 1965. During the 1960s and 1970s Hinduism exercised fascination contributing to the development of New Age thought. During the same decades the International Society for Krishna Consciousness (a Vaishnavite Hindu reform organization) was founded in the US.\\r\\nIn 2001, there were an estimated 766,000 Hindus in the US, about 0.2% of the total population.[81][82] According to the Association of Statisticians of American Religious Bodies newsletter published March, 2017, based on data from 2010, Bah's were the largest minority religion in 92 counties out of the 3143 counties in the country.[17]\\r\\nIn 2004 the Hindu American Foundationa national institution protecting rights of the Hindu community of U.S.was founded.\\r\\nAmerican Hindus have one of the highest rates of educational attainment and household income among all religious communities, and tend to have lower divorce rates.[46]\\r\\nAdherents of Jainism first arrived in the United States in the 20th century. The most significant time of Jain immigration was in the early 1970s. The United States has since become a center of the Jain Diaspora. The Federation of Jain Associations in North America is an umbrella organization of local American and Canadian Jain congregations to preserve, practice, and promote Jainism and the Jain way of life.[83]\\r\\nSikhism is a religion originating from South Asia (predominantly in modern-day India) which was introduced into the United States when, around the turn of the 20th century, Sikhs started emigrating to the United States in significant numbers to work on farms in California. They were the first community to come from India to the US in large numbers.[84][clarification needed] The first Sikh Gurdwara in America was built in Stockton, California, in 1912.[85] In 2007, there were estimated to be between 250,000 and 500,000 Sikhs living in the United States, with the largest populations living on the East and West Coasts, with additional populations in Detroit, Chicago, and Austin.[86][87]\\r\\nThe United States also has a number of non-Punjabi converts to Sikhism.[88]\\r\\nIn 2004 there were an estimated 56,000 Taoists in the US.[89] Taoism was popularized throughout the world through the writings and teachings of Lao Tzu and other Taoists as well as the practice of Qigong, Tai Chi Chuan and other Chinese martial arts.[90]\\r\\nThis group includes atheists, agnostics and people who describe their religion as \\"nothing in particular\\".[91]\\r\\n\\"Unaffiliated\\" does not necessarily mean \\"non-religious\\". Some people who are unaffiliated with any particular religion express religious beliefs (such as belief in one or more gods or in reincarnation) and engage in religious practices (such as prayer).[citation needed]\\r\\nA 2001 survey directed by Dr. Ariela Keysar for the City University of New York indicated that, amongst the more than 100 categories of response, \\"no religious identification\\" had the greatest increase in population in both absolute and percentage terms. This category included atheists, agnostics, humanists, and others with no stated religious preferences. Figures are up from 14.3?million in 1990 to 34.2?million in 2008, representing an increase from 8% of the total population in 1990 to 15% in 2008.[44] A nationwide Pew Research study published in 2008 put the figure of unaffiliated persons at 16.1%,[82] while another Pew study published in 2012 was described as placing the proportion at about 20% overall and roughly 33% for the 18ÿ29-year-old demographic.[92]\\r\\nIn a 2006 nationwide poll, University of Minnesota researchers found that despite an increasing acceptance of religious diversity, atheists were generally distrusted by other Americans, who trusted them less than Muslims, recent immigrants and other minority groups in \\"sharing their vision of American society\\". They also associated atheists with undesirable attributes such as amorality, criminal behavior, rampant materialism and cultural elitism.[93][94] However, the same study also reported that \\"The researchers also found acceptance or rejection of atheists is related not only to personal religiosity, but also to one's exposure to diversity, education and political orientation ÿ with more educated, East and West Coast Americans more accepting of atheists than their Midwestern counterparts.\\"[95] Some surveys have indicated that doubts about the existence of the divine were growing quickly among Americans under 30.[96]\\r\\nOn 24 March 2012, American atheists sponsored the Reason Rally in Washington, D.C., followed by the American Atheist Convention in Bethesda, Maryland. Organizers called the estimated crowd of 8,000ÿ10,000 the largest-ever US gathering of atheists in one place.[97]\\r\\nIn the United States, Enlightenment philosophy (which itself was heavily inspired by deist ideals) played a major role in creating the principle of religious freedom, expressed in Thomas Jefferson's letters and included in the First Amendment to the United States Constitution. American Founding Fathers, or Framers of the Constitution, who were especially noted for being influenced by such philosophy of deism include Thomas Jefferson, Benjamin Franklin, Cornelius Harnett, Gouverneur Morris, and Hugh Williamson. Their political speeches show distinct deistic influence. Other notable Founding Fathers may have been more directly deist. These include Thomas Paine, James Madison, possibly Alexander Hamilton, and Ethan Allen.[98]\\r\\nVarious polls have been conducted to determine Americans' actual beliefs regarding a god:\\r\\n\\"Spiritual but not religious\\" (SBNR) is self-identified stance of spirituality that takes issue with organized religion as the sole or most valuable means of furthering spiritual growth. Spirituality places an emphasis upon the wellbeing of the \\"mind-body-spirit,\\"[106] so holistic activities such as tai chi, reiki, and yoga are common within the SBNR movement.[107] In contrast to religion, spirituality has often been associated with the interior life of the individual.[108]\\r\\nOne fifth of the US public and a third of adults under the age of 30 are reportedly unaffiliated with any religion, however they identify as being spiritual in some way. Of these religiously unaffiliated Americans, 37% classify themselves as spiritual but not religious.[109]\\r\\nMany other religions are represented in the United States, including Shinto, Caodaism, Thelema, Santera, Kemetism, Religio Romana, Kaldanism, Zoroastrianism, Vodou, Pastafarianism, and many forms of New Age spirituality.\\r\\nNative American religions historically exhibited much diversity, and are often characterized by animism or panentheism.[110] The membership of Native American religions in the 21st century comprises about 9,000 people.[111]\\r\\nNeopaganism in the United States is represented by widely different movements and organizations. The largest Neopagan religion is Wicca, followed by Neo-Druidism.[112][113] Other neopagan movements include Germanic Neopaganism, Celtic Reconstructionist Paganism, Hellenic Polytheistic Reconstructionism, and Semitic neopaganism.\\r\\nAccording to the American Religious Identification Survey (ARIS), there are approximately 30,000 druids in the United States.[114] Modern Druidism came to North America first in the form of fraternal Druidic organizations in the nineteenth century, and orders such as the Ancient Order of Druids in America were founded as distinct American groups as early as 1912. In 1963, the Reformed Druids of North America (RDNA) was founded by students at Carleton College, Northfield, Minnesota. They adopted elements of Neopaganism into their practices, for instance celebrating the festivals of the Wheel of the Year.[115]\\r\\nWicca advanced in North America in the 1960s by Raymond Buckland, an expatriate Briton who visited Gardner's Isle of Man coven to gain initiation.[116] Universal Eclectic Wicca was popularized in 1969 for a diverse membership drawing from both Dianic and British Traditional Wiccan backgrounds.[117]\\r\\nA group of churches which started in the 1830s in the United States is known under the banner of \\"New Thought\\". These churches share a spiritual, metaphysical and mystical predisposition and understanding of the Bible and were strongly influenced by the Transcendentalist movement, particularly the work of Ralph Waldo Emerson. Another antecedent of this movement was Swedenborgianism, founded on the writings of Emanuel Swedenborg in 1787.[118] The New Thought concept was named by Emma Curtis Hopkins (\\"teacher of teachers\\") after Hopkins broke off from Mary Baker Eddy's Church of Christ, Scientist. The movement had been previously known as the Mental Sciences or the Christian Sciences. The three major branches are Religious Science, Unity Church and Divine Science.\\r\\nUnitarian Universalists (UU's) are among the most liberal of all religious denominations in America[119]. The shared creed includes beliefs in inherent dignity, a common search for truth, respect for beliefs of others, compassion, and social action.[120] They are unified by their shared search for spiritual growth and by the understanding that an individual's theology is a result of that search and not obedience to an authoritarian requirement.[121]. UU's have historical ties to anti-war, civil rights, and LGBT rights movements[122], as well as providing inclusive church services for the broad spectrum of liberal Christians, liberal Jews, secular humanists, LGBT, Jewish-Christian parents and partners, Earth-centered/Wicca, and Buddhist meditation adherents.[123]\\r\\nThe First Amendment guarantees both the free practice of religion and the non-establishment of religion by the federal government (later court decisions have extended that prohibition to the states).[125] The U.S. Pledge of Allegiance was modified in 1954 to add the phrase \\"under God\\", in order to distinguish itself from the state atheism espoused by the Soviet Union.[126][127][128][129]\\r\\nVarious American presidents have often stated the importance of religion. On February 20, 1955, President Dwight D. Eisenhower stated that \\"Recognition of the Supreme Being is the first, the most basic, expression of Americanism.\\"[130] President Gerald Ford agreed with and repeated this statement in 1974.[131]\\r\\nThe U.S. Census does not ask about religion. Various groups have conducted surveys to determine approximate percentages of those affiliated with each religious group.\\r\\nReligion in the United States according to Gallup, Inc. (2016)[132]\\r\\nReligion in the United States according to the Pew Research Center (2014)[2]\\r\\nA 2013 survey reported that 31% of Americans attend religious services at least weekly. It was conducted by the Public Religion Research Institute with a margin of error of 2.5.[134]\\r\\nIn 2006, an online Harris Poll (they stated that the magnitude of errors cannot be estimated due to sampling errors, non-response, etc.; 2,010 U.S. adults were surveyed)[135] found that 26% of those surveyed attended religious services \\"every week or more often\\", 9% went \\"once or twice a month\\", 21% went \\"a few times a year\\", 3% went \\"once a year\\", 22% went \\"less than once a year\\", and 18% never attend religious services.\\r\\nIn a 2009 Gallup International survey, 41.6%[136] of American citizens said that they attended a church, synagogue, or mosque once a week or almost every week. This percentage is higher than other surveyed Western countries.[137][138] Church attendance varies considerably by state and region. The figures, updated to 2014, ranged from 51% in Utah to 17% in Vermont.\\r\\nIn August 2010, 67% of Americans said religion was losing influence, compared with 59% who said this in 2006. Majorities of white evangelical Protestants (79%), white mainline Protestants (67%), black Protestants (56%), Catholics (71%), and the religiously unaffiliated (62%) all agreed that religion was losing influence on American life; 53% of the total public said this was a bad thing, while just 10% see it as a good thing.[140]\\r\\nPoliticians frequently discuss their religion when campaigning, and fundamentalists and black Protestants are highly politically active. However, to keep their status as tax-exempt organizations they must not officially endorse a candidate. Historically Catholics were heavily Democratic before the 1970s, while mainline Protestants comprised the core of the Republican Party. Those patterns have faded awayCatholics, for example, now split about 50ÿ50. However, white evangelicals since 1980 have made up a solidly Republican group that favors conservative candidates. Secular voters are increasingly Democratic.[141]\\r\\nOnly three presidential candidates for major parties have been Catholics, all for the Democratic party:\\r\\nJoe Biden is the first Catholic vice president.[143]\\r\\nJoe Lieberman was the first major presidential candidate that was Jewish, on the Gore-Lieberman campaign of 2000 (although John Kerry and Barry Goldwater both had Jewish ancestry, they were practicing Christians). Bernie Sanders ran against Hillary Clinton in the Democratic primary of 2016. He was the first major Jewish candidate to compete in the presidential primary process. However, Sanders noted during the campaign that he does not actively practice any religion.[144]\\r\\nIn 2006 Keith Ellison of Minnesota became the first Muslim elected to Congress; when re-enacting his swearing-in for photos, he used the copy of the Qur'an once owned by Thomas Jefferson.[145] Andr Carson is the second Muslim to serve in Congress.\\r\\nA Gallup poll released in 2007[146] indicated that 53% of Americans would refuse to vote for an atheist as president, up from 48% in 1987 and 1999. But then the number started to drop again and reached record low 43% in 2012 and 40% in 2015. [147][148]\\r\\nThe 2012 Republican presidential nominee Mitt Romney is Mormon and a member of The Church of Jesus Christ of Latter-day Saints. He is the former governor of the state of Massachusetts, and his father George Romney was the governor of the state of Michigan. The Romneys were involved in Mormonism in their states and in the state of Utah.\\r\\nThe table below is based mainly on data reported by individual denominations to the Yearbook of American and Canadian Churches, and published in 2011 by the National Council of Churches of Christ in USA. It only includes religious bodies reporting 60,000 or more members. The definition of a member is determined by each religious body.[149]\\r\\nThe Association of Religion Data Archives (ARDA) surveyed congregations for their memberships. Churches were asked for their membership numbers. Adjustments were made for those congregations that did not respond and for religious groups that reported only adult membership.[150] ARDA estimates that most of the churches not responding were black Protestant congregations. Significant difference in results from other databases include the lower representation of adherents of 1) all kinds (62.7%), 2) Christians (59.9%), 3) Protestants (less than 36%); and the greater number of unaffiliated (37.3%).\\r\\nThe United States government does not collect religious data in its census. The survey below, the American Religious Identification Survey (ARIS) of 2008, was a random digit-dialed telephone survey of 54,461 American residential households in the contiguous United States. The 1990 sample size was 113,723; 2001 sample size was 50,281.\\r\\nAdult respondents were asked the open-ended question, \\"What is your religion, if any?\\" Interviewers did not prompt or offer a suggested list of potential answers. The religion of the spouse or partner was also asked. If the initial answer was \\"Protestant\\" or \\"Christian\\" further questions were asked to probe which particular denomination. About one third of the sample was asked more detailed demographic questions.\\r\\nReligious Self-Identification of the U.S. Adult Population: 1990, 2001, 2008[44]\\r\\nFigures are not adjusted for refusals to reply; investigators suspect refusals are possibly more representative of \\"no religion\\" than any other group.\\r\\nHighlights:[44]\\r\\nThe table below shows the religious affiliations among the ethnicities in the United States, according to the Pew Forum 2014 survey.[2] People of Black ethnicity were most likely to be part of a formal religion, with 85% percent being Christians. Protestant denominations make up the majority of the Christians in the ethnicities.","input":"What is the largest religion in the us?"},{"output":"Pakistan","context":"South Asia or Southern Asia (also known as Indian subcontinent) is a term used to represent the southern region of the Asian continent, which comprises the sub-Himalayan SAARC countries and, for some authorities, adjoining countries to the west and east. Topographically, it is dominated by the Indian Plate, which rises above sea level as Nepal and northern parts of India situated south of the Himalayas and the Hindu Kush. South Asia is bounded on the south by the Indian Ocean and on land (clockwise, from west) by West Asia, Central Asia, East Asia, and Southeast Asia.\\r\\nThe current territories of Afghanistan, Bangladesh, Bhutan, Maldives, Nepal, India, Pakistan, and Sri Lanka form South Asia.[7] The South Asian Association for Regional Cooperation (SAARC) is an economic cooperation organisation in the region which was established in 1985 and includes all eight nations comprising South Asia.[8]\\r\\nSouth Asia covers about 5.2 million km2 (2 million mi2), which is 11.71% of the Asian continent or 3.5% of the world's land surface area.[7] The population of South Asia is about 1.749 billion or about one fourth of the world's population, making it both the most populous and the most densely populated geographical region in the world.[3] Overall, it accounts for about 39.49% of Asia's population, over 24% of the world's population, and is home to a vast array of peoples.[9][10][11]\\r\\nIn 2010, South Asia had the world's largest population of Hindus, Jains and Sikhs. It also has the largest population of Muslims in Asia-Pacific region,[12][13] as well as over 35 million Christians and 25 million Buddhists.[14]\\r\\n\\r\\n\\r\\nThe total area of South Asia and its geographical extent is not clear cut as systemic and foreign policy orientations of its constituents are quite asymmetrical.[16] Aside from the central region of South Asia, formerly part of the British Empire, there is a high degree of variation as to which other countries are included in South Asia.[17][18][19][20]\\r\\nModern definitions of South Asia are consistent in including Afghanistan, India, Pakistan, Bangladesh, Sri Lanka, Nepal, Bhutan and Maldives as the constituent countries.[21][22][23][24][25][26] Myanmar is included by some scholars in South Asia, but in Southeast Asia by others.[18][27] Some do not include Afghanistan,[18] others question whether Afghanistan should be considered a part of South Asia or the Middle East.[28][29]\\r\\nThe current territories of Bangladesh, India, and Pakistan, which were the core of the British Empire prior to 1947, form the central region of South Asia, in addition to Afghanistan,[21][22][23][24][25][26] which was a British protectorate until 1919, after the Afghans lost to the British in the Second Anglo-Afghan war. The mountain countries of Nepal and Bhutan, and the island countries of Sri Lanka and Maldives are generally included as well. Myanmar (formerly Burma) is often added, and by various deviating definitions based on often substantially different reasons, the British Indian Ocean Territory and the Tibet Autonomous Region are included as well.[16][30][31][32][33][34][35][36][37]\\r\\nThe common concept of South Asia is largely inherited from the administrative boundaries of the British Raj,[38] with several exceptions. The Aden Colony, British Somaliland and Singapore, though administered at various times under the Raj, have not been proposed as any part of South Asia.[39] Additionally Burma was administered as part of the Raj until 1937, but is now considered a part of Southeast Asia and is a member state of ASEAN. The 562 princely states that were protected by but not directly ruled by the Raj became administrative parts of South Asia upon joining Union of India or Dominion of Pakistan.[40][41][42] Geopolitically, it had formed the whole territory of Greater India,[27][43]\\r\\nThe South Asian Association for Regional Cooperation (SAARC), a contiguous block of countries, started in 1985 with seven countries?ÿ Bangladesh, Bhutan, India, the Maldives, Nepal, Pakistan and Sri Lanka?ÿ and added Afghanistan as an eighth member in 2007.[44][45] China and Myanmar have also applied for the status of full members of SAARC.[46][47] This bloc of countries include two independent countries that were not part of the British Raj ÿ Nepal, and Bhutan. Afghanistan was a British protectorate from 1878 until 1919, after the Afghans lost to the British in the Second Anglo-Afghan war. The World Factbook, based on geo-politics, people, and economy defines South Asia as comprising Afghanistan, Bangladesh, Bhutan, British Indian Ocean Territory, India, Maldives, Nepal, Pakistan, and Sri Lanka.[48] The South Asia Free Trade Agreement incorporated Afghanistan in 2011, and the World Bank grouping of countries in the region also includes all eight members comprising South Asia and SAARC as well,[49][50] and the same goes for the United Nations Children's Fund (UNICEF).[51][52]\\r\\nThe Centres for South Asian Studies at both the University of Michigan and the University of Virginia include Tibet along with the eight members of SAARC in their research programs, but exclude the Maldives.[59][60] The South Asian Studies Program of Rutgers University and the University of California, Berkeley Centre for South Asia Studies also include the Maldives.[61][62]\\r\\nThe South Asian Studies Program of Brandeis University defines the region as comprising \\"India, Pakistan, Bangladesh, Sri Lanka, Nepal, Bhutan, and in certain contexts Afghanistan, Burma, Maldives and Tibet\\".[63] The similar program of Columbia University includes Afghanistan, Bangladesh, India, the Maldives, Nepal, Pakistan, and Sri Lanka in their study and excludes Burma.[64]\\r\\nThe United Nations Statistics Division's scheme of sub-regions include all eight members of the SAARC as part of Southern Asia, along with Iran[65] only for statistical purposes.[66] Population Information Network (POPIN) includes Afghanistan, Bangladesh, Burma, India, Nepal, Pakistan and Sri Lanka as part of South Asia. Maldives, in view of its characteristics, was admitted as a member Pacific POPIN subregional network only in principle.[67] The HirschmanÿHerfindahl index of the United Nations Economic and Social Commission for Asia and the Pacific for the region includes only the original seven signatories of SAARC.[68]\\r\\nThe British Indian Ocean Territory is connected to the region by a publication of Jane's for security considerations.[69] The region may also include the disputed territory of Aksai Chin, which was part of the British Indian princely state of Jammu and Kashmir, but is now administered as part of the Chinese autonomous region of Xinjiang.[70]\\r\\nThe inclusion of Myanmar in South Asia is without consensus, with many considering it a part of Southeast Asia and others including it within South Asia.[18][27] Afghanistan was of importance to the British colonial empire, especially after the Second Anglo-Afghan War over 1878ÿ1880. Afghanistan remained a British protectorate until 1919, when a treaty with Vladimir Lenin included the granting of independence to Afghanistan. Following India's partition, Afghanistan has generally been included in South Asia, with some considering it a part of Southwest Asia.[16] During the Soviet war in Afghanistan (1979ÿ1989) American foreign policy considered Pakistan and Afghanistan in Southwest Asia, while others included it as a part of South Asia.[7] There is no universal agreement among scholars on which countries should be included within South Asia.[18]\\r\\nIn the past, a lack of a coherent definition for South Asia resulted in not only a lack of academic studies, but also in a lack interest for such studies.[71] The confusion existed also because of the lack of a clear boundary ÿ geographically, geopolitical, socio-culturally, economically or historically ÿ between South Asia and other parts of Asia, especially the Middle East and Southeast Asia.[72] Identification with a South Asian identity was also found to be significantly low among respondents in an older two-year survey across Bangladesh, India, Nepal, Pakistan, and Sri Lanka.[73] However, modern definitions of South Asia are very consistent in including Afghanistan, India, Pakistan, Bangladesh, Sri Lanka, Nepal, Bhutan and Maldives as the constituent countries.[21][22][23][24][25][26]\\r\\nAccording to the Oxford English Dictionary, the term \\"subcontinent\\" signifies a \\"subdivision of a continent which has a distinct geographical, political, or cultural identity\\" and also a \\"large land mass somewhat smaller than a continent\\".[74][75] Historians Catherine Asher and Cynthia Talbot state that the term \\"Indian subcontinent\\" describes a natural physical landmass in South Asia that has been relatively isolated from the rest of Eurasia.[76] The Indian subcontinent is also a geological term referring to the land mass that drifted northeastwards from ancient Gondwana, colliding with the Eurasian plate nearly 55 million years ago, towards the end of Palaeocene. This geological region largely includes Bangladesh, Bhutan, India, Maldives, Nepal, Pakistan and Sri Lanka.[77]\\r\\nThe use of the term Indian subcontinent began in the British Empire, and has been a term particularly common in its successors.[78] This region has also been labelled as \\"India\\" (in its classical and pre-modern sense), \\"Greater India\\", or as South Asia.[27][43]\\r\\nAccording to anthropologist John R. Lukacs, \\"the Indian Subcontinent occupies the major landmass of South Asia\\",[79] while the political science professor Tatu Vanhanen states, \\"the seven countries of South Asia constitute geographically a compact region around the Indian Subcontinent\\".[80] According to Chris Brewster, India, Pakistan, Bangladesh, Sri Lanka, Nepal and Bhutan constitute the Indian subcontinent; with Afghanistan and Maldives included it is more commonly referred to as South Asia.[81] The geopolitical boundaries of Indian subcontinent, according to Dhavendra Kumar, include \\"India, Pakistan, Bangladesh, Sri Lanka, Nepal, Bhutan and other small islands of the Indian Ocean\\".[82] Maldives, the country consisting of a small archipelago southwest of the peninsula, is considered part of the Indian subcontinent.[83]\\r\\nThe terms \\"Indian subcontinent\\" and \\"South Asia\\" are sometimes used interchangeably.[30][78] The South Asia term is particularly common when scholars or officials seek to differentiate this region from East Asia.[84] According to historians Sugata Bose and Ayesha Jalal, the Indian subcontinent has come to be known as South Asia \\"in more recent and neutral parlance.\\"[85] This \\"neutral\\" notion refers to the concerns of Pakistan and Bangladesh, particularly given the recurring conflicts between India and Pakistan, wherein the dominant placement of \\"India\\" as a prefix before the subcontinent might offend some political sentiments.[27]\\r\\nThere is no globally accepted definition on which countries are a part of South Asia or Indian subcontinent.[18][19][20] While Afghanistan is not considered as a part of the Indian subcontinent, Afghanistan is often included in South Asia.[20] Similarly, Myanmar is included by some scholars in South Asia but not in Indian subcontinent.[27]\\r\\nThe history of core South Asia begins with evidence of human activity of Homo sapiens, as long as 75,000 years ago, or with earlier hominids including Homo erectus from about 500,000 years ago.[86] The Indus Valley Civilization, which spread and flourished in the northwestern part of South Asia from c. 3300 to 1300 BCE in present-day Northern India, Pakistan and Afghanistan, was the first major civilization in South Asia.[87] A sophisticated and technologically advanced urban culture developed in the Mature Harappan period, from 2600 to 1900 BCE.[88]\\r\\nThe earliest prehistoric culture have roots in the mesolithic sites as evidenced by the rock paintings of Bhimbetka rock shelters dating to a period of 30,000 BCE or older,[note 2] as well as neolithic times.[note 3] According to anthropologist Possehl, the Indus Valley Civilization provides a logical, if somewhat arbitrary, starting point for South Asian religions, but these links from the Indus religion to later-day South Asian traditions are subject to scholarly dispute.[89]\\r\\nThe Vedic period, named after the Vedic religion of the Indo-Aryans,[note 4] lasted from c. 1900 to 500 BCE.[91][92] The Indo-Aryans were pastoralists[93] who migrated into north-western India after the collapse of the Indus Valley Civilization,[90][94] Linguistic and archaeological data show a cultural change after 1500 BCE,[90] with the linguistic and religious data clearly showing links with Indo-European languages and religion.[95] By about 1200 BCE, the Vedic culture and agrarian lifestyle was established in the northwest and northern Gangetic plain of South Asia.[93][96][97] Rudimentary state-forms appeared, of which the Kuru-Pa?cla union was the most influential.[98][99] The first recorded state-level society in South Asia existed around 1000 BCE.[93] In this period, states Samuel, emerged the Brahmana and Aranyaka layers of Vedic texts, which merged into the earliest Upanishads.[100] These texts began to ask the meaning of a ritual, adding increasing levels of philosophical and metaphysical speculation,[100] or \\"Hindu synthesis\\".[101]\\r\\nIncreasing urbanisation of India between 800 and 400 BCE, and possibly the spread of urban diseases, contributed to the rise of ascetic movements and of new ideas which challenged the orthodox Brahmanism.[102] These ideas led to Sramana movements, of which Mahavira (c. 549ÿ477 BCE), proponent of Jainism, and Buddha (c. 563-483), founder of Buddhism, were the most prominent icons.[103]\\r\\nThe Greek army led by Alexander the Great stayed in the Hindu Kush region of South Asia for several years and then later moved into the Indus valley region. Later, the Maurya Empire extended over much of South Asia in the 3rd century BCE. Buddhism spread beyond the Indian subcontinent, through northwest into Central Asia. The Bamiyan Buddhas of Afghanistan and the edicts of A?oka suggest that the Buddhist monks spread Buddhism (Dharma) in eastern provinces of the Seleucid Empire, and possibly even farther into West Asia.[104][105][106] The Theravada school spread south from India in the 3rd century BCE, to Sri Lanka, later to Southeast Asia.[107] Buddhism, by the last centuries of the 1st millennium BCE, was prominent in the Himalayan region, Gandhara, Hindu Kush region and Bactria.[108][109][110]\\r\\nFrom about 500 BCE through about 300 CE, the Vedic-Brahmanic synthesis or \\"Hindu synthesis\\" continued.[101] Classical Hindu and Sramanic (particularly Buddhist) ideas spread within Indian subcontinent, as well outside South Asia.[111][112][113] The Gupta Empire ruled over a large part of the subcontinent between 4th and 7th centuries, a period that saw the construction of major temples, monasteries and universities such as the Nalanda.[114][115][116] During this era, and through the 10th century, numerous cave monasteries and temples such as the Ajanta Caves, Badami cave temples and Ellora Caves were built in South Asia.[117][118][119]\\r\\nIslam came as a political power in the fringe of South Asia in 8th century CE when the Arab general Muhammad bin Qasim conquered Sindh and Multan in southern Punjab in modern-day Pakistan.[120] By 962 CE, Hindu and Buddhist kingdoms in South Asia were under a wave of raids from Muslim armies from Central Asia.[121] Among them was Mahmud of Ghazni, who raided and plundered kingdoms in north India from east of the Indus river to west of Yamuna river seventeen times between 997 and 1030.[122] Mahmud of Ghazni raided the treasuries but retracted each time, only extending Islamic rule into western Punjab.[123][124]\\r\\nThe wave of raids on north Indian and western Indian kingdoms by Muslim warlords continued after Mahmud of Ghazni, plundering and looting these kingdoms.[125] The raids did not establish or extend permanent boundaries of their Islamic kingdoms. The Ghurid Sultan Mu'izz al-Din Muhammad began a systematic war of expansion into north India in 1173.[126] He sought to carve out a principality for himself by expanding the Islamic world.[122][127] Mu'izz sought a Sunni Islamic kingdom of his own extending east of the Indus river, and he thus laid the foundation for the Muslim kingdom that became the Delhi Sultanate.[122] Some historians chronicle the Delhi Sultanate from 1192 due to the presence and geographical claims of Mu'izz al-Din in South Asia by that time.[128] The Delhi Sultanate covered varying parts of South Asia, and was ruled by a series of dynasties, called Mamluk, Khalji, Tughlaq, Sayyid and Lodi dynasties. Muhammad bin Tughlaq came to power in 1325, launched a war of expansion and the Delhi Sultanate reached it largest geographical reach over the Indian subcontinent during his 26-year rule.[129] A Sunni Sultan, Muhammad bin Tughlaq persecuted non-Muslims such as Hindus, as well as non-Sunni Muslims such as Shia and Mahdi sects.[130][131][132]\\r\\nRevolts against the Delhi Sultanate sprang up in many parts of South Asia during the 14th century. After the death of Muhammad bin Tughlaq, the Bengal Sultanate came to power in 1352 CE, as the Delhi Sultanate began disintegrating. The Bengal Sultanate remained in power through the early 16th century. It was reconquered by the armies of the Mughal Empire. The state religion of the Bengal Sultanate was Islam, and the region under its rule, a region that ultimately emerged as the modern nation of Bangladesh, saw a growth of a syncretic form of Islam.[133][134] In the Deccan region, the Hindu kingdom Vijayanagara Empire came to power in 1336 and remained in power through the 16th century, after which it too was reconquered and absorbed into the Mughal Empire.[135][136]\\r\\nAbout 1526, the Punjab governor Dawlat Khan Lodؐ reached out to the Mughal Babur and invited him to attack Delhi Sultanate. Babur defeated and killed Ibrahim Lodi in the Battle of Panipat in 1526. The death of Ibrahim Lodi ended the Delhi Sultanate, and the Mughal Empire replaced it.[137]\\r\\nThe modern history period of South Asia, that is 16th-century onwards, witnessed the start of the Central Asian dynasty named the Mughals, with Turkish-Mongol roots and Sunni Islam theology. The first ruler was Babur, whose empire extended the northwest and Indo-Gangetic Plain regions of South Asia. The Deccan and northeastern region of the South Asia was largely under Hindu kings such as those of Vijayanagara Empire and Ahom kingdom,[138] with some regions such as parts of modern Telangana and Andhra Pradesh under local Sultanates such as the Shia Islamic rulers of Golconda Sultanate.[139]\\r\\nThe Mughal Empire continued its wars of expansion after Babur's death. With the fall of Rajput kingdoms and Vijayanagara, its boundaries reached all of west, as well as the Marathi and Kannada speaking regions of the Deccan peninsula. The Mughal Empire was marked by a period of artistic exchanges and a Central Asian and South Asian architecture synthesis, with remarkable buildings such as the Taj Mahal.[140] It also marked an extended period of religious persecution.[141] Two of the religious leaders of Sikhism, Guru Arjan and Guru Tegh Bahadur were arrested under orders of the Mughal emperors, asked to convert to Islam, and executed when they refused.[142][143][144] Religious taxes on non-Muslims called jizya were imposed. Buddhist, Hindu and Sikh temples were desecrated. However, not all Muslim rulers persecuted non-Muslims. Akbar, a Mughal ruler for example, sought religious tolerance and abolished jizya.[145] After his death, the persecution of non-Muslims in South Asia returned.[146] The persecution and religious violence in South Asia peaked during Aurangzeb era, with him issuing orders in 1669, to all his governors of provinces to \\"destroy with a willing hand the schools and temples of the infidels, and that they were strictly enjoined to put an entire stop to the teaching and practice of idolatrous forms of worship\\".[147][148] In Aurangzeb's time, almost all of South Asia was claimed by the Mughal Empire. However, this claim was violently challenged in various regions of South Asia, particularly by the Sikh Guru Gobind Singh in the northwest,[149] and by Shivaji in the Deccan regions.[150]\\r\\nMaritime trading between South Asia and European merchants began after the Portuguese explorer Vasco de Gama returned to Europe. After the death of Aurangzeb and the collapse of the Mughal Empire, the region came under the rule of many small Islamic sultanates and Hindu kingdoms. British, French, Portuguese colonial interests struck treaties with these rulers, and established their trading ports. In the northwest South Asia, a large region was consolidated into the Sikh Empire by Ranjit Singh.[151][page?needed][152] After his death, the British Empire expanded their interests till the Hindu Kush region. In the east, the Bengal region was split into Muslim East Bengal and Hindu West Bengal, by the colonial British empire, in early 1900s, a split that was reversed. However, after the World War II, at the eve of India's independence, the region was split again into East Pakistan and West Bengal. East Pakistan became Bangladesh in 1971.[153][154]\\r\\nAccording to Saul Cohen, early colonial era strategists treated South Asia with East Asia, but in reality the South Asia region excluding Afghanistan is a distinct geopolitical region separated from other nearby geostrategic realms, one that is geographically diverse.[155] The region is home to a variety of geographical features, such as glaciers, rainforests, valleys, deserts, and grasslands that are typical of much larger continents. It is surrounded by three water bodies?ÿ the Bay of Bengal, the Indian Ocean and the Arabian Sea?ÿ and has acutely varied climate zones. The tip of the Indian Peninsula had the highest quality pearls.[156]\\r\\nThe boundaries of South Asia vary based on how the region is defined. South Asia's northern, eastern, and western boundaries vary based on definitions used, while the Indian Ocean is the southern periphery. Most of this region rests on the Indian Plate and is isolated from the rest of Asia by mountain barriers.[157][158] Much of the region consists of a peninsula in south-central Asia, rather resembling a diamond which is delineated by the Himalayas on the north, the Hindu Kush in the west, and the Arakanese in the east,[159] and which extends southward into the Indian Ocean with the Arabian Sea to the southwest and the Bay of Bengal to the southeast.[30][32]\\r\\nAccording to Robert M. Cutler ÿ a scholar of Political Science at Carleton University,[160] the terms South Asia, Southwest Asia and Central Asia are distinct, but the confusion and disagreements have arisen due to the geopolitical movement to enlarge these regions into Greater South Asia, Greater Southwest Asia and Greater Central Asia. The frontier of Greater South Asia, states Cutler, between 2001ÿ2006 has been geopolitically extended to eastern Iran and western Afghanistan in the west, and in the north to northeastern Iran, northern Afghanistan, and southern Uzbekistan.[160]\\r\\nMost of this region is a subcontinent resting on the Indian Plate, the northerly portion of the Indo-Australian Plate, separated from the rest of the Eurasian Plate. The Indian Plate includes most of South Asia, forming a land mass which extends from the Himalayas into a portion of the basin under the Indian Ocean, including parts of South China and Eastern Indonesia, as well as Kunlun and Karakoram ranges,[161][162][163][page?needed] and extending up to but not including Ladakh, Kohistan, the Hindu Kush range and Balochistan.[164][165][166] It may be noted that geophysically the Yarlung Tsangpo River in Tibet is situated at the outside of the border of the Subcontinental structure, while the Pamir Mountains in Tajikistan are situated inside that border.[167]\\r\\nIt was once a small continent before colliding with the Eurasian Plate about 50ÿ55 million years ago and giving birth to the Himalayan range and the Tibetan plateau. It is the peninsular region south of the Himalayas and Kuen Lun mountain ranges and east of the Indus River and the Iranian Plateau, extending southward into the Indian Ocean between the Arabian Sea (to the southwest) and the Bay of Bengal (to the southeast).\\r\\nThe climate of this vast region varies considerably from area to area from tropical monsoon in the south to temperate in the north. The variety is influenced by not only the altitude, but also by factors such as proximity to the sea coast and the seasonal impact of the monsoons. Southern parts are mostly hot in summers and receive rain during monsoon periods. The northern belt of Indo-Gangetic plains also is hot in summer, but cooler in winter. The mountainous north is colder and receives snowfall at higher altitudes of Himalayan ranges.\\r\\nAs the Himalayas block the north-Asian bitter cold winds, the temperatures are considerably moderate in the plains down below. For most part, the climate of the region is called the Monsoon climate, which keeps the region humid during summer and dry during winter, and favours the cultivation of jute, tea, rice, and various vegetables in this region.\\r\\nSouth Asia is largely divided into four broad climate zones:[169]\\r\\nMaximum relative humidity of over 80% has been recorded in Khasi and Jaintia Hills and Sri Lanka, while the area adjustment to Pakistan and western India records lower than 20%ÿ30%.[169] Climate of South Asia is largely characterized by monsoons. South Asia depends critically on monsoon rainfall.[170] Two monsoon systems exist in the region:[171]\\r\\nThe warmest period of the year precedes the monsoon season (March to mid June). In the summer the low pressures are centered over the Indus-Gangetic Plain and high wind from the Indian Ocean blows towards the center. The monsoons are second coolest season of the year because of high humidity and cloud covering. But, at the beginning of June the jetstreams vanish above the Tibetan Plateau, low pressure over the Indus Valley deepens and the Intertropical Convergence Zone (ITCZ) moves in. The change is violent. Moderately vigorous monsoon depressions form in the Bay of Bengal and make landfall from June to September.[169]\\r\\n(2017)[180]\\r\\n(2016)[181]\\r\\nThis list includes dependent territories within their sovereign states (including uninhabited territories), but does not include claims on Antarctica. EEZ+TIA is exclusive economic zone (EEZ) plus total internal area (TIA) which includes land and internal waters.\\r\\nThe population of South Asia is about 1.749 billion which makes it the most populated region in the world.[182] It is socially very mixed, consisting of many language groups and religions, and social practices in one region that are vastly different from those in another.[183]\\r\\nSouth Asia is home to some of the most populated cities in the world. Delhi, Karachi, Mumbai, and Dhaka are four of the world's largest megacities.\\r\\nThere are numerous languages in South Asia. The spoken languages of the region are largely based on geography and shared across religious boundaries, but the written script is sharply divided by religious boundaries. In particular, Muslims of South Asia such as in Afghanistan and Pakistan use the Arabic alphabet and Persian Nastaliq. Till 1971, Muslim Bangladesh (then known as East Pakistan) too mandated only the Nastaliq script, but thereafter has adopted regional scripts and particularly Bengali. Non-Muslims of South Asia, and some Muslims in India, on the other hand use their traditional ancient heritage scripts such as those derived from Brahmi script for Indo-European languages and non-Brahmi scripts for Dravidian languages and others.[187]\\r\\nThe Nagari script has been the primus inter pares of the traditional South Asian scripts.[188] The Devanagari script is used for over 120 South Asian languages,[189] including Hindi,[190] Marathi, Nepali, Pali, Konkani, Bodo, Sindhi and Maithili among other languages and dialects, making it one of the most used and adopted writing systems in the world.[191] The Devanagari script is also used for classical Sanskrit texts.[189]\\r\\nThe largest spoken language in this region is Hindi, followed by Bengali, Telugu, Marathi, Gujarati and Punjabi.[187] In the modern era, new syncretic languages developed in the region such as Urdu that is used by Muslim community of northern Indian subcontinent (particularly Pakistan and northern states of India).[192] The Punjabi language spans three religions: Islam, Hinduism and Sikhism. The spoken language is similar, but it is written in three scripts. The Sikh use Gurmukhi alphabet, Muslim Punjabis in Pakistan use the Nastaliq script, while Hindu Punjabis in India use the Gurmukhi or Ngarؐ script. The Gurmukhi and Nagari scripts are distinct but close in their structure, but the Persian Nastaliq script is very different.[193]\\r\\nEnglish, with British spelling, is commonly used in urban areas and is a major economic lingua franca of South Asia.[194]\\r\\nIn 2010, South Asia had the world's largest population of Hindus, Jains and Sikhs,[12] about 510 million Muslims,[12] as well as over 25 million Buddhists and 35 million Christians.[14] Hindus make up about 68 percent or about 1 billion and Muslims at 31 percent or 510 million of the overall South Asia population,[195][196] while Buddhists, Jains, Christians and Sikhs constitute most of the rest. The Hindus, Buddhists, Jains, Sikhs and Christians are concentrated in India, Nepal, Sri Lanka and Bhutan, while the Muslims are concentrated in Afghanistan (99%), Bangladesh (90%), Pakistan (96%) and Maldives (100%).[12]\\r\\nIndian religions are the religions that originated in the Indian subcontinent; namely Hinduism, Jainism, Buddhism and Sikhism.[197] The Indian religions are distinct yet share terminology, concepts, goals and ideas, and from the Indian subcontinent spread into East Asia and southeast Asia.[197] Early Christianity and Islam were introduced into coastal regions of South Asia by merchants who settled among the local populations. Later Sindh, Balochistan, and parts of the Punjab region saw conquest by the Arab caliphates along with an influx of Muslims from Persia and Central Asia, which resulted in spread of both Shia and Sunni Islam in parts of northwestern region of South Asia. Subsequently, under the influence of Muslim rulers of the Islamic sultanates and the Mughal Empire, Islam spread in South Asia.[198][199]\\r\\nIndia is the largest and fastest growing economy in the region (US$2.180 trillion) and makes up almost 82% of the South Asian economy; it is the world's 7th largest in nominal terms and 3rd largest by purchasing power adjusted exchange rates (US$8.020 trillion).[210] India is the only member of powerful G-20 major economies and BRICS from the region. It is the fastest growing major economy in the world and one of the world's fastest registering a growth of 7.3% in FY 2014ÿ15. Pakistan has the next largest economy($304.3 billion) and the 5th highest GDP per capita in the region,[211] followed by Bangladesh and then by Sri Lanka which has the 2nd highest per capita and is the 4th largest economy in the region. According to a World Bank report in 2015, driven by a strong expansion in India, coupled with favorable oil prices, from the last quarter of 2014 South Asia become the fastest-growing region in the world[212]\\r\\nThe Major Market stock exchanges in the region are Bombay Stock Exchange (BSE) with market Capitalization of $1.68 trillion (11th largest in the world), National Stock Exchange of India (NSE) with market capitalization of $1.64 trillion (12th largest in the world), and Karachi Stock Exchange with market capitalization of $60 billion.[213]\\r\\nEconomic data is sourced from the International Monetary Fund, current as of April 2017, and is given in US dollars.[214]\\r\\n(2017)[180]\\r\\n(2017)[215]\\r\\n(2017)[216]\\r\\nAccording to WHO, South Asia is home to two out of the three countries in the world still affected by polio, Pakistan and Afghanistan, with 306 & 28 polio cases registered in 2014 respectively.[219] Attempts to eradicate polio have been badly hit by opposition from militants in both countries, who say the program is cover to spy on their operations. Their attacks on immunization teams have claimed 78 lives since December 2012.[220]\\r\\nAccording to the World Bank's 2011 report, based on 2005 ICP PPP, about 24.6% of the South Asian population falls below the international poverty line of $1.25/day.[221] Afghanistan and Bangladesh rank the highest, with 30.6% and 43.3% of their respective populations below the poverty line. Bhutan, Maldives and Sri Lanka have the lowest number of people below the poverty line, with 2.4%, 1.5% and 4.1% respectively. India has lifted the most people in the region above the poverty line between 2008 and 2011, around 140 million. As of 2011, 21.9% of India's population lives below the poverty line, compared to 41.6% in 2005.[222][223]\\r\\nThe World Bank estimates that India is one of the highest ranking countries in the world for the number of children suffering from malnutrition. The prevalence of underweight children in India is among the highest in the world, and is nearly double that of Sub Saharan Africa with dire consequences for mobility, mortality, productivity and economic growth.[224]\\r\\nAccording to the World Bank, 70% of the South Asian population and about 75% of South Asia's poor live in rural areas and most rely on agriculture for their livelihood[225] according to the UN's Food and Agricultural Organisation. In 2015, approximately 281 million people in the region were malnourished. The report says that Nepal reached both the WFS target as well as MDG and is moving towards bringing down the number of undernourished people to less than 5% of the population.[217] Bangladesh reached the MDG target with the National Food Policy framework?ÿ with only 16.5% of the population undernourished. In India, the malnourished comprise just over 15 percent of the population. While the number of malnourished people in neighborhood has shown a decline over the last 25 years, the number of under-nourished in Pakistan displays an upward trend.There were 28.7 million hungry in Pakistan in the 1990s?ÿ a number that has steadily increased to 41.3 million in 2015 with 22% of the population malnourished. Approximately 194.6 million people are undernourished in India, which accounts for the highest number of people suffering from hunger in any single country.[217][226]\\r\\nThe 2006 report stated \\"the low status of women in South Asian countries and their lack of nutritional knowledge are important determinants of high prevalence of underweight children in the region\\". Corruption and the lack of initiative on the part of the government has been one of the major problems associated with nutrition in India. Illiteracy in villages has been found to be one of the major issues that need more government attention. The report mentioned that although there has been a reduction in malnutrition due to the Green Revolution in South Asia, there is concern that South Asia has \\"inadequate feeding and caring practices for young children\\".[227]\\r\\nIndia[228][229][230] and Pakistan[231][232] are the dominant political powers in the region. India is by far the largest country in the area covering around three-fourths the land area of the subcontinent.[citation needed] India has the largest population of around three times the combined population of the 6 other countries in the subcontinent.[233] India is also the world's largest democracy[234] India's annual defence budget for 2013ÿ14 is $39.2 Billion[235] which is equal to the whole Pakistan's Federal budget of $39.3 billion for 2014ÿ15.[236]\\r\\nBangladesh is a unitary state and parliamentary democracy.[237] Bangladesh also stands out as one of the few Muslim-majority democracies. It is a moderate and generally secular and tolerant  though sometimes this is getting stretched at the moment  alternative to violent extremism in a very troubled part of the world, said Dan Mozena, the U.S. ambassador to Bangladesh. Although Bangladesh's legal code is secular, more citizens are embracing a conservative version of Islam, with some pushing for sharia law, analysts say. Experts say that the rise in conservatism reflects the influence of foreign-financed Islamic charities and the more austere version of Islam brought home by migrant workers in Persian Gulf countries.[238]\\r\\nDiplomacy among the countries of South Asia has been mainly driven by populist politics, with the centre-stage taken by India-Pakistan conflict ever since their independence in 1947, and then the creation of Bangladesh under tense circumstances in 1971. During the height of Cold war, the elite political leaders of Pakistan aligned with the US, while India played crucial role in forming the Non-Aligned Movement and while maintaining goodwill relations with the USSR.\\r\\nPakistan's governance is one of the most conflicted in the region. The military rule and the unstable government in Pakistan has become a concern for the South Asian region. In Nepal, the governance has struggled to come in the side of democracy and it only showed signs in the recent past, basically in the 21st century, to support the democratic system. The political situation in Sri Lanka has been dominated by an increasingly assertive Sinhalese nationalism, and the emergence of a Tamil separatist movement under LTTE, which was suppressed in May 2009. Myanmar's politics is dominated by a military Junta, which has sidelined the democratic forces led by Aung San Suu Kyi.\\r\\nGovernance Indicators (2015)[241]\\r\\nof violence/terrorism","input":"What is the second largest country in south asia?"},{"output":"Haiti","context":"","input":"What are the neighboring countries of dominican republic?"},{"output":"19th century","context":"","input":"When did the british empire reach its peak?"},{"output":"Los Angeles, California","context":"American Horror Story: Hotel is the fifth season of the FX horror anthology television series American Horror Story. It premiered on October 7, 2015, and concluded January 13, 2016. The series was renewed in October 2014, with the subtitle Hotel being announced in February 2015. Hotel marks the first season to not feature series mainstays Jessica Lange and Frances Conroy. Returning cast from previous seasons of the series include: Evan Peters, Sarah Paulson, Denis O'Hare, Lily Rabe, Kathy Bates, Angela Bassett, Chlo? Sevigny, Finn Wittrock, Wes Bentley, Gabourey Sidibe, Mare Winningham, Matt Bomer, Christine Estabrook, Matt Ross, John Carroll Lynch, and Anthony Ruivivar, along with new cast members Lady Gaga and Cheyenne Jackson. Breaking from the anthological format, like Freak Show, the season is interconnected to the first and third seasons, and features an appearance by the Murder House, its original owner Dr. Charles Montgomery (Ross), its realtor Marcy (Estabrook), and the psychic Billie Dean Howard (Paulson), as well as the appearance of the witch Queenie (Sidibe).\\r\\nThe plot centers around the enigmatic Hotel Cortez in Los Angeles, California, that catches the eye of an intrepid homicide detective (Bentley). The Cortez is host to the strange and bizarre, spearheaded by its owner, The Countess (Gaga), who is a bloodsucking fashionista. The hotel is loosely based on an actual hotel built in 1893 by H. H. Holmes in Chicago, Il. for the 1893 World's Columbian Exposition. It became known as the 'Murder Castle' as it was built for Holmes to torture, murder, and dispose of evidence just as is the Cortez. This season features two murderous threats in the form of the Ten Commandments Killer, a serial offender who selects his victims in accordance with biblical teachings, and \\"the Addiction Demon\\", who roams the hotel armed with a drill bit dildo.\\r\\nAccording to creators Brad Falchuk and Ryan Murphy, thematically, Hotel is much darker than previous seasons. Inspiration came from old hotel horror films and actual hotels situated in downtown Los Angeles with a reputation for sinister events, including the Cecil. The cycle also marks a return to filming in Los Angeles, where the first two seasons were shot. Hotel features one of the most expansive sets in American Horror Story history, with production designer Mark Worthington building two stories on a soundstage, along with a working elevator and stairway. In July 2015, FX launched a marketing campaign for the series, with most trailers and teasers touting Gaga's involvement.\\r\\nAlthough Hotel was originally reported to consist of thirteen episodes ( la Asylum, Coven, and Freak Show), that number was later revised to twelve ( la Murder House). The season garnered a total of eight Emmy Award nominations, including two acting nominations for Paulson and Bates. It was the first time, however, that a season of American Horror Story was not nominated for Outstanding Limited Series. In addition, Gaga won the Golden Globe Award for Best Actress ÿ Mini-Series or Television Film while Hotel received a nomination for Best Mini-Series or Television Film.\\r\\n\\r\\n\\r\\nThe fifth season of American Horror Story focuses on the Downtown Los Angeles Hotel Cortez which has been recently purchased by a New York fashion designer, Will Drake (Cheyenne Jackson). The 90 year old hotel is haunted by demons and mysterious ghosts including the founder, James Patrick March (Evan Peters); heroin junkie Sally (Sarah Paulson); and the strap-on-wielding Addiction Demon. Staff and residents of the hotel include the 111-year-old \\"vampire\\", Elizabeth/The Countess (Lady Gaga), and her longtime paramour, the former drug-addict Donovan (Matt Bomer), as well as Donovan's mother, the manager and front desk clerk Iris (Kathy Bates); the eccentric transgender bartender Liz Taylor (Denis O'Hare); and the Countess' vengeance-obsessed lover, Ramona Royale (Angela Bassett).\\r\\nAs the season unfolds, the information and backstory of each character is revealed. Elizabeth was married to James March and had an illegal abortion at the Murder House (of AHS Season One) back in 1926. Ramona was a Hollywood actress who had a romantic past with Elizabeth, before the latter killed Ramona's new boyfriend in a jealous rage. Liz was formerly known as Nick Pryor and lived in Topeka, Kansas up until 1984 when she was given a makeover and christened as Liz Taylor by Elizabeth. Sally, after supplying Donovan heroin, was shoved out a window by an angered Iris. Finally, James tortured and murdered many visitors to his hotel, which he built with a series of secret chutes and hallways used primarily for covertly disposing the bodies of his victims.\\r\\nThe present and past stories of the Hotel are interwoven with the present tale of Detective John Lowe (Wes Bentley), who is first drawn to the hotel by a series of murders committed by a serial killer the victims of whom each exemplify a sin in violation of one of the Ten Commandments, and his wife, Dr. Alex Lowe (Chloe Sevigny). As the season unfolds, it is revealed that certain past events? including the earlier disappearance of Lowe's young son Holden? are also entangled in the stories of the Countess and the Hotel Cortez.\\r\\nOn October 13, 2014, FX renewed the series for a fifth season for an October 2015 premiere.[13] Network president John Landgraf stated that the season would necessitate a \\"huge reinvention\\" for the series.[14] The season's subtitle was confirmed as Hotel in February 2015. The theme and Gaga's involvement were hinted in the previous installment as an image of a top hat, an arcane clue alluding to the 1935 screwball musical comedy film Top Hat, which is set in a hotel and features a song called \\"Cheek to Cheek\\", also the title of Gaga's duet album with Tony Bennett.[15] Co-creator Ryan Murphy explained that the casting included a number of actors and singers, but would be a much darker season compared to the previous ones. Inspiration came from old hotel horror films and actual hotels situated in downtown Los Angeles, with horrific reputations. This included The Cecil, where the death of 21-year-old Canadian student Elisa Lam occurred. Murphy had watched a surveillance video of Lam in the hotel, in which she displayed erratic behavior just hours prior to her supposed death.[16][17] It was around this time that the writing for Hotel was conceptualized,[18][19] which included Murphy's personal phobia and fears, a fear that had not been explored since the first season.[20]\\r\\nThe upcoming season that we're doing is much more horror-based; it's much more dark. It's about a theme and an idea that's very close to my heart that I've always wanted to do that's a little bloodier and grislier, I think, than anything that we've done before; it's straight horror this year. Murder House, I thought, was a very primal season because everybody's great fear is about the bogeyman under the bed in their house, and this feels similar to me in that when you check into a hotel, there are certain things beyond your control... Other people have the keys to your room; they can come in there. You're not exactly safe, it's a very unsettling idea.[18]\\r\\nMurphy and some of the cast appeared at the 2015 Comic-Con International and revealed further information about the series. \\"[Angela Bassett, Kathy Bates, Matt Bomer, Sarah Paulson and Evan Peters] are bad boys and girls this time.\\"[21] Regarding the season having no primary character, Murphy confessed that \\"the thing that's different about the season is that before we've always been very driven by the Jessica Lange character. She was always the lead character... This year, it's a true ensemble and I think we have more male parts and more male stories. The Wes Bentley part is really big, the Matt Bomer part is really big; Evan Peters and Finn Wittrock are really big. [But] that's not to say that the women aren't either.\\"[20][22]\\r\\nCo-creator Brad Falchuk explained that like the first and second season of the series, Hotel would explore the \\"trapped\\" horror trope, though the actions would not be limited to just within the premises. \\"This season, the horror is sneaking out of the hotel,\\" he added explaining that the plot would revolve around the hotel in the center, with a more noir like ambience.[23] Named as Hotel Cortez, the titular structure was built by James March in 1930, who was created as a rich and charming but deeply psychotic character. The season features two tormentors, The Ten Commandments Killer, who is inspired by biblical teachings, and The Addiction Demon, who wields a drill bit dildo. They are in the vein of previous seasons' Bloody Face and Rubber Man, respectively.[24] The Halloween episode, \\"Devil's Night\\", features a dinner with \\"the biggest serial killers of all time\\", including Wuornos and John Wayne Gacy.[25]\\r\\nThe Hotel's two-story lobby set, along with a working elevator, was constructed over the course of seven weeks. While no particular hotel served as inspiration, production designer Mark Worthington was influenced by Timothy Pflueger and William Van Alen when selecting patterns and schemes, stating, \\"Tonally, I thought Art Deco would make sense for the horror genre because it can be dark and spiky and odd and the composition is strange. It's beautiful, but it isn't necessarily inviting.\\" The hotel consists of labyrinthine structures housing March's murderous fantasies with dead ends, secret rooms and includes plot-lines corresponding to it. A painting of Hernn Corts, after whom the establishment is named, hangs in the reception area. Worthington and his team had a hand in creating even the smallest of details; such as hotel insignias for the light fixtures, bar coasters, and a venus flytrap column carving that reflects the nature of Gaga's character. The staircase was structured in such a way as to not pull focus from the elevator, which will serve as prime location.[26] The exterior of the set was inspired by the James Oviatt Building in Downtown Los Angeles, while the interior decorations were modeled from the Cicada restaurant situated inside the Oviatt.[27]\\r\\nIn February 2015, it was announced that American singer Lady Gaga had joined the show.[29][30] Murphy stated she wanted her role in the series to be \\"evil\\". He also explained that Hotel would be devoid of any musical numbers. Instead Gaga's character, Elizabeth/The Countess, is a fashion icon and owner of the Hotel Cortez.[31][32] Created as a glamorous socialite character, The Countess maintains her beauty by imbibing human blood.[24] Murphy was so pleased with Gaga's performance that he invited her back for the yet-to-be-confirmed sixth season of the series, before Hotel had even made its debut.[33] In March 2015, series star Jessica Lange definitively announced that she would not be returning for the fifth season.[34][35] During PaleyFest 2015, it was announced that Matt Bomer and Cheyenne Jackson would co-star.[36] Afterwards, more castings were confirmed, including Sarah Paulson, Evan Peters, Kathy Bates, and Angela Bassett.[37] Murphy tweeted about the latter's involvement in Hotel, including a plotline with Gaga, as a character called Ramona Royale, an actress and former lover of The Countess returning to the titular hotel for revenge.[31][38] Chlo? Sevigny, who was a recurring special guest in Asylum, returned to the series for Hotel, playing the wife of Wes Bentley's character, a detective.[31][39]\\r\\nIn May 2015, it was announced that Max Greenfield would also be joining the cast, in a role later revealed to be that of an addict. Greenfield had to dye his hair platinum blond for the role.[31][40] His is intertwined to that of Sally's (Paulson) and together with The Addiction Demon feature \\"the most disturbing scene\\" the show had ever produced, according to Murphy.[41] In an interview with Vanity Fair, Paulson described Sally as someone who is \\"selfish and greedy\\", with hygiene problems.[42] The next month, Murphy announced that Denis O'Hare would return as a cross-dressing bar worker in the hotel.[43] He also confirmed that Finn Wittrock would return in the new role of Tristan Duffy, a male model who is involved in a love triangle with Gaga and Bomer, and later a fated love of Liz's.[22][37] Wittrock explained that the character might have similarities to his previous Freak Show character, Dandy Mott.[44] In July 2015, Murphy stated that Coven alum Emma Roberts would return for a few episodes toward the finale, after completing filming on her Fox series, Scream Queens. Her character would be associated with James March (Peters).[45] While promoting Queens in September 2015, Roberts spoke about her role with less certainty, but optimism, stating, \\"...it's just everything you could dream of and more. Everything you could nightmare about and more. Granted, things over there are always changing, but I definitely want to go back to it. If it still stands, what Ryan told me, everyone's in for a great shock.\\"[46] However, she later confirmed that she would be unable to return, due to her demanding feature schedule. Furthermore, she did state that Murphy and her had already discussed about a \\"devilish\\" role for her in the forthcoming season six.[47]\\r\\nLater in July, Richard T. Jones joined the cast as Detective Hahn, a homicide detective,[48] for an eight-episode arc. That same day, Helena Mattsson announced that she had also joined the series in an unspecified role.[49] Series alum Lily Rabe portrayed infamous serial killer Aileen Wuornos during the Halloween installment and the finale.[31] With her appearance in the season, Rabe is one of three cast members to appear in all five seasons (along with Paulson and Peters) of the show.[50] Naomi Campbell was cast as a fashion editor who does not get along with Gaga's character.[51] In August 2015, Murphy revealed that M?dchen Amick joined the season as a \\"mother of a boy who becomes ill\\", and shares screen time with Alex Lowe (Sevigny). Later in the month, Darren Criss was announced to guest star as a hipster that has conflicts with Iris (Bates),[52] while Mare Winningham joined as the laundress of the Cortez, who works closely with Mr. March, in the 1920s.[53] Christine Estabrook returned to the series as Marcy; the realtor who sold the first season's Murder House to the Harmons.[54] Gabourey Sidibe appeared in the eleventh episode as her Coven persona Queenie. Paulson also reprised her first season role of psychic Billie Dean Howard, appearing in the final episode of the season.[55]\\r\\nPrincipal photography for the season began on July 14, 2015, in Los Angeles, California, marking a return to where the series shot its first two cycles (Murder House and Asylum).[56] According to the Los Angeles Times, creative reasons, not economic factors, was the deciding key for moving the series from Louisiana back to Los Angeles since Hotel's story is connected to the city.[57] Murphy revealed a six-story hotel set was being built on the Fox lot. A dummy set of the hotel was built at Comic-Con, showing an Art Deco style building from the 1920s, inspired by the old Hollywood era.[58] Murphy announced at the TCA Summer Press Tour in August 2015 that he would be directing the season's Halloween episode, \\"Devil's Night\\", marking the first time in series history that he will helm more than the premiere. He stated he would direct it \\"because I love the script so much, when we finished it I said, 'I can't give this to anybody else'.\\"[31] However, ultimately Murphy did not direct the episode. In an interview with Entertainment Tonight, Murphy spoke about Gaga's entrance scene, confirming it to be about six minutes long and describing it as \\"like a silent movie with no dialogue, and lots of blood and nudity\\".[59]\\r\\nGreenfield recalled that Murphy wanted to push the limits of the scenes between him, Paulson and The Addiction Demon, while admitting that it was scary. Paulson described it as a normal day of shooting for her since she was accustomed to the theatrics surrounding the show. She added, \\"None of it's crazy to me. I walk in and I'm like 'Hello conical dildo demon person'. I don't even think twice.\\"[41] For The Countess and Donovan, who both suffer from blood lust, Murphy was insistent on chainmail gloves being used as their weapons of choice. Costume designer Lou Eyrich created the custom gloves in the mold of armor, deriving inspiration from artist Daphne Guinness; \\"We wanted it to look both rock-n-roll but old at the same time. But then the nail that pops out with diamonds on the edge to slice you,\\" said Eyrich.[60]\\r\\nFilming also took place at the Los Angeles County Museum of Art, in front of Chris Burden's art installation called Urban Light, where Gaga was seen in a floor length pink gown shooting scenes. Media reported that the filming involved a party scene with Gaga walking through the installation while singer Dinah Washington's \\"Coquette\\" played in the background.[61][62] Entertainment Weekly's Tim Stack spent three days on set, where he witnessed the filming of a foursome/murder scene, involving Gaga and Bomer's characters. Murphy recounted Gaga's day of filming stating, \\"You write a foursome for her and you expect a lot of questions. She never did that. She showed up and she was wearing diamond pasties, a Band-Aid on her hoo-ha, heels, and a black veil that Alexander McQueen made for her on the day before his death.\\"[63] Additional filming for the exterior shots of the Hotel Cortez took place outside the James Oviatt Building.[27][64] Other locations include the lower level of the Los Angeles Theatre at 615 South Broadway, The Majestic Downtown at 650 South Spring Street acting as John Lowe's office, the Loews Hollywood Hotel at 1755 Highland Avenue filming The Ten Commandments Killer murder scene, and Hollywood Forever Cemetery, where scenes with Gaga and Bomer were shot.[27] Also used was the house from the first season located at 1120 Westchester Place.[65]\\r\\nO'Hare revealed that he had filmed three episodes by September 2015, with his scenes involving Bomer, Sevigny and Bates mostly. \\"We're kind of doing it piecemeal. You'll do five days on this one, three days on that one. You know, they always start out rocking. There's no warm-up. You're in it,\\" the actor explained.[66][67] He later went on to compare the aesthetic to Murder House, stating, \\"It feels like season 1 in many ways... and I think it's because we're back in LA. You can't help it!\\" He also revealed that his character, Liz Taylor, would be wearing a dress that Lange was supposed to wear in Coven.[68] The actor explained that for his part, he had to shave his body including his head, and wear eye make-up, since the character was inspired by actress Elizabeth Taylor's films like BUtterfield 8 (1960) and Cleopatra (1963).[69]\\r\\nIn February 2015, Gaga tweeted a link to the first promotional video for the upcoming season with the caption \\"Make your reservation now. #GagaAHSHotel\\" announcing her presence in the season and the official title.[70] In July 2015, a promotional trading card was unveiled by Entertainment Weekly, available at Comic-Con, where after entering the hotel set built there, one could receive the trading card with a promotional key.[58][71] The first official teaser for the season was released later that month, showing Gaga's long-nailed hand ringing the bell at the front desk.[72] In August 2015, FX revealed the premiere date of the season along with a new teaser poster, showing an Art Deco peephole on a wooden door, beyond which an obscure image revealed a blond woman putting a body to bed.[73] Later that month, Entertainment Weekly exclusively unveiled two teaser trailers of the season, entitled \\"Beauty Rest\\" and \\"Do Not Disturb\\", set to singer Heidi Feek's cover of Elvis Presley's 1956 single, \\"Heartbreak Hotel\\".[74]\\r\\nOn August 26, Entertainment Weekly revealed exclusive cast photos, along with character descriptions.[75] Gaga also took to her Twitter account to release another photo, showing her as The Countess with three cherubic blond boys, who are seemingly sucking on bottles of blood. The singer captioned the image: \\"We are family. Meet my magical children. HOTEL #AHS.\\"[76] Murphy released three new teasers through his Twitter account, titled \\"Towhead\\", \\"Sleepwalk\\", and \\"Jeepers Peepers\\", all set to \\"Heartbreak Hotel\\".[77] Jef Rouner from Houston Press complimented the teasers, describing them as \\"things of fleeting, awful beauty. So far I've seen six for this season and at least one of them is creepier than every episode of Coven combined... Each one of these is usually less than 15 seconds long and they are murderously effective. I find myself wanting to watch the show again.\\"[78] On September 10, 2015, an extended teaser was released, featuring a psychedelic tour of the hotel, with cameos by most of the cast.[79] Few days later, two more trailers were released, one showed the hand of an addict, with a keyhole in place of the needle point, while the other, titled \\"Above & Below\\", portrays Gaga as The Countess, with several psychedelic intercuts inside a hotel, featuring Rammstein's \\"Du hast\\".[80][81]\\r\\nAmerican retail chain Hot Topic announced on their Instagram account that starting September 28, 2015, they will launch a clothing and apparel line based on Hotel, that will be sold in-store and online.[82] On September 16, 2015, a featurette was released, giving more details about the season and showing some footage.[83] An actual scene from the season, released in October 2015, showed Bentley's character resting in the Hotel, while Greenfield's character hiding underneath his bed.[84] Same day the title sequence of the season was released by Murphy, consisting of the same soundtrack like previous seasons, intercut with scenes of a dirty hotel and the Ten Commandments written across a wall.[69] Jacob Bryant from Variety was impressed with the clip, saying that \\"The opening credits for [American Horror Story] have always managed to be unsettling, but season five's creepy credits might top the list.\\"[85]\\r\\nAmerican Horror Story: Hotel initially received mixed reviews from critics, but, as the season progressed, the reviews became more positive. The review aggregator Rotten Tomatoes gave the season a 63% approval rating (average episode score of 75%) with an average rating of 6.38/10 based on 43 reviews. The site's consensus reads, \\"Favoring garish style over effective storytelling, the fifth American Horror Story strands a talented cast at Ryan Murphy's Hotel.\\"[86] On Metacritic, the season was given a score of 60 out of 100 based on 24 reviews, indicating \\"mixed or average reviews\\".[87]\\r\\nDan Fienberg of The Hollywood Reporter gave a positive review, writing, \\"Early on, Hotel hasn't hooked me with its storytelling, but it's always fun to see what the series does with its repertory acting company and with new additions. Throw in the normal grotesquerie and visual panache and that should keep me going for a while, even if all of the humor appears to have been funneled into Scream Queens.\\"[88] Amber Dowling of TheWrap also gave a positive review, saying, \\"It's a visual, visceral romp into what is being set up to be another haphazard foray into the world of horror, as imagined by Murphy and his writing counterpart Brad Falchuk. The show has rarely made sense in terms of story, and this is no exception.\\"[89] Willa Paskin of Slate called this season a \\"promising new start\\", saying, \\"AHS: Hotel more obviously resembles the first two, better seasons of American Horror Story than it does the latter, lesser two.\\"[90]\\r\\nOn the other hand, Matt Zoller Seitz of New York Magazine found the season \\"confusing, tedious, annoyingly precious, and often ostentatiously brutal\\", but also praised it for being \\"darkly beautiful, deeply weird, and (sometimes) exhilarating.\\"[91] Although Scott D. Pierce from The Salt Lake Tribune praised the production design and the cinematography, he said \\"the storytelling is derivative; the scares are non-existent; and it's all about style without much substance.\\"[92] Mike Hale from The New York Times complained that it \\"suffers from the absence of Jessica Lange\\".[93] IGN's Matt Fowler gave a rating of 5.9 out of 10, criticizing the season as \\"mediocre\\" and concluding \\"all weight and meaning is gone\\".[94]\\r\\nGaga's performance has received mixed reviews from critics. Matt Zoller Seitz of Vulture called Gaga \\"terrible here in the way that Madonna was terrible in a lot of her '90s films, at once too poised and too blank.\\"[95] David Weiland of San Francisco Chronicle said Gaga \\"makes an enormous visual impact, but the minute she opens her mouth to deliver a line, it's obvious that acting just isn't one of her many talents.\\"[96] Ben Travers of Indiewire wrote that he \\"wouldn't go so far as to say Gaga's talent adds much to the proceedings, but her presence? and the manner in which its captured? certainly does.\\"[97] On the other hand, Emily L. Stephens from The A.V. Club and Jeff Jensen of Entertainment Weekly both gave a B- rating. Stephens praised Gaga's first appearance as \\"slickly exploitative and hellishly effective\\"[98] while Jensen described her as \\"the show's most potent symbol for all of its themes about our Bad Romance with fame, fortune, sex, sex, and more sex, materialism and consumerism, the denial of death and the corrupt want for cultural immortality\\".[99] Brian Lowry of Variety praised the look of Gaga's character as \\"gloriously photographed\\" and felt her addition to the show was \\"extraordinarily well-timed\\".[100]\\r\\nIn its fifth season, the series has been nominated for 64 awards, 20 of which were won.\\r\\nHotel's premiere episode, \\"Checking In\\", was initially watched by 5.81 million viewers. After factoring in delayed viewing, the episode rose to 9.1 million, with 6.13 million in the 18-49 demographic, while combined linear, nonlinear and encore viewing, it drew 12.17 million viewers through October 11. Variety stated that \\"Checking In\\" could become FX's most-watched telecast, with the 60 full data tabulated.[127] Through its first four episodes, Hotel averaged at 3.7 rating in 18-49 adult zone and 6.9 million viewers total, which is up by 7% and 1% from previous installment, Freak Show, respectively, pacing ahead of the average ratings of all prior installments on a Live +3 basis.[128] The season finale, \\"Be Our Guest\\", initially watched by 2.24 million viewers, more than doubled its 18-49 rating with three days of delayed viewing, going from 1.1 million to 2.3 million, an 109% of increase. The episode increased 94% in total viewers, upping to 4.3 million.[129]","input":"Where is season 5 american horror story filmed?"},{"output":"Cook County Jail","context":"The Cook County Jail, located on 96 acres (39 hectares) in Cook County, Illinois, is the largest single site jail in the United States. Located at 2700 South California Avenue in the city of Chicago, it houses about 6500[1] prisoners and employs 3900 law enforcement officials and 7000 civilian employees.\\r\\nThe jail has held several well-known and infamous criminals, including Tony Accardo, Frank Nitti, Larry Hoover, Jeff Fort, Richard Speck, John Wayne Gacy and the Chicago Seven.\\r\\nIt was one of three sites in which executions were carried out by electrocution in Illinois. Between 1928 and 1962, the electric chair was used 67 times at the jail, including the state's last electrocution of James Duke on August 24, 1962. The state's other electrocutions were carried out at the Stateville Correctional Center in Crest Hill and at the Menard Correctional Center in Chester.\\r\\n\\r\\n\\r\\nIn July 2008, the civil rights division of the United States Department of Justice released a report finding that the Eighth Amendment civil rights of the inmates has been systematically violated.[2][3] The report found that the CCJ failed to adequately protect inmates from harm or risk of harm from other inmates or staff; failed to provide adequate suicide prevention; failed to provide adequate sanitary environmental conditions; failed to provide adequate fire safety precautions; and failed to provide adequate medical and mental health care.\\r\\nSpecific alleged violations that have resulted in Federal sanctions and/or class action lawsuits include:\\r\\nThe Cook County jail was the setting used for the musical Chicago, as well as its 2002 film adaptation. It has also been in segments of TV series including Chicago Fire and Better Call Saul.\\r\\nB.B. King's Live in Cook County Jail album features a live recording of a concert that he performed for the jail's inmates on September 10, 1970.\\r\\nA live album Concert: Friday the 13th - Cook County Jail featuring performances by jazz musicians Jimmy McGriff and Lucky Thompson was released on the Groove Merchant label in 1973)\\r\\nThe song \\"My Long Walk to Jail\\" on Filter's 2002 album The Amalgamut includes a sample of an incoming call from Cook County Jail.\\r\\nThe Cook County Prison was referenced to by Elwood Blues (Dan Aykroyd) in the film The Blues Brothers as serving oatmeal to inmates.\\r\\nCoordinates: 415029N 874151W? / ?41.8414N 87.6975W? / 41.8414; -87.6975","input":"What's the largest jail in the united states?"},{"output":"Clothing in ancient Rome generally comprised a short-sleeved or sleeveless, knee-length tunic for men and boys, and a longer, usually sleeved tunic for women and girls. On formal occasions, adult male citizens could wear a woolen toga, draped over their tunic, and married citizen women wore a woolen mantle, known as a palla, over a stola, a simple, long-sleeved, voluminous garment that hung to midstep. Clothing, footwear and accoutrements identified gender, status, rank and social class, and thus offered a means of social control. This was probably most apparent in the segregation of seating tiers at public theatres, games and festivals. Magistrates, priesthoods and the military had their own distinctive and privileged forms of dress.","context":"Clothing in ancient Rome generally comprised a short-sleeved or sleeveless, knee-length tunic for men and boys, and a longer, usually sleeved tunic for women and girls. On formal occasions, adult male citizens could wear a woolen toga, draped over their tunic, and married citizen women wore a woolen mantle, known as a palla, over a stola, a simple, long-sleeved, voluminous garment that hung to midstep. Clothing, footwear and accoutrements identified gender, status, rank and social class, and thus offered a means of social control. This was probably most apparent in the segregation of seating tiers at public theatres, games and festivals. Magistrates, priesthoods and the military had their own distinctive and privileged forms of dress.\\r\\nThe toga was considered Rome's \\"national costume\\" but for day-to-day activities, most Romans preferred more casual, practical and comfortable clothing; in various forms, the tunic was the basic garment for all classes, both sexes and most occupations. It was usually made of linen, and was augmented as necessary with underwear, or with various kinds of cold-or-wet weather wear, such as knee-breeches for men, and cloaks, coats and hats. In colder parts of the empire, full length trousers were worn. Most urban Romans wore shoes, slippers, boots or sandals of various types; in the countryside, some wore clogs.\\r\\nRelative to the overall basic cost of living, even simple clothing was expensive, and was recycled many times down the social scale. Most clothing was simple in structure and basic form, and its production required minimal cutting and tailoring, but all was produced by hand and every process required skill, knowledge and time. Spinning and weaving were thought virtuous, frugal occupations for Roman women of all classes. Wealthy matrons, including Augustus' wife Livia, might show their traditionalist values by producing home-spun clothing, but most men and women who could afford it bought their clothing from specialist artisans.\\r\\nRome's governing elite produced laws designed to limit public displays of personal wealth and luxury. None were particularly successful; the same wealthy elite had an appetite for luxurious clothing. Exotic fabrics were available, at a price; silk damasks, translucent gauzes, cloth of gold, and intricate embroideries; and vivid, expensive dyes such as saffron yellow or Tyrian purple. Not all dyes were costly, however, and most Romans wore colourful clothing. Clean, bright clothing was a mark of respectability and status among all social classes. The fastenings and brooches used to secure garments such as cloaks provided further opportunities for personal embellishment and display.\\r\\n\\r\\n\\r\\nThe basic garment for both genders and all classes was the tunica (tunic), often worn beneath one or more additional layers. In its simplest form, the tunic was a single rectangle of woven fabric, originally woolen, but from the mid-republic onward, increasingly made from linen. It was sewn into a sleeveless tubular shape and pinned around the shoulders like a Greek chiton, to form openings for the neck and arms. In some examples from the eastern part of the empire, neck openings were formed in the weaving. Sleeves could be added. Most working men wore knee-length, short-sleeved tunics, secured at the waist with a belt. Some traditionalists considered long sleeved tunics appropriate only for women, very long tunics on men as a sign of effeminacy, and short or unbelted tunics as marks of servility. Women's tunics were usually ankle or foot-length, long-sleeved, and could be worn loosely or belted.[1] Though essentially simple in basic design, tunics could also be luxurious in their fabric, colours and detailing.[2]\\r\\nLoincloths, known as subligacula or subligaria could be worn under a tunic. They could also be worn on their own, particularly by slaves who engaged in hot, sweaty or dirty work. Women wore both loincloth and strophium (a breast cloth) under their tunics; and some wore tailored underwear for work or leisure.[3] A 4th century AD Sicillian mosaic shows several \\"bikini girls\\" performing athletic feats; in 1953 a Roman leather bikini bottom was excavated from a well in London.\\r\\nRoman society was graded into several citizen and non-citizen classes and ranks, ruled by a powerful minority of wealthy, landowning citizen-aristocrats. Even the lowest grade of citizenship carried certain privileges denied to non-citizens, such as the right to vote for representation in government. In tradition and law, an individual's place in the citizen-hierarchy - or outside it - should be immediately evident in their clothing. The seating arrangements at theatres and games enforced this idealised social order, with varying degrees of success.\\r\\nIn literature and poetry, Romans were the gens togata (\\"togate race\\"), descended from a tough, virile, intrinsically noble peasantry of hard-working, toga-wearing men and women. The toga's origins are uncertain; it may have begun as a simple, practical work-garment and blanket for peasants and herdsmen. It eventually became formal wear for male citizens; at much the same time, respectable female citizens adopted the stola. The morals, wealth and reputation of citizens were subject to official scrutiny. Male citizens who failed to meet a minimum standard could be demoted in rank, and denied the right to wear a toga; by the same token, female citizens could be denied the stola. Respectable citizens of either sex might thus be distinguished from freedmen, foreigners, slaves and infamous persons.[5]\\r\\nThe basic, unadorned toga virilis (\\"toga of manhood\\") was a semi-elliptical, white woolen cloth some 6 feet in width and 12 feet in length, draped across the shoulders and around the body, over a plain white linen tunic. A commoner's toga virilis was a naturally off-white; the senatorial version was more voluminous, and brighter. The toga praetexta of curule magistrates and some priesthoods added a wide purple edging, and was worn over a tunic with two vertical purple stripes. It could also be worn by noble and freeborn boys and girls, and represented their protection under civil and divine law. Equites wore the trabea (a shorter, \\"equestrian\\" form of white toga or a purple-red wrap, or both) over a white tunic with two narrow vertical purple-red stripes. The toga pulla, used for mourning, was made of dark wool. The rare, prestigious toga picta and tunica palmata were entirely purple, save for their gold embroidery; they were originally awarded to Roman generals for the day of their triumph, and later worn by emperors and Imperial consuls.\\r\\nFrom at least the late Republic onward, the toga became increasingly unsuited to manual work or physically active leisure. It was expensive, heavy, hot and sweaty, hard to keep clean, costly to launder and challenging to wear. A toga worn correctly constrained both posture and gait; it was best suited to stately processions, oratory, sitting in the theatre or circus, and displaying oneself before one's peers and inferiors while \\"ostentatiously doing nothing\\" at salutationes.[6] These early morning, formal \\"greeting sessions\\" were an essential part of Roman life, in which clients attended their patrons, competing for favours or investment in business ventures. A client who dressed well and correctly - in his toga, if a citizen - showed respect for himself and his patron, and might stand out among the crowd. A canny patron might equip his entire family, his friends, freedmen, even his slaves, with elegant, costly and impractical clothing, impyling his entire extended family's condition as one of \\"honorific leisure\\" (otium), buoyed by limitless wealth.[7]\\r\\nThe vast majority of citizens had to work for a living, and avoided wearing the toga whenever possible.[8] Several emperors tried to compel its use as the public dress of true Romanitas but none were particularly successful.[9] The aristocracy clung to it as a mark of their prestige, but eventually abandoned it for the more comfortable and practical pallium.\\r\\nBesides tunics, married citizen women wore a simple garment known as a stola (pl. stolae) which was associated with traditional Roman female virtues.[10] Stolae typically comprised two rectangular segments of cloth joined at the side by fibulae and buttons in a manner allowing the garment to drape freely over the front of the wearer.\\r\\nOver the stola, women often wore the palla, a sort of rectangular shawl up to 11 feet long, and five wide. It could be worn as a coat, or draped over the left shoulder, under the right arm, and then over the left arm. No respectable woman went bareheaded in public, so the palla could also serve as a hooded cloak.[11][12] The combination of stola and palla identified the wearer as a respectable married woman, not to be insulted or trifled with, and certainly not available for sexual predation. In contrast, some Roman literary sources have been interpreted as evidence that high-caste women convicted of adultery, and high-class female prostitutes (meretrices), were not only forbidden public use of the stola, but were expected to wear the toga instead, as a sign of their infamy.[13][14]\\r\\nFor citizens, salutationes meant wearing the toga appropriate to their rank.[15] For freedmen, it meant whatever dress disclosed their status and wealth; a man should be what he seemed, and low rank was no bar to making money. Notwithstanding the commonplace snobbery and mockery of their social superiors, some freedmen and freedwomen were highly cultured and well-connected. Most freedmen became clients of their former master, and could share his personal and business connections. Those with an aptitude for business could amass a fortune; and many did. They could function as patrons, own grand town-houses, and \\"dress to impress\\".[16][17]\\r\\nUnder Roman dress codes, the only certain condition imposed on freedmen was a negative; they were explicitly forbidden to wear any kind of toga. Elite invective mocked the aspirations of wealthy, upwardly mobile freedmen who boldly flouted this probibition, and donned a toga, or even the trabea of an equites, to insert themselves as equals among their social superiors at the games and theatres. If detected, they were evicted from their seats.[18]\\r\\nRoman infants were usually swaddled. Apart from those few, typically formal garments reserved for adults, most children wore a scaled-down version of what their parents wore. Girls often wore a long tunic that reached the foot or instep, belted at the waist and very simply decorated, most often white. Outdoors, they might wear another tunic over it. Boy's tunics were shorter.\\r\\nBoys and girls of citizen family wore amulets to protect them from immoral or baleful influences such as the evil eye and sexual predation. For boys, the amulet was a bulla, worn around the neck; the equivalent for girls was a crescent-shaped lunula. The toga praetexta, which was thought to offer similar apotropaic protection, was formal wear for freeborn boys until puberty, usually around 14 years of age, when they gave their toga praetexta and childhood bulla into the care of their family lares and put on the adult male's toga virilis. According to some Roman literary sources, freeborn girls might also wear - or at least, had the right to wear - a toga praetexta until marriage, when they offered their childhood toys, and perhaps their maidenly praetexta to Fortuna Virginalis; others claim a gift made to the family Lares, or to Venus, as part of their passage to adulthood. In traditionalist families, unmarried girls might be expected to wear their hair demurely bound in a fillet.[19][20]\\r\\nNotwithstanding such attempts to protect the maidenly virtue of Roman girls, there is little anecdotal or artistic evidence of their use or effective imposition. Some unmarried daughters of respectable families seem to have enjoyed going out and about in flashy clothing, jewellery, perfume and make-up;[21] and some parents, anxious to find the best and wealthiest possible match for their daughters, seem to have encouraged it.[22]\\r\\nRomans used a wide variety of practical and decorative footwear, all of it flat soled (without heels). Outdoor shoes were often hobnailed for grip and durability.[23] The commonest types of footwear were a one-piece shoe (carbatina), sometimes with semi-openwork uppers: a usually thin-soled sandal (solea), secured with thongs: a laced, soft half-shoe (soccus): a usually hobnailed, thick-soled walking shoe (calcea): and a heavy-duty, hobnailed standard-issue military marching boot (caliga). Thick-soled wooden clogs, with leather uppers, were available for use in wet weather, and by rustics and field-slaves[24]\\r\\nShoemakers employed sophisticated strapwork and delicate cutting to create intricate decorative patterns. Indoors, most reasonably well-off Romans of both sexes wore slippers or light shoes of felt or leather.[24] Brides on their wedding-day may have worn distinctively orange-coloured light soft shoes or slippers (lutei socci).[25]\\r\\nPublic protocol required red ankle boots for senators, and shoes with crescent-shaped buckles for equites, though some wore Greek-style sandals to \\"go with the crowd\\".[26][27] Costly footwear was a mark of wealth or status, but being completely unshod need not be a mark of poverty. Cato the younger showed his impeccable Republican morality by going publicly barefoot; many images of the Roman gods, and later, statues of the semi-divine Augustus, were unshod.[28][29]\\r\\nFashions in footwear reflected changes in social conditions. For example, during the unstable middle Imperial era, the military was overtly favoured as the true basis for power; at around this time, a so-called \\"Gallic sandal\\" - up to 4 inches broad at the toe - developed as outdoor wear for men and boys, reminiscent of the military boot. Meanwhile, outdoor footwear for women, young girls and children remained elegantly pointed at the toe.[24]\\r\\nFor the most part, common soldiers seem to have dressed in belted, knee-length tunics for work or leisure. In the northern provinces, the traditionally short sleeved tunic might be replaced by a warmer, long-sleeved version. Soldiers on active duty wore short trousers under a military kilt, sometimes with a leather jerkin or felt padding to cushion their armour, and a triangular scarf tucked in at the neck.[3] For added protection from wind and weather, they could wear the sagum, a heavy-duty cloak also worn by civilians. According to Roman tradition, soldiers had once worn togas to war, hitching them up with what was known as a \\"Gabine cinch\\"; but by the mid-Republican era, this had been abandoned.[30] Henceforth, citizen-soldiers wore togas only for formal occasions. Cicero's \\"sagum-wearing\\" soldiers versus \\"toga-wearing\\" civilians are rhetorical and literary trope, referring to a wished-for transition from military might to peaceful, civil authority.[31][32] When on duty in the city, the Praetorian guard concealed their weapons beneath their white \\"civilian\\" togas.[33]\\r\\nThe sagum distinguished common soldiers from the highest ranking commanders, who wore a larger, purple-red cloak, the paludamentum.[34] The colour of the ranker's sagum is uncertain.[35] Roman military clothing was probably less uniform and more adaptive to local conditions and supplies than is suggested by its idealised depictions in contemporary literature, statuary and monuments.[36] Nevertheless, Rome's levies abroad were supposed to represent Rome in her purest form; provincials were supposed to adopt Roman ways, not vice versa. Even when foreign garments - such as trousers - proved more practical than standard issue, soldiers and commanders who used them were viewed with disdain and alarm by their more conservative compatriots, for undermining Rome's military virtus by \\"going native\\".[37][38]\\r\\nIn Mediterranean climates, soldiers typically wore hobnailed \\"open boots\\" (caligae). In colder and wetter climates, an enclosing \\"shoeboot\\" was preferred.[39] Some of the Vindolanda tablets mention the despatch of clothing - including cloaks, socks, and warm underwear - by families to their relatives, serving at Brittania's northern frontier.[40]\\r\\nDuring the early and middle Republican era, conscripted soldiers and their officers were expected to provide or pay for all their personal equipment. From the late republic onwards, they were salaried professionals, and bought their own clothing from legionary stores, quartermasters or civilian contractors. Military needs were prioritised. Clothing was expensive to start with, and the military demand was high; this inevitably pushed up prices, and a common soldier's clothing expenses could be more than a third of his annual pay. In the rampant inflation of the later Imperial era, as currency and salaries were devalued, deductions from military salaries for clothing and other staples were replaced by payments in kind, leaving common soldiers adequately clothed but with little cash for their dependents, or eventual retirement.[41]\\r\\nMost priesthoods were reserved to high status, male Roman citizens, usually magistrates or ex-magistrates. Most traditional religious rites required that the priest wore a toga praetexta, in a manner described as capite velato (head covered [by a fold of the toga]) when performing augury, reciting prayers or supervising at sacrifices.[42] Where a rite prescribed the free use of both arms, the priest could employ the cinctus Gabinus (\\"Gabine cinch\\") to tie back the toga's inconvenient folds.[43]\\r\\nThe Vestal Virgins tended Rome's sacred fire, in Vesta's temple, and prepared essential sacrificial materials employed by different cults of the Roman state. They were highly respected, and possessed unique rights and privileges; their persons were sacred and inviolate. Their presence was required at various religious and civil rites and ceremonies. Their costume was predominantly white, woolen, and had elements in common with high-status Roman bridal dress. They wore a white, priestly infula, a white suffibulum (veil) and a white palla, with red ribbons to symbolise their devotion to Vesta's sacred fire, and white ribbons as a mark of their purity.[44]\\r\\nThe Flamen priesthood was dedicated to various deities of the Roman state. They wore a close-fitting, rounded cap (Apex) topped with a spike of olive-wood; and the laena, a long, semi-circular \\"flame-coloured\\" cloak fastened at the shoulder with a brooch or fibula. Their senior was the Flamen dialis, who was the high priest of Jupiter and was married to the Flamenica dialis. He was not allowed to divorce, leave the city, ride a horse, touch iron, or see a corpse. The laena was thought to predate the toga.[45] The twelve Salii (\\"leaping priests\\" of Mars) were young patrician men, who processed through the city in a form of war-dance during the festival of Mars, singing the Carmen Saliare. They too wore the apex, but otherwise dressed as archaic warriors, in embroidered tunics and breastplates. Each carried a sword, wore a short, red military cloak (paludamentum) and ritually struck a bronze shield, whose ancient original was said to have fallen from heaven.[46]\\r\\nRome recruited many non-native deities, cults and priesthoods as protectors and allies of the state. Aesculapius, Apollo, Ceres and Proserpina were worshiped using the so-called \\"Greek rite\\", which employed Greek priestly dress, or a Romanised version of it. The priest presided in Greek fashion, with his head bare or wreathed.[47]\\r\\nIn 204 BC, the Galli priesthood were brought to Rome from Phrygia, to serve the \\"Trojan\\" Mother Goddess Cybele and her consort Attis on behalf of the Roman state. They were legally protected but flamboyantly \\"un-Roman\\". They were eunuchs, and told fortunes for money; their public rites were wild, frenzied and bloody, and their priestly garb was \\"womanly\\". They wore long, flowing robes of yellow silk, extravagant jewellery, perfume and make-up, and turbans or exotic versions of the \\"phrygian\\" hat over long, bleached hair.[48][49]\\r\\nRoman fashions underwent very gradual change from the late Republic to the end of the Western empire, 600 years later.[50] In part, this reflects the expansion of Rome's empire, and the adoption of provincial fashions perceived as attractively exotic, or simply more practical than traditional forms of dress. Changes in fashion also reflect the increasing dominance of a military elite within government, and a corresponding reduction in the value and status of traditional civil offices and ranks. In the later empire after Diocletian's reforms, clothing worn by soldiers and non-military government beaucrats became highly decorated, with woven or embellished strips, clavi, and circular roundels, orbiculi, added to tunics and cloaks. These decorative elements usually comprised geometrical patterns and stylised plant motifs, but could include human or animal figures.[51] The use of silk also increased steadily and most courtiers in late antiquity wore elaborate silk robes. Heavy military-style belts were worn by bureaucrats as well as soldiers, revealing the general militarization of late Roman government. Trousers  considered barbarous garments worn by Germans and Persians  achieved only limited popularity in the latter days of the empire, and were regarded by conservatives as a sign of cultural decay.[52] The toga, traditionally seen as the sign of true Romanitas, had never been popular or practical. Most likely, its official replacement in the East by the more comfortable pallium and paenula simply acknowledged its disuse.[53] In early medieval Europe, kings and aristocrats dressed like the late Roman generals they sought to emulate, not like the older toga-clad senatorial tradition.[54]\\r\\nWool was the most commonly used fibre in Roman clothing. The sheep of Tarentum were renowned for the quality of their wool, although the Romans never ceased trying to optimise the quality of wool through cross-breeding. Miletus in Asia Minor and the province of Gallia Belgica were also renowned for the quality of their wool exports, the latter producing a heavy, rough wool suitable for winter.[55] For most garments, white wool was preferred; it could then be further bleached, or dyed. Naturally dark wool was used for the toga pulla and work garments subjected to dirt and stains.[56]\\r\\nIn the provinces, private landowners and the State held large tracts of grazing land, where large numbers of sheep were raised and sheared. Their wool was processed and woven in dedicated manufactories. Britannia was noted for its woolen products, which included a kind of duffel coat (the Birrus Brittanicus), fine carpets, and felt linings for army helmets.[57]\\r\\nSilk and cotton were imported to Rome from China and India. As early as the 3rd century BC silk was being purchased by Roman traders from the Carthaginian ports of Tyre and Beirut, where it was woven and dyed after arriving from the far East.[58] As Roman weaving techniques developed, silk yarn was used to make geometrically or freely figured damask, tabbies and tapestry. Some of these silk fabrics were extremely fine - around 50 threads or more per centimeter. Production of such highly decorative, costly fabrics seems to have been a speciality of weavers in the eastern Roman provinces, where the earliest Roman horizontal looms were developed.[59]\\r\\nVarious sumptuary laws and price controls were passed to limit the purchase and use of silk. In the early Empire the Senate passed legislation forbidding the wearing of silk by men because it was viewed as effeminate[60] but there was also a connotation of immorality or immodesty attached to women who wore the material[61], as illustrated by Seneca the Elder:\\r\\n\\"I can see clothes of silk, if materials that do not hide the body, nor even one's decency, can be called clothes... Wretched flocks of maids labour so that the adulteress may be visible through her thin dress, so that her husband has no more acquaintance than any outsider or foreigner with his wife's body.\\" (Declamations Vol. 1)\\r\\nThe Emperor Aurelian is said to have forbidden his wife to buy a mantle of Tyrian purple silk. The Historia Augusta claims that the emperor Elagabalus was the first Roman to wear garments of pure silk (holoserica) as opposed to the usual silk/cotton blends (subserica); this is presented as further evidence of his notorious decadence.[62][63] Moral dimensions aside, Roman importation and expenditure on silk represented a significant, inflationary drain on Rome's gold and silver coinage, to the benefit of foreign traders and loss to the empire. Diocletian's Edict on Maximum Prices of 301 AD set the price of one kilo of raw silk at 4,000 gold coins.[64]\\r\\nWild silk, cocoons collected from the wild after the insect had eaten its way out, was also known;[65] being of shorter smaller lengths, its fibres had to be spun into somewhat thicker yarn than the cultivated variety. A rare luxury cloth with a beautiful golden sheen, known as sea silk, was made from the long silky filaments or byssus produced by Pinna nobilis, a large Mediterranean clam.[66]\\r\\nPliny the Elder describes the production of linen from flax and hemp. After harvesting, the plant stems were retted to loosen the outer layers and internal fibres, stripped, pounded and then smoothed. Following this, the materials were woven. Flax, like wool, came in various speciality grades and qualities. In Pliny's opinion, the whitest (and best) was imported from Spanish Saetabis; at double the price, the strongest and most long-lasting was from Retovium. The whitest and softest was produced in Latium, Falerii and Paelignium. Natural linen was a \\"greyish brown\\" that faded to off-white through repeated laundering and exposure to sunlight. It did not readily absorb the dyes in use at the time, and was generally bleached, or used in its raw, undyed state.[67]\\r\\nHigh quality fabrics were also woven from nettle; poppy-stem fibre was sometimes interwoven with flax, to produce a glossy smooth, lightweight and luxuriant fabric. Preparation techniques were much the same as those for linen.[68]\\r\\nReady-made clothing was available for all classes, at a price; the cost of a new cloak for an ordinary commoner might represent three fifths of their annual subsistence. Clothing was recycled down the social scale, until it fell to rags. Centonarii (\\"patch-workers\\") made a living by sewing clothing and other items from recycled fabric patches.[69] Owners of slave-run farms and sheep-flocks were advised that whenever the opportunity arose, female slaves should be fully occupied in the production of homespun woolen cloth; this would likely be good enough for clothing the better class of slave or supervisor.[70]\\r\\nSelf-sufficiency in clothing paid off. The carding, combing, spinning and weaving of wool were part of daily housekeeping for most women. Those of middling or low income could supplement their personal or family income by spinning and selling yarn, or by weaving fabric for sale. In traditionalist, wealthy households, the family's wool-baskets, spindles and looms were positioned in the semi-public reception area (atrium), where the mater familias and her familia could thus demonstrate their industry and frugality; a largely symbolic and moral activity, rather than practical necessity.[71] Augustus was particularly proud that his wife and daughter had set the best possible example to other Roman women by spinning and weaving his clothing.[72] High-caste brides were expected to make their own wedding garments, using a traditional vertical loom.[73]\\r\\nMost fabric and clothing was produced by professionals whose trades, standards and specialities were protected by guilds; these in turn were recognised and regulated by local authorities.[74] Pieces were woven as closely as possible to their intended final shape, with minimal waste, cutting and sewing thereafter. Once a woven piece of fabric was removed from the loom, its loose end-threads could be tied off, and left as a decorative fringe, or used to add differently coloured \\"Etruscan style\\" borders, as in the purple-red border of the toga praetexta, and the vertical coloured stripe of some tunics;[74] a technique known as \\"tablet weaving\\".[75] Weaving on an upright, hand-powered loom was a slow process. The earliest evidence for the transition from vertical to more efficient horizontal, foot-powered looms comes from Egypt, around 298 AD.[76] Even then, the lack of mechanical aids in spinning made yarn production a major bottleneck in the manufacture of cloth.\\r\\nFrom Rome's earliest days, a wide variety of colours and coloured fabrics would have been available; in Roman tradition, the first association of professional dyers dated back to the days of King Numa. Roman dyers would certainly have had access to the same locally produced, usually plant-based dyes as their neighbours on the Italian peninsula, producing various shades of red, yellow, blue, green, and brown; blacks could be achieved using iron salts and oak gall. Other dyes, or dyed cloths, could have been obtained by trade, or through experimentation. For the very few who could afford it, cloth-of-gold (lam) was almost certainly available, possibly as early as the 7th century BC.[77]\\r\\nThroughout the Regal, Republican and Imperial eras, the fastest, most expensive and sought-after dye was imported Tyrian purple, obtained from the murex. Its hues varied according to processing, the most desirable being a dark \\"dried-blood\\" red.[78] Purple had long-standing associations with regality, and with the divine. It was thought to sanctify and protect those who wore it, and was officially reserved for the border of the toga praetexta, and for the solid purple toga picta. Edicts against its wider, more casual use were not particularly successful; it was also used by wealthy women and, somewhat more disreputably, by some men.[79][80] Verres is reported as wearing a purple pallium at all-night parties, not long before his trial, disgrace and exile for corruption. For those who could not afford genuine Tyrian purple, counterfeits were available.[81] The expansion of trade networks during the early Imperial era brought the dark blue of Indian indigo to Rome; though desirable and costly in itself, it also served as a base for fake Tyrian purple.[82]\\r\\nFor red hues, madder was one of the cheapest dyes available. Saffron yellow was much admired, but costly. It was a deep, bright and fiery yellow-orange, and was associated with purity and constancy. It was used for the flammeum (meaning \\"flame-coloured\\"), a veil used by Roman brides and the Flamenica Dialis, who was virgin at marriage and forbidden to divorce.[83]\\r\\nSpecific colours were associated with chariot-racing teams and their supporters. The oldest of these were the Reds and the Whites. During the later Imperial era, the Blues and Greens dominated chariot-racing and, up to a point, civil and political life in Rome and Constantinople. Though the teams and their supporters had official recognition, their rivalry sometimes spilled into civil violence and riot, both within and beyond the circus venue.[84]\\r\\nThe Romans had two methods of converting animal skins to leather: tanning produced a soft, supple brown leather; \\"tawing\\" in alum and salt produced a soft, pale leather that readily absorbed dyes. Both these processes produced a strong, unpleasant odour, so tanners and tawers shops were usually placed well away from urban centres. Unprocessed animal hides were supplied directly to tanners by butchers, as a byproduct of meat production; some was turned to rawhide, which made a durable shoe-sole. Landowners and livestock ranchers, many of whom were of the elite class, drew a proportion of profits at each step of the process that turned their animals into leather or hide. The trade in leather and hides was empire-wide. The Roman military consumed large quantities of leather; for jerkins, belts, boots, saddles, harness and strapwork, but mostly for military tents.[85][86]\\r\\nThe almost universal habit of public bathing ensured that most Romans kept their bodies clean, reducing the need for frequent washing of garments and bedsheets. Nevertheless, dirt, spillage and staining were constant hazards, and most Romans lived in apartment blocks that lacked facilities for washing clothes on any but the smallest scale. Professional laundries (fullonicae, singular fullonica) were highly malodorous but essential and commonplace features of every city and town. Small fulling enterprises could be found at local market-places; others operated on an industrial scale, and would have required a considerable investment of money and manpower, especially slaves. [87]\\r\\nBasic laundering and fulling techniques were simple, and labour-intensive. Garments were placed in large tubs containing aged urine, then well trodden by bare-footed workers. They were well-rinsed, manually or mechanically wrung, and spread over wicker frames to dry. Whites could be further brightened by bleaching with sulphur fumes. Some colours could be restored to brightness by \\"polishing\\" or \\"refinishing\\" with Cimolian earth. Others would have required separate treatment. In the best-equipped establishments, garments were further smoothed under pressure, using screw-presses.[88] The process was punishingly harsh to fabrics, but purity and cleanliness of clothing was in itself a mark of status. The high-quality woolen togas of the senatorial class were intensively laundered to an exceptional, snowy white, using the best and most expensive ingredients. Lower ranking citizens used togas of duller wool, more cheaply laundered; for reasons that remain unclear, the clothing of different status groups might have been laundered separately.[89]\\r\\nFront of house, fullonicae were run by enterprising citizens of lower social class, or by freedmen and freedwomen; behind the scenes, their enterprise might be supported discreetly by a rich or elite patron, in return for a share of the profits.[90] The Roman elite seem to have despised the fulling and laundering professions as ignoble; though perhaps no more than they despised all manual trades. The fullers themselves evidently thought theirs a respectable and highly profitable profession, worth celebration and illustration in murals and memorials.[91] Pompeian mural paintings of launderers and fullers at work show garments in a rainbow variety of colours, but not white; fullers seem to have been particularly valued for their ability to launder dyed garments without loss of colour, sheen or \\"brightness\\", rather than merely whitening, or bleaching.[92] New cloth and clothing may also have been laundered; the process would have partially felted and strengthened woolen fabrics.[93]","input":"What clothes did they wear in ancient rome?"},{"output":"the creation of the United States Constitution","context":"","input":"What was the outcome of the philadelphia convention?"},{"output":"March 17, 2017","context":"The Originals, a one-hour American supernatural drama, was renewed for a fourth season by The CW on March 17, 2016, by The CW's President, Mark Pedowitz.[1] The 2016ÿ17 United States television season debut of The Originals was pushed to midseason, which saw the fourth-season premiere on March 17, 2017. It concluded on June 23, 2017, after 13 episodes.","input":"When did season 4 of the originals air?"},{"output":"1 July 1997","context":"The transfer of sovereignty over Hong Kong from the United Kingdom to China, referred to as \\"the Handover\\" internationally or \\"the Return\\" in China, took place on 1 July 1997. The landmark event marked the end of British administration in Hong Kong, and is often regarded as marking the end of the British Empire.\\r\\n\\r\\n\\r\\nHong Kong's territory was acquired from three separate treaties: the Treaty of Nanking in 1842, the Convention of Peking in 1860, and The Convention for the Extension of Hong Kong Territory in 1898, which gave the UK the control of Hong Kong Island, Kowloon (area south of Boundary Street), and the New Territories (area north of Boundary Street and south of the Sham Chun River, and outlying islands), respectively.\\r\\nAlthough Hong Kong Island and Kowloon had been ceded to the United Kingdom in perpetuity, the control on the New Territories was a 99-year lease. The finite nature of the 99-year lease did not hinder Hong Kong's development as the New Territories were combined as a part of Hong Kong.\\r\\nHowever, by 1997, it was impractical to separate the three territories and only return the New Territories. In addition, with the scarcity of land and natural resources in Hong Kong Island and Kowloon, the New Territories were being developed with large-scale infrastructures and other developments, with the break-even day lying well past 30 June 1997. Thus, the status of the New Territories after the expiry of the 99-year lease became important for Hong Kong's economic development.[1]\\r\\nWhen the People's Republic of China obtained its seat in the United Nations as a result of the UN General Assembly Resolution 2758 in 1971, it began to act diplomatically on the sovereignty issues of Hong Kong and Macau. In March 1972, the Chinese UN representative, Huang Hua, wrote to the United Nations Decolonization Committee to state the position of the Chinese government:\\r\\nThe same year, on 8 November, the United Nations General Assembly passed the resolution on removing Hong Kong and Macau from the official list of colonies.[2]\\r\\nIn March 1979 the Governor of Hong Kong, Murray MacLehose, paid his first official visit to the People's Republic of China (PRC), taking the initiative to raise the question of Hong Kong's sovereignty with Deng Xiaoping.[3] Without clarifying and establishing the official position of the PRC government, the arranging of real estate leases and loans agreements in Hong Kong within the next 18 years would become difficult.[1]\\r\\nIn response to concerns over land leases in the New Territories, MacLehose proposed that British administration of the whole of Hong Kong, as opposed to sovereignty, be allowed to continue after 1997.[4] He also proposed that contracts include the phrase \\"for so long as the Crown administers the territory\\".[5]\\r\\nIn fact, as early as the mid-1970s, Hong Kong had faced additional risks raising loans for large-scale infrastructure projects such as its Mass Transit Railway (MTR) system and a new airport. Caught unprepared, Deng asserted the necessity of Hong Kong's return to China, upon which Hong Kong would be given special status by the PRC government.\\r\\nMacLehose's visit to the PRC raised the curtain on the issue of Hong Kong's sovereignty: Britain was made aware of the PRC's aspiration to resume sovereignty over Hong Kong and began to make arrangements accordingly to ensure the sustenance of her interests within the territory, as well as initiating the creation of a withdrawal plan in case of emergency.\\r\\nThree years later, Deng received the former British Prime Minister Edward Heath, who had been dispatched as the special envoy of Prime Minister Margaret Thatcher to establish an understanding of the PRC's view with regards to the question of Hong Kong; during their meeting, Deng outlined his plans to make the territory a special economic zone, which would retain its capitalist system under Chinese sovereignty.[6]\\r\\nIn the same year, Edward Youde, who succeeded MacLehose as the 26th Governor of Hong Kong, led a delegation of five Executive Councillors to London, including Chung Sze-yuen, Lydia Dunn, and Roger Lobo.[7] Chung presented their position on the sovereignty of Hong Kong to Thatcher, encouraging her to take into consideration the interests of the native Hong Kong population in her upcoming visit to China.[7]\\r\\nIn light of the increasing openness of the PRC government and economic reforms on the mainland, the then British Prime Minister Margaret Thatcher sought the PRC's agreement to a continued British presence in the territory.[8]\\r\\nHowever, the PRC took a contrary position: not only did the PRC wish for the New Territories, on lease until 1997, to be placed under the PRC's jurisdiction, it also refused to recognize the \\"unfair and unequal treaties\\" under which Hong Kong Island and Kowloon had been ceded to Britain in perpetuity.[9] Consequently, the PRC recognized only the British administration in Hong Kong, but not British sovereignty.[9]\\r\\nIn the wake of Governor MacLehose's visit, Britain and the PRC established initial diplomatic contact for further discussions of the Hong Kong question, paving the way for Thatcher's first visit to the PRC in September 1982.[10]\\r\\nMargaret Thatcher, in discussion with Deng Xiaoping, reiterated the validity of an extension of the lease of Hong Kong territory, particularly in light of binding treaties, including the Treaty of Nanking in 1842, the Convention of Peking in 1856, and the Convention for the Extension of Hong Kong Territory signed in 1890.\\r\\nIn response, Deng Xiaoping cited clearly the lack of room for compromise on the question of sovereignty over Hong Kong; the PRC, as the successor of Qing Dynasty and the Republic of China on the mainland, would recover the entirety of the New Territories, Kowloon and Hong Kong Island. China considered treaties about Hong Kong as unequal and ultimately refused to accept any outcome that would indicate permanent loss of sovereignty over Hong Kong's area, whatever wording the former treaties had.[11]\\r\\nDuring talks with Thatcher, China planned to invade and seize Hong Kong if the negotiations set off unrest in the colony. Thatcher later said that Deng told her bluntly that China could easily take Hong Kong by force, stating that \\"I could walk in and take the whole lot this afternoon\\", to which she replied that \\"there is nothing I could do to stop you, but the eyes of the world would now know what China is like\\".[12]\\r\\nAfter her visit with Deng in Beijing, Thatcher was received in Hong Kong as the first British Prime Minister to set foot on the territory whilst in office. At a press conference, Thatcher re-emphasised the validity of the three treaties, asserting the need for countries to respect treaties on universal terms: \\"There are three treaties in existence; we stick by our treaties unless we decide on something else. At the moment, we stick by our treaties.\\".[8]\\r\\nAt the same time, at the 5th session of the 5th National People's Congress, the constitution was amended to include a new Article 31 which stated that the country might establish Special Administrative Regions (SARs) when necessary.[13]\\r\\nThe additional Article would hold tremendous significance in settling the question of Hong Kong and later Macau, putting into social consciousness the concept of \\"One country, two systems\\". The concept would prove useful to deploy until the territories were secured and conditions were ripe for its gradual abrogation.\\r\\nA few months after Thatcher's visit to Beijing, the PRC government had yet to open negotiations with the British government regarding the sovereignty of Hong Kong.\\r\\nShortly before the initiation of sovereignty talks, Governor Youde declared his intention to represent the population of Hong Kong at the negotiations. This statement sparked a strong response from the PRC, prompting Deng Xiaoping to denounce talk of \\"the so-called 'three-legged stool'\\", which implied that Hong Kong was a party to talks on its future, alongside Beijing and London.[14]\\r\\nAt the preliminary stage of the talks, the British government proposed an exchange of sovereignty for administration and the implementation of a British administration post-handover.[8]\\r\\nThe PRC government refused, contending that the notions of sovereignty and administration were inseparable, and although it recognised Macau as a \\"Chinese territory under Portuguese administration\\", this was only temporary.[15]\\r\\nIn fact, during informal exchanges between 1979 and 1981, the PRC had proposed a \\"Macau solution\\" in Hong Kong, under which it would remain under British administration at China's discretion.[3]\\r\\nHowever, this had previously been rejected following the 1967 Leftist riots, with the then Governor, David Trench, claiming the leftists' aim was to leave the UK without effective control, or \\"to Macau us\\".[16]\\r\\nThe conflict that arose at that point of the negotiations ended the possibility of further negotiation. During the reception of former British Prime Minister Edward Heath during his sixth visit to the PRC, Deng Xiaoping commented quite clearly on the impossibility of exchanging sovereignty for administration, declaring an ultimatum: the British government must modify or give up its position or the PRC will announce its resolution of the issue of Hong Kong sovereignty unilaterally.[17]\\r\\nIn 1983, Typhoon Ellen ravaged Hong Kong, causing great amounts of damage to both life and property.[18] The Hong Kong dollar plummeted on Black Saturday, and the Financial Secretary John Bremridge publicly associated the economic uncertainty with the instability of the political climate.[19] In response, the PRC government condemned Britain through the press for \\"playing the economic card\\" in order to achieve their ends: to intimidate the PRC into conceding to British demands.[20]\\r\\nGovernor Youde with nine members of the Hong Kong Executive Council travelled to London to discuss with Prime Minister Thatcher the crisis of confidencethe problem with morale among the people of Hong Kong arising from the ruination of the Sino-British talks. The session concluded with Thatcher's writing of a letter addressed to the PRC Premier Zhao Ziyang.\\r\\nIn the letter, she expressed Britain's willingness to explore arrangements optimising the future prospects of Hong Kong while utilising the PRC's proposals as a foundation. Furthermore, and perhaps most significantly, she expressed Britain's concession on its position of a continued British presence in the form of an administration post-handover.\\r\\nTwo rounds of negotiations were held in October and November. On the sixth round of talks in November, Britain formally conceded its intentions of either maintaining a British administration in Hong Kong or seeking some form of co-administration with the PRC, and showed its sincerity in discussing PRC's proposal on the 1997 issue. Obstacles were cleared.\\r\\nSimon Keswick, chairman of Jardine Matheson & Co., said they were not pulling out of Hong Kong, but a new holding company would be established in Bermuda instead.[21] The PRC took this as yet another plot by the British. The Hong Kong government explained that it had been informed about the move only a few days before the announcement. The government would not and could not stop the company from making a business decision.\\r\\nJust as the atmosphere of the talks was becoming cordial, members of the Legislative Council of Hong Kong felt impatient at the long-running secrecy over the progress of Sino-British talks on the Hong Kong issue. A motion, tabled by legislator Roger Lobo, declared \\"This Council deems it essential that any proposals for the future of Hong Kong should be debated in this Council before agreement is reached\\", was passed unanimously.[22]\\r\\nThe PRC attacked the motion furiously, referring to it as \\"somebody's attempt to play the three-legged stool trick again\\".[23] At length, the PRC and Britain initiated the Joint Declaration on the question of Hong Kong's future in Beijing. Zhou Nan, the then PRC Deputy Foreign Minister and leader of the negotiation team, and Sir Richard Evans, British Ambassador to Beijing and leader of the team, signed respectively on behalf of the two governments.[24]\\r\\nThe Sino-British Joint Declaration was signed by the Prime Ministers of the People's Republic of China and the United Kingdom governments on 19 December 1984 in Beijing. The Declaration entered into force with the exchange of instruments of ratification on 27 May 1985 and was registered by the People's Republic of China and United Kingdom governments at the United Nations on 12 June 1985.\\r\\nIn the Joint Declaration, the People's Republic of China Government stated that it had decided to resume the exercise of sovereignty over Hong Kong (including Hong Kong Island, Kowloon, and the New Territories) with effect from 1 July 1997 and the United Kingdom Government declared that it would restore Hong Kong to the PRC with effect from 1 July 1997. In the document, the People's Republic of China Government also declared its basic policies regarding Hong Kong.\\r\\nIn accordance with the \\"One Country, Two Systems\\" principle agreed between the United Kingdom and the People's Republic of China, the socialist system of the People's Republic of China would not be practised in the Hong Kong Special Administrative Region (HKSAR), and Hong Kong's previous capitalist system and its way of life would remain unchanged for a period of 50 years. This would have left Hong Kong unchanged until 2047.\\r\\nThe Joint Declaration provided that these basic policies should be stipulated in the Hong Kong Basic Law. The ceremony of the signing of the Sino-British Joint Declaration took place at 18:00, 19 December 1984 at the Western Main Chamber of the Great Hall of the People. The Hong Kong and Macao Affairs Office at first proposed a list of 60-80 Hong Kong people to attend the ceremony. The number was finally extended to 101.\\r\\nThe list included Hong Kong government officials, members of the Legislative and Executive Councils, chairmen of the Hongkong and Shanghai Banking Corporation and Standard Chartered Bank, prominent businessmen such as Li Ka-shing, Pao Yue-kong and Fok Ying-tung, and also Martin Lee Chu-ming and Szeto Wah.\\r\\nThe Basic Law was drafted by a Drafting Committee composed of members from both Hong Kong and mainland China. A Basic Law Consultative Committee formed purely by Hong Kong people was established in 1985 to canvas views in Hong Kong on the drafts.\\r\\nThe first draft was published in April 1988, followed by a five-month public consultation exercise. The second draft was published in February 1989, and the subsequent consultation period ended in October 1989.\\r\\nThe Basic Law was formally promulgated on 4 April 1990 by the NPC, together with the designs for the flag and emblem of the HKSAR. Some members of the Basic Law drafting committee were ousted by Beijing following the 4 June 1989 Tiananmen Square protests, after voicing views supporting the students.\\r\\nThe Basic Law was said to be a mini-constitution drafted with the participation of Hong Kong people. The political system had been the most controversial issue in the drafting of the Basic Law. The special issue sub-group adopted the political model put forward by Louis Cha. This \\"mainstream\\" proposal was criticised for being too conservative.[citation needed]\\r\\nAccording to Clauses 158 and 159 of the Basic Law, powers of interpretation and amendment of the Basic Law are vested in the Standing Committee of the National People's Congress and the National People's Congress, respectively. Hong Kong's people have limited influence.\\r\\nAfter the Tiananmen Square protests of 1989, the Executive Councillors and the Legislative Councillors unexpectedly held an urgent meeting, in which they agreed unanimously that the British Government should give the people of Hong Kong the right of abode in the United Kingdom.[25]\\r\\nMore than 10,000 Hong Kong residents rushed to Central in order to get an application form for residency in the United Kingdom. On the eve of the deadline, over 100,000 lined up overnight for a BN(O) application form. While mass migration did begin well before 1989, the event did lead to the peak migration year in 1992 with 66,000 leaving.[26]\\r\\nMany citizens were pessimistic towards the future of Hong Kong and the transfer of the region's sovereignty. A tide of emigration, which was to last for no less than five years, broke out. At its peak, citizenship of small countries, such as Tonga, was also in great demand.[27]\\r\\nSingapore, which also had a predominantly Chinese population, was another popular destination, with the country's Commission (now Consulate-General) being besieged by anxious Hong Kong residents.[28] By September 1989, 6000 applications for residency in Singapore had been approved by the Commission.[29]\\r\\nSome consul staff were suspended or arrested for their corrupt behaviour in granting immigration visas. In April 1997, the acting immigration officer at the US Consulate-General, James DeBates, was suspended after his wife was arrested for smuggling of Chinese migrants into the United States.[30] The previous year, his predecessor, Jerry Stuchiner, had been arrested for smuggling forged Honduran passports into the territory before being sentenced to 40 months in prison.[31]\\r\\nCanada (Vancouver and Toronto), United Kingdom (London, Glasgow, and Manchester), Australia (Sydney and Melbourne), and the United States (San Francisco and New York) were, by and large, the most popular destinations. The United Kingdom devised the British Nationality Selection Scheme, granting 50,000 families British citizenship under the British Nationality Act (Hong Kong) 1990.[32]\\r\\nVancouver was among the most popular destinations, earning the nickname of \\"Hongcouver\\".[33] Richmond, a suburb of Vancouver, was nicknamed \\"Little Hong Kong\\".[34] Other popular settlements are found in Auckland, New Zealand and Dublin, Ireland. All in all, from the start of the settlement of the negotiation in 1984 to 1997, nearly 1 million people emigrated; consequently, Hong Kong suffered serious loss of capital.[35]\\r\\nChris Patten became the last governor of Hong Kong. This was regarded as a turning point in Hong Kong's history. Unlike his predecessors, Patten was not a diplomat, but a career politician and former Member of Parliament. He introduced democratic reforms which pushed PRCÿBritish relations to a standstill and affected the negotiations for a smooth handover.\\r\\nPatten introduced a package of electoral reforms in the Legislative Council. These reforms proposed to enlarge the electorate, thus making voting in the Legislative Council more democratic. This move posed significant changes because Hong Kong citizens would have the power to make decisions regarding their future.\\r\\nThe handover ceremony was held at the new wing of the Hong Kong Convention and Exhibition Centre in Wan Chai on the night of 30 June 1997.\\r\\nThe principal British guest was Prince Charles, who read a farewell speech on behalf of the Queen. The newly elected Prime Minister, Tony Blair, the Foreign Secretary Robin Cook, the departing Governor Chris Patten and General Sir Charles Guthrie, Chief of the Defence Staff, also attended.\\r\\nRepresenting the People's Republic of China were the President, Jiang Zemin, the Premier, Li Peng, and the first Chief Executive Tung Chee-hwa. The event was broadcast around the world.[36][37]\\r\\nAfter the Tiananmen Square protests of 1989, the Hong Kong government proposed a grand \\"Rose Garden Project\\" to restore faith and solidarity among the residents.[124] As the construction of the new Hong Kong International Airport would extend well after the handover, Governor Wilson met PRC Premier Li Peng in Beijing to ease the mind of the PRC government.[125]\\r\\nThe communist press published stories that the project was an evil plan to bleed Hong Kong dry before the handover, leaving the territory in serious debt.[126] After three years of negotiations, Britain and the PRC finally reached an agreement over the construction of the new airport, and signed a Memorandum of Understanding.[127] Removing hills and reclaiming land, it took only a few years to construct the new airport.\\r\\nThe Walled City was originally a single fort built in the mid-19th century on the site of an earlier 17th century watch post on the Kowloon Peninsula of Hong Kong.[128] After the ceding of Hong Kong Island to Britain in 1842 (Treaty of Nanjing), Manchu Qing Dynasty authorities of China felt it necessary for them to establish a military and administrative post to rule the area and to check further British influence in the area.\\r\\nThe 1898 Convention which handed additional parts of Hong Kong (the New Territories) to Britain for 99 years excluded the Walled City, with a population of roughly 700. It stated that China could continue to keep troops there, so long as they did not interfere with Britain's temporary rule.\\r\\nBritain quickly went back on this unofficial part of the agreement, attacking Kowloon Walled City in 1899, only to find it deserted. They did nothing with it, or the outpost, and thus posed the question of Kowloon Walled City's ownership squarely up in the air. The outpost consisted of a yamen, as well as buildings which grew into low-lying, densely packed neighbourhoods from the 1890s to 1940s.\\r\\nThe enclave remained part of Chinese territory despite the turbulent events of the early 20th century that saw the fall of the Qing government, the establishment of the Republic of China and later, a Communist Chinese government (PRC).\\r\\nSquatters began to occupy the Walled City, resisting several attempts by Britain in 1948 to drive them out. The Walled City became a haven for criminals and drug addicts, as the Hong Kong Police had no right to enter the City and China refused maintainability. The 1949 foundation of the People's Republic of China added thousands of refugees to the population, many from Guangdong; by this time, Britain had had enough, and simply adopted a \\"hands-off\\" policy.\\r\\nA murder that occurred in Kowloon Walled City in 1959 set off a small diplomatic crisis, as the two nations each tried to get the other to accept responsibility for a vast tract of land now virtually ruled by anti-Manchurian Triads.\\r\\nAfter the Joint Declaration in 1984, the PRC allowed British authorities to demolish the City and resettle its inhabitants. The mutual decision to tear down the walled city was made in 1987.[129] The government spent up to HK$ 3 billion to resettle the residents and shops.\\r\\nSome residents were not satisfied with the compensation, and some even obstructed the demolition in every possible way.[130] Ultimately, everything was settled, and the Walled City became a park.[131]\\r\\nRennie's Mill got its name from a Canadian businessman named Alfred Herbert Rennie, who established a flour mill at Junk Bay. The business failed, and Rennie hanged himself there in 1908. The incident gave the Chinese name for the site Tiu Keng Leng (?X), meaning \\"Hanging (neck) Ridge\\". The name was later formally changed to similar-sounding Tiu King Leng (?X) because it was regarded as inauspicious.\\r\\nIn the 1950s the (British) Government of Hong Kong settled a considerable number of refugees from Chinaformer Nationalist soldiers and other Kuomintang supportersat Rennie's Mill, following the Chinese civil war. For many years the area was a Kuomintang enclave known as \\"Little Taiwan\\", with the flag of the Republic of China flying, its own school system and practically off-limits to the Royal Hong Kong Police Force.\\r\\nIn 1996 the Hong Kong government finally forcibly evicted Rennie's Mill's residents, ostensibly to make room for new town developments, as part of the Tseung Kwan O New Town, but widely understood to be a move to please the Communist Chinese government before Hong Kong reverted to Communist Chinese rule in 1997.\\r\\nBefore the eviction, Rennie's Mill could be reached by the winding, hilly and narrow Po Lam Road South. At that time, Rennie's Mill's only means of public transport were the routes 90 and 290 of Kowloon Motor Bus, which were operated by minibuses, and by water transport.\\r\\nThe Republic of China on Taiwan promulgated the Laws and Regulations Regarding Hong Kong & Macao Affairs on 2 April 1997 by Presidential Order, and the Executive Yuan on 19 June 1997 ordered the provisions pertaining to Hong Kong to take effect on 1 July 1997.[132]\\r\\nThe United StatesÿHong Kong Policy Act or more commonly known as the Hong Kong Policy Act (P.L no. 102-383m 106 Stat. 1448) is a 1992 act enacted by the United States Congress. It allows the United States to continue to treat Hong Kong separately from China for matters concerning trade export and economics control after the handover.[133]\\r\\nThe United States was represented by then Secretary of State Madeleine Albright at the Hong Kong handover ceremony.[134] However, she partially boycotted it in protest of China's dissolution of the democratically elected Hong Kong legislature.[135]\\r\\nScholars have begun to study the complexities of the transfer as shown in the popular media, such as films, television and video and online games. For example, Hong Kong director Fruit Chan made a sci-fi thriller The Midnight After (2014) that stressed the sense of loss and alienation represented by survivors in an apocalyptic Hong Kong. Chan infuses a political agenda in the film by playing on Hong Kongers' collective anxiety towards communist China.[136] Yiman Wang has argued that America has viewed China through the prisms of films from Shanghai and Hong Kong, with a recent emphasis on futuristic disaster films set in Hong Kong after the transfer goes awry.[137]","input":"When did great britain give back hong kong?"},{"output":"The mount point for the EFI system partition","context":"The EFI system partition (ESP) is a partition on a data storage device (usually a hard disk drive or solid-state drive) that is used by computers adhering to the Unified Extensible Firmware Interface (UEFI). When a computer is booted, UEFI firmware loads files stored on the ESP to start installed operating systems and various utilities. An ESP needs to be formatted with a file system whose specification is based on the FAT file system and maintained as part of the UEFI specification; therefore, the file system specification is independent from the original FAT specification.[1][2]\\r\\nAn ESP contains the boot loaders or kernel images for all installed operating systems (which are contained in other partitions), device driver files for hardware devices present in a computer and used by the firmware at boot time, system utility programs that are intended to be run before an operating system is booted, and data files such as error logs.[3]\\r\\n\\r\\n\\r\\nThe EFI system partition needs to be formatted with a file system whose specification is maintained as part of the UEFI specification; the file system itself is based on the FAT file system but is independent from the original FAT specification.[1] The globally unique identifier (GUID) for the EFI system partition in the GUID Partition Table (GPT) scheme is C12A7328-F81F-11D2-BA4B-00A0C93EC93B, while its ID in the master boot record (MBR) partition-table scheme is 0xEF. Both GPT- and MBR-partitioned disks can contain an EFI system partition, as UEFI firmware is required to support both partitioning schemes. Also, El Torito bootable format for CD-ROMs and DVDs is supported.[3]\\r\\nUEFI provides backward compatibility with legacy systems by reserving the first block (sector) of the partition for compatibility code, effectively creating a legacy boot sector. On legacy BIOS-based systems, the first sector of a partition is loaded into memory and execution is transferred to this code. UEFI firmware does not execute the code in the MBR, except when booting in legacy BIOS mode through the Compatibility Support Module (CSM).[3]\\r\\nThe UEFI specification requires MBR partition tables to be fully supported.[3] However, some UEFI implementations immediately switch to the BIOS-based CSM booting upon detecting certain types of partition table on the boot disk, effectively preventing UEFI booting to be performed from EFI system partitions contained on MBR-partitioned disks.[4]\\r\\nUEFI firmware supports booting from removable storage devices such as USB flash drives. For that purpose, a removable device needs to be formatted with a FAT12, FAT16 or FAT32 file system, while a boot loader needs to be stored according to the standard ESP file hierarchy, or by providing a complete path of a boot loader to the system's boot manager.[3]\\r\\nGRUB 2 and elilo serve as conventional, full-fledged standalone UEFI boot managers for Linux. Once loaded by a UEFI firmware, they both can access and boot kernel images from all devices, partitions and file systems they support, without being limited to the EFI system partition.\\r\\nEFI Boot Stub makes it possible to boot a Linux kernel image without the use of a conventional UEFI boot loader. By masquerading itself as a PE/COFF image and appearing to the firmware as a UEFI application, an x86 kernel image with EFI Boot Stub enabled can be directly loaded and executed by a UEFI firmware. Such kernel images can still be loaded and run by BIOS-based boot loaders; thus, EFI Boot Stub allows a single kernel image to work in any boot environment.[5]\\r\\nLinux kernel's support for the EFI Boot Stub is enabled by turning on option CONFIG_EFI_STUB (EFI stub support) during the kernel configuration.[6] It was merged into version 3.3 of the Linux kernel mainline, released on March 18, 2012.[7] Gummiboot (a.k.a. systemd-boot) is a simple UEFI boot manager that loads and runs configured UEFI images, accessing only the EFI system partition. Configuration file fragments, kernel images and initrd images are required to reside on the EFI system partition, as Gummiboot does not provide support for accessing files on other partitions or file systems. Linux kernels need to be built with CONFIG_EFI_STUB enabled so they can be directly executed as UEFI images.[8]\\r\\nThe mount point for the EFI system partition is usually /boot/efi, where its content is accessible after Linux is booted.[9]\\r\\nOn Mactel computers based on x64, the EFI system partition is initially left blank and unused for booting.[10] However, the EFI system partition is used as a staging area for firmware updates.[11]\\r\\nThe system will still boot after the EFI partition is deleted, in which case the boot manager will allow users to choose whether to start a Boot Camp partition or the default Mac OS X, but firmware updates will fail.[citation needed]\\r\\nMicrosoft recommends that when partitioning a disk, the EFI system partition be the first partition on the disk.[12] This is not a requirement of the EFI specification itself. On Windows XP 64-Bit Edition and later, access to the EFI system partition is obtained by running the mountvol /s command.\\r\\nTrueOS' UEFI support (for amd64 only) has been added to the installer and the boot manager since version 10.1 with the default EFI boot manager to be rEFInd.[13] This includes ACPI detection and setup of Root System Description Pointer (RSDP),[14] eXtended System Descriptor Table (XSDT),[15] and Root System Description Table (RSDT)[16] pass-through values to the kernel. A new installation is needed in order to install UEFI support as it requires the creation of a small FAT partition. The current UEFI does not support secure boot.","input":"What is /boot/efi in linux?"},{"output":"the atmosphere","context":"Global commons is a term typically used to describe international, supranational, and global resource domains in which common-pool resources are found. Global commons include the earth's shared natural resources, such as the high oceans, the atmosphere and outer space and the Antarctic in particular.[1] Cyberspace may also meet the definition of a global commons.\\r\\n\\r\\n\\"Global commons\\" is a term typically used to describe international, supranational, and global resource domains in which common-pool resources are found. In economics, common goods are rivalrous and non-excludable, constituting one of the four main types of goods.[2] A common-pool resource, also called a common property resource, is a special case of a common good (or public good) whose size or characteristics makes it costly, but not impossible, to exclude potential users.  Examples include both natural or human-made resource domains (e.g., a \\"fishing hole\\" or an irrigation system). Unlike global public goods, global common-pool resources face problems of congestion, overuse, or degradation because they are subtractable (which makes them rivalrous).[3]\\r\\n\\r\\nThe term \\"commons\\" originates from the term common land in the British Isles.[4] \\"Commoners rights\\" referred to traditional rights held by commoners, such as mowing meadows for hay or grazing livestock on common land held in the open field system of old English common law.  Enclosure was the process that ended those traditional rights, converting open fields to private property.  Today, many commons still exist in England, Wales, Scotland, and the United States, although their extent is much reduced from the millions of acres that existed until the 17th century.[5] There are still over 7,000 registered commons in England alone.[6]\\r\\n\\r\\nThe term \\"global commons\\" is typically used to indicate the earth's shared natural resources, such as the deep oceans, the atmosphere, outer space and the Northern and Southern polar regions, the Antarctic in particular.[7]\\r\\n\\r\\nAccording to the World Conservation Strategy, a report on conservation published by the International Union for Conservation of Nature and Natural Resources (IUCN) in collaboration with UNESCO and with the support of the United Nations Environment Programme (UNEP) and the World Wildlife Fund (WWF):\\r\\n\\r\\n\\"A commons is  a tract of land or water owned or used jointly by the members of a community. The global commons includes those parts of the Earth's surface beyond national jurisdictions? notably the open ocean and the living resources found there? or held in common? notably the atmosphere. The only landmass that may be regarded as part of the global commons is Antarctica ...\\"[8]\\r\\n\\r\\nToday, the Internet, World Wide Web and resulting cyberspace are often referred to as global commons.[9]  Other usages sometimes include references to open access information of all kinds, including arts and culture, language and science, though these are more formally referred to as the common heritage of mankind.[10]\\r\\n\\r\\nThe key challenge of the global commons is the design of governance structures and management systems capable of addressing the complexity of multiple public and private interests, subject to often unpredictable changes, ranging from the local to the global level.[11] As with global public goods, management of the global commons requires pluralistic legal entities, usually international and supranational, public and private, structured to match the diversity of interests and the type of resource to be managed, and stringent enough with adequate incentives to ensure compliance.[12] Such management systems are necessary to avoid, at the global level, the classic tragedy of the commons, in which common resources become overexploited.[13]\\r\\n\\r\\nThere are several key differences in management of resources in the global commons from those of the commons, in general.[14] There are obvious differences in scale of both the resources and the number of users at the local versus the global level.  Also, there are differences in the shared culture and expectations of resource users; more localized commons users tend to be more homogeneous and global users more heterogeneous. This contributes to differences in the possibility and time it takes for new learning about resource usage to occur at the different levels. Moreover, global resource pools are less likely to be relatively stable and the dynamics are less easily understood. Many of the global commons are non-renewable on human time scales. Thus, resource degradation is more likely to be the result of unintended consequences that are unforeseen, not immediately observable, or not easily understood. For example, the carbon dioxide emissions that drive climate change continue to do so for at least a millennium after they enter the atmosphere[15] and species extinctions last forever.  Importantly, because there are significant differences in the benefits, costs, and interests at the global level, there are significant differences in externalities between more local resource uses and uses of global-level resources.\\r\\n\\r\\nSeveral environmental protocols have been established (see List of international environmental agreements) as a type of international law, \\"an intergovernmental document intended as legally binding with a primary stated purpose of preventing or managing human impacts on natural resources.\\"[16] International environmental protocols came to feature in environmental governance after trans-boundary environmental problems became widely perceived in the 1960s.[17]  Following the Stockholm Intergovernmental Conference in 1972, creation of international environmental agreements proliferated.[18] Due to the barriers already discussed, environmental protocols are not a panacea for global commons issues.  Often, they are slow to produce the desired effects, tend to the lowest common denominator, and lack monitoring and enforcement. They also take an incremental approach to solutions where sustainable development principles suggest that environmental concerns should be mainstream political issues.\\r\\n\\r\\nThe global or world ocean, as the interconnected system of the Earth's oceanic (or marine) waters that comprise the bulk of the hydrosphere, is a classic global commons.  It is divided into a number of principal oceanic areas that are delimited by the continents and various oceanographic features.  In turn, oceanic waters are interspersed by many smaller seas, gulfs, and bays.  Further, most freshwater bodies ultimately empty into the ocean and are derived through the Earth's water cycle from ocean waters.  The Law of the Sea is a body of public international law governing relationships between nations in respect to navigational rights, mineral rights, and jurisdiction over coastal waters.  Maritime law, also called Admiralty law, is a body of both domestic law governing maritime activities and private international law governing the relationships between private entities which operate vessels on the oceans. It deals with matters including marine commerce, marine navigation, shipping, sailors, and the transportation of passengers and goods by sea. However, these bodies of law do little to nothing to protect deep oceans from human threats.\\r\\n\\r\\nIn addition to providing significant means of transportation, a large proportion of all life on Earth exists in its ocean, which contains about 300 times the habitable volume of terrestrial habitats.  Specific marine habitats include coral reefs, kelp forests, seagrass meadows, tidepools, muddy, sandy and rocky bottoms, and the open ocean (pelagic) zone, where solid objects are rare and the surface of the water is the only visible boundary. The organisms studied range from microscopic phytoplankton and zooplankton to huge cetaceans (whales) 30 meters (98 feet) in length.\\r\\n\\r\\nAt a fundamental level, marine life helps determine the very nature of our planet. Marine life resources provide food (especially food fish), medicines, and raw materials. It is also becoming understood that the well-being of marine organisms and other organisms are linked in very fundamental ways. The human body of knowledge regarding the relationship between life in the sea and important cycles is rapidly growing, with new discoveries being made nearly every day. These cycles include those of matter (such as the carbon cycle) and of air (such as Earth's respiration, and movement of energy through ecosystems including the ocean). Marine organisms contribute significantly to the oxygen cycle, and are involved in the regulation of the Earth's climate.[19] Shorelines are in part shaped and protected by marine life, and some marine organisms even help create new land.[20]\\r\\n\\r\\nThe United Nations Environment Programme (UNEP) has identified several areas of need in managing the global ocean: strengthen national capacities for action, especially in developing countries; improve fisheries management;     reinforce cooperation in semi-enclosed and regional seas; strengthen controls over ocean disposal of hazardous and nuclear wastes; and advance the Law of the Sea. Specific problems identified as in need of attention include rising sea levels; contamination by hazardours chemicals (including oil spills); microbiological contamination; ocean acidification; harmful algal blooms; and over-fishing and other overexploitation.[21] Further, the Pew Charitable Trusts Environmental Initiative program has identified a need for a worldwide system of very large, highly protected marine reserves where fishing and other extractive activities are prohibited.[22]\\r\\n\\r\\nThe atmosphere is a complex dynamic natural gaseous system that is essential to support life on planet Earth. A primary concern for management of the global atmosphere is air pollution, the introduction into the atmosphere of chemicals, particulates, or biological materials that cause discomfort, disease, or death to humans, damage other living organisms such as food crops, or damage the natural environment or built environment. Stratospheric ozone depletion due to air pollution has long been recognized as a threat to human health as well as to the Earth's ecosystems.\\r\\n\\r\\nPollution of breathable air is a central problem in the management of the global commons. Pollutants can be in the form of solid particles, liquid droplets, or gases and may be natural or man-made.  Although controversial and limited in scope by methods of enforcement, in several parts of the world the polluter pays principle, which makes the party responsible for producing pollution responsible for paying for the damage done to the natural environment, is accepted. It has strong support in most Organisation for Economic Co-operation and Development (OECD) and European Community (EC) countries. It is also known as extended producer responsibility (EPR). EPR seeks to shift the responsibility dealing with waste from governments (and thus, taxpayers and society at large) to the entities producing it. In effect, it attempts to internalise the cost of waste disposal into the cost of the product, theoretically resulting in producers improving the waste profile of their products, decreasing waste and increasing possibilities for reuse and recycling.\\r\\n\\r\\nThe 1979 Convention on Long-Range Transboundary Air Pollution, or CLRTAP, is an early international effort to protect and gradually reduce and prevent air pollution. It is implemented by the European Monitoring and Evaluation Programme (EMEP), directed by the United Nations Economic Commission for Europe (UNECE).  The Montreal Protocol on Substances that Deplete the Ozone Layer, or Montreal Protocol (a protocol to the Vienna Convention for the Protection of the Ozone Layer), is an international treaty designed to protect the ozone layer by phasing out the production of numerous substances believed to be responsible for ozone depletion. The treaty was opened for signature on 16 September 1987, and entered into force on 1 January 1989.\\r\\n\\r\\nGlobal dimming is the gradual reduction in the amount of global direct irradiance at the Earth's surface, which has been observed for several decades after the start of systematic measurements in the 1950s. Global dimming is thought to have been caused by an increase in particulates such as sulfate aerosols in the atmosphere due to human action.[23] It has interfered with the  hydrological cycle by reducing evaporation and may have reduced rainfall in some areas. Global dimming also creates a cooling effect that may have partially masked the effect of greenhouse gases on global warming.\\r\\n\\r\\nAlong with global warming, generalized climate change is an ongoing global commons concern.  Although global warming is now a generally accepted scientific observation, the precise causes of global warming are still a matter of research and debate. The Kyoto Protocol to the United Nations Framework Convention on Climate Change (UNFCCC) is an international environmental treaty that sets binding obligations on industrialised countries to reduce emissions of greenhouse gases and prevent potentially harmful anthropogenic (i.e., human-induced) interference in the climate system.[24] There are 192 parties to the convention, including 191 states and the European Union, but not all have ratified and implemented the protocol.[25]\\r\\n\\r\\nThe eight Arctic nations  Canada,  Denmark ( Greenland and the Faroe Islands),  Norway, the United States (Alaska), Sweden, Finland,  Iceland, and Russia, are all members of the treaty organization, the Arctic Council, as are organizations representing six indigenous populations. The Council operates on consensus basis, mostly dealing with environmental treaties and not addressing boundary or resource disputes.[26]  Currently, the Antarctic Treaty and related agreements, collectively called the Antarctic Treaty System or ATS, regulate international relations with respect to Antarctica, Earth's only continent without a native human population. The treaty, entering into force in 1961 and currently having 50 signatory nations, sets aside Antarctica as a scientific preserve, establishes freedom of scientific investigation and bans military activity on that continent.[27]\\r\\n\\r\\nClimate change in the Arctic region is leading to widespread ecosystem restructuring.[28] The distribution of species is changing along with the structure of food webs. Changes in ocean circulation appear responsible for the first exchanges of zooplankton between the North Pacific and North Atlantic regions in perhaps 800,000 years. These changes can allow the transmission of diseases from subarctic animals to Arctic ones, and vice versa, posing an additional threat to species already stressed by habitat loss and other impacts. Where these changes lead is not yet clear, but are likely to have far-reaching impacts on Arctic marine ecosystems.\\r\\n\\r\\nClimate models tend to reinforce that temperature trends due to global warming will be much smaller in Antarctica than in the Arctic,[29] but ongoing research may show otherwise.[30][31]\\r\\n\\r\\nManagement of outer space global commons has been contentious since the successful launch of the Sputnik satellite by the former Soviet Union on 4 October 1957.  There is no clear boundary between Earth's atmosphere and space, although there are several standard boundary designations: one that deals with orbital velocity (the Krmn line), one that depends on the velocity of charged particles in space, and some that are determined by human factors such as the height at which human blood begins to boil without a pressurized environment (the Armstrong line).\\r\\n\\r\\nSpace policy regarding a country's civilian space program, as well as its policy on both military use and commercial use of outer space, intersects with science policy, since national space programs often perform or fund research in space science, and also with defense policy, for applications such as spy satellites and anti-satellite weapons.  It also encompasses government regulation of third-party activities such as commercial communications satellites and private spaceflight[32] as well as the creation and application of space law and space advocacy organizations that exist to support the cause of space exploration.\\r\\n\\r\\nThe Outer Space Treaty provides a basic framework for international space law. It covers the legal use of outer space by nation states. The treaty states that outer space is free for all nation states to explore and is not subject to claims of national sovereignty. It also prohibits the deployment of nuclear weapons in outer space. The treaty was passed by the United Nations General Assembly in 1963 and signed in 1967 by the USSR, the United States of America and the United Kingdom. As of mid-year, 2013 the treaty has been ratified by 102 states and signed by an additional 27 states.\\r\\n\\r\\nBeginning in 1958, outer space has been the subject of multiple resolutions by the United Nations General Assembly. Of these, more than 50 have concerned the international co-operation in the peaceful uses of outer space and preventing an arms race in space. Four additional space law treaties have been negotiated and drafted by the UN's Committee on the Peaceful Uses of Outer Space. Still, there remain no legal prohibitions against deploying conventional weapons in space and anti-satellite weapons have been successfully tested by the US, USSR and China. The 1979 Moon Treaty turned the jurisdiction of all heavenly bodies (including the orbits around such bodies) over to the international community. However, this treaty has not been ratified by any nation that currently practices manned spaceflight.\\r\\n\\r\\nIn 1976 eight equatorial states (Ecuador, Colombia, Brazil, Congo, Zaire, Uganda, Kenya, and Indonesia) met in Bogot, Colombia to make the \\"Declaration of the First Meeting of Equatorial Countries,\\" also known as \\"the Bogot Declaration\\", a claim to control the segment of the geosynchronous orbital path corresponding to each country. These claims are not internationally accepted.\\r\\n\\r\\nThe International Space Station programme is a joint project among five participating space agencies: NASA, the Russian Federal Space Agency (RSA), Japan Aerospace Exploration Agency (JAXA), European Space Agency (ESA), and Canadian Space Agency (CSA). National budget constraints led to the merger of three space station projects into the International Space Station. In 1993 the partially built components for a Soviet/Russian space station Mir-2, the proposed American Freedom, and the proposed European Columbus merged into this multinational programme.[33] The ownership and use of the space station is established by intergovernmental treaties and agreements. The ISS is arguably the most expensive single item ever constructed,[34] and may be one of the most significant instances of international cooperation in modern history.\\r\\n\\r\\nAccording to the original Memorandum of Understanding between NASA and the RSA, the International Space Station was intended to be a laboratory, observatory and factory in space. It was also planned to provide transportation, maintenance, and act as a staging base for possible future missions to the Moon, Mars and asteroids. In the 2010 United States National Space Policy, it was given additional roles of serving commercial, diplomatic[35]  and educational purposes.[36]\\r\\n\\r\\nAs a global system of computers interconnected by telecommunication technologies consisting of millions of private, public, academic, business, and government resources, it is difficult to argue that the Internet is a global commons.  These computing resources are largely privately owned and subject to private property law, although many are government owned and subject to public law. The World Wide Web, as a system of interlinked hypertext documents, either public domain (like Wikipedia itself) or subject to copyright law, is, at best, a mixed good.\\r\\n\\r\\nThe resultant virtual space or cyberspace, however, is often viewed as an electronic global commons that allows for as much or more freedom of expression as any public space. Access to those digital commons and the actual freedom of expression allowed does vary widely by geographical area.  Management of the electronic global commons presents as many issues as do other commons.  In addition to issues related to inequity in access, issues such as net neutrality, Internet censorship, Internet privacy, and electronic surveillance arise.[37]","input":"What is an example of a global commons?"},{"output":"twenty-nine","context":"Executive:\\r\\nParliament:\\r\\nJudiciary:\\r\\nNational coalitions:\\r\\nState Level:\\r\\nLocal governments:\\r\\nArticle 171 of the Constitution of India provides for the establishment of a Vidhan Parishad. The Vidhan Parishad or Legislative Council is the upper house in those states of India that have a bicameral legislature. As of 2017[update], seven (7) (out of twenty-nine) states have a Legislative Council.[1] They are Andhra Pradesh, Bihar, Jammu and Kashmir, Karnataka, Maharashtra, Telangana, and Uttar Pradesh.They are elected by local bodies ,legislative assembly, governor, graduates, teacher etc.Odisha too is planing to make a legislative council. And members are known asMLC.\\r\\n\\r\\n\\r\\nEach Member of the Legislative Council (MLC) serves for a six-year term, with terms staggered so that the terms of one-third of a Council's membership expire every two years. This arrangement parallels that for the Rajya Sabha, the upper house of the Parliament of India.\\r\\nMLCs must be citizens of India, at least 30 years' old, mentally sound, not an insolvent, and on the voters' list of the state for which he or she is contesting an election. He or she may not be a Member of Parliament at the same time.\\r\\nThe size of the Vidhan Parishad cannot be more than one-third the membership of the Vidhan Sabha. However, its size cannot be less than 40 members (except in Jammu and Kashmir, where there are 36 by an Act of Parliament.)\\r\\nMLCs are chosen in the following manner:\\r\\nThe existence of a Legislative Council has proven politically controversial. A number of states that have had their Council abolished have subsequently requested its re-establishment; conversely, proposals for the re-establishment of the Council for a state have also met with opposition. Proposals for abolition or re-establishment of a state's Legislative Council require confirmation by the Parliament of India.\\r\\nIn April 2007, the State of Andhra Pradesh re-established its Legislative Council. The State's main opposition party, the Telugu Desam Party, had stated that it would abolish the council again if it came to power in the state.\\r\\nAfter the victory of the Akali Dal-BJP in Punjab, newly elected Chief Ministers Prakash Singh Badal stated that he would re-constitute the state's Vidhan Parishad but was not established","input":"How many state of india has vidhan parishad?"},{"output":"seven to ten days","context":"A kitten, also known as a kitty or kitty cat, is a juvenile cat. After being born, kittens are totally dependent on their mother for survival and they do not normally open their eyes until after seven to ten days. After about two weeks, kittens quickly develop and begin to explore the world outside the nest. After a further three to four weeks, they begin to eat solid food and grow adult teeth. Domestic kittens are highly social animals and enjoy human companionship.\\r\\n\\r\\n\\r\\nThe word \\"kitten\\" derives from the Middle English word kitoun, which in turn came from the Old French chitoun or cheton.[1] Juvenile big cats are called \\"cubs\\" rather than kittens; either term may be used for the young of smaller wild felids, such as ocelots, caracals and lynx, but \\"kitten\\" is usually more common for these species.[2]\\r\\nA feline litter usually consists of two to five kittens[3] born after a gestation lasting between 64 and 67 days, with an average length of 66 days.[3] Kittens emerge in a sac called the amnion, which is bitten off and eaten by the mother cat.[4]\\r\\nFor the first several weeks, kittens are unable to urinate or defecate without being stimulated by their mother.[5] They are also unable to regulate their body temperature for the first three weeks, so kittens born in temperatures less than 27?C (81?F) can die from hypothermia if their mother does not keep them warm.[6] The mother's milk is very important for the kittens' nutrition and proper growth. This milk transfers antibodies to the kittens, which helps protect them against infectious disease.[7] Newborn kittens are unable to produce concentrated urine, and so have a very high requirement for fluids.[8] Kittens open their eyes about seven to ten days after birth. At first, the retina is poorly developed and vision is poor. Kittens are not able to see as well as adult cats until about ten weeks after birth.[9]\\r\\nKittens develop very quickly from about two weeks of age until their seventh week. Their coordination and strength improve. They play-fight with their litter-mates and begin to explore the world outside the nest or den. They learn to wash themselves and others as well as play hunting and stalking games, showing their inborn ability as predators. These innate skills are developed by the kittens' mother or other adult cats, who bring live prey to the nest. Later, the adult cats demonstrate hunting techniques for the kittens to emulate.[10] As they reach three to four weeks old, the kittens are gradually weaned and begin to eat solid food, with weaning usually complete by six to eight weeks.[11] Kittens generally begin to lose their baby teeth around three months of age, and have a complete set of adult teeth by nine months.[12] Kittens live primarily on solid food after weaning, but usually continue to suckle from time to time until separated from their mothers. Some mother cats will scatter their kittens as early as three months of age, while others continue to look after them until they approach sexual maturity.[13]\\r\\nThe sex of kittens is usually easy to determine at birth. By six to eight weeks they are harder to sex because of the growth of fur in the genital region. The male's urethral opening is round, whereas the female's urethral opening is a slit. Another marked difference is the distance between anus and urethral opening, which is greater in males than in females.[14]\\r\\nKittens are highly social animals and spend most of their waking hours interacting with available animals and playing on their own. Play with other kittens peaks in the third or fourth month after birth, with more solitary hunting and stalking play peaking later, at about five months.[15]\\r\\nKittens are vulnerable because they like to find dark places to hide, sometimes with fatal results if they are not watched carefully.[16]\\r\\nAlthough domestic kittens are commonly sent to new homes at six to eight weeks of age, it has been suggested that being with their mother and litter-mates from six to twelve weeks is important for a kitten's social and behavioural development.[15] Usually, breeders and foster/rescue homes will not sell or adopt out a kitten that is younger than twelve weeks. In many jurisdictions, it is illegal to give away kittens younger than eight weeks of age.[17] Kittens generally reach sexual maturity at around seven months old. A cat reaches full \\"adulthood\\" around one year of age.[18]\\r\\nDomestic kittens in developed societies are usually vaccinated against common illnesses from two to three months of age. The usual combination vaccination protects against feline viral rhinotracheitis (FVR), feline calicivirus (C), and feline panleukopenia (P). This FVRCP inoculation is usually given at eight, twelve, and sixteen weeks, and an inoculation against rabies may be given at sixteen weeks. Kittens are usually spayed or neutered at seven months of age, but kittens may be neutered as young as seven weeks (if large enough), especially in animal shelters.[19] Such early neutering does not appear to have any long-term health risks to cats, and may even be beneficial in male cats.[20] Kittens are commonly wormed against roundworms from about four weeks.[21]\\r\\nKittens require a high-calorie diet that contains more protein than the diet of adult cats.[22] Young orphaned kittens require cat milk every two to four hours, and they need physical stimulation to defecate and urinate.[5] Cat milk replacement is manufactured to feed to young kittens, because cow's milk does not provide all the necessary nutrients.[23] Human-reared kittens tend to be very affectionate with humans as adults and sometimes more dependent on them than kittens reared by their mothers, but they can also show volatile mood swings and aggression.[24] Depending on the age at which they were orphaned and how long they were without their mothers, these kittens may be severely underweight and can have health problems later in life, such as heart conditions. The compromised immune system of orphaned kittens (from lack of antibodies found naturally in the mother's milk) can make them especially susceptible to infections, making antibiotics a necessity.[25]","input":"How many days till kittens open their eyes?"},{"output":"Ethan Allen","context":"1 captured near Fort Saint-Jean[7]\\r\\nThe Capture of Fort Ticonderoga occurred during the American Revolutionary War on May 10, 1775, when a small force of Green Mountain Boys led by Ethan Allen and Colonel Benedict Arnold overcame a small British garrison at the fort and looted the personal belongings of the garrison. Cannons and other armaments from the fort were later transported to Boston and used to fortify Dorchester Heights and break the standoff at the Siege of Boston.\\r\\nAfter seizing Ticonderoga, a small detachment captured the nearby Fort Crown Point on May 11. Seven days later, Arnold and 50 men boldly raided Fort Saint-Jean on the Richelieu River in southern Quebec, seizing military supplies, cannons, and the largest military vessel on Lake Champlain.\\r\\nAlthough the scope of this military action was relatively minor, it had significant strategic importance. It impeded communication between northern and southern units of the British Army, and gave the nascent Continental Army a staging ground for the invasion of Quebec later in 1775. It also involved two larger-than-life personalities in Allen and Arnold, each of whom sought to gain as much credit and honor as possible for these events. Most significantly, artillery from Ticonderoga would be dragged across Massachusetts to the heights commanding Boston Harbor, forcing the British to withdraw from that city.\\r\\n\\r\\n\\r\\nIn 1775, Fort Ticonderoga's location did not appear to be as strategically important as it had been in the French and Indian War, when the French famously defended it against a much larger British force in the 1758 Battle of Carillon, and when the British captured it in 1759. After the 1763 Treaty of Paris, in which the French ceded their North American territories to the British, the fort was no longer on the frontier of two great empires, guarding the principal waterway between them.[9] The French had blown up the fort's powder magazine when they abandoned the fort, and it had fallen further into disrepair since then. In 1775 it was garrisoned by only a small detachment of the 26th Regiment of Foot, consisting of two officers and forty-six men, with many of them \\"invalids\\" (soldiers with limited duties because of disability or illness). Twenty-five women and children lived there as well. Because of its former significance, Fort Ticonderoga still had a high reputation as the \\"gateway to the continent\\" or the \\"Gibraltar of America\\", but in 1775 it was, according to historian Christopher Ward, \\"more like a backwoods village than a fort.\\"[5]\\r\\nEven before shooting started in the American Revolutionary War, American Patriots were concerned about Fort Ticonderoga. The fort was a valuable asset for several reasons. Within its walls was a collection of heavy artillery including cannons, howitzers, and mortars, armaments that the Americans had in short supply.[10][11] The fort was situated on the shores of Lake Champlain, a strategically important route between the Thirteen Colonies and the British-controlled northern provinces. British forces placed there would expose the colonial forces in Boston to attack from the rear.[10] After the war began with the Battles of Lexington and Concord on April 19, 1775, the British General Thomas Gage realized the fort would require fortification, and several colonists had the idea of capturing the fort.\\r\\nGage, writing from the besieged city of Boston following Lexington and Concord, instructed Quebec's governor, General Guy Carleton, to rehabilitate and refortify the forts at Ticonderoga and Crown Point.[12] Carleton did not receive this letter until May 19, well after the fort had been captured.[13]\\r\\nBenedict Arnold had frequently traveled through the area around the fort, and was familiar with its condition, manning, and armaments. En route to Boston following news of the events of April 19, he mentioned the fort and its condition to members of Silas Deane's militia.[14] The Connecticut Committee of Correspondence acted on this information; money was \\"borrowed\\" from the provincial coffers and recruiters were sent into northwestern Connecticut, western Massachusetts, and the New Hampshire Grants (now Vermont) to raise volunteers for an attack on the fort.[15]\\r\\nJohn Brown, an American spy from Pittsfield, Massachusetts who had carried correspondence between revolutionary committees in the Boston area and Patriot supporters in Montreal, was well aware of the fort and its strategic value.[9] Ethan Allen and other Patriots in the disputed New Hampshire Grants territory also recognized the fort's value, as it played a role in the dispute over that area between New York and New Hampshire.[16] Whether either took or instigated action prior to the Connecticut Colony's recruitment efforts is unclear. Brown had notified the Massachusetts Committee of Safety in March of his opinion that Ticonderoga \\"must be seized as soon as possible should hostilities be committed by the King's Troops.\\"[16][17]\\r\\nWhen Arnold arrived outside Boston, he told the Massachusetts Committee of Safety about the cannons and other military equipment at the lightly defended fort. On May 3, the Committee gave Arnold a colonel's commission and authorized him to command a \\"secret mission\\", which was to capture the fort.[18] He was issued S100, some gunpowder, ammunition, and horses, and instructed to recruit up to 400 men, march on the fort, and ship back to Massachusetts anything he thought useful.[19]\\r\\nArnold departed immediately after receiving his instructions. He was accompanied by two captains, Eleazer Oswald and Jonathan Brown, who were charged with recruiting the necessary men. Arnold reached the border between Massachusetts and the Grants on May 6, where he learned of the recruitment efforts of the Connecticut Committee, and that Ethan Allen and the Green Mountain Boys were already on their way north. Riding furiously northward (his horse was subsequently destroyed), he reached Allen's headquarters in Bennington the next day.[20] Upon arrival, Arnold was told that Allen was in Castleton, 50 miles (80?km) to the north, awaiting supplies and more men. He was also warned that, although Allen's effort had no official sanction, his men were unlikely to serve under anyone else. Leaving early the next day, Arnold arrived in Castleton in time to join a war council, where he made a case to lead the expedition based on his formal authorization to act from the Massachusetts Committee.[21]\\r\\nThe force that Allen had assembled in Castleton included about 100 Green Mountain Boys, about 40 men raised by James Easton and John Brown at Pittsfield, and an additional 20 men from Connecticut.[22] Allen was elected colonel, with Easton and Seth Warner as his lieutenants.[21] When Arnold arrived on the scene, Samuel Herrick had already been sent to Skenesboro and Asa Douglas to Panton with detachments to secure boats. Captain Noah Phelps, a member of the \\"Committee of War for the Expedition against Ticonderoga and Crown Point\\", had reconnoitered the fort disguised as a peddler seeking a shave. He saw that the fort walls were dilapidated, learned from the garrison commander that the soldiers' gunpowder was wet, and that they expected reinforcements at any time.[23][24] He reported this intelligence to Allen, following which they planned a dawn raid.[23]\\r\\nMany of the Green Mountain Boys objected to Arnold's wish to command, insisting that they would go home rather than serve under anyone other than Ethan Allen. Arnold and Allen worked out an agreement, but no documented evidence exists concerning the deal. According to Arnold, he was given joint command of the operation. Some historians have supported Arnold's contention, while others suggest he was merely given the right to march next to Allen.[Note 1]\\r\\nBy 11:30?pm on May 9, the men had assembled at Hand's Cove (in what is now Shoreham, Vermont) and were ready to cross the lake to Ticonderoga. However, boats did not arrive until 1:30?am, and they were inadequate to carry the whole force.[25] Eighty-three of the Green Mountain Boys made the first crossing with Arnold and Allen, and Douglas went back for the rest.[2] As dawn approached, Allen and Arnold became fearful of losing the element of surprise, so they decided to attack with the men at hand. The only sentry on duty at the south gate fled his post after his musket misfired, and the Americans rushed into the fort. The Patriots then roused the small number of sleeping troops at gunpoint, and began confiscating their weapons. Allen, Arnold, and a few other men charged up the stairs toward the officers' quarters. Lieutenant Jocelyn Feltham, assistant to Captain William Delaplace, was awakened by the noise, and called to wake the captain.[26] Stalling for time, Feltham demanded to know by what authority the fort was being entered. Allen, who later claimed that he said it to Captain Delaplace, replied, \\"In the name of the Great Jehovah and the Continental Congress!\\"[27] Delaplace finally emerged from his chambers (fully clothed, not with \\"his breeches in his hand\\", as Allen would later say) and surrendered his sword.[27]\\r\\nNobody was killed in the assault. The only injury was to one American, Gideon Warren,[28] who was slightly injured by a sentry with a bayonet.[8] Eventually, as many as 400 men arrived at the fort, which they plundered for liquor and other provisions. Arnold, whose authority was not recognized by the Green Mountain Boys, was unable to stop the plunder. Frustrated, he retired to the captain's quarters to await forces that he had recruited, reporting to the Massachusetts Provincial Congress that Allen and his men were \\"governing by whim and caprice\\" at the fort, and that the plan to strip the fort and send armaments to Boston was in peril.[29] When Delaplace protested the seizure of his private liquor stores, Allen issued him a receipt for the stores, which he later submitted to Connecticut for payment.[30] Arnold's disputes with Allen and his unruly men were severe enough that there were times when some of Allen's men drew weapons.[29]\\r\\nOn May 12, Allen sent the prisoners to Connecticut's Governor Jonathan Trumbull with a note saying \\"I make you a present of a Major, a Captain, and two Lieutenants of the regular Establishment of George the Third.\\"[31] Arnold busied himself over the next few days with cataloging the military equipment at Ticonderoga and Crown Point, a task made difficult by the fact that walls had collapsed on some of the armaments.[32]\\r\\nSeth Warner sailed a detachment up the lake and captured nearby Fort Crown Point, garrisoned by only nine men. It is widely recorded that this capture occurred on May 10; this is attributed to a letter Arnold wrote to the Massachusetts Committee of Safety on May 11, claiming that an attempt to sail up to Crown Point was frustrated by headwinds. However Warner claimed, in a letter dated May 12 from \\"Head Quarters, Crown Point\\", that he \\"took possession of this garrison\\" the day before.[6] It appears likely that, having failed on May 10, the attempt was repeated the next day with success, as reported in Warner's memoir.[33] A small force was also sent to capture Fort George on Lake George, which was held by only two soldiers.[34]\\r\\nTroops recruited by Arnold's captains began to arrive, some after seizing Philip Skene's schooner Katherine and several bateaux at Skenesboro.[35][36] Arnold rechristened the schooner Liberty. The prisoners had reported that the lone British warship on Lake Champlain was at Fort Saint-Jean, on the Richelieu River north of the lake. Arnold, uncertain whether word of Ticonderoga's capture had reached Saint-Jean, decided to attempt a raid to capture the ship. He had Liberty outfitted with guns, and sailed north with 50 of his men on May 14.[37] Allen, not wanting Arnold to get the full glory for that capture, followed with some of his men in bateaux, but Arnold's small fleet had the advantage of sail, and pulled away from Allen's boats. By May 17, Arnold's small fleet was at the northern end of the lake. Seeking intelligence, Arnold sent a man to reconnoiter the situation at Fort Saint-Jean. The scout returned later that day, reporting that the British were aware of the fall of Ticonderoga and Crown Point, and that troops were apparently on the move toward Saint-Jean. Arnold decided to act immediately.[38]\\r\\nRowing all night, Arnold and 35 of his men brought their bateaux near the fort. After a brief scouting excursion, they surprised the small garrison at the fort, and seized supplies there, along with the HMS?Royal George, a seventy-ton sloop-of-war.[39] Warned by their captives that several companies were on their way from Chambly, they loaded the more valuable supplies and cannons on the George, which Arnold renamed the Enterprise. Boats that they could not take were sunk, and the enlarged fleet returned to Lake Champlain.[4] This activity was observed by Moses Hazen, a retired British officer who lived near the fort. Hazen rode to Montreal to report the action to the local military commander, and then continued on to Quebec City, where he reported the news to General Carleton on May 20. Major Charles Preston and 140 men were immediately dispatched from Montreal to Saint-Jean in response to Hazen's warning.[40]\\r\\nFifteen miles out on the lake, Arnold's fleet met Allen's, which was still heading north. After an exchange of celebratory gunfire, Arnold opened his stores to feed Allen's men, who had rowed 100 miles (160?km) in open boats without provisions. Allen, believing he could seize and hold Fort Saint-Jean, continued north, while Arnold sailed south.[41] Allen arrived at Saint-Jean on May 19, where he was warned that British troops were approaching by a sympathetic Montreal merchant who had raced ahead of those troops on horseback.[42] Allen, after penning a message for the merchant to deliver to the citizens of Montreal, returned to Ticonderoga on May 21, leaving Saint-Jean just as the British forces arrived.[42][43] In their haste to escape the British, three men were left behind; one was captured, but the other two eventually returned south by land.[7]\\r\\nEthan Allen and his men eventually drifted away from Ticonderoga, especially once the alcohol began to run out, and Arnold largely controlled affairs from a base at Crown Point.[34][44] He oversaw the fitting of the two large ships, eventually taking command of Enterprise because of a lack of knowledgeable seamen. His men began rebuilding Ticonderoga's barracks, and worked to extract armaments from the rubble of the two forts and build gun carriages for them.[44]\\r\\nConnecticut sent about 1,000 men under Colonel Benjamin Hinman to hold Ticonderoga, and New York also began to raise militia to defend Crown Point and Ticonderoga against a possible British attack from the north. When Hinman's troops arrived in June, there was once again a clash over leadership. None of the communications to Arnold from the Massachusetts committee indicated that he was to serve under Hinman; when Hinman attempted to assert authority over Crown Point, Arnold refused to accept it, as Hinman's instructions only included Ticonderoga.[45] The Massachusetts committee eventually sent a delegation to Ticonderoga. When they arrived on June 22 they made it clear to Arnold that he was to serve under Hinman. Arnold, after considering for two days, disbanded his command, resigned his commission, and went home, having spent more than S1,000 of his own money in the effort to capture the fort.[46]\\r\\nWhen Congress received news of the events, it drafted a second letter to the inhabitants of Quebec, which was sent north in June with James Price, another sympathetic Montreal merchant. This letter, and other communications from the New York Congress, combined with the activities of vocal American supporters, stirred up the Quebec population in the summer of 1775.[47]\\r\\nWhen news of the fall of Ticonderoga reached England, Lord Dartmouth wrote that it was \\"very unfortunate; very unfortunate indeed\\".[48]\\r\\nNews of the capture of Ticonderoga and Crown Point, and especially the raids on Fort Saint-Jean, electrified the Quebec population. Colonel Dudley Templer, in charge of the garrison at Montreal, issued a call on May 19 to raise a militia for defense of the city, and requested Indians living nearby to also take up arms. Only 50 men, mostly French-speaking landowning seigneurs and petty nobility, were raised in and around Montreal, and they were sent to Saint-Jean; no Indians came to their aid. Templer also prevented merchants sympathetic to the American cause from sending supplies south in response to Allen's letter.[49]\\r\\nGeneral Carleton, notified by Hazen of the events on May 20, immediately ordered the garrisons of Montreal and Trois-Rivires to fortify Saint-Jean. Some troops garrisoned at Quebec were also sent to Saint-Jean. Most of the remaining Quebec troops were dispatched to a variety of other points along the Saint Lawrence, as far west as Oswegatchie, to guard against potential invasion threats.[50] Carleton then traveled to Montreal to oversee the defense of the province from there, leaving the city of Quebec in the hands of Lieutenant Governor Hector Cramah.[51] Before leaving, Carleton prevailed on Monsignor Jean-Olivier Briand, the Bishop of Quebec, to issue his own call to arms in support of the provincial defense, which was circulated primarily in the areas around Montreal and Trois-Rivires.[52]\\r\\nIn July 1775, General Philip Schuyler began using the fort as the staging ground for the invasion of Quebec that was launched in late August.[53] In the winter of 1775ÿ1776, Henry Knox directed the transportation of the guns of Ticonderoga to Boston. The guns were placed upon Dorchester Heights overlooking the besieged city and the British ships in the harbor, prompting the British to evacuate their troops and Loyalist supporters from the city in March 1776.[54]\\r\\nBenedict Arnold again led a fleet of ships at the Battle of Valcour Island, and played other key roles in thwarting Britain's attempt to recapture the fort in 1776.[55] The British did recapture the fort in July 1777 during the Saratoga campaign, but had abandoned it by November after Burgoyne's surrender at Saratoga.[56]\\r\\nAlthough Fort Ticonderoga was not at the time an important military post, its capture had several important results. Rebel control of the area meant that overland communications and supply lines between British forces in Quebec and those in Boston and later New York were severed, so the British military command made an adjustment to their command structure.[57] This break in communication was highlighted by the fact that Arnold, on his way north to Saint-Jean, intercepted a message from Carleton to Gage, detailing the military troop strengths in Quebec.[58] Command of British forces in North America, previously under a single commander, was divided into two commands. General Carleton was given independent command of forces in Quebec and the northern frontier, while General William Howe was appointed Commander-in-Chief of forces along the Atlantic coast, an arrangement that had worked well between Generals Wolfe and Amherst in the French and Indian War.[57] In this war, however, cooperation between the two forces would prove to be problematic and would play a role in the failure of the Saratoga campaign in 1777, as General Howe apparently abandoned an agreed-upon northern strategy, leaving General John Burgoyne without southern support in that campaign.[59]\\r\\nBeginning on the day of the fort's capture, Allen and Arnold began a war of words, each attempting to garner for himself as much credit for the operation as possible. Arnold, unable to exert any authority over Allen and his men, began to keep a diary of events and actions, which was highly critical and dismissive of Allen.[34] Allen, in the days immediately after the action, also began to work on a memoir. Published several years later (see Further reading), the memoir fails to mention Arnold at all. Allen also wrote several versions of the events, which John Brown and James Easton brought to a variety of Congresses and committees in New York, Connecticut, and Massachusetts. Randall (1990) claims that Easton took accounts written by both Arnold and Allen to the Massachusetts committee, but conveniently lost Arnold's account on the way, ensuring that Allen's version, which greatly glorified his role in the affair, would be preferred.[60] Smith (1907) indicates that it was highly likely that Easton was interested in claiming Arnold's command for himself.[61] There was clearly no love lost between Easton and Arnold. Allen and Easton returned to Crown Point on June 10 and called a council of war while Arnold was with the fleet on the lake, a clear breach of military protocol. When Arnold, whose men now dominated the garrison, asserted his authority, Easton insulted Arnold, who responded by challenging Easton to a duel. Arnold later reported, \\"On refusing to draw like a gentleman, he having a [sword] by his side and cases of loaded pistols in his pockets, I kicked him very heartily and ordered him from the Point.\\"[62]","input":"Who were the leaders of the battle of fort ticonderoga?"},{"output":"Bachendri Pal","context":"Instructor ÿ National Adventure Foundation[1]\\r\\nBachendri Pal (born 24 May 1954) is an Indian mountaineer, who in 1984 became the first Indian woman to reach the summit of Mount Everest.[3]\\r\\n\\r\\n\\r\\nBachendri Pal belonged to a family of very moderate means. She was born on 24 May 1954 in a village in the Himalayas named Nakuri in Uttarkashi, District of Garhwal, what is now Uttarakhand, a state in the northern part of India. She was one of seven children to Hansa Devi and Shri Kishan Singh Pal ÿ a border tradesman who supplied groceries from India to Tibet. She was born only five days before the first anniversary of the original ascension of Mount Everest by Tenzing Norgay and Edmund Hillary. She completed her M.A. and B.Ed. from D.A.V.Post Graduate College Dehradun. She got her first taste of mountaineering thrill at the age of 12, when she, along with her friends, scaled a 13,123?ft (3,999.9?m) high peak during a school picnic. On the invitation of her school principal, she was sent to college for higher studies and became the first girl to 1982, during her course at NIM, she climbed Mt. Gangotri 21,900?ft (6,675.1?m) and Mt. Rudragaria 19,091?ft (5,818.9?m). In that time, she got employment as an instructor at the National Adventure Foundation (NAF), which had set up an adventure school for training women to learn mountaineering.[1]\\r\\nPal encountered stiff opposition from her family and relatives when she decided to opt for a career as a professional mountaineer rather than as a schoolteacher. She soon found success in her chosen field, however.\\r\\nAfter summitting a number of smaller peaks, she was selected to join India's first mixed-gender team to attempt an expedition to Mount Everest in 1984.[3]\\r\\nIn 1984, India had scheduled its fourth expedition, christened Everest'84, to Mount Everest. Bachendri Pal was selected as one of the members of the elite group of six Indian women and eleven men who were privileged to attempt an ascent to the Mount Everest (Sagarmatha in Nepalese). The news filled them with a sense of ecstasy and excitement.[citation needed] The team was flown to Kathmandu, the capital of Nepal, in March 1984, and from there the team moved onwards. Recalling her first glimpse of the Mount Everest, Bachendri once reminisced: \\"We the hill people have always worshiped the mountainsmy overpowering emotion at this awe-inspiring spectacle was, therefore, devotional\\".[4] The team commenced its ascent in May 1984. Her team almost met disaster when an avalanche buried its camp, and more than half the group abandoned the ascent because of injury or fatigue. Bachendri Pal and the remainder of the team pressed on to reach the summit.[3] Bachendri Pal recalls this accident: \\"I was sleeping in one of the tents with my teammates at Camp III at an altitude of 24,000?ft (7,315.2?m). On the night of 15ÿ16 May 1984, at around 00:30?hours IST, I was jolted awake; something had hit me hard; I also heard a deafening sound and soon after I found myself being enveloped within a very cold mass of material\\".[4]\\r\\nOn 22 May 1984, Ang Dorjee (the Sherpa Sirdar) and some other climbers joined the team to ascend to the summit of Mount Everest; Bachendri was the only woman in this group. They reached the South Col and spent the night there at Camp IV at the altitude of 26,000?ft (7,924.8?m). On 23 May 1984, early morning at 6:20?a.m., they continued the ascent, climbing \\"vertical sheets of frozen ice\\"; cold winds were blowing at the speed of about 100?km per hour and temperatures touching minus 30 to 40?degrees Celsius. On 23 May 1984, the team reached the summit of Mount Everest at 1:07?p.m. IST and Bachendri Pal created history.[5] She achieved this feat on the day before her 30th birthday, and six days before the 31st anniversary of the first ascension to Mount Everest.\\r\\nBachendri Pal continued to be active after ascending the highest peak in the world. She successfully led:\\r\\nBachendri Pal along with Premlata Agarwal and a group of ace climbers, which includes Mt. Everest summiteers have quietly arrived in Uttarkashi and carried out relief and rescue operations in the remotest high altitude villages of the Himalayas that have been ravaged in the 2013 North India floods, where even the Indian Army youngsters couldn't reach.[8]\\r\\nBachendri Pal has been conferred with following awards and accolades:[6][9]","input":"Who is the first indian woman to climb everest?"},{"output":"gold-plated bronze on a black metal base","context":"Moonlight\\r\\nThe Shape of Water\\r\\nThe Academy Awards, also known as the Oscars,[1] are a set of 24 awards for artistic and technical merit in the American film industry, given annually by the Academy of Motion Picture Arts and Sciences (AMPAS), to recognize excellence in cinematic achievements as assessed by the Academy's voting membership. The various category winners are awarded a copy of a golden statuette, officially called the \\"Academy Award of Merit\\", which has become commonly known by its nickname \\"Oscar\\". The sculpture was created by George Stanley.[2] The awards, first presented in 1929 at the Hollywood Roosevelt Hotel, are overseen by AMPAS.[3][4]\\r\\nThe awards ceremony was first broadcast on radio in 1930 and televised for the first time in 1953. It is now seen live in more than 200 countries and can be streamed live online.[5] The Academy Awards ceremony is the oldest worldwide entertainment awards ceremony. Its equivalents ÿ the Emmy Awards for television, the Tony Awards for theater, and the Grammy Awards for music ÿ are modeled after the Academy Awards.[6]\\r\\nThe 90th Academy Awards ceremony, honoring the best films of 2017, was held on 4 March 2018, at the Dolby Theatre, in Los Angeles, California. The ceremony was hosted by Jimmy Kimmel and was broadcast on ABC. A total of 3,072 Oscars have been awarded from the inception of the award through the 90th.[7]\\r\\n\\r\\n\\r\\nThe first Academy Awards presentation was held on 16 May 1929, at a private dinner function at the Hollywood Roosevelt Hotel with an audience of about 270 people. The post-awards party was held at the Mayfair Hotel.[8] The cost of guest tickets for that night's ceremony was $5 ($71 in 2017 dollars). Fifteen statuettes were awarded, honoring artists, directors and other participants in the film-making industry of the time, for their works during the 1927ÿ28 period. The ceremony ran for 15?minutes.\\r\\nWinners were announced to media three months earlier. That was changed for the second ceremony in 1930. Since then, for the rest of the first decade, the results were given to newspapers for publication at 11:00?pm on the night of the awards.[8] This method was used until an occasion when the Los Angeles Times announced the winners before the ceremony began; as a result, the Academy has, since 1941, used a sealed envelope to reveal the name of the winners.[8]\\r\\nThe first Best Actor awarded was Emil Jannings, for his performances in The Last Command and The Way of All Flesh. He had to return to Europe before the ceremony, so the Academy agreed to give him the prize earlier; this made him the first Academy Award winner in history. At that time, the winners were recognized for all of their work done in a certain category during the qualifying period; for example, Jannings received the award for two movies in which he starred during that period, and Janet Gaynor later won a single Oscar for performances in three films. With the fourth ceremony, however, the system changed, and professionals were honored for a specific performance in a single film. For the first six ceremonies, the eligibility period spanned two calendar years.[8]\\r\\nAt the 29th ceremony, held on 27 March 1957, the Best Foreign Language Film category was introduced. Until then, foreign-language films had been honored with the Special Achievement Award.\\r\\nThe 74th Academy Awards, held in 2002, presented the first Academy Award for Best Animated Feature.[9]\\r\\nSince 1973, all Academy Awards ceremonies have ended with the Academy Award for Best Picture.\\r\\nTraditionally, the previous year's winner for Best Actor and Best Supporting Actor present the awards for Best Actress and Best Supporting Actress, while the previous year's winner for Best Actress and Best Supporting Actress present the awards for Best Actor and Best Supporting Actor.\\r\\nIn addition to the Academy Award of Merit (Oscar award), there are nine honorary (non-competitive) awards presented by the Academy from time to time (except for the Academy Honorary Award, the Technical Achievement Award, and the Student Academy Awards, which are presented annually):\\r\\nThe Academy also awards Nicholl Fellowships in Screenwriting.\\r\\nThe best known award is the Academy Award of Merit, more popularly known as the Oscar statuette.[10] Made of gold-plated bronze on a black metal base, it is 13.5?in (34.3?cm) tall, weighs 8.5?lb (3.856?kg), and depicts a knight rendered in Art Deco style holding a crusader's sword standing on a reel of film with five spokes. The five spokes represent the original branches of the Academy: Actors, Writers, Directors, Producers, and Technicians.[11]\\r\\nThe model for the statuette is said to be Mexican actor Emilio \\"El Indio\\" Fernndez.[12] Sculptor George Stanley (who also did the Muse Fountain at the Hollywood Bowl) sculpted Cedric Gibbons' design. The statuettes presented at the initial ceremonies were gold-plated solid bronze. Within a few years the bronze was abandoned in favor of Britannia metal, a pewter-like alloy which is then plated in copper, nickel silver, and finally, 24-karat gold.[13] Due to a metal shortage during World War II, Oscars were made of painted plaster for three years. Following the war, the Academy invited recipients to redeem the plaster figures for gold-plated metal ones.[14] The only addition to the Oscar since it was created is a minor streamlining of the base. The original Oscar mold was cast in 1928 at the C.W. Shumway & Sons Foundry in Batavia, Illinois, which also contributed to casting the molds for the Vince Lombardi Trophy and Emmy Award's statuettes. From 1983 to 2015,[15] approximately 50 Oscars in a tin alloy with gold plating were made each year in Chicago by Illinois manufacturer R.S. Owens & Company.[16] It takes between three and four weeks to manufacture 50 statuettes.[12] In 2016, the Academy returned to bronze as the core metal of the statuettes, handing manufacturing duties to Rock Tavern, New York-based Polich Tallix Fine Art Foundry.[17] While based on a digital scan of an original 1929 Oscar, the statuettes retain their modern-era dimensions and black pedestal. Cast in liquid bronze from 3D-printed ceramic molds and polished, they are then electroplated in 24-karat gold by Brooklyn, New Yorkÿbased Epner Technology. The time required to produce 50 such statuettes is roughly three months.[18] R.S. Owens is expected to continue producing other awards for the Academy and service existing Oscars that need replating.[19]\\r\\nThe origin of the name Oscar is disputed. One biography of Bette Davis, who was a president of the Academy, claims she named the Oscar after her first husband, band leader Harmon Oscar Nelson.[20] Another claimed origin is the Academy's Executive Secretary, Margaret Herrick, first saw the award in 1931 and made reference to the statuette's reminding her of her \\"Uncle Oscar\\" (a nickname for her cousin Oscar Pierce).[21] Columnist Sidney Skolsky was present during Herrick's naming and seized the name in his byline, \\"Employees have affectionately dubbed their famous statuette 'Oscar'.\\" [22]\\r\\nOne of the earliest mentions of the term Oscar dates to a Time magazine article about the 1934 6th Academy Awards.[23] Walt Disney also thanked the Academy for his Oscar as early as 1932.[24] The trophy officially received the name \\"Oscar\\" in 1939 by the Academy of Motion Picture Arts and Sciences.\\r\\nTo prevent information identifying the Oscar winners from leaking ahead of the ceremony, Oscar statuettes presented at the ceremony have blank baseplates. Until 2010, winners returned their statuettes to the Academy, and had to wait several weeks to have their names inscribed on their respective Oscars. Since 2010, winners have had the option of having engraved nameplates applied to their statuettes at an inscription-processing station at the Governor's Ball, a party held immediately after the Oscar ceremony. The R.S. Owens company has engraved nameplates made before the ceremony, bearing the name of every potential winner. The nameplates for the non-winning nominees are later recycled.[25][26]\\r\\nSince 1950, the statuettes have been legally encumbered by the requirement that neither winners nor their heirs may sell the statuettes without first offering to sell them back to the Academy for US$1. If a winner refuses to agree to this stipulation, then the Academy keeps the statuette. Academy Awards not protected by this agreement have been sold in public auctions and private deals for six-figure sums.[27] In December 2011, Orson Welles' 1941 Oscar for Citizen Kane (Academy Award for Best Original Screenplay) was put up for auction, after his heirs won a 2004 court decision contending that Welles did not sign any agreement to return the statue to the Academy.[28] On 20 December 2011, it sold in an online auction for US$861,542.[29]\\r\\nIn 1992, Harold Russell needed money for his wife's medical expenses. In a controversial decision, he consigned his 1946 Oscar for Best Supporting Actor for The Best Years of Our Lives to Herman Darvick Autograph Auctions, and on 6 August 1992, in New York City, the Oscar sold to a private collector for $60,500. Since he won the award before 1950, he was not required to offer it to the Academy first. Russell defended his decision, saying, \\"I don't know why anybody would be critical. My wife's health is much more important than sentimental reasons. The movie will be here, even if Oscar isn't.\\" Harold Russell is the only Academy Award-winning actor to ever sell an Oscar.[30]\\r\\nWhile the Oscar is owned by the recipient, it is essentially not on the open market.[31] Michael Todd's grandson tried to sell Todd's Oscar statuette to a movie prop collector in 1989, but the Academy won the legal battle by getting a permanent injunction. Although some Oscar sales transactions have been successful, some buyers have subsequently returned the statuettes to the Academy, which keeps them in its treasury.[32]\\r\\nSince 2004, Academy Award nomination results have been announced to the public in late January. Prior to that, the results were announced in early February.\\r\\nThe Academy of Motion Picture Arts and Sciences (AMPAS), a professional honorary organization, maintains a voting membership of over 8,000 as of 2018[update].[33]\\r\\nAcademy membership is divided into different branches, with each representing a different discipline in film production. Actors constitute the largest voting bloc, numbering 1,311 members (22 percent) of the Academy's composition. Votes have been certified by the auditing firm PricewaterhouseCoopers (and its predecessor Price Waterhouse) for the past 83 annual awards ceremonies.[34][35]\\r\\nAll AMPAS members must be invited to join by the Board of Governors, on behalf of Academy Branch Executive Committees. Membership eligibility may be achieved by a competitive nomination or a member may submit a name based on other significant contributions to the field of motion pictures.\\r\\nNew membership proposals are considered annually. The Academy does not publicly disclose its membership, although as recently as 2007 press releases have announced the names of those who have been invited to join. The 2007 release also stated that it has just under 6,000 voting members. While the membership had been growing, stricter policies have kept its size steady since then.[36]\\r\\nIn 2012, the results of a study conducted by the Los Angeles Times were published describing the demographic breakdown of approximately 88% of AMPAS' voting membership. Of the 5,100+ active voters confirmed, 94% were Caucasian, 77% were male, and 54% were found to be over the age of 60. 33% of voting members are former nominees (14%) and winners (19%).[37]\\r\\nIn May 2011, the Academy sent a letter advising its 6,000 or so voting members that an online system for Oscar voting would be implemented in 2013.[38]\\r\\nAccording to Rules 2 and 3 of the official Academy Awards Rules, a film must open in the previous calendar year, from midnight at the start of 1 January to midnight at the end of 31 December, in Los Angeles County, California, and play for seven consecutive days, to qualify (except for the Best Foreign Language Film, Best Documentary Feature, and Best Documentary Short Subject).[39][40]\\r\\nThe Best Foreign Language Film award does not require a U.S. release. The Best Documentary Feature award requires week-long releases in both Los Angeles County and New York City during the previous calendar year.[41]\\r\\nThe Best Documentary Short Subject award has noticeably different eligibility rules from most other competitive awards. First, the qualifying period for release does not coincide with a calendar year, instead covering a one-year period starting on 1 September and ending on 31 August of the calendar year before the ceremony. Second, there are multiple methods of qualification. The main method is a week-long theatrical release in either Los Angeles County or New York City during the eligibility period. Films also can qualify by winning specified awards at one of a number of competitive film festivals designated by the Academy. Finally, a film that is selected as a gold, silver, or bronze medal winner in the Documentary category of the immediately previous Student Academy Awards is also eligible.[42]\\r\\nFor example, the 2009 Best Picture winner, The Hurt Locker, was actually first released in 2008, but did not qualify for the 2008 awards as it did not play its Oscar-qualifying run in Los Angeles until mid-2009, thus qualifying for the 2009 awards. Foreign films must include English subtitles, and each country can submit only one film per year.[43]\\r\\nRule 2 states that a film must be feature-length, defined as a minimum of 40?minutes, except for short-subject awards, and it must exist either on a 35 mm or 70 mm film print or in 24?frame/s or 48?frame/s progressive scan digital cinema format with a minimum projector resolution of 2048 by 1080 pixels.[44] Effective with the 90th Academy Awards, presented in 2018, multi-part and limited series will be ineligible for the Best Documentary Feature award. This followed the win of O.J.: Made in America, an eight-hour presentation that was screened in a limited release before being broadcast in five parts on ABC and ESPN, in that category in 2017. The Academy's announcement of the new rule made no direct mention of that film.[45]\\r\\nProducers must submit an Official Screen Credits online form before the deadline; in case it is not submitted by the defined deadline, the film will be ineligible for Academy Awards in any year. The form includes the production credits for all related categories. Then, each form is checked and put in a Reminder List of Eligible Releases.\\r\\nIn late December ballots and copies of the Reminder List of Eligible Releases are mailed to around 6,000 active members. For most categories, members from each of the branches vote to determine the nominees only in their respective categories (i.e. only directors vote for directors, writers for writers, actors for actors, etc.). In the special case of Best Picture, all voting members are eligible to select the nominees. In all major categories, a variant of the single transferable vote is used, with each member casting a ballot with up to five nominees (ten for Best Picture) ranked preferentially.[46][47][48] In certain categories, including Foreign Film, Documentary and Animated Feature Film, nominees are selected by special screening committees made up of members from all branches.\\r\\nIn most categories the winner is selected from among the nominees by plurality voting of all members.[46][48] Since 2009, the Best Picture winner has been chosen by instant runoff voting.[48][49] Since 2013, re-weighted range voting has been used to select the nominees for the Best Visual Effects.[50][51]\\r\\nFilm companies will spend as much as several million dollars on marketing to awards voters for a movie in the running for Best Picture, in attempts to improve chances of receiving Oscars and other movie awards conferred in Oscar season. The Academy enforces rules to limit overt campaigning by its members so as to try to eliminate excesses and prevent the process from becoming undignified. It has an awards czar on staff who advises members on allowed practices and levies penalties on offenders.[52] For example, a producer of the 2009 Best Picture nominee The Hurt Locker was disqualified as a producer in the category when he contacted associates urging them to vote for his film and not another that was seen as the front-runner (The Hurt Locker eventually won).\\r\\nThe major awards are presented at a live televised ceremony, commonly in late February or early March following the relevant calendar year, and six weeks after the announcement of the nominees. It is the culmination of the film awards season, which usually begins during November or December of the previous year. This is an elaborate extravaganza, with the invited guests walking up the red carpet in the creations of the most prominent fashion designers of the day. Black tie dress is the most common outfit for men, although fashion may dictate not wearing a bow-tie, and musical performers sometimes do not adhere to this. (The artists who recorded the nominees for Best Original Song quite often perform those songs live at the awards ceremony, and the fact that they are performing is often used to promote the television broadcast.)\\r\\nThe Academy Awards is the only awards show televised live in all United States time zones (excluding Hawaii; they aired live in Alaska since 2011 for the first time since 1996), Canada, the United Kingdom, and gathers millions of viewers elsewhere throughout the world. The Oscars were first televised in 1953 by NBC, which continued to broadcast the event until 1960, when ABC took over, televising the festivities (including the first color broadcast of the event in 1966) through 1970. NBC regained the rights for five years (1971ÿ75), then ABC resumed broadcast duties in 1976 and its current contract with the Academy runs through 2028.[53] The Academy has also produced condensed versions of the ceremony for broadcast in international markets (especially those outside of the Americas) in more desirable local timeslots. The ceremony was broadcast live internationally for the first time via satellite since 1970, but only two South American countries, Chile and Brazil, purchased the rights to air the broadcast. By that time, the television rights to the Academy Awards had been sold in 50 countries. A decade later, the rights were already being sold to 60 countries, and by 1984, the TV rights to the Awards were licensed in 76 countries.\\r\\nThe ceremonies were moved up from late March/early April to late February since 2004 to help disrupt and shorten the intense lobbying and ad campaigns associated with Oscar season in the film industry. Another reason was because of the growing TV ratings success coinciding with the NCAA Basketball Tournament, which would cut into the Academy Awards audience. (In 1976 and 1977, ABC's regained Oscars were moved from Tuesday to Monday and went directly opposite NBC's NCAA title game.) The earlier date is also to the advantage of ABC, as it now usually occurs during the highly profitable and important February sweeps period. Some years, the ceremony is moved into first Sunday of March in order to avoid clash with the Winter Olympic Games. Another reason for the move to late February and early March is also to avoid the awards ceremony occurring so close to the religious holidays of Passover and Easter, which for decades had been a grievance from members and the general public. Advertising is somewhat restricted, however, as traditionally no movie studios or competitors of official Academy Award sponsors may advertise during the telecast. The production of the Academy Awards telecast currently holds the distinction of winning the most Emmys in history, with 47 wins and 195 nominations overall since that award's own launch in 1949.[54]\\r\\nAfter many years of being held on Mondays at 9:00?pm Eastern/6:00 p.m Pacific, since the 1999 ceremonies, it was moved to Sundays at 8:30?pm ET/5:30?pm PT.[55] The reasons given for the move were that more viewers would tune in on Sundays, that Los Angeles rush-hour traffic jams could be avoided, and an earlier start time would allow viewers on the East Coast to go to bed earlier.[56] For many years the film industry opposed a Sunday broadcast because it would cut into the weekend box office.[57] In 2010, the Academy contemplated moving the ceremony even further back into January, citing TV viewers' fatigue with the film industry's long awards season. However, such an accelerated schedule would dramatically decrease the voting period for its members, to the point where some voters would only have time to view the contending films streamed on their computers (as opposed to traditionally receiving the films and ballots in the mail). Furthermore, a January ceremony on Sunday would clash with National Football League playoff games.[58]\\r\\nOriginally scheduled for 8 April 1968, the 40th Academy Awards ceremony was postponed for two days, because of the assassination of Dr. Martin Luther King, Jr.. On 30 March 1981, the 53rd Academy Awards was postponed for one day, after the shooting of President Ronald Reagan and others in Washington, D.C.\\r\\nIn 1993, an In Memoriam segment was introduced,[59] honoring those who had made a significant contribution to cinema who had died in the preceding 12 months, a selection compiled by a small committee of Academy members.[60] This segment has drawn criticism over the years for the omission of some names. Criticism was also levied for many years regarding another aspect, with the segment having a \\"popularity contest\\" feel as the audience varied their applause to those who had died by the subject's cultural impact; the applause has since been muted during the telecast, and the audience is discouraged from clapping during the segment and giving silent reflection instead.\\r\\nIn terms of broadcast length, the ceremony generally averages three and a half hours. The first Oscars, in 1929, lasted 15?minutes. At the other end of the spectrum, the 2002 ceremony lasted four hours and twenty-three minutes.[61][62] In 2010, the organizers of the Academy Awards announced winners' acceptance speeches must not run past 45?seconds. This, according to organizer Bill Mechanic, was to ensure the elimination of what he termed \\"the single most hated thing on the show\\" ÿ overly long and embarrassing displays of emotion.[63] In 2016, in a further effort to streamline speeches, winners' dedications were displayed on an on-screen ticker.[64] During the 2018 ceremony, host Jimmy Kimmel acknowledged how long the ceremony had become, by announcing that he would give a brand-new jet ski to whoever gave the shortest speech of the night (a reward won by Mark Bridges when accepting his Best Costume Design award for Phantom Thread).[65]\\r\\nAlthough still dominant in ratings, the viewership of the Academy Awards have steadily dropped; the 88th Academy Awards were the lowest-rated in the past eight years (although with increases in male and 18-49 viewership), while the show itself also faced mixed reception. Following the show, Variety reported that ABC was, in negotiating an extension to its contract to broadcast the Oscars, seeking to have more creative control over the broadcast itself. Currently and nominally, AMPAS is responsible for most aspects of the telecast, including the choice of production staff and hosting, although ABC is allowed to have some input on their decisions.[66] In August 2016, AMPAS extended its contract with ABC through 2028: the contract neither contains any notable changes, nor gives ABC any further creative control over the telecast.[67]\\r\\nHistorically, the \\"Oscarcast\\" has pulled in a bigger haul when box-office hits are favored to win the Best Picture trophy. More than 57.25 million viewers tuned to the telecast for the 70th Academy Awards in 1998, the year of Titanic, which generated close to US$600 million at the North American box office pre-Oscars.[68] The 76th Academy Awards ceremony in which The Lord of the Rings: The Return of the King (pre-telecast box office earnings of US$368 million) received 11 Awards including Best Picture drew 43.56 million viewers.[69] The most watched ceremony based on Nielsen ratings to date, however, was the 42nd Academy Awards (Best Picture Midnight Cowboy) which drew a 43.4% household rating on 7 April 1970.[70]\\r\\nBy contrast, ceremonies honoring films that have not performed well at the box office tend to show weaker ratings. The 78th Academy Awards which awarded low-budgeted, independent film Crash (with a pre-Oscar gross of US$53.4 million) generated an audience of 38.64 million with a household rating of 22.91%.[71] In 2008, the 80th Academy Awards telecast was watched by 31.76 million viewers on average with an 18.66% household rating, the lowest rated and least watched ceremony at the time, in spite of celebrating 80 years of the Academy Awards.[72] The Best Picture winner of that particular ceremony was another independently financed film (No Country for Old Men).\\r\\nIn 1929, the first Academy Awards were presented at a banquet dinner at the Hollywood Roosevelt Hotel. From 1930 to 1943, the ceremony alternated between two venues: the Ambassador Hotel on Wilshire Boulevard and the Biltmore Hotel in downtown Los Angeles.\\r\\nGrauman's Chinese Theatre in Hollywood then hosted the awards from 1944 to 1946, followed by the Shrine Auditorium in Los Angeles from 1947 to 1948. The 21st Academy Awards in 1949 were held at the Academy Award Theatre at what was the Academy's headquarters on Melrose Avenue in Hollywood.[73]\\r\\nFrom 1950 to 1960, the awards were presented at Hollywood's Pantages Theatre. With the advent of television, the awards from 1953 to 1957 took place simultaneously in Hollywood and New York, first at the NBC International Theatre (1953) and then at the NBC Century Theatre, after which the ceremony took place solely in Los Angeles. The Oscars moved to the Santa Monica Civic Auditorium in Santa Monica, California in 1961. By 1969, the Academy decided to move the ceremonies back to Los Angeles, this time to the Dorothy Chandler Pavilion at the Los Angeles County Music Center.\\r\\nIn 2002, the Dolby Theatre (previously known as the Kodak Theatre) became the presentation's current venue.[74]\\r\\nIn the first year of the awards, the Best Directing award was split into two categories (Drama and Comedy). At times, the Best Original Score award has also been split into separate categories (Drama and Comedy/Musical). From the 1930s through the 1960s, the Art Direction (now Production Design), Cinematography, and Costume Design awards were likewise split into two categories (black-and-white films and color films). Prior to 2012, the Production Design award was called Art Direction, while the Makeup and Hairstyling award was called Makeup.\\r\\nThe Board of Governors meets each year and considers new award categories. To date, the following proposed categories have been rejected:\\r\\nThe Special Academy Awards are voted on by special committees, rather than by the Academy membership as a whole. They are not always presented on a consistent annual basis.\\r\\nDue to the positive exposure and prestige of the Academy Awards, studios spend millions of dollars and hire publicists specifically to promote their films during what is typically called the \\"Oscar season\\". This has generated accusations of the Academy Awards being influenced more by marketing than quality. William Friedkin, an Academy Award-winning film director and former producer of the ceremony, expressed this sentiment at a conference in New York in 2009, describing it as \\"the greatest promotion scheme that any industry ever devised for itself\\".[80]\\r\\nTim Dirks, editor of AMC's filmsite.org, has written of the Academy Awards,\\r\\nUnfortunately, the critical worth, artistic vision, cultural influence and innovative qualities of many films are not given the same voting weight. Especially since the 1980s, moneymaking \\"formula-made\\" blockbusters with glossy production values have often been crowd-pleasing titans (and Best Picture winners), but they haven't necessarily been great films with depth or critical acclaim by any measure.[81]\\r\\nTypical criticism of the Academy Awards for Best Picture is that among the winners and nominees there is an over-representation of romantic historical epics, biographical dramas, romantic dramedies, and family melodramas, most of which are released in the U.S. the last three months of the calendar year. The Oscars have been infamously known for selecting specific genres of movies to be awarded. This has led to the coining of the term 'Oscar bait', describing such movies. This has led at times to more specific criticisms that the Academy is disconnected from the audience, e.g., by favoring 'Oscar bait' over audience favorites, or favoring historical melodramas over critically acclaimed movies that depict current life issues.[82]\\r\\nThe Academy Awards have long received criticism over its lack of diversity among the nominees.[83][84][85] This criticism is based on the statistics from every Academy Awards since 1929 which shows us that only 6.4% of academy award nominees have been non-white and since 1991, 11.2% of nominees have been non-white, with the rate of winners being even more polarizing [86]. The 88th awards ceremony became the target of a boycott, popularized on social media by the #OscarsSoWhite, based on critics' perception that its all-white acting nominee list reflected bias. In response, the Academy initiated \\"historic\\" changes in membership by the year 2020.[87][88]\\r\\nActing prizes in certain years have been criticized for not recognizing superior performances so much as being awarded for personal popularity[89] or presented as a \\"career honor\\" to recognize a distinguished nominee's entire body of work.[90]\\r\\nSome winners critical of the Academy Awards have boycotted the ceremonies and refused to accept their Oscars. The first to do so was screenwriter Dudley Nichols (Best Writing in 1935 for The Informer). Nichols boycotted the 8th Academy Awards ceremony because of conflicts between the Academy and the Writers' Guild.[91] Nichols eventually accepted the 1935 award three years later, at the 1938 ceremony. Nichols was nominated for three further Academy Awards during his career.\\r\\nGeorge C. Scott became the second person to refuse his award (Best Actor in 1970 for Patton) at the 43rd Academy Awards ceremony. Scott described it as a 'meat parade', saying 'I don't want any part of it.\\"[92][93][94]\\r\\nThe third person to refuse the award was Marlon Brando, who refused his award (Best Actor for 1972's The Godfather), citing the film industry's discrimination and mistreatment of Native Americans. At the 45th Academy Awards ceremony, Brando sent actress and Civil rights activist Sacheen Littlefeather to read a 15-page speech detailing his criticisms.[91]\\r\\nThe following events are closely associated with the annual Academy Awards:\\r\\nIt has become a tradition to give out gift bags to the presenters and performers at the Oscars. In recent years, these gifts have also been extended to award nominees and winners.[95] The value of each of these gift bags can reach into the tens of thousands of dollars. In 2014, the value was reported to be as high as US$80,000.[96] The value has risen to the point where the U.S. Internal Revenue Service issued a statement regarding the gifts and their taxable status.[97] Oscar gift bags have included vacation packages to Hawaii and Mexico and Japan, a private dinner party for the recipient and friends at a restaurant, videophones, a four-night stay at a hotel, watches, bracelets, vacation packages, spa treatments, bottles of vodka, maple salad dressing, and weight-loss gummie candy.[95][98][99] Some of the gifts have even had a \\"risque\\" element to them; in 2014, the adult products retailer Adam & Eve had a \\"Secret Room Gifting Suite\\". Celebrities visiting the gifting suite included Judith Hoag, Carolyn Hennesy, Kate Linder, Chris Mulkey, Jim O'Heir, and NBA player John Salley.[100]\\r\\nFrom 2006 onwards, results are Live+SD, all previous years are Live viewing[101]\\r\\nThe term \\"Oscar\\" is a registered trademark of the AMPAS; however, in the Italian language, it is used generically to refer to any award or award ceremony, regardless of which field.[109][110]","input":"What is the academy award statue made of?"},{"output":"September 11","context":"Related to Hurricane Ivan\\r\\nHistory\\r\\nEffects\\r\\nOther wikis\\r\\nHurricane Ivan was a large, long-lived, Cape Verde-type hurricane that caused widespread damage in the Caribbean and United States. The cyclone was the ninth named storm, the sixth hurricane and the fourth major hurricane of the active 2004 Atlantic hurricane season. Ivan formed in early September, and reached Category 5 strength on the Saffir-Simpson Hurricane Scale.\\r\\nIvan caused catastrophic damage to Grenada as a strong Category 3 storm, heavy damage to Jamaica as a strong Category 4 storm and then Grand Cayman, Cayman Islands and the western tip of Cuba as a Category 5 storm. After peaking in strength, the hurricane moved north-northwest across the Gulf of Mexico to strike Pensacola/Milton, Florida and Alabama as a strong Category 3 storm, causing significant damage. Ivan dropped heavy rains on the Southeastern United States as it progressed northeast and east through the eastern United States, becoming an extratropical cyclone. The remnant low from the storm moved into the western subtropical Atlantic and regenerated into a tropical cyclone, which then moved across Florida and the Gulf of Mexico into Louisiana and Texas, causing minimal damage. Ivan caused an estimated US$18?billion (2004?USD, $22.8?billion 2017?USD) in damages to the United States, making it the seventh costliest hurricane ever to strike the country.\\r\\n\\r\\n\\r\\nOn September 2, 2004, Tropical Depression Nine formed from a large tropical wave southwest of Cape Verde. As the system moved to the west, it strengthened gradually, becoming Tropical Storm Ivan on September 3 and reaching hurricane strength on September 5, 1,150 miles (1,850?km) to the east of Tobago. Later that day, the storm intensified rapidly, and by 5?pm EDT (2100 UTC), Ivan became a Category 3 hurricane with winds of 125 miles per hour (200?km/h). The National Hurricane Center said that the rapid strengthening of Ivan on September 5 was unprecedented at such a low latitude in the Atlantic basin.[1][2]\\r\\nAs it moved east, Ivan weakened slightly because of wind shear in the area.[3] The storm passed over Grenada on September 7, battering several of the Windward Islands. As it entered the Caribbean Sea, Ivan reintensified rapidly and became a Category?5 hurricane just north of the Windward Netherlands Antilles (Cura?ao and Bonaire) and Aruba on September 9 with winds reaching 160?mph (260?km/h). Ivan weakened slightly as it moved west-northwest towards Jamaica. As Ivan approached the island late on September 10, it began a westward jog that kept the eye and the strongest winds to the south and west. However, because of its proximity to the Jamaican coast, the island was battered with hurricane-force winds for hours.[1]\\r\\nAfter passing Cuba, Ivan resumed a more northerly track and regained Category?5 strength. Ivan's strength continued to fluctuate as it moved west on September 11, and the storm attained its highest winds of 163?mph (262?km/h) as it passed within 30 miles (50?km) of Grand Cayman. Ivan reached its peak strength with a minimum central pressure of 910 millibars (27?inHg) on September 12. Ivan passed through the Yucatn Channel late on September 13 while its eyewall affected the westernmost tip of Cuba. Once over the Gulf of Mexico, it weakened slightly to Category?4 strength, which it maintained while approaching the Gulf Coast of the United States.[1]\\r\\nJust before it made landfall in the United States, Ivan's eyewall weakened considerably, and its southwestern portion almost disappeared.[citation needed] Around 2?am CDT September 16 (0700 UTC), Ivan made landfall on the U.S. mainland in Gulf Shores, Alabama as a Category?3 hurricane with 120?mph (190?km/h) winds; some hurricane information sources put the winds from Hurricane Ivan near 130?mph (210?km/h) upon landfall in Alabama and northwestern Florida.[4][5] Ivan then continued inland, maintaining hurricane strength until it was over central Alabama. Ivan weakened rapidly that evening and became a tropical depression the same day, still over Alabama. Ivan lost tropical characteristics on September 18 while crossing Virginia. Later that day, the remnant low drifted off the U.S. mid-Atlantic coast into the Atlantic Ocean, and the low pressure disturbance continued to dump rain on the United States.[1]\\r\\nOn September 20, Ivan's remnant surface low completed an anticyclonic loop and moved across the Florida peninsula. As it continued west across the northern Gulf of Mexico, the system reorganized and again took on tropical characteristics.[1] On September 22 the National Weather Service, \\"after considerable and sometimes animated in-house discussion [regarding] the demise of Ivan,\\"[6] determined that the low was in fact a result of the remnants of Ivan and thus named it accordingly. On the evening of September 23, the revived Ivan made landfall near Cameron, Louisiana as a tropical depression. Ivan finally dissipated on September 24 as it moved overland into Texas.[1]\\r\\nIvan set 18 new records for intensity at low latitudes. When Ivan first became a Category 3 hurricane on September 3 (1800 UTC), it was centered near 10.2?degrees north from the equator. This is the most southerly location on record for a major hurricane in the Atlantic basin.[1] Just six hours later, Ivan also became the most southerly Category?4 hurricane on record in the Atlantic basin when it reached that intensity while located at 10.6?degrees north.[7] Finally, at midnight (UTC) on September 9 while centered at 13.7?degrees north, Ivan became the most southerly Category?5 hurricane on record in the Atlantic basin.[7] The latter record would not be surpassed until Hurricane Matthew in 2016, which reached Category?5 intensity at 13.4?degrees north.[8] Ivan was also tied with the \\"Cuba hurricane\\" as the tenth-most intense Atlantic hurricane on record attaining a minimum atmospheric pressure of 910 millibars (26.87 inHg). This record was later surpassed by Hurricane Maria of 2017 which attained a minimum atmospheric pressure of 908 millibars (26.81 inHg).\\r\\nIvan had held the world record of 33 (with 32 consecutive) six-hour periods of intensity at or above Category?4 strength. This record was broken two years later by Pacific Hurricane/Typhoon Ioke, which had 36 (33 consecutive) six-hour periods at Category?4 strength. This contributed to Ivan's total Accumulated Cyclone Energy (ACE) of 70.38.[9] The tornado outbreak associated with Ivan spawned 120 tornadoes, more than any other tropical cyclone worldwide.\\r\\nScientists from the Naval Research Laboratory at Stennis Space Center, Mississippi have used a computer model to predict that, at the height of the storm, the maximum wave height within Ivan's eyewall reached 131 feet (40?m).[10]\\r\\nBy September 5, a hurricane watch was posted for Barbados. Early on the following day, a tropical storm watch was issued for Grenada. Later that day, hurricane watches were also put into effect for Saint Lucia, and Martinique. A tropical storm warning was issued for Saint Vincent and Grenadines and Tobago and Grenada. By 1500 UTC on September 6, the hurricane watches and tropical storm watches and warnings were upgraded to a hurricane warning and expanded to: Barbados, Saint Vincent and Grenadines, Saint Lucia, Tobago, Grenada. Simultaneously, a tropical storm warning was issued for Trinidad. On September 7, the hurricane warning in effect for several countries was downgraded to a tropical storm warning. By September, all tropical storm and hurricane watches and warnings were discontinued in the eastern portions of the Windward Islands.[1]\\r\\nAs Ivan continued westward, a hurricane watch was issued for the ABC islands on September 8.[1] Many schools and businesses were closed in the Netherlands Antilles,[11] and about 300?people evacuated their homes on Cura?ao.[12]\\r\\nIn the Caribbean, 500,000?Jamaicans were told to evacuate from coastal areas,[13] but only 5,000 were reported to have moved to shelters.[14] 12,000 residents and tourists were evacuated from Isla Mujeres off the Yucatn Peninsula.[15]\\r\\nIn Louisiana, mandatory evacuations of vulnerable areas in Jefferson, Lafourche, Plaquemines, St. Charles, St. James, St. John the Baptist, and Tangipahoa parishes took place, with voluntary evacuations ordered in six other parishes. More than one-third of the population of Greater New Orleans evacuated voluntarily, including more than half of the residents of New Orleans itself. At the height of the evacuation, intense traffic congestion on local highways caused delays of up to 12?hours. About a thousand special-needs patients were housed at the Louisiana Superdome during the storm. Ivan was considered a particular threat to the New Orleans area because dangers of catastrophic flooding. However, Plaquemines and St. Bernard Parishes suffered a moderate amount of wind damage. Hurricane preparedness for New Orleans was judged poor.[16] At one point, the media sparked fears of an \\"Atlantean\\" catastrophe if the hurricane were to make a direct strike on the city.[17] These fears were not realized, as the storm's path turned further east.\\r\\nIn Mississippi, evacuation of mobile homes and vulnerable areas took place in Hancock, Jackson, and Harrison counties.[18] In Alabama, evacuation in the areas of Mobile and Baldwin counties south of Interstate 10 was ordered, including a third of the incorporated territory of the City of Mobile, as well as several of its suburbs.[19] In Florida, a full evacuation of the Florida Keys began at 7:00?am EDT September 10 but was lifted at 5:00?am EDT September 13 as Ivan tracked further west than originally predicted.[20] Voluntary evacuations were declared in ten counties along the Florida Panhandle, with strong emphasis in the immediate western counties of Escambia, Santa Rosa, and Okaloosa. Ivan prompted the evacuation of 270?animals at \\"The Little Zoo That Could\\" in Alabama. The evacuation had to be completed within a couple of hours, with only 28 volunteers available to move the animals.[21]\\r\\nIvan killed 64?people in the Caribbeanmainly in Grenada and Jamaicathree in Venezuela, and 25 in the United States, including fourteen in Florida. Thirty-two more deaths in the United States were indirectly attributed to Ivan. While traversing the eastern United States, Ivan spawned 120 tornadoes, striking communities along concentric arcs on the leading edge of the storm.[26] In Florida, Blountstown, Marianna, and Panama City Beach suffered three of the most devastating tornadoes. A Panama City Beach news station was nearly hit by an F2 tornado during the storm.[27] Ivan also caused over US$13?billion (2004 USD, $16.5?billion 2017?USD) in damages in the United States and US$3?billion in the Caribbean (2004?USD, $3.8?billion 2017?USD).\\r\\nIvan passed directly over Grenada on September 7, 2004, killing 39?people. The capital, St. George's, was severely damaged and several notable buildings were destroyed, including the residence of the prime minister. Ivan also caused extensive damage to a local prison, allowing most of the inmates to escape. The island, in the words of a Caribbean disaster official, suffered \\"total devastation.\\" According to a member of the Grenadian parliament, at least 85% of the small island was devastated.[28] Extensive looting was reported. In all, damage on the island totalled US$815?million (2004?USD, $1.03?billion 2017?USD).[1]\\r\\nElsewhere in the Caribbean, a pregnant woman was killed in Tobago when a tree fell on top of her home,[11] and a 75-year-old Canadian woman drowned in Barbados.[1] Three deaths were reported in Venezuela.[29] Over five hundred homes on Barbados[30] and around 60?homes in Saint Vincent and the Grenadines were either damaged or destroyed.[1][25]\\r\\nOn September 11 ÿ 12, the center of Ivan passed near Jamaica, causing significant wind and flood damage. Overall, 17?people were killed in Jamaica and 18,000?people were left homeless as a result of the flood waters and high winds.[31] Most of the major resorts and hotels fared well, though, and were reopened only a few days after Ivan had passed.[32] Damage on Jamaica totaled US$360?million (2004?USD, $456?million 2017?USD).[1]\\r\\nIn the Cayman Islands, Governor Bruce Dinwiddy described damage as \\"very, very severe and widespread.\\" Despite strict building codes which made the islands' buildings well able to withstand even major hurricanes, Ivan's winds and storm surge were so strong that a quarter or more of the buildings on the islands were reported to be uninhabitable, with 85% damaged to some extent. Much of Grand Cayman still remained without power, water, or sewer services for several months later. After five months, barely half the pre-Ivan hotel rooms were usable. Only one person was killed on the islands,[22] though at first many deaths were suspected because of the many graves that were washed up during the storm.[citation needed] Damage across the territory was catastrophic, with losses amounting to US$2.86?billion or 183?percent of its gross domestic product.[24] The Letter from the Cayman Islands Government Office in the United Kingdom, 8 October 2004 by McKeeva Bush, Leader of Government Business details the intensity, extent of damage, and recovery process during the months that followed.[33]\\r\\nThere were four deaths in the Dominican Republic. The region's Caribbean Development Bank estimates Ivan caused over US$3?billion (2004?USD, $3.8?billion 2017?USD) damage on island nations, mostly in the Cayman Islands, Grenada, and Jamaica.[1] Minor damage, including some beach erosion, was reported in the ABC islands.[34]\\r\\nEven though Ivan did not make landfall on Cuban soil, its storm surge caused localized flooding on Santiago de Cuba and Granma, on the southern part of the island. At Cienfuegos, the storm produced waves of 15 feet (4.6?m), and Pinar del Ro recorded 13.3 inches (340?mm) of rainfall. While there were no casualties on the island, the Cuban government estimates that about US$1.2?billion (2004?USD, $1.52?billion 2017?USD) of property damage were directly due to Ivan.[22]\\r\\nAlong with the 14?deaths in Florida, Ivan is blamed for eight deaths in North Carolina, two in Georgia, and one in Mississippi. An additional 32?deaths were reported as indirectly caused by the storm.[1]\\r\\nAs it passed over the Gulf of Mexico off the coast of Louisiana, Ivan caused the destruction of Taylor Energy's Mississippi Canyon 20-A production platform, 550 feet above 28 producing oil and gas wells drilled in water 479 feet deep. Waves estimated to be 71 feet caused tremendous pressures below the surface, causing a landslide that obliterated the platform. Hundreds of gallons of oil per day were still leaking onto the surface of the Gulf ten years later in 2014, and continue to appear to the present date [2].\\r\\nIvan caused an estimated US$13?billion (2004?USD, $16.5?billion 2017?USD) in damage in the United States alone, making it the third costliest hurricane on record at the time, just behind Hurricane Charley's US$14?billion (2004 USD, $17.8?billion 2017?USD), and above Hurricane Hugo. It has since fallen to seventh place.[35]\\r\\nAs Ivan made landfall on the U.S. coastline in Florida, there was heavy damage as observed in Pensacola, Gulf Breeze, Navarre Beach, and Pensacola Beach, dwellings situated far inland, as much as 20 miles (32?km) from the Gulf coast, along the shorelines of Escambia Bay, East Bay, Blackwater Bay, and Ward Basin in Escambia County and Santa Rosa County, and Fort Walton Beach, Florida on the eastern side of the storm. The area just west of Pensacola, including the community of Warrington (which includes Pensacola NAS), Perdido Key, and Innerarity Point, took the brunt of the storm. Some of the subdivisions in this part of the county were completely destroyed, with a few key roads in the Perdido area only opened in late 2005, over a year after the storm hit. Shattered windows from gusts and flying projectiles experienced throughout the night of the storm were common. As of December 2007, roads remained closed on Pensacola Beach because of damage from Ivan's storm surge.[39]\\r\\nIn Pensacola, the Interstate 10 Escambia Bay Bridge was heavily damaged, with as much as a quarter-mile (400?m) of the bridge collapsing into the bay. The causeway that carries U.S. Highway 90 across the northern part of the same bay was also heavily damaged. Virtually all of Perdido Key, an area on the outskirts of Pensacola that bore the brunt of Ivan's winds and rain, was essentially leveled. High surf and wind brought extensive damage to Innerarity Point.[39]\\r\\nOn September 26, 2006, over two years after Ivan struck the region, funding for the last 501?FEMA-provided trailers ran out for those living in Santa Rosa and Escambia counties.[39]\\r\\nThe city of Demopolis, over 100 miles (160?km) inland in west-central Alabama, endured wind gusts estimated at 90?mph (140?km/h), while Montgomery saw wind gusts in the 60 to 70?mph (97 to 113?km/h) range at the height of the storm.[40]\\r\\nThe heaviest damage as Ivan made landfall on the U.S. coastline was observed in Baldwin County in Alabama, where the storm's eye (and eyewall) made landfall. High surf and wind brought extensive damage to Orange Beach near the border with Florida. There, two five-story condominium buildings were undermined to the point of collapse by Ivan's storm surge of 14 feet (4.3?m). Both were made of steel-reinforced concrete. Debris gathered in piles along the storm tide, exacerbating the damage when the floodwaters crashed into homes sitting on pilings.[41] Brewton, a community about 50 miles (80?km) inland, also suffered severe damage.\\r\\nIn addition to the damage to the southern portions of the state, there was extensive damage to the state's electrical grid. At the height of the outages, Alabama Power reported 489,000?subscribers had lost electrical powerroughly half of its subscriber base.\\r\\nFurther inland, Ivan caused major flooding, bringing the Chattahoochee River near Atlanta and many other rivers and streams to levels at or near 100-year records. The Delaware River and its tributaries crested just below their all-time records set by Hurricane Diane in 1955. Locations in southern New Hampshire and Massachusetts received over 7?inches of rainfall from the remnants of Ivan, causing flooding and mudslides. In Connecticut, high winds moved in quickly and unexpectedly, and a boater was killed when his trimaran capsized in 50-knot winds on Long Island Sound.[42]\\r\\nIn western North Carolina, many streams and rivers reached well above flood stage in an area that was heavily flood damaged just a week and a half prior from the remnants of Hurricane Frances, causing many roads to be closed. High winds contributed to widespread power outages throughout the mountainous region. The Blue Ridge Parkway as well as Interstate 40 through the Pigeon River gorge in Haywood County, North Carolina sustained major damage, and landslides were common across the mountains. There was major flooding along the French Broad River and Swannanoa River in Asheville, North Carolina and along the Pigeon River near Canton, North Carolina. As a result of the rain, a major debris flow of mud, rocks, trees, and water surged down Peek's Creek, near Franklin, North Carolina, sweeping away 15?houses and killing five people.[43][44]\\r\\nThe system also spawned deadly tornadoes as far north as Maryland[45] and destroyed seven oil platforms in the Gulf of Mexico while at sea. While crossing over the Mid-Atlantic states, Ivan's remnants spawned 117 tornadoes across the eastern United States, with the 40?tornadoes spawned in Virginia on September 17 setting a daily record for the commonwealth.[46] Ivan then moved into the Wheeling, West Virginia and Pittsburgh area, causing major flooding and gusty winds. Pittsburgh International Airport recorded the highest 24-hour rainfall for Pittsburgh, recording 5.95 inches (151?mm) of rain.[47] Ivan's rain caused widespread flooding. The Juniata River basin was flooded, and the Frankstown Branch crested at its highest level ever.[48] After Ivan regenerated in the Gulf of Mexico, it caused further heavy rainfall up to 8 inches (200?mm) in areas of Louisiana and Texas.\\r\\nOn the morning of September 21, the remnant mid-level circulation of Ivan combined with a frontal system. This produced a plume of moisture over the Canadian Maritimes for four days, producing heavy rainfall totaling 6.2 inches (160?mm) in Gander, Newfoundland. High winds of up to 89?mph (143?km/h) downed trees and caused power outages in Newfoundland, Prince Edward Island, and eastern Nova Scotia. The system produced intense waves of up to 50 feet (15?m) near Cape Bonavista. The system killed two when it grounded a fishing vessel and was indirectly responsible for four traffic fatalities in Newfoundland.[49]\\r\\nGrenada suffered serious economic repercussions following the destruction caused by Ivan. Before Ivan, the economy of Grenada was projected to grow by 4.7%, but the island's economy instead contracted by nearly 3% in 2004. The economy was also projected to grow by at least 5% through 2007, but, as of 2005[update], that estimate had been lowered to less than 1%. The government of Grenada also admitted that government debt, 130% of the island's GDP, was \\"unsustainable\\" in October 2004 and appointed a group of professional debt advisors in January 2005 to help seek a cooperative restructuring agreement with creditors.[50]\\r\\nMore than US$150?million was sent to Grenada in 2004 to aid reconstruction following Ivan, but the economic situation remains fragile. The International Monetary Fund reports that as \\"difficult enough as the present fiscal situation is, it is unfortunately quite easy to envisage circumstances that would make it even more so.\\" Furthermore, \\"shortfalls in donor financing and tax revenues, or events such as a further rise in global oil prices, pose a grave risk.\\"[51]\\r\\nBy two days after Ivan's passage, USAIDs hurricane recovery program distributed emergency relief supplies for families who were displaced by the storm. During phase one of the recovery program, communities restored three tourist sites, cleared agricultural lands, and completed disaster mitigation. In addition, the U.S. Peace Corps completed thirty small projects in rural communities and low income neighborhoods. 66 health clinics, 25 schools, and 62 water and sanitation systems were repaired during the first phase of recovery. About 1,379 farmers, herders and micro businesses became eligible for grants. By 2005, 55 schools and colleges were repaired, while restoration of 1,560 houses had occurred.[52]\\r\\nOn September 27, 2004, President of the United States George W. Bush submitted a budget to the United States Congress which requested over $7 billion (2004 USD) in aid to victims of Hurricane Ivan and Jeanne in the following states: Alabama, Florida, Georgia, Louisiana, Mississippi, North Carolina, Ohio, Pennsylvania, and West Virginia. Over half of the $7 billion (2004 USD) was to cover uninsured damage to property and public infrastructure. $889 million was spent to repair Department of Defense facilities. About $600 million was earmarked for emergency repairs to highways and road damaged by Hurricanes Charley, Frances, Ivan, and Jeanne. The Small Business Administration (SBA) used $472 million to provide loans for small businesses and homeowners affected by the storm. Approximately $400 million was given by the United States Department of Agriculture to provide financial assistance agricultural producers suffering crop and other losses. Around $132 million (2004 USD) was used to repair Federal facilities by several government agencies, including: United States Coast Guard, Federal Bureau of Prisons, the United States Forest Service, and the Federal Aviation Administration. The United States Army Corps of Engineers used $81 million (2004 USD) for restoration of coastal areas affected by Ivan. In addition, $50 million (2004 USD) of which was for disaster and famine assistance funds Grenada, Jamaica, and Haiti.[53]\\r\\nFollowing the storm in Alabama, more than 167,700 people applied for assistance in 65 counties in the state. over 51 counties in the state became eligible for public assistance. As a result, the U.S. Department of Homeland Security's Federal Emergency Management Agency (FEMA) and the Alabama Emergency Management Agency (AEMA) received $735 million (2004 USD), which was spent in disaster assistance, and included: low-interest loans for homeowners and businesses, disaster food stamps, Disaster Unemployment Assistance to those left unemployed as a result of Ivan, \\"Project Rebound\\", and to fill the 5,856 National Flood Insurance Program claims. In addition, there were repairs to public infrastructure such as roads, bridges, buildings, utilities, facilities, and parks. 20 Disaster Recovery Centers were opened in 13 counties, which also included the Poarch Creek Indian Reservation. Overall, FEMA paid 90% of the $735 million (2004 USD), while the AEMA paid for the other 10%.[54]\\r\\nIvan is suspected of bringing spores of soybean rust from Venezuela into the United States, the first ever occurrences of soybean rust found in North America. Since the Florida soybean crop had already been mostly harvested, economic damage was limited. Some of the most severe outbreaks in South America have been known to reduce soybean crop yields by half or more.[55] Following the storm, more than 138,500 residents in 15 counties of the Florida Panhandle applied for federal and state aid. In those counties, a total of $162.6 million was approved by FEMAs Individuals and Households Program. In addition, residents of 24 other counties in Florida were eligible for grants and loans. By September 2005, more than $1.4 billion (2004 USD) in federal and state assistance was approved for residents and communities in the Florida Panhandle. In addition, the National Flood Insurance Program paid nearly $869 million (2004 USD) for more than 9,800 insurance claims after Ivan.[56]\\r\\nMore than $4 million (2004 USD) in disaster assistance was approved for Mississippi by FEMA and the Mississippi Emergency Management Agency (MEMA). In addition, the SBA issued nearly 3,000 applications for low-interest loans to homeowners, renters, landlords, businesses, and non-profit organizations. The loans covered up to $200,000 in real estate repairs/replacements and up to $40,000 in repairs/replacements of personal property.[57]\\r\\nResidents and business owners in eight parishes of Louisiana became eligible for disaster assistance. By one week before the deadline to apply on November 15, 2004, about 9,527 residents applied for disaster assistance. Overall, FEMA and the Government of Louisiana provided more than $3.8 million (2004 USD) to those that requested assistance. In addition, the SBA also allowed applications for loans to repair personal property until that day.[58]\\r\\nThis storm marked the third occasion the name \\"Ivan\\" had been used to name a tropical cyclone in the Atlantic, as well as the fifth of six occurrences worldwide. Because of the severe damage and number of deaths in the Caribbean and United States, the name Ivan was retired in the spring of 2005 by the World Meteorological Organization and will never again be used in the Atlantic basin.[59] It was replaced by Igor for the 2010 season.[60]\\r\\nIvan broke several hydrological records; it is credited with possibly causing the largest ocean wave ever recorded, a 91-foot (28-meter) wave that may have been as high as 131?ft (40?m), and the fastest seafloor current, at 2.25?m/s (5?mph).[61]","input":"When did hurricane ivan hit the cayman islands?"},{"output":"Toyota Motor Corporation","context":"","input":"What is the parent company for all toyota divisions worldwide?"},{"output":"December 20, 1951","context":"","input":"When was nuclear power first used to generate electricity?"},{"output":"an Italic settlement in the Italian peninsula, dating from the 8th century BC, that grew into the city of Rome","context":"","input":"Where did the civilisation of ancient rome begin?"},{"output":"Sir Edward Elgar","context":"The Pomp and Circumstance Marches (full title Pomp and Circumstance Military Marches), Op.?39, are a series of marches for orchestra composed by Sir Edward Elgar. They include some of Elgar's best-known compositions.\\r\\nThe title is taken from Act III, Scene 3 of Shakespeare's Othello:\\r\\nFarewell the neighing steed and the shrill trump,\\r\\nThe spirit-stirring drum, th'ear-piercing fife,\\r\\nThe royal banner, and all quality,\\r\\nPride, pomp, and circumstance of glorious war![1]\\r\\nBut also, on the score of the first march, Elgar set as a motto for the whole set of marches a verse from Lord de Tabley's poem \\"The March of Glory\\",[2] which (as quoted by Elgar's biographer Basil Maine) begins[3]\\r\\nLike a proud music that draws men on to die\\r\\nMadly upon the spears in martial ecstasy,\\r\\nA measure that sets heaven in all their veins\\r\\n???And iron in their hands.\\r\\nI hear the Nation march\\r\\nBeneath her ensign as an eagle's wing;\\r\\nO'er shield and sheeted targe\\r\\nThe banners of my faith most gaily swing;\\r\\nMoving to victory with solemn noise,\\r\\nWith worship and with conquest, and the voice of myriads.\\r\\nproclaiming the \\"shows of things\\" (Maine's quotation marks):[4] the na?ve assumption that the splendid show of military pageantry\\"Pomp\\"has no connection with the drabness and terror\\"Circumstance\\"of actual warfare.[2] The first four marches were all written before the events of World War I shattered that belief, and the styles in which wars were written about spurned the false romance of the battle-song.[2]\\r\\nThe Pomp and Circumstance marches are\\r\\nThe first five were all published by Boosey & Co. as Elgar's Op.?39, and each of the marches is dedicated to a particular musical friend of Elgar's.\\r\\nEach march takes about five minutes to play.[5]\\r\\nMarch No. 1, was composed in 1901 and dedicated \\"to my friend Alfred E. Rodewald and the members of the Liverpool Orchestral Society\\".\\r\\nThe instrumentation is: two piccolos (2nd ad lib.), two flutes, two oboes, two clarinets in A, bass clarinet in A, two bassoons, contrabassoon, four horns in F, two trumpets in F, two cornets in A, three trombones, tuba, three timpani, percussion (bass drum, cymbals, triangle, side drum, jingles, glockenspiel (ad. lib.) and tambourine (ad lib.)), two harps, organ, and strings.\\r\\nThe best known of the six marches, Pomp And Circumstance March No. 1 In D had its premiere, along with March No. 2, in Liverpool on 19 October 1901, with Alfred Rodewald[6] conducting the Liverpool Orchestral Society.[7] Elgar and his wife attended, and it was a \\"frantic\\" success.[8] Both marches were played two days later at a London Promenade Concert (which the Elgars unintentionally missed) in the Queen's Hall London, conducted by Henry Wood, with March No. 1 played second. Wood remembered that the audience \\"...rose and yelled... the one and only time in the history of the Promenade concerts that an orchestral item was accorded a double encore.\\"[9]\\r\\nThe Trio contains the tune known as \\"Land of Hope and Glory\\". In 1902 the tune was re-used, in modified form, for the Land of hope and glory section of his Coronation Ode for King Edward VII.[10] The words were further modified to fit the original tune, and the result has since become a fixture at the Last Night of the Proms, and an English sporting anthem.\\r\\n\\r\\nIn the United States, the Trio section \\"Land of Hope and Glory\\" of March No. 1 is often known simply as \\"Pomp and Circumstance\\" or as \\"The Graduation March\\" and is played as the processional tune at virtually all high school and some college graduation ceremonies.[11] It was first played at such a ceremony on 28 June 1905, at Yale University, where the Professor of Music Samuel Sanford had invited his friend Elgar to attend commencement and receive an honorary doctorate of music. Elgar accepted, and Sanford made certain he was the star of the proceedings, engaging the New Haven Symphony Orchestra, the College Choir, the Glee Club, the music faculty members, and New York musicians to perform two parts from Elgar's oratorio The Light of Life and, as the graduates and officials marched out, \\"Pomp and Circumstance\\" March No. 1. Elgar repaid the compliment by dedicating his Introduction and Allegro to Sanford later that year.[12] The tune soon became de rigueur at American graduations, used primarily as a processional at the opening of the ceremony.[13]\\r\\nMarch No. 1 opens with an introduction marked Allegro, con molto fuoco.[14][15] The introduction leads to a new theme: strong pairs of beats alternating with short notes, and a bass which persistently clashes with the tune. The bass tuba and full brass is held back until the section is repeated by the full orchestra. A little rhythmic pattern is played by the strings, then repeated high and low in the orchestra before the section is concluded by a chromatic upward scale from the woodwind. The whole of this lively march section is repeated. The bridging section between this and the well-known Trio has rhythmic chords from the brass punctuating high held notes from the wind and strings, before a fanfare from trumpets and trombones leads into the theme with which the march started. There are a few single notes that quieten, ending with a single quiet tap from side drum and cymbal accompanied by all the bassoons.[16] The famous, lyrical \\"Land of Hope and Glory\\" trio follows (in the subdominant key of G), played softly (by the first violins, four horns and two clarinets) and repeated by the full orchestra including two harps. What follows is a repetition of what has been heard before, including a fuller statement of the Trio (this time in the 'home' key of D) in which the orchestra is joined by organ as well as the two harps. The march ends, not with the big tune, but with a short section containing a brief reminder of the brisk opening march.\\r\\nMarch No. 2 was composed in 1901 and dedicated \\"To my friend Granville Bantock\\".\\r\\nThe instrumentation is: piccolo, 2 flutes, 2 oboes, 2 clarinets in A, bass clarinet in A, 2 bassoons, contrabassoon, 4 horns in F, 2 trumpets in F, 2 cornets in A, 3 trombones, tuba, timpani (3), percussion (2 side drums,[17] triangle, glockenspiel & jingles, bass drum & cymbals), and strings.\\r\\nIt was first performed, as was March No. 1, by the Liverpool Orchestral Society conducted by Alfred Rodewald, in Liverpool on 19 October 1901.[18] Both marches were played two days later at a London Promenade Concert.\\r\\nThe second is the shortest and most simply constructed of the marches. The composer Charles Villiers Stanford is said to have preferred this march to the first, and thought this the finest of all the marches. After a loud call to attention from the brass, a simple staccato theme, tense and repetitive, is played quietly by the strings, being gradually joined by other instruments before building up to an abrupt climax. This section is repeated. The second theme, confidently played by horns and clarinets, with contrasting triple and duple rhythms, is one which was sketched by Elgar a few years before: this is developed and ends with flourishes from the strings and brass joined by the glockenspiel. The opening staccato theme returns, concluded by a quiet swirling bass passage, which leads into the Trio section (in the tonic major key of A) which consists of a delightfully simple tune in thirds played by the woodwind (flutes, oboes, clarinets and bassoons), answered conclusively by the strings and brass. This Trio section is repeated, and the march concluded with a brilliant little coda, which includes a drum roll on the snare drum, a shattering chord in A Minor, briefly played by horns, and followed by a final cadence.\\r\\nMarch No. 3 was completed in November 1904 and published in 1905. It was dedicated \\"To my friend Ivor Atkins\\". It was first performed on 8 March 1905, in the Queen's Hall, London, conducted by the composer.\\r\\nThe instrumentation is: piccolo, 2 flutes, 2 oboes, cor anglais, 2 clarinets in B?, bass clarinet in B?, 3 bassoons,[19] contrabassoon, 4 horns in F, 2 trumpets in B?, 2 cornets in B?, 3 trombones, tuba, timpani (3), percussion (tenor drum, side drum, bass drum & cymbals), and strings.\\r\\nMarch No. 3 differs from the others in its opening mood, which is deliberately solemn. It begins with a dark subdued quick march led by low clarinets, three bassoons and the horns (with drum-beats inserted between the notes of the tune), before a vigorous theme (with brass alone at the first beats), erupts from the full orchestra. The dark theme re-appears, is then restarted boldly, then ended abruptly. The central section commences with a perky tune played by a solo clarinet with simple string accompaniment, which is followed by another of Elgar's noble tunes played by the strings of the orchestra. All the themes re-appear and there is the final section which ends abruptly.\\r\\nMarch No. 4 is as upbeat and ceremonial as No. 1, containing another big tune in the central Trio section.\\r\\nMarch No. 4 was completed on 7 June 1907, and dedicated \\"To my friend Dr. G. Robertson Sinclair, Hereford\\".[20] It was first performed on 24 August 1907, in the Queen's Hall, London, conducted by the composer.\\r\\nThe instrumentation is: piccolo (with 3rd flute), 2 flutes, 2 oboes, cor anglais, 2 clarinets in B?, bass clarinet in B?, 2 bassoons, contrabassoon, 4 horns in F, 3 trumpets in A, 3 trombones, tuba, timpani (3), percussion (side drum, bass drum & cymbals), 2 harps, and strings.\\r\\nThe Trio was used by Elgar in a song called \\"The King's Way\\" which he wrote, to his wife's words, in celebration of the opening of an important new London street called Kingsway in 1909.[21]\\r\\nIn World War II, No. 4 also acquired words: a patriotic poem by A. P. Herbert with the refrain beginning \\"All men must be free\\" was used as \\"Song of Liberty\\".[22]\\r\\nIn the wedding of Charles, Prince of Wales, and Lady Diana Spencer, Pomp and Circumstance No. 4 served as the recessional. As Diana's veil was lifted and the couple bowed and curtsied to Queen Elizabeth II, the opening notes sounded and continued as they walked down the aisle of St Paul's Cathedral out to the portico and the waiting crowds.[citation needed]\\r\\nThe march has an opening section consisting mainly of two-bar rhythmic phrases which are repeated in various forms, and a lyrical Trio constructed like the famous \\"Land of Hope and Glory\\" trio of March No. 1.\\r\\nThe first eight bars of the march is played by the full orchestra with the melody played by the violas[23] and upper woodwind. Both harps play from the beginning, while the cellos, double basses and timpani contribute a simple bass figure. The bass clarinet, contrabassoon, trombones and tuba are held \\"in reserve\\" for the repeat, when the first violins join the violas with the tune. There are subdued fanfares from the brass interrupted by little flourishes from the strings before the opening march is repeated. There is pause, then a little section which starts forcefully but quietens, leading into the Trio. The Trio follows the pattern of March No. 1, with the melody (in the subdominant key of C) played by clarinet, horn and violins. The violins start the Trio tune on the lowest note they can play, an \\"open\\" G-string, which gives a recognisable \\"twang\\" to this one note, and they are directed to play the passage \\"sul G\\"[24] on the same string, for the sake of the tone-colour, and the accompaniment is from the harps, low strings and bassoons. The grand tune is repeated, as we expect, by the full orchestra; the opening march section returns; the grand tune is repeated once more, in the \\"home\\" key of G major; and the last word is had by a re-statement of the opening rhythmic patterns. The march prepares the audience for its end as surely as a train pulling into a station, with the violins, violas, and cellos ending on their resonant \\"open\\" G.\\r\\nMarch No. 5 was composed in 1930, much later than the others, and dedicated \\"To my friend Dr. Percy C. Hull, Hereford\\".[25][26] Its first public performance was on 20 September 1930 in a Queen's Hall concert conducted by Sir Henry Wood, though it had been recorded two days earlier in the Kingsway Hall, London, conducted by Elgar himself in spite of his poor health.[27]\\r\\nThe instrumentation is: piccolo, 2 flutes, 2 oboes, cor anglais, 2 clarinets in B?, bass clarinet in B?, 2 bassoons, contrabassoon, 4 horns in F, 3 trumpets in B?, 3 trombones, tuba, timpani (3), percussion (side drum, bass drum & cymbals), and strings.\\r\\nWithout introduction, its opening episode is extended with enormous confidence and proceeds directly into the Trio section. The Trio starts quietly in a similar way to the introduction of his First Symphony: just a moving bass line and a tune, also in the same key (A?). The tune is re-stated strongly, as we expect, then developed. The re-statement of the opening employs the same instruments of the orchestra, but is this time started as soft as possible for just four bars before a quick crescendo restores its spirit to as it was in the beginning. There is more development before a big return of the Trio theme, in the home key of C, and a triumphant ending which might bring to mind the conclusion of Grieg's In the Hall of the Mountain King.\\r\\nElgar left sketches for a sixth Pomp and Circumstance march, to be the final work in the set. In 2005, these were sent by the lawyer for the Elgar Will Trust in a bundle to the English composer Anthony Payne. Also included was an article titled \\"Circumstantial Evidence\\" by the Elgar authority Christopher Kent from the August 1997 Musical Times explaining the sketches. One idea in the sketches was marked by the composer \\"jolly good\\". Kent believed that Elgar's compositional thoughts and time were by then engaged with the Third Symphony and The Spanish Lady, and that the main theme for the march was \\"unpromising\\".\\r\\nPayne felt there was not enough in the sketches to complete the march, but fortunately three pages of score in Elgar's handwriting were discovered at the Royal School of Church Music Colles Library marked \\"P&C 6\\". In 2006, the score and sketches were turned into a performing version. Payne observed in the program notes that \\"Nowhere else in the Pomp and Circumstance marches does Elgar combine compound and duple metres in this way\\". Payne concluded the piece with a brief allusion to the first Pomp and Circumstance March.\\r\\nThe world premiere was on 2 August 2006 with Andrew Davis conducting the BBC Symphony Orchestra at The Proms at Royal Albert Hall. The first recording was by the BBC National Orchestra of Wales under Richard Hickox.\\r\\nThe instrumentation is: piccolo, 2 flutes, 2 oboes, cor anglais, 2 clarinets in B?, bass clarinet in B?, 2 bassoons, contrabassoon, 4 horns in F, 3 trumpets in B?, 3 trombones, tuba, timpani (4), percussion (side drum, cymbals, bass drum, jingles, glockenspiel), and strings.\\r\\nFor piano solo: The first four marches were arranged by Adolf Schmid and March No. 5 by Victor Hely-Hutchinson.\\r\\nFor piano duo: March No. 1 was arranged by Adolf Schmid.[28]\\r\\nFor organ: March No. 1 was arranged by Edwin H. Lemare and March No. 4 was arranged by G. R. Sinclair.[20] Marches 1ÿ5 have been arranged in simplified and abbreviated form by William McVicker; concert transcriptions of Marches 2, 3, and 5, matching the Lemare and Sinclair arrangements, have been made by Michael Brough for use at Holy Trinity Sloane Street but have not yet been published.\\r\\nFor military band: The first four marches were arranged by M. Retford and March No. 5 by T. Conway Brown.[29]\\r\\nFor brass band: March No. 1 was arranged (transposed to B?) by J. Ord Hume.[30]\\r\\nNotes\\r\\nSources","input":"Who is the composer of pomp and circumstance?"},{"output":"134,385","context":"","input":"What is the population of charleston south carolina?"},{"output":"converted times","context":"The sports under the umbrella of athletics, particularly track and field use a large number of statistics. In order to report that information efficiently, numerous abbreviations have grown to be common in the sport.\\r\\nStarting in 1948 by Bert Nelson and Cordner Nelson, Track and Field News became the leader in creating and defining abbreviations in this field. But these abbreviations have also been adopted by, among others, the IAAF; the world governing body, various domestic governing bodies, the Association of Track and Field Statisticians, the Association of Road Racing Statisticians, the Associated Press and the individual media outlets who receive their reports. These abbreviations also appear in Wikipedia.\\r\\n\\r\\n\\r\\nAlmost all races record a time. Evolving since experiments in the 1930s, to their official use at the 1968 Summer Olympics and official acceptance in 1977, fully automatic times have become common. As this evolution has occurred, the rare early times were specified as FAT times. As they are now commonplace, automatic times are now expressed using the hundredths of a second. Hand times (watches operated by human beings) are not regarded as accurate and thus are only accepted to the accuracy of a tenth of a second even when the watch displays greater accuracy. If the mark was set before 1977, a converted time to the tenth was recorded for record purposes, because they did not have a system to compare between the timing methods. Frequently in those cases there is a mark to the 100th retained for that race. Over this period of evolution, some reports show hand times also followed with an \\"h\\" or \\"ht\\" to distinguish hand times.\\r\\nWith two different timing methods came the inevitable desire to compare times. Track and Field News initiated adding .24 to hand times as a conversion factor. Many electronic hand stopwatches display times to the hundredth. Frequently those readings are recorded, but are not accepted as valid (leading to confused results). Some low level meets have even hand timed runners and have switched places according to the time displayed on the stopwatch. All of this is, of course, wrong. Hand times are not accurate enough to be accepted for record purposes for short races. Human reaction time is not perfectly identical between different human beings. Hand times involve human beings reacting, pushing the stopwatch button when they see the smoke or hear the sound of the Starting pistol, then reacting (possibly anticipating) the runner crossing the finish line. The proper procedure for converting hand times would be to round any hundredths up to the next higher even tenth of a second and then add the .24 to get a time for comparison purposes only.[1] But many meets displayed the converted marks accurate to the hundredth making the results look like they were taken with fully automatic timing. In these cases, some meets have displayed a 4 or a 0 in the hundredths column for all races. When detected, reports of these times are followed by a \\"c\\" or ' to indicate converted times.\\r\\nRoad race times are only considered accurate to a full second. To distinguish a full second time with hours, from a minute time with hundredths of a second, colons are used to separate hours from minutes, and minutes from seconds. A period is used to separate seconds from hundredths of a second.\\r\\nTransponder timing is becoming more common. The RFID detection system times the transponder chip, usually located on a runner's shoe as opposed to the official timing of the torso. Accurate to a full second, this is not significant, but in breaking microscopic ties, the data does not correspond to timing rules. Most road races cannot fit all participants onto the start line. Depending on the size of the field, some athletes could be several city blocks away from the start line and in the large crowd, could take minutes to get across the line. Results frequently indicate two times, the \\"gun time\\" would be the official time from the firing of the starting gun, but the mat time shows the time the shoe crossed a sensing mat at the start line to the time the shoe crossed the sensing mat at the finish line.\\r\\nOccasionally, when breaking ties using photo finish, times are displayed to the thousandth of a second. These times to the thousandth are not used for record purposes but times to the thousandth can be used to break ties between adjacent heats. Rules specify if a tie is broken this way, that all heats involved are recorded with the same timing system.\\r\\nMost records are subject to ratification by the governing body for that record. On the world level, that is the IAAF. Each body has their own procedure for ratifying the records: for example, USATF, the governing body for the United States, only ratifies records once a year at their annual meeting at the beginning of December. Until a record is ratified, it is regarded as \\"Pending\\" which is sometimes indicated by a following P\\r\\nWhen a J is added, it indicates a junior record (if a junior does not reach their 20th birthday in the calendar year of the mark)\\r\\nSome records are ratified or tracked, but they are not to the same standard of quality or accuracy as a record. The term is \\"bests.\\" IAAF lists bests for the Youth division and for road racing records such as the marathon. It also tracks athlete personal achievements as bests. A Y indicates Youth. A youth athlete has not reached or will not reach their 18th birthday in the calendar year of competition.\\r\\nFor events where wind assistance is a factor (outdoor races 220 yards or less, Long Jump and Triple Jump), the wind reading is usually reported in metres per second or \\"m/s\\"\\r\\nAthlete disqualifications often reference the IAAF rule number under which the athlete was disqualified.\\r\\nThis is typically written in the format (false start as example): DQ R162.7[2]\\r\\nThe various organizing bodies of the sport are abbreviated into alphabet soup.\\r\\nDue to the large number of athletics events that are regularly contested, presentations of results and statistics often use abbreviations to refer to the events, rather than the full form.","input":"What does c mean in track and field?"},{"output":"United States","context":"The all-time medal table  for all Olympic Games from 1896 to 2018, including Summer Olympic Games, Winter Olympic Games, and a combined total of both, is tabulated below. These Olympic medal counts do not include the 1906 Intercalated Games which are no longer recognized by the International Olympic Committee (IOC) as official Games.\\r\\nThe IOC itself does not publish all-time tables, and publishes unofficial tables only per single Games. This table was thus compiled by adding up single entries from the IOC database.[1]\\r\\n\\r\\nThe results are attributed to the IOC country code as currently displayed by the IOC database. Usually, a single code corresponds to a single National Olympic Committee (NOC). When different codes are displayed for different years, medal counts are combined in the case of a simple change of IOC code (such as from HOL to NED for the Netherlands) or simple change of country name (such as from Ceylon to Sri Lanka). As the medals are attributed to each NOC, not all totals include medals won by athletes from that country for another NOC, such as before independence of that country (see individual footnotes for special cases such as combined teams). Names in italic are national entities that no longer exist. The totals of NOCs are not combined with those of their predecessors and successors.\\r\\n\\r\\nThe table is pre-sorted by the name of each Olympic Committee, but can be displayed as sorted by any other column, such as the total number of gold medals or total number of overall medals. To sort by gold, silver, and then bronze, sort first by the bronze column, then the silver, and then the gold. The table does not include revoked medals (e.g. due to doping).\\r\\n\\r\\nMedal totals in this table are current as of the 2018 Winter Olympics in Pyeongchang County, South Korea, and all changes in medal standings due to doping cases up to and including 25 February 2018 are taken into account.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTotal\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTotal\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTotal\\r\\n\\r\\nThe sum total of gold, silver and bronze medals are not equal for the following reasons:\\r\\n\\r\\nAfter the 2018 Winter Olympics in Pyeongchang, 71 of the current 206 National Olympic Committees have yet to win an Olympic medal. Seven historic National Olympic Committees and the Refugee Olympic Team are also included in this list.\\r\\n\\r\\nThe sections above are based on information published by the International Olympic Committee. Various sources deal with some of the entries in the preceding sections differently.\\r\\n\\r\\nFor the 1900 Summer Olympics several countries are credited with appearances that are not considered official by the IOC. Only one of these cases concerns a medal. A gold medal that is officially added to France's total is given to Luxembourg.[3]\\r\\n\\r\\nSimilar cases of disputed nationality affect the 1904 Summer Olympics. France is credited with a silver medal[4] and Norway with two gold medals.[5] In the table above these medals are listed under the United States.\\r\\nFurthermore, Newfoundland is occasionally listed as a separate country at the 1904 Olympics, and would be listed as a historical NOC without medals.[6]\\r\\n\\r\\nOther differences from the official table are based on disagreements about which events were Olympic. This affects several of the events in the 1900 and 1904 Olympics. In addition, some sources include the 1906 Intercalated Games when compiling their medal tables.[7]\\r\\n\\r\\nFrom 1924 through 1936, the IOC on several occasions awarded gold medals for feats of alpinism and aeronautics that occurred in the preceding four-year Olympiad. In 1924, 1932 and 1936, gold medals were awarded to 25 mountaineers from six nations for expeditions in the Himalayas and the Matterhorn. In 1936, a gold medal was awarded to Hermann Schreiber of Switzerland for crossing the Alps in a glider in 1935, the first time that had been done.[2][8][9][10] Some sources include these IOC awards of gold medals in the overall count.\\r\\n\\r\\nGermany has competed under five different designations, including as two separate teams at several Games. Sources vary in how they present the medals won by these teams. The table below shows sourced combinations of these teams, when applied to the updated medal totals from the main table.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTotal\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTotal\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTotal\\r\\n\\r\\nThe Russian Federation, the Russian Empire and the Olympic Athletes from Russia are often combined outside of IOC sources. The Soviet Union is sometimes combined with the post-union team that competed in 1992. Few sources combine the Soviet Union and Russia as many republics which subsequently gained independence (Armenia, Azerbaijan, Belarus, Estonia, Georgia,  Kazakhstan, Kyrgyzstan, Latvia, Lithuania, Moldova, Tadjikistan, Turkmenistan, Ukraine, Uzbekistan) contributed to the medal tally of the USSR.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTotal\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTotal\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTotal\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nThe nationalities of many medalists at the 1904 Olympics are disputed as many competitors were recent immigrants to the United States who had not yet been granted US citizenship.\\r\\n\\r\\n\\r\\n\\r\\nAll-time medal counts are compiled by various sources, often adding up related results for Russia, Germany, Serbia, Czech Republic etc.\\r\\n\\r\\nSources","input":"What country has won the most summer olympic medals?"},{"output":"American Airlines","context":"\\r\\n\\r\\nCharlotte Douglas International Airport (IATA: CLT, ICAO: KCLT, FAA LID: CLT) is a joint civil-military public international airport located in Charlotte, North Carolina, United States.  Established in 1935 as Charlotte Municipal Airport, in 1954 the airport was renamed Douglas Municipal Airport after former Charlotte mayor Ben Elbert Douglas Sr., who had overseen the airport's construction. The airport gained its current name in 1982 and, as of September 2017, it is the second largest hub for American Airlines after Dallas/Fort Worth International Airport, with service to 161 domestic and international destinations.[4] As of 2016 it was the 11th busiest airport in the United States, ranked by passenger traffic and 6th by aircraft movements. It was also the 7th busiest airport in the world ranked by aircraft movements[5] However, Charlotte is the largest airport in the United States without any nonstop service to Asia, and it only ranks 19th by international passenger traffic. The airport serves as a major gateway to the Caribbean Islands. CLT covers 5,558 acres (2,249 ha) of land.[1]\\r\\n\\r\\nThe city received Works Progress Administration funding to establish Charlotte's first municipal airport; the airport was, at the time, the largest single WPA project, incorporating a terminal, hangar, beacon tower and three runways.[6]\\r\\n\\r\\nIn 1936, the Charlotte Municipal Airport opened, operated by the City of Charlotte; Eastern Air Lines began scheduled passenger service in 1937. The original passenger terminal still exists, and is currently used for offices and training rooms by various Aviation-related organizations.\\r\\n\\r\\nThe United States Army Air Forces took control of the airport and established Charlotte Air Base in early 1941, which was renamed Morris Field shortly following the attack on Pearl Harbor. The US military invested more than $5 million in airfield improvements by the time the facility was returned to the City of Charlotte in 1946.[6] The airfield was used by the Third Air Force for antisubmarine patrols and training.\\r\\n\\r\\nIn 1954, a 70,000-square-foot (6,500?m2) passenger terminal opened and the airport was renamed Douglas Municipal Airport in honor of former Charlotte Mayor Ben Elbert Douglas, Sr., who had overseen the airport's opening 20 years earlier. The terminal had two floors, though passenger operations were confined to the ground floor. Ticketing and baggage claim were on each side of an open space that bisected the building from north to south, and a mezzanine restaurant and airline offices overlooked this open space. Delta Air Lines began scheduled passenger service in 1956. The OAG for April 1957 shows 57 weekday departures on Eastern, 7 Piedmont, 6 Capital, 4 Delta and 2 Southern. Nonstop flights did not reach beyond Newark, Pittsburgh, Columbus, Louisville, Birmingham, and Jacksonville.\\r\\n\\r\\nEastern Air Lines began scheduled jet flights with the Boeing 720 in early 1962.[7] Eastern used the west pier, Piedmont and Delta the center pier, and United and Southern used the east pier.\\r\\n\\r\\nA major renovation project in the late 1960s expanded the facility considerably. Eastern opened a unit terminal in 1967, replacing the old west pier. This new facility had eight dedicated gates for Eastern, each with its own departure lounge, a snack bar and separate baggage claim space. Eastern passengers continued to check in at the main terminal.\\r\\n\\r\\nIn 1969, a new enclosed concourse was built parallel to the center pier. When it was completed, Piedmont, Eastern, and Delta moved in and the old center pier was demolished. The new concourse also had separate departure lounges, as well as restrooms and an enlarged baggage claim area. United's flights continued to use the east pier, with an enclosed holding room added for waiting passengers. In 1973, Eastern added two more gates to the end of its west concourse.\\r\\n\\r\\nAs of April 1975, the airport had 97 weekday departures to 32 destinations on seven airlines.[8][9]\\r\\n\\r\\nAfter airline deregulation in 1978, passenger numbers at the terminal nearly doubled between 1978 and 1980, and a new 10,000-foot (3,000?m) parallel runway and control tower opened in 1979. The airport's master plan called for a new terminal across the runway from the existing site, with ground broken in 1979. At the time, the airport had only two concourses: one used exclusively by Eastern, and one used by other carriers, including United, Delta, Piedmont, and several commuter airlines.[10]\\r\\n\\r\\nIn 1979, Piedmont Airlines chose Charlotte as the hub for its expanding route network. To accommodate booming growth, a new 325,000-square-foot (30,200?m2) passenger terminal designed by Odell Associates opened in 1982, and the airport was renamed Charlotte Douglas International Airport.[11] Concourses B and C were expanded in 1987 and 1984 respectively, while Concourse A was built in 1986 to handle future growth[11]\\r\\n\\r\\nIn 1987 Piedmont started non-stop 767 flights to London. In the mid-1980s the old terminal site was converted to a cargo center, and the central concourse and Eastern unit terminal were removed to make way for more cargo buildings. The original main building still stands and is used for office space. The old control tower was removed in the late 1990s. In 1989 Piedmont merged with USAir; the new merged operations kept the USAir name.\\r\\n\\r\\nIn 1990, a new 80,000-square-foot (7,400?m2) international and commuter concourse (Concourse D) opened, and in 1991 further expansion of the central terminal building continued, reflective of USAir's dominating presence at the airport. A monumental bronze statue of Queen Charlotte of Mecklenburg-Strelitz (the namesake of the city), created by Raymond Kaskey, was placed in front of the main terminal.\\r\\n\\r\\nIn 1990, Lufthansa began Boeing 747 service to Germany; this service was, however, discontinued shortly thereafter. In 1994, British Airways began service to London via a \\"global alliance\\" with USAir. This was later discontinued, as the airlines chose different alliances (though they now are both in Oneworld). Lufthansa restarted service to Charlotte in 2003 and now operates flights between Charlotte and Munich, utilizing Airbus A340-600 and Airbus A330-300 aircraft.\\r\\n\\r\\nIn 1999, plans were announced for the construction of a regional carrier concourse (present-day Concourse E) and for the expansion of Concourses A and D. This expansion was designed by The Wilson Group and LS3P Associates Ltd.[12]\\r\\n\\r\\nIn 2002, the new 32-gate Concourse E opened,[13] and US Airways began non-stop service to Belize, Freeport, Providenciales, Punta Cana, and St. Croix. The airline closed its Concourse D US Airways Club location in 2002.\\r\\n\\r\\nIn 2003, the main ticketing hall was expanded to the east, providing 13 additional ticketing counters and a new security checkpoint; Concourse D was expanded by an additional nine gates. That year, US Airways began service to Costa Rica, Mexico City, and St. Kitts.\\r\\n\\r\\nFollowing the 2005 acquisition of US Airways by America West Airlines in a reverse takeover,[14] Charlotte remained the primary domestic hub for the airline. The majority of US Airways' international routes remained at the airline's second-largest hub, Philadelphia.\\r\\n\\r\\nWith US Airways' acquisition of American Airlines in 2013, Charlotte became a fortress hub for the merged airline.\\r\\n\\r\\nOn July 16, 2013, the North Carolina General Assembly passed a bill, introduced by state Senators Bob Rucho and Bill Brawley in February 2013, transferring possession of the airport to a 13-member regional authority. The bill's sponsors claimed that transferring control to the authority would allow for more efficient operations.[15] Then-Mayor of Charlotte Patsy Kinsey expressed regret for the decision, saying it would throw the airport into \\"chaos and instability.\\"[16] The City of Charlotte was granted a restraining order against the state by Judge Robert Sumner, however, in order to maintain control of the airport. A court date was set for August 1, 2013 to determine the fate of the airport, with former Charlotte mayor Richard Vinroot representing the State as well as the former director of the airport, Jerry Orr.[17] Orr sent a letter to the City after the passage of the bill saying his \\"employment as Executive Director of the Airport Authority commenced and (his) employment by the City as Aviation Director terminated\\", but with the granting of the restraining order, this was interpreted as a resignation by the City and chief financial officer of the airport Brent Cagle was named Acting Director.[18]\\r\\n\\r\\nThe August 1, 2013 court date yielded a verdict that the transfer, should it occur, would need prior approval from the Federal Aviation Administration, a division of the US Department of Transportation, which was headed by former Charlotte Mayor Anthony Foxx, who recused himself from the matter.\\r\\n\\r\\nCharlotte Douglas International Airport remains under the control of the city after a ruling 13 Oct 2014 in Mecklenburg Superior Court.[19] Judge Robert Ervin ruled in the city's favor, saying that the state overlooked the need to get a federal operating certificate before taking control of the airport from the city.[20]\\r\\n\\r\\nIt was later revealed that the drive to transfer control of the airport out of city hands was motivated in part by reports of a federal investigation into public corruption in Charlotte. That investigation ultimately resulted in Mayor Patrick Cannon being arrested (and ultimately pleading guilty) to public corruption charges.[21]\\r\\n\\r\\nBetween 2007 and 2015, the airport completed $1.5 billion worth of construction projects, part of which later became known as the \\"CLT 2015\\" plan. These projects included a new airport entrance roadway, new hourly parking decks with a centralized rental car facility, a regional intermodal cargo facility, an expansion of the east-side terminal lobby, new checked baggage handling systems, and additional space for concessions and shops.[22]\\r\\n\\r\\nConstruction of the airport's fourth runway began in the spring of 2007. At 9,000 feet (2,700?m) long, the new \\"third parallel\\" allows three independent approaches for arrivals even from the south, potentially increasing capacity by 33 percent. The new runway lies west of the three existing runways. The construction of the fourth runway required the relocation of parts of Wallace Neel Road (which had been the Western boundary of the airport) to an alignment located farther to the west. Construction occurred in two phases. The first phase, which began in March 2007, included grading and drainage. The second phase included the paving and lighting of the runway. In August 2009, crews paved the last section.[23]\\r\\n\\r\\nOn the morning of November 20, 2008, runway 18R/36L was renumbered runway 18C/36C in anticipation of the upcoming commissioning of the new third parallel runway, which would carry the 18R/36L designation when opened. The runway opened January 6, 2010. The cost for the runway and taxiways was $325 million, with the federal government paying $124 million and the rest funded by a $3 passenger facility charge.[24] The new runway was initially certified for visual approaches only, but on February 11, 2010, was approved for instrument approaches as well. The runway construction also necessitated rerouting several roads around the airport. Within these plans, a new interchange at the I-485 outerbelt is planned to connect the airport and another relocated road.\\r\\n\\r\\nIn November 2013, the airport released plans for the largest expansion in its history. Part of these plans were integrated into the \\"CLT 2015\\" project bundle, and others would later be known as the \\"Destination CLT\\" plan. This will improve multiple concourses, add an additional food court and multiple new parking decks. The project aimed to aid the airport in coping with the pronounced increase of passengers at the airport in recent years. The plans were to expand the terminal lobby to the north, construct a fifth runway, and at the time included plans for a new international terminal. The new runway is going to be built in between the existing runways 18R/36L and 18C/36C, and at 12,000?ft, the new runway will be Charlotte's longest to date. Officials hope to begin construction on the runway in 2020.[25][26]\\r\\n\\r\\nOn September 28, 2010, construction began on a 60,000-square-foot (5,574 m2) expansion to the eastern side of the existing terminal lobby. The first phase of the terminal expansion officially opened on June 29, 2012 connecting the terminal lobby directly to Concourse E, adding a fifth security checkpoint, and additional lobby and baggage recheck areas. The second phase completed in March 2013 added offices for US Customs and Border Protection and TSA, and expanded space for arrival baggage claim. Upon completion, CLT now has 5 security checkpoints and 20 total security screening lanes.[22][27]\\r\\n\\r\\nBeginning in November 2013, the airport began studying expansions to the airfield and terminals. The results of these studies, along with other planning by the Aviation Department, resulted in the \\"Destination CLT\\" plan and an updated Airport Area Plan that will take the airport through 2035.[1] These plans incorporated the projects that were not yet completed in \\"CLT 2015\\", and added an overall vision to the planned growth of the airport. These plans represent a total planned $2.5 billion investment. Planned expansions include new terminal-side roadway and entrance ramps, expanding the remaining terminal lobby, adding gates to Concourses A, B, C, and E, erecting a new food court, remodeling Concourses A, B, and C, and building the long-anticipated Fourth Parallel Runway.[28]\\r\\n\\r\\nOn May 4, 2015, airport officials formally announced the completion of the \\"CLT 2015\\" plan, and kicked off construction under the \\"Destination CLT\\" plan.[25] The first project to begin was a new terminal-side roadway. The new roadway will have two levels with a total of 16 lanes, 8 on each level for arrivals and departures respectively. The roadway is also being built further away from the existing terminal lobby, to allow for another project that will expand the lobby area northward. Pedestrian tunnels will be built to connect Level 1 of the Hourly Parking and Rental Car Deck, to the future terminal expansion. The roadway is expected to complete in 2018.\\r\\n\\r\\nThe airport broke ground on Phase I of a new Concourse A North on February 29, 2016. This concourse was initially envisioned as a separate satellite terminal for international flights, but the airport later found additional domestic gates were needed more than international capacity.[29] This first phase is estimated to open in Spring 2018 and includes tearing down the old individual rental car buildings, closing the overflow Cell Phone Parking Lot north of the Terminal A ramp, expanding the ramp and taxiways, and building 9 new gates. The second phase will add 16 more gates to the concourse, predicted to open in 2022. Both expansions are estimated to total $500,000,000, financed by airport revenue bonds and future passenger facility charges.[26]\\r\\n\\r\\nWith the original part of the current terminal built in 1982, and officials felt the terminal needed refreshed. A $55,000,000 remodeling project began in Fall 2016, starting with Concourse A, scheduled to continue then to B, and lastly C. The airport plans to remove carpeting and replace it with terrazzo, install new wall finishes, new LED lighting and other cosmetic improvements. The renovation project is anticipated to be completed in 2019. Later plans will add 10-12 gates to Concourse C by 2024, and 8-10 gates to Concourse B by 2026.[28]\\r\\n\\r\\nTerminal Eastside Expansion Phase II began in 2016 and intends to add 51,000 square feet (4,738 m2) across 3 floors of space to the airport terminal between Concourses D and E. The project will add another food court to the secure side of the terminal, expand concession areas in the pre-security area, improve passenger movement between Concourse E to the rest of the airport, add a Mother's Room, and 12,000 square feet (1,114 m2) of future office space. It is scheduled to be complete in the Summer of 2018.[28]\\r\\n\\r\\nThe parking options at Charlotte Douglas have improved in recent years. There have been two new Daily Parking decks erected since 2005, providing almost 6,000 additional parking spaces for the traveling public. There are also four Long Term lots, with Long Term 1 and 2, the main parking lots, contributing a combined 6,500 spaces. In addition, there is the Daily North lot (formerly Remote), which is between the Daily and Long Term lots, with about 1,500 spaces. A new $40 million Business Valet Parking Deck, which utilizes Post Tension Concrete for each massive 250,000-square-foot (23,000?m2) level, has opened. The airport has aligned with a customer service program called SmartPark, which allows customers to call a 24-hour hotline to receive updates on parking conditions.  The airport also has valet parking that provides vehicle washing, detailing, and even paintless dent removal services for an additional charge.\\r\\n\\r\\nIn November 2014, the airport opened its new terminal-adjacent hourly parking deck. This deck provides 4,000 public parking spaces on levels 4 through 7. The 3,000 spaces on levels 1 through 3 are dedicated to rental cars.[30] The opening of this new deck allows for the future redevelopment of the former rental car lots with a new concourse.\\r\\n\\r\\nAll lots except the hourly deck can only be reached from the terminal via shuttle. Business Valet picks up outside Concourse D on the Departures level. All other shuttles pick up on the Arrivals level in the B zone and D zone.  The Daily decks have a shuttle that makes one stop for both decks. Long Term 1 shares with Daily North and makes stops at lettered bus stops. Long Term 2 and 3 share a shuttle and also make stops at lettered bus stops.\\r\\n\\r\\nIn July 2012, an offsite parking lot called Park n Go opened on Scott Futrell Drive.\\r\\n\\r\\nOnsite parking costs are as follows:[31]\\r\\n\\r\\nIn April 2018, The Parking Spot opened a new, energy-efficient airport parking facility adjacent to Charlotte Douglas International Airport. This new location is located less than a mile from CLT at 6210 Wilkinson Blvd. It provides shuttle service to and from the airport every 5-7 minutes and offers both covered and uncovered parking.[33]\\r\\n\\r\\nAs of July 2018, The Parking Spot was offering uncovered parking for only $5.50/day when you make a reservation.[34]\\r\\n\\r\\nCharlotte Douglas International Airport is one of the few airports in the United States with a public viewing area. Here, visitors can watch planes take off, land, and taxi to and from runway 18C/36C. It is credited with having one of the best airport views in the United States.\\r\\n\\r\\nCharlotte Douglas International Airport is one of a small number of major \\"hub\\" airports in the world that has an aviation museum located on the field. The museum, established in 1992, has a collection of over 50 aircraft, including a DC-3 that is painted in Piedmont Airlines livery. The museum also has an aviation library with over 9,000 volumes and a very extensive photography collection. Rare aircraft in the collection include one of only two surviving Douglas D-558 Skystreak aircraft and the second (and oldest surviving) U.S.-built Harrier, which was used as the flight-test aircraft and accumulated over 5,000 flight-test hours.\\r\\n\\r\\nIn January 2011, the museum acquired N106US, the US Airways Airbus A320 ditched by captain Chesley Sullenberger as US Airways Flight 1549 in the Hudson River on January 15, 2009. This aircraft, which was delivered on June 10, 2011, is about 35 years younger than any other commercial airliner on display in a museum.\\r\\n\\r\\nConcourse A North\\r\\n\\r\\nAdmirals Club:[36] Concourses B and C/D connector\\r\\nUSO Lounge: Atrium\\r\\n\\r\\nThere was an additional US Airways Club located in Concourse D, which was closed due to US Airways costcutting.\\r\\nBritish Airways also operated a lounge in the Main Atrium, which became a USO Lounge after they canceled service to Charlotte.\\r\\n\\r\\nCATS' Sprinter Enhanced Bus Service connects the airport to the uptown Charlotte Transportation Center (this route used to be known as the \\"Route 5-Airport\\"). It arrives and departs in front of Zone D Baggage Claim in the commercial lanes, and is easily identifiable by its green livery and \\"Sprinter\\" decals.\\r\\n\\r\\nThe service is operated from the airport every 20 minutes MondayÿFriday from 5:50am to 7:00pm; after 7:00pm, service is offered every 30 minutes until 12:02am. On Saturday and Sunday, Sprinter operates from the airport every hour from 6:00am to 8:00am, every half-hour from 8:00am to 9:00pm, and every hour from 9:00pm to 1:00am. The trip time from the airport to downtown is approximately 20 minutes (depending on traffic conditions), with one-way fares at $2.20, the same as all local routes in the CATS system.[64] View the Sprinter Schedule for more detailed schedule and route information (click the link and choose 'Route 5-Airport').\\r\\n\\r\\nThe following companies operate from within the Rental Car Facility: Advantage, Alamo, Avis, Budget, Dollar, Enterprise, Hertz and National.[65]\\r\\n\\r\\n As a joint civil-military facility, the airport is home to Charlotte Air National Guard Base (Charlotte ANGB) and its host unit, the 145th Airlift Wing (145 AW) of the North Carolina Air National Guard, located in a military cantonment area on the east side of the airport. As an Air National Guard organization within the U.S. Air Force, the federal mission of the 145 AW is theater airlift and it is operationally gained by the Air Mobility Command (AMC). The 145 AW is composed of over 300 full-time and over 1000 traditional part-time military personnel, operating and maintaining C-130 Hercules aircraft in support of combatant commanders worldwide or as otherwise directed by higher authority. Its state mission is to respond to requirements, typically of a humanitarian or disaster-relief nature, as identified by the Governor of North Carolina.[66] The 145 AW's C-130H aircraft can also be equipped with the Modular Airborne Fire Fighting System (MAFFS), making them able to discharge large quantities of Phos-Chek, a water-based fire retardant slurry, at low altitude. In this capacity, the 145 AW is one of a select group of Air National Guard and Air Force Reserve Command C-130 units that, under the direction of U.S. Northern Command (USNORTHCOM), can deploy and provide military support to civilian authorities across the United States in combatting wild fires and forest fires.\\r\\n\\r\\nCharlotte ANGB also maintains a USAF Aircraft Rescue and Firefighting (ARFF) unit, emergency vehicles, and associated crash station/fire station on the installation, providing the airport with an additional crash/fire/rescue (CFR) capability that can augment the airport's own civilian ARFF organization.\\r\\n\\r\\nCharlotte Douglas International Airport is also home to the USO of NC (United Service Organization of North Carolina) Travel Center, which functions as an airport lounge for military personnel (including veterans) and their families. Staffed by volunteers, the centers offer comfortable chairs, books, magazines, television, movies, video games, play areas for children, and refreshments. Internet and phone use is available free of charge.","input":"What airline has its hub in charlotte nc?"},{"output":"16 September 2003","context":"","input":"When did the channel tunnel rail link open?"},{"output":"president pro tempore","context":"Seniority in the United States Senate is valuable as it confers a number of benefits and is based on length of continuous service, with ties broken by a series of factors. Customarily, the terms \\"senior senator\\" and \\"junior senator\\" are used to distinguish the two senators representing a particular state.\\r\\n\\r\\n\\r\\nThe United States Constitution does not mandate differences in rights or power, but Senate rules give more power to senators with more seniority. Generally, senior senators will have more power, especially within their own caucuses. In addition, by custom, senior senators from the president's party control federal patronage appointments in their states.\\r\\nThe president pro tempore of the Senate is traditionally the most senior member of the majority party.\\r\\nThere are several benefits, including the following:\\r\\nThe beginning of an appointment does not necessarily coincide with the date the Senate convenes or when the new Senator is sworn in.[citation needed] In the case of Senators first elected in a general election for the upcoming Congress, their terms begin on the first day of the new Congress. Since 1935, that means January 3 of odd-numbered years. The seniority date for an appointed senator is usually the date of the appointment, although the actual term does not begin until they take the oath of office. An incoming Senator who holds another office, including membership in the U.S. House of Representatives, must resign from that office before becoming a Senator.\\r\\nA senator's seniority is primarily determined by length of continuous service; for example, a senator who has served for 12 years is more senior than one who has served for 10 years. Because several new senators usually join at the beginning of a new Congress, seniority is determined by prior federal or state government service and, if necessary, the amount of time spent in the tiebreaking office. These tiebreakers in order are:[1]\\r\\nWhen more than one senator has served in the same previous role, length of time in that prior office is used to break the tie. For instance, Ben Cardin, Bernie Sanders, Sherrod Brown, Bob Casey, Bob Corker, Claire McCaskill, Amy Klobuchar, Sheldon Whitehouse and Jon Tester took office on January 3, 2007, and the first three senators mentioned had previously served in the House of Representatives. Cardin, having served 20 years, is more senior than Sanders, who served 16 years, who in turn is more senior than Brown who served 14 years. Casey, Corker, McCaskill, Klobuchar, Whitehouse, and Tester rank in that order because as of the 2000 census, Pennsylvania, Tennessee, Missouri, Minnesota, Rhode Island, and Montana had populations that ranked in that order, so Tester was ranked 100th in seniority when the 110th Congress convened.\\r\\nOnly relevant factors are listed below. For senators whose seniority is based on their state's respective population, the state population ranking is given as determined by the relevant United States Census current at the time that they first took their seat.[2][3][4][5]\\r\\n?? Republican R (51) ??? ?? Democratic D (47) ??? ?? Independent I (2)\\r\\n1 (1789)\\r\\n2 (1791)\\r\\n3 (1793)\\r\\n4 (1795)\\r\\n5 (1797)\\r\\n6 (1799)\\r\\n7 (1801)\\r\\n8 (1803)\\r\\n9 (1805)\\r\\n10 (1807)\\r\\n11 (1809)\\r\\n12 (1811)\\r\\n13 (1813)\\r\\n14 (1815)\\r\\n15 (1817)\\r\\n16 (1819)\\r\\n17 (1821)\\r\\n18 (1823)\\r\\n19 (1825)\\r\\n20 (1827)\\r\\n21 (1829)\\r\\n22 (1831)\\r\\n23 (1833)\\r\\n24 (1835)\\r\\n25 (1837)\\r\\n26 (1839)\\r\\n27 (1841)\\r\\n28 (1843)\\r\\n29 (1845)\\r\\n30 (1847)\\r\\n31 (1849)\\r\\n32 (1851)\\r\\n33 (1853)\\r\\n34 (1855)\\r\\n35 (1857)\\r\\n36 (1859)\\r\\n37 (1861)\\r\\n38 (1863)\\r\\n39 (1865)\\r\\n40 (1867)\\r\\n41 (1869)\\r\\n42 (1871)\\r\\n43 (1873)\\r\\n44 (1875)\\r\\n45 (1877)\\r\\n46 (1879)\\r\\n47 (1881)\\r\\n48 (1883)\\r\\n49 (1885)\\r\\n50 (1887)\\r\\n51 (1889)\\r\\n52 (1891)\\r\\n53 (1893)\\r\\n54 (1895)\\r\\n55 (1897)\\r\\n56 (1899)\\r\\n57 (1901)\\r\\n58 (1903)\\r\\n59 (1905)\\r\\n60 (1907)\\r\\n61 (1909)\\r\\n62 (1911)\\r\\n63 (1913)\\r\\n64 (1915)\\r\\n65 (1917)\\r\\n66 (1919)\\r\\n67 (1921)\\r\\n68 (1923)\\r\\n69 (1925)\\r\\n70 (1927)\\r\\n71 (1929)\\r\\n72 (1931)\\r\\n73 (1933)\\r\\n74 (1935)\\r\\n75 (1937)\\r\\n76 (1939)\\r\\n77 (1941)\\r\\n78 (1943)\\r\\n79 (1945)\\r\\n80 (1947)\\r\\n81 (1949)\\r\\n82 (1951)\\r\\n83 (1953)\\r\\n84 (1955)\\r\\n85 (1957)\\r\\n86 (1959)\\r\\n87 (1961)\\r\\n88 (1963)\\r\\n89 (1965)\\r\\n90 (1967)\\r\\n91 (1969)\\r\\n92 (1971)\\r\\n93 (1973)\\r\\n94 (1975)\\r\\n95 (1977)\\r\\n96 (1979)\\r\\n97 (1981)\\r\\n98 (1983)\\r\\n99 (1985)\\r\\n100 (1987)\\r\\n101 (1989)\\r\\n102 (1991)\\r\\n103 (1993)\\r\\n104 (1995)\\r\\n105 (1997)\\r\\n106 (1999)\\r\\n107 (2001)\\r\\n108 (2003)\\r\\n109 (2005)\\r\\n110 (2007)\\r\\n111 (2009)\\r\\n112 (2011)\\r\\n113 (2013)\\r\\n114 (2015)\\r\\n115 (2017)\\r\\n\\r\\nCurrent","input":"Who is the highest ranking member of the senate?"},{"output":"called for the abolition of national banks","context":"The People's Party, also known as the Populist Party or the Populists, was an agrarian-populist political party in the United States. For a few years, from 1892 to 1896, it played a major role as a left-wing force in American politics. It was merged into the Democratic Party in 1896; a small independent remnant survived until 1908. It drew support from angry farmers in the West and South. It was highly critical of banks and railroads, and allied itself with the labor movement.[1][2][3]\\r\\nEstablished in 1891, as a result of the Populist movement, the People's Party reached its peak in the 1892 presidential election, when its ticket, composed of James B. Weaver and James G. Field, won 8.5% of the popular vote and carried five states (Colorado, Idaho, Kansas, Nevada and North Dakota), and the 1894 House of Representatives elections, when it took over 10% of the vote. Built on a coalition of poor, white cotton farmers in the South (especially North Carolina, Alabama and Texas) and hard-pressed wheat farmers in the Plains states (especially Kansas and Nebraska), the Populists represented a radical combination of agrarianism and urbanism with hostility to banks, landowners, Eastern elites, railroads, and the gold standard.[4]\\r\\nThough Henry George refused to campaign for the Populist Party, many of his supporters did, which created a contest for power between Georgists and socialists in states such as Illinois, where Clarence Darrow lobbied for the Georgist 'single-taxers'. Another major Georgist figure in the People's Party was Congressman \\"Sockless Jerry\\" Simpson from Kansas. The Texas Farmers' Alliance and the Texas People's Party both adopted Georgist planks in their platforms.[5] In more urban states such as New York, the Georgist wing reportedly \\"practically dominated\\" the People's Party.[6] Other state branches of the People's Party adopted less radical land tax planks in their platforms.[7]\\r\\nThe party sometimes allied with labor unions in the North and Republicans in the South. In the 1896 presidential elections the Populists endorsed the Democratic presidential nominee, William Jennings Bryan, adding their own vice presidential nominee. By joining with the Democrats, the People's Party lost its independent identity and rapidly withered away.\\r\\nLater, after the dissolution of the party, the term populist acquired a generic meaning and throughout most of the 20th century and into the 21st, the term means, \\"a believer in the rights, wisdom, or virtues of the common people.\\"[8][9]\\r\\n\\r\\n\\r\\nA People's Party grew out of a large mood of agrarian unrest in response to low agricultural prices in the South and the trans-Mississippi West, as well as thought that the \\"Eastern Elites\\" were taking advantage of the farmers by charging higher rates on loans and trains.[10] The Farmers' Alliance, formed in Lampasas, Texas, in 1876, promoted collective economic action by farmers and achieved widespread popularity in the South and Great Plains. The Farmers' Alliance ultimately did not achieve its wider economic goals of collective economic action against brokers, railroads, and merchants, and many in the movement advocated for changes in national policy. By the late 1880s, the Alliance had developed a political agenda that called for regulation and reform in national politics, most notably an opposition to the gold standard to counter the high deflation in agricultural prices in relation to other goods such as farm implements.\\r\\nIn 1886, an entirely different \\"People's Party\\" elected 6 assemblymen to the Wisconsin State Assembly and 1 senator to the Wisconsin State Senate. However this was a labor party, and by the 1888 elections it was using the Union Labor Party label.\\r\\nIn December 1888 the National Agricultural Wheel and the Southern Farmers Alliance met at Meridian, Mississippi, where the national farmers convention was held that year. In that meeting they decided to consolidate the two parties pending ratification. This consolidation gave the organization a new name, the Farmers and Laborers Union of America, and by 1889 the merger had been ratified, although there were conflicts between conservative Alliance men and political Wheelers in Texas and Arkansas, which delayed the unification in these states until 1890 and 1891 respectively. The merger eventually united white Southern Alliance and Wheel members, but it would not include African American members of agricultural organizations.[11]\\r\\nDuring their move towards consolidation in 1889, the leaders of both Southern Farmers Alliance and the Agricultural Wheel organizations contacted Terence V. Powderly, leader of the Knights of Labor. This contact between leaders of the farmers movement and Powderly helped pave the way for a series of reform conferences held between December 1889 and July 1892 that resulted in the formation of the national Peoples (or Populist) Party.[12]\\r\\nThe drive to create a new political party out of the movement arose from the belief that the two major parties, Democrats and Republicans, were controlled by bankers, landowners and elites hostile to the needs of the small farmer. The movement reached its peak in 1892 when the party held a convention chaired by Frances Willard (leader of the WCTU and a friend of Powderly's)[13] in Omaha, Nebraska and nominated candidates for the national election.\\r\\nThe party's platform, commonly known as the Omaha Platform, called for the abolition of national banks, a graduated income tax, direct election of Senators, civil service reform, a working day of eight hours and Government control of all railroads, telegraphs, and telephones. In the 1892 Presidential election, James B. Weaver received 1,027,329 votes. Weaver carried four states (Colorado, Kansas, Idaho, and Nevada) and received electoral votes from Oregon and North Dakota as well.\\r\\nThe party flourished most among farmers in the Southwest and Great Plains, as well as making significant gains in the South, where they faced an uphill battle given the firmly entrenched monopoly of the Democratic Party. Success was often obtained through electoral fusion, with the Democrats outside the South, but with alliances with the Republicans in Southern states like Alabama, North Carolina, Tennessee, and Texas.[14] For example, in the elections of 1894, a coalition of Populists and Republicans led by Populist Marion Butler swept state and local offices in North Carolina, and the coalition would go on to elect Republican Daniel Lindsay Russell as Governor in 1896.[15]\\r\\nQuite separate from the Populists were the Silverites in the western mining states, who demanded Free silver to solve the Panic of 1893. By allowing the coining of silver coins, they hoped to make the value of the money more than what it represented, which would lead to inflation of the currency, and thus, reduce the debt of the farmers to the Eastern Elites. This idea led former Greenback Party members to join the Populist Party.\\r\\nThe Populists followed the Prohibition Party in actively including women in their affairs. Some southern Populists, including Thomas E. Watson of Georgia, openly talked of the need for poor blacks and poor whites to set aside their racial differences in the name of shared economic self-interest. Regardless of these rhetoric appeals, however, racism did not evade the People's Party. Prominent Populist Party leaders such as Marion Butler, a United States Senator from North Carolina, at least partially demonstrated a dedication to the cause of white supremacy, and there appears to have been some support for this viewpoint among the rank-and-file of the party's membership.[16] After 1900 Watson himself became an outspoken white supremacist and became the party's presidential nominee in 1904 and 1908, winning 117,000 and 29,000 votes.\\r\\nBy 1896, the Democratic Party took up many of the People's Party's causes at the national level, and the party began to fade from national prominence. In that year's presidential election, the Democrats nominated William Jennings Bryan, who focused on the free silver issue as a solution to the economic depression and the maldistribution of power. One of the great orators of the day, Bryan generated enormous excitement among Democrats with his \\"Cross of Gold\\" speech, and appeared in the middle of 1896 to have a good chance of winning the election, if the Populists voted for him.\\r\\nThe Populists had the choice of endorsing Bryan or running their own candidate. After great infighting at their St. Louis convention they decided to endorse Bryan but with their own vice presidential nominee, Thomas E. Watson of Georgia. Watson was cautiously open to cooperation, but after the election would recant any hope he had in the possibility of cooperation as a viable tool.[17] Bryan's strength was based on the traditional Democratic vote (minus the middle class and German Catholics); he swept the old Populist strongholds in the west and South, and added the silverite states in the west, but did poorly in the industrial heartland. He lost to Republican William McKinley by a margin of 600,000 votes and lost again in a rematch in 1900 by a larger margin. Historians believe this was because of the tactics Bryan used, which had not been used before; he had aggressively \\"run\\" for president, while traditional candidates would use \\"front porch campaigns.\\" [18]\\r\\nIn 1894-96 the Populist wave of agrarian unrest swept through the cotton and tobacco regions of the South. The most dramatic impact came in North Carolina, where the poor white farmers who comprised the Populist party formed a working coalition with the Republican Party, then largely controlled by blacks in the low country, and poor whites in the mountain districts. They took control of the state legislature in both 1894 and 1896, and the governorship in 1896. Restrictive rules on voting were repealed. In 1895 the Legislature rewarded its black allies with patronage, naming 300 black magistrates in eastern districts, as well as deputy sheriffs and city policemen. They also received some federal patronage from the coalition congressman, and state patronage from the governor.[19]\\r\\nThe Populist movement never recovered from the failure of 1896, and national fusion with the Democrats proved disastrous to the party in the South. National alliance with the Democrats sapped the ability of the Populists to fight the Democrats locally in the South. Early on, this was less of an issue in the Western states where Republicans were strong, as the Democratic-Populist alliance was a more natural fit there, but eventually ended the party.\\r\\nIn North Carolina, the state Democratic-party orchestrated propaganda campaign in newspapers across the state, and created a brutal and violent white supremacy election campaign to defeat the North Carolina Populists and GOP, the Fusionist revolt in North Carolina collapsed in 1898, and white Democrats returned to power. The gravity of the crisis was underscored by a major race riot in Wilmington, in 1898, two days after the election. Knowing they had just retaken control of the state legislature, the Democrats were confident they could not be overcome. They attacked and overcame the Fusionists; mobs roamed the black neighborhoods, shooting, killing, burning buildings, and making a special target of the black newspaper.[20] There were no further insurgencies in any Southern states involving a successful black coalition at the state level. By 1900, the gains of the populist-Republican coalition were reversed, and the Democrats ushered in disfranchisement[21]: practically all blacks lost their vote. The Populist/Republican alliance which had governed North Carolina, the only state in which it had any success, fell apart.\\r\\nTennessees Populist Party was demoralized by a diminishing membership, and puzzled and split by the dilemma of whether to fight the state-level enemy (the Democrats) or the national foe (the Republicans and Wall Street). By 1900 the Peoples Party of Tennessee was a shadow of what it once was.[22] A similar pattern was repeated elsewhere throughout the South, where the Populist Party had previously sought alliances with the Republican Party against the dominant state Democrats, including in Watson's Georgia.\\r\\nIn 1900, while many Populist voters supported Bryan again, the weakened party nominated a separate ticket of Wharton Barker and Ignatius L. Donnelly, and disbanded afterwards. Populist activists either retired from politics, joined a major party, or followed Eugene Debs into his new Socialist Party. In 1904, the party was re-organized, and Thomas E. Watson was their nominee for president in 1904 and in 1908, after which the party disbanded again.\\r\\nIn A Preface to Politics, published in 1913, Walter Lippmann wrote, \\"As I write, a convention of the Populist Party has just taken place. Eight delegates attended the meeting, which was held in a parlor.\\"[23] This may record the last gasp of the party organization.\\r\\nSince the 1890s historians have vigorously debated the nature of Populism; most scholars have been liberals[citation needed] who admired the Populists for their attacks on banks and railroads. Some historians see a close link between the Populists of the 1890s and the progressives of 1900-1912, but most of the leading progressives (except Bryan himself) fiercely opposed Populism. For example, Theodore Roosevelt, George W. Norris, Robert La Follette Sr., William Allen White and Woodrow Wilson all strongly opposed Populism. It is debated whether any Populist ideas made their way into the Democratic party during the New Deal era. The New Deal farm programs were designed by experts (like Henry Wallace) who had nothing to do with Populism.[24]\\r\\nSome historians see the populists as forward-looking liberal reformers. Others view them as reactionaries trying to recapture an idyllic and utopian past. For some they were radicals out to restructure American life, and for others they were economically hard-pressed agrarians seeking government relief. Much recent scholarship emphasizes Populism's debt to early American republicanism.[25] Clanton (1991) stresses that Populism was \\"the last significant expression of an old radical tradition that derived from Enlightenment sources that had been filtered through a political tradition that bore the distinct imprint of Jeffersonian, Jacksonian, and Lincolnian democracy.\\" This tradition emphasized human rights over the cash nexus of the Gilded Age's dominant ideology.[26]\\r\\nFrederick Jackson Turner and a succession of western historians depicted the Populist as responding to the closure of the frontier. Turner explained:\\r\\nThe most influential Turner student of Populism was John D. Hicks, who emphasized economic pragmatism over ideals, presenting Populism as interest group politics, with have-nots demanding their fair share of America's wealth which was being leeched off by nonproductive speculators. Hicks emphasized the drought that ruined so many Kansas farmers, but also pointed to financial manipulations, deflation in prices caused by the gold standard, high interest rates, mortgage foreclosures, and high railroad rates. Corruption accounted for such outrages and Populists presented popular control of government as the solution, a point that later students of republicanism emphasized.[28]\\r\\nIn the 1930s C. Vann Woodward stressed the southern base, seeing the possibility of a black-and-white coalition of poor against the overbearing rich. Georgia politician Tom Watson served as Woodward's hero.[29] In the 1950s, however, scholars such as Richard Hofstadter portrayed the Populist movement as an irrational response of backward-looking farmers to the challenges of modernity. He discounted third party links to Progressivism and argued that Populists were provincial, conspiracy-minded, and had a tendency toward scapegoatism that manifested itself as nativism, anti-Semitism, anti-intellectualism, and Anglophobia. The antithesis of anti-modern Populism was modernizing Progressivism according to Hofstadter's model, with such leading progressives as Theodore Roosevelt, Robert La Follette Sr., George Norris and Woodrow Wilson pointed as having been vehement enemies of Populism, though William Jennings Bryan did cooperate with them and accepted the Populist nomination in 1896.[30]\\r\\nMichael Kazin's The Populist Persuasion (1995) argued that Populism reflected a rhetorical style that manifested itself in spokesmen like Father Charles Coughlin in the 1930s and Governor George Wallace in the 1960s.\\r\\nGoodwyn (1976)[31] and Postel (2007) reject the notion that the Populists were traditionalistic and anti-modern. Quite the reverse, they argue, the Populists aggressively sought self-consciously progressive goals. Goodwyn criticizes Hofstadters reliance on secondary sources to characterize the Populists, working instead with the material generated by the Populists themselves. Goodwyn determined that the farmers cooperatives gave rise to a Populist culture, and their efforts to free farmers from lien merchants revealed to them the political structure of the economy, which propelled them into politics. The Populists sought diffusion of scientific and technical knowledge, formed highly centralized organizations, launched large-scale incorporated businesses, and pressed for an array of state-centered reforms. Hundreds of thousands of women committed to Populism seeking a more modern life, education, and employment in schools and offices. A large section of the labor movement looked to Populism for answers, forging a political coalition with farmers that gave impetus to the regulatory state. Progress, however, was also menacing and inhumane, Postel notes. White Populists embraced social-Darwinist notions of racial improvement, Chinese exclusion and separate-but-equal.[32]\\r\\nPopulists saw the Panic of 1893 as confirmation that evil global conspiracies and big city villains were to blame. Historian Hasia Diner says:\\r\\nApproximately forty-five members of the party served in the U.S. Congress between 1891 and 1902. These included six United States Senators:\\r\\nThe following were Populist members of the U.S. House of Representatives:\\r\\n52nd United States Congress\\r\\n53rd United States Congress\\r\\n54th United States Congress\\r\\n55th United States Congress\\r\\n56th United States Congress\\r\\n57th United States Congress","input":"What were the planks of the peoples party?"},{"output":"eight","context":"B vitamins are a class of water-soluble vitamins that play important roles in cell metabolism. Though these vitamins share similar names, they are chemically distinct that often coexist in the same foods. In general, dietary supplements containing all eight are referred to as a vitamin B complex. Individual B vitamin supplements are referred to by the specific number or name of each vitamin: B1 = thiamine, B2 = riboflavin, B3 = niacin, etc. Some are better known by name than number: niacin, pantothenic acid, biotin and folate.\\r\\nEach B vitamin is either a cofactor (generally a coenzyme) for key metabolic processes or is a precursor needed to make one.\\r\\n\\r\\n\\r\\nNote: other substances once thought to be vitamins were given numbers in the B-vitamin numbering scheme, but were subsequently discovered to be either not essential for life or manufactured by the body, thus not meeting the two essential qualifiers for a vitamin. See section #Related compounds for numbers 4, 8, 10, 11, and others.\\r\\nNAD carries hydrogens and their electrons during metabolic reactions, including the pathway from the citric acid cycle to the electron transport chain. NADP is a coenzyme in lipid and nucleic acid synthesis.[4]\\r\\nSeveral named vitamin deficiency diseases may result from the lack of sufficient B vitamins. Deficiencies of other B vitamins result in symptoms that are not part of a named deficiency disease.\\r\\n\\r\\nBecause water-soluble B vitamins are eliminated in the urine, taking large doses of certain B vitamins usually only produces transient side-effects. General side effects may include restlessness, nausea and insomnia. These side-effects are almost always caused by dietary supplements and not foodstuffs.\\r\\nB vitamins are found in whole unprocessed foods. Processed carbohydrates such as sugar and white flour tend to have lower B vitamin than their unprocessed counterparts. For this reason, it is required by law in many countries (including the United States) that the B vitamins thiamine, riboflavin, niacin, and folic acid be added back to white flour after processing. This is sometimes called \\"Enriched Flour\\" on food labels. B vitamins are particularly concentrated in meat such as turkey, tuna and liver.[18] Good sources for B vitamins include legumes (pulses or beans), whole grains, potatoes, bananas, chili peppers, tempeh, nutritional yeast, brewer's yeast, and molasses. Although the yeast used to make beer results in beers being a source of B vitamins,[19] their bioavailability ranges from poor to negative as drinking ethanol inhibits absorption of thiamine (B1),[20][21] riboflavin (B2),[22] niacin (B3),[23] biotin (B7),[24] and folic acid (B9).[25][26] In addition, each of the preceding studies further emphasizes that elevated consumption of beer and other alcoholic beverages results in a net deficit of those B vitamins and the health risks associated with such deficiencies.\\r\\nThe B12 vitamin is not abundantly available from plant products, making B12 deficiency a legitimate concern for vegans.[citation needed] Manufacturers of plant-based foods will sometimes report B12 content, leading to confusion about what sources yield B12. The confusion arises because the standard US Pharmacopeia (USP) method for measuring the B12 content does not measure the B12 directly. Instead, it measures a bacterial response to the food. Chemical variants of the B12 vitamin found in plant sources are active for bacteria, but cannot be used by the human body. This same phenomenon can cause significant over-reporting of B12 content in other types of foods as well.[27]\\r\\nA popular way of increasing one's vitamin B intake is through the use of dietary supplements. B vitamins are commonly added to energy drinks, many of which have been marketed with large amounts of B vitamins[28] with claims that this will cause the consumer to \\"sail through your day without feeling jittery or tense.\\"[28] Some nutritionists have been critical of these claims, pointing out for instance that while B vitamins do \\"help unlock the energy in foods,\\" most Americans acquire the necessary amounts easily in their diets.[28]\\r\\nBecause they are soluble in water, excess B vitamins are generally readily excreted, although individual absorption, use and metabolism may vary\\"[28] The elderly and athletes may need to supplement their intake of B12 and other B vitamins due to problems in absorption and increased needs for energy production.[medical citation needed] In cases of severe deficiency, B vitamins, especially B12, may also be delivered by injection to reverse deficiencies.[29][unreliable medical source?] Both type 1 and type 2 diabetics may also be advised to supplement thiamine based on high prevalence of low plasma thiamine concentration and increased thiamine clearance associated with diabetes.[30] Also, Vitamin B9 (folic acid) deficiency in early embryo development has been linked to neural tube defects. Thus, women planning to become pregnant are usually encouraged to increase daily dietary folic acid intake and/or take a supplement.[31]\\r\\nMany of the following substances have been referred to as vitamins as they were once believed to be vitamins. They are no longer considered as such, and the numbers that were assigned to them now form the \\"gaps\\" in the true series of B-complex vitamins described above (e.g., there is no vitamin B4). Some of them, though not essential to humans, are essential in the diets of other organisms; others have no known nutritional value and may even be toxic under certain conditions.","input":"How many types of vitamin b do we have?"},{"output":"Russell M. Nelson","context":"","input":"Who is the oldest apostle in the lds church?"},{"output":"Roger Bannister","context":"In the sport of athletics, a four-minute mile means completing a mile run (1,760 yards, or 1,609.344 metres) in less than four minutes. It was first achieved in 1954 by Roger Bannister in 3:59.4.[1] The \\"four-minute barrier\\" has since been broken by many male athletes, and is now the standard of all male professional middle distance runners. In the last 50 years the mile record has been lowered by almost 17 seconds, and currently stands at 3:43.13.[2] Running a mile in four minutes translates to a speed of 15 miles per hour (24.14?km/h, or 2:29.13 per kilometre, or 14.91 seconds per 100 metres).[3] It also means 22 feet per second {5280/4= 1320?ft per minute, 1320/60= 22 feet per second}.\\r\\n\\r\\n\\r\\nBreaking the four-minute barrier was first achieved on 6 May 1954 at Oxford University's Iffley Road Track, by Englishman Roger Bannister,[4] with the help of fellow-runners Chris Chataway and Chris Brasher as pacemakers.[5]\\r\\nTwo months later, during the 1954 British Empire and Commonwealth Games hosted in Vancouver, B.C., two competing runners, Australia's John Landy and Bannister, ran the distance of one mile in under four minutes. The race's end is memorialised in a photo, and later a statue, of the two, with Landy looking over his left shoulder, just as Bannister is passing him on the right. Landy thus lost the race. The statue was placed in front of the Pacific National Exhibition entrance plaza.[6]\\r\\nNew Zealand's John Walker, the first man to run the mile under 3:50, ran 135 sub-four-minute miles during his career (during which he was the first person to run over 100 sub-four-minute miles), and American Steve Scott has run the most sub-four-minute miles, with 136. Algeria's Noureddine Morceli was the first under 3:45. Currently, the mile record is held by Morocco's Hicham El Guerrouj, who ran a time of 3:43.13 in Rome in 1999.\\r\\nIn 1964, America's Jim Ryun became the first high-school runner to break four minutes for the mile, running 3:59.0 as a junior and a then American record 3:55.3 as a senior in 1965.[7] Tim Danielson (1966) and Marty Liquori (1967) also came in under four minutes, but Ryun's high-school record stood until Alan Webb ran 3:53.43 in 2001.[8] Ten years later, in 2011, Lukas Verzbicas became the fifth high-schooler under four minutes.[9] In 2015, Matthew Maton and Grant Fisher became the sixth and seventh high-schoolers to break four minutes, both running 3:59.38 about a month apart.[10] Webb was the first high schooler to run sub-4 indoors, running 3:59.86 in early 2001. On 6 February 2016, Andrew Hunter significantly improved upon Webb's mark, running 3:58.25 on the same New York Armory track[11] and 3:57.81 two weeks later.[12] Hunter achieved the 4 minute mile mark outdoors later in the season at the Prefontaine Classic. At that same meet Michael Slagowski ran his second sub-4 minute of the season.[13] Reed Brown dipped under the barrier on June 1, 2017, running the 4th fastest high school mile time ever recorded in a race: 3:59.30.[14]\\r\\nAnother illustration of the progression of performance in the men's mile is that, in 1994, forty years after Bannister's breaking of the barrier, the Irish runner Eamonn Coghlan became the first man over the age of 40 to run a sub-four-minute mile.[15] Because Coghlan surpassed the mark indoors and before the IAAF validated indoor performances as being eligible for outdoor records, World Masters Athletics still had not recognised a sub-4-minute-mile performance as a record in the M40 division. Many elite athletes made the attempts to extend their careers beyond age 40 to challenge that mark. Over 18 years after Coghlan, that was finally achieved by UK's Anthony Whiteman, running 3:58.79 on 2 June 2012.[16]\\r\\nNo woman has yet run a four-minute mile. As of 2015[update], the women's world record is held by retired Russian Svetlana Masterkova, with a time of 4:12.56 in 1996.[17]\\r\\nIn 1997, Daniel Komen of Kenya ran two miles in less than eight minutes, doubling up on Bannister's accomplishment.[18] He did it again in February 1998, falling just 0.3 seconds behind his previous performance, and is still the only individual to accomplish the feat.[19]\\r\\nSome sources (including Olympic medalist Peter Radford[20]) contend the first successful four-minute mile was run in London by James Parrott on 9 May 1770.[21][22] Parrott's route began on Goswell Road, before turning down Old Street, finishing at St Leonard's, Shoreditch. Although timing methods at this time were ÿ following the invention of the chronometer by John Harrison ÿ accurate enough to measure the four minutes correctly, the record is not recognised by modern sporting bodies.[23] Neal Bascomb notes in The Perfect Mile that \\"even nineteenth-century historians cast a skeptical eye on the account.\\"[24]\\r\\nThen in 1796, the Sporting Magazine reported that on 10 October of that year a young man called Weller, one of three brothers, \\"undertook for a wager of three guineas (about 5 months' pay at that time) to run one mile on the Banbury road, in four minutes, which he performed two seconds within the time.\\"[25]\\r\\nIn other words, the magazine reports that he ran a mile in three minutes, fifty eight seconds. By that time, a mile could be routinely measured to within a few inches;[26] watches, thanks to John Harrison, could measure 4 minutes to within 0.0009 sec (i.e. gain or lose 10 seconds a month),[27] and after about 1750 the mass production of highly accurate watches was well underway.[28]\\r\\nIt is also reputed that Glenn Cunningham achieved a four-minute mile in a workout in the 1920s. In addition to being unsubstantiated, a workout run would not count as a record.[29]\\r\\nIn 1988, the ABC and the BBC co-produced The Four Minute Mile, a miniseries dramatisation of the race to the four-minute mile, featuring Richard Huw as Bannister and Nique Needles as John Landy (who was simultaneously pursuing the milestone). It was written by David Williamson and directed by Jim Goddard.\\r\\nIn 2004, Neal Bascomb wrote a book entitled The Perfect Mile about Roger Bannister, John Landy, and Wes Santee, portraying their individual attempts to break the four-minute mile and the context of the sport of mile racing. A second film version (entitled Four Minutes) was made in 2005, starring Jamie Maclachlan as Bannister.\\r\\nIn June 2011 the watch used to time the original event was donated by Jeffrey Archer to a charity auction for Oxford University Athletics Club; it sold for S97,250.[30]\\r\\nIn July 2016 the BBC broadcast the documentary Bannister: Everest on the Track, The Roger Bannister Story with firsthand interviews from Bannister and various other figures on the first sub-4 minute mile.[31][32]","input":"Who ran the first sub 4 minute mile?"},{"output":"Europe","context":"","input":"Where did world war 1 first take place?"},{"output":"public education and state-funded schools","context":"Level 2 and above: 87.4%\\r\\nLevel 3 and above: 60.3%\\r\\n(of 19 year olds in 2015)[9]\\r\\nEducation in England is overseen by the United Kingdom's Department for Education. Local government authorities are responsible for implementing policy for public education and state-funded schools at a local level.\\r\\nEngland also has a tradition of independent schools (sometimes termed \\"public schools\\") and home schooling alongside state schools; legally, parents may choose to educate their children by any suitable means. State-funded schools can be categorised as grammar schools, which are selective, or comprehensive schools, which are not. These can be further subdivided into free schools, other academies and state-run schools. More freedom is given to free schools, including most religious schools, and other academies in terms of curriculum, but all are subject to assessment and inspection by Ofsted.\\r\\nThe state-funded education system is divided into stages based upon age:[10] Early Years Foundation Stage (ages 3ÿ5); primary education (ages 5 to 11), subdivided into Key Stage 1 (KS1) Infants (ages 5 to 7) and Key Stage 2 (KS2) Juniors (ages 7 to 11); secondary education (ages 11 to 16), subdivided into Key Stage 3 (KS3; ages 11 to 14) and Key Stage 4 (KS4; ages 14 to 16); Key Stage 5 is post-16 education (ages 16 to 18); and tertiary education (for ages 18+).[11]\\r\\nAt age 16 the students typically take exams for the General Certificate of Secondary Education or other Level 1/2 qualifications. While education is compulsory until 18, schooling is only compulsory to 16, thus post-16 education can take a number of forms, and may be academic or vocational. This can involve continued schooling, known as \\"sixth form\\" or \\"college\\", leading (typically after two years of further study) to A-level qualifications (similar to a high school diploma in some other countries), or a number of alternative Level 3 qualifications such as BTEC, the International Baccalaureate or the Cambridge Pre-U. It can also include work-based apprenticeships or traineeships, or volunteering.[12][13]\\r\\nHigher education often begins with a three-year bachelor's degree. Postgraduate degrees include master's degrees, either taught or by research, and doctoral level research degrees that usually take at least three years. Tuition fees for first degrees are up to S9,000 per academic year for English, Welsh and European Union students, although these are set to rise to S9,250 for students starting from 2017.[14]\\r\\nThe Regulated Qualifications Framework (RQF) covers national school examinations and vocational education qualifications. It is referenced to the European Qualifications Framework, and thus to other qualifications frameworks across the European Union.[15] The Framework for Higher Education Qualifications (FHEQ), which is tied to the RQF, covers degrees and other qualifications from degree-awarding bodies.[16] This is referenced to the Qualifications Framework of the European Higher Education Area developed under the Bologna process.[17]\\r\\n\\r\\n\\r\\nUntil 1870 all schools were charitable or private institutions, but in that year the Elementary Education Act 1870 permitted local governments to complement the existing elementary schools in order to fill any gaps. The Education Act 1902 allowed local authorities to create secondary schools. The Education Act 1918 abolished fees for elementary schools.\\r\\nFull-time education is compulsory for all children aged 5 to 18, either at school or otherwise, with a child beginning primary education during the school year they turn 5.[18] Children between the ages of 3 and 5 are entitled to 600 hours per year of optional, state-funded, pre-school education. This can be provided in \\"playgroups\\", nurseries, community childcare centres or nursery classes in schools.\\r\\nThe age at which a student may choose to stop education is commonly known as the \\"leaving age\\" for compulsory education. This age was raised to 18 by the Education and Skills Act 2008; the change took effect in 2013 for 16-year-olds and 2015 for 17-year-olds. From this time, the school leaving age (which remains 16) and the education leaving age (which is now 18) have been separated.[19] State-provided schooling and sixth-form education are paid for by taxes.\\r\\nAll children in England must currently therefore receive an effective education (at school or otherwise) from the first \\"prescribed day\\", which falls on or after their fifth birthday until their 18th birthday, and must remain in school until the last Friday in June of the school year in which they turn 16.[13][20][21] The education leaving age was raised in 2013 to the year in which they turn 17 and in 2015 to their 18th birthday for those born on or after 1 September 1997.[19] The prescribed days are 31 August, 31 December and 31 March.[22][23] The school year begins on 1 September (or 1 August if a term starts in August).[24]\\r\\nThe Compulsory stages of education are broken into a Foundation Stage (actually covering the last part of optional and first part of compulsory education), 4 Key Stages, and post-16 education (sometimes unofficially termed Key Stage Five, which takes a variety of forms including 6th Form (covering the last 2 years of Secondary Education in schools).\\r\\nBelow is a table summarizing the most common names of the various schools and stages. Grammar schools are normally state-funded but selective schools, admitting children from 11 years old onward, but there are exceptions such as Manchester Grammar School.\\r\\nSome 93% of children between the ages of 3 and 18 are in education in state-funded schools without charge (other than for activities such as swimming, theatre visits and field trips for which a voluntary payment can be requested, and limited charges at state-funded boarding schools[25]).\\r\\nSince 1998, there have been six main types of maintained (state-funded) school in England:[26][27][28]\\r\\nIn addition, 3 of the 15 City Technology Colleges established in the 1980s still remain, the rest having converted to academies. These are state-funded all-ability secondary schools which charge no fees but which are independent of local authority control. There are also a small number of state-funded boarding schools.\\r\\nEnglish state-funded primary schools are almost all local schools with a small catchment area. More than half are owned by the Local Authority, though many are (nominally) voluntary controlled and some are voluntary aided. Some schools just include infants (aged 4 to 7) and some just juniors (aged 7 to 11). Some are linked, with automatic progression from the infant school to the junior school, and some are not. A few areas still have first schools for ages around 4 to 8 and middle schools for ages 8 or 9 to 12 or 13.\\r\\nEnglish secondary schools are mostly comprehensive, although the intake of comprehensive schools can vary widely, especially in urban areas with several local schools. Nearly 90% of state-funded secondary schools are specialist schools, receiving extra funding to develop one or more subjects in which the school specialises, which can select up to 10% of their intake for aptitude in the specialism (though relatively few of them have taken up this option). In a few areas children can enter a grammar school if they pass the eleven plus exam; there are also a number of isolated fully selective grammar schools and a few dozen partially selective schools.[31] A significant minority of state-funded schools are faith schools, which are attached to religious groups, most often the Church of England or the Roman Catholic Church.\\r\\nAll state-funded schools are regularly inspected by the Office for Standards in Education, often known simply as Ofsted. Ofsted publish reports on the quality of education at a particular school on a regular basis. Schools judged by Ofsted to be providing an inadequate standard of education may be subject to special measures, which could include replacing the governing body and senior staff.\\r\\nApproximately 7% of school children in England attend privately run, fee-paying independent schools. Some independent schools for 13ÿ18-year-olds are known for historical reasons as 'public schools' and for 8ÿ13-year-olds as 'prep schools'. Some schools offer scholarships for those with particular skills or aptitudes, or bursaries to allow students from less financially well-off families to attend. Independent schools do not have to follow the National Curriculum, and their teachers are not required or regulated by law to have official teaching qualifications.\\"[32]\\r\\nThe 1944 Education Act (Section 36) stated that parents are responsible for the education of their children, \\"by regular attendance at school or otherwise\\", which allows children to be educated at home. The legislation places no requirement for parents who choose not to send their children to school to follow the National Curriculum, or to give formal lessons, or to follow school hours and terms, and parents do not need to be qualified teachers.[33] A small but increasing numbers of parents do choose to educate their children outside the conventional school systems.[34][35][36] Officially referred to as \\"Elective Home Education\\", teaching ranges from structured homeschooling (using a school-style curriculum) to less-structured unschooling.[37][38] Education Otherwise has supported parents who wished to educate their children outside school since the 1970s. The state provides no financial support to parents who choose to educate their children outside of school.\\r\\nStudents at both state schools and independent schools typically take GCSE examinations, which mark the end of compulsory education in school. Above school-leaving age, the independent and state sectors are similarly structured. In the 16ÿ18 age group, sixth form education is not compulsory, but mandatory education or training until the age of 18 is being phased in under the Education and Skills Act 2008.\\r\\nThis took effect for 16-year-olds in 2013 and for 17-year-olds in September 2015. While students may still leave school on the last Friday in June, they must remain in education of some form until their 18th birthday.[13]\\r\\nStudents over 16 typically study in the sixth form of a school, in a separate sixth form college, or in a Further Education (FE) College. Courses at FE colleges, referred to as further education courses, can also be studied by adults over 18. Students typically study Level 3 qualifications such as A-levels, BTEC National awards and level 3 NVQs. Some 16ÿ18 students will be encouraged to study Key Skills in Communication, Application of Number, and Information Technology at this time.\\r\\nThe National Apprenticeship Service helps people 16 or more years of age enter apprenticeships in order to learn a skilled trade. Traineeships are also overseen by the National Apprenticeship Service, and are education and a training programmes that are combined with work experience to give trainees the skills needed to get an apprenticeship.[39]\\r\\nApprenticeships come in four levels: Intermediate (level 2), Advanced (level 3), Higher (level 4 ÿ 7) and Degree (level 6 ÿ 7). Intermediate apprenticeships are equivalent to 5 GCSEs at A* ÿ C, Advanced to 2 A-levels, Higher to a foundation degree or above, and Degree apprenticeships to a bachelor's or master's degree.[40]\\r\\nA study in 2014 found that unemployment rates among former apprentices one year after completing their apprenticeships were one-third those of university graduates one year after finishing their degrees.[41] A 2015 study by the Sutton Trust found that, while average net[42] lifetime earnings for those who had completed level 5 apprenticeships were higher than those for graduates from non-Russell Group universities, most apprenticeships offered were at levels 2 and 3, providing little improvement over earnings from secondary school qualifications. The report also found that apprenticeships had a lower perceived value compared to degrees in Britain than in many other countries.[43]\\r\\nIn 2015, the Department announced a major restructuring of the further education sector, through 37 area reviews of post-16 provision.[44] The proposals were criticised by NUS Vice President for Further Education Shakira Martin for not sufficiently taking into account the impact on learners;[45][46] the Sixth Form Colleges' Association similarly criticised the reviews for not directly including providers of post-16 education other than colleges, such as school and academy sixth forms and independent training providers.[47]\\r\\nHigher education in England is provided by Higher Education (HE) colleges, university colleges, universities and private colleges. Students normally enter higher education as undergraduates from age 18 onwards, and can study for a wide variety of vocational and academic qualifications, including certificates of higher education and higher national certificates at level 4, diplomas of higher education, higher national diplomas and foundation degrees at level 5, bachelor's degrees (normally with honours) at level 6, and integrated master's degrees and degrees in medicine, dentistry, and veterinary science at level 7.[48]\\r\\nHistorically, undergraduate education outside a small number of private colleges and universities has been largely state-financed since the 1960s, with a small contribution from top-up fees introduced in the 1990s,[49] however fees of up to S9,000 per annum have been charged from October 2012. There is a perceived hierarchy among universities, with the Russell Group seen as being composed of the country's more prestigious universities.[50] League tables of universities are produced by private companies and generally cover the whole UK.\\r\\nThe state does not control university syllabuses, but it does influence admission procedures through the Office for Fair Access (OFFA), which approves and monitors access agreements to safeguard and promote fair access to higher education. The independent Quality Assurance Agency for Higher Education inspects universities to assure standards, advises on the granting of degree awarding powers and University title, and maintains the Quality Code for Higher Education, which includes the Framework for Higher Education Qualification.[51] Unlike most degrees, the state has control over teacher training courses, and standards are monitored by Ofsted inspectors.[52]\\r\\nThe typical first degree offered at English universities is the bachelor's degree with honours, which usually lasts for three years, although more vocational foundation degrees, typically lasting two years (or full-time equivalent) are also available in some institutions. Many institutions now offer an integrated master's degree, particularly in STEM subjects, as a first degree, which typically lasts for four years, the first three years running parallel to the bachelor's course. During a first degree students are known as undergraduates. The difference in fees between integrated and traditional postgraduate master's degrees (and that fees are capped at the first degree level for the former) makes taking an integrated master's degree as a first degree a more attractive option. Integrated master's degrees are often the standard route to chartered status for STEM professionals in England.[53]\\r\\nStudents who have completed a first degree can apply for postgraduate and graduate courses. These include:\\r\\nPostgraduate education is not automatically financed by the state.\\r\\nUntil the academic year 2011-2012 most undergraduates paid fees that were set at a maximum of S3,375 per annum. These fees are repayable after graduation, contingent on attaining a certain level of income, with the state paying all fees for students from the poorest backgrounds. UK students are generally entitled to student loans for maintenance. Undergraduates admitted from the academic year 2012-2013 have paid tuition fees set at a maximum of up to S9,000 per annum, with most universities charging over S6,000 per annum, and other higher education providers charging less.\\r\\nPostgraduate fees vary but are generally more than undergraduate fees, depending on the degree and university. There are numerous bursaries (awarded to low income applicants) to offset undergraduate fees and, for postgraduates, full scholarships are available for most subjects, and are usually awarded competitively.\\r\\nDifferent arrangements apply to English students studying in Scotland, and to Scottish and Welsh students studying in England. Students from outside the UK and the EU attending English universities are charged differing amounts, often in the region of S5,000 - S20,000 per annum[55] for undergraduate and postgraduate degrees. The actual amount differs by institution and subject, with the lab based subjects charging a greater amount.\\r\\nAdult education, continuing education or lifelong learning is offered to students of all ages. This can include the vocational qualifications mentioned above, and also:\\r\\nThe two qualifications frameworks in England are the Regulated Qualifications Framework (RQF), for qualifications regulated by Ofqual, and the Framework for Higher Education Qualifications (FHEQ) for qualifications granted by bodies with degree awarding powers, overseen by the Quality Assurance Agency. These share a common numbering scheme for their levels, which was also used for the earlier Qualifications and Credit Framework. The RQF is linked to the European Qualifications Framework (EQF) and the FHEQ to the Qualifications Framework of the European Higher Education Area (QF-EHEA).[15][16][17][56]\\r\\nAccording to the Schools Minister, strong evidence has been emerging of grade inflation across subjects in recent years.[57] The Confederation of British Industry, the EEF and the British Chambers of Commerce are also complaining of falling academic standards. Employers often experience difficulty in finding young people who have such basic employability skills as literacy, numeracy, problem solving, teamworking and time management. As a result, employers either have to pay for employees' remedial education, or they must hire foreign candidates.[58][59][60]\\r\\nKatharine Birbalsingh has written of the problems she perceives in many community schools. She cites the impossibility of effective classroom management, bad teachers who cannot be dismissed, and government policies encouraging \\"soft\\" subjects. Birbalsingh has visited schools in Jamaica and India where pupils are desperate to gain the kind of education to which pupils in her own school (and their parents) were indifferent. She was a deputy head teacher in south London until she spoke at a Conservative Party conference in 2010 and was quickly sacked.[61] Frank Chalk, who taught at an inner-city school for ten years before resigning in frustration, makes similar claims.[62]\\r\\nAn analysis of 2010 school data by The Guardian found that state faith schools were not taking a fair share of the poorest pupils in their local areas, as indicated by free school meal entitlement. Not only was this so at an overall national level, but also in the postcode areas nearby the schools. This suggested selection by religion was leading to selection of children from more well-off families.[64]\\r\\nA survey of 2000 teachers by The Guardian in 2011 identified a widespread reason for not enjoying the job: lack of trust and respect by senior staff, parents and governments.[65] Writing about her own reasons for leaving teaching, a contributing editor to the newspaper's Guardian Teacher Network described the realisation of needing to leave the profession as having slowly crept up on her. Being a mature entrant, she questioned things in her aspiration to improve education and was reluctant to \\"be moulded into a standard shape\\".[66]\\r\\nContinual government changes in assessment and accountability also heap additional pressure on teachers, with many having to change their teaching materials continuously, purely to keep up with new initiatives, which seem to be driven more by politics than academic research.[67]\\r\\nThroughout the time they are in full-time education children in the north of England do less well than children in the south east. There are also variations in attainment between towns in the north. Calls have been made to remedy this.[68]\\r\\nMany teachers claim they are working 55 or 60 hours a week and many are leaving the profession due to work pressure and fear for their mental health.[69][70] The Institute for Fiscal Studies claims plans for school spending would cause a \\"real-terms cut of 2.8% in per-pupil funding between 2016 and 2022\\".[71] Just under a quarter of teachers who qualified since 2011 have left the profession.[72] Schools are asking parents for money because funding is falling short of what they need.[73]\\r\\nThere is a gap in performance between pupils from better off families and poorer pupils. To counter this a cross party committee of MP's suggested giving subsidized housing to teachers willing to work in deprived schools. Also it was suggested would-be head teachers should spend time in senior positions in struggling schools before they qualify to be heads.[74]","input":"What types of school are there in england?"},{"output":"April 15, 2013","context":"El Se?or de los Cielos is an American telenovela created by Luis Zelkowicz, based on an original idea by Mariano Calasso, and Andrs L܇pez and it started airing on American broadcast channel Telemundo on April 15, 2013. Produced by Argos Comunicaci܇n and Telemundo Studios, and Caracol Internacional in the first season, and distributed by Telemundo Internacional. The series is based on the life and work of Amado Carrillo Fuentes, the former leader of the Juarez Cartel. The program features an ensemble cast, headed by its longest-serving actors Rafael Amaya as Aurelio Casillas and Fernanda Castillo as M܇nica Robles.\\r\\nThe series has won several awards including; International Emmy Award for Non-English Language Us Primetime Program.[1]Premios Tu Mundo for Novela or serie of the Year for three consecutive years.\\r\\n\\r\\n\\r\\nThe series follows the life of Aurelio Casillas (Rafael Amaya), a drug lord who is recognized for knowing how to transfer drug substances to Mexico, Colombia, and the United States. Aurelio and his wife Ximena (Ximena Herrera) have three children, Rutila (Carmen Aub), Heriberto (Ruy Senderos), and Luz Marina (Gala Montes). He also has his faithful brother Chacorta (Ra~l Mndez) who helps him in all his illicit businesses. Aurelio would end up being cornered by the authorities after being found as he decides to undergo a face operation in which he supposedly dies, thus causing his empire of power to fall.\\r\\nIn season two, believed to be dead, Aurelio returns to enact his revenge on police officer Leonor Ballesteros (Carmen Villalobos), who through her fault ends up operating his face in a failed attempt to escape from the authorities. Aurelio meets Victoria Navrez (Marlene Favela), a beautiful woman who throws herself to be the governor of Mexico, Aurelio after seeing her actions falls in love and decides to deceive her by posing as another man and usurping the identity of Danilo Ferro. To the arrival of the new personages also is Jos Mara \\"Chema\\" Venegas (Mauricio Ochmann), one of the great enemies of Aurelio who decides to take all its power to Aurelio. At the end of the season, Leonor captures Aurelio and lasts several months in prison naval bunker.\\r\\nIn season three, after several months imprisoned in naval bunker in Mexico, Aurelio receives the mutilated head of his brother Chacorta in a box, implying that his brother has died, who gives him reason to wage a war against all his enemies and to look for the murderer of his brother, to avenge his death. After all the problems with the government and the DEA, Aurelio learns that he has a serious illness in one of his kidneys and decides to remember his past in order to find his lost son.\\r\\nIn season four, Aurelio after being cornered by his illness and not find a relative who can donate a kidney, decides to undertake a search with all the lovers he has had to find a lost son who can donate a kidney. After several attempts of searching, Aurelio finds a young man named Ismael (Ivn Arana), who can be his salvation. His enemies after learning of his illness decide to do everything possible to get him out of the way. When everything seems to take its course and achieve total stability in the world of legal and illegal businesses of Aurelio, Emiliana Contreras (Vanessa Villela) appears a beautiful woman sent by Feyo Aguilera (Leonardo Daniel) to be able to make Aurelio fall, but her mission is to become the lover Of Aurelio to thus be able to finally make the fall of Aurelio's empire.\\r\\nIn season five, Aurelio, will unleash a new war; But this time it will not be against his former rivals and enemies; But this time it will be against his own family; Because of the betrayals he had previously experienced on their part; Later, Aurelio (Rafael Amaya), will also face a serious illness, which puts at stake his power and prestige since this man fails to get a donor, so they can make his transplant, necessary to continue living. On the other hand, M܇nica (Fernanda Castillo) fights for its life after being rescued from Vctor's hands in the middle of their wedding, since while it fled received a shot that apparently is mortal; After being taken from his side, Vctor (Jorge Luis Moreno) searches everywhere to find M܇nica's whereabouts and continue with her wedding plans.\\r\\nDuring the ongoing production of season five, Telemundo announced in a series renewal press release that a season six would follow season five after it has aired.[2] On May 16, 2017, it was confirmed that the series would have a film\\"as a concentrate of the five seasons\\", which will be produced by Rafael Amaya.[3]\\r\\n\\r\\nDuring the broadcast of the second season, Telemundo published a comic based on the second season of the series, portraying part of the plot of the series.[10] In September 2014, the character played by Rafael Amaya, appeared in the series Se?ora Acero in the first two episodes of the series as a special guest,[11] in November 2015 returned to have a special participation in the series in an episode with Ana Lucia Dominguez.[12] The character of Aurelio has appeared twice in the adaptation of La Reina del Sur, titled Queen of the South.[13]\\r\\nTelemundo confirmed on May 15, 2016 that the series would see a Spin-off based upon actor Mauricio Ochmann's character \\"El Chema\\".[14] El Chema premiered on December 6, 2016.\\r\\nAll songs in the series are composed by Marco Flores and performed by Cardenales de Nuevo Le܇n. In the first season the main theme was \\"El jefe de todos\\".[15] For the second season, the opening theme song was changed to \\"El verdadero Jefe de todos\\".[16] In the third season the song was composed around the main character of the series and is titled \\"Aurelio Casillas\\".[17] In the fourth season, the song of the main theme was changed to \\"El cuarto corrido\\".[18]In the fifth season the opening theme titled \\"El quinto corrido\\", this time performed by Don Chayo, member of the band Cardenales de Nuevo Le܇n.","input":"When did el senor de los cielos start?"},{"output":"the Santiago Canyon Fire of 1889","context":"California has dry, windy, and often hot weather conditions from late spring through autumn that can produce moderate to devastating wildfires. At times, these wildfires are fanned or made worse from strong, dry winds, known as Diablo winds in the northern part of the state and Santa Ana winds to the south. Wildfires in California are growing more dangerous as people build in rural burn areas. United States taxpayers pay about US$3 billion a year to fight wildfires, and big fires can lead to billions of dollars in property losses.[1]\\r\\nThe following is a list of notable wildfires of various sizes that have occurred in California.\\r\\n\\r\\n\\r\\nSince 1932, when accurate records started to be kept, these are the 20 largest wildfires in California, according to the California Department of Forestry and Fire Protection (CAL FIRE).[2] However, the largest wildfire in recorded California history was the Santiago Canyon Fire of 1889, which burned at least 300,000 acres (120,000?ha) in southern California.[3]\\r\\nNote:  means that the fire is currently burning. Total size and position in the list are subject to change.\\r\\nA list of the 20 deadliest fires, according to CAL FIRE, can be found here:\\r\\nThe deadliest fire in California in recorded history was the 1933 Griffith Park Fire in Los Angeles, which killed 29 civilians who tried to fight and/or escape the fire. The second deadliest fire was the 1991 Oakland Hills fire, which killed 25 people. The third deadliest fire was the 2017 Tubbs Fire in Napa and Sonoma Counties, which killed 22 people in and around Santa Rosa.\\r\\nA list of the 20 most destructive fires, according to CAL FIRE, can be found here: http://www.fire.ca.gov/communications/downloads/fact_sheets/Top20_Destruction.pdf[8]\\r\\nStarting in 2001, the National Interagency Fire Center began keeping more accurate records on the total fire acreage burned in each state.[9]\\r\\nA 2015 study[36] addressed whether the increase in fire risk in California is attributable to climate change.[37]\\r\\nNote: ? means fire is currently burning. Check primary sources for up-to-date statistics.\\r\\nIn some parts of California, fires can recur in areas with histories of fires. Examples of this are in Oakland, where fires of various size and ignition occurred in 1923, 1931, 1933, 1937, 1946, 1955, 1960, 1961, 1968, 1970, 1980, 1990, 1991, 1995, 2002, and 2008.[90][91] Other examples being Orange County, Riverside County, San Bernardino County, and Los Angeles County. In the case of Orange and San Bernardino, these two counties share a county border that runs north to south through the Chino Hills State Park, with the park's landscape ranging from large green coastal sage scrub, grassland, and woodland, to areas of brown sparsely dense vegetation made drier by droughts or hot summers. The valley's grass and barren land can become easily susceptible to dry spells and drought, therefore making it a prime spot for brush fires and conflagration, many of which have occurred since 1914. Hills and canyons have seen brush or wildfires in 1914, the 1920s, 1930s, 1940s, 1950s, 1960s, 1970s, 1980s, 1990s, 2000s, and into today.[92]\\r\\nOn occasion, freak lightning strikes from thunderstorms may also spark wildfires in areas that have seen past ignition. Examples of this are the 1999 Megram Fire and the 2008 California wildfires.[citation needed]","input":"What was the largest forest fire in california history?"},{"output":"Theodosian Walls","context":"Muslim conquest of the Levant\\r\\nMuslim conquest of Egypt\\r\\nMuslim conquest of North Africa\\r\\nUmayyad invasions of Anatolia\\r\\nand Constantinople\\r\\nArabÿByzantine border warfare\\r\\nSicily and Southern Italy\\r\\nNaval warfare and raids\\r\\nByzantine Reconquest\\r\\nThe Second Arab siege of Constantinople in 717ÿ718 was a combined land and sea offensive by the Muslim Arabs of the Umayyad Caliphate against the capital city of the Byzantine Empire, Constantinople. The campaign marked the culmination of twenty years of attacks and progressive Arab occupation of the Byzantine borderlands, while Byzantine strength was sapped by prolonged internal turmoil. In 716, after years of preparations, the Arabs, led by Maslama ibn Abd al-Malik, invaded Byzantine Asia Minor. The Arabs initially hoped to exploit Byzantine civil strife and made common cause with the general Leo III the Isaurian, who had risen up against Emperor Theodosius III. Leo, however, tricked them and secured the Byzantine throne for himself.\\r\\nAfter wintering in the western coastlands of Asia Minor, the Arab army crossed into Thrace in early summer 717 and built siege lines to blockade the city, which was protected by the massive Theodosian Walls. The Arab fleet, which accompanied the land army and was meant to complete the city's blockade by sea, was neutralized soon after its arrival by the Byzantine navy through the use of Greek fire. This allowed Constantinople to be resupplied by sea, while the Arab army was crippled by famine and disease during the unusually hard winter that followed. In spring 718, two Arab fleets sent as reinforcements were destroyed by the Byzantines after their Christian crews defected, and an additional army sent overland through Asia Minor was ambushed and defeated. Coupled with attacks by the Bulgars on their rear, the Arabs were forced to lift the siege on 15 August 718. On its return journey, the Arab fleet was almost completely destroyed by natural disasters and Byzantine attacks.\\r\\nThe siege's failure had wide-ranging repercussions. The rescue of Constantinople ensured the continued survival of Byzantium, while the Caliphate's strategic outlook was altered: although regular attacks on Byzantine territories continued, the goal of outright conquest was abandoned. Historians consider the siege to be one of history's most important battles, as its failure postponed the Muslim advance into Southeastern Europe for centuries.\\r\\n\\r\\n\\r\\nFollowing the first Arab siege of Constantinople (674ÿ678), the Arabs and Byzantines experienced a period of peace. After 680, the Umayyad Caliphate was in the throes of the Second Muslim Civil War and the consequent Byzantine ascendancy in the East enabled the emperors to extract huge amounts of tribute from the Umayyad government in Damascus.[3] In 692, as the Umayyads emerged as victors from the Muslim Civil War, Emperor Justinian II (r. 685ÿ695 and 705ÿ711) re-opened hostilities. The result was a series of Arab victories that led to the loss of Byzantine control over Armenia and the Caucasian principalities, and a gradual encroachment upon Byzantine borderlands. Year by year, the Caliphate's generals, usually members of the Umayyad family, launched raids into Byzantine territory and captured fortresses and towns.[4] After 712, the Byzantine defensive system began to show signs of collapse: Arab raids penetrated further and further into Asia Minor, border fortresses were repeatedly attacked and sacked, and references to Byzantine reaction in the sources become more and more scarce.[5] In this, the Arabs were aided by the prolonged period of internal instability that followed the first deposition of Justinian II in 695, in which the Byzantine throne changed hands seven times in violent coups.[6] In the words of the Byzantinist Warren Treadgold, \\"the Arab attacks would in any case have intensified after the end of their own civil war?... With far more men, land and wealth than Byzantium, the Arabs had begun to concentrate all their strength against it. Now they threatened to extinguish the empire entirely by capturing its capital.\\"[7]\\r\\nThe information available on the siege comes from sources composed in later dates, which are often mutually contradictory. The main Byzantine source is the extensive and detailed account of the Chronicle of Theophanes the Confessor (760ÿ817) and secondarily the brief account in the Breviarium of Patriarch Nikephoros I of Constantinople (died 828), which shows small differences, mainly chronological, from Theophanes's version.[8] For the events of the siege, both authors appear to have used a primary account composed during the reign of Leo III the Isaurian (r. 717ÿ741) which therefore contains a favourable depiction of the latter, while Theophanes apparently relies on an unknown biography of Leo (ignored by Nikephoros) for the events of 716. The eighth century chronicler Theophilus of Edessa records the years leading up to the siege and the siege itself in some detail, paying particular attention to the diplomacy between Maslama and Leo III.[9] The Arab sources, mainly the 11th-century Kitab al-'Uyun and the more concise narrative in the History of the Prophets and Kings by al-Tabari (838ÿ923), rely on primary accounts by early 9th-century Arab writers, but are more confused and contain several legendary elements. The Syriac language accounts are based on Agapius of Hierapolis (died 942), who likely drew from the same primary source as Theophanes, but are far briefer.[10]\\r\\nThe Arab successes opened the way for a second assault on Constantinople, an undertaking already initiated under Caliph al-Walid I (r. 705ÿ715). Following his death, his brother and successor Sulayman (r. 715ÿ717) took up the project with increased vigour, according to Arab accounts because of a prophecy that a Caliph bearing the name of a prophet would capture Constantinople; Sulayman (Solomon) was the only member of the Umayyad family to bear such a name. According to Syriac sources, the new Caliph swore \\"to not stop fighting against Constantinople before having exhausted the country of the Arabs or to have taken the city\\".[11] The Umayyad forces began assembling at the plain of Dabiq north of Aleppo, under the direct supervision of the Caliph. As Sulayman was too sick to campaign himself, however, he entrusted command to his brother Maslama ibn Abd al-Malik.[12] The operation against Constantinople came at a time when the Umayyad state was undergoing a period of continuous expansion to the east and west. Muslim armies advanced into Transoxiana, India and the Visigothic Kingdom of Hispania.[13]\\r\\nArab preparations, especially the construction of a large fleet, did not go unnoticed by the worried Byzantines. Emperor Anastasios II (r. 713ÿ715) sent an embassy to Damascus under the patrician and urban prefect, Daniel of Sinope, ostensibly in order to plea for peace, but in reality to spy on the Arabs. Anastasios, in turn, began to prepare for the inevitable siege: the fortifications of Constantinople were repaired and equipped with ample artillery (catapults and other siege weapons), while food stores were brought into the city. In addition, those inhabitants who could not stockpile food for at least three years were evacuated.[14] Anastasios strengthened his navy and in early 715 dispatched it against the Arab fleet that had come to Phoenixusually identified with modern Finike in Lycia, it may also be modern Fenaket across Rhodes,[15] or perhaps Phoenicia (modern Lebanon), famed for its cedar forests[16]to collect timber for their ships. At Rhodes, however, the Byzantine fleet, encouraged by the soldiers of the Opsician Theme, rebelled, killed their commander John the Deacon and sailed north to Adramyttium. There, they acclaimed a reluctant tax collector, Theodosius, as emperor.[17] Anastasios crossed into Bithynia in the Opsician Theme to confront the rebellion, but the rebel fleet sailed on to Chrysopolis. From there, it launched attacks against Constantinople, until, in late summer, sympathizers within the capital opened its gates to them. Anastasios held out at Nicaea for several months, finally agreeing to resign and retire as a monk.[18] The accession of Theodosios, who from the sources comes across as both unwilling and incapable, as a puppet emperor of the Opsicians provoked the reaction of the other themes, especially the Anatolics and the Armeniacs under their respective strategoi (generals) Leo the Isaurian and Artabasdos.[19]\\r\\nIn these conditions of near-civil war, the Arabs began their carefully prepared advance. In September 715, the vanguard, under general Sulayman ibn Mu'ad, marched over Cilicia into Asia Minor, taking the strategic fortress of Loulon on its way. They wintered at Afik, an unidentified location near the western exit of the Cilician Gates. In early 716, Sulayman's army continued into central Asia Minor. The Umayyad fleet under Umar ibn Hubayra cruised along the Cilician coast, while Maslama ibn Abd al-Malik awaited developments with the main army in Syria.[20]\\r\\nThe Arabs hoped that the disunity among the Byzantines would play to their advantage. Maslama had already established contact with Leo the Isaurian. French scholar Rodolphe Guilland theorized that Leo offered to become a vassal of the Caliphate, although the Byzantine general intended to use the Arabs for his own purposes. In turn, Maslama supported Leo hoping to maximize confusion and weaken the Empire, easing his own task of taking Constantinople.[21]\\r\\nSulayman's first objective was the strategically important fortress of Amorium, which the Arabs intended to use as a base the following winter. Amorium had been left defenceless in the turmoil of the civil war and would have easily fallen, but the Arabs chose to bolster Leo's position as a counterweight to Theodosios. They offered the city terms of surrender if its inhabitants would acknowledge Leo as emperor. The fortress capitulated, but still did not open its gates to the Arabs. Leo came to the vicinity with a handful of soldiers and executed a series of ruses and negotiations to garrison 800 men in the town. The Arab army, thwarted in its objective and with supplies running low, withdrew. Leo escaped to Pisidia and, in summer, supported by Artabasdos, was proclaimed and crowned as Byzantine emperor, openly challenging Theodosios.[22][23]\\r\\nLeo's success at Amorium was fortunately timed, since Maslama with the main Arab army had in the meantime crossed the Taurus Mountains and was marching straight for the city. In addition, as the Arab general had not received news of Leo's double-dealing, he did not devastate the territories he marched throughthe Armeniac and Anatolic themes, whose governors he still believed to be his allies.[24] On meeting up with Sulayman's retreating army and learning what had transpired, Maslama changed direction: he attacked Akroinon and from there marched to the western coastlands to spend the winter. On his way, he sacked Sardis and Pergamon. The Arab fleet wintered in Cilicia.[25] Leo, in the meantime, began his own march on Constantinople. He captured Nicomedia, where he found and captured, among other officials, Theodosios's son, and then marched to Chrysopolis. In spring 717, after short negotiations, he secured Theodosios's resignation and his recognition as emperor, entering the capital on 25 March. Theodosios and his son were allowed to retire to a monastery as monks, while Artabasdos was promoted to the position of kouropalates and received the hand of Leo's daughter, Anna.[26]\\r\\nFrom the outset, the Arabs prepared for a major assault on Constantinople. The late 8th-century Syriac Zuqnin Chronicle reports that the Arabs were \\"innumerable\\", while the 12th-century Syriac chronicler Michael the Syrian mentions a much-inflated 200,000 men and 5,000 ships. The 10th-century Arab writer al-Mas'udi mentions 120,000 troops, and the account of Theophanes the Confessor 1,800 ships. Supplies for several years were hoarded, and siege engines and incendiary materials (naphtha) were stockpiled. The supply train is said to have numbered 12,000 men, 6,000 camels and 6,000 donkeys, while according to the 13th-century historian Bar Hebraeus, the troops included 30,000 volunteers (mutawa) for the Holy War (jihad).[27] Whatever the true numbers, the attackers were considerably more numerous than the defenders; according to Treadgold, the Arab host may have outnumbered the entire Byzantine army.[1] Little is known on the detailed composition of the Arab force, but it appears that it mostly consisted of and was led by Syrians and Jazirans of the elite ahl al-Sham (\\"People of Syria\\"), the main pillar of the Umayyad regime and veterans of the struggle against Byzantium.[28] Alongside Maslama, Umar ibn Hubayra, Sulayman ibn Mu'ad, and Bakhtari ibn al-Hasan are mentioned as his lieutenants by Theophanes and Agapius of Hierapolis, while the later Kitab al-'Uyun replaces Bakhtari with Abdallah al-Battal.[29]\\r\\nAlthough the siege consumed a large part of the Caliphate's manpower and resources,b[?] it was still capable of launching raids against the Byzantine frontier in eastern Asia Minor during the siege's duration: in 717, Caliph Sulayman's son Daud captured a fortress near Melitene and in 718 Amr ibn Qais raided the frontier.[30] On the Byzantine side, the numbers are unknown. Aside from Anastasius II's preparations (which might have been neglected following his deposition),[31] the Byzantines could count on the assistance of the Bulgars, with whom Leo concluded a treaty that possibly included alliance against the Arabs.[32]\\r\\nIn early summer, Maslama ordered his fleet to join him and with his army crossed the Hellespont (Dardanelles) at Abydos into Thrace. The Arabs began their march on Cnstantinople, thoroughly devastating the countryside, gathering supplies, and sacking the towns they encountered.[33] In mid-July or mid-August,a[?] the Arab army reached Constantinople and isolated it completely on land by building a double siege wall of stone, one facing the city and one facing the Thracian countryside, with their camp positioned between them. According to Arab sources, at this point Leo offered to ransom the city by paying a gold coin for every inhabitant, but Maslama replied that there could not be peace with the vanquished, and that the Arab garrison of Constantinople had already been selected.[34]\\r\\nThe Arab fleet under Sulayman (often confused with the Caliph himself in the medieval sources) arrived on 1 September, anchoring at first near the Hebdomon. Two days later, Sulayman led his fleet into the Bosphorus and the various squadrons began anchoring by the European and Asian suburbs of the city: one part sailed south of Chalcedon to the harbours of Eutropios and Anthemios to watch over the southern entrance of the Bosporus, while the rest of the fleet sailed into the strait, passed by Constantinople and began making landfall on the coasts between Galata and Kleidion, cutting the Byzantine capital's communication with the Black Sea. But as the Arab fleet's rearguard, twenty heavy ships with 2,000 marines, was passing the city, the southerly wind stopped and then reversed, drifting them towards the city walls, where a Byzantine squadron attacked them with Greek fire. Theophanes reported that some went down with all hands, while others, burning, sailed down to the Princes' Islands of Oxeia and Plateia. The victory encouraged the Byzantines and dejected the Arabs, who, according to Theophanes, had originally intended to sail to the sea walls during the night and try to scale them using the ships' steering paddles. The same night, Leo drew up the chain between the city and Galata, closing the entrance to the Golden Horn. The Arab fleet became reluctant to engage the Byzantines, and withdrew to the safe harbour of Sosthenion further north on the European shore of the Bosporus.[35]\\r\\nThe Arab army was well-provisioned, with Arab accounts reporting high mounds of supplies piled up in their camp, and had even brought along wheat to sow and harvest the next year. The failure of the Arab navy to blockade the city, however, meant that the Byzantines too could ferry in provisions. In addition, the Arab army had already devastated the Thracian countryside during its march and could not rely on it for foraging. The Arab fleet and the second Arab army, which operated in the Asian suburbs of Constantinople, were able to bring in limited supplies to Maslama's army.[36] As the siege drew into winter, negotiations opened between the two sides, extensively reported by Arab sources but ignored by Byzantine historians. According to the Arab accounts, Leo continued to play a double game with the Arabs. One version claims that he tricked Maslama into handing over most of his grain supplies, while another claims that the Arab general was persuaded to burn them altogether, so as to show the inhabitants of the city that they faced an imminent assault and induce them to surrender.[37] The winter of 718 was extremely harsh; snow covered the ground for over three months. As the supplies in the Arab camp ran out, a terrible famine broke out: the soldiers ate their horses, camels, and other livestock, and the bark, leaves and roots of trees. They swept the snow of the fields they had sown to eat the green shoots, and reportedly resorted to cannibalism and eating their own excrement. Consequently, the Arab army was ravaged by epidemics; with great exaggeration, the Lombard historian Paul the Deacon put the number of their dead of hunger and disease at 300,000.[38]\\r\\nThe situation looked set to improve in spring when the new Caliph, Umar II (r. 717ÿ720), sent two fleets to the besiegers' aid: 400 ships from Egypt under a commander named Sufyan and 360 ships from Africa under Izid, all laden with supplies and arms. At the same time, a fresh army began marching through Asia Minor to assist in the siege. When the new fleets arrived in the Sea of Marmara, they kept their distance from the Byzantines and anchored on the Asian shore. The Egyptians were in the Gulf of Nicomedia near modern Tuzla and the Africans south of Chalcedon (at Satyros, Bryas and Kartalimen). Most of the Arab fleets' crews were composed of Christian Egyptians, however, and they began deserting to the Byzantines upon their arrival. Notified by the Egyptians of the advent and disposition of the Arab reinforcements, Leo launched his fleet in an attack against the new Arab fleets. Crippled by the defection of their crews, and helpless against Greek fire, the Arab ships were destroyed or captured along with the weapons and supplies they carried. Constantinople was now safe from a seaborne attack.[39] On land too the Byzantines were victorious: their troops managed to ambush the advancing Arab army under a commander named Mardasan and destroy it in the hills around Sophon, south of Nicomedia.[40]\\r\\nConstantinople could now be easily resupplied by sea and the city's fishermen went back to sea, as the Arab fleet did not sail again. Still suffering from hunger and pestilence, the Arabs lost a major battle against the Bulgars, who killed, according to Theophanes, 22,000. It is unclear, however, whether the Bulgars attacked the Arab encampment because of their treaty with Leo or whether the Arabs strayed into Bulgar territory seeking provisions, as reported by the Syriac Chronicle of 846. Michael the Syrian mentions that the Bulgars participated in the siege from the beginning, with attacks against the Arabs as they marched through Thrace and subsequently on their encampment, but this is not corroborated elsewhere.[41] The siege had clearly failed, and Caliph Umar sent orders to Maslama to retreat. After thirteen months of siege, on 15 August 718, the Arabs departed. The date coincided with the feast of the Dormition of the Theotokos (Assumption of Mary), and it was to her that the Byzantines ascribed their victory. The retreating Arabs were not hindered or attacked on their return, but their fleet lost more ships in a storm in the Marmara Sea while other ships were set afire by ashes from the volcano of Santorini, and some of the survivors were captured by the Byzantines, so that Theophanes claims that only five vessels made it back to Syria.[42] Arab sources claim that altogether 150,000 Muslims perished during the campaign, a figure which, according to the Byzantinist John Haldon, \\"while certainly inflated, is nevertheless indicative of the enormity of the disaster in medieval eyes\\".[43]\\r\\nThe expedition's failure weakened the Umayyad state. As historian Bernard Lewis commented, \\"Its failure brought a grave moment for Umayyad power. The financial strain of equipping and maintaining the expedition caused an aggravation of the fiscal and financial oppression which had already aroused such dangerous opposition. The destruction of the fleet and army of Syria at the sea walls of Constantinople deprived the regime of the chief material basis of its power\\".[44] The blow to the Caliphate's might was severe, and although the land army did not suffer losses in the same degree as the fleet, Umar is recorded as contemplating withdrawing from the recent conquests of Hispania and Transoxiana, as well as a complete evacuation of Cilicia and other Byzantine territories that the Arabs had seized over the previous years. Although his advisors dissuaded him from such drastic actions, most Arab garrisons were withdrawn from the Byzantine frontier districts they had occupied in the lead-up to the siege. In Cilicia, only Mopsuestia remained in Arab hands as a defensive bulwark to protect Antioch.[45] The Byzantines even recovered some territory in western Armenia for a time. In 719, the Byzantine fleet raided the Syrian coast and burned down the port of Laodicea and, in 720 or 721, the Byzantines attacked and sacked Tinnis in Egypt.[46] Leo also restored control over Sicily, where news of the Arab siege of Constantinople and expectations of the city's fall had prompted the local governor to declare an emperor of his own, Basil Onomagoulos. It was during this time, however, that effective Byzantine control over Sardinia and Corsica ceased.[47] Besides this, the Byzantines failed to exploit their success in launching attacks of their own against the Arabs. In 720, after a hiatus of two years, Arab raids against Byzantium resumed, although now they were no longer directed at conquest, but rather seeking booty. The Arab attacks would intensify again over the next two decades, until the major Byzantine victory at the Battle of Akroinon in 740. Coupled with military defeats on the other fronts of the overextended Caliphate, and the internal instability which culminated in the Abbasid Revolution, the age of Arab expansion came to an end.[48]\\r\\nThe second Arab siege of Constantinople was far more dangerous for Byzantium than the first as, unlike the loose blockade of 674ÿ678, the Arabs launched a direct, well-planned attack on the Byzantine capital, and tried to cut off the city completely from land and sea.[30] The siege represented a final effort by the Caliphate to \\"cut off the head\\" of the Byzantine Empire, after which the remaining provinces, especially in Asia Minor, would be easy to capture.[49] The Arab failure was chiefly logistical, as they were operating too far from their Syrian bases, but the superiority of the Byzantine navy through the use of Greek fire, the strength of Constantinople's fortifications, and the skill of Leo III in deception and negotiations also played important roles.[50]\\r\\nThe failure of the Arab siege led to a profound change in the nature of warfare between Byzantium and the Caliphate. The Muslim goal of conquest of Constantinople was effectively abandoned, and the frontier between the two empires stabilized along the line of the Taurus and Antitaurus Mountains, over which both sides continued to launch regular raids and counter-raids. In this incessant border warfare, frontier towns and fortresses changed hands frequently, but the general outline of the border remained unaltered for over two centuries, until the Byzantine conquests of the 10th century.[51] The eastern fleets of the Caliphate entered a century-long decline; only the Ifriqiyan fleets maintained regular raids on Byzantine Sicily, until they too subsided after 752.[52] Indeed, with the exception of the advance of the Abbasid army under Harun al-Rashid up to Chrysopolis in 782, no other Arab army would ever come within sight of the Byzantine capital again.[53] Consequently, on the Muslim side the raids themselves eventually acquired an almost ritual character, and were valued mostly as a demonstration of the continuing jihad and sponsored by the Caliph as a symbol of his role as the leader of the Muslim community.[54]\\r\\nThe outcome of the siege was of considerable macrohistorical importance. The Byzantine capital's survival preserved the Empire as a bulwark against Islamic expansion into Europe until the 15th century, when it fell to the Ottoman Turks. Along with the Battle of Tours in 732, the successful defence of Constantinople has been seen as instrumental in stopping Muslim expansion into Europe. Historian Ekkehard Eickhoff writes that \\"had a victorious Caliph made Constantinople already at the beginning of the Middle Ages into the political capital of Islam, as happened at the end of the Middle Ages by the Ottomansthe consequences for Christian Europe [...] would have been incalculable\\", as the Mediterranean would have become an Arab lake, and the Germanic successor states in Western Europe would have been cut off from the Mediterranean roots of their culture.[55] Military historian Paul K. Davis summed the siege's importance as follows: \\"By turning back the Moslem invasion, Europe remained in Christian hands, and no serious Moslem threat to Europe existed until the fifteenth century. This victory, coincident with the Frankish victory at Tours (732), limited Islam's western expansion to the southern Mediterranean world.\\"[56] Thus the historian John B. Bury called 718 \\"an ecumenical date\\", while the Greek historian Spyridon Lambros likened the siege to the Battle of Marathon and Leo III to Miltiades.[57] Consequently, military historians often include the siege in lists of the \\"decisive battles\\" of world history.[58]\\r\\nAmong Arabs, the 717ÿ718 siege became the most famous of their expeditions against Byzantium. Several accounts survive, but most were composed at later dates and are semi-fictional and contradictory. In legend, the defeat was transformed into a victory: Maslama departed only after symbolically entering the Byzantine capital on his horse accompanied by thirty riders, where Leo received him with honour and led him to the Hagia Sophia. After Leo paid homage to Maslama and promised tribute, Maslama and his troops30,000 out of the original 80,000 that set out for Constantinopledeparted for Syria.[59] The tales of the siege influenced similar episodes in Arabic epic literature. A siege of Constantinople is found in the tale of Omar bin al-Nu'uman and his sons in the Thousand and One Nights, while both Maslama and the Caliph Sulayman appear in a tale of the Hundred and One Nights from the Maghreb. The commander of Maslama's bodyguard, Abdallah al-Battal, became a celebrated figure in Arab and Turkish poetry as \\"Battal Gazi\\" for his exploits in the Arab raids of the next decades. Similarly, the 10th-century epic Delhemma, related to the cycle around Battal, gives a fictionalized version of the 717ÿ718 siege.[60]\\r\\nLater Muslim and Byzantine tradition also ascribed the building of Constantinople's first mosque, near the city's praetorium, to Maslama. In reality, the mosque near the praetorium was probably erected in about 860, as a result of an Arab embassy in that year.[61] Ottoman tradition also ascribed the building of the Arap Mosque (located outside Constantinople proper in Galata) to Maslama, although it erroneously dated this to around 686, probably confusing Maslama's attack with the first Arab siege in the 670s.[62] The passing of the Arab army also left traces at Abydos, where \\"Maslama's Well\\" and a mosque attributed to him were still known in the 10th century.[53]\\r\\nEventually, following their repeated failures before Constantinople, and the continued resilience of the Byzantine state, the Muslims began to project the fall of Constantinople to the distant future. Thus the city's fall came to be regarded as one of the signs of the arrival of the end times in Islamic eschatology.[63] The siege became a motif in Byzantine apocalyptic literature as well, with decisive final battles against the Arabs before the walls of Constantinople being featured in the early 8th-century Greek translation of the Syriac Apocalypse of Pseudo-Methodius and the Apocalypse of Daniel, written either at about the time of the siege or a century later.[64]\\r\\n^?a:?Theophanes the Confessor gives the date as 15 August, but modern scholars believe that this is probably meant to mirror the Arabs' departure date in the next year. Patriarch Nikephoros I on the other hand explicitly records the duration of the siege as 13 months, implying that it began on 15 July.[65]\\r\\n^?b:?According to the historian Hugh N. Kennedy, based on the numbers found in the contemporary army registers (diwans), the total manpower available to the Umayyad Caliphate in ca. 700 ranged between 250,000 and 300,000 men, spread throughout the various provinces. It is unclear, however, what portion of this number could actually be fielded for any particular campaign, and does not account for surplus manpower that could be mobilized in exceptional circumstances.[66]\\r\\nCoordinates: 410044N 285834E? / ?41.01224N 28.976018E? / 41.01224; 28.976018","input":"What helped protect constantinople from invasion by land?"},{"output":"celebrate the centennial of the French Revolution","context":"The Eiffel Tower (/?a?f?l ?ta?.?r/ EYE-f?l TOW-?r; French: tour Eiffel, pronounced?[tu???f?l] ?listen) is a wrought iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\\r\\nConstructed from 1887ÿ89 as the entrance to the 1889 World's Fair, it was initially criticized by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France and one of the most recognisable structures in the world.[3] The Eiffel Tower is the most-visited paid monument in the world; 6.91?million people ascended it in 2015.\\r\\nThe tower is 324 metres (1,063?ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410?ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17?ft). Excluding transmitters, the Eiffel Tower is the second-tallest structure in France after the Millau Viaduct.\\r\\nThe tower has three levels for visitors, with restaurants on the first and second levels. The top level's upper platform is 276?m (906?ft) above the ground ÿ the highest observation deck accessible to the public in the European Union. Tickets can be purchased to ascend by stairs or lift (elevator) to the first and second levels. The climb from ground level to the first level is over 300 steps, as is the climb from the first level to the second. Although there is a staircase to the top level, it is usually accessible only by lift.\\r\\n\\r\\n\\r\\nThe design of the Eiffel Tower was the product of Maurice Koechlin and mile Nouguier, two senior engineers working for the Compagnie des tablissements Eiffel, after discussion about a suitable centrepiece for the proposed 1889 Exposition Universelle, a world's fair to celebrate the centennial of the French Revolution. Eiffel openly acknowledged that inspiration for a tower came from the Latting Observatory built in New York City in 1853.[4] In May 1884, working at home, Koechlin made a sketch of their idea, described by him as \\"a great pylon, consisting of four lattice girders standing apart at the base and coming together at the top, joined together by metal trusses at regular intervals\\".[5] Eiffel initially showed little enthusiasm, but he did approve further study, and the two engineers then asked Stephen Sauvestre, the head of company's architectural department, to contribute to the design. Sauvestre added decorative arches to the base of the tower, a glass pavilion to the first level, and other embellishments.\\r\\nThe new version gained Eiffel's support: he bought the rights to the patent on the design which Koechlin, Nougier, and Sauvestre had taken out, and the design was exhibited at the Exhibition of Decorative Arts in the autumn of 1884 under the company name. On 30 March 1885, Eiffel presented his plans to the Socit des Ingnieurs Civils; after discussing the technical problems and emphasising the practical uses of the tower, he finished his talk by saying the tower would symbolise,\\r\\nNot only the art of the modern engineer, but also the century of Industry and Science in which we are living, and for which the way was prepared by the great scientific movement of the eighteenth century and by the Revolution of 1789, to which this monument will be built as an expression of France's gratitude.[6]\\r\\nLittle progress was made until 1886, when Jules Grvy was re-elected as president of France and douard Lockroy was appointed as minister for trade. A budget for the exposition was passed and, on 1 May, Lockroy announced an alteration to the terms of the open competition being held for a centrepiece to the exposition, which effectively made the selection of Eiffel's design a foregone conclusion, as entries had to include a study for a 300?m (980?ft) four-sided metal tower on the Champ de Mars.[6] (A 300-meter tower was then considered a herculean engineering effort). On 12 May, a commission was set up to examine Eiffel's scheme and its rivals, which, a month later, decided that all the proposals except Eiffel's were either impractical or lacking in details.\\r\\nAfter some debate about the exact location of the tower, a contract was signed on 8 January 1887. This was signed by Eiffel acting in his own capacity rather than as the representative of his company, and granted him 1.5 million francs toward the construction costs: less than a quarter of the estimated 6.5 million francs. Eiffel was to receive all income from the commercial exploitation of the tower during the exhibition and for the next 20 years. He later established a separate company to manage the tower, putting up half the necessary capital himself.[7]\\r\\nThe proposed tower had been a subject of controversy, drawing criticism from those who did not believe it was feasible and those who objected on artistic grounds. These objections were an expression of a long-standing debate in France about the relationship between architecture and engineering. It came to a head as work began at the Champ de Mars: a \\"Committee of Three Hundred\\" (one member for each metre of the tower's height) was formed, led by the prominent architect Charles Garnier and including some of the most important figures of the arts, such as Adolphe Bouguereau, Guy de Maupassant, Charles Gounod and Jules Massenet. A petition called \\"Artists against the Eiffel Tower\\" was sent to the Minister of Works and Commissioner for the Exposition, Charles Alphand, and it was published by Le Temps on 14 February 1887:\\r\\nWe, writers, painters, sculptors, architects and passionate devotees of the hitherto untouched beauty of Paris, protest with all our strength, with all our indignation in the name of slighted French taste, against the erection  of this useless and monstrous Eiffel Tower  To bring our arguments home, imagine for a moment a giddy, ridiculous tower dominating Paris like a gigantic black smokestack, crushing under its barbaric bulk Notre Dame, the Tour Saint-Jacques, the Louvre, the Dome of les Invalides, the Arc de Triomphe, all of our humiliated monuments will disappear in this ghastly dream. And for twenty years  we shall see stretching like a blot of ink the hateful shadow of the hateful column of bolted sheet metal.[8]\\r\\nGustave Eiffel responded to these criticisms by comparing his tower to the Egyptian pyramids: \\"My tower will be the tallest edifice ever erected by man. Will it not also be grandiose in its way? And why would something admirable in Egypt become hideous and ridiculous in Paris?\\"[9] These criticisms were also dealt with by douard Lockroy in a letter of support written to Alphand, ironically saying,[10] \\"Judging by the stately swell of the rhythms, the beauty of the metaphors, the elegance of its delicate and precise style, one can tell this protest is the result of collaboration of the most famous writers and poets of our time\\", and he explained that the protest was irrelevant since the project had been decided upon months before, and construction on the tower was already under way.\\r\\nIndeed, Garnier was a member of the Tower Commission that had examined the various proposals, and had raised no objection. Eiffel was similarly unworried, pointing out to a journalist that it was premature to judge the effect of the tower solely on the basis of the drawings, that the Champ de Mars was distant enough from the monuments mentioned in the protest for there to be little risk of the tower overwhelming them, and putting the aesthetic argument for the tower: \\"Do not the laws of natural forces always conform to the secret laws of harmony?\\"[11]\\r\\nSome of the protesters changed their minds when the tower was built; others remained unconvinced.[12] Guy de Maupassant supposedly ate lunch in the tower's restaurant every day because it was the one place in Paris where the tower was not visible.[13]\\r\\nBy 1918, it had become a symbol of Paris and of France after Guillaume Apollinaire wrote a nationalist poem in the shape of the tower (a calligram) to express his feelings about the war against Germany.[14] Today, it is widely considered to be a remarkable piece of structural art, and is often featured in films and literature.\\r\\nWork on the foundations started on 28 January 1887.[15] Those for the east and south legs were straightforward, with each leg resting on four 2?m (6.6?ft) concrete slabs, one for each of the principal girders of each leg. The west and north legs, being closer to the river Seine, were more complicated: each slab needed two piles installed by using compressed-air caissons 15?m (49?ft) long and 6?m (20?ft) in diameter driven to a depth of 22?m (72?ft)[16] to support the concrete slabs, which were 6?m (20?ft) thick. Each of these slabs supported a block of limestone with an inclined top to bear a supporting shoe for the ironwork.\\r\\nEach shoe was anchored to the stonework by a pair of bolts 10?cm (4?in) in diameter and 7.5?m (25?ft) long. The foundations were completed on 30 June, and the erection of the ironwork began. The visible work on-site was complemented by the enormous amount of exacting preparatory work that took place behind the scenes: the drawing office produced 1,700 general drawings and 3,629 detailed drawings of the 18,038 different parts needed.[17] The task of drawing the components was complicated by the complex angles involved in the design and the degree of precision required: the position of rivet holes was specified to within 0.1?mm (0.0039?in) and angles worked out to one second of arc. The finished components, some already riveted together into sub-assemblies, arrived on horse-drawn carts from a factory in the nearby Parisian suburb of Levallois-Perret and were first bolted together, with the bolts being replaced with rivets as construction progressed. No drilling or shaping was done on site: if any part did not fit, it was sent back to the factory for alteration. In all, 18,038 pieces were joined together using 2.5?million rivets.[15]\\r\\nAt first the legs were constructed as cantilevers, but about halfway to the first level, construction was paused in order to create a substantial timber scaffold. This renewed concerns about the structural integrity of the tower, and sensational headlines such as \\"Eiffel Suicide!\\" and \\"Gustave Eiffel Has Gone Mad: He Has Been Confined in an Asylum\\" appeared in the tabloid press.[18] At this stage, a small \\"creeper\\" crane designed to move up the tower was installed in each leg. They made use of the guides for the lifts which were to be fitted in the four legs. The critical stage of joining the legs at the first level was completed by the end of March 1888.[15] Although the metalwork had been prepared with the utmost attention to detail, provision had been made to carry out small adjustments in order to precisely align the legs; hydraulic jacks were fitted to the shoes at the base of each leg, capable of exerting a force of 800 tonnes, and the legs were intentionally constructed at a slightly steeper angle than necessary, being supported by sandboxes on the scaffold. Although construction involved 300 on-site employees,[15] only one person died thanks to Eiffel's stringent safety precautions and the use of movable gangways, guardrails and screens.\\r\\nThe start of the erection of the metalwork.\\r\\n7 December 1887: Construction of the legs with scaffolding.\\r\\n20 March 1888: Completion of the first level.\\r\\n15 May 1888: Start of construction on the second stage.\\r\\n21 August 1888: Completion of the second level.\\r\\n26 December 1888: Construction of the upper stage.\\r\\n15 March 1889: Construction of the cupola.\\r\\nEquipping the tower with adequate and safe passenger lifts was a major concern of the government commission overseeing the Exposition. Although some visitors could be expected to climb to the first level, or even the second, lifts clearly had to be the main means of ascent.[19]\\r\\nConstructing lifts to reach the first level was relatively straightforward: the legs were wide enough at the bottom and so nearly straight that they could contain a straight track, and a contract was given to the French company Roux, Combaluzier & Lepape for two lifts to be fitted in the east and west legs.[20] Roux, Combaluzier & Lepape used a pair of endless chains with rigid, articulated links to which the car was attached. Lead weights on some links of the upper or return sections of the chains counterbalanced most of the car's weight. The car was pushed up from below, not pulled up from above: to prevent the chain buckling, it was enclosed in a conduit. At the bottom of the run, the chains passed around 3.9?m (12?ft 10?in) diameter sprockets. Smaller sprockets at the top guided the chains.[20]\\r\\nInstalling lifts to the second level was more of a challenge because a straight track was impossible. No French company wanted to undertake the work. The European branch of Otis Brothers & Company submitted a proposal but this was rejected: the fair's charter ruled out the use of any foreign material in the construction of the tower. The deadline for bids was extended but still no French companies put themselves forward, and eventually the contract was given to Otis in July 1887.[21] Otis were confident they would eventually be given the contract and had already started creating designs.\\r\\nThe car was divided into two superimposed compartments, each holding 25 passengers, with the lift operator occupying an exterior platform on the first level. Motive power was provided by an inclined hydraulic ram 12.67?m (41?ft 7?in) long and 96.5?cm (38.0?in) in diameter in the tower leg with a stroke of 10.83?m (35?ft 6?in): this moved a carriage carrying six sheaves. Five fixed sheaves were mounted higher up the leg, producing an arrangement similar to a block and tackle but acting in reverse, multiplying the stroke of the piston rather than the force generated. The hydraulic pressure in the driving cylinder was produced by a large open reservoir on the second level. After being exhausted from the cylinder, the water was pumped back up to the reservoir by two pumps in the machinery room at the base of the south leg. This reservoir also provided power to the lifts to the first level.\\r\\nThe original lifts for the journey between the second and third levels were supplied by Lon Edoux. A pair of 81?m (266?ft) hydraulic rams were mounted on the second level, reaching nearly halfway up to the third level. One lift car was mounted on top of these rams: cables ran from the top of this car up to sheaves on the third level and back down to a second car. Each car only travelled half the distance between the second and third levels and passengers were required to change lifts halfway by means of a short gangway. The 10-ton cars each held 65 passengers.[22]\\r\\nThe main structural work was completed at the end of March 1889 and, on 31 March, Eiffel celebrated by leading a group of government officials, accompanied by representatives of the press, to the top of the tower.[12] Because the lifts were not yet in operation, the ascent was made by foot, and took over an hour, with Eiffel stopping frequently to explain various features. Most of the party chose to stop at the lower levels, but a few, including the structural engineer, mile Nouguier, the head of construction, Jean Compagnon, the President of the City Council, and reporters from Le Figaro and Le Monde Illustr, completed the ascent. At 2:35?pm, Eiffel hoisted a large Tricolour to the accompaniment of a 25-gun salute fired at the first level.[23]\\r\\nThere was still work to be done, particularly on the lifts and facilities, and the tower was not opened to the public until nine days after the opening of the exposition on 6 May; even then, the lifts had not been completed. The tower was an instant success with the public, and nearly 30,000 visitors made the 1,710-step climb to the top before the lifts entered service on 26 May.[24] Tickets cost 2 francs for the first level, 3 for the second, and 5 for the top, with half-price admission on Sundays,[25] and by the end of the exhibition there had been 1,896,987 visitors.[3]\\r\\nAfter dark, the tower was lit by hundreds of gas lamps, and a beacon sent out three beams of red, white and blue light. Two searchlights mounted on a circular rail were used to illuminate various buildings of the exposition. The daily opening and closing of the exposition were announced by a cannon at the top.\\r\\nOn the second level, the French newspaper Le Figaro had an office and a printing press, where a special souvenir edition, Le Figaro de la Tour, was made. There was also a patisserie.\\r\\nAt the top, there was a post office where visitors could send letters and postcards as a memento of their visit. Graffitists were also catered for: sheets of paper were mounted on the walls each day for visitors to record their impressions of the tower. Gustave Eiffel described some of the responses as vraiment curieuse (\\"truly curious\\").[26]\\r\\nFamous visitors to the tower included the Prince of Wales, Sarah Bernhardt, \\"Buffalo Bill\\" Cody (his Wild West show was an attraction at the exposition) and Thomas Edison.[24] Eiffel invited Edison to his private apartment at the top of the tower, where Edison presented him with one of his phonographs, a new invention and one of the many highlights of the exposition.[27] Edison signed the guestbook with this message:\\r\\nTo M Eiffel the Engineer the brave builder of so gigantic and original specimen of modern Engineering from one who has the greatest respect and admiration for all Engineers including the Great Engineer the Bon Dieu, Thomas Edison.\\r\\nEiffel had a permit for the tower to stand for 20 years. It was to be dismantled in 1909, when its ownership would revert to the City of Paris. The City had planned to tear it down (part of the original contest rules for designing a tower was that it should be easy to dismantle) but as the tower proved to be valuable for communication purposes, it was allowed to remain after the expiry of the permit.\\r\\nEiffel made use of his apartment at the top of the tower to carry out meteorological observations, and also used the tower to perform experiments on the action of air resistance on falling bodies.[28]\\r\\nFor the 1900 Exposition Universelle, the lifts in the east and west legs were replaced by lifts running as far as the second level constructed by the French firm Fives-Lille. These had a compensating mechanism to keep the floor level as the angle of ascent changed at the first level, and were driven by a similar hydraulic mechanism to the Otis lifts, although this was situated at the base of the tower. Hydraulic pressure was provided by pressurised accumulators located near this mechanism.[21] At the same time the lift in the north pillar was removed and replaced by a staircase to the first level. The layout of both first and second levels was modified, with the space available for visitors on the second level. The original lift in the south pillar was removed 13 years later.\\r\\nOn 19 October 1901, Alberto Santos-Dumont, flying his No.6 airship, won a 100,000-franc prize offered by Henri Deutsch de la Meurthe for the first person to make a flight from St. Cloud to the Eiffel Tower and back in less than half an hour.[29]\\r\\nMany innovations took place at the Eiffel Tower in the early 20th century. In 1910, Father Theodor Wulf measured radiant energy at the top and bottom of the tower. He found more at the top than expected, incidentally discovering what are known today as cosmic rays.[30] Just two years later, on 4 February 1912, Austrian tailor Franz Reichelt died after jumping from the first level of the tower (a height of 57 metres) to demonstrate his parachute design.[31] In 1914, at the outbreak of World War?I, a radio transmitter located in the tower jammed German radio communications, seriously hindering their advance on Paris and contributing to the Allied victory at the First Battle of the Marne.[32] From 1925 to 1934, illuminated signs for Citro?n adorned three of the tower's sides, making it the tallest advertising space in the world at the time.[citation needed] In April 1935, the tower was used to make experimental low-resolution television transmissions, using a shortwave transmitter of 200 watts power. On 17 November, an improved 180-line transmitter was installed.[33]\\r\\nOn two separate but related occasions in 1925, the con artist Victor Lustig \\"sold\\" the tower for scrap metal.[34] A year later, in February 1926, pilot Leon Collet was killed trying to fly under the tower. His aircraft became entangled in an aerial belonging to a wireless station.[35] A bust of Gustave Eiffel by Antoine Bourdelle was unveiled at the base of the north leg on 2 May 1929.[36] In 1930, the tower lost the title of the world's tallest structure when the Chrysler Building in New York City was completed.[37] In 1938, the decorative arcade around the first level was removed.[38]\\r\\nUpon the German occupation of Paris in 1940, the lift cables were cut by the French. The tower was closed to the public during the occupation and the lifts were not repaired until 1946.[39] In 1940, German soldiers had to climb the tower to hoist a swastika-centered Reichskriegsflagge,[40] but the flag was so large it blew away just a few hours later, and was replaced by a smaller one.[41] When visiting Paris, Hitler chose to stay on the ground. When the Allies were nearing Paris in August 1944, Hitler ordered General Dietrich von Choltitz, the military governor of Paris, to demolish the tower along with the rest of the city. Von Choltitz disobeyed the order.[42] On 25 June, before the Germans had been driven out of Paris, the German flag was replaced with a Tricolour by two men from the French Naval Museum, who narrowly beat three men led by Lucien Sarniguet, who had lowered the Tricolour on 13 June 1940 when Paris fell to the Germans.[39]\\r\\nA fire started in the television transmitter on 3 January 1956, damaging the top of the tower. Repairs took a year, and in 1957, the present radio aerial was added to the top.[43] In 1964, the Eiffel Tower was officially declared to be a historical monument by the Minister of Cultural Affairs, Andr Malraux.[44] A year later, an additional lift system was installed in the north pillar.[45]\\r\\nAccording to interviews, in 1967, Montreal Mayor Jean Drapeau negotiated a secret agreement with Charles de Gaulle for the tower to be dismantled and temporarily relocated to Montreal to serve as a landmark and tourist attraction during Expo 67. The plan was allegedly vetoed by the company operating the tower out of fear that the French government could refuse permission for the tower to be restored in its original location.[46]\\r\\nIn 1982, the original lifts between the second and third levels were replaced after 97 years in service. These had been closed to the public between November and March because the water in the hydraulic drive tended to freeze. The new cars operate in pairs, with one counterbalancing the other, and perform the journey in one stage, reducing the journey time from eight minutes to less than two minutes. At the same time, two new emergency staircases were installed, replacing the original spiral staircases. In 1983, the south pillar was fitted with an electrically driven Otis lift to serve the Jules Verne restaurant.[citation needed] The Fives-Lille lifts in the east and west legs, fitted in 1899, were extensively refurbished in 1986. The cars were replaced, and a computer system was installed to completely automate the lifts. The motive power was moved from the water hydraulic system to a new electrically driven oil-filled hydraulic system, and the original water hydraulics were retained solely as a counterbalance system.[45] A service lift was added to the south pillar for moving small loads and maintenance personnel three years later.\\r\\nRobert Moriarty flew a Beechcraft Bonanza under the tower on 31 March 1984.[47] In 1987, A.J. Hackett made one of his first bungee jumps from the top of the Eiffel Tower, using a special cord he had helped develop. Hackett was arrested by the police.[48] On 27 October 1991, Thierry Devaux, along with mountain guide Herv Calvayrac, performed a series of acrobatic figures while bungee jumping from the second floor of the tower.[49] Facing the Champ de Mars, Devaux used an electric winch between figures to go back up to the second floor. When firemen arrived, he stopped after the sixth jump.[citation needed]\\r\\nFor its \\"Countdown to the Year 2000\\" celebration on 31 December 1999, flashing lights and high-powered searchlights were installed on the tower. Fireworks were set off all over it. An exhibition above a cafeteria on the first floor commemorates this event. The searchlights on top of the tower made it a beacon in Paris's night sky, and 20,000 flashing bulbs gave the tower a sparkly appearance for five minutes every hour on the hour.[50]\\r\\nThe lights sparkled blue for several nights to herald the new millennium On 31 December 2000. The sparkly lighting continued for 18 months until July 2001. The sparkling lights were turned on again on 21 June 2003, and the display was planned to last for 10 years before they needed replacing.[51]\\r\\nThe tower received its 200,000,000th guest on 28 November 2002.[52] The tower has operated at its maximum capacity of about 7?million visitors since 2003.[53] In 2004, the Eiffel Tower began hosting a seasonal ice rink on the first level.[54] A glass floor was installed on the first level during the 2014 refurbishment.[55]\\r\\nThe puddled iron (wrought iron) of the Eiffel Tower weighs 7,300 tons,[56] and the addition of lifts, shops and antennae have brought the total weight to approximately 10,100?tons.[57] As a demonstration of the economy of design, if the 7,300?tons of metal in the structure were melted down, it would fill the square base, 125 metres (410?ft) on each side, to a depth of only 6.25?cm (2.46?in) assuming the density of the metal to be 7.8?tons per cubic metre.[58] Additionally, a cubic box surrounding the tower (324?m x 125?m x 125?m) would contain 6,200?tons of air, weighing almost as much as the iron itself. Depending on the ambient temperature, the top of the tower may shift away from the sun by up to 18?cm (7?in) due to thermal expansion of the metal on the side facing the sun.[59]\\r\\nWhen it was built, many were shocked by the tower's daring form. Eiffel was accused of trying to create something artistic with no regard to the principles of engineering. However, Eiffel and his team ÿ experienced bridge builders ÿ understood the importance of wind forces, and knew that if they were going to build the tallest structure in the world, they had to be sure it could withstand them. In an interview with the newspaper Le Temps published on 14 February 1887, Eiffel said:\\r\\nIs it not true that the very conditions which give strength also conform to the hidden rules of harmony?  Now to what phenomenon did I have to give primary concern in designing the Tower? It was wind resistance. Well then! I hold that the curvature of the monument's four outer edges, which is as mathematical calculation dictated it should be  will give a great impression of strength and beauty, for it will reveal to the eyes of the observer the boldness of the design as a whole.[60]\\r\\nHe used graphical methods to determine the strength of the tower and empirical evidence to account for the effects of wind, rather than a mathematical formula. Close examination of the tower reveals a basically exponential shape.[61] All parts of the tower were over-designed to ensure maximum resistance to wind forces. The top half was even assumed to have no gaps in the latticework.[62] In the years since it was completed, engineers have put forward various mathematical hypotheses in an attempt to explain the success of the design. The most recent, devised in 2004 after letters sent by Eiffel to the French Society of Civil Engineers in 1885 were translated into English, is described as a non-linear integral equation based on counteracting the wind pressure on any point of the tower with the tension between the construction elements at that point.[61]\\r\\nThe Eiffel Tower sways by up to 9?centimetres (3.5?in) in the wind.[63]\\r\\nWhen originally built, the first level contained three restaurantsone French, one Russian and one Flemishand an \\"Anglo-American Bar\\". After the exposition closed, the Flemish restaurant was converted to a 250-seat theatre. A promenade 2.6-metre (8?ft 6?in) wide ran around the outside of the first level. At the top, there were laboratories for various experiments, and a small apartment reserved for Gustave Eiffel to entertain guests, which is now open to the public, complete with period decorations and lifelike mannequins of Eiffel and some of his notable guests.[64]\\r\\nIn May 2016, an apartment was created on the first level to accommodate four competition winners during the UEFA Euro 2016 football tournament in Paris in June. The apartment has a kitchen, two bedrooms, a lounge, and views of Paris landmarks including the Seine, the Sacre Coeur, and the Arc de Triomphe.[65]\\r\\nThe arrangement of the lifts has been changed several times during the tower's history. Given the elasticity of the cables and the time taken to align the cars with the landings, each lift, in normal service, takes an average of 8 minutes and 50 seconds to do the round trip, spending an average of 1 minute and 15 seconds at each level. The average journey time between levels is 1 minute. The original hydraulic mechanism is on public display in a small museum at the base of the east and west legs. Because the mechanism requires frequent lubrication and maintenance, public access is often restricted. The rope mechanism of the north tower can be seen as visitors exit the lift.[citation needed]\\r\\nGustave Eiffel engraved on the tower the names of 72 French scientists, engineers and mathematicians in recognition of their contributions to the building of the tower. Eiffel chose this \\"invocation of science\\" because of his concern over the artists' protest. At the beginning of the 20th century, the engravings were painted over, but they were restored in 1986ÿ87 by the Socit Nouvelle d'exploitation de la Tour Eiffel, a company operating the tower.[66]\\r\\nThe tower is painted in three shades: lighter at the top, getting progressively darker towards the bottom to perfectly complement the Parisian sky.[67] It was originally reddish brown; this changed in 1968 to a bronze colour known as \\"Eiffel Tower Brown\\".[68]\\r\\nThe only non-structural elements are the four decorative grill-work arches, added in Sauvestre's sketches, which served to make the tower look more substantial and to make a more impressive entrance to the exposition.[69]\\r\\nOne of the great Hollywood movie clichs is that the view from a Parisian window always includes the tower. In reality, since zoning restrictions limit the height of most buildings in Paris to seven storeys, only a small number of tall buildings have a clear view of the tower.[citation needed]\\r\\nMaintenance of the tower includes applying 60?tons of paint every seven years to prevent it from rusting. The tower has been completely repainted at least 19 times since it was built. Lead paint was still being used as recently as 2001 when the practice was stopped out of concern for the environment.[51]\\r\\nThe nearest Paris Mtro station is Bir-Hakeim and the nearest RER station is Champ de Mars-Tour Eiffel.[70] The tower itself is located at the intersection of the quai Branly and the Pont d'Ina.\\r\\nMore than 250?million people have visited the tower since it was completed in 1889.[3] In 2015, there were 6.91?million visitors.[71] The tower is the most-visited paid monument in the world.[72] An average of 25,000 people ascend the tower every day which can result in long queues.[73] Tickets can be purchased online to avoid the long queues.\\r\\nThe tower has two restaurants: Le 58 Tour Eiffel on the first level, and Le Jules Verne, a gourmet restaurant with its own lift on the second level. This restaurant has one star in the Michelin Red Guide. It is run by the multi-Michelin star chef Alain Ducasse[74] and owes its name to the famous science-fiction writer Jules Verne. Additionally, there is a champagne bar at the top of the Eiffel Tower.\\r\\nAs one of the most iconic landmarks in the world, the Eiffel Tower has been the inspiration for the creation of many replicas and similar towers. An early example is Blackpool Tower in England. The mayor of Blackpool, Sir John Bickerstaffe, was so impressed on seeing the Eiffel Tower at the 1889 exposition that he commissioned a similar tower to be built in his town. It opened in 1894 and is 158.1?metres (518?ft) tall.[75] Tokyo Tower in Japan, built as a communications tower in 1958, was also inspired by the Eiffel Tower.[76]\\r\\nThere are various scale models of the tower in the United States, including a half-scale version at the Paris Las Vegas, Nevada, one in Paris, Texas built in 1993, and two 1:3 scale models at Kings Island, Ohio, and Kings Dominion, Virginia, amusement parks opened in 1972 and 1975 respectively. Two 1:3 scale models can be found in China, one in Durango, Mexico that was donated by the local French community, and several across Europe.[77]\\r\\nIn 2011, the TV show Pricing the Priceless on the National Geographic Channel speculated that a full-size replica of the tower would cost approximately US$480?million to build.[78]\\r\\n\\r\\nThe tower has been used for making radio transmissions since the beginning of the 20th century. Until the 1950s, sets of aerial wires ran from the cupola to anchors on the Avenue de Suffren and Champ de Mars. These were connected to longwave transmitters in small bunkers. In 1909, a permanent underground radio centre was built near the south pillar, which still exists today. On 20 November 1913, the Paris Observatory, using the Eiffel Tower as an aerial, exchanged wireless signals with the United States Naval Observatory, which used an aerial in Arlington, Virginia. The object of the transmissions was to measure the difference in longitude between Paris and Washington, D.C.[79] Today, radio and digital television signals are transmitted from the Eiffel Tower.\\r\\nA television antenna was first installed on the tower in 1957, increasing its height by 18.7?m (61.4?ft). Work carried out in 2000 added a further 5.3?m (17.4?ft), giving the current height of 324?m (1,063?ft).[51] Analogue television signals from the Eiffel Tower ceased on 8 March 2011.\\r\\nThe tower and its image have long been in the public domain.[80] In June 1990 a French court ruled that a special lighting display on the tower in 1989 to mark the tower's 100th anniversary was an \\"original visual creation\\" protected by copyright. The Court of Cassation, France's judicial court of last resort, upheld the ruling in March 1992.[81] The Socit d'Exploitation de la Tour Eiffel (SETE) now considers any illumination of the tower to be a separate work of art that falls under copyright.[82] As a result, the SNTE alleges that it is illegal to publish contemporary photographs of the lit tower at night without permission in France and some other countries for commercial use.[83][84]\\r\\nThe imposition of copyright has been controversial. The Director of Documentation for what was then called the Socit Nouvelle d'exploitation de la Tour Eiffel (SNTE), Stphane Dieu, commented in 2005: \\"It is really just a way to manage commercial use of the image, so that it isn't used in ways [of which] we don't approve\\".[85] SNTE made over ?1?million from copyright fees in 2002.[86] However, it could also be used to restrict the publication of tourist photographs of the tower at night, as well as hindering non-profit and semi-commercial publication of images of the illuminated tower.[80]\\r\\nFrench doctrine and jurisprudence allows pictures incorporating a copyrighted work as long as their presence is incidental or accessory to the subject being represented,[87] a reasoning akin to the de minimis rule. Therefore, SETE may be unable to claim copyright on photographs of Paris which happen to include the lit tower.\\r\\nThe Eiffel Tower was the world's tallest structure when completed in 1889, a distinction it retained until 1929 when the Chrysler Building in New York City was topped out.[88] The tower has lost its standing both as the world's tallest structure and the world's tallest lattice tower but retains its status as the tallest freestanding (non-guyed) structure in France.","input":"What was the reason for the eiffel tower?"},{"output":"Sandra Day O'Connor","context":"\\r\\n\\r\\nSandra Day O'Connor (born March 26, 1930) is a retired Associate Justice of the Supreme Court of the United States, having served from her appointment in 1981 by Ronald Reagan until 2006. She is the first woman to have served on the Court.[5]\\r\\n\\r\\nPrior to O'Connor's tenure on the Court, she was an elected official and judge in Arizona serving as the first female Majority Leader of a state senate as the Republican leader in the Arizona Senate.[6] Upon her nomination to the Court, O'Connor was confirmed unanimously by the Senate. On July 1, 2005, she announced her intention to retire effective upon the confirmation of a successor.[7] Samuel Alito was nominated to take her seat in October 2005, and joined the Court on January 31, 2006.\\r\\n\\r\\nConsidered a federalist and a moderate Republican, O'Connor tended to approach each case narrowly without arguing for sweeping precedents. She most frequently sided with the Court's conservative bloc, although in the latter years of her tenure, she was regarded as having the swing opinion in many cases. She often wrote concurring opinions that limited the reach of the majority holding. Her majority opinions in landmark cases include Grutter v. Bollinger and Hamdi v. Rumsfeld. She also wrote in part the per curiam majority opinion in Bush v. Gore, and was one of three co-authors of the lead opinion in Planned Parenthood v. Casey.\\r\\n\\r\\nO'Connor was Chancellor of The College of William & Mary in Williamsburg. Several publications have named her among the most powerful women in the world.[8][9] On August 12, 2009, she was awarded the Presidential Medal of Freedom, the highest civilian honor of the United States, by President Barack Obama.\\r\\n\\r\\nSandra Day was born in El Paso, Texas, the daughter of Harry Alfred Day, a rancher, and Ada Mae (Wilkey).[10] She grew up on a cattle ranch near Duncan, Arizona.[11] The ranch was nine miles from the nearest paved road.[12] The family home did not have running water nor electricity until Sandra was seven years old.[13] She hunted from a young age, using a .22-caliber rifle to shoot jackrabbits for food.[12] She began driving as soon as she could see over the dashboard and had to learn to change flat automobile tires herself.[11][12] Sandra had two younger siblings, a sister and a brother, respectively eight and ten years her junior.[13] Her sister was Ann Day, who served in the Arizona Legislature.[14] She later wrote a book with her brother, H. Alan Day, Lazy B: Growing up on a Cattle Ranch in the American West (2002), about her childhood experiences on the ranch. For most of her early schooling, O'Connor lived in El Paso with her maternal grandmother,[13] and attended school at the Radford School for Girls, a private school.[citation needed] The family cattle ranch was too far from schools, although O'Connor was able to return to the ranch for holidays and the summer.[13] O'Connor spent her eighth-grade year living at the ranch and riding a bus 32 miles to school.[13] She graduated sixth in her class at Austin High School in El Paso in 1946.[15]\\r\\n\\r\\nShe attended Stanford University, where she received her B.A. in Economics in 1950.[16] She continued at the Stanford Law School for her law degree in 1952.[16] There, she served on the Stanford Law Review with its presiding editor-in-chief, future Supreme Court Chief Justice William Rehnquist, who was the class valedictorian[17] and whom she briefly dated during law school.[18] She has stated that she graduated third in her law school class,[19] though Stanford's official position is that the law school did not rank students in 1952.[20]\\r\\n\\r\\nOn December 20, 1952, six months after graduating from law school, she married John Jay O'Connor III.[21] The pair had met as students at Stanford Law School.[12]\\r\\n\\r\\nShe eventually found employment as a deputy county attorney in San Mateo, California after she offered to work for no salary and without an office, sharing space with a secretary.[22] She worked with San Mateo County district attorney Louis Dematteis and deputy district attorney Keith Sorenson.[21]\\r\\n\\r\\nWhen her husband was drafted, she decided to pick up and go with him to work in Germany as a civilian attorney for the Army's Quartermaster Corps.[23] They remained there for three years before returning to the states where they settled in Maricopa County, Arizona to begin their family. They had three sons: Scott (born 1958), Brian (born 1960), and Jay (born 1962).[24][13] Following Brian's birth, O'Connor took a five-year hiatus from the practice of law.[13]\\r\\n\\r\\nShe volunteered in various political organizations like the Maricopa County Young Republicans and served on the presidential campaign for Arizona Senator Barry M. Goldwater in 1964.[25][13]\\r\\n\\r\\nO'Connor served as assistant Attorney General of Arizona from 1965 to 1969 until she was appointed to fill a vacancy in the Arizona Senate.[13] She ran for and won the election for the seat the following year.[13] By 1973, she became the first woman to serve as Arizona's or any state's Majority Leader.[26][27] She developed a reputation as a skilled negotiator and a moderate. After serving two full terms, O'Connor decided to leave the Senate.[27]\\r\\n\\r\\nIn 1974, she was elected to the Maricopa County Superior Court[28] serving from 1975 to 1979 when she was elevated to the Arizona State Court of Appeals. She served on the Court of Appeals-Division One until 1981 when she was appointed to the Supreme Court by President Ronald Reagan.[29]\\r\\n\\r\\nOn July 7, 1981, Reagan?ÿ who had pledged during his 1980 presidential campaign to appoint the first woman to the Court[30]?ÿ announced he would nominate O'Connor as an Associate Justice of the Supreme Court to replace the retiring Potter Stewart.[31] O'Connor received notification from President Reagan of her nomination on the day prior to the announcement and did not know that she was a finalist for the position.[22]\\r\\n\\r\\nReagan wrote in his diary on July 6, 1981: \\"Called Judge O'Connor and told her she was my nominee for supreme court. Already the flak is starting and from my own supporters. Right to Life people say she is pro abortion. She declares abortion is personally repugnant to her. I think she'll make a good justice.\\"[32] O'Connor told Reagan she did not remember whether she had supported the view of repealing Arizona's law banning abortion.[33] However, she had cast a preliminary vote in the Arizona State Senate in 1970 in favor of a bill to repeal the state's criminal-abortion statute.[34] In 1974, O'Connor had opined against a measure to prohibit abortions in some Arizona hospitals.[34] Pro-life and religious groups opposed O'Connor's nomination because they suspected, correctly, she would not be willing to overturn Roe v Wade.[35] U.S. Senate Republicans, including Don Nickles of Oklahoma, Steve Symms of Idaho, and Jesse Helms of North Carolina called the White House to express their discontent over the nomination; Nickles said he and \\"other profamily Republican senators would not support\\" O'Connor.[35] Helms, Nickles, and Symms nevertheless voted for confirmation.[36]\\r\\n\\r\\nReagan formally nominated O'Connor on August 19, 1981.[37]\\r\\n\\r\\nConservative activists such as the Reverend Jerry Falwell, Howard Phillips, and Peter Gemma also spoke out against the nomination. Gemma called the nomination \\"a direct contradiction of the Republican platform to everything that candidate Reagan said and even President Reagan has said in regard to social issues.\\"[38] Gemma, the executive director of the  National Pro-Life Political Action Committee, had sought to delay O'Connor's confirmation by challenging her record, including support for the Equal Rights Amendment.[39]\\r\\n\\r\\nO'Connor's confirmation hearing before the Senate Judiciary Committee began on September 9, 1981.[40] It was the first televised confirmation hearing for a Supreme Court Justice.[41] The confirmation hearing lasted three days and largely focused on the issue of abortion.[42] When asked, O'Connor refused to telegraph her views on abortion, and she was careful not to leave the impression that she supported abortion rights.[43] The Judiciary Committee approved O'Connor with seventeen votes in favor and one vote of present.[42]\\r\\n\\r\\nOn September 21, O'Connor was confirmed by the U.S. Senate with a vote of 99ÿ0;[31][44] Senator Max Baucus of Montana was absent from the vote, and sent O'Connor a copy of A River Runs Through It by way of apology.[45] In her first year on the Court she received over 60,000 letters from the public, more than any other justice in history.[46]\\r\\n\\r\\nO'Connor was part of the federalism movement and approached each case as narrowly as possible,[citation needed] avoiding generalizations that might later \\"paint her into a corner\\" for future cases.[citation needed] Initially, her voting record aligned closely with the conservative William Rehnquist (voting with him 87% of the time her first three years at the Court).[47] From that time until 1998, O'Connor's alignment with Rehnquist ranged from 93.4% to 63.2%, hitting above 90% in three of those years.[48] In nine of her first sixteen years on the Court, O'Connor voted with Rehnquist more than with any other justice.[48]\\r\\n\\r\\nLater on, as the Court's make-up became more conservative (e.g., Anthony Kennedy replacing Lewis Powell, and Clarence Thomas replacing Thurgood Marshall), O'Connor often became the swing vote on the Court. However, she usually disappointed the Court's more liberal bloc in contentious 5ÿ4 decisions: from 1994 to 2004, she joined the traditional conservative bloc of Rehnquist, Antonin Scalia, Anthony Kennedy, and Thomas 82 times; she joined the liberal bloc of John Paul Stevens, David Souter, Ruth Bader Ginsburg, and Stephen Breyer only 28 times.[49]\\r\\n\\r\\nO'Connor's relatively small[50] shift away from conservatives on the Court seems to have been due at least in part to Thomas's views.[51] When Thomas and O'Connor were voting on the same side, she would typically write a separate opinion of her own, refusing to join his.[52] In the 1992 term, O'Connor did not join a single one of Thomas' dissents.[53]\\r\\n\\r\\nSome of the cases in which O'Connor was the deciding vote include:\\r\\n\\r\\nO'Connor played an important role in other notable cases, such as:\\r\\n\\r\\nOn February 22, 2005, with Rehnquist and Stevens (who were senior to her) absent, she became the senior justice presiding over oral arguments in the case of Kelo v. City of New London and becoming the first woman to do so before the Court.[citation needed]\\r\\n\\r\\nSandra Day O'Connor was unpredictable in many of her court decisions, especially those regarding First Amendment Establishment Cause issues. This might be due to the fact that instead of letting herself be guided by her conservative ideologies, she decided on a case-by-case basis and voted with careful deliberation in a way that she felt benefited individual rights and the Constitution (which she viewed to be \\"an ever changing work in progress.\\") Barry Lynn, executive director of Americans United for Separation of Church and State, said, \\"O'Connor was a conservative, but she saw the complexity of church-state issues and tried to choose a course that respected the country's religious diversity\\" (Hudson 2005). O'Connor voted in favor of religious institutions,[clarification needed] such as in Zelman v. Simmons-Harris, Mitchell v. Helms, and Rosenberger v. University of Virginia. Conversly, in Lee v. Weisman she was part of the majority in the case that saw religious prayer and pressure to stand in silence at a graduation ceremony as part of a religious act that coerced people to support or to participate in religion, which is strictly prohibited by the Establishment Clause. This is consistent with a similar case, Santa Fe Independent School District v. Doe, involving prayer at a school football game. In this case O'Connor joined the majority opinion that stated prayer at school football games violates the Establishment Clause (Oyez, 2016). O'Connor was the first justice to articulate the \\"no endorsement\\" standard for the Establishment Clause.[55] In Lynch v. Donnelly, O'Connor wrote for a 5ÿ4 majority that a nativity scene in a public Christmas display did not violate the First Amendment because it was not expressing an endorsement or disapproval of any religion.[55]\\r\\n\\r\\nAccording to George Washington University law professor Jeffrey Rosen, \\"O'Connor was an eloquent opponent of intrusive group searches that threatened privacy without increasing security. In a 1983 opinion upholding searches by drug-sniffing dogs, she recognized that a search is most likely to be considered constitutionally reasonable if it is very effective at discovering contraband without revealing innocent but embarrassing information.\\"[56] Washington College of Law law professor Andrew Taslitz, referencing O'Connor's dissent in a 2001 case, said of her Fourth Amendment jurisprudence: \\"O'Connor recognizes that needless humiliation of an individual is an important factor in determining Fourth Amendment reasonableness.\\"[57] O'Connor once quoted the social contract theory of John Locke as influencing her views on the reasonableness and constitutionality of government action.[58]\\r\\n\\r\\nIn the 1990 and 1995 Missouri v. Jenkins rulings, O'Connor voted with the majority that district courts had no authority to require the state of Missouri to increase school funding in order to counteract racial inequality. In the 1991 Freeman v. Pitts case, O'Connor joined a concurring opinion in a plurality, agreeing that a school district that had formerly been under judicial review for racial segregation could be freed of this review, even though not all desegregation targets had been met. Law professor Herman Schwartz criticized these rulings, writing that in both cases \\"both the fact and effects of segregation were still present.\\"[59]\\r\\n\\r\\nIn 1987's McCleskey v. Kemp, O'Connor joined a 5ÿ4 majority that voted to uphold the death penalty for an African American man, Warren McCleskey, convicted of killing a white police officer, despite statistical evidence that black defendants were more likely to receive the death penalty than others both in Georgia and in the U.S. as a whole.[59][60][61]\\r\\n\\r\\nIn 1996's Shaw v. Hunt and Shaw v. Reno, O'Connor joined a Rehnquist opinion, following an earlier precedent from an opinion she authored in 1993, in which the Court struck down an electoral districting plan designed to facilitate the election of two black representatives out of twelve from North Carolina, a state that had not had any black representative since Reconstruction, despite being approximately 20% black[59]the Court held that the districts were unacceptably gerrymandered and O'Connor called the odd shape of the district in question, North Carolina's 12th, \\"bizarre\\".\\r\\n\\r\\nLaw Professor Herman Schwartz called O'Connor \\"the Court's leader in its assault on racially oriented affirmative action,\\"[59] although she joined with the Court in upholding the constitutionality of race-based admissions to universities.[30]\\r\\n\\r\\nIn late 2008, O'Connor said she believed racial affirmative action should continue to help heal the inequalities created by racial discrimination. She stressed this would not be a cure-all but rather a bandage and that society has to do much more to correct our racial imbalance. In 2003, she authored a majority Supreme Court opinion (Grutter v. Bollinger) saying racial affirmative action wouldn't be constitutional permanently, but long enough to correct past discriminationwith an approximate limit of around 25 years, or until 2028.[62]\\r\\n\\r\\nIn her confirmation hearings and early days on the Court, O'Connor was carefully ambiguous on the issue of abortion, as some conservatives questioned her pro-life credentials on the basis of some of her votes in the Arizona legislature.[35] O'Connor generally dissented from 1980s opinions which took an expansive view of Roe v. Wade; she criticized that decision's \\"trimester approach\\" sharply in her dissent in 1983's City of Akron v. Akron Center for Reproductive Health. She criticized Roe in Thornburgh v. American College of Obstetricians and Gynecologists: \\"... I dispute not only the wisdom but also the legitimacy of the Court's attempt to discredit and pre-empt state abortion regulation regardless of the interests it serves and the impact it has.\\"[63] In 1989, O'Connor stated during the deliberations over the Webster case that she would not overrule Roe.[64] While on the Court, O'Connor did not vote to strike down any restrictions on abortion until Hodgson v. Minnesota in 1990.[65]\\r\\n\\r\\nO'Connor allowed certain limits to be placed on access to abortion, but supported the fundamental right to abortion protected by the Due Process Clause of the Fourteenth Amendment. In Planned Parenthood v. Casey, O'Connor used a test she had originally developed in City of Akron v. Akron Center for Reproductive Health to limit the holding of Roe v. Wade, opening up a legislative portal where a State could enact measures so long as they did not place an \\"undue burden\\" on a woman's right to an abortion. Casey revised downward the standard of scrutiny federal courts would apply to state abortion restrictions, a major departure from Roe. However it preserved Roe's core constitutional precept: that the Fourteenth Amendment implies and protects a fundamental right to control the outcomes of one's reproductive actions. Writing the plurality opinion for the Court, O'Connor, along with Justices Kennedy and Souter, famously declared: \\"At the heart of liberty is the right to define one's own concept of existence, of meaning, of the universe, and of the mystery of human life. Beliefs about these matters could not define the attributes of personhood were they formed under compulsion of the State.\\"[66]\\r\\n\\r\\nO'Connor was a vigorous defender of the citing of foreign laws in judicial decisions.[67] In a well-publicized October 28, 2003, speech at the Southern Center for International Studies, O'Connor said:\\r\\n\\r\\nThe impressions we create in this world are important and can leave their mark ... [T]here is talk today about the \\"internationalization of legal relations\\". We are already seeing this in American courts, and should see it increasingly in the future. This does not mean, of course, that our courts can or should abandon their character as domestic institutions. But conclusions reached by other countries and by the international community, although not formally binding upon our decisions, should at times constitute persuasive authority in American courtswhat is sometimes called \\"transjudicialism\\".[68]\\r\\nIn the speech she noted the 2002 Court case Atkins v. Virginia, in which the majority decision (which included her) cited disapproval of the death penalty in Europe as part of its argument. This speech, and the general concept of relying on foreign law and opinion, was widely criticized by conservatives.[69] In May 2004, the U.S. House of Representatives responded by passing a non-binding resolution, the \\"Reaffirmation of American Independence Resolution\\", stating that \\"U.S. judicial decisions should not be based on any foreign laws, court decisions, or pronouncements of foreign governments unless they are relevant to determining the meaning of American constitutional and statutory law.\\"[70]\\r\\n\\r\\nO'Connor once quoted the constitution of the Middle Eastern nation of Bahrain, which states that \\"[n]o authority shall prevail over the judgement of a judge, and under no circumstances may the course of justice be interfered with.\\" Further, \\"[i]t is in everyone's interest to foster the rule-of-law evolution.\\" O'Connor proposed that such ideas be taught in American law schools, high schools and universities. Critics contend that such thinking is contrary to the U.S. Constitution and establishes a rule of man, rather than law.[68] In her retirement, she has continued to speak and organize conferences on the issue of judicial independence.\\r\\n\\r\\nO'Connor's case-by-case approach routinely placed her in the center of the Court and drew both criticism and praise. The Washington Post columnist Charles Krauthammer, for example, described her as lacking a judicial philosophy and instead displaying \\"political positioning embedded in a social agenda\\".[71] Conservative commentator, Ramesh Ponnuru, wrote that, even though O'Connor \\"has voted reasonably well\\", her tendency to issue very case-specific rulings \\"undermines the predictability of the law and aggrandizes the judicial role\\".[72]\\r\\n\\r\\nO'Connor has said she felt a responsibility to demonstrate women could do the job of justice.[22] She faced some practical concerns, including the lack of a woman's restroom near the Courtroom.[22]\\r\\n\\r\\nTwo years after O'Connor joined the Court, The New York Times published an editorial which mentioned the \\"nine men\\"[73] of the \\"SCOTUS\\", or Supreme Court of the United States.[73] O'Connor responded with a letter to the editor reminding the Times that the  Court was no longer composed of nine men and referred to herself as FWOTSC (First Woman On The Supreme Court).[74]\\r\\n\\r\\nIn several speeches broadcast nationally on the cable network C-SPAN, she mentioned feeling some relief from the media clamor when Ruth Bader Ginsburg joined her as an Associate Justice of the Court in 1993.[citation needed] In May 2010, O'Connor warned female Supreme Court nominee Elena Kagan about the \\"unpleasant\\" process of confirmation hearings.[75]\\r\\n\\r\\nIn 2003, she wrote a book titled The Majesty of the Law: Reflections of a Supreme Court Justice (ISBN?0-375-50925-9).[citation needed] In 2005, she wrote a children's book, Chico, named for her favorite horse, which gives an autobiographical description of her childhood.[76]\\r\\n\\r\\nOn December 12, 2000, The Wall Street Journal reported that O'Connor was reluctant to retire with a Democrat in the presidency: \\"At an Election Night party at the Washington, D.C. home of Mary Ann Stoessel, widow of former Ambassador Walter Stoessel, the justice's husband, John O'Connor, mentioned to others her desire to step down, according to three witnesses. But Mr. O'Connor said his wife would be reluctant to retire if a Democrat were in the White House and would choose her replacement. Justice O'Connor declined to comment.\\"[77]\\r\\n\\r\\nBy 2005, the composition of the Court had been unchanged for eleven years, the second-longest period in American history without any such change. Rehnquist was widely expected to be the first justice to retire during Bush's term, owing to his age and his battle with cancer, although rumors of O'Connor's possible retirement circulated as well.[78] Before deciding to retire, O'Connor consulted Rehnquist on his plans in an attempt to avoid having two retirements at the same time.[79]\\r\\n\\r\\nOn July 1, 2005, O'Connor announced her intention to retire. In her letter to Bush, she stated that her retirement from active service would take effect upon the confirmation of her successor.[78] Her letter did not provide a reason for her departure; however, a Supreme Court spokeswoman confirmed O'Connor was leaving to spend time with her husband,[78] who was suffering from Alzheimer's disease.[79] At her retirement she was still in good health, an exception to the usual practice of justices serving until death or nearly incapacitated.[79]\\r\\n\\r\\nOn July 19, Bush nominated D.C. Circuit Judge John Roberts to succeed O'Connor. O'Connor heard the news over the car radio on the way back from a fishing trip.[80] She felt he was an excellent and highly qualified choicehe had argued numerous cases before the Court during her tenure.[citation needed] However, she was disappointed her replacement was not a woman.[81]\\r\\n\\r\\nO'Connor had expected to leave the Court before the next term started on October 3, 2005.[82][83] However, Rehnquist died on September 3,[84] creating an immediate vacancy on the Court.[85] Two days later, Bush withdrew Roberts as his nominee for her seat and instead appointed him to fill the vacant office of Chief Justice.[86] O'Connor agreed to stay on the Court until her replacement was named and confirmed.[86] She spoke at the late Chief Justice's funeral.[87] On October 3, Bush nominated White House Counsel Harriet Miers to replace O'Connor.[88] After much criticism and controversy over her nomination, on October 27, Miers asked Bush to withdraw her nomination.[89] Bush accepted, reopening the search for O'Connor's successor.[89]\\r\\n\\r\\nThe continued delays in confirming a successor further extended O'Connor's time on the Court.[83] She continued to hear oral argument on cases, including cases dealing with controversial issues such as physician-assisted suicide and abortion.[83] O'Connor's last Court opinion, Ayotte v. Planned Parenthood of New England, written for a unanimous court, was a procedural decision that involved a challenge to a New Hampshire abortion law.[90]\\r\\n\\r\\nOn October 31, Bush nominated Third Circuit Judge Samuel Alito to replace O'Connor;[91] Alito was confirmed and sworn in on January 31, 2006.[92] Since retiring, she has continued to hear cases and rendered over a dozen opinions in federal appellate courts across the country, filling in as a substitute judge when vacations or vacancies leave their three-member panels understaffed.[93]\\r\\n\\r\\nDuring a March 2006 speech at Georgetown University, O'Connor said some political attacks on the independence of the Courts pose a direct threat to the constitutional freedoms of Americans. She said \\"any reform of the system is debatable as long as it is not motivated by 'nakedly partisan reasoning' retaliation because congressmen or senators dislike the result of the cases. Courts interpret the law as it was written, not as the congressmen might have wished it was written\\", and \\"it takes a lot of degeneration before a country falls into dictatorship, but we should avoid these ends by avoiding these beginnings.\\"[94] She echoed her concerns for an independent judiciary during the dedication address at the Elon University School of Law in September of that same year.\\r\\n\\r\\nOn November 19, 2008, O'Connor published an introductory essay to a themed issue on judicial accountability in the Denver University Law Review. She calls for a better public understanding of judicial accountability.[95] On November 7, 2007, at a conference on her landmark opinion in Strickland v. Washington (1984) sponsored by the Constitution Project, highlighted the lack of proper legal representation for many of the poorest defendants.[96] O'Connor also urged the creation of a system for \\"merit selection for judges,\\" a cause for which she had frequently advocated.[96][97]\\r\\n\\r\\nOn August 7, 2008, O'Connor and Abdurrahman Wahid, former President of Indonesia, wrote an editorial in the Financial Times stating concerns about the threatened imprisonment of Malaysian opposition leader Anwar Ibrahim.[98]\\r\\n\\r\\nFollowing the Court's Citizens United v. Federal Election Commission decision on corporate political spending, O'Connor offered measured criticism of the decision, telling Georgetown law students and lawyers, \\"that the Court has created an unwelcome new path for wealthy interests to exert influence on judicial elections.\\"[99]\\r\\n\\r\\nO'Connor argued in favor of President Barack Obama naming the replacement for Antonin Scalia in February 2016, mere days after Scalia's death, opposing Republican arguments that the next president should get to fill the vacancy. She said, \\"I think we need somebody there to do the job now and let's get on with it\\"; and that \\"[y]ou just have to pick the best person you can under the circumstances, as the appointing authority must do. It's an important position and one that we care about as a nation and as a people. And I wish the president well as he makes choices and goes down that line. It's hard.\\"[100]\\r\\n\\r\\nJudge William H. Pryor, Jr., a conservative jurist, has criticized O'Connor's speeches and op-eds for hyperbole and factual inaccuracy, based in part on O'Connor's opinions as to whether judges face a rougher time in the public eye today than in the past.[101][102]\\r\\n\\r\\nO'Connor has reflected on her time on the Supreme Court by saying that she regrets the Court hearing the Bush v. Gore case in 2000 because it \\"stirred up the public\\" and \\"gave the Court a less-than-perfect reputation.\\" The former justice told the Chicago Tribune that \\"Maybe the Court should have said, 'We're not going to take it, goodbye,'...It turned out the election authorities in Florida hadn't done a real good job there and kind of messed it up. And probably the Supreme Court added to the problem at the end of the day.\\"[103][104][105]\\r\\n\\r\\nAs a Retired Supreme Court Justice (roughly equivalent to senior status for judges of lower federal courts), O'Connor continued to receive a full salary,[citation needed] maintained a staffed office with at least one law clerk, and heard cases on a part-time basis in federal district courts and courts of appeals as a visiting judge.[106] By 2008, O'Connor had sat for cases with the 2nd, 8th, and 9th Circuits.[107] O'Connor heard an Arizona voting rights case which the Supreme Court later reviewed.[106] In Arizona v. Inter Tribal Council of Arizona, a 7-2 majority affirmed O'Connor and the rest of 9th Circuit panel, and struck down a provision of Arizona's voting registration law.[108] O'Connor hired a law clerk for the October 2015 term, but did not hire a law clerk for the subsequent term.[109][110]\\r\\n\\r\\nShe stated that she planned to travel, spend time with family, and, because of her fear of the attacks on judges by legislators, would work with the American Bar Association on a commission to help explain the separation of powers and the role of judges. She also announced that she was working on a new book, which will focus on the early history of the Court. She is a trustee on the board of the Rockefeller Foundation.\\r\\n\\r\\nOn October 4, 2005, President Gene Nichol of the College of William & Mary announced that O'Connor had accepted[111] the largely ceremonial role of becoming the 23rd Chancellor of the College, replacing Henry Kissinger, and following in the position held by Margaret Thatcher, Chief Justice Warren Burger, and President George Washington. The Investiture Ceremony was held April 7, 2006. O'Connor continued to make semi-regular visits to the college until she was succeeded in that post by former Secretary of Defense Robert Gates.[112]\\r\\n\\r\\nO'Connor was a member of the 2006 Iraq Study Group, appointed by the U.S. Congress.[113]\\r\\n\\r\\nIn October 2006, O'Connor sat as a member of panels of the United States Courts of Appeals for the Second, Eighth, and Ninth Circuits, to hear arguments in one-day's cases in each court.[114]\\r\\n\\r\\n\\r\\nO'Connor chaired the Jamestown 2007 celebration, commemorating the 400th anniversary of the founding of the colony at Jamestown, Virginia in 1607. Her appearances in Jamestown dovetailed with her appearances and speeches as chancellor at The College of William & Mary nearby. \\r\\nAs of Spring 2006, O'Connor teaches a two-week course called \\"The Supreme Court\\" at the University of Arizona's James E. Rogers College of Law every spring semester.\\r\\n\\r\\nIn the fall of 2007, O'Connor and W. Scott Bales taught a course at the Sandra Day O'Connor College of Law at Arizona State University.\\r\\n\\r\\nShe wrote the 2013 book Out of Order: Stories from the History of the Supreme Court.[115]\\r\\n\\r\\nOn May 15, 2006, O'Connor gave the commencement address at the William & Mary School of Law, where she said that judicial independence is \\"under serious attack at both the state and national level\\".[116]\\r\\n\\r\\nIn 2008, O'Connor was named an inaugural Harry Rathbun Visiting Fellow by the Office for Religious Life at Stanford University. On April 22, 2008, she gave \\"Harry's Last Lecture on a Meaningful Life\\" in honor of the former Stanford Law professor who shaped her undergraduate and law careers.[117]\\r\\n\\r\\nOn September 17, 2014, O'Connor appeared on the television show Jeopardy! and provided a couple of video answers to the category 'Supreme Court' which appeared on the show. On the same day in Concord, New Hampshire, she gave a talk alongside her former colleague Justice David Souter about the importance of meaningful civics education in the United States.[118]\\r\\n\\r\\nIn February 2009, O'Connor launched Our Courts, a website she created to offer interactive civics lessons to students and teachers because she was concerned about the lack of knowledge among most young Americans about how their government works. She also serves as a co-chair with Lee H. Hamilton for the Campaign for the Civic Mission of Schools.[119] On March 3, 2009, O'Connor appeared on the satirical television program The Daily Show with Jon Stewart to promote the website. In August 2009, the website added two online interactive games.[120] The initiative expanded, becoming iCivics in May 2010, and continues to offer free lessons plans, games, and interactive videogames for middle and high school educators.[121]\\r\\n\\r\\nShe served on the Board of Trustees of the National Constitution Center in Philadelphia, a museum dedicated to the U.S. Constitution.[122][123] By November 2015, O'Connor had transitioned to being a Trustee Emeritus for the Center.[124]\\r\\n\\r\\nIn 2009, O'Connor founded the 501(c)3 non-profit organization, O'Connor House, dedicated to solving complex issues through civil discourse and collaborative action.[125] The organization moved the O'Connor family home to Arizona Historical Society Museum at Papago Park in Tempe.[126] In March 2015, O'Connor's non-profit organization, O'Connor House, became the Sandra Day O'Connor Institute. O'Connor serves as Founder and Advisor to the O'Connor Institute.\\r\\n\\r\\nIn April 2013, the Board of Directors of Justice at Stake, a national judicial reform advocacy organization, announced that O'Connor would be joining the organization as Honorary Chair.\\"[127]\\r\\n\\r\\nO'Connor is the Co-Chair of the National Advisory Board at the National Institute for Civil Discourse (NICD). The institute was created at the University of Arizona after the tragic shooting of former Congresswoman Gabrielle Giffords in 2011, that killed 6 people and wounded 13 others.\\r\\n\\r\\nUpon her appointment to the Supreme Court, O'Connor and her husband moved to the Kalorama area of Washington, D.C. The O'Connors became active parts of the Washington D.C. social scene. Sandra Day O'Connor played tennis and golf in her spare time.[13]\\r\\n\\r\\nO'Connor was successfully treated for breast cancer in 1988 (she also had her appendix removed that year).[128] That same year, John O'Connor left the Washington, D.C. law firm of Miller & Chevalier for a practice which required him to split his time between Washington, D.C. and Phoenix.[13]\\r\\n\\r\\nHer husband suffered from Alzheimer's disease for nearly twenty years until his death in 2009,[24] and she has become involved in raising awareness of the disease. After retiring from the Court, O'Connor moved to Phoenix, Arizona.[12]","input":"Who is the first female supreme court chief justice?"},{"output":"Benjamin Harrison","context":"The Sherman Antitrust Act (Sherman Act,[1] 26?Stat.?209, 15 U.S.C.?ҡ?1ÿ7) is a landmark federal statute in the history of United States antitrust law (or \\"competition law\\") passed by Congress in 1890 under the presidency of Benjamin Harrison. It allowed certain business activities that federal government regulators deem to be competitive, and recommended the federal government to investigate and pursue trusts.\\r\\nIn the general sense, a trust is a centuries-old legal arrangement whereby one party conveys property to a trustee to hold for a beneficiary. These are commonly used to hold inheritances for the benefit of children, for example. The specific sense from 19th-century America used in the law refers to a type of trust which combines several large businesses for monopolistic purposes ÿ to exert complete control over a market ÿ though the law addresses monopolistic practices even if they have nothing to do with this specific legal arrangement.[2] In most countries outside the United States, antitrust law is known as \\"competition law\\".\\r\\nThe law attempts to prevent the artificial raising of prices by restriction of trade or supply.[3] \\"Innocent monopoly\\", or monopoly achieved solely by merit, is perfectly legal, but acts by a monopolist to artificially preserve that status, or nefarious dealings to create a monopoly, are not. The purpose of the Sherman Act is not to protect competitors from harm from legitimately successful businesses, nor to prevent businesses from gaining honest profits from consumers, but rather to preserve a competitive marketplace to protect consumers from abuses.[4]\\r\\nOver time, the act has also been used more broadly, to oppose the combination of entities that could potentially harm competition, such as monopolies or cartels.\\r\\n\\r\\n\\r\\nThe Sherman Act is divided into three sections. Section 1 delineates and prohibits specific means of anticompetitive conduct, while Section 2 deals with end results that are anti-competitive in nature. Thus, these sections supplement each other in an effort to prevent businesses from violating the spirit of the Act, while technically remaining within the letter of the law. Section 3 simply extends the provisions of Section 1 to U.S. territories and the District of Columbia.\\r\\nThe Clayton Antitrust Act, passed in 1914, prescribes certain additional activities that had been discovered to fall outside the scope of the Sherman Antitrust Act. For example, the Clayton Act added certain practices to the list of impermissible activities:\\r\\nThe RobinsonÿPatman Act of 1936 amended the Clayton Act. The amendment proscribed certain anti-competitive practices in which manufacturers engaged in price discrimination against equally-situated distributors.\\r\\nThe federal government began filing cases under the Sherman Antitrust Act in 1890. Some cases were successful and others were not; many took several years to decide, including appeals.\\r\\nNotable cases filed under the act include:[7]\\r\\nAs explained by the U.S. Supreme Court in Spectrum Sports, Inc. v. McQuillan 506 U.S. 447 (1993):\\r\\nThe purpose of the [Sherman] Act is not to protect businesses from the working of the market; it is to protect the public from the failure of the market. The law directs itself not against conduct which is competitive, even severely so, but against conduct which unfairly tends to destroy competition itself.[9]\\r\\nAccording to its authors, it was not intended to impact market gains obtained by honest means, by benefiting the consumers more than the competitors. Senator George Hoar of Massachusetts, another author of the Sherman Act, said the following:\\r\\n\\"... [a person] who merely by superior skill and intelligence...got the whole business because nobody could do it as well as he could was not a monopolist..(but was if) it involved something like the use of means which made it impossible for other persons to engage in fair competition.\\"[10]\\r\\nAt Apex Hosiery Co. v. Leader 310 U. S. 469, 310 U. S. 492-93 and n. 15:\\r\\nThe legislative history of the Sherman Act, as well as the decisions of this Court interpreting it, show that it was not aimed at policing interstate transportation or movement of goods and property. The legislative history and the voluminous literature which was generated in the course of the enactment and during fifty years of litigation of the Sherman Act give no hint that such was its purpose.[11] They do not suggest that, in general, state laws or law enforcement machinery were inadequate to prevent local obstructions or interferences with interstate transportation, or presented any problem requiring the interposition of federal authority.[12] In 1890, when the Sherman Act was adopted, there were only a few federal statutes imposing penalties for obstructing or misusing interstate transportation.[13] With an expanding commerce, many others have since been enacted safeguarding transportation in interstate commerce as the need was seen, including statutes declaring conspiracies to interfere or actual interference with interstate commerce by violence or threats of violence to be felonies.[14] The law was enacted in the era of \\"trusts\\" and of \\"combinations\\" of businesses and of capital organized and directed to control of the market by suppression of competition in the marketing of goods and services, the monopolistic tendency of which had become a matter of public concern. The goal was to prevent restraints of free competition in business and commercial transactions which tended to restrict production, raise prices, or otherwise control the market to the detriment of purchasers or consumers of goods and services, all of which had come to be regarded as a special form of public injury.[15] For that reason the phrase \\"restraint of trade,\\" which, as will presently appear, had a well understood meaning in common law, was made the means of defining the activities prohibited. The addition of the words \\"or commerce among the several States\\" was not an additional kind of restraint to be prohibited by the Sherman Act, but was the means used to relate the prohibited restraint of trade to interstate commerce for constitutional purposes, Atlantic Cleaners & Dyers v. United States, 286 U. S. 427, 286 U. S. 434, so that Congress, through its commerce power, might suppress and penalize restraints on the competitive system which involved or affected interstate commerce. Because many forms of restraint upon commercial competition extended across state lines so as to make regulation by state action difficult or impossible, Congress enacted the Sherman Act, 21 Cong.Rec. 2456. It was in this sense of preventing restraints on commercial competition that Congress exercised \\"all the power it possessed.\\" Atlantic Cleaners & Dyers v. United States, supra, 286 U. S. 435.\\r\\nAt Addyston Pipe and Steel Company v. United States, 85 F.2d 1, affirmed, 175 U. S. 175 U.S. 211;\\r\\nAt Standard Oil Co. of New Jersey v. United States, 221 U. S. 1, 221 U. S. 54-58.\\r\\nCongress claimed power to pass the Sherman Act through its constitutional authority to regulate interstate commerce. Therefore, federal courts only have jurisdiction to apply the Act to conduct that restrains or substantially affects either interstate commerce or trade within the District of Columbia. This requires that the plaintiff must show that the conduct occurred during the flow of interstate commerce or had an appreciable effect on some activity that occurs during interstate commerce.\\r\\nA Section 1 violation has three elements:[16]\\r\\nA Section 2 monopolization violation has two elements:[17]\\r\\nSection 2 also bans attempted monopolization, which has the following elements:\\r\\nViolations of the Sherman Act fall (loosely[18]) into two categories:\\r\\nA modern trend has increased difficulty for antitrust plaintiffs as courts have come to hold plaintiffs to increasing burdens of pleading. Under older Section 1 precedent, it was not settled how much evidence was required to show a conspiracy. For example, a conspiracy could be inferred based on parallel conduct, etc. That is, plaintiffs were only required to show that a conspiracy was conceivable. Since the 1970s, however, courts have held plaintiffs to higher standards, giving antitrust defendants an opportunity to resolve cases in their favor before significant discovery under FRCP 12(b)(6). That is, to overcome a motion to dismiss, plaintiffs, under Bell Atlantic Corp. v. Twombly, must plead facts consistent with FRCP 8(a) sufficient to show that a conspiracy is plausible (and not merely conceivable or possible). This protects defendants from bearing the costs of antitrust \\"fishing expeditions\\"; however it deprives plaintiffs of perhaps their only tool to acquire evidence (discovery).\\r\\nSecond, courts have employed more sophisticated and principled definitions of markets. Market definition is necessary, in rule of reason cases, for the plaintiff to prove a conspiracy is harmful. It is also necessary for the plaintiff to establish the market relationship between conspirators to prove their conduct is within the per se rule.\\r\\nIn early cases, it was easier for plaintiffs to show market relationship, or dominance, by tailoring market definition, even if it ignored fundamental principles of economics. In U.S. v. Grinnell, 384 U.S. 563 (1966), the trial judge, Charles Wyzanski, composed the market only of alarm companies with services in every state, tailoring out any local competitors; the defendant stood alone in this market, but had the court added up the entire national market, it would have had a much smaller share of the national market for alarm services that the court purportedly used. The appellate courts affirmed this finding; however, today, an appellate court would likely find this definition to be flawed. Modern courts use a more sophisticated market definition that does not permit as manipulative a definition.[citation needed]\\r\\nSection 2 of the Act forbade monopoly. In Section 2 cases, the court has, again on its own initiative, drawn a distinction between coercive and innocent monopoly. The act is not meant to punish businesses that come to dominate their market passively or on their own merit, only those that intentionally dominate the market through misconduct, which generally consists of conspiratorial conduct of the kind forbidden by Section 1 of the Sherman Act, or Section 3 of the Clayton Act.\\r\\nThe Act was aimed at regulating businesses. However, its application was not limited to the commercial side of business. Its prohibition of the cartel was also interpreted to make illegal many labor union activities. This is because unions were characterized as cartels as well (cartels of laborers).[30] This persisted until 1914, when the Clayton Act created exceptions for certain union activities.\\r\\nTo determine whether a particular state statute that restrains competition was intended to be preempted by the Act, courts will engage in a two-step analysis, as set forth by the Supreme Court in Rice v. Norman Williams Co.:\\r\\nThe Sherman Act has seen much controversy. One branch of the criticism focuses on whether the Act improves competition and benefits consumers, or merely aids inefficient businesses at the expense of more innovative ones. Alan Greenspan, in his essay entitled Antitrust[31] condemns the Sherman Act as stifling innovation and harming society. \\"No one will ever know what new products, processes, machines, and cost-saving mergers failed to come into existence, killed by the Sherman Act before they were born. No one can ever compute the price that all of us have paid for that Act which, by inducing less effective use of capital, has kept our standard of living lower than would otherwise have been possible.\\" Greenspan summarized the nature of antitrust law as: \\"a jumble of economic irrationality and ignorance.\\"[32]\\r\\nGreenspan at that time was a disciple and friend of Ayn Rand, and he first published Antitrust in Rand's monthly publication The Objectivist Newsletter. Rand, who described herself as \\"a radical for capitalism,\\"[33] opposed antitrust law not only on economic grounds but also morally, as a violation of property rights, asserting that the \\"meaning and purpose\\" of antitrust law is \\"the penalizing of ability for being ability, the penalizing of success for being success, and the sacrifice of productive genius to the demands of envious mediocrity.\\"[34]\\r\\nOther normative criticism of antitrust policy has been framed in utilitarian terms. Critics, having assumed that some kind of competition law is inevitable, argue as to what its central policy should be, and raise doubts as to whether it is accomplishing its goal. A common tactic is to choose a goal, and then cite evidence that it supports the opposite.[original research?] For example, during a debate over the act in 1890, Representative William Mason said \\"trusts have made products cheaper, have reduced prices; but if the price of oil, for instance, were reduced to one cent a barrel, it would not right the wrong done to people of this country by the trusts which have destroyed legitimate competition and driven honest men from legitimate business enterprise.\\"[35] Consequently, if the primary goal of the act is to protect consumers, and consumers are protected by lower prices, the act may be harmful if it reduces economy of scale, a price-lowering mechanism, by breaking up big businesses. Mason put small business survival, a justice interest, on a level concomitant with the pure economic rationale of consumer interest.\\r\\nThe converse argument is that if lowering prices alone is not the goal, and instead protecting competitions and markets as well as consumers is the goal, the law again arguably has the opposite effect? it could be protectionist. Economist Thomas DiLorenzo notes that Senator Sherman sponsored the 1890 William McKinley tariff just three months after the Sherman Act, and agrees with The New York Times which wrote on October 1, 1890: \\"That so-called Anti-Trust law was passed to deceive the people and to clear the way for the enactment of this Pro-Trust law relating to the tariff.\\" The Times went on to assert that Sherman merely supported this \\"humbug\\" of a law \\"in order that party organs might say...'Behold! We have attacked the trusts. The Republican Party is the enemy of all such rings.'\\" [36]\\r\\nDilorenzo writes: \\"Protectionists did not want prices paid by consumers to fall. But they also understood that to gain political support for high tariffs they would have to assure the public that industries would not combine to increase prices to politically prohibitive levels. Support for both an antitrust law and tariff hikes would maintain high prices while avoiding the more obvious bilking of consumers.\\"[37]\\r\\nThe criticism of antitrust law is often associated with conservative politics. For example, conservative legal scholar, judge, and failed Supreme Court nominee Robert Bork was well known for his outspoken criticism of the antitrust regime. Another conservative legal scholar and judge, Richard Posner of the Seventh Circuit does not condemn the entire regime, but expresses concern with the potential that it could be applied to create inefficiency, rather than to avoid inefficiency.[38] Posner further believes, along with a number of others, including Bork, that genuinely inefficient cartels and coercive monopolies, the target of the act, would be self-corrected by market forces, making the strict penalties of antitrust legislation unnecessary.[38]\\r\\nConversely, liberal Supreme Court Justice William O. Douglas criticized the judiciary for interpreting and enforcing the antitrust law unequally:\\r\\n\\"From the beginning it [the Sherman Act] has been applied by judges hostile to its purposes, friendly to the empire builders who wanted it emasculated... trusts that were dissolved reintegrated in new forms... It is ironic that the Sherman Act was truly effective in only one respect, and that was when it was applied to labor unions. Then the courts read it with a literalness that never appeared in their other decisions.\\"[39]\\r\\nAccording to a 2018 study in the journal Public Choice, \\"Senator John Sherman of Ohio was motivated to introduce an antitrust bill in late 1889 partly as a way of enacting revenge on his political rival, General and former Governor Russell Alger of Michigan, because Sherman believed that Alger personally had cost him the presidential nomination at the 1888 Republican national convention... Sherman was able to pursue his revenge motive by combining it with the broader Republican goals of preserving high tariffs and attacking the trusts. \\"[40]","input":"Who was president during the sherman anti-trust act?"},{"output":"Lok Sabha","context":"Coordinates: 28373N 771230E? / ?28.61750N 77.20833E? / 28.61750; 77.20833\\r\\nGovernment coalition (334)\\r\\nNational Democratic Alliance (334)\\r\\nOpposition Parties (211)\\r\\nUnited Progressive Alliance (50)\\r\\nJanata Parivar Parties (6)\\r\\nUnaligned Parties (144)\\r\\nOthers (11)\\r\\nThe Lok Sabha (House of the People) is the Lower house of India's bicameral Parliament, with the Upper house being the Rajya Sabha. Members of the Lok Sabha are elected by adult universal suffrage and a first-past-the-post system to represent their respective constituencies, and they hold their seats for five years or until the body is dissolved by the President on the advice of the council of ministers. The house meets in the Lok Sabha Chambers of the Sansad Bhavan in New Delhi.\\r\\nThe maximum strength of the House allotted by the Constitution of India is 552. Currently the house has 545 seats which is made up by election of up to 543 elected members and at a maximum, 2 nominated members of the Anglo-Indian Community by the President of India. A total of 131 seats (24.03%) are reserved for representatives of Scheduled Castes (84) and Scheduled Tribes (47). The quorum for the House is 10% of the total membership. The Lok Sabha, unless sooner dissolved, continues to operate for five years from the date appointed for its first meeting. However, while a proclamation of emergency is in operation, this period may be extended by Parliament by law.[3][4]\\r\\nAn exercise to redraw Lok Sabha constituencies' boundaries is carried out by the Boundary Delimitation Commission of India every decade based on the Indian census, last of which was conducted in 2001.[5] This exercise earlier also included redistribution of seats among states based on demographic changes but that provision of the mandate of the commission was suspended in 1976 following a constitutional amendment to incentivise the family planning programme which was being implemented.[6] The 16th Lok Sabha was elected in May 2014 and is the latest to date.\\r\\nThe Lok Sabha has its own television channel, Lok Sabha TV, headquartered within the premises of Parliament.[7]\\r\\n\\r\\n\\r\\nA major portion of the Indian subcontinent was under British rule from 1858 to 1947.[8] During this period, the office of the Secretary of State for India (along with the Council of India) was the authority through whom British Parliament exercised its rule in the Indian sub-continent, and the office of Viceroy of India was created, along with an Executive Council in India, consisting of high officials of the British government. The Indian Councils Act 1861 provided for a Legislative Council consisting of the members of the Executive Council and non-official members. The Indian Councils Act 1892 established legislatures in each of the provinces of British India and increased the powers of the Legislative Council. Although these Acts increased the representation of Indians in the government, their power still remained limited, and the electorate very small. The Indian Councils Act 1909 and the Government of India Act 1919 further expanded the participation of Indians in the administration. The Indian Independence Act 1947, passed by the British parliament on 18 July 1947, divided British India (which did not include the Princely States) into two new independent countries, India and Pakistan, which were to be dominions under the Crown until they had each enacted a new constitution. The Constituent Assembly was divided into two for the separate nations, with each new Assembly having sovereign powers transferred to it for the respective dominion.\\r\\nThe Constitution of India was adopted on 26 November 1949 and came into effect on 26 January 1950, proclaiming India to be a sovereign, democratic republic. This contained the founding principles of the law of the land which would govern India in its new form, which now included all the princely states which had not acceded to Pakistan.\\r\\nAccording to Article 79 (Part V-The Union.)[9] of the Constitution of India, the Parliament of India consists of the President of India and the two Houses of Parliament known as the Council of States (Rajya Sabha) and the House of the People (Lok Sabha).\\r\\nThe Lok Sabha (House of the Leaders) was duly constituted for the first time on 17 April 1952 after the first General Elections held from 25 October 1951 to 21 February 1952.\\r\\nArticle 84 (Part V.The Union)[10] of Indian Constitution sets qualifications for being a member of Lok Sabha, which are as follows:\\r\\nHowever, a member can be disqualified of being a member of Parliament:\\r\\nA seat in the Lok Sabha will become vacant in the following circumstances: (during normal functioning of the House)\\r\\nFurthermore, as per article 101 (Part V.The Union) [11] of Indian Constitution; A person cannot be?: (1) a member of both Houses of Parliament and provision shall be made by Parliament by law for the vacation by a person who is chosen a member of both Houses of his seat in one House or the other.(2) a member both of Parliament and of a House of the Legislature of a State.\\r\\nSystem of elections in Lok Sabha [12]\\r\\nMembers of the Lok Sabha are directly elected by the people of India, on the basis of Universal Suffrage. For the purpose of holding direct elections to Lok Sabha; each state is divided into territorial constituencies. In this respect, the constitution of India makes the following two provisions:\\r\\nNote: The expression population here refers to the population ascertained at the preceding census (2001 Census) of which relevant figure have been published.\\r\\nThe Lok Sabha has certain powers that make it more powerful than the Rajya Sabha.\\r\\nIn conclusion, it is clear that the Lok Sabha is more powerful than the Rajya Sabha in almost all matters. Even in those matters in which the Constitution has placed both Houses on an equal footing, the Lok Sabha has more influence due to its greater numerical strength. This is typical of any Parliamentary democracy, with the lower House always being more powerful than the upper.\\r\\nThe Rules of Procedure and Conduct of Business in Lok Sabha and Directions issued by the Speaker from time to time there under regulate the procedure in Lok Sabha. The items of business, notice of which is received from the Ministers/ Private Members and admitted by the Speaker, are included in the daily List of Business which is printed and circulated to members in advance. For various items of business to be taken up in the House the time is allotted by the House on the recommendations of the Business Advisory Committee. The Speaker presides over the sessions of the House and regulates procedure.\\r\\nThree sessions of Lok Sabha take place in a year:\\r\\nWhen in session, Lok Sabha holds its sittings usually from 11 A.M. to 1 P.M. and from 2 P.M. to 6 P.M. On some days the sittings are continuously held without observing lunch break and are also extended beyond 6 P.M. depending upon the business before the House. Lok Sabha does not ordinarily sit on Saturdays and Sundays and other closed holidays.\\r\\nThe first hour of every sitting is called Question Hour. Asking questions in Parliament is the free and unfettered right of members, and during Question Hour they may ask questions of ministers on different aspects of administration and government policy in the national and international spheres. Every minister whose turn it is to answer to questions has to stand up and answer for his department's acts of omission or commission.\\r\\nQuestions are of three typesStarred, Unstarred and Short Notice. A Starred Question is one to which a member desires an oral answer in the House and which is distinguished by an asterisk mark. An unstarred Question is one which is not called for oral answer in the house and on which no supplementary questions can consequently be asked. An answer to such a question is given in writing. Minimum period of notice for starred/ unstarred question is 10 clear days. If the questions given notice of are admitted by the Speaker, they are listed and printed for answer on the dates allotted to the Ministries to which the subject matter of the question pertains.\\r\\nThe normal period of notice does not apply to short notice questions which relate to matters of urgent public importance. However, a Short Notice Question may be answered only on short notice if so permitted by the Speaker and the Minister concerned is prepared to answer it at shorter notice. A short notice question is taken up for answer immediately after the Question Hour, popularly known as Zero Hour.\\r\\nZero Hour: The time immediately following the Question Hour has come to be known as \\"Zero Hour\\". It starts at around 12 noon (hence the name) and members can, with prior notice to the Speaker, raise issues of importance during this time. Typically, discussions on important Bills, the Budget, and other issues of national importance take place from 2 pm onwards.\\r\\nAfter the Question Hour, the House takes up miscellaneous items of work before proceeding to the main business of the day. These may consist of one or more of the following: Adjournment Motions, Questions involving breaches of Privileges, Papers to be laid on the Table, Communication of any messages from Rajya Sabha, Intimations regarding President's assent to Bills, Calling Attention Notices, Matters under Rule 377, Presentation of Reports of Parliamentary Committee, Presentation of Petitions, miscellaneous statements by Ministers, Motions regarding elections to Committees, Bills to be withdrawn or introduced.\\r\\nThe main business of the day may be consideration of a Bill or financial business or consideration of a resolution or a motion.\\r\\nLegislative proposals in the form of a Bill can be brought forward either by a Minister or by a private member. In the former case it is known as Government Bill and in the latter case it is known as a Private Members' Bill. Every Bill passes through three stagescalled three readingsbefore it is passed. To become law it must be passed by both the Houses of Parliament, Lok Sabha and Rajya Sabha, and then assented to by the president.\\r\\nThe presentation, discussion of, and voting on the annual General and Railways budgetsfollowed by the passing of the Appropriations Bill and the Finance Billis a long, drawn-out process that takes up a major part of the time of the House during its Budget Session every year.\\r\\nAmong other kinds of business that come up before the House are resolutions and motions. Resolutions and motions may be brought forward by Government or by private members. Government may move a resolution or a motion for obtaining the sanction to a scheme or opinion of the House on an important matter of policy or on a grave situation. Similarly, a private member may move a resolution or motion in order to draw the attention of the House and of the Government to a particular problem. The last two and half hours of sitting on every Friday are generally allotted for transaction of private members' business. While private members' bills are taken up on one Friday, private members' resolutions are taken up on the succeeding Friday, and so on.\\r\\nMost of the business of drafting a bill or amendments are initially discussed and debated in the parliamentary committees. Since the time for legislation is limited, work of all departments of the government and any special focus tasks is delegated to the committees, wherein the committees shall prepare the initial draft of the bill / amendment for the consideration by both the houses. They consist of members from both the houses\\r\\nThere are primarily two kinds of parliamentary committees based on their nature -\\r\\nA Half-an-Hour Discussion can be raised on a matter of sufficient public importance which has been the subject of a recent question in Lok Sabha irrespective of the fact whether the question was answered orally or the answer was laid on the Table of the House and the answer which needs elucidation on a matter of fact. Normally not more than half an hour is allowed for such a discussion. Usually, half-an-hour discussion is listed on Mondays, Wednesdays and Fridays only. In one session, a member is allowed to raise not more than two half-an-hour discussions. During the discussion, the member, who has given notice, makes a short statement and not more than four members, who have intimated earlier and have secured one of the four places in the ballot, are permitted to ask a question each for further elucidating any matter of fact. Thereafter, the Minister concerned replies. There is no formal motion before the House nor voting.\\r\\nMembers may raise discussions on matters of urgent public importance with the permission of the Speaker. Such discussions may take place on two days in a week. No formal motion is moved in the House nor is there any voting on such a discussion.\\r\\nAfter the member who initiates discussion on an item of business has spoken, other members can speak on that item of business in such order as the Speaker may call upon them. Only one member can speak at a time and all speeches are directed to the Chair. A matter requiring the decision of the House is decided by means of a question put by the Speaker on a motion made by a member.\\r\\nA division is one of the forms in which the decision of the House is ascertained. Normally, when a motion is put to the House members for and against it indicate their opinion by saying \\"Aye\\" or \\"No\\" from their seats. The Chair goes by the voices and declares that the motion is either accepted or rejected by the House. If a member challenges the decision, the Chair orders that the lobbies be cleared. Then the division bell is rung and an entire network of bells installed in the various parts and rooms in Parliament House and Parliament House Annexe rings continuously for three and a half minutes. Members and Ministers rush to the Chamber from all sides. After the bell stops, all the doors to the Chamber are closed and nobody can enter or leave the Chamber till the division is over. Then the Chair puts the question for second time and declares whether in its opinion the \\"Ayes\\" or the \\"Noes\\", have it. If the opinion so declared is again challenged, the Chair asks the votes to be recorded by operating the Automatic Vote Recording Equipment.\\r\\nWith the announcement of the Speaker for recording the votes, the Secretary- General presses the button of a key board. Then a gong sounds serving as a signal to members for casting their votes. For casting a vote each member present in the Chamber has to press a switch and then operate one of the three push buttons fixed in his seat. The push switch must be kept pressed simultaneously until the gong sounds for the second time after 10 seconds. There are two Indicator Boards installed in the wall on either side of the Speaker's Chair in the Chamber. Each vote cast by a member is flashed here. Immediately after the votes are cast, they are totalled mechanically and the details of the results are flashed on the Result Indicator Boards installed in the railings of the Speaker's and Diplomatic Galleries.\\r\\nDivisions are normally held with the aid of the Automatic Vote Recording Equipment. Where so directed by the Speaker in terms of relevant provision in the Rules of Procedure etc. in Lok Sabha, Divisions may be held either by distribution of 'Aye'/'No' and 'Abstention' slips to members in the House or by the members recording their votes by going into the lobbies. There is an Indicator Board in the machine room showing the name of each member. The result of Division and vote cast by each member with the aid of Automatic Vote Recording Equipment appear on this Board also. Immediately a photograph of the Indicator Board is taken. Later the Photograph is enlarged and the names of members who voted 'Ayes' and for 'Noes' are determined with the help of the photograph and incorporated in Lok Sabha Debates.\\r\\nThree versions of Lok Sabha Debates are prepared viz., the Hindi version, the English version and the Original version. Only the Hindi and English versions are printed. The Original version, in cyclostyled form, is kept in the Parliament Library for record and reference. The Hindi version all Questions asked and Answers given thereto in Hindi and the speeches made in Hindi as also verbatim Hindi translation of Questions and Answers and of speeches made in English or in regional languages. The English version contains Lok Sabha proceedings in English and the English translation of the proceedings which take place in Hindi or in any regional language. The Original version, however, contains proceedings in Hindi or in English as they actually take place in the House and also the English/Hindi translation of speeches made in regional languages.\\r\\nIf conflicting legislation is enacted by the two Houses, a joint sitting is held to resolve the differences. In such a session, the members of the Lok Sabha would generally prevail, since the Lok Sabha includes more than twice as many members as the Rajya Sabha.\\r\\nSpeaker and Deputy Speaker\\r\\nAs per Article 93 of Indian Constitution, the Lok Sabha has a Speaker and a Deputy Speaker. In the Lok Sabha, the lower House of the Indian Parliament, both presiding officersthe Speaker and the Deputy Speaker- are elected from among its members by a simple majority of members present and voting in the House. As such, no specific qualifications are prescribed for being elected the Speaker. The Constitution only requires that Speaker should be a member of the House. But an understanding of the Constitution and the laws of the country and the rules of procedure and conventions of Parliament is considered a major asset for the holder of the office of the Speaker. Vacation and resignation of, and removal from, the offices of Speaker and Deputy Speaker is mentioned under Article 94 of the Constitution of India. As per Article 94 of Indian Constitution. A Speaker or a Deputy Speaker, should vacate his/her office, a) if he/she ceases to be a member of the House of the People, b) he/she resigns, c) removed from his office by a resolution of the House of the People passed by a majority.\\r\\nThe Speaker of Lok Sabha is at once a member of the House and also its Presiding Officer. The Speaker of the Lok Sabha conducts the business in the house. He/she decides whether a bill is a money bill or not. He/she maintains discipline and decorum in the house and can punish a member for their unruly behaviour by suspending them. He/she permits the moving of various kinds of motions and resolutions like the motion of no confidence, motion of adjournment, motion of censure and calling attention notice as per the rules. The Speaker decides on the agenda to be taken up for discussion during the meeting. It is the Speaker of the Lok Sabha who presides over joint sittings called in the event of disagreement between the two Houses on a legislative measure. Following the 52nd Constitution amendment, the Speaker is vested with the power relating to the disqualification of a member of the Lok Sabha on grounds of defection. The Speaker makes obituary references in the House, formal references to important national and international events and the valedictory address at the conclusion of every Session of the Lok Sabha and also when the term of the House expires. Though a member of the House, the Speaker does not vote in the House except on those rare occasions when there is a tie at the end of a decision. Till date, the Speaker of the Lok Sabha has not been called upon to exercise this unique casting vote. While the office of Speaker is vacant due to absence/resignation/removal, the duties of the office shall be performed by the Deputy Speaker or, if the office of Deputy Speaker is also vacant, by such member of the House of the People as the President may appoint for the purpose.\\r\\nShri G. V. Mavalankar was the first Speaker of Lok Sabha (15 May 1952- 27 February 1956) and Shri M. Ananthasayanam Ayyangar was the first Deputy Speaker of Lok Sabha (30 May 1952 ÿ 7 March 1956). In the 16th Lok Sabha, Sumitra Mahajan was elected as the speaker on 3 June 2014, and is its second woman speaker and Shri M. Thambidurai as the deputy speaker.\\r\\nThe Lok Sabha has also a separate non-elected Secretariat staff.[13]\\r\\nLok Sabha is constituted after the general election as follows:\\r\\nCurrently elected members of 16th Lok Sabha by their political party (As of 18 December 2017):[15]","input":"Which is more powerful lok sabha or rajya sabha?"},{"output":"December 8, 1941","context":"On December 8, 1941, the United States Congress declared war (Public Law 77-328, 55 STAT 795) on the Empire of Japan in response to that country's surprise attack on Pearl Harbor the prior day. It was formulated an hour after the Infamy Speech of US President Franklin D. Roosevelt. Japan had sent a message to the United States to its embassy in Washington earlier, but because of problems at the embassy in decoding the very long message ÿ the high-security level assigned to the declaration meant that only personnel with very high clearances could decode it, which slowed down the process ÿ it was not delivered to the U.S. Secretary of State until after the Pearl Harbor attack. Following the U.S. declaration, Japan's allies, Germany and Italy, declared war on the United States, bringing the United States fully into World War II.\\r\\n\\r\\n\\r\\nThe attack on Pearl Harbor took place before a declaration of war by Japan had been delivered to the United States. It was originally stipulated that the attack should not commence until thirty minutes after Japan had informed the US that it was withdrawing from further peace negotiations,[1][2] but the attack began before the notice could be delivered. Tokyo transmitted the 5,000-word notification ÿ known as the \\"14-Part Message\\" ÿ in two blocks to the Japanese Embassy in Washington. However, because of the very secret nature of the message, it had to be decoded, translated and typed up by high embassy officials, who were unable to do these tasks in the available time. Hence, the ambassador did not deliver it until after the attack had begun. But even if it had been, the notification was worded so that it actually neither declared war nor severed diplomatic relations, so it was not a proper declaration of war as required by diplomatic traditions.[3]\\r\\nThe United Kingdom declared war on Japan nine hours before the US did, partially due to Japanese attacks on the British colonies of Malaya, Singapore, and Hong Kong; and partially due to Winston Churchill's promise to declare war \\"within the hour\\" of a Japanese attack on the United States.[4]\\r\\nPresident Roosevelt formally requested the declaration in his Infamy Speech, addressed to a joint session of Congress and the nation at 12:30?p.m. on December 8.[5] The declaration was quickly brought to a vote; it passed the Senate, and then passed the House at 1:10?p.m.[5] The vote was 82?ÿ 0 in the Senate and 388?ÿ 1 in the House. Jeannette Rankin, a pacifist and the first woman elected to Congress (in 1916), cast the only vote against the declaration, eliciting hisses from some of her peers. Several colleagues requested she change her vote to make the resolution unanimousor at least to abstainbut she refused.[6][7] \\"As a woman, I can't go to war,\\" she said, \\"and I refuse to send anyone else.\\" Nine other women held Congressional seats at the time. After the vote, an angry mob followed her from the Capitol building, forcing her to take refuge in a telephone booth until United States Capitol Police could rescue her. Two days later a similar war declaration against Germany and Italy came to vote; Rankin abstained. Nine other women voted in favor of the declaration of war.\\r\\nRoosevelt signed the declaration at 4:10 p.m the same day.[5] The power to declare war is assigned exclusively to Congress in the United States Constitution, making it an open question whether his signature was technically necessary.[5] However, his signature was symbolically powerful and resolved any doubts.\\r\\nWhereas the Imperial Government of Japan has committed unprovoked acts of war against the Government and the people of the United States of America:","input":"When did the u.s. declare war on japan?"},{"output":"Originally voiced by Lacey Chabert during the first season","context":"Megan \\"Meg\\" Griffin is a character from the American animated television series Family Guy. The eldest child of the Griffin family, Meg is the family's scapegoat who receives the least of their attention and bears the brunt of their abuse. She is often bullied, ridiculed, and ignored.\\r\\nMeg made her first appearance on television when Family Guy debuted on Fox on January 31, 1999, with the episode \\"Death Has a Shadow\\". Originally voiced by Lacey Chabert during the first season, she has been voiced by Mila Kunis since season 2, although Chabert returned to voice Meg in Yug Ylimaf and Back to the Pilot.\\r\\n\\r\\n\\r\\nMeg is a self-conscious and emotionally fragile teenage girl. She is treated unfairly by her parents and has multiple insecurities that cause her to try to be part of the \\"in-crowd\\", which only results in her getting rebuffed by Connie D'Amico,[1] the popular and beautiful yet mean-spirited and superficial head cheerleader of the local high school, James Woods Regional High School. However, another student named Neil Goldman is attracted to her.[2] She is also usually the butt of Peter's jokes due to her \\"homeliness\\" and unpopularity. Everyone in her family (except Chris) makes fun of her: Peter resorts to outrageous stunts and names; Stewie and Brian employ subtle but effective jokes, but typically choose not to verbalize them in front of Meg; Lois constantly puts Meg down while boosting her own egotistical image. However, in the episode \\"Seahorse Seashell Party\\", she strongly insults and defames her family for their actions against her, causing them to distance themselves in shame and sending Peter and Lois into a depression, though she later apologizes. On one occasion, the whole family even goes as far as to read her diary and mock her deepest secrets. However, on some occasions the family's true love for her has been proven, mostly by Chris. She has been so insecure about herself that she has engaged in dangerous sexual behavior just for attention. She is also prone to violent releases of her repressed rage, as shown in \\"Road to Rupert\\", when she assaults a man who insulted her after a fender-bender. Many of the show's \\"Meg\\" storylines involve her trying to improve her typical life, trying to find a boyfriend, and reaching breaking points with her family and other bullies who victimize her. In the episode \\"Chris Cross\\", it's strongly hinted that Meg is working with Dutch neo-Nazis when she orders Chris to retrieve mail she receives from the Netherlands at a private post office box, saying she's \\"part of a group that trashes Anne Frank's house every year.\\"\\r\\nOn the season 1 DVD commentary for the Drawn Together episode \\"Hot Tub\\", Cree Summer claims she was offered the role to play Meg but was dismissed by the producers. Meg was voiced by an uncredited Lacey Chabert for the first season, and by Mila Kunis in subsequent seasons after Chabert became busy with school and appearing on Party of Five at the time,[3] although some of her work became second season episodes due to production order. Mila Kunis won the role after auditions and a slight rewrite of the character, in part due to her performance on That '70s Show.[4] MacFarlane called Kunis back after her first audition, instructing her to speak more slowly, and then told her to come back another time and enunciate more. Once she claimed that she had it under control, MacFarlane hired her.[4] MacFarlane stated that Kunis \\"had a very natural quality to Meg\\" and she's \\"in a lot of ways [...] almost more right for the character\\". Kunis's voice is first heard as Meg in Episode 3 of season two \\"Da Boom\\", and the voices switch back and forth in the broadcast order until settling on Kunis.[5] Tara Strong provides Meg's singing voice in \\"Don't Make Me Over\\".[6]\\r\\nMeg, like Peter, has short brown hair and wears eyeglasses due to myopia. She also wears a pink beanie even underneath other headgear. Her clothing consists of a pink T-shirt with a white collar and white cuffs, blue jeans, and white shoes. She occasionally is seen wearing dresses or formal wear usually without her trademark cap. She is slightly shorter than her younger brother Chris. Meg is also self-conscious about her appearance (\\"I'm so fat and gross.\\").[10]\\r\\nMeg is very unpopular in high school due to both her plain appearance and meek personality. She desperately tries to be part of the cool crowd, but is usually coldly rebuffed. Because of her eagerness for acceptance, she has been recruited \\"unknowingly\\" into a suicidal religious cult,[11] and later recruited again unwittingly into her school's Lesbian Alliance (\\"Brian Sings and Swings\\").[12] However, Meg does have a moderate number of friends, the best of which being with a group of girls who are often seen with her during occasions such as her slumber parties and gossiping about boys.[13] In later episodes, these girls, known by the names Beth, Patty, Collette, Esther, and Ruth, are characterized as being highly unpopular and dateless, much like Meg, even saying that she was the only one of them who ever had a boyfriend (actually a decaying corpse she was \\"dating\\").\\r\\nMeg is so unpopular at school that one student fires a nail gun into his own stomach twice (in shop class) in order to avoid a date with her, and then in a later episode shoots his own brother as an excuse not to go to a dance with her the following night. On another occasion, Meg and Lois are looking for new clothes for Meg, but with no luck (a saleswoman ended up pouring gasoline on herself, lighting a match, catching fire, and then jumping out of a window after looking at Meg in a pair of jeans). However, she is sought by nerd Neil Goldman. In \\"8 Simple Rules for Buying My Teenage Daughter\\", Neil starts dating a girl named Cecilia, Meg becomes instantly jealous and pretends to date Jake Tucker to make him jealous. This leads to her signing a contract to become Neil's girlfriend and (not knowing at first) his slave, but she gets him to tear up the contract after Lois seduces him. Perverted neighbor Glenn Quagmire has shown a repeated interest in her, mostly due to his very low standards, asking if she has reached the age of consent. Quagmire comes close to succeeding when Lois tells Peter to back off after he was ruining Meg and Quagmire's 'dates'. Then, they rescue Meg after Glenn takes her to his cabin, Peter and Lois arriving in time before anything happens.[14] In several episodes she is shown dating, including stories with characters Mayor Adam West[15] and nudist Jeff Campbell.[16] She also loses her virginity unknowingly on live television to Saturday Night Live host Jimmy Fallon after having a drastic makeover; but, before all that happens, she goes out with a rebel at her school named Craig Hoffman.[17] In \\"Jerome is the New Black\\", Jerome, an old flame of Lois's and Peter's new friend, admits to having sex with Meg, to which Peter replies indifferently.\\r\\nIn the episode \\"Brian Sings and Swings\\", a lesbian student named Sarah invites Meg to join in her Lesbian Alliance Club, with Meg not knowing at first what kind of club it was. Desperate to fit in, she pretends to be a lesbian and also pretends to be attracted to Sarah and even goes so far as to kiss her to prove it. At the end of the episode, Meg goes over to Sarah's house to admit she lied about being a lesbian (Sarah thought that Meg came over to have sex and even undresses when Meg is telling her that she lied), much to Glenn's (who was hiding in Sarah's closet) disappointment. She also used to have a crush on anchorman Tom Tucker, but it ended after she discovered his vanity and selfishness.\\r\\nIn other episodes, she is portrayed as chronically incapable of finding a boyfriend. For her Junior Prom she accepts a pity date from Brian, the family dog and only after threatening suicide.[10]\\r\\nEarlier in season 2, she dated Joe Swanson's son Kevin Swanson, but in \\"Stew-Roids\\" it is mentioned that Kevin died in Iraq. In the episode, \\"Prick Up Your Ears\\", she dates a boy named Doug, but he breaks up with her when he sees her naked right before almost having sex. In the episode \\"Peter's Daughter\\", Meg falls in love with a medical student named Michael Milano after coming out of a short coma (caused by Peter when he asked her to \\"rescue\\" beer and make him a sandwich out of an already flooded kitchen) and they start to date. After he breaks up with Meg (because of Peter being overprotective of her after promising that if she came out of the coma, he would \\"treat her like a princess\\"), she announces that she is pregnant by Michael and the two get engaged. After finding out that she is not actually pregnant, Meg tells Michael the truth hoping that he will stay, however, Michael quickly leaves Meg at the altar. In the episode \\"Dial Meg for Murder\\", she is dating a convict, while in the episode \\"Go, Stewie, Go!\\" she dates an attractive young man named Anthony, who is absolutely normal (much to the surprise of many of the other characters). They were so shocked that they had to do tests just to see if he was completely normal which annoyed Meg. It is presumed that she broke up with him after he and Lois had an affair. Meg also shows extremely possessive behavior when she encounters someone she believes she has a romantic connection with such as kidnapping Brian and detaining Bonnie Swanson at the airport by planting a gun in her purse. Overall, Meg has shown romantic interest in and dated several men throughout the series. However, there have been several instances in which she has shown hints of being bisexual or a lesbian: examples of this include \\"Brian Sings and Swings\\", \\"Stew-Roids\\", and \\"Dial Meg for Murder\\".\\r\\nIn November 2016, when asked by Splitsider if the writers will further develop the characters of Chris and Meg in future episodes, showrunner Alec Sulkin confirmed that the series crew members are working on doing so and added that there are plans for an episode where Meg comes out as a lesbian, taking inspiration from previous instances in which she exhibited signs of lesbian characteristics, like when she joins a lesbian alliance group at school in \\"Brian Sings and Swings\\" and is identified as a transgender man named \\"Ron\\" in Stewie Griffin: The Untold Story, the latter which takes place in the future. However, Sulkin also noted that the plotline has not yet been finalized and thus isn't officially set to be used in an episode.[18][19]\\r\\nIn the first three seasons of the show, the family had inadvertently humiliated Meg due to their bumbling or stupidity, though they cared for her and meant well. In the post-cancellation seasons, this began to change as the show started to flesh out the characters to the point where it appears that most of the population of Quahog dislike her for no reason other than her simply being \\"Meg\\".[20] In an interview, Mila Kunis stated: \\"Meg gets picked on a lot. But it's funny. It's like the middle child. She is constantly in the state of being an awkward 16-year-old, when you're kind of going through puberty and what-not. She's just in perpetual mode of humiliation, and it's fun.\\"[21]\\r\\nMeg is also the most misunderstood, at least by Peter, Chris, and others who are shown avoiding her company, disparaging her in person, gathering in her bedroom to read her diary for laughs, etc.[22] Peter reminds Lois, \\"We agreed that if we could only save two, we'd leave Meg!\\"[23] even randomly shooting her when she simply said \\"Hi Dad\\" (\\"Peter's Daughter\\") but despite this he also was going to say \\"I love you\\" in \\"Hell Comes to Quahog\\", and in \\"Road to Rupert\\" he stated they were 'secret best friends' before throwing lemonade in her face, saying he would have to continue to treat her badly in public in order to maintain his reputation due to \\"peer pressure\\", thus giving hope that they may be on good terms. Occasionally, when Meg asks a question to Peter or just speaks when he is in the room, Peter responds by saying \\"Shut up, Meg,\\" which is immediately followed by a line from another character.\\r\\nWhen the family tries an anger management technique of writing letters and not sending them, Meg finds Peter's letter to her, which says, \\"Dear Meg, for the first four years of your life, I thought you were a house cat.\\" [24] And in Peter's short story of her birth, they had to go back to get her once they realized they grabbed the afterbirth. In the episode \\"Stewie Kills Lois\\", Peter tells guests on a cruise ship about how he and Lois had gone to get an abortion but decided against it when they arrived at the clinic and found out the abortionist had one hand. He then says \\"two and a half months later, our daughter Meg was born\\" ÿ indicating that they had tried to abort her when Lois was already over six months pregnant. Another hint to this is when Meg is in the car with Lois and at an attempt to make civilized conversation, says, \\"Hey Meg, did you know that if you're on birth control and you take an antibiotic it makes it not work? 'Cause no one told me! I just thought you should know\\" and laughs awkwardly. On Meg's 17th birthday, her mother and father both try to hide from Meg that they do not remember her age.[25] Peter states that Meg sucks in the episode \\"PTV\\", and Chris says that people think the same thing about her in \\"Long John Peter\\". In \\"Not All Dogs Go to Heaven\\", Brian says to Meg's face that she lives in a home \\"where nobody respects or cares about [her], not even enough to get [her] a damn mumps shot!\\" Chris, however, has more of a typical brother-sister relationship with Meg, with Chris telling those who condemn her that it is never her fault. Chris, at one point, threatened to quit his job at the local mini-mart if his boss didn't re-hire Meg (at the insistence of Lois).[26] Like Chris, Meg has an anthropomorphic monkey in her closet, and although she has proved it, her family coldly state that they were talking about Chris', not hers. Cleveland comments to Peter \\"Meg is my least favorite of your children.\\"[27]\\r\\nApparently, a double standard also exists against Meg, further underscoring the mistreatment she suffers from the people around her. In \\"Big Man on Hippocampus\\", an episode wherein Peter loses his memory, as he reacquaints himself with the pleasures of sex, Lois tells him that it is inappropriate to have sex with his own children; in response, Meg attempts an incest joke. She then lambastes her for this and kicks her out of the room.[28] However, in the season finale \\"Partial Terms of Endearment\\", Lois tells a joke that implies that it was Meg that gave birth to Stewie, and apart from a shocked reaction from the latter, Lois receives no such violent reaction.[29] Additionally, in \\"Model Misbehavior\\", when Lois starts a modeling career, Peter states that he will pleasure himself to Lois' pictures, followed by Chris and Meg both exclaiming \\"Me too!\\" to which Peter shouts \\"Oh God, Meg, that's sick! That's your mother!\\", ignoring the fact that Chris said the same thing. Meg responded by saying \\"I was only trying to fit in!\\" Peter immediately kicked her out of the house. Later, during an unrelated conversation, when she spoke, he said \\"Meg, who let you back in the house?\\"\\r\\nBrian's attention softens the lack of respect from Peter and the neighbors; he admits that he cares for Meg when she goes out with Mayor Adam West. Lois has also often shown sympathy for Meg, for example, taking her to Spring Break at the beach. Lois very often comforts Meg when she is down; however, she gives up one attempt after 45 minutes and gives her a Sylvia Plath novel and a bottle of Ambien, and with a \\"Whatever happens, happens,\\" leaves Meg to her misery.[27] One of the most cruel examples comes in the episode \\"You Can't Do That On Television, Peter\\". When Peter is mauled by a puma, Meg uses medical training to save his life. However, no one thanks her afterward and when she tries to point it out, Peter just tells her to get him water.\\r\\nThe family's treatment of Meg finally reaches her limit in \\"Dial Meg for Murder\\" when Meg emerges from a short stint in a Young Offenders Institution as a hardened criminal, abusing her family and beating up anyone who makes fun of her. It is only after a conversation with Brian that she changes her ways. However, it comes to a head once again in \\"Seahorse Seashell Party\\", when Meg finally grows tired of her mistreatment and lashes out against Lois and Peter, informing them of their own flaws. Lois condescendingly tells Meg that she is simply taking her own problems out on everyone else invoking Meg to bring up her mother's delinquent past. Meg tells her that she is far from the perfect parent, harshly berates her for constantly and ruthlessly pointing out Meg's shortcomings. Lois tries to justify that she's a better person because of her past and she is open that she isn't the perfect parent, but Meg tells her that she's the farthest thing from and states how she has neglected to protect her from harm and guide her through life. Meg also informs Lois that when she turns 18, she may never want to see her again.\\r\\nThis breaks Lois' heart and she finally admits that she's been a terrible mother to Meg. Finally, Meg turns on Peter who, unable to comprehend her insults, thinks that his daughter's argument is amusing, even when she points out Peter's destructive tendencies and that he would go to jail if someone could witness his negative treatment towards her. It dawns on Peter that he is being insulted when Meg calls him a \\"waste of a man.\\" A disillusioned Peter asks Lois to tell Meg to \\"knock it off,\\" but Lois refuses because he didn't stick up for her. Within moments, Peter turns his abusive criticisms and insults on Chris and Lois. Peter finally runs to his room crying, with Lois running after him, leaving behind Meg and Brian, who is now fully recovered from his trip, to discuss what just happened. Brian likes that Meg stood up for herself, but she sadly tells him that even though she meant every word, seeing Peter turn on everyone like wolves has made her think that it is ultimately her non-ideal role to serve as the Griffins' \\"lightning rod that absorbs all the dysfunction\\". He commends her on her maturity, and even goes on to say that Meg is the \\"strongest person\\" in the family. She soon apologizes to the others and says that she is actually the one at fault. Since this episode, the abuse that Meg receives begins to fade away as a storyline. She also notices that Peter's pro wrestler sister Karen treats Peter exactly the way Peter treats her, and they bond over this with a plan to embarrass Karen at a wrestling showwhich goes awry when Meg hits her with a metal folding chair instead of a breakaway one and injures Karen to the point where she ends up in a coma and (as it is implied) possibly will die from her injuries without a blood transfusion.","input":"Who is the original voice of meg griffin?"},{"output":"military operation related to homeland security and support to federal, state, and local agencies","context":"\\r\\n\\r\\nOperation Noble Eagle (ONE) is the United States and Canadian military operation related to homeland security and support to federal, state, and local agencies. The operation began 14 September 2001, in response to the September 11 attacks.[1]\\r\\n\\r\\nOperation Noble Eagle began with the mobilization of thousands of National Guard and reserve personnel to perform security missions on military installations, airports and other potential targets such as bridges, power plants, and port facilities. These reservists were called to active duty under a mobilization authority known as a partial mobilization (10 USC 12302). In a time of national emergency declared by the President, partial mobilization authorizes the President to order members of the ready reserve to active duty for a period not to exceed 24 consecutive months. Additionally, in 2001 and 2002, thousands of members of the national guard were activated at the order of their respective governors to provide additional security at airports. They were called up under Title 32 of the U.S. Code, which means they were under state control, but with federal pay and benefits.[1] The Royal Canadian Air Force assisted in providing defense of the northern border of the United States. The United States Army's 759th Military Police Battalion, 144th Military Police Company and the 177th Military Police Brigade were assigned the task of protecting the White House, the Pentagon and the Capitol, along with other Army National Guard units that were tasked with augmenting the Air Force's air defense perimeter around the National Capital Region.\\r\\n\\r\\nUnited States civilian and military leaders are beginning[needs update] to regard the costly air defense operation above North American cities as a permanent defense requirement demanding significant attention from NORAD. The current focus is on improving command and control of the homeland air defense mission.[2]\\r\\n\\r\\nThe United States Department of Defense provided F-15 Eagles[3] and F-16 Fighting Falcons to this operation, and the Canadian Forces provided CF-18s.[4] The US Army National Guard provided short range air defense systems (AVENGER/MANPADS) to provide close range air defense protection under the control of the US Air Force JADOC (joint air defense operations center) and CONR.\\r\\n\\r\\nThe Joint Air Defense Operations Center Suite (JADOC) Suite is an integrated USAF connectivity center used in point defense of the National Capital Region. The Suite provides capability for tactical C2 execution for the Joint Task Force/Joint Forces Air Component Commander (JTF/JFACC) under the Operation Noble Eagle (ONE) mission as part of the National Capital Region-Integrated Air Defense System (NCR-IADS - National Capital Region Integrated Air Defense Systems). It is currently fielded and operational. It was developed in response to an urgent need request for enhanced air defense of the NCR per SECDEF direction through a joint rapid acquisition cell. The architecture consists of an air surveillance fusion system and the C2 connectivity center, which includes a Link 16 capable terminal and Situational Awareness Data Link (SADL) capability.\\r\\n\\r\\nThe NCR-IAD's warning equipment include visual warning lasers, flare cannons, FAA radars, military radar systems, advanced electro-optical sensors (Enhanced Regional Situation Awareness), and regular cameras. Its \\"engagement assets\\" include the Norwegian Advanced Surface to Air Missile System (NASAMS) for long-range air defense; SLAMRAMMs  Surface Launched Advanced Medium Range Air to Air Missile batteries mounted on Humvees  for protection close-in; Sentinel phased array warning radar, which detects smaller targets at a range of 75 miles; and the Army's AVENGER system for threats closer to the ground or closer to the center of the protected area. Stinger missiles situated around D.C. require humans to aim them and confirm targeting.\\r\\n\\r\\nAs the Canadian geographical component of NORAD, CANR provides airspace surveillance and control, and directs all air sovereignty activities for the Canadian NORAD Region. CANR and its assigned Air Force assets throughout the country ensure air safety and security against potential air threats and have supported special events such as the G-8 Summit and the visits of foreign dignitaries.[5]\\r\\n\\r\\nThe Canadian NORAD Region (CANR) flew Operation Noble Eagle (ONE) air defense protection missions in the Windsor, Ontario/Detroit, Michigan area on 5 February 2006, in support of Super Bowl XL at Ford Field. These types of missions had become more common at organized entertainment such as the Super Bowl.[5]","input":"What was the purpose of operation noble eagle?"},{"output":"Draco","context":"Draco (/?dre?ko?/; Greek: ?, Drakn; fl. c. 7th century BC) was the first recorded legislator of Athens in Ancient Greece. He replaced the prevailing system of oral law and blood feud by a written code to be enforced only by a court of law. Draco was the first democratic legislator, he was requested by the Athenian citizens to be a lawgiver for the city-state, but the citizens were fully unaware that Draco would establish harsh laws.[1] Draco's written law was characterized by its harshness. To this day, the adjective draconian refers to similarly unforgiving rules or laws, in English[2] and other European languages.[3]\\r\\n\\r\\n\\r\\nDuring the 39th Olympiad, in 622 or 621 BC, Draco established the legal code with which he is identified.\\r\\nLittle is known about his life. He may have belonged to the Greek nobility of Attica, with which the 10th-century [Suda] text records him as contemporaneous, prior to the period of the Seven Sages of Greece. It also relates a folkloric story of his death in the Aeginetan theatre.[4] In a traditional ancient Greek show of approval, his supporters \\"threw so many hats and shirts and cloaks on his head that he suffocated, and was buried in that same theatre\\".[5] The truth about his death is still unclear, but we do know that Draco was driven out of Athens by the Athenians to the neighbouring island of Aegina, where he spent the remainder of his life.[6]\\r\\nThe laws (Ĳ? - thesmoi) that he laid were the first written constitution of Athens. So that no one would be unaware of them, they were posted on wooden tablets (?? - axones), where they were preserved for almost two centuries on steles of the shape of three-sided pyramids (?ǚ? - kyrbeis).[7] The tablets were called axones, perhaps because they could be pivoted along the pyramid's axis to read any side.\\r\\nThe constitution featured several major innovations:\\r\\nThe laws were particularly harsh. For example, any debtor whose status was lower than that of his creditor was forced into slavery.[9] The punishment was more lenient for those owing a debt to a member of a lower class. The death penalty was the punishment for even minor offences, such as stealing a cabbage.[10] Concerning the liberal use of the death penalty in the Draconic code, Plutarch states: \\"It is said that Drakon himself, when asked why he had fixed the punishment of death for most offences, answered that he considered these lesser crimes to deserve it, and he had no greater punishment for more important ones\\".[11]\\r\\nAll his laws were repealed by Solon in the early 6th century BC, with the exception of the homicide law.[12]\\r\\nAfter much debate, the Athenians decided to revise the laws, including the homicide law, in 409 BC.[13] The homicide law is a highly fragmented inscription but states that it is up to the victim's relatives to prosecute a killer. According to the preserved part of the inscription, unintentional homicides received a sentence of exile.\\r\\nIt is not clear whether Draco's law specified the punishment for intentional homicide. In 409 BC, intentional homicide was punished by death, but Draco's law begins, 'ϫ? ?? ?  []?[ϫ]? [][? ?? ϫ, ?][]', which is ambiguous and difficult to translate. One possible translation offers, \\"Even if a man not intentionally kills another, he is exiled\\".[14]\\r\\nDraco introduced the lot-chosen Council of Four Hundred,[15] distinct from the Areopagus, which evolved in later constitutions to play a large role in Athenian democracy. Aristotle notes that Draco, while having the laws written, merely legislated for an existing unwritten Athenian constitution[16] such as setting exact qualifications for eligibility for office.\\r\\nDraco extended the franchise to all free men who could furnish themselves with a set of military equipment. They elected the Council of Four Hundred from among their number; nine archons and the treasurers were drawn from persons possessing an unencumbered property of not less than ten minas, the generals (strategoi) and commanders of cavalry (hipparchoi) from those who could show an unencumbered property of not less than a hundred minas and had children born in lawful wedlock over ten years of age. Thus, in the event of their death, their estate could pass to a competent heir. These officers were required to hold to account the prytanes (councillors), strategoi (generals) and hipparchoi (cavalry officers) of the preceding year until their accounts had been audited. \\"The Council of Areopagus was guardian of the laws, and kept watch over the magistrates to see that they executed their offices in accordance with the laws. Any person who felt himself wronged might lay an information before the Council of Areopagus, on declaring what law was broken by the wrong done to him. But, as has been said before, loans were secured upon the persons of the debtors, and the land was in the hands of a few.\\"[17]\\r\\nTranslation of original inscription","input":"Who is regarded as the first law giver?"},{"output":"September 1998","context":"Tuition fees were first introduced across the entire United Kingdom in September 1998 under the Labour government as a means of funding tuition to undergraduate and postgraduate certificate students at universities, with students being required to pay up to S1,000 a year for tuition.[1][2] However, as a result of the establishment of devolved national administrations for Scotland, Wales and Northern Ireland, different arrangements now exist with regard to the charging of tuition fees in each of the countries of the United Kingdom.\\r\\n\\r\\n\\r\\nIn May 1996, Gillian Shephard, Secretary of State for Education and Employment, commissioned an inquiry, led by the then Chancellor of the University of Nottingham, Sir Ron Dearing, into the funding of British higher education over the next 20 years.[3] This National Committee of Inquiry into Higher Education reported to the new Labour Government, in the summer of 1997, stating additional billions of funding would be needed over the period, including S350 million in 1998-9 and S565 million in 1999-2000, in order to expand student enrolment, provide more support for part-time students and ensure an adequate infrastructure.[4][5] The committee, as part of its brief, had controversially investigated the possibility of students contributing to the cost of this expansion, either through loans, a graduate tax, deferred contributions or means testing state assistance, as their report notes[6]:\\r\\n20.40 We do not underestimate the strength of feeling on the issue of seeking a contribution towards tuition costs: nor do we dispute the logic of the arguments put forward. A detailed assessment of the issues has, however, convinced us that the arguments in favour of a contribution to tuition costs from graduates in work are strong, if not widely appreciated. They relate to equity between social groups, broadening participation, equity with part-time students in higher education and in further education, strengthening the student role in higher education, and identifying a new source of income that can be ring-fenced for higher education.\\r\\n20.41 We have, therefore, analysed the implications of a range of options against the criteria set out in paragraph 20.2. There is a wide array of options from which to choose, ranging from asking graduates to contribute only to their living costs through to asking all graduates to contribute to their tuition costs. We have chosen to examine four options in depth\\r\\nIn response to the findings, the Teaching and Higher Education Act 1998 was published on 26 November 1997, and enacted on 16 July 1998, part of which introduced tuition fees in all the countries of the United Kingdom.[7] The act introduced a means-tested method of payment for students based on the amount of money their families earned.[8] Starting with 1999-2000, maintenance grants for living expenses would also be replaced with loans and paid back at a rate of 9 percent of a graduate's income above S10,000.[7]\\r\\nFollowing devolution in 1999, the newly devolved governments in Scotland and Wales brought in their own acts on tuition fees. The Scottish Parliament established, and later abolished a graduate endowment to replace the fees.[9] Wales introduced maintenance grants of up to S1,500 in 2002, a value which has since risen to over S5000.[10]\\r\\nIn England, tuition fee caps rose with the Higher Education Act 2004. Under the Act, universities in England could begin to charge variable fees of up to S3000 a year for students enrolling on courses as from the academic year of 2006-07 or later. This was also introduced in Northern Ireland in 2006-07 and introduced in Wales in 2007-08. In 2009-10 the cap rose to S3225 a year to take account of inflation.[11] Following the Browne Review in 2010, the cap was controversially raised to S9,000 a year, sparking large student protests in London. A judicidal review against the raised fees failed in 2012, and so the new fee system came into use that September.[12]\\r\\nStudents pay interest on loans. In 2012 this rate was set at the retail price index (RPI) plus 3%. Students who started university between 1998 and 2011 pay Bank of England base rate plus 1%. Students who started university before 1998 pay interest set at the RPI rate. As a consequence of the 2012 change, students who graduate in 2017 will pay 6.1% interest, despite the Bank of England base rate being 0.25%.[13]\\r\\nFurther adjustments were put forth in the 2015 budget, with a proposed fee increase in line with inflation from the 2017-18 academic year onwards, and the planned scrapping of maintenance grants from September 2016.[14] The changes were debated by the Third Delegated Legislation Committee in January 2016, rather than in the Commons. The lack of a vote on the matter has drawn criticism, as by circumventing the Commons the measures \\"automatically become law\\".[15] Tuition fees and perceptions about them are directly linked to satisfaction[16].\\r\\nIn England, tuition fees are capped at S9,250 a year for UK and EU students, with around 76% of all institutions charging the full amount in 2015-16.[18] A loan of the same size is available for most universities, although students of private institutions are only eligible for S6,000 a year loans.\\r\\nFrom 2017-18 onwards, the S9,000 fee cap will rise with inflation. Maintenance grants are also available to current students in England, although these are scheduled to cease with the 2016-17 academic year.[15] Maintenance loans are available for living costs, and these are means tested. These loans are scheduled to increase in size for 2016-17, when the maintenance grant system is phased out.[19] There will be a vote in the autumn to consider a further increase effective with the 2017-18 year. Several universities have already advertised fees of S9,250 for the year in anticipation of such a vote passing.[20]\\r\\nIn the 2015 spending review, the government also proposed a freeze in the repayment threshold for tuition fee loans at S21,000; a figure which was previously set to rise with average earnings. The changes, if passed, will affect all Plan 2 tuition fee loans, backdated to cover loans taken out from 2012.[21][22]\\r\\nMany commentators suggested that the 2012 rise in tuition fees in England would put poorer students off applying to university.[23] However, the gap between rich and poor students has slightly narrowed (from 30.5% in 2010 to 29.8% in 2013) since the introduction of the higher fees.[24] This may be because universities have used tuition fees to invest in bursaries and outreach schemes.[25] In 2016, The Guardian noted that the number of disadvantaged students applying to university had increased by 72% from 2006 to 2015, a bigger rise than in Scotland, Wales or Northern Ireland.[26] It wrote that most of the gap between richer and poorer students tends to open up between Key Stage 1 and Key Stage 4 (i.e. at secondary school), rather than when applying for university, and so the money raised from tuition fees should be spent there instead.[26]\\r\\nResearch by the National Bureau of Economic Research found that the introduction of tuition fees had \\"increased funding per head, rising enrolments, and a narrowing of the participation gap between advantaged and disadvantaged students.\\"[27]\\r\\nTuition fees are currently capped at S3,805 in Northern Ireland, with loans of the same size available from Student Finance NI.[28] Loan repayments are made when income rises above S17,335 a year, with graduates paying back a percentage of their earnings above this threshold.[29]\\r\\nTuition is handled by the Student Awards Agency Scotland (SAAS), which does not charge fees to what it defines as \\"Young Students\\". Young Students are defined as those under 25, without children, marriage, civil partnership or cohabiting partner, who have not been outside of full-time education for more than three years. Fees exist for those outside the young student definition, typically from S1,200 to S1,800 for undergraduate courses, dependent on year of application and type of qualification. Postgraduate fees can be up to S3,400.[30]\\r\\nThe system has been in place since 2007 when graduate endowments were abolished.[31] Labour's education spokesperson Rhona Brankin criticised the Scottish system for failing to address student poverty.[32] Scotland has fewer disadvantaged students than England, Wales or Northern Ireland and disadvantaged students receive around S560 a year less in financial support than their counterparts in England do.[25]\\r\\nLike their English counterparts, Welsh universities are able to charge up to S9,000 a year in tuition fees. However, Welsh students can apply for fee grants of up to S5,190, in addition to a S3,810 loan to cover these costs.[33] This system also applies to Welsh students who study elsewhere in the United Kingdom.\\r\\nThere have been two main proposed alternative ways of funding university studies: from general taxation or by a graduate tax.\\r\\nTuition is paid for by general taxation in Germany, although only 27% of young people gain higher education qualification there, whereas in the UK the comparable figure is 48%.[34] Fully or partly funding universities from general taxation has been criticised by the Liberal Democrat party as a 'tax cut for the rich and a tax rise for the poor' because people would be taxed to pay for something that many would not derive a benefit from, while graduates generally earn more due to their qualifications and only have to pay them back.[35]\\r\\nJeremy Corbyn, current Labour leader, has stated that he would remove tuition fees and instead fund higher education by increasing National Insurance and Corporation Tax.[36] In the long term this would cost the government about S8 billion a year.[37]\\r\\nIn July 2017 Lord Adonis, former Number 10 Policy Unit staffer and education minister largely responsible for introducing tuition fees, said that the system had become a \\"Frankenstein's monster\\" putting many students over S50,000 in debt. He argued the system should either be scrapped or fees reverted to between S1,000 and S3,000 per the initial scheme.[38][39]\\r\\nDuring the 2015 Labour leadership election, Andy Burnham said that he would introduce a graduate tax to replace fees. He was ultimately unsuccessful in his bid for leadership. A graduate tax has been criticised because there would be no way to recover the money from students who move to a different country, or foreign students who return home.[40]","input":"When were tuition fees introduced in the uk?"},{"output":"a state of conflict between nations that does not involve direct military action but is pursued primarily through economic and political actions, propaganda, acts of espionage or proxy wars waged by surrogates","context":"A cold war is a state of conflict between nations that does not involve direct military action but is pursued primarily through economic and political actions, propaganda, acts of espionage or proxy wars waged by surrogates. This term is most commonly used to refer to the Soviet-American Cold War. The surrogates are typically states that are \\"satellites\\" of the conflicting nations, i.e., nations allied to them or under their political influence. Opponents in a cold war will often provide economic or military aid, such as weapons, tactical support or military advisors, to lesser nations involved in conflicts with the opposing country.\\r\\n\\r\\n\\r\\nThe expression \\"cold war\\" was rarely used before 1945. Some writers credit the fourteenth century Spaniard Don Juan Manuel for first using the term (in Spanish), when dealing with the conflict between Christianity and Islam as a \\"cold war\\". However he used the term \\"tepid\\" not \\"cold\\". The word \\"cold\\" first appeared in a faulty translation of his work in the 19th century.[1]\\r\\nAt the end of World War II, George Orwell used the term in the essay \\"You and the Atomic Bomb\\" published October 19, 1945, in the British newspaper Tribune. Contemplating a world living in the shadow of the threat of nuclear war, he warned of a \\"peace that is no peace\\", which he called a permanent \\"cold war\\".[2] Orwell directly referred to that war as the ideological confrontation between the Soviet Union and the Western powers.[3] Moreover, in The Observer of March 10, 1946, Orwell wrote that \\"[a]fter the Moscow conference last December, Russia began to make a cold war on Britain and the British Empire.\\"[4]\\r\\nThe definition which has now become fixed is of a war waged through indirect conflict. The first use of the term in this sense, to describe the postÿWorld War II geopolitical tensions between the USSR and its satellites and the United States and its western European allies (which in practice acted as satellites of the opposing force) is attributed to Bernard Baruch, an American financier and presidential advisor.[5] In South Carolina, on April 16, 1947, he delivered a speech (by journalist Herbert Bayard Swope)[6] saying, \\"Let us not be deceived: we are today in the midst of a cold war.\\"[7] Newspaper reporter-columnist Walter Lippmann gave the term wide currency, with the book Cold War (1947).[8]\\r\\nSince the NATO-Warsaw Pact Cold War (1945ÿ1991), a number of global and regional tensions have also been called \\"Cold War.\\"\\r\\nCold War II,[9][10] also called Second Cold War,[11][12][13] Cold War 2.0,[14][15] or the New Cold War[16][17] refers to a renewed state of political and military tension between opposing geopolitical power-blocs, with one bloc typically reported as being led by Russia or China,[18] and the other led by the United States or NATO. This is akin to the original Cold War that saw a global confrontation between the Western Bloc led by the United States and the Eastern Bloc led by the Soviet Union, Russia's predecessor.\\r\\nAn Atlantic Council member Bilal Y. Saab,[19] an About.com writer Primoz Manfreda,[20] an Iranian scholar Seyyed Hossein Mousavian and a Princeton University scholar Sina Toossi,[21] journalist Kim Ghattas,[22] Foreign Policy journalist Yochi Dreazen,[23] Brookings Institution researcher Sultan Barakat,[24] and Newsweek journalist Jonathan Broder[25] use the term \\"cold war\\" to refer to tensions between Saudi Arabia and Iran. In February 2016, a University of Isfahan professor Ali Omidi dismissed the assumptions that the conflict between Iran and Saudi Arabia would grow tense.[26]\\r\\nA commentator Ehsan Ahrari,[27] a writer Bruce Riedel,[28] a political commentator Sanjaya Baru[29] and a Princeton University academic Zia Mian[30] have used the term \\"cold war\\" since 2002 to refer to long-term tensions between India and Pakistan, which were part of the British India until its partition in 1947.\\r\\nA Naval Postgraduate School academic Edward A. Olsen,[31][32] a British politician David Alton,[33] a York University professor Hyun Ok Park,[34] and a University of Southern California professor David C. Kang[35] used the term to refer to tensions between North Korea and South Korea, which have been divided since the end of World War II in 1945. They interchangeably called it the \\"Korean Cold War\\".\\r\\nChina's Defense Ministry spokesman Geng Yansheng,[36] The Diplomat editor Shannon Tiezzi,[37] and The Guardian columnist Simon Tisdall[38] used the term to refer to tensions between China and Japan. China's state-run newspaper Global Times says that China and Japan \\"are stuck in a state of 'cold peace' and [m]ust avoid sliding into a cold war.\\"[39]\\r\\nImran Ali Sandano of the University of Sindh,[40] Arup K. Chatterjee of the Jindal Global Law School,[41] journalist Bertil Lintner,[42] writer Bruno Ma??es,[43] politician-lawyer P. Chidambaram,[44] and some others[45][46] use the terms like \\"new cold war\\" to refer to growing tensions between China and India.","input":"What kind of war is the cold war?"},{"output":"26 miles","context":"Berkeley Springs is a town in, and the county seat of, Morgan County, West Virginia, USA,[4] in the state's Eastern Panhandle. While the area was part of Virginia (prior to 1861), the town was incorporated as Bath. Since 1802, it has been referred to by the name of its original Virginia post office, Berkeley Springs. The population of the town was 624 at the 2010 United States Census. The town is located within the Hagerstown-Martinsburg, MD-WV Metropolitan Statistical Area. Berkeley Springs is a sister city to Bath, Somerset, England.\\r\\nThe area contains mineral water springs that were frequented by Native Americans indigenous to the area, possibly for thousands of years. After settlement by Europeans, the mineral springs drew many visitors from urban areas. Notable colonial visitors to the area included George Washington and James Rumsey. Berkeley Springs remained a popular resort area during the early years of the United States. It is the home of the Berkeley Springs International Water Tasting,[5] the longest running and largest such event in the world.\\r\\nThe area continues to be a popular resort area with tourism the main industry in the county and four full-service spas using the mineral water. A historic building whose construction began in 1888, was built as a retreat for Rosa and Samuel Taylor Suit overlooking the town. It often is called \\"Berkeley Castle\\".\\r\\nBerkeley Springs is a noted arts community with working artists accounting for approximately 1% of the county population of 16,000. Since 1994, the town has been listed in all four editions of John Villani's \\"100 Best Art Towns in America\\" (one of only 11 towns so rated).\\r\\n\\r\\n\\r\\nDuring colonial times in 1748, George Washington, then just 16 years old, was part of the team that surveyed the Eastern Panhandle region for Thomas Fairfax, 6th Lord Fairfax of Cameron. Washington returned several times over the next several years with his half-brother, Lawrence, who was ill, in hopes that the warm springs might improve his health. The springs, and their rumored medicinal benefits, attracted numerous Aboriginal Americans as well as Europeans to the area.\\r\\nWhile vacationing in the area in 1767, Washington made note of how busy the colonial town had become. Lord Fairfax had built a summer home there and a \\"private bath\\" making the area a popular destination for Virginia's social elite. With the advent of independence, An act for establishing a town at the Warm Springs in the county of Berkeley[6] was adopted by the Virginia General Assembly in December 1776. Officially, the town was named Bath, in honor of England's spa city of Bath. George Washington, his family members and several of the colonial elite were among the town's first landowners. The town's main north-south street was named Washington and the main east-west street was named Fairfax. Also, four acres were set aside for \\"suffering humanity.\\" The area around the springs always was public land known as The Grove and overseen by a state-appointed group of Berkeley Springs trustees. This would become a historic park with its springs and bathhouses, which was made part of the West Virginia state park system in 1925. Nearby, Cacapon State Park was opened in 1933. The mountain that gives its name to the park has an elevation of 2,320 feet (710?m) above sea level.\\r\\nBath's population increased during and immediately after, the Revolutionary War as wounded soldiers and others came to the area believing that the warm springs had medicinal qualities. Bath gained a reputation then as a somewhat wild town where eating, drinking, dancing, and gambling on the daily horse races were the order of the day.\\r\\nIn 1772, the springs were part of the newly formed Berkeley County, named after its colonial governor, Norborne Berkeley. The waters became known as Berkeley Springs because the existing protocol was to name springs after the colonial Virginia county in which they were located. Previously, the area had been called Warm Springs and Medicinal Springs among other names.\\r\\nBath became known permanently to the world as Berkeley Springs in 1802 when the Virginia postal system was established in the new nation and there already was a Bath, Virginia, in Bath County.\\r\\nWest Virginia became a state following the Wheeling Conventions of 1861, in which 50 northwestern counties of Virginia decided to break away from Virginia during the American Civil War. The new state was admitted to the Union on June 20, 1863. Berkeley Springs remained the conventional name used for the town. Its Sister City is Bath, England.\\r\\nBerkeley Springs is located at 393732N 781337W? / ?39.62556N 78.22694W? / 39.62556; -78.22694 (39.625562, -78.226862),[7] in the Appalachian Mountains. The town lies in the Eastern Panhandle of West Virginia 26 miles NW of Martinsburg, West Virginia and 36 miles W of Hagerstown, Maryland. Berkeley Springs is the county seat of Morgan County. Morgan County makes up one of the western counties in the Eastern Panhandle.\\r\\nAccording to the United States Census Bureau, the incorporated town of Bath has a total area of 0.34 square miles (0.88?km2), all land.[1]\\r\\nThe main road through the town is U.S. Route 522. West Virginia Route 9 runs east and west through the town.\\r\\nThere are two rivers in Morgan County. The Potomac makes up the northern border and the Cacapon River cuts through the center of the county connecting with the Potomac at Great Cacapon. The Cacapon and Sleepy Creek Mountains are the two most notable mountains in the county. Berkeley Springs is nestled in the extreme northern Shenandoah Valley at an elevation of approximately 499 feet. Warm Spring Run cuts through the center of the town and eventually, connects with the Potomac River near the Hancock Station. Sleepy Creek connects with the Potomac along River Road north and east of the town.\\r\\nBerkeley Springs experiences a wide array of various weather conditions. There are four distinct seasons in the area. The summers usually are warm, but occasionally can be hot. The fall is variable, and winter is very unpredictable. Temperatures in winter may vary from -4 degrees Fahrenheit to the lower 70s (rare). In recent years the area has had approximately 15-25?inches of snow. The spring season usually has a lot of precipitation. Recent years have been noticeably wetter than average.\\r\\nSevere weather does affect the area and may be dangerous at certain times. July 18, 2006, was an example when, on a summer day that was very moist and humid, a cold front sparked storms. Berkeley Springs Emergency Management reported 2.75\\" hail that caused $750,000 in damage. Flash flooding caused an additional $125,000 in damage. Wind damage accounted for $13,000.\\r\\nOther notable events include impacts from Hurricane Ivan, which resulted in two tornadoes, only causing minor damage. It also resulted in numerous road closures from flooding.\\r\\nThe 2009-2010 winter season was the worst for Morgan county since records have been kept. Nearly 100?inches of snow fell over the county. Best estimates for the county are approximately 90?inches. Three big snowfalls accounted for a large amount of the total snowfall for the season. 33?inches, 17?inches, and 13?inches were the top three biggest snowfalls for the year. It was the greatest accumulation since 2003, the prior year when Morgan County had a double digit snowfall accumulation. During the 09-10 season the county experienced three. The county also had its first blizzard since 1996 and that was the 13?inch storm in mid February.\\r\\nIn September 2012, a storm that dropped six inches of rain in three hours caused widespread flooding in the town.[8]\\r\\nThe first white thanksgiving in years occurred November 2014. Heavy snow fell on the 26th and lasted through Thanksgiving. Storm totals were 21 inches in Paw Paw, 6-7 inches near Sleepy Creek, and 11 inches 1 mile SE of Berkeley Springs. Thundersnow was also observed, which is rare in winter storms.\\r\\nThe warmest month is generally July (86 degrees) while the coldest is January (41 degrees).[9]\\r\\nAs of the census[2] of 2010, there were 624 people, 314 households, and 158 families residing in the incorporated town of Bath. The population density was 1,835.3 inhabitants per square mile (708.6/km2). There were 416 housing units at an average density of 1,223.5 per square mile (472.4/km2). The racial makeup of the town was 96.5% White, 0.5% Native American, 0.3% Asian, 1.0% from other races, and 1.8% from two or more races. Hispanic or Latino of any race were 2.7% of the population.\\r\\nThere were 314 households of which 23.9% had children under the age of 18 living with them, 31.8% were married couples living together, 14.6% had a female householder with no husband present, 3.8% had a male householder with no wife present, and 49.7% were non-families. 43.0% of all households were made up of individuals and 17.5% had someone living alone who was 65 years of age or older. The average household size was 1.99 and the average family size was 2.74.\\r\\nThe median age in the town was 42.9 years. 21.2% of residents were under the age of 18; 6.5% were between the ages of 18 and 24; 25.1% were from 25 to 44; 27% were from 45 to 64; and 19.9% were 65 years of age or older. The gender makeup of the town was 47.8% male and 52.2% female.\\r\\nAs of the census[12] of 2000, there were 663 people, 331 households, and 160 families residing in the town. The population density was 2,706.0 inhabitants per square mile (1,023.9/km2). There were 379 housing units at an average density of 1,546.9 per square mile (585.3/km2). The racial makeup of the town was 97.1% White, 1.7% African American, 0.2% Native American, 0.3% from other races, and 0.8% from two or more races. Hispanic or Latino of any race were 0.6% of the population.\\r\\nThere were 331 households out of which 20.5% had children under the age of 18 living with them, 35.6% were married couples living together, 10.0% had a female householder with no husband present, and 51.4% were non-families. 46.5% of all households were made up of individuals and 25.1% had someone living alone who was 65 years of age or older. The average household size was 1.98 and the average family size was 2.85.\\r\\nIn the town, the population was spread out with 19.5% under 18, 11.0% from 18 to 24, 23.4% from 25 to 44, 22.3% from 45 to 64, and 23.8% who were 65 years or older. The median age was 41 years. For every 100 females there were 80.2 males. For every 100 females age 18 and over, there were 77.4 males.\\r\\nThe median income for a household in the town was $24,934, and the median income for a family was $33,333. Males had a median income of $25,156 versus $23,611 for females. The per capita income for the town was $14,917. About 13.4% of families and 18.7% of the population were below the poverty line, including 26.8% of those under age 18 and 15.7% of those age 65 or over.","input":"How far is berkley springs wv from martinsburg wv?"},{"output":"as far south as James Bay in Canada","context":"","input":"Where do most polar bears live in canada?"},{"output":"22 April 2013","context":"Manchester United Football Club is a professional football club based in Old Trafford, Greater Manchester, England, that competes in the Premier League, the top flight of English football. Nicknamed \\"the Red Devils\\", the club was founded as Newton Heath LYR Football Club in 1878, changed its name to Manchester United in 1902 and moved to its current stadium, Old Trafford, in 1910.\\r\\nManchester United have won a record 20 League titles, 12 FA Cups, 5 League Cups and a record 21 FA Community Shields. The club has also won three UEFA Champions Leagues, one UEFA Europa League, one UEFA Cup Winners' Cup, one UEFA Super Cup, one Intercontinental Cup and one FIFA Club World Cup. In 1998ÿ99, the club became the first in the history of English football to achieve the treble of the Premier League, the FA Cup and the UEFA Champions League.[3] In 2016ÿ17, by winning the UEFA Europa League, they became one of five clubs to have won all three main UEFA club competitions. In addition, they became the only professional English club to have won every ongoing honour available to the first team that is organised by a national or international governing body.\\r\\nThe 1958 Munich air disaster claimed the lives of eight players. In 1968, under the management of Matt Busby, Manchester United became the first English football club to win the European Cup. Alex Ferguson won 38 trophies as manager, including 13 Premier League titles, 5 FA Cups and 2 UEFA Champions Leagues, between 1986 and 2013,[4][5][6] when he announced his retirement. Jos Mourinho is the club's current manager, having been appointed on 27 May 2016.\\r\\nManchester United was the highest-earning football club in the world for 2016ÿ17, with an annual revenue of ?576.3?million,[7] and the world's most valuable football club in 2017, valued at S2.86?billion.[8] As of June 2015, it is the world's most valuable football brand, estimated to be worth $1.2?billion.[9][10] After being floated on the London Stock Exchange in 1991, the club was purchased by Malcolm Glazer in May 2005 in a deal valuing the club at almost S800?million, after which the company was taken private again, before going public once more in August 2012, when they made an initial public offering on the New York Stock Exchange. Manchester United is one of the most widely supported football clubs in the world,[11][12] and has its strongest rivalries with Liverpool, Manchester City and Leeds United.\\r\\nManchester United was formed in 1878 as Newton Heath LYR Football Club by the Carriage and Wagon department of the Lancashire and Yorkshire Railway (LYR) depot at Newton Heath.[13] The team initially played games against other departments and railway companies, but on 20 November 1880, they competed in their first recorded match; wearing the colours of the railway company ÿ green and gold ÿ they were defeated 6ÿ0 by Bolton Wanderers' reserve team.[14] By 1888, the club had become a founding member of The Combination, a regional football league. Following the league's dissolution after only one season, Newton Heath joined the newly formed Football Alliance, which ran for three seasons before being merged with the Football League. This resulted in the club starting the 1892ÿ93 season in the First Division, by which time it had become independent of the railway company and dropped the \\"LYR\\" from its name.[13] After two seasons, the club was relegated to the Second Division.[13]\\r\\nIn January 1902, with debts of S2,670 ÿ equivalent to S270,000 in 2018[nb 1] ÿ the club was served with a winding-up order.[15] Captain Harry Stafford found four local businessmen, including John Henry Davies (who became club president), each willing to invest S500 in return for a direct interest in running the club and who subsequently changed the name;[16] on 24 April 1902, Manchester United was officially born.[17][nb 2] Under Ernest Mangnall, who assumed managerial duties in 1903, the team finished as Second Division runners-up in 1906 and secured promotion to the First Division, which they won in 1908 ÿ the club's first league title. The following season began with victory in the first ever Charity Shield[18] and ended with the club's first FA Cup title. Manchester United won the First Division for the second time in 1911, but at the end of the following season, Mangnall left the club to join Manchester City.[19]\\r\\nIn 1922, three years after the resumption of football following the First World War, the club was relegated to the Second Division, where it remained until regaining promotion in 1925. Relegated again in 1931, Manchester United became a yo-yo club, achieving its all-time lowest position of 20th place in the Second Division in 1934. Following the death of principal benefactor John Henry Davies in October 1927, the club's finances deteriorated to the extent that Manchester United would likely have gone bankrupt had it not been for James W. Gibson, who, in December 1931, invested S2,000 and assumed control of the club.[20] In the 1938ÿ39 season, the last year of football before the Second World War, the club finished 14th in the First Division.[20]\\r\\nIn October 1945, the impending resumption of football led to the managerial appointment of Matt Busby, who demanded an unprecedented level of control over team selection, player transfers and training sessions.[21] Busby led the team to second-place league finishes in 1947, 1948 and 1949, and to FA Cup victory in 1948. In 1952, the club won the First Division, its first league title for 41 years.[22] With an average age of 22, the back-to-back title winning side of 1956 were labelled \\"the Busby Babes\\" by the media, a testament to Busby's faith in his youth players.[23] In 1957, Manchester United became the first English team to compete in the European Cup, despite objections from The Football League, who had denied Chelsea the same opportunity the previous season.[24] En route to the semi-final, which they lost to Real Madrid, the team recorded a 10ÿ0 victory over Belgian champions Anderlecht, which remains the club's biggest victory on record.[25]\\r\\nThe following season, on the way home from a European Cup quarter-final victory against Red Star Belgrade, the aircraft carrying the Manchester United players, officials and journalists crashed while attempting to take off after refuelling in Munich, Germany. The Munich air disaster of 6 February 1958 claimed 23 lives, including those of eight players?ÿ Geoff Bent, Roger Byrne, Eddie Colman, Duncan Edwards, Mark Jones, David Pegg, Tommy Taylor and Billy Whelan?ÿ and injured several more.[26][27]\\r\\nReserve team manager Jimmy Murphy took over as manager while Busby recovered from his injuries and the club's makeshift side reached the FA Cup final, which they lost to Bolton Wanderers. In recognition of the team's tragedy, UEFA invited the club to compete in the 1958ÿ59 European Cup alongside eventual League champions Wolverhampton Wanderers. Despite approval from the FA, the Football League determined that the club should not enter the competition, since it had not qualified.[28][29] Busby rebuilt the team through the 1960s by signing players such as Denis Law and Pat Crerand, who combined with the next generation of youth players ÿ including George Best ÿ to win the FA Cup in 1963. The following season, they finished second in the league, then won the title in 1965 and 1967. In 1968, Manchester United became the first English (and second British) club to win the European Cup, beating Benfica 4ÿ1 in the final[30] with a team that contained three European Footballers of the Year: Bobby Charlton, Denis Law and George Best.[31] Matt Busby resigned as manager in 1969 and was replaced by the reserve team coach, former Manchester United player Wilf McGuinness.[32]\\r\\nFollowing an eighth-place finish in the 1969ÿ70 season and a poor start to the 1970ÿ71 season, Busby was persuaded to temporarily resume managerial duties, and McGuinness returned to his position as reserve team coach. In June 1971, Frank O'Farrell was appointed as manager, but lasted less than 18?months before being replaced by Tommy Docherty in December 1972.[34] Docherty saved Manchester United from relegation that season, only to see them relegated in 1974; by that time the trio of Best, Law, and Charlton had left the club.[30] The team won promotion at the first attempt and reached the FA Cup final in 1976, but were beaten by Southampton. They reached the final again in 1977, beating Liverpool 2ÿ1. Docherty was dismissed shortly afterwards, following the revelation of his affair with the club physiotherapist's wife.[32][35]\\r\\nDave Sexton replaced Docherty as manager in the summer of 1977. Despite major signings, including Joe Jordan, Gordon McQueen, Gary Bailey, and Ray Wilkins, the team failed to achieve any significant results; they finished in the top two in 1979ÿ80 and lost to Arsenal in the 1979 FA Cup Final. Sexton was dismissed in 1981, even though the team won the last seven games under his direction.[36] He was replaced by Ron Atkinson, who immediately broke the British record transfer fee to sign Bryan Robson from West Bromwich Albion. Under Atkinson, Manchester United won the FA Cup twice in three years ÿ in 1983 and 1985. In 1985ÿ86, after 13 wins and two draws in its first 15 matches, the club was favourite to win the league, but finished in fourth place. The following season, with the club in danger of relegation by November, Atkinson was dismissed.[37]\\r\\nAlex Ferguson and his assistant Archie Knox arrived from Aberdeen on the day of Atkinson's dismissal,[38] and guided the club to an 11th-place finish in the league.[39] Despite a second-place finish in 1987ÿ88, the club was back in 11th place the following season.[40] Reportedly on the verge of being dismissed, victory over Crystal Palace in the 1990 FA Cup Final replay (after a 3ÿ3 draw) saved Ferguson's career.[41][42] The following season, Manchester United claimed its first Cup Winners' Cup title and competed in the 1991 UEFA Super Cup, beating European Cup holders Red Star Belgrade 1ÿ0 in the final at Old Trafford. A second consecutive League Cup final appearance followed in 1992, in which the team beat Nottingham Forest 1ÿ0 at Wembley.[37] In 1993, the club won its first league title since 1967, and a year later, for the first time since 1957, it won a second consecutive title ÿ alongside the FA Cup ÿ to complete the first \\"Double\\" in the club's history.[37]\\r\\nIn the 1998ÿ99 season, Manchester United became the first team to win the Premier League, FA Cup and UEFA Champions League ÿ \\"The Treble\\" ÿ in the same season.[44] Losing 1ÿ0 going into injury time in the 1999 UEFA Champions League Final, Teddy Sheringham and Ole Gunnar Solskj?r scored late goals to claim a dramatic victory over Bayern Munich, in what is considered one of the greatest comebacks of all time.[45] The club also won the Intercontinental Cup after beating Palmeiras 1ÿ0 in Tokyo.[46] Ferguson was subsequently knighted for his services to football.[47]\\r\\nManchester United won the league again in the 1999ÿ2000 and 2000ÿ01 seasons. The team finished third in 2001ÿ02, before regaining the title in 2002ÿ03.[48] They won the 2003ÿ04 FA Cup, beating Millwall 3ÿ0 in the final at the Millennium Stadium in Cardiff to lift the trophy for a record 11th time.[49] In the 2005ÿ06 season, Manchester United failed to qualify for the knockout phase of the UEFA Champions League for the first time in over a decade,[50] but recovered to secure a second-place league finish and victory over Wigan Athletic in the 2006 Football League Cup Final. The club regained the Premier League in the 2006ÿ07 and 2007ÿ08 seasons, and completed the European double by beating Chelsea 6ÿ5 on penalties in the 2008 UEFA Champions League Final in Moscow's Luzhniki Stadium. Ryan Giggs made a record 759th appearance for the club in this game, overtaking previous record holder Bobby Charlton.[51] In December 2008, the club won the 2008 FIFA Club World Cup and followed this with the 2008ÿ09 Football League Cup, and its third successive Premier League title.[52][53] That summer, Cristiano Ronaldo was sold to Real Madrid for a world record S80?million.[54] In 2010, Manchester United defeated Aston Villa 2ÿ1 at Wembley to retain the League Cup, its first successful defence of a knockout cup competition.[55]\\r\\nAfter finishing as runner-up to Chelsea in the 2009ÿ10 season, United achieved a record 19th league title in 2010ÿ11, securing the championship with a 1ÿ1 away draw against Blackburn Rovers on 14 May 2011.[56] This was extended to 20 league titles in 2012ÿ13, securing the championship with a 3ÿ0 home win against Aston Villa on 22 April 2013.[57]\\r\\nOn 8 May 2013, Ferguson announced that he was to retire as manager at the end of the football season, but would remain at the club as a director and club ambassador.[58][59] The club announced the next day that Everton manager David Moyes would replace him from 1 July, having signed a six-year contract.[60][61][62] Ryan Giggs took over as interim player-manager 10 months later, on 22 April 2014, when Moyes was sacked after a poor season in which the club failed to defend their Premier League title and failed to qualify for the UEFA Champions League for the first time since 1995ÿ96.[63] They also failed to qualify for the Europa League, meaning that it was the first time Manchester United hadn't qualified for a European competition since 1990.[64] On 19 May 2014, it was confirmed that Louis van Gaal would replace Moyes as Manchester United manager on a three-year deal, with Giggs as his assistant.[65] Malcolm Glazer, the patriarch of the Glazer family that owns the club, died on 28 May 2014.[66]\\r\\nAlthough Van Gaal's first season saw United once again qualify for the Champions League through a fourth-place finish in the Premier League, his second season saw United go out of the same tournament in the group stage.[67] United also fell behind in the title race for the third consecutive season, finishing in 5th place, in spite of several expensive signings during Van Gaal's tenure. However, that same season, Manchester United won the FA Cup for a 12th time.[68] Despite this victory, Van Gaal was sacked as manager just two days later,[69] with Jos Mourinho appointed in his place on 27 May, signing a three-year contract.[70] That season, United finished in sixth place while winning the EFL Cup for the fifth time and the Europa League for the first time, as well as the FA Community Shield for a record 21st time. Wayne Rooney scored his 250th goal with United, surpassing Sir Bobby Charlton as United's all-time top scorer, before leaving the club at the end of the season to return to Everton.\\r\\nThe club crest is derived from the Manchester City Council coat of arms, although all that remains of it on the current crest is the ship in full sail.[71] The devil stems from the club's nickname \\"The Red Devils\\"; it was included on club programmes and scarves in the 1960s, and incorporated into the club crest in 1970, although the crest was not included on the chest of the shirt until 1971.[71]\\r\\nNewton Heath's uniform in 1879, four years before the club played its first competitive match, has been documented as 'white with blue cord'.[72] A photograph of the Newton Heath team, taken in 1892, is believed to show the players wearing red-and-white quartered jerseys and navy blue knickerbockers.[73] Between 1894ÿ96, the players wore distinctive green and gold jerseys[73] which were replaced in 1896 by white shirts, which were worn with navy blue shorts.[73]\\r\\nAfter the name change in 1902, the club colours were changed to red shirts, white shorts, and black socks, which has become the standard Manchester United home kit.[73] Very few changes were made to the kit until 1922 when the club adopted white shirts bearing a deep red \\"V\\" around the neck, similar to the shirt worn in the 1909 FA Cup Final. They remained part of their home kits until 1927.[73] For a period in 1934, the cherry and white hooped change shirt became the home colours, but the following season the red shirt was recalled after the club's lowest ever league placing of 20th in the Second Division and the hooped shirt dropped back to being the change.[73] The black socks were changed to white from 1959 to 1965, where they were replaced with red socks up until 1971, when the club reverted to black. Black shorts and/or white socks are sometimes worn with the home strip, most often in away games, if there is a clash with the opponent's kit. Since 1997ÿ98, white socks have been the preferred choice for European games, which are typically played on weeknights, to aid with player visibility.[74] The current home kit is a red Henley shirt, with black-and-white-banded cuffs and the trademark Adidas three stripes in white across the shoulders.[75]\\r\\nThe Manchester United away strip has often been a white shirt, black shorts and white socks, but there have been several exceptions. These include an all-black strip with blue and gold trimmings between 1993 and 1995, the navy blue shirt with silver horizontal pinstripes worn during the 1999ÿ2000 season,[76] and the 2011ÿ12 away kit, which had a royal blue body and sleeves with hoops made of small midnight navy blue and black stripes, with black shorts and blue socks.[77] An all-grey away kit worn during the 1995ÿ96 season was dropped after just five games, most notoriously against Southampton where Alex Ferguson forced the team to change into the third kit during half-time of its final outing. The reason for dropping it being that the players claimed to have trouble finding their teammates against the crowd, United failed to win a competitive game in the kit.[78] In 2001, to celebrate 100 years as \\"Manchester United\\", a reversible white/gold away kit was released, although the actual match day shirts were not reversible.[79]\\r\\nThe club's third kit is often all-blue, this was most recently the case during the 2014ÿ15 season.[80] Exceptions include a green-and-gold halved shirt worn between 1992 and 1994, a blue-and-white striped shirt worn during the 1994ÿ95 and 1995ÿ96 seasons and once in 1996ÿ97, an all-black kit worn during the Treble-winning 1998ÿ99 season, and a white shirt with black-and-red horizontal pinstripes worn between 2003ÿ04 and 2005ÿ06.[81] From 2006ÿ07 to 2013ÿ14, the third kit was the previous season's away kit, albeit updated with the new club sponsor in 2006ÿ07 and 2010ÿ11, apart from 2008ÿ09 when an all-blue kit was launch to mark the 40th anniversary of the 1967ÿ68 European Cup success.[82]\\r\\nNewton Heath initially played on a field on North Road, close to the railway yard; the original capacity was about 12,000, but club officials deemed the facilities inadequate for a club hoping to join The Football League.[83] Some expansion took place in 1887, and in 1891, Newton Heath used its minimal financial reserves to purchase two grandstands, each able to hold 1,000 spectators.[84] Although attendances were not recorded for many of the earliest matches at North Road, the highest documented attendance was approximately 15,000 for a First Division match against Sunderland on 4 March 1893.[85] A similar attendance was also recorded for a friendly match against Gorton Villa on 5 September 1889.[86]\\r\\nIn June 1893, after the club was evicted from North Road by its owners, Manchester Deans and Canons, who felt it was inappropriate for the club to charge an entry fee to the ground, secretary A. H. Albut procured the use of the Bank Street ground in Clayton.[87] It initially had no stands, by the start of the 1893ÿ94 season, two had been built; one spanning the full length of the pitch on one side and the other behind the goal at the \\"Bradford end\\". At the opposite end, the \\"Clayton end\\", the ground had been \\"built up, thousands thus being provided for\\".[87] Newton Heath's first league match at Bank Street was played against Burnley on 1 September 1893, when 10,000 people saw Alf Farman score a hat-trick, Newton Heath's only goals in a 3ÿ2 win. The remaining stands were completed for the following league game against Nottingham Forest three weeks later.[87] In October 1895, before the visit of Manchester City, the club purchased a 2,000-capacity stand from the Broughton Rangers rugby league club, and put up another stand on the \\"reserved side\\" (as distinct from the \\"popular side\\"). However, weather restricted the attendance for the Manchester City match to just 12,000.[88]\\r\\nWhen the Bank Street ground was temporarily closed by bailiffs in 1902, club captain Harry Stafford raised enough money to pay for the club's next away game at Bristol City and found a temporary ground at Harpurhey for the next reserves game against Padiham.[89] Following financial investment, new club president John Henry Davies paid S500 for the erection of a new 1,000-seat stand at Bank Street.[90] Within four years, the stadium had cover on all four sides, as well as the ability to hold approximately 50,000 spectators, some of whom could watch from the viewing gallery atop the Main Stand.[90]\\r\\nHowever, following Manchester United's first league title in 1908 and the FA Cup a year later, it was decided that Bank Street was too restrictive for Davies' ambition;[90] in February 1909, six weeks before the club's first FA Cup title, Old Trafford was named as the home of Manchester United, following the purchase of land for around S60,000. Architect Archibald Leitch was given a budget of S30,000 for construction; original plans called for seating capacity of 100,000, though budget constraints forced a revision to 77,000. The building was constructed by Messrs Brameld and Smith of Manchester. The stadium's record attendance was registered on 25 March 1939, when an FA Cup semi-final between Wolverhampton Wanderers and Grimsby Town drew 76,962?spectators.[91]\\r\\nBombing in the Second World War destroyed much of the stadium; the central tunnel in the South Stand was all that remained of that quarter. After the war, the club received compensation from the War Damage Commission in the amount of S22,278. While reconstruction took place, the team played its \\"home\\" games at Manchester City's Maine Road ground; Manchester United was charged S5,000 per year, plus a nominal percentage of gate receipts.[92] Later improvements included the addition of roofs, first to the Stretford End and then to the North and East Stands. The roofs were supported by pillars that obstructed many fans' views, and they were eventually replaced with a cantilevered structure. The Stretford End was the last stand to receive a cantilevered roof, completed in time for the 1993ÿ94 season.[32] First used on 25 March 1957 and costing S40,000, four 180-foot (55?m) pylons were erected, each housing 54?individual floodlights. These were dismantled in 1987 and replaced by a lighting system embedded in the roof of each stand, which remains in use today.[93]\\r\\nThe Taylor Report's requirement for an all-seater stadium lowered capacity at Old Trafford to around 44,000 by 1993. In 1995, the North Stand was redeveloped into three tiers, restoring capacity to approximately 55,000. At the end of the 1998ÿ99 season, second tiers were added to the East and West Stands, raising capacity to around 67,000, and between July 2005 and May 2006, 8,000 more seats were added via second tiers in the north-west and north-east quadrants. Part of the new seating was used for the first time on 26 March 2006, when an attendance of 69,070 became a new Premier League record.[94] The record was pushed steadily upwards before reaching its peak on 31 March 2007, when 76,098 spectators saw Manchester United beat Blackburn Rovers 4ÿ1, with just 114 seats (0.15?per?cent of the total capacity of 76,212) unoccupied.[95] In 2009, reorganisation of the seating resulted in a reduction of capacity by 255 to 75,957.[96][97] Manchester United has the second highest average attendance of European football clubs only behind Borussia Dortmund.[98][99][100]\\r\\nManchester United is one of the most popular football clubs in the world, with one of the highest average home attendance in Europe.[101] The club states that its worldwide fan base includes more than 200 officially recognised branches of the Manchester United Supporters Club (MUSC), in at least 24 countries.[102] The club takes advantage of this support through its worldwide summer tours. Accountancy firm and sports industry consultants Deloitte estimate that Manchester United has 75?million fans worldwide,[11] while other estimates put this figure closer to 333?million.[103] The club has the third highest social media following in the world among sports teams (after Barcelona and Real Madrid), with over 71 million Facebook fans as of September 2016.[12][104] A 2014 study showed that Manchester United had the loudest fans in the Premier League.[105]\\r\\nSupporters are represented by two independent bodies; the Independent Manchester United Supporters' Association (IMUSA), which maintains close links to the club through the MUFC Fans Forum,[106] and the Manchester United Supporters' Trust (MUST). After the Glazer family's takeover in 2005, a group of fans formed a splinter club, F.C. United of Manchester. The West Stand of Old Trafford ÿ the \\"Stretford End\\" ÿ is the home end and the traditional source of the club's most vocal support.[107]\\r\\nManchester United has rivalries with Arsenal, Leeds United, Liverpool, and Manchester City, against whom they contest the Manchester derby.[108][109]\\r\\nThe rivalry with Liverpool is rooted in competition between the cities during the Industrial Revolution when Manchester was famous for its textile industry while Liverpool was a major port.[110] Manchester United and Liverpool are also the two most successful teams in England and, at many points in their history, they have battled each other for the league title (most recently in the 2008ÿ09 season). Their matches are usually considered by the players and their fans as the biggest in any given season.\\r\\nThe \\"Roses Rivalry\\" with Leeds stems from the Wars of the Roses, fought between the House of Lancaster and the House of York, with Manchester United representing Lancashire and Leeds representing Yorkshire.[111]\\r\\nThe rivalry with Arsenal arises from the numerous times the two teams, as well as managers Alex Ferguson and Arsne Wenger, have battled for the Premier League title. With 33 titles between them (20 for Manchester United, 13 for Arsenal) this fixture has become known as one of the finest Premier League match-ups in history.[112][113]\\r\\nManchester United has been described as a global brand; a 2011 report by Brand Finance, valued the club's trademarks and associated intellectual property at S412?million ÿ an increase of S39?million on the previous year, valuing it at S11?million more than the second best brand, Real Madrid ÿ and gave the brand a strength rating of AAA (Extremely Strong).[114] In July 2012, Manchester United was ranked first by Forbes magazine in its list of the ten most valuable sports team brands, valuing the Manchester United brand at $2.23?billion.[115] The club is ranked third in the Deloitte Football Money League (behind Real Madrid and Barcelona).[116] In January 2013, the club became the first sports team in the world to be valued at $3?billion. Forbes Magazine valued the club at $3.3?billion ÿ $1.2?billion higher than the next most valuable sports team.[117] They were overtaken by Real Madrid for the next four years, but Manchester United returned to the top of the Forbes list in June 2017, with a valuation of $3.689?billion.[118]\\r\\nThe core strength of Manchester United's global brand is often attributed to Matt Busby's rebuilding of the team and subsequent success following the Munich air disaster, which drew worldwide acclaim.[107] The \\"iconic\\" team included Bobby Charlton and Nobby Stiles (members of England's World Cup winning team), Denis Law and George Best. The attacking style of play adopted by this team (in contrast to the defensive-minded \\"catenaccio\\" approach favoured by the leading Italian teams of the era) \\"captured the imagination of the English footballing public\\".[119] Busby's team also became associated with the liberalisation of Western society during the 1960s; George Best, known as the \\"Fifth Beatle\\" for his iconic haircut, was the first footballer to significantly develop an off-the-field media profile.[119]\\r\\nAs the second English football club to float on the London Stock Exchange in 1991, the club raised significant capital, with which it further developed its commercial strategy. The club's focus on commercial and sporting success brought significant profits in an industry often characterised by chronic losses.[120] The strength of the Manchester United brand was bolstered by intense off-the-field media attention to individual players, most notably David Beckham (who quickly developed his own global brand). This attention often generates greater interest in on-the-field activities, and hence generates sponsorship opportunities ÿ the value of which is driven by television exposure.[121] During his time with the club, Beckham's popularity across Asia was integral to the club's commercial success in that part of the world.[122]\\r\\nBecause higher league placement results in a greater share of television rights, success on the field generates greater income for the club. Since the inception of the Premier League, Manchester United has received the largest share of the revenue generated from the BSkyB broadcasting deal.[123] Manchester United has also consistently enjoyed the highest commercial income of any English club; in 2005ÿ06, the club's commercial arm generated S51?million, compared to S42.5?million at Chelsea, S39.3?million at Liverpool, S34?million at Arsenal and S27.9?million at Newcastle United. A key sponsorship relationship was with sportswear company Nike, who managed the club's merchandising operation as part of a S303?million 13-year partnership between 2002 and 2015.[124] Through Manchester United Finance and the club's membership scheme, One United, those with an affinity for the club can purchase a range of branded goods and services. Additionally, Manchester United-branded media services ÿ such as the club's dedicated television channel, MUTV ÿ have allowed the club to expand its fan base to those beyond the reach of its Old Trafford stadium.[11]\\r\\nIn an initial five-year deal worth S500,000, Sharp Electronics became the club's first shirt sponsor at the beginning of the 1982ÿ83 season, a relationship that lasted until the end of the 1999ÿ2000 season, when Vodafone agreed a four-year, S30?million deal.[125] Vodafone agreed to pay S36?million to extend the deal by four years, but after two seasons triggered a break clause in order to concentrate on its sponsorship of the Champions League.[125]\\r\\nTo commence at the start of the 2006ÿ07 season, American insurance corporation AIG agreed a four-year S56.5?million deal which in September 2006 became the most valuable in the world.[126][127] At the beginning of the 2010ÿ11 season, American reinsurance company Aon became the club's principal sponsor in a four-year deal reputed to be worth approximately S80?million, making it the most lucrative shirt sponsorship deal in football history.[128] Manchester United announced their first training kit sponsor in August 2011, agreeing a four-year deal with DHL reported to be worth S40?million; it is believed to be the first instance of training kit sponsorship in English football.[129][130] The DHL contract lasted for over a year before the club bought back the contract in October 2012, although they remained the club's official logistics partner.[131] The contract for the training kit sponsorship was then sold to Aon in April 2013 for a deal worth S180?million over eight years, which also included purchasing the naming rights for the Trafford Training Centre.[132]\\r\\nThe club's first kit manufacturer was Umbro, until a five-year deal was agreed with Admiral Sportswear in 1975.[133] Adidas received the contract in 1980,[134] before Umbro started a second spell in 1992.[135] Umbro's sponsorship lasted for ten years, followed by Nike's record-breaking S302.9?million deal that lasted until 2015; 3.8?million replica shirts were sold in the first 22 months with the company.[136][137] In addition to Nike and Chevrolet, the club also has several lower-level \\"platinum\\" sponsors, including Aon and Budweiser.[138]\\r\\nOn 30 July 2012, United signed a seven-year deal with American automotive corporation General Motors, which replaced Aon as the shirt sponsor from the 2014ÿ15 season. The new $80m-a-year shirt deal is worth $559m over seven years and features the logo of General Motors brand Chevrolet.[139][140] Nike announced that they would not renew their kit supply deal with Manchester United after the 2014ÿ15 season, citing rising costs.[141][142] Since the start of the 2015ÿ16 season, Adidas has manufactured Manchester United's kit as part of a world-record 10-year deal worth a minimum of S750?million.[143][144]\\r\\nOriginally funded by the Lancashire and Yorkshire Railway Company, the club became a limited company in 1892 and sold shares to local supporters for S1 via an application form.[16] In 1902, majority ownership passed to the four local businessmen who invested S500 to save the club from bankruptcy, including future club president John Henry Davies.[16] After his death in 1927, the club faced bankruptcy yet again, but was saved in December 1931 by James W. Gibson, who assumed control of the club after an investment of S2,000.[20] Gibson promoted his son, Alan, to the board in 1948,[145] but died three years later; the Gibson family retained ownership of the club through James' wife, Lillian,[146] but the position of chairman passed to former player Harold Hardman.[147]\\r\\nPromoted to the board a few days after the Munich air disaster, Louis Edwards, a friend of Matt Busby, began acquiring shares in the club; for an investment of approximately S40,000, he accumulated a 54?per?cent shareholding and took control in January 1964.[148] When Lillian Gibson died in January 1971, her shares passed to Alan Gibson who sold a percentage of his shares to Louis Edwards' son, Martin, in 1978; Martin Edwards went on to become chairman upon his father's death in 1980.[149] Media tycoon Robert Maxwell attempted to buy the club in 1984, but did not meet Edwards' asking price.[149] In 1989, chairman Martin Edwards attempted to sell the club to Michael Knighton for S20?million, but the sale fell through and Knighton joined the board of directors instead.[149]\\r\\nManchester United was floated on the stock market in June 1991 (raising S6.7?million),[150] and received yet another takeover bid in 1998, this time from Rupert Murdoch's British Sky Broadcasting Corporation. This resulted in the formation of Shareholders United Against Murdoch ÿ now the Manchester United Supporters' Trust ÿ who encouraged supporters to buy shares in the club in an attempt to block any hostile takeover. The Manchester United board accepted a S623?million offer,[151] but the takeover was blocked by the Monopolies and Mergers Commission at the final hurdle in April 1999.[152] A few years later, a power struggle emerged between the club's manager, Alex Ferguson, and his horse-racing partners, John Magnier and J. P. McManus, who had gradually become the majority shareholders. In a dispute that stemmed from contested ownership of the horse Rock of Gibraltar, Magnier and McManus attempted to have Ferguson removed from his position as manager, and the board responded by approaching investors to attempt to reduce the Irishmen's majority.[153]\\r\\nIn May 2005, Malcolm Glazer purchased the 28.7?per?cent stake held by McManus and Magnier, thus acquiring a controlling interest through his investment vehicle Red Football Ltd in a highly leveraged takeover valuing the club at approximately S800?million (then approx. $1.5?billion).[154] Once the purchase was complete, the club was taken off the stock exchange.[155] In July 2006, the club announced a S660?million debt refinancing package, resulting in a 30?per?cent reduction in annual interest payments to S62?million a year.[156][157] In January 2010, with debts of S716.5?million ($1.17?billion),[158] Manchester United further refinanced through a bond issue worth S504?million, enabling them to pay off most of the S509?million owed to international banks.[159] The annual interest payable on the bonds ÿ which mature on 1 February 2017 ÿ is approximately S45?million per annum.[160] Despite restructuring, the club's debt prompted protests from fans on 23 January 2010, at Old Trafford and the club's Trafford Training Centre.[161][162] Supporter groups encouraged match-going fans to wear green and gold, the colours of Newton Heath. On 30 January, reports emerged that the Manchester United Supporters' Trust had held meetings with a group of wealthy fans, dubbed the \\"Red Knights\\", with plans to buying out the Glazers' controlling interest.[163]\\r\\nIn August 2011, the Glazers were believed to have approached Credit Suisse in preparation for a $1?billion (approx. S600?million) initial public offering (IPO) on the Singapore stock exchange that would value the club at more than S2?billion.[164] However, in July 2012, the club announced plans to list its IPO on the New York Stock Exchange instead.[165] Shares were originally set to go on sale for between $16 and $20 each, but the price was cut to $14 by the launch of the IPO on 10 August, following negative comments from Wall Street analysts and Facebook's disappointing stock market debut in May. Even after the cut, Manchester United was valued at $2.3?billion, making it the most valuable football club in the world.[166]\\r\\nNote: Flags indicate national team as defined under FIFA eligibility rules. Players may hold more than one non-FIFA nationality.\\r\\nNote: Flags indicate national team as defined under FIFA eligibility rules. Players may hold more than one non-FIFA nationality.\\r\\nManchester United are one of the most successful clubs in Europe.[208] The club's first trophy was the Manchester Cup, which it won as Newton Heath LYR in 1886.[209] In 1908, the club won its first league title, and won the FA Cup for the first time the following year. Manchester United won the most trophies in the 1990s; five league titles, four FA Cups, one League Cup, five Charity Shields (one shared), one UEFA Champions League, one UEFA Cup Winners' Cup, one UEFA Super Cup and one Intercontinental Cup.\\r\\nThe club holds the record for most top-division titles (20) ÿ including a record 13 Premier League titles ÿ and FA Community Shields (21). It was also the first English club to win the European Cup in 1968, and, as of 2017[update], is the only British club to have won the Club World Cup, in 2008. United also became the sole British club to win the Intercontinental Cup, in 1999. The club's most recent trophy came in May 2017, with the 2016ÿ17 UEFA Europa League.\\r\\nIn winning the 2016ÿ17 UEFA Europa League, United became the fifth club in history to have won the \\"European Treble\\" of European Cup/UEFA Champions League, European Cup Winners' Cup/UEFA Cup Winners' Cup, and UEFA Cup/UEFA Europa League after Juventus, Ajax, Bayern Munich and Chelsea.[210][211]\\r\\nEspecially short competitions such as the Charity/Community Shield, Intercontinental Cup (now defunct), FIFA Club World Cup or UEFA Super Cup are not generally considered to contribute towards a Double or Treble.[212]","input":"When did man utd last win premier league?"},{"output":"?????????","context":"\\r\\n\\r\\n\\r\\nThiruvathirai or Thiruvathira or Arudhra Darisanam (Malayalam: ?????????, Tamil: ??????????) is a Hindu festival celebrated in the South Indian states of Kerala and Tamil Nadu.[1][2][3] Thiruvathirai(Arudhra) in Tamil means \\"sacred big wave\\", using which this universe was created by Lord Shiva about 132 trillion years ago. Chidambaram[4] in Tamil Nadu, the Sri Natarajar temple's annual Festival,[5] is celebrated on this date. In the month of Makaram Thiruvathira Star is celebrated in Mathira Peedika Devi Temple, owned by Thiruvithamcore Devaswom Board, near Kadakkal in Kollam District of Kerala state.Thiruvathira has a connection with lord moon.\\r\\n\\r\\n\\r\\nThiruvadirai - Arudra Darsisanam is celebrated in a grand manner in 5 Sabhas of Sri Natarajar namely 1) Kanakasabha (Gold) - at Chidambaram (Thillai or Tillai), 2) Velli Sabhai (Silver) at Madurai, 3) Ratnasabha (Ruby) at Tiruvalankadu, 4) Tamrasabha (Copper) at Tirunelveli, 5) Chitrasabha (Pictures) at Kutralam.\\r\\nIn Thillai Chidambaram 10 day Festival is held during Thiruvathirai. On the 9th day night (i.e., 10th day very early morning) Maha Abhishekam will be done to Lord Nataraja and Goddess Sivakamasundari at Raja Sabhai at around 3 am. The MahaAbhishekam will be held for about 3-4 hours.\\r\\nThen special Thiruvabaranam (Sacred Jewels) Alankaram, Rahasiya Pujai will be done to Sri Natarajar. Pancha Murthi Thiruveethi Ula, will be held at around 12 pm Noon. Soon after Pancha Murthi Ula in the afternoon Lord Natarajar and Goddess Sivakami will bless devotees with Aarudra Darisanam and enter Kanaka Sabhai (Golden Sabha).[6]\\r\\nIt takes place on the full moon night in the Tamil month of Margazhi[7][8] (DecemberÿJanuary) and this is also the longest night in a year.[9] Literary and historical evidence in the form of stone inscriptions state that the festival has been celebrated on this day for more than 1500 years. Lord Shiva is praised in Tamil by many names, one of them is Athiraiyan (???????? ), from Thiruvathirai (Thiru + Athirai).[10][11]\\r\\nTamil hymns of Maanikavasagar's Thiruvasagam (particularly the hymns Thiruvempavai and Thiruppalliezhuchi) are chanted in temples. On the very day of Thiruvathirai the idols of Nataraja (Lord Shiva) and his consort Shivagami (Parvati) are taken out of the temple premises for a grand procession. It is one of the major events in almost all the Shiva temples in Tamil Nadu.\\r\\nSambandar sung in Tevaram during 7th-9th century, how Thiruvathirai celebrated at Kabaleeshwaram (present day Mylapore, Chennai).[12]\\r\\n\\r\\n\\"??????? ???? ???????? ?????????\\r\\n??????? ??????????? ??????? ???? ?????????\\r\\n??????? ?????? ??????????? ??????????\\r\\n????????? ?????? ???????? ??????????\\"\\r\\nAppar wrote a separate pathigam (10 songs) in Tevaram,[13] in the name Thiruvathirai Pathigam which describes the importance and celebrations of Thiruvathirai.\\r\\nIn 4th Tirumurai he sang about the celebration in Tiruvarur[14][15]\\r\\n\\r\\n\\"?????? ????? ????????? ???? ????????\\r\\n??????? ???? ??????? ????? ???????????\\r\\n???????? ??? ??????? ???? ????????\\r\\n???????? ?????? ???? ?????????\\"\\r\\nThe cosmic dance of Lord Shiva represents five activities ÿ Creation, Protection, Destruction, Embodiment and Release. In essence, it represents the continuous cycle of creation and destruction. This cosmic dance[16] takes place in every particle and is the source of all energy. Arudra Darshan[17] celebrates this ecstatic dance of Lord Shiva.[18]\\r\\nIt is essentially a Shaivite festival and celebrates the cosmic dance of Lord Shiva, which is represented by the Nataraja form.[18][19][20] Arudhra (Thiruvathirai in Tamil) signifies the golden red flame and Shiva performs the dance in the form this red-flamed light. Lord Shiva is supposed to be incarnated in the form of Lord Nataraja during the Arudra Darshan day.[18]\\r\\nMost of the temples[21][22][23] around the world with Lord Nataraja and Shiva[24] as deity perform the Arudhra Darshan. Neivedhyam (food for God) made for Lord Nataraja on that day is Thiruvathirai Kali.[25]\\r\\nThe festival is celebrated by Sri Lankan Tamils at Thinnapuram Sundareswarar Temple, it is called Eezhathu Chidambaram.[26][27]\\r\\nIn 2013, Arudhara Darshan is on December 18.[28]\\r\\nSignificance in Tamil Nadu\\r\\nIn Tamil Nadu, the unmarried women will fast during the day time. They will take food before sunrise and start their fasting. They will break the fast after witnessing the moon rise. Nonbu (fasting) starts nine days before and ends on Thiruvathirai day[29] so totally they fast for ten days.[30]\\r\\nThere is special food called Thiruvadhirai kali made of Rice, Jaggery, Moong dhall, Coconut, Cardamom and Ghee with Thiruvathirai ezhlu curry koottu,[31] which is made out of seven vegetables, that is cooked and served on this day. They choose from Pooshanikai (pumpkin), Paranghikai (ash gourd), Vazhakkai (plantain), Pacha mochai (field beans), Sarkaraivalli kizhangu (sweet potato), Cheppan kizhangu (colocasia), Urulai kizhangu (potato), Katharikai (eggplant) etc.\\r\\nThe dancing form of Lord Shiva is taken out on procession from all Shiva temples in Tamil Nadu. In Chidambaram, The night before the full moon, Abishekam,[32] or holy shower, to the Lord Shiva is performed with the nine most precious gems (navarathnam), including diamonds, coral, pearls, jade and emerald, among others. On the day of full moon, the chariot procession takes place. The most important Arudhra Darshan festival takes place at the Chidambaram Shiva Temple in Tamil Nadu. The cosmic dance of Lord Shiva is enacted on the day.\\r\\nIn Kerala, the festival is celebrated as the birthday of Lord Shiva. Thiruvathira is the nakshatra or \\"star\\" as per the Malayalam calendar of Lord Shiva. Another belief is that the festival commemorates the death of Kamadeva, the Hindu god of erotic desire.[33] It is believed that on this day, the Goddess Parvathi finally met Lord Shiva after her long penance and Lord Shiva took her as a saha-dharma chaarini (equal partner). Both Parvathi and Shiva present this ideal to devotees in the form of Ardha-Nareeshawara (half male, half female form).\\r\\nIn Kerala, Thiruvathira is an important traditional festival along with the other popular festivals, Onam and Vishu. This has been celebrated by the Nambuthiri, Kshatriya and Nair communities of Kerala from days of yore. It is largely a festival for women; unmarried women observe a partial fast on this day to get good husbands and married women take a fast from the preceding day (Makayiram nakshatra) and on the day of Thiruvathira for the well being of their husband and family. The first Thiruvathira of a newly wedded woman is her poothiruvathira.\\r\\nThe fast essentially involves abstaining from rice-based food. The typical meal includes cooked broken wheat and Thiruvathira puzhukku, a delightful mix of tuber vegetables: colocasia (chembu), yam (chena), Chinese potato (koorka), sweet potato (madhurakizhangu) with long beans (vanpayar) and raw plantain fruit (ethakaya), cooked with a thick paste of freshly ground coconut. The dessert is koova payasam, a sweet dish made of arrow root powder, jaggery and coconut milk.\\r\\nThiruvathirakali is a dance form performed by women on the day of Thiruvathira to the accompaniment of Thiruvathira paattu, folk songs telling tales of lovesick Parvati, her longing and penance for Lord Shiva's affection and Shiva's might and power. The sinuous movements executed by the group of dancers around a nilavilakku embody lasya or the amorous charm and grace of the feminine. The dance follows a circular, pirouetting pattern accompanied by clapping of the hands and singing. Today, Thiruvathirakali has become a popular dance form for all seasons.Thiruvathira kali is a typical dance form of Kerala. This is a female group dance made up of simple yet very attractive steps. In ancient times, women use to perform this dance in their homes during festivals and functions, giving it the Malayalam name aka Kaikottikali: aka-inside + kaikottikali-play claping hands. Lore has it that Thiruvathira Kali is in memory of Lord Siva taking Parvathi as his wife. A group of women dressed in typical Kerala style with mundu and neriyathu and the hair bun adorned with jasmine garlands perform this dance during festival seasons.. Kaikottikkali spreads the message of joy and also illustrates the emotions of a married woman towards her beloved and of the unmarried woman longing for one. Thiruvathira is also known as the Kerala's own version of Karva Chauth [34]\\r\\nThe record for the world's largest Thiruvathira belongs to Twenty20 Kizhakkambalam. The 16-minute performance, held on 1st May 2017, has been adjudged the largest Thiruvathira ever held with a total of 6,582 girls and women in the age group of 10-75 participating in it.Along with 2,500 woman and children from Kerala, a Russian woman and more than 4,500 woman from 20 other states too took part in the event.Women were trained by renowned teachers and practitioners of the dance form for the 16 minutes performance.The traditional Kasavu Mundu and Neriyath worn by the dancers were provided by Kitex Group. The event was organised by Twenty20Kizhakkambalam, the CSR wing of the Kitex Group, along with Chavara Cultural Centre and the Parvanendu School of Thiruvathira.\\r\\n\\"The record for the world's largest Thiruvathira belongs to Twenty20 Kizhakkambalam,\\" said Rishi Nath, adjudicator of Guinness World Record while handing over the certificate to Sabu M Jacob MD Kitex Apparel Park and Chief Co ordinator Twenty20.\\r\\nThe previous Guinness Record for largest Kaikottikkali dance performance set on 2nd February 2015 with 5211 women led by Smt. Jitha Binoy under the banner \\"Thanima\\" of Irinjalakuda, Thrissur.[35] The previous Guinness record for largest Kaikottikkali dance was achieved on 9th November 2012 at Dombivli near Mumbai by the Mumbai Pooram foundation, a socio-cultural organisation. In the true Onam Spirit, 2639 women including Keralites, Maharashtrians, Gujarathis, Bengalees, Tamilians, Telugu, Kannadigas and others from all parts of the country representing all religions danced to the tunes of specially written songs.[36] On 14 December 2013, over 3000 women participated in a Thiruvathira Kali event held at Kochi and attempted to set a new world record. The event was organised in connection with the Thiruvathira festival which falls on 18 December 2013.[35][37]\\r\\n{Kizhakkambalam, Kochi dances its way into record books with largest ever Thiruvathira' dance By Express News Service | Published: 02nd May 2017 09:17 PM | Last Updated: 02nd May 2017 09:21 PM}","input":"What is the meaning of thiruvathira in malayalam?"},{"output":"a person who has been admitted to Canada as a non-Canadian citizen permanent resident","context":"Permanent residence of Canada is a status of a person who is not a Canadian citizen but who has been granted permission to live and work in Canada without any time limit on their stay.[1]\\r\\nTo become a permanent resident, a foreign national must make an application to Immigration, Refugees and Citizenship Canada (IRCC), formerly known as Citizenship and Immigration Canada. A permanent resident must live in Canada for two years out of every five, or risk losing that status.\\r\\n\\r\\n\\r\\nA Permanent Resident holds many of the same rights and responsibilities as a Canadian citizen, among others the right to live, study and work (subject to the restrictions of regulated professions), including for the federal or provincial government, anywhere in Canada.\\r\\nPermanent residents may obtain social benefits, employment insurance and Canada Pension Plan payments, and may avail themselves of the rights, freedoms, and protections of the Canadian Charter of Rights and Freedoms, other than those only granted to Canadian citizens.\\r\\nPermanent residents may apply for Canadian citizenship after four years in Canada; however, this is not mandatory.[2]\\r\\nThe main differences from citizenship are that permanent residents cannot:\\r\\nA permanent resident must live in Canada for two years out of every five, or risk losing that status. Time spent travelling with a Canadian spouse or child, on a business trip for a Canadian business or working for a Canadian or provincial government office abroad can be included in the calculation.\\r\\nPermanent residents also risk loss for serious crimes (those that may be punished by more than 10 years in Canada or actually being imprisoned for more than 6 months in Canada), being a security risk or associated with organized crime.\\r\\nFailing to meet the residence or admissibility requirements above results in loss of permanent residence status when the finding of which becomes final without appeal, if the finding is made outside Canada, and upon the person being removed from Canada if the finding is made inside Canada.\\r\\nA person automatically loses permanent residence status upon becoming a Canadian citizen.\\r\\nA permanent resident may also voluntarily give up or renounce their status if the person possesses a citizenship or right of abode in another country. A person who gives up their status inside Canada becomes a temporary resident and must leave Canada within 6 months.\\r\\nA permanent resident does not lose their status if their permanent resident card expires.\\r\\nIn 2002, the Department of Citizenship and Immigration started issuing the Permanent Resident Card (originally billed as and commonly referred to as the Maple Leaf Card) to all new Canadian permanent residents. All existing permanent residents were given the option of applying for a Permanent Resident Card at a cost of $50, though possessing a card is not mandatory except in the case of international travel.[3] From December 31, 2003, every permanent resident must be able to present his or her Permanent Resident Card upon boarding a commercial carrier (aircraft, train or bus) in order to travel to Canada. As the Permanent Resident Card may be issued only in Canada, those permanent residents who are outside Canada and without a Permanent Resident Card may apply for a single-use Travel Document which can be obtained from Canadian embassies abroad.\\r\\nThe Permanent Resident Card expires every five years, and then may be renewed by making application and proving that the applicant has been physically present in Canada for the requisite time period, or has otherwise satisfied the residency requirements. Although an individual may meet the residency requirements by living outside of Canada with a Canadian citizen spouse, or working outside Canada for a Canadian business, the Permanent Resident Card cannot be renewed without being present in Canada and having a Canadian address.\\r\\nWhile the PR Card was introduced to facilitate ease of travel for permanent residents, it can also be used as a convenient method of proving status to government authorities, employers and schools.\\r\\nIn January 2015 the Government of Canada opened a new program to apply for Permanent Residence named \\"Express Entry\\". It replaced the existing model for almost all Economic immigrants (whereby applicants were granted residency on a 'first come, first served' basis) and gave priority to those most likely to succeed economically in Canada. It achieves this by assigning weight to factors such as age, education, work experience, and skills.[4] Some provincial nominees, Quebec applicants and all non-economic immigrants still apply using a paper-based system.\\r\\nThe term \\"landed immigrant\\" (French: immigrant re?u) is an old classification for a person who has been admitted to Canada as a non-Canadian citizen permanent resident. The current official classification for such a person is simply \\"permanent resident\\". The term \\"Landed immigrant\\" has been in use for so long that it is still (15 years later) part of the Canadian vocabulary and still appears in some government publications and forms.\\r\\nTo become a landed immigrant from outside Canada, one had to legally enter Canada, or \\"land\\", at one of the designated ports of entry. Upon entry the immigrant's passport was be stamped with the words \\"Immigrant Landed\\". Once the immigrant had landed, an IMM 1000 form (Record of Landing or Confirmation of Permanent Residence) was be given to provide an official record of landed status.\\r\\nA person can become a Permanent Resident either by applying outside Canada or inside Canada. While the \\"Application Status\\" web application on the CIC website reflects this by showing different processing times, the differences and consequences for the applicant are not clearly identified.\\r\\nOnly the following applications listed in Section 72(2) of the Immigration and Refugee Protection Regulations[5] may be made from within Canada and they are processed at a CPC in Canada:\\r\\nOn humanitarian and compassionate grounds,[6][7] the requirement to apply from outside Canada may be waived for other applications.\\r\\nProcessing is as follows:\\r\\nDuring processing,\\r\\nAfter processing, the applicant becomes a permanent resident after completing a final interview at a local IRCC office where a Confirmation of Permanent Residence will be issued.\\r\\nAll applications that are not \\"Within Canada\\" applications are processed outside Canada at a visa office, usually the Embassy or Consulate of Canada in or responsible for the country of nationality of the applicant or alternatively in the country where the applicant was lawfully admitted to for at least 1 year. All Express Entry applications are \\"Outside Canada\\" applications. Usually Stage 1 processing is done in Canada and then the application is sent to the visa office for Stage 2 and finalization.\\r\\nProcessing times are published weekly on the IRCC website[9]","input":"What is a landed immigrant status in canada?"},{"output":"April 28, 2001","context":"Space tourism is space travel for recreational, leisure or business purposes. There are several different types of space tourism, including orbital, sub orbital and lunar space tourism.?To date orbital space tourism has been performed only by the Russian Space Agency. Work also continues towards developing sub-orbital space tourism vehicles. This is being done by aerospace companies like Blue Origin and Virgin Galactic. In addition, SpaceX (an aerospace manafacturer) announced in 2017 that they are planning on sending two space tourists on a lunar free return trajectory aboard their Dragon V2 spacecraft in 2018.The spacecraft will be launched by the Falcon Heavy rocket.[1]\\r\\nDuring the period from 2001 to 2009, the publicized price for flights brokered by Space Adventures to the International Space Station aboard a Russian Soyuz spacecraft were in the range of US$20ÿ40 million. 7 space tourists made 8 space flights during this time. Some space tourists have signed contracts with third parties to conduct certain research activities while in orbit.\\r\\nRussia halted orbital space tourism in 2010 due to the increase in the International Space Station crew size, using the seats for expedition crews that would have been sold to paying spaceflight participants.[2][3] Orbital tourist flights were set to resume in 2015 but one planned was postponed indefinitely and none have occurred since 2009.[4]\\r\\nAs an alternative term to \\"tourism\\", some organizations such as the Commercial Spaceflight Federation use the term \\"personal spaceflight\\". The Citizens in Space project uses the term \\"citizen space exploration\\".[5]\\r\\n\\r\\n\\r\\nThe Soviet space program was aggressive in broadening the pool of cosmonauts. The Soviet Intercosmos program included cosmonauts selected from Warsaw Pact member countries (Czechoslovakia, Poland, East Germany, Bulgaria, Hungary, Romania) and later from allies of the USSR (Cuba, Mongolia, Vietnam) and non-aligned countries (India, Syria, Afghanistan). Most of these cosmonauts received full training for their missions and were treated as equals, but were generally given shorter flights than Soviet cosmonauts. The European Space Agency (ESA) took advantage of the program as well.\\r\\nThe US space shuttle program included payload specialist positions which were usually filled by representatives of companies or institutions managing a specific payload on that mission. These payload specialists did not receive the same training as professional NASA astronauts and were not employed by NASA. In 1983, Ulf Merbold from ESA and Byron Lichtenberg from MIT (engineer and Air Force fighter pilot) were the first payload specialists to fly on the Space Shuttle, on mission STS-9.[6][7]\\r\\nIn 1984, Charles D. Walker became the first non-government astronaut to fly, with his employer McDonnell Douglas paying $40,000 for his flight. NASA was also eager to prove its capability to Congressional sponsors. During the 1970s, Shuttle prime contractor Rockwell International studied a $200ÿ300 million removable cabin that could fit into the Shuttle's cargo bay. The cabin could carry up to 74 passengers into orbit for up to three days. Space Habitation Design Associates proposed, in 1983, a cabin for 72 passengers in the bay. Passengers were located in six sections, each with windows and its own loading ramp, and with seats in different configurations for launch and landing. Another proposal was based on the Spacelab habitation modules, which provided 32 seats in the payload bay in addition to those in the cockpit area. A 1985 presentation to the National Space Society stated that although flying tourists in the cabin would cost $1 to 1.5 million per passenger without government subsidy, within 15 years 30,000 people a year would pay $25,000 each to fly in space on new spacecraft. The presentation also forecast flights to lunar orbit within 30 years and visits to the lunar surface within 50 years.[8]\\r\\nAs the shuttle program expanded in the early 1980s, NASA began a Space Flight Participant program to allow citizens without scientific or governmental roles to fly. Christa McAuliffe was chosen as the first Teacher in Space in July 1985 from 11,400 applicants. 1,700 applied for the Journalist in Space program. An Artist in Space program was considered, and NASA expected that after McAuliffe's flight two to three civilians a year would fly on the shuttle. After McAuliffe was killed in the Challenger disaster in January 1986, the programs were canceled. McAuliffe's backup, Barbara Morgan, eventually got hired in 1998 as a professional astronaut and flew on STS-118 as a mission specialist.[9]:84ÿ85 A second journalist-in-space program, in which NASA green-lighted Miles O'Brien to fly on the space shuttle, was scheduled to be announced in 2003. That program was canceled in the wake of the Columbia disaster on STS-107 and subsequent emphasis on finishing the International Space Station before retiring the space shuttle.\\r\\nWith the realities of the post-Perestroika economy in Russia, its space industry was especially starved for cash. The Tokyo Broadcasting System (TBS) offered to pay for one of its reporters to fly on a mission. For $28 million, Toyohiro Akiyama was flown in 1990 to Mir with the eighth crew and returned a week later with the seventh crew. Akiyama gave a daily TV broadcast from orbit and also performed scientific experiments for Russian and Japanese companies. However, since the cost of the flight was paid by his employer, Akiyama could be considered a business traveler rather than a tourist.\\r\\nIn 1991, British chemist Helen Sharman was selected from a pool of 13,000 applicants to be the first Briton in space.[10] The program was known as Project Juno and was a cooperative arrangement between the Soviet Union and a group of British companies. The Project Juno consortium failed to raise the funds required, and the program was almost cancelled. Reportedly Mikhail Gorbachev ordered it to proceed under Soviet expense in the interests of international relations, but in the absence of Western underwriting, less expensive experiments were substituted for those in the original plans. Sharman flew aboard Soyuz TM-12 to Mir and returned aboard Soyuz TM-11.\\r\\nAt the end of the 1990s, MirCorp, a private venture that was by then in charge of the space station, began seeking potential space tourists to visit Mir in order to offset some of its maintenance costs. Dennis Tito, an American businessman and former JPL scientist, became their first candidate. When the decision to de-orbit Mir was made, Tito managed to switch his trip to the International Space Station (ISS) through a deal between MirCorp and US-based Space Adventures, Ltd., despite strong opposition from senior figures at NASA; from the beginning of the ISS expeditions, NASA stated it wasn't interested in space guests.[11] Nonetheless, Dennis Tito visited the ISS on April 28, 2001, and stayed for seven days, becoming the first \\"fee-paying\\" space tourist. He was followed in 2002 by South African computer millionaire Mark Shuttleworth. The third was Gregory Olsen in 2005, who was trained as a scientist and whose company produced specialist high-sensitivity cameras. Olsen planned to use his time on the ISS to conduct a number of experiments, in part to test his company's products. Olsen had planned an earlier flight, but had to cancel for health reasons. The Subcommittee on Space and Aeronautics Committee On Science of the House of Representatives held on June 26, 2001 reveals the shifting attitude of NASA towards paying space tourists wanting to travel to the ISS. The hearing's purpose was to, \\"Review the issues and opportunities for flying nonprofessional astronauts in space, the appropriate government role for supporting the nascent space tourism industry, use of the Shuttle and Space Station for Tourism, safety and training criteria for space tourists, and the potential commercial market for space tourism\\".[12] The subcommittee report was interested in evaluating Dennis Tito's extensive training and his experience in space as a nonprofessional astronaut.\\r\\nBy 2007, space tourism was thought to be one of the earliest markets that would emerge for commercial spaceflight.[13]:11 However, as of 2014[update] this private exchange market has not emerged to any significant extent.\\r\\nSpace Adventures is the only company that has sent paying passengers to space.[14][15] In conjunction with the Federal Space Agency of the Russian Federation and Rocket and Space Corporation Energia, Space Adventures facilitated the flights for all of the world's first private space explorers. The first three participants paid in excess of $20 million (USD) each for their 10-day visit to the ISS.\\r\\nin February, 2003, the space shuttle Columbia disintegrated on re-entry into the Earth's atmosphere, killing all seven astronauts aboard. After this disaster, space tourism on the Russian Soyuz program was temporarily put on hold, because Soyuz vehicles became the only available transport to the ISS. On July 26, 2005, Space Shuttle Discovery (mission STS-114) marked the shuttle's return to space. Consequently, in 2006, space tourism was resumed. On September 18, 2006, an Iranian American businesswoman named Anousheh Ansari became the fourth space tourist (Soyuz TMA-9).[16]) On April 7, 2007, Charles Simonyi, an American businessman of Hungarian descent, joined their ranks (Soyuz TMA-10). Simonyi became the first repeat space tourist, paying again to fly on Soyuz TMA-14 in MarchÿApril 2009. Canadian Guy Lalibert became the next space tourist in September, 2009 aboard Soyuz TMA-16. The British singer Sarah Brightman initiated plans (costing a reported $52 million) and participated in preliminary training in early 2015, expecting to then fly (and to perform while in orbit) in September 2015, but in May 2015 she postponed the plans indefinitely.[4][17][18]\\r\\nSeveral plans have been proposed for using a space station as a hotel:\\r\\nIn February 2017, Elon Musk announced that substantial deposits from two individuals had been received by Space X for a Moon loop flight using a free return trajectory and that this could happen as soon as late 2018.[30] Musk said that the cost of the mission would be \\"comparable\\" to that of sending an astronaut to the International Space Station, about $70 million US dollars in 2017.[31]\\r\\nNo suborbital space tourism has occurred yet, but since it is projected to be more affordable, many companies view it as a money-making proposition. Most are proposing vehicles that make suborbital flights peaking at an altitude of 100ÿ160?km (62ÿ99?mi).[32] Passengers would experience three to six minutes of weightlessness, a view of a twinkle-free starfield, and a vista of the curved Earth below. Projected costs are expected to be about $200,000 per passenger.[33]\\r\\nUnder the Outer Space Treaty signed in 1967, the launch operator's nationality and the launch site's location determine which country is responsible for any damages occurred from a launch.[47]\\r\\nAfter valuable resources were detected on the Moon, private companies began to formulate methods to extract the resources. Article II of the Outer Space Treaty dictates that \\"outer space, including the Moon and other celestial bodies, is not subject to national appropriation by claim of sovereignty, by means of use or occupation, or by any other means\\".[48] However, countries have the right to freely explore the Moon and any resources collected are property of that country when they return.\\r\\nIn December 2005, the US government released a set of proposed rules for space tourism.[49] These included screening procedures and training for emergency situations, but not health requirements.\\r\\nUnder current US law, any company proposing to launch paying passengers from American soil on a suborbital rocket must receive a license from the Federal Aviation Administration's Office of Commercial Space Transportation (FAA/AST). The licensing process focuses on public safety and safety of property, and the details can be found in the Code of Federal Regulations, Title 14, Chapter III.[50] This is in accordance with the Commercial Space Launch Amendments Act passed by Congress in 2004.[51]\\r\\nIn March 2010, the New Mexico legislature passed the Spaceflight Informed Consent Act. The SICA gives legal protection to companies who provide private space flights in the case of accidental harm or death to individuals. Participants sign an Informed Consent waiver, dictating that spaceflight operators can not be held liable in the \\"death of a participant resulting from the inherent risks of space flight activities\\". Operators are however not covered in the case of gross negligence or willful misconduct.[52]\\r\\nA 2010 study published in Geophysical Research Letters raised concerns that the growing commercial spaceflight industry could accelerate global warming. The study, funded by NASA and The Aerospace Corporation, simulated the impact of 1,000 suborbital launches of hybrid rockets from a single location, calculating that this would release a total of 600 tonnes of black carbon into the stratosphere. They found that the resultant layer of soot particles remained relatively localised, with only 20% of the carbon straying into the southern hemisphere, thus creating a strong hemispherical asymmetry.[53] This unbalance would cause the temperature to decrease by about 0.4?C (0.72?F) in the tropics and subtropics, whereas the temperature at the poles would increase by between 0.2 and 1?C (0.36 and 1.80?F). The ozone layer would also be affected, with the tropics losing up to 1.7% of ozone cover, and the polar regions gaining 5ÿ6%.[54] The researchers stressed that these results should not be taken as \\"a precise forecast of the climate response to a specific launch rate of a specific rocket type\\", but as a demonstration of the sensitivity of the atmosphere to the large-scale disruption that commercial space tourism could bring.[53]\\r\\nSeveral organizations have been formed to promote the space tourism industry, including the Space Tourism Society, Space Future, and HobbySpace. UniGalactic Space Travel Magazine is a bi-monthly educational publication covering space tourism and space exploration developments in companies like SpaceX, Orbital Sciences, Virgin Galactic and organizations like NASA.\\r\\nClasses in space tourism are currently taught at the Rochester Institute of Technology in New York,[55] and Keio University in Japan.[56]\\r\\nA web-based survey suggested that over 70% of those surveyed wanted less than or equal to 2 weeks in space; in addition, 88% wanted to spacewalk (only 14% of these would do it for a 50% premium), and 21% wanted a hotel or space station.[57]\\r\\nThe concept has met with some criticism from some, including politicians, notably Gnter Verheugen, vice-president of the European Commission, who said of the EADS Astrium Space Tourism Project: \\"It's only for the super rich, which is against my social convictions\\".[58]\\r\\nAs of October 2013, NBC News and Virgin Galactic have come together to create a new reality television show titled Space Race. The show \\"will follow contestants as they compete to win a flight into space aboard Virgin Galactic's SpaceShipTwo rocket plane. It is not to be confused with the Children's Space TV show called \\"Space Racers\\"[59]\\r\\nMany private space travelers have objected to the term \\"space tourist\\", often pointing out that their role went beyond that of an observer, since they also carried out scientific experiments in the course of their journey. Richard Garriott additionally emphasized that his training was identical to the requirements of non-Russian Soyuz crew members, and that teachers and other non-professional astronauts chosen to fly with NASA are called astronauts. He has said that if the distinction has to be made, he would rather be called \\"private astronaut\\" than \\"tourist\\".[60] Dennis Tito has asked to be known as an \\"independent researcher\\",[citation needed] and Mark Shuttleworth described himself as a \\"pioneer of commercial space travel\\".[61] Gregory Olsen prefers \\"private researcher\\",[62] and Anousheh Ansari prefers the term \\"private space explorer\\".[16] Other space enthusiasts object to the term on similar grounds. Rick Tumlinson of the Space Frontier Foundation, for example, has said: \\"I hate the word tourist, and I always will ... 'Tourist' is somebody in a flowered shirt with three cameras around his neck.\\"[63] Russian cosmonaut Maksim Surayev told the press in 2009 not to describe Guy Lalibert as a tourist: \\"It's become fashionable to speak of space tourists. He is not a tourist but a participant in the mission.\\"[64]\\r\\n\\"Spaceflight participant\\" is the official term used by NASA and the Russian Federal Space Agency to distinguish between private space travelers and career astronauts. Tito, Shuttleworth, Olsen, Ansari, and Simonyi were designated as such during their respective space flights. NASA also lists Christa McAuliffe as a spaceflight participant (although she did not pay a fee), apparently due to her non-technical duties aboard the STS-51-L flight.\\r\\nThe US Federal Aviation Administration awards the title of \\"Commercial Astronaut\\" to trained crew members of privately funded spacecraft. The only people currently holding this title are Mike Melvill and Brian Binnie, the pilots of SpaceShipOne.\\r\\nA 2010 report from the Federal Aviation Administration, titled \\"The Economic Impact of Commercial Space Transportation on the U. S Economy in 2009\\", cites studies done by Futron, an aerospace and technology-consulting firm, which predict that space tourism could become a billion-dollar market within 20 years.[65] In addition, in the decade since Dennis Tito journeyed to the International Space Station, eight private citizens have paid the $20 million fee to travel to space. Space Adventures suggests that this number could increase fifteen-fold by 2020.[66] These figures do not include other private space agencies such as Virgin Galactic, which as of 2014 has sold approximately 700 tickets priced at $200,000 or $250,000 dollars each and has accepted more than $80 million in deposits.[67]","input":"When did the first tourist travel into space?"},{"output":"Pope Francis","context":"","input":"Who is the current pope of the world?"},{"output":"Alex Rodriguez","context":"The highest paid player in Major League Baseball (MLB) from the 2013 season is New York Yankees' third baseman Alex Rodriguez with an annual salary of $29,000,000, $4 million higher than the second-highest paid player, Cliff Lee. MLB does not have a hard salary cap, instead employing a luxury tax which applies to teams whose total payroll exceeds certain set thresholds for a given season.[1][2] Free agency did not exist in MLB prior to the end of the reserve clause in the 1970s, allowing owners before that time to wholly dictate the terms of player negotiations and resulting in significantly lower salaries. Babe Ruth, widely regarded as one of the greatest baseball players ever, earned an estimated $910,696 ($14,341,967 inflation-adjusted from 1931 dollars) over his entire playing career.[3] When asked whether he thought he deserved to earn $80,000 a year ($1,146,932 inflation-adjusted), while the president, Herbert Hoover, had a $75,000 salary, Ruth famously remarked, \\"What the hell has Hoover got to do with it? Besides, I had a better year than he did.\\"[4][5]\\r\\nRodriguez has signed two record-breaking contracts over the course of his career. First, he signed a $252 million, 10-year contract with the Texas Rangers in December 2000 ($350,462,609 inflation-adjusted from 2000 dollars).[6] Sandy Alderson called the deal \\"stupefying\\", while Sports Illustrated noted that Rodriguez's early salaries under the contract ($21 million) would be greater than the annual payroll of the entire Minnesota Twins team that year ($15.8 million).[6] The deal was the largest sports contract in history, doubling the total value of Kevin Garnett's $126 million National Basketball Association contract (the previous record holder) and more than doubling Mike Hampton's $121 million contract, the previous MLB record which had been signed just days before.[6] The Rangers later traded Rodriguez to the Yankees in exchange for Alfonso Soriano before the 2004 season, though they agreed to pay $67 million of the $179 million outstanding on the contract.[7] Despite this, he opted out of the remainder of his deal after the 2007 season and renegotiated a new $275 million, 10-year agreement with the Yankees, breaking his own record for the largest sports contract.[8] Under this deal, Rodriguez also receives $6 million each if and when he ties the career home run totals of Willie Mays (660), Babe Ruth (714), Hank Aaron (755), and Barry Bonds (762), along with another $6 million for breaking Bonds' mark.[8]\\r\\nFirst base was the highest paid position in 2010; regular starters at that position earned an average salary of $9,504,165 in compared to an overall average of $3,014,572.[9] Pitcher Nolan Ryan was the first player to earn an annual salary above $1 million, signing a $4.5 million, 4-year contract with the Houston Astros in 1979.[10] Kirby Puckett and Rickey Henderson signed the first contracts which paid an average of $3 million a year in November 1989, while 2010 was the first season where the MLB average salary rose above that same mark.[9][11] Five of the twenty highest paid players in 2013 were members of the Yankees. Their team payroll for 2013 was $228,835,490, roughly $12 million above the second-largest Los Angeles Dodgers.[12] The Yankees have drawn criticism for their payroll, with some claiming it undermines the parity of MLB.[13][14]\\r\\n\\r\\n\\r\\nThis table refers to the salary for 2015 alone, not the overall average value of the contract.\\r\\nNew York Yankees\\r\\nHouston Astros","input":"Who is currently the highest paid mlb player?"},{"output":"May 26, 1896","context":"The Dow Jones Industrial Average /?da? ?d?o?nz/, also called DJIA, the Industrial Average, the Dow Jones, the Dow Jones Industrial, ^DJI, the Dow 30 or simply the Dow, is a stock market index, and one of several indices created by Wall Street Journal editor and Dow Jones & Company co-founder Charles Dow. The industrial average was first calculated on May 26, 1896.[2] Currently owned by S&P Dow Jones Indices, which is majority owned by S&P Global, it is the most notable of the Dow Averages, of which the first (non-industrial) was originally published on February 16, 1885. The averages are named after Dow and one of his business associates, statistician Edward Jones. It is an index that shows how 30 large publicly owned companies based in the United States have traded during a standard trading session in the stock market.[3] It is the second-oldest U.S. market index after the Dow Jones Transportation Average, which was also created by Dow.\\r\\nThe Industrial portion of the name is largely historical, as many of the modern 30 components have little or nothing to do with traditional heavy industry. The average is price-weighted, and to compensate for the effects of stock splits and other adjustments, it is currently a scaled average. The value of the Dow is not the actual average of the prices of its component stocks, but rather the sum of the component prices divided by a divisor, which changes whenever one of the component stocks has a stock split or stock dividend, so as to generate a consistent value for the index. Since the divisor is currently less than one, the value of the index is larger than the sum of the component prices. Although the Dow is compiled to gauge the performance of the industrial sector within the American economy, the index's performance continues to be influenced by not only corporate and economic reports, but also by domestic and foreign political events such as war and terrorism, as well as by natural disasters that could potentially lead to economic harm.\\r\\n\\r\\n\\r\\nBeginning on March 18, 2015, after the close, the Dow Jones Industrial Average consists of the following 30 major companies:\\r\\nThe components of the DJIA have changed 51 times since its beginning in May 26, 1896. General Electric has had the longest continuous presence on the index, with its latest addition being in 1907. More recent changes to the index include the following:\\r\\nIn 1884, Charles Dow composed his first stock average, which contained nine railroads and two industrial companies that appeared in the Customer's Afternoon Letter, a daily two-page financial news bulletin which was the precursor to The Wall Street Journal. On January 2, 1886, the number of stocks represented in what we now call the Dow Jones Transportation Average dropped from 14 to 12, as the Central Pacific Railroad and Central Railroad of New Jersey were dropped from that index. Though comprising the same number of stocks, this index contained only one of the original twelve industrials that would eventually form Dow's most famous index.[10]\\r\\nDow calculated his first average purely of industrial stocks on May 26, 1896, creating what is now known as the Dow Jones Industrial Average. Of the original 12 industrials, only General Electric currently remains part of that index.[11] The other 11 were:[12]\\r\\nWhen it was first published in the mid-1880s, the index stood at a level of 62.76. It reached a peak of 78.38 during the summer of 1890, but ended up hitting its all-time low of 28.48 in the summer of 1896 during the depths of what later became known as the Panic of 1896. Many of the biggest percentage price moves in the Dow occurred early in its history, as the nascent industrial economy matured. A brief war in 1898 between the United States and the Spanish Empire might have only had a minor impact in the Dow's direction.[citation needed]\\r\\nThe 1900s (decade) would see the Dow halt its momentum as it worked its way through a pair of cataclysmic financial crises; the Panic of 1901 and the Panic of 1907. The Dow would remain stuck in a trading range between 53 and 103 points until late 1914. The negativity surrounding the 1906 San Francisco earthquake did little to improve the economic climate.\\r\\nAt the start of the 1910s, the decade would begin with the Panic of 1910ÿ1911 stifling economic growth for a lengthy period of time. History would later take its course on July 30, 1914; as the average stood at a level of 71.42 when a decision was made to close down the New York Stock Exchange, and suspend trading for a span of four and a half months. Some historians believe the exchange closed because of a concern that markets would plunge as a result of panic over the onset of World War I. An alternative explanation is that the Secretary of the Treasury, William Gibbs McAdoo, closed the exchange because he wanted to conserve the U.S. gold stock in order to launch the Federal Reserve System later that year, with enough gold to keep the United States at par with the gold standard. When the markets reopened on December 12, 1914, the index closed at 74.56, a gain of 4.4 percent. This is frequently reported as a large drop, due to using a later redefinition. Reports from the time say that the day was positive.[14] Following World War I, the United States would experience another downturn in economic activity in what became known as the post-World War I recession. The Dow's performance would remain virtually unchanged from the closing value of the previous decade, adding only 8.26%, from 99.05 points at the beginning of 1910, to a level of 107.23 points at the end of 1919.[15]\\r\\nDuring the 1920s, specifically in 1928, the components of the Dow were increased to 30 stocks near the economic height of that decade, which was nicknamed the Roaring Twenties. The prosperous nature of the economic climate, muted the negative influence of an early 1920s recession plus certain international conflicts such as the Polish-Soviet war, the Irish Civil War, the Turkish War of Independence and the initial phase of the Chinese Civil War. The Crash of 1929 in October and the ensuing Great Depression over the next several years returned the average to its starting point, almost 90% below its peak. By July 8, 1932, following its intra-day low of 40.56, the Dow would end up closing the session at 41.22. The high of 381.17 on September 3, 1929, would not be surpassed until 1954, in inflation-adjusted numbers. However, the bottom of the 1929 Crash came just 2? months later on November 13, 1929, when intra-day it was at the 195.35 level, closing slightly higher at 198.69.[16] For the decade, the Dow would end off with a healthy 131.7% gain, from 107.23 points at the beginning of 1920, to a level of 248.48 points at the end of 1929, just before the bulk of the Crash.[17]\\r\\nMarked by global instability and the Great Depression, the 1930s contended with several consequential European and Asian outbreaks of war, leading up to catastrophic World War II in 1939. Other conflicts during the decade which affected the stock market included the 1936ÿ1939 Spanish Civil War, the 1935ÿ1936 Second Italo-Abyssinian War, the Soviet-Japanese Border War of 1939 and the Second Sino-Japanese War from 1937. On top of that, the United States dealt with a painful recession in 1937 and 1938 which temporarily brought economic recovery to a halt. The largest one-day percentage gain in the index, 15.34%, happened on March 15, 1933, in the depths of the 1930s bear market when the Dow gained 8.26 points to close at 62.10. However, as a whole throughout the Great Depression, the Dow posted some of its worst performances, for a negative return during most of the 1930s for new and old stock market investors. For the decade, the Dow Jones average was down from 248.48 points at the beginning of 1930, to a stable level of 150.24 points at the end of 1939, a loss of about 40%.[18]\\r\\nPost-war reconstruction during the 1940s, along with renewed optimism of peace and prosperity, brought about a 39% surge in the Dow from around the 148 level to 206. The strength in the Dow occurred despite a brief recession in 1949 and other global conflicts which started a short time later including the latter stages of the Chinese Civil War, the Greek Civil War, the Indo-Pakistani War of 1947 and the 1948 ArabÿIsraeli War.\\r\\nDuring the 1950s, the Korean War, the Algerian War, the Cold War and other political tensions such as the Cuban Revolution, as well as widespread political and economic changes in Africa during the initial stages of European Decolonization, did not stop the Dow's bullish climb higher. Additionally, the U.S. would also make its way through two grinding recessions; one in 1953 and another in 1958. A 200% increase in the average from a level of 206 to 616 ensued over the course of that decade.\\r\\nThe Dow's bullish behavior began to stall during the 1960s as the U.S. became entangled with foreign political issues. U.S. military excursions included the Bay of Pigs Invasion involving Cuba, the Vietnam War, the Portuguese Colonial War, the Colombian civil war which the U.S. assisted with short-lived counter-guerrilla campaigns, as well as domestic issues such as the Civil Rights Movement and several influential political assassinations. For the decade though, and despite a mild recession between 1960 and 1961, the average still managed a respectable 30% gain from the 616 level to 800.\\r\\nThe 1970s marked a time of economic uncertainty and troubled relations between the U.S. and certain Middle-Eastern countries. To begin with, the decade started off with the ongoing Recession of 1969ÿ70. Following that, the 1970s Energy Crisis ensued which included the 1973ÿ75 recession, the 1973 Oil Crisis as well as the 1979 energy crisis beginning as a prelude to a disastrous economic climate injected with stagflation; the combination between high unemployment and high inflation. However, on November 14, 1972, the average closed above the 1,000 mark (1,003.16) for the first time, during a brief relief rally in the midst of a lengthy bear market. Between January 1973 and December 1974, the average lost 48% of its value in what became known as the 1973ÿ1974 Stock Market Crash; with the situation being exacerbated by the events surrounding the Yom Kippur War. The index closed at 577.60, on December 4, 1974. During 1976, the index went above 1000 several times, and it closed the year at 1,004.75. Although the Vietnam War ended in 1975, new tensions arose towards Iran surrounding the Iranian Revolution in 1979. Other notable disturbances such as the Lebanese Civil War, the Ethiopian Civil War, the Indo-Pakistani War of 1971 and the Angolan Civil War which the U.S. and Soviet Union considered critical to the global balance of power, seemed to have had little influence towards the financial markets. Performance-wise for the decade, gains remained virtually flat, rising less than 5% from about the 800 level to 838.\\r\\nThe 1980s decade started with the early 1980s recession. In early 1981, it broke above 1000 several times, but then retreated. The largest one-day percentage drop occurred on Black Monday; October 19, 1987, when the average fell 22.61%. There were no clear reasons given to explain the crash, but program trading may have been a major contributing factor. On October 13, 1989, the Dow stumbled into another downfall, the 1989 Mini-Crash which initiated the collapse of the junk bond market as the Dow registered a loss of almost 7%.\\r\\nFor the decade, the Dow made a 228% increase from the 838 level to 2,753; despite the market crashes, Silver Thursday, an early 1980s recession, the 1980s oil glut, the Japanese asset price bubble and other political distractions such as the Soviet war in Afghanistan, the Falklands War, the IranÿIraq War, the Second Sudanese Civil War and the First Intifada in the Middle East. The index had only two negative years, which were in 1981 and 1984.\\r\\nThe 1990s brought on rapid advances in technology along with the introduction of the dot-com era. To start off, the markets contended with the 1990 oil price shock compounded with the effects of the Early 1990s recession and a brief European situation surrounding Black Wednesday. Certain influential foreign conflicts such as the 1991 Soviet coup d'tat attempt which took place as part of the initial stages of the Dissolution of the USSR and the Fall of Communism; the First and Second Chechen Wars, the Persian Gulf War and the Yugoslav Wars failed to dampen economic enthusiasm surrounding the ongoing Information Age and the \\"irrational exuberance\\" (a phrase coined by Alan Greenspan) of the Internet Boom. Even the occurrences of the Rwandan Genocide and the Second Congo War, termed as \\"Africa's World War\\" that involved 8 separate African nations which together between the two killed over 5 million people; didn't seem to have any noticeable negative financial impact on the Dow either. Between late 1992 and early 1993, the Dow staggered through the 3,000 level making only modest gains as the Biotechnology sector suffered through the downfall of the Biotech Bubble; as many biotech companies saw their share prices rapidly rise to record levels and then subsequently fall to new all-time lows.\\r\\nOn November 21, 1995, the DJIA closed above the 5,000 level (5,023.55) for the first time. Over the following two years, the Dow would rapidly tower above the 6,000 level during the month of October in 1996, and the 7,000 level in February 1997. On its march higher into record territory, the Dow easily made its way through the 8,000 level in July 1997. However, later in that year during October, the events surrounding the Asian Financial Crisis plunged the Dow into a 554-point loss to a close of 7,161.15; a retrenchment of 7.18% in what became known as the 1997 Mini-Crash. Although internationally there was negativity surrounding the 1998 Russian financial crisis along with the subsequent fallout from the 1998 collapse of the derivatives Long-Term Capital Management hedge fund involving bad bets placed on the movement of the Russian ruble, the Dow would go on to surpass the 9,000 level during the month of April in 1998, making its sentimental push towards the symbolic 10,000 level. On March 29, 1999, the average closed above the 10,000 mark (10,006.78) after flirting with it for two weeks. This prompted a celebration on the trading floor, complete with party hats. The scene at the exchange made front-page headlines on many U.S. newspapers such as The New York Times. On May 3, 1999, the Dow achieved its first close above the 11,000 mark (11,014.70). Total gains for the decade exceeded 315%; from the 2,753 level to 11,497.\\r\\nThe Dow averaged a 5.3% return compounded annually for the 20th century, a record Warren Buffett called \\"a wonderful century\\"; when he calculated that to achieve that return again, the index would need to close at about 2,000,000 by December 2099.[19] Even during the height of the dot-com era, authors James K. Glassman and Kevin A. Hassett went so far as to publish a book entitled Dow 36,000: The New Strategy for Profiting From the Coming Rise in the Stock Market. Their theory was to imply that stocks were still cheap and it was not too late to benefit from rising prices during the Internet boom.\\r\\nCharacterized by fear on the part of newer investors, the uncertainty of the 2000s (decade) brought on a significant bear market. There was indecision on whether the cyclical bull market represented a prolonged temporary bounce or a new long-term trend. Ultimately, there was widespread resignation and disappointment as the lows were revisited, and in some cases, surpassed near the end of the decade.\\r\\nThe third largest one-day point drop in DJIA history, and largest at the time, occurred on September 17, 2001, the first day of trading after the September 11, 2001 attacks, when the Dow fell 684.81 points, or 7.1%. However, the Dow had been in a downward trend for virtually all of 2001 prior to September 11, losing well over 1000 points between January 2 and September 10, and had lost 187.51 points on September 6, followed by losing 235.4 points on September 7.[20] By the end of that week, the Dow had fallen 1,369.70 points, or 14.3%. However, the Dow began an upward trend shortly after the attacks, and quickly regained all lost ground to close above the 10,000 level for the year.\\r\\nDuring 2002, the average remained subdued without making substantial gains due to the stock market downturn of 2002 as well as the lingering effects of the dot-com bubble. In 2003, the Dow held steady within the 7,000 to 9,000-point level range by the early 2000s recession, the Afghan War and the Iraq War. But by December of that year, the Dow remarkably returned to the 10,000 mark. In October 2006, four years after its bear market low, the DJIA set fresh record theoretical, intra-day, daily close, weekly, and monthly highs for the first time in almost seven years, closing above the 12,000 level for the first time on the 19th anniversary of Black Monday (1987). On February 27, 2007, the Dow Jones Industrial Average fell 3.3% (415.30 points), its biggest point drop since 2001. The initial drop was caused by a global sell-off after Chinese stocks experienced a mini-crash, yet by April 25, the Dow passed the 13,000 level in trading and closed above that milestone for the first time. On July 19, 2007, the average passed the 14,000 level, completing the fastest 1,000-point advance for the index since 1999. One week later, a 450-point intra-day loss, owing to turbulence in the U.S. sub-prime mortgage market and the soaring value of the yuan,[21][22] initiated another correction falling below the 13,000 mark, about 10% from its highs.\\r\\nOn October 9, 2007, the Dow Jones Industrial Average closed at a record high of 14,164.53. Two days later on October 11, the Dow traded at an intra-day level high of 14,198.10,[23] a mark which would not be matched until March 2013.[24] In what would normally take many years to accomplish; numerous reasons were cited for the Dow's extremely rapid rise from the 11,000 level in early 2006, to the 14,000 level in late 2007. They included future possible takeovers and mergers, healthy earnings reports particularly in the tech sector, and moderate inflationary numbers; fueling speculation the Federal Reserve would not raise interest rates.\\r\\nOn September 15, 2008, a wider financial crisis became evident when Lehman Brothers filed for Chapter 11 bankruptcy along with the economic effect of record high oil prices which reached almost $150 per barrel two months earlier. When opening that morning, it immediately lost 300 points and overall the DJIA lost more than 500 points for only the sixth time in history, returning to its mid-July lows below the 11,000 level. A series of \\"bailout\\" packages, including the Emergency Economic Stabilization Act of 2008, proposed and implemented by the Federal Reserve and U.S. Treasury, as well as FDIC-sponsored bank mergers, did not prevent further losses. After nearly six months of extreme volatility during which the Dow experienced its largest one-day point loss, largest daily point gain, and largest intra-day range (more than 1,000 points), the index closed at a new twelve-year low of 6,547.05 on March 9, 2009 (after an intra-day low of 6,469.95[25] during the March 6 session), its lowest close since April 1997, and had lost 20% of its value in only six weeks.\\r\\nTowards the latter half of 2009, the average rallied towards the 10,000 level amid optimism that the Late-2000s (decade) Recession, the United States Housing Bubble and the Global Financial Crisis of 2008ÿ2009, were easing and possibly coming to an end. For the decade, the Dow saw a rather substantial pullback for a negative return from the 11,497 level to 10,428, a loss of a little over 9%.\\r\\nDuring the early part of the 2010s, aided somewhat by the loose monetary policy practiced by the Federal Reserve, the Dow made a notable rally attempt, though with significant volatility due to growing global concerns such as the 2010 European sovereign debt crisis, the Dubai debt crisis, and the United States debt ceiling crisis. On May 6, 2010, the index lost around 400 points over the day, then just after 2:30?pm EDT, it lost about 600 points in just a few minutes, and gained the last amount back about as quickly. The intra-day change at the lowest point was 998.50 points, the largest intra-day point decline ever, representing an intra-day loss of 9.2%. The event, during which the Dow bottomed out at 9,869 before recovering to end with a 3.2% daily loss at 10,520.32, became known as the 2010 Flash Crash.[26] The index closed the half-year at 9,774.02 for a loss of 7.7%.[27]\\r\\nOn May 3, 2013, the Dow surpassed the 15,000 mark for the first time, while later on November 18, it closed above the 16,000 level.[28] Following a strong jobs report on July 3, 2014, the Dow traded above the 17,000 mark for the first time.[29] On December 23, 2014, the DJIA traded above the 18,000 boundary for the first time, after data showed the U.S. economy posted its strongest growth in more than a decade.[30] The index closed 2014 at 17,823.07 for a gain of 71% for the five years.[31]\\r\\nDuring the summer of 2015 the Dow began to retreat from its all-time high due to overwhelming economic factors entering correction for the first time since 2011.[citation needed] By October, the Dow had exited correction rallying 14% from its August lows, but failed to hit a record high set back in May.[citation needed] In November and December, the Dow continued to retreat from the 14% rally in October, leading some to call it a bear market.[32][33] This led to the Dow closing at 17,425.03 for 2015, the first annual loss since 2008.[34] After nearly 14 months since the last record close, the Dow finally achieved a fresh new, central-bank debt fuelled record close on July 20, 2016 at 18,595.03 along with an intraday high of 18,622.01.[35]\\r\\nDespite anticipations of post-election selloffs, the Dow rallied significantly after Donald Trump was elected President. On January 25, 2017, the Dow hit a record high of 20,000, an increase of 1,667 points since his election in November 2016.[36][37] On March 1, 2017, the Dow broke through the 21,000 level,[38] reaching a new all-time high. The 1,000 point gain took just 35 days being tied for the fastest time ever. The Dow hit yet another new high just 5 months later, surpassing the 22,000 level [39] on August 2, 2017.\\r\\nInvesting in the DJIA is made widely accessible in equities through exchange-traded funds (ETFs) as well as in derivatives through option contracts and futures contracts.\\r\\nThe index is tracked by an exchange-traded fund, the SPDR Dow Jones Industrial Average (NYSE?Arca:?DIA), commonly called \\"diamonds\\". This fund is part of the SPDR family of ETFs from State Street Global Advisors. This fund was introduced in 1998, and it was previously called DIAMONDS Trust, Series 1.\\r\\nDue to the advent of pre-market trading, the \\"diamonds\\" provide a very accurate opening value for the average. As an example, if the ETF opens the trading session with a 76S loss; then that would strongly indicate roughly a 76-point loss for the Dow within the first few seconds or so, even before all of its components open for trade. Likewise, if the ETF starts the trading session higher by $1.12, then that would signal an approximate gain for the Dow of 112 points at the open, even if some components begin trading at 9:31?am or 9:33?am due to a delay.\\r\\nAnother asset management firm, ProFunds, issue other related DJIA ETFs through ProShares such as the Inverse Performance (NYSE?Arca:?DOG) for a bearish strategy on the average. That is, if the DJIA falls by some percentage, the ETF gains an equal percentage; thus giving an alternative to a sell short position if one has a bearish goal in mind.\\r\\nProFunds also issues the 2x (NYSE?Arca:?DDM), which attempts to match the daily performance of the DJIA by 200% and the Inverse 2x (NYSE?Arca:?DXD), which attempts to match the inverse daily performance by 200%. In the case of 2x performance, the ETF increases the buying power by leveraging money without using margin. Currently, there are also 3x performance ETFs issued by ProShares that exist too; which attempt to replicate (300% leverage) against the Dow. For 3x performance, the symbol is (NYSE?Arca:?UDOW), and for Inverse 3x performance, it is (NYSE?Arca:?SDOW).[40][41]\\r\\nIn the derivatives market, the CME Group through its subsidiaries the Chicago Mercantile Exchange (CME) and the Chicago Board of Trade (CBOT), issues Futures Contracts; the E-mini Dow ($5) Futures (YM), which track the average and trade on their exchange floors respectively. Trading is typically carried out in an Open Outcry auction, or over an electronic network such as CME's Globex platform. Dow Futures is one of the most important premarket tool and reflect the mood in which DJIA will open.\\r\\nThe Chicago Board Options Exchange (CBOE) issues Options Contracts on the Dow through the root symbol DJX in combination with long-term expiration options called DJX LEAPS. There are also options on the various ETFs; Performance ETFs, Inverse Performance ETFs, 2x Performance ETFs, Inverse 2x Performance ETFs, 3x Performance ETFs, and Inverse 3x Performance ETFs.\\r\\nTo calculate the DJIA, the sum of the prices of all 30 stocks is divided by a divisor, the Dow Divisor. The divisor is adjusted in case of stock splits, spinoffs or similar structural changes, to ensure that such events do not in themselves alter the numerical value of the DJIA. Early on, the initial divisor was composed of the original number of component companies; which made the DJIA at first, a simple arithmetic average. The present divisor, after many adjustments, is less than one (meaning the index is larger than the sum of the prices of the components). That is:\\r\\nwhere p are the prices of the component stocks and d is the Dow Divisor.\\r\\nEvents such as stock splits or changes in the list of the companies composing the index alter the sum of the component prices. In these cases, in order to avoid discontinuity in the index, the Dow Divisor is updated so that the quotations right before and after the event coincide:\\r\\nThe Dow Divisor was 0.14523396877348 on September 22, 2017.[42] Presently, every $1 change in price in a particular stock within the average, equates to a 6.885 (or 1  0.14523396877348) point movement.\\r\\nWith the current inclusion of only 30 stocks, critics such as Ric Edelman argue that the DJIA is not a very accurate representation of overall market performance. Still, it is the most cited and most widely recognized of the stock market indices.[43][44] Additionally, the DJIA is criticized for being a price-weighted average, which gives higher-priced stocks more influence over the average than their lower-priced counterparts, but takes no account of the relative industry size or market capitalization of the components. For example, a $1 increase in a lower-priced stock can be negated by a $1 decrease in a much higher-priced stock, even though the lower-priced stock experienced a larger percentage change. In addition, a $1 move in the smallest component of the DJIA has the same effect as a $1 move in the largest component of the average. For example, during SeptemberÿOctober 2008, former component AIG's reverse split-adjusted stock price collapsed from $22.76 on September 8 to $1.35 on October 27; contributing to a roughly 3,000-point drop in the index.[45]\\r\\nAs of August 2017, Boeing and 3M are among the highest priced stocks in the average and therefore have the greatest influence on it. Alternately, Cisco Systems and General Electric are among the lowest priced stocks in the average and have the least amount of sway in the price movement.[46] Many critics of the DJIA recommend the float-adjusted market-value weighted S&P 500 or the Wilshire 5000, the latter of which includes all U.S. equity securities, as better indicators of the U.S. stock market.\\r\\nA study between the correlation of components of the Dow Jones Industrial Average compared with the movement of the index, finds that the correlation is higher in a time period where the average recedes and goes down. The correlation is lowest in a time when the average is flat or rises a modest amount.[47]","input":"When did the dow jones industrial average start?"},{"output":"29 August 1949","context":"The Soviet atomic bomb project[1] (Russian: Ѿ־Ҷҳ ŤѾ쿹 Ҿ񼼷 ɾ񼼼, Sovetskiy proyekt atomnoy bomby) was the classified research and development program that was authorized by Joseph Stalin in the Soviet Union to develop nuclear weapons during World War II.[2][3]\\r\\n\\r\\nAlthough the Soviet scientific community discussed the possibility of an atomic bomb throughout the 1930s,[4][5] going as far as making a concrete proposal to develop such a weapon in 1940,[6][7][8] the full-scale program was initiated only in response to the intelligence reports collected by Soviet intelligence through their spy ring in the United States on the secretive Manhattan Project.\\r\\n\\r\\nBecause of the conspicuous silence of the scientific publications on the subject of nuclear fission by German, American, and British scientists, Russian physicist Georgy Flyorov suspected that the Allied powers had secretly been developing a \\"superweapon\\"[3] since 1939. Flyorov wrote a letter to Stalin urging him to start this program in 1942.[9]:78ÿ79 Initial efforts were slowed due to the German invasion of the Soviet Union and remained largely composed of the intelligence knowledge gained from the spy rings working in the U.S.' Manhattan Project in 1943.[2]\\r\\n\\r\\nAfter learning of the atomic bombings of Hiroshima and Nagasaki that ended the Pacific War in 1945, the program was aggressively pursued, mainly through effective intelligence gathering about the German weapon project and the American Manhattan Project.[10] The Russian efforts also rounded up captured German scientists to join their program, and relied heavily on knowledge passed by the spy ring to the Russian intelligence agencies.[11]:242ÿ243\\r\\n\\r\\nOn 29 August 1949, the Soviet Union secretly conducted its first successful weapon test (First Lightning), based on the American design at the Semipalatinsk in Kazakhstan.[2]\\r\\n\\r\\nAs early as 1910 in Russia, independent research was being conducted on radioactive elements by several Russian scientists.:44[12]:24ÿ25[13] Despite the hardship faced by the Russian academy of sciences during the national revolution in 1917, followed by the violent civil war in 1922, the Russian scientists had made remarkable efforts towards the advancement of physics research in the Soviet Union in the 1930s.[14]:35ÿ36 Before the first revolution in 1905, the mineralogist Vladimir Vernadsky had made a number of public calls for a survey of Russia's uranium deposits but none were heeded.[14]:37\\r\\n\\r\\nHowever, such early efforts were independently and privately funded by various organizations until 1922 when the Radium Institute in Petrograd (now Saint Petersburg) opened and industrialized the research.:44[12]\\r\\n\\r\\nFrom the 1920s until the late 1930s, Russian physicists had been conducting joint research with their European counterparts on the advancement of atomic physics at the Cavendish Laboratory run by a British physicist, Ernest Rutherford, where Georgi Gamov and Pyotr Kapitsa had studied and researched.[14]:36\\r\\n\\r\\nInfluential research towards the advancement of nuclear physics was guided by Abram Ioffe, who was the director at the Leningrad Physical-Technical Institute (LPTI), having sponsored various research programs at various technical schools in the Soviet Union.[14]:36 The discovery of the neutron by the British physicist James Chadwick further provided promising expansion of the LPTI's program, with the operation of the first cyclotron to energies of over 1 MeV, and the first \\"splitting\\" of the atomic nucleus by John Cockcroft and Ernest Walton.[14]:36ÿ37 Russian physicists began pushing the government, lobbying in the interest of the development of science in the Soviet Union, which had received little interest due to the upheavals created during the Russian revolution and the February Revolution.[14]:36ÿ37 Earlier research was directed towards the medical and scientific exploration of radium, that could be retrieved from borehole water from the Ukhta oilfields.[14]:37\\r\\n\\r\\nIn 1939, German chemist Otto Hahn reported his discovery of fission, achieved by the splitting of uranium with neutrons that produced the much lighter element barium. This eventually led to the realization among Russian scientists, and their American counterparts, that such reaction could have military significance.:20[15] The discovery excited the Russian physicists, and they began conducting their independent investigations on nuclear fission, mainly aiming towards power generation, as many were skeptical of possibility of creating an atomic bomb anytime soon.:25[16] Early efforts were led by Yakov Frenkel (a physicist specialised on condensed matter), who did the first theoretical calculations on continuum mechanics directly relating the kinematics of binding energy in fission process in 1940.:99[15] Georgy Flyorov's and Lev Rusinov's collaborative work on thermal reactions concluded that 3-1 neutrons were emitted per fission only days after similar conclusions had been reached by the team of Frdric Joliot-Curie.:63[15]:200[17]\\r\\n\\r\\nAfter a strong lobbying of Russian scientists, the Soviet government did initially set up a commission that was to address the \\"uranium problem\\" and investigate the possibility of chain reaction and Isotope separation.:33[18] The Uranium Problem Commission was ineffective due to the German invasion of Soviet Union that eventually limited the focus on research as Russia became engaged in a bloody conflict along the Eastern Front for the next four years.:114ÿ115[19]:200[20] The Soviet atomic weapons program had no significance and most work was unclassified as the papers were continuously published as public domain in academic journals.:33[18]\\r\\n\\r\\nJoseph Stalin, the Soviet leader, had mostly disregarded the atomic knowledge possessed by the Russian scientists and had most of the scientists working in the metallurgy and mining industry or serving in the Soviet Armed Forces technical branches during the World War II's eastern front in 1940ÿ42.:xx[21]\\r\\n\\r\\nIn 1940ÿ42, Georgy Flyorov, a Russian physicist serving as an officer in the Soviet Air Force, noted that despite progress in other areas of physics, the German, British, and American scientists had ceased publishing papers on nuclear science; clearly they each had active secret research programs.:230[22]\\r\\n\\r\\nIn April 1942, Flyorov directed two classified letters to Stalin, warning him of the consequences of the development of atomic weapons: \\"...the results will be so overriding [that] it won't be necessary to determine who is to blame for the fact that this work has been neglected in our country.\\":xxx[23] The second letter, by Flyorov and Konstantin Petrzhak, highly emphasized the importance of a \\"uranium bomb\\": \\"it is essential to manufacture a uranium bomb without a delay.\\":230[22]\\r\\n\\r\\nUpon reading the Flyorov letters, Stalin immediately pulled Russian physicists from their respective military services and authorized an atomic bomb project, under engineering physicist Anatoly Alexandrov and nuclear physicist Igor V. Kurchatov.:230[22]:xx[21] For this purpose, the Laboratory No. 2 near Moscow was established under Kurchatov.:230[22] At the same time, Flyorov was moved to Dubna, where he established the Laboratory of Nuclear Reactions, focusing on synthetic elements and thermal reactions.:xx[21] In late 1942, the State Defense Committee officially delegated the program to the Soviet Army, with major wartime logistical efforts later being supervised by Lavrentiy Beria, the head of NKVD.:115:114ÿ115[19]\\r\\n\\r\\nIn 1945, the Arzamas 16 site near Moscow was established under Yakov Zel'dovich and Yuli Khariton who performed calculations on  nuclear combustion theory, alongside Isaak Pomeranchuk.:117ÿ118[24] Despite early and accelerated efforts, it was reported by historians that efforts on building a bomb using weapon-grade uranium seemed hopeless to Russian scientists.:117ÿ118[24] Igor Kurchatov had harboured doubts working towards the uranium bomb, made progress on a bomb using weapon-grade plutonium after British data was provided by the NKVD.:117ÿ118[24]\\r\\n\\r\\nThe situation dramatically changed when the Soviet Union learned of the atomic bombings of Hiroshima and Nagasaki in 1945, which eventually led to the unilateral surrender of Japan to the United States, ending the war in Asia.:2ÿ5[25]\\r\\n\\r\\nImmediately after the atomic bombing, the Soviet Politburo took control of the atomic bomb project by establishing a special committee to oversee the development of nuclear weapons as soon as possible.:2ÿ5[25] On 9 April 1946, the Council of Ministers created the KBÿ11 Design Bureau that worked towards mapping the first nuclear weapon design, primarily based on American approach and detonated with weapon-grade plutonium.:2ÿ5[25] From this point, the work on the program was carried out quickly, resulting in the first nuclear reactor near Moscow on 25 October 1946.:2ÿ5[25]\\r\\n\\r\\nFrom 1941ÿ46, the Soviet Union's Ministry of Foreign Affairs handled the logistics of the atomic bomb project, with Foreign Minister Vyacheslav Molotov controlling the direction of the program.:33[26] However, Molotov proved to be a weak administrator, and the program stagnated.[27] In contrast to American military administration in their atomic bomb project, the Russians' program was directed by political dignitaries such as Molotov, Lavrentiy Beria, Georgii Malenkov, and Mikhail Pervukhin  there were no military members.[28]\\r\\n\\r\\nAfter the atomic bombings of Hiroshima and Nagasaki, the program's leadership changed, when Stalin appointed Lavrentiy Beria on 22 August 1945.[27] Beria is noted for leadership that helped the program to its final implementation.[27]\\r\\n\\r\\nBeria understood the necessary scope and dynamics of research. This man, who was the personification of evil to modern Russian history, also possessed the great energy and capacity to work. The scientists who met him could not fail to recognize his intelligence, his will power, and his purposefullness. They found him first-class administrator who could carry a job through to completion...\\r\\nThe new Committee, under Beria, retained Georgii Malenkov and added Nikolai Voznesensky and Boris Vannikov, People's Commissar for Armament.[27] Under the administration of Beria, the NKVD co-opted atomic spies of the Soviet Atomic Spy Ring into the Western Allied program, and infiltrated the German nuclear program.[27]\\r\\n\\r\\nThe atomic and industrial espionages in the United States by American sympathisers of communism who were controlled by their rezident Russian officials in North America greatly aided the speed of the Soviet atomic project from 1942ÿ54.:105ÿ106[29]:287ÿ305[30] Willingness of sharing classified information to Soviet Union by recruited American communist sympathizers increased when the Soviet Union faced possible defeat during the German invasion in World War II.:287ÿ289[30] The Russian intelligence network in the United Kingdom also played a vital role in setting up the spy rings in the United States when the Russian State Defense Committee approved resolution 2352[clarification needed], in September 1942.:105ÿ106[29]\\r\\n\\r\\nFor this purpose, the spy Harry Gold, controlled by Semyon Semyonov, was used for a wide range of espionage that included industrial espionage in the American chemical industry and obtaining sensitive atomic information that was handed over to him by the British physicist Klaus Fuchs.:289ÿ290[30] Knowledge and further technical information that were passed by the American Theodore Hall, a theoretical physicist, and Klaus Fuchs had a significant impact on the direction of Russian development of nuclear weapons.:105[29]\\r\\n\\r\\nLeonid Kvasnikov, a Russian chemical engineer turned KGB officer, was assigned for this special purpose and moved to New York City to coordinate such activities.[31] Anatoli Yatzkov, another NKVD official in New York, was also involved in obtaining sensitive information gathered by Sergei Kournakov from Saville Sax.[31]\\r\\n\\r\\nThe existence of Russian spies were exposed by the U.S. Army's secretive Venona project in 1943.:54[32]\\r\\n\\r\\nFor example, Soviet work on methods of uranium isotope separation was altered when it was reported, to Kurchatov's surprise, that the Americans had opted for the Gaseous diffusion method. While research on other separation methods continued throughout the war years, the emphasis was placed on replicating U.S. success with gaseous diffusion. Another important breakthrough, attributed to intelligence, was the possibility of using plutonium instead of uranium in a fission weapon. Extraction of plutonium in the so-called \\"uranium pile\\" allowed bypassing of the difficult process of uranium separation altogether, something that Kurchatov had learned from intelligence from the Manhattan project.[citation needed]\\r\\n\\r\\nIn 1945, the Soviet intelligence obtained rough blueprints of the first U.S. atomic device.[33][34] Alexei Kojevnikov has estimated, based on newly released Soviet documents, that the primary way in which the espionage may have sped up the Soviet project was that it allowed Khariton to avoid dangerous tests to determine the size of the critical mass: \\"tickling the dragon's tail,\\" as it was called in the U.S., consumed a good deal of time and claimed at least two lives; see Harry Daghlian and Louis Slotin.\\r\\n\\r\\nThe published Smyth Report of 1945 on the Manhattan Project was translated into Russian, and the translators noted that a sentence on the effect of \\"poisoning\\" of Plutonium-239 in the first (lithograph) edition had been deleted from the next (Princeton) edition by Groves. This change was noted by the Russian translators, and alerted the Soviet Union to the problem (which had meant that reactor-bred plutonium could not be used in a simple gun-type bomb like the proposed Thin Man).\\r\\n\\r\\nOne of the key pieces of information, which Soviet intelligence obtained from Fuchs, was a cross-section for D-T fusion. This data was available to top Soviet officials roughly three years before it was openly published in the Physical Review in 1949. However, this data was not forwarded to Vitaly Ginzburg or Andrei Sakharov until very late, practically months before publication.[citation needed] Initially both Ginzburg and Sakharov estimated such a cross-section to be similar to the D-D reaction. Once the actual cross-section become known to Ginzburg and Sakharov, the Sloika design become a priority, which resulted in a successful test in 1953.\\r\\n\\r\\nIn the 1990s, with the declassification of Soviet intelligence materials, which showed the extent and the type of the information obtained by the Soviets from US sources, a heated debate ensued in Russia and abroad as to the relative importance of espionage, as opposed to the Soviet scientists' own efforts, in the making of the Soviet bomb. The vast majority of scholars[Like whom?] agree that whereas the Soviet atomic project was first and foremost a product of local expertise and scientific talent, it is clear that espionage efforts contributed to the project in various ways and most certainly shortened the time needed to develop the atomic bomb.[citation needed]\\r\\n\\r\\nComparing the timelines of H-bomb development, some researchers came to the conclusion that the Soviets had a gap in access to classified information regarding the H-bomb at least between late 1950 and some time in 1953. Earlier, e.g., in 1948, Fuchs gave the Soviets a detailed update of the classical super progress[clarification needed], including an idea to use lithium, but did not explain it was specifically lithium-6. Teller accepted the fact that \\"classical super\\" scheme was infeasible by 1951, following results obtained by various researchers (including Stanislaw Ulam) and calculations performed by John von Neumann in late 1950.\\r\\n\\r\\nYet the research for the Soviet analogue of \\"classical super\\" continued until December 1953, when the researchers were reallocated to a new project working on what later became a true H-bomb design, based on radiation implosion. This remains an open topic for research, whether the Soviet intelligence was able to obtain any specific data on Teller-Ulam design in 1953 or early 1954. Yet, Soviet officials directed the scientists to work on a new scheme, and the entire process took less than two years, commencing around January 1954 and producing a successful test in November 1955. It also took just several months before the idea of radiation implosion was conceived, and there is no documented evidence claiming priority. It is also possible that Soviets were able to obtain a document lost by John Wheeler on a train in 1953, which reportedly contained key information about thermonuclear weapon design.\\r\\n\\r\\nEarly ideas of the fusion bomb came from espionage and internal Soviet studies. Though the espionage did help Soviet studies, the early American H-bomb concepts had substantial flaws, so it may have confused, rather than assisted, the Soviet effort for a nuclear bomb.[35] The designers of early thermonuclear bombs envisioned using an atomic bomb as a trigger to provide the needed heat and compression to initiate the thermonuclear reaction in a layer of liquid deuterium between the fissile material and the surrounding chemical high explosive.[36] The group would realize that a lack of sufficient heat and compression of the deuterium would result in an insignificant fusion of the deuterium fuel.[36]\\r\\n\\r\\nAndrei Sakharov's study group at FIAN in 1948 came up with a second concept which was adding a shell of natural, unenriched uranium around the deuterium would increase the deuterium concentration at the uranium-deuterium boundary and the overall yield of the device, because the natural uranium would capture neutrons and itself fission as part of the thermonuclear reaction. This idea of a layered fission-fusion-fission bomb led Sakharov to call it the sloika, or layered cake.[36] It was also known as the RDS-6S, or Second Idea Bomb.[37] This second bomb idea was not a fully evolved thermonuclear bomb in the contemporary sense, but a crucial step between pure fission bombs and the thermonuclear \\"supers.\\"[38] Due to the three-year lag in making the key breakthrough of radiation compression from the United States the Soviet Union's development efforts followed a different course of action. In the United States they decided to skip the single-stage fusion bomb and make a two-stage fusion bomb as their main effort.[36][39] Unlike the Soviet Union, the analog RDS-7 advanced fission bomb was not further developed, and instead, the single-stage 400-kiloton RDS-6S was the Soviet's bomb of choice.[36]  \\r\\nThe RDS-6S Layer Cake design was detonated on 12 August 1953, producing a yield of 400 kilotons, about ten times more powerful than any previous Soviet test. Around this time the United States detonated its first super using radiation compression on 1 November 1952, code-named Mike. Though the Mike was about twenty times greater than the RDS-6S, it was not a design that was practical to use, unlike the RDS-6S.[36]\\r\\n\\r\\nFollowing the successful launching of the RDS-6S, Sakharov proposed an upgraded version called RDS-6SD.[36] This bomb was proved to be faulty, and it was neither built nor tested. The Soviet team had been working on the RDS-6T concept, but it also proved to be a dead end. \\r\\nIn 1954, Sakharov worked on a third concept, a two-stage thermonuclear bomb.[36] The third idea used the radiation wave of a fission bomb, not simply heat and compression, to ignite the fusion reaction, and paralleled the discovery made by Ulam and Teller. Unlike the RDS-6S boosted bomb, which placed the fusion fuel inside the primary A-bomb trigger, the thermonuclear super placed the fusion fuel in a secondary structure a small distance from the A-bomb trigger, where it was compressed and ignited by the A-bomb's x-ray radiation.[36] The KB-11 Scientific-Technical Council approved plans to proceed with the design on 24 December 1954. Technical specifications for the new bomb were completed on 3 February 1955, and it was designated the RDS-37.[36]\\r\\n\\r\\nThe RDS-37 was successfully tested on 22 November 1955 with a yield of 1.6 megaton. The yield was almost a hundred times greater than the first Soviet atomic bomb six years before, showing that the Soviet Union could compete with the United States.[36][40]\\r\\n\\r\\nThe single largest problem during the early Soviet project was the procurement of uranium ore, as the USSR had limited domestic sources at the beginning of the project.  The era of domestic uranium mining can be dated exactly, to November 27, 1942, the date of a directive issued by the all-powerful wartime State Defense Committee.  The first Soviet uranium mine was established in Taboshar, present-day Tajikistan, and was producing at an annual rate of a few tons of uranium concentrate by May 1943.[41]  Taboshar was the first of many officially secret Soviet closed cities related to uranium mining and production.[42]\\r\\n\\r\\nDemand from the experimental bomb project was far higher.  The Americans, with the help of Belgian businessman Edgar Sengier in 1940,  had already blocked access to known sources in Congo, South Africa, and Canada.  In December 1944 Stalin took the uranium project away from Vyacheslav Molotov and gave to it to Lavrentiy Beria.  The first Soviet uranium processing plant was established as the Leninabad Mining and Chemical Combine in Chkalovsk (present-day Buston, Ghafurov District), Tajikistan, and new production sites identified in relative proximity.  This posed a need for labor, a need that Beria would fill with forced labor:  tens of thousands of Gulag prisoners were brought to work in the mines, the processing plants, and related construction.\\r\\n\\r\\nDomestic production was still insufficient when the Soviet F-1 reactor, which began operation in December 1946, was fueled using uranium confiscated from the remains of the German atomic bomb project. This uranium had been mined in the Belgian Congo, and the ore in Belgium fell into the hands of the Germans after their invasion and occupation of Belgium in 1940.\\r\\n\\r\\nFurther sources of uranium in the early years of the program were mines in East Germany (via the deceptively-named SAG Wismut), Czechoslovakia, Bulgaria, Romania (near Stei) and Poland. Boris Pregel sold 0.23 tonnes of uranium oxide to the Soviet Union during the war, with the authorisation of the U.S. Government.[43][44][45]\\r\\n\\r\\nEventually, large domestic sources were discovered in the Soviet Union (including those now in Kazakhstan).\\r\\n\\r\\nThe uranium for the Soviet nuclear weapons program came from mine production in the following countries,[46]\\r\\n\\r\\nRDS-1, the first Soviet atomic test was internally code-named  First Lightning (楿־ 񶶷, or Pervaya Molniya) August 29, 1949, and was code-named by the Americans as Joe 1. The design was very similar to the first US \\"Fat Man\\" plutonium bomb, using a TNT/hexogen implosion lens design.\\r\\n\\r\\nOn September 24, 1951, the 38.3 kiloton device RDS-2 was tested based on a tritium \\"boosted\\" uranium implosion device with a levitated core.[47]  This test was code named Joe 2 by the CIA.\\r\\n\\r\\nRDS-3 was the third Soviet atomic bomb. On October 18, 1951, the 41.2 kiloton device was detonated - a boosted weapon using a composite construction of levitated plutonium core and a uranium-235 shell. Code named Joe 3 in the USA, this was the first Soviet air-dropped bomb test. Released at an altitude of 10?km, it detonated 400 meters above the ground.\\r\\n\\r\\nRDS-4 represented a branch of research on small tactical weapons. It was a boosted fission device using plutonium in a \\"levitated\\" core design. The first test was an air drop on August 23, 1953, yielding 28 kilotons. In 1954, the bomb was also used during Snowball exercise in Totskoye, dropped by Tu-4 bomber on the simulated battlefield, in the presence of 40,000 infantry, tanks, and jet fighters. The RDS-4 comprised the warhead of the R-5M, the first medium-range ballistic missile in the world, which was tested with a live warhead for the first and only time on February 5, 1956\\r\\n\\r\\nRDS-5 was a small plutonium based device, probably using a hollow core. Two different versions were made and tested.\\r\\n\\r\\nRDS-6, the first Soviet test of a hydrogen bomb, took place on August 12, 1953, and was nicknamed Joe 4 by the Americans. It used a layer-cake design of fission and fusion fuels (uranium 235 and lithium-6 deuteride) and produced a yield of 400 kilotons. This yield was about ten times more powerful than any previous Soviet test.[36] When developing higher level bombs, the Soviets proceeded with the RDS-6 as their main effort instead of the analog RDS-7 advanced fission bomb. This led to the third idea bomb which is the RDS-37.[36]\\r\\n\\r\\nA much lower-power version of the RDS-4 with a 3-10 kiloton yield, the RDS-9 was developed for the T-5 nuclear torpedo. A 3.5 kiloton underwater test was performed with the torpedo on September 21, 1955.\\r\\n\\r\\nThe first Soviet test of a \\"true\\" hydrogen bomb in the megaton range was conducted on November 22, 1955. It was dubbed RDS-37 by the Soviets. It was of the multi-staged, radiation implosion thermonuclear design called Sakharov's \\"Third Idea\\" in the USSR and the Teller-Ulam design in the USA.[48]\\r\\n\\r\\nJoe 1, Joe 4, and RDS-37 were all tested at the Semipalatinsk Test Site in Kazakhstan.\\r\\n\\r\\nThe Tsar Bomba (ђ^-ɾ񼼼) was the largest, most powerful thermonuclear weapon ever detonated. It was a three-stage hydrogen bomb with a yield of about 50 megatons.[49] This is equivalent to ten times the amount of all the explosives used in World War II combined.[50] It was detonated on October 30, 1961, in the Novaya Zemlya archipelago, and was capable of approximately 100 megatons, but was purposely reduced shortly before the launch. Although weaponized, it was not entered into service; it was simply a demonstrative testing of the capabilities of the Soviet Union's military technology at that time. The heat of the explosion was estimated to potentially inflict third degree burns at 100?km distance of clear air.[51]\\r\\n\\r\\nChagan was a shot in the Nuclear Explosions for the National Economy or Project 7, the Soviet equivalent of the US Operation Plowshare to investigate peaceful uses of nuclear weapons. It was a subsurface detonation. It was fired on January 15, 1965. The site was a dry bed of the Chagan River at the edge of the Semipalatinsk Test Site, and was chosen such that the lip of the crater would dam the river during its high spring flow. The resultant crater had a diameter of 408 meters and was 100 meters deep. A major lake (10,000?m3) soon formed behind the 20ÿ35?m high upraised lip, known as Chagan Lake or Balapan Lake.[citation needed]\\r\\n\\r\\nThe photo is sometimes confused with RDS-1 in literature.\\r\\n\\r\\nDuring the Cold War, the Soviet Union created at least nine closed cities, known as Atomgrads[citation needed], in which nuclear weapons-related research and development took place. After the dissolution of the Soviet Union, all of the cities changed their names (most of the original code-names were simply the oblast and a number). All are still legally \\"closed\\", though some have parts of them accessible to foreign visitors with special permits (Sarov, Snezhinsk, and Zheleznogorsk).\\r\\n\\r\\nThe Soviets started experimenting with nuclear technology in 1943, and first tested a nuclear weapon in August 1949. Many of the fission based devices left behind radioactive isotopes which have contaminated air, water and soil in the areas immediately surrounding, downwind and downstream of the blast site. According to the records that the Russian government released in 1991, the Soviet Union tested 969 nuclear devices between 1949 and 1990.[52]:1 Soviet scientists conducted the tests with little regard for environmental and public health consequences. The detrimental effects that the toxic waste generated by weapons testing and processing of radioactive materials are still felt to this day. Even decades later, the risk of developing various types of cancer, especially that of the thyroid and the lungs, continues to be elevated far above national averages for people in affected areas.[53]:1385 Iodine-131, a radioactive isotope that is a major byproduct of fission-based weapons, is retained in the thyroid gland, and so poisoning of this kind is commonplace in impacted populations.[53]:1386\\r\\n\\r\\nThe Soviets set off 214 nuclear bombs in the open air between 1949 and 1962, when the United Nations banned atmospheric tests worldwide.[52]:6 The billions of radioactive particles released into the air exposed countless people to extremely mutagenic and carcinogenic materials, resulting in a myriad of deleterious genetic maladies and deformities. The majority of these tests took place at the Semipalatinsk Test Site, or STS, located in northeast Kazakhstan.[52]:61 The testing at STS alone exposed hundreds of thousands of Kazakh citizens to the harmful effects, and the site continues to be one of the most highly irradiated places on the planet.[54]:A167 When the earliest tests were being conducted, even the scientists had only a poor understanding of the medium- and long-term effects of radiation exposure. In fact, the STS was chosen as the primary site for open-air testing precisely because the Soviets were curious about the potential for lasting harm that their weapons held.[53]:1389\\r\\n\\r\\nContamination of air and soil due to atmospheric testing is only part of a wider issue. Water contamination due to improper disposal of spent uranium and decay of sunken nuclear-powered submarines is a major problem in the Kola Peninsula in northwest Russia. Although the Russian government states that the radioactive power cores are stable, various scientists have come forth with serious concerns about the 32,000 spent nuclear fuel elements that remain in the sunken vessels.[54]:A166 There have been no major incidents other than the explosion and sinking of a nuclear-powered submarine in August 2000, but many international scientists are still uneasy at the prospect of the hulls eroding, releasing uranium into the sea and causing considerable contamination.[54]:A166 Although the submarines pose an environmental risk, they have yet to cause serious harm to public health. However, water contamination in the area of the Mayak test site, especially at Lake Karachay, is extreme, and has gotten to the point where radioactive byproducts have found their way into drinking water supplies. It has been an area of concern since the early 1950s, when the Soviets began disposing of tens of millions of cubic meters of radioactive waste by pumping it into the small lake.[54]:A165 Half a century later, in the 1990s, there are still hundreds of millions of curies of waste in the Lake, and at points contamination has been so severe that a mere half-hour of exposure to certain regions would deliver a dose of radiation sufficient to kill 50% of humans.[54]:A165 Although the area immediately surrounding the lake is devoid of population, the lake has the potential to dry up in times of drought. Most significantly, in 1967, it dried up and winds carried radioactive dust over thousands of square kilometers, exposing at least 500,000 citizens to a range of health risks.[54]:A165 To control dust, Soviet scientists piled concrete on top of the lake. Although this was effective in helping mediate the amount of dust, the weight of the concrete pushed radioactive materials into closer contact with standing underground groundwater.[54]:A166 It is difficult to gauge the overall health and environmental effects of the water contamination at Lake Karachay because figures on civilian exposure are unavailable, making it hard to show causation between elevated cancer rates and radioactive pollution specifically from the lake.\\r\\n\\r\\nContemporary efforts to manage radioactive contamination in the Soviet Union are few and far between. Public awareness of the past and present dangers, as well as the Russian government's investment in current cleanup efforts, are likely dampened by the lack of media attention STS and other sites have gotten in comparison to isolated nuclear incidents such as Hiroshima, Nagasaki, Chernobyl and Three-Mile Island.[55] The domestic government's investment in cleanup measures seems to be driven by economic concerns rather than care for public health. The most significant political legislation in this area is a bill agreeing to turn the already contaminated former weapons complex Mayak into an international radioactive waste dump, accepting cash from other countries in exchange for taking their radioactive byproducts of nuclear industry.[54]:A167 Although the bill stipulates that the revenue go towards decontaminating other test sites such as Semipalatinsk and the Kona Peninsula, experts doubt whether this will actually happen given the current political and economic climate in Russia.[54]:A168","input":"When did the soviet union test its first atomic bomb?"},{"output":"peachicks","context":"Pavo cristatus\\r\\nPavo muticus\\r\\nAfropavo congensis\\r\\nThe peafowl include three species of birds in the genera Pavo and Afropavo of the Phasianidae family, the pheasants and their allies. There are two Asiatic species: the blue or Indian peafowl originally of India, Sri Lanka, and Pakistan; and the green peafowl of Myanmar, Indochina, and Java; and one African species, the Congo peafowl, native only to the Congo Basin. Male peafowl are known for their piercing call and their extravagant plumage. The latter is especially prominent in the Asiatic species, who have an eye-spotted \\"tail\\" or \\"train\\" of covert feathers which they display as part of a courtship ritual. The term peacock is properly reserved for the male; the female is known as a peahen, and the immature offspring are sometimes called peachicks.[1]\\r\\nThe functions of the elaborate iridescent coloration and large \\"train\\" of peacocks have been the subject of extensive scientific debate. Charles Darwin suggested they served to attract females, and the showy features of the males had evolved by sexual selection. More recently, Amotz Zahavi proposed in his handicap theory that these features acted as honest signals of the males' fitness, since less fit males would be disadvantaged by the difficulty of surviving with such large and conspicuous structures.\\r\\n\\r\\n\\r\\nThe Indian peacock has iridescent blue and green plumage, mostly metallic blue and green. But the green peacock has green and bronze body feathers. In both species females are as big as males but lack the train and the head ornament. [2] The peacock \\"tail,\\" known as a \\"train,\\" consists not of tail quill feathers, but highly elongated upper tail coverts. These feathers are marked with eyespots, best seen when a peacock fans his tail. Both sexes of all species have a crest atop the head. The Indian peahen has a mixture of dull grey, brown, and green in her plumage. The female also displays her plumage to ward off female competition or signal danger to her young.\\r\\nThe green peafowl differs from the Indian peafowl in that the male has green and gold plumage and black wings with a sheen of blue. Unlike the Indian peafowl, the green peahen is similar to the male, only having shorter upper tail coverts, a more coppery neck, and overall less iridescence.\\r\\nThe Congo peacock male does not display his covert feathers, but uses his actual tail feathers during courtship displays. These feathers are much shorter than those of the Indian and green species, and the ocelli are much less pronounced. Females of the Indian and African species are dull grey and/or brown.\\r\\nChicks of both sexes in all the species are cryptically coloured. They vary between yellow and tawny, usually with patches of darker brown or light tan and \\"dirty white\\" ivory.\\r\\nOccasionally, peafowl appear with white plumage. Although albino peafowl do exist, this is quite rare, and almost all white peafowl are not, in fact, albinos; they have a different condition called leucism, which causes an overall reduction in different types of pigment. This can result in the complete lack of coloration of their plumage, while preserving normal eye colour. By contrast, true albino peafowl have a complete lack of melanin, resulting in the albino's characteristic red or pink eyes. Leucistic peachicks are born yellow and become fully white as they mature.\\r\\nAs with many birds, vibrant iridescent plumage colours are not primarily pigments, but structural colouration. Optical interference Bragg reflections based on regular, periodic nanostructures of the barbules (fiber-like components) of the feathers produce the peacock's colours. Slight changes to the spacing of these barbules result in different colours. Brown feathers are a mixture of red and blue: one colour is created by the periodic structure and the other is created by a FabryÿProt interference peak from reflections from the outer and inner boundaries. Such structural coloration causes the iridescence of the peacock's hues. Interference effects depend on light angle rather than actual pigments.[3]\\r\\nCharles Darwin suggested in On the Origin of Species that the peafowl's plumage had evolved through sexual selection. He expanded upon this in his second book, The Descent of Man and Selection in Relation to Sex.\\r\\nThe sexual struggle is of two kinds; in the one it is between individuals of the same sex, generally the males, in order to drive away or kill their rivals, the females remaining passive; whilst in the other, the struggle is likewise between the individuals of the same sex, in order to excite or charm those of the opposite sex, generally the females, which no longer remain passive, but select the more agreeable partners.[4]\\r\\nSexual selection is the ability of male and female organisms to exert selective forces on each other with regard to mating activity.[5] The strongest driver of sexual selection is gamete size. In general, eggs are bigger than sperm and females produce fewer gametes than males. This leads to eggs being a bigger investment, and therefore to females being choosy about the traits that will be passed on to her offspring by males. The peahen's reproductive success and the likelihood of survival of her chicks is partly dependent on the genotype of the mate.[6] Females generally have more to lose when mating with an inferior male due to her gametes being more costly than the male's.\\r\\nMultiple hypotheses attempt to explain the evolution of female choice. Some of these suggest direct benefits to females, such as protection, shelter, or nuptial gifts that sway the female's choice of mate. Another hypothesis is that females choose mates with good genes. Males with more exaggerated secondary sexual characteristics, such as bigger, brighter peacock trains, tend to have better genes in the peahen's eyes.[7] These better genes will directly benefit her offspring, as well as her fitness and reproductive success. Runaway selection also seeks to clarify the evolution of the peacock's train. In runaway sexual selection, linked genes in males and females code for sexually dimorphic traits in males, and preference for those traits in females.[8] The close spatial association of alleles for loci involved in the train in males, and for preference for more exuberant trains in females, on the chromosome (linkage disequilibrium) causes a positive feedback loop that exaggerates both the male traits and the female preferences. Another hypothesis is sensory bias, in which females have a preference for a trait in a non-mating context that becomes transferred to mating. Multiple causality for the evolution of female choice is also possible.\\r\\nWork concerning female behaviour in many species of animals has sought to confirm Darwin's basic idea of female preference for males with certain characteristics as a major force in the evolution of species.[9] Females have often been shown to distinguish small differences among potential mates, and to prefer mating with individuals bearing the most exaggerated characters.[10] In some cases, those males have been shown to be more healthy and vigorous, suggesting that the ornaments serve as markers indicating the males' abilities to survive and, thus, their genetic qualities.\\r\\nThe peacock's train and iridescent plumage are perhaps the best-known example of traits believed to have arisen through sexual selection, though with some controversy.[11] Male peafowl erect their trains to form a shimmering fan in their display to females. Marion Petrie tested whether or not these displays signalled a male's genetic quality by studying a feral population of peafowl in Whipsnade Wildlife Park in southern England. The number of eyespots in the train predicted a male's mating success. She was able to manipulate this success by cutting the eyespots off some of the males' tails:[12] females lost interest in pruned males and became attracted to untrimmed ones. Males with fewer eyespots, and thus with lower mating success, suffered from greater predation.[13] She allowed females to mate with males with differing numbers of eyespots, and reared the offspring in a communal incubator to control for differences in maternal care. Chicks fathered by more ornamented males weighed more than those fathered by less ornamented males, an attribute generally associated with better survival rate in birds. These chicks were released into the park and recaptured one year later. Those with heavily ornamented feathers were better able to avoid predators and survive in natural conditions.[9] Thus, Petrie's work has shown correlations between tail ornamentation, mating success, and increased survival ability in both the ornamented males and their offspring.\\r\\nFurthermore, peafowl and their sexual characteristics have been used in the discussion of the causes for sexual traits. Amotz Zahavi used the excessive tail plumes of male peafowls as evidence for his \\"Handicap Principle\\".[14] Since these trains are likely to be deleterious to the survival of an individual (as the brilliant plumes are visible to predators and the longer plumes make escape from danger more difficult), Zahavi argued that only the fittest males could survive the handicap of a large train. Thus, a brilliant train serves as an honest indicator for females that these highly ornamented males are good at surviving for other reasons, and are therefore preferable mates.[15] This theory may be contrasted with Ronald Fisher's theory (and Darwin's hypothesis) that male sexual traits are the result of initially arbitrary aesthetic selection by females.\\r\\nIn contrast to Petrie's findings, a seven-year Japanese study of free-ranging peafowl concluded that female peafowl do not select mates solely on the basis of their trains. Mariko Takahashi found no evidence that peahens preferred peacocks with more elaborate trains (such as with more eyespots), a more symmetrical arrangement, or a greater length.[16] Takahashi determined that the peacock's train was not the universal target of female mate choice, showed little variance across male populations, and did not correlate with male physiological condition. Adeline Loyau and her colleagues responded that alternative and possibly central explanations for these results had been overlooked.[17] They concluded that female choice might indeed vary in different ecological conditions.\\r\\nMerle Jacobs' food-courtship theory states that peahens are attracted to peacocks for the resemblance of their eye spots to blue berries.[18]\\r\\nIt has been suggested that a peacock's train, loud call, and fearless behaviour have been formed by natural selection (not sexual selection), and served as an aposematic display to intimidate predators and rivals.[19]\\r\\nA peacock's copulation success rate depends on the colours of his eyespots (ocelli) and the angle at which they are displayed. The angle at which the ocelli are displayed during courtship is more important in a peahen's choice of males than train size or number of ocelli.[20] Peahens pay careful attention to the different parts of a peacock's train during his display. The lower train is usually evaluated during close-up courtship, while the upper train is more of a long-distance attraction signal. Actions such as train rattling and wing shaking also kept the peahens' attention.[21]\\r\\nAlthough an intricate display catches a peahen's attention, the redundant signal hypothesis also plays a crucial role in keeping this attention on the peacock's display. The redundant signal hypothesis explains that whilst each signal that a male projects is about the same quality, the addition of multiple signals enhances the reliability of that mate. This idea also suggests that the success of multiple signalling is not only due to the repetitiveness of the signal, but also of multiple receivers of the signal. In the peacock species, males congregate a communal display during breeding season and the peahens observe. Peacocks first defend their territory through intra-sexual behaviour, defending their areas from intruders. They fight for areas within the congregation to display a strong front for the peahens. Central positions are usually taken by older, dominant males, which influences mating success. Certain morphological and behavioural traits come in to play during inter and intra-sexual selection, which include train length for territory acquisition and visual and vocal displays involved in mate choice by peahens.[22]\\r\\nIn courtship, vocalisation stands to be a primary way for peacocks to attract peahens. Some studies suggest that the intricacy of the \\"song\\" produced by displaying peacocks proved to be impressive to peafowl. Singing in peacocks usually occurs just before, just after, or sometimes during copulation.[23]\\r\\nPeafowl are forest birds that nest on the ground, but roost in trees. They are terrestrial feeders. All species of peafowl are believed to be polygamous. In common with other members of the Galliformes, the males possess metatarsal spurs or \\"thorns\\" on their legs used during intraspecific territorial fights with other members of their kind.\\r\\nPeafowl are omnivores and eat mostly plant parts, flower petals, seed heads, insects and other arthropods, reptiles, and amphibians. Wild peafowl look for their food scratching around in leaf litter either early in the morning or at dusk. They retreat to the shade and security of the woods for the hottest portion of the day. These birds are not picky and will eat almost anything they can fit in their beak and digest. They actively hunt insects like ants, crickets and termites; millipedes; and other arthropods and small mammals.[24] Indian peafowl also eat small snakes.[25]\\r\\nDomesticated peafowl may also eat bread and cracked grain such as oats and corn, cheese, cooked rice and sometimes cat food. It is noticed by keepers that Peafowl enjoy protein rich food including larvae that infest granaries, different kinds of meat and fruit, as well as vegetables including dark leafy greens, broccoli, carrots, beans, beets, and peas.[26]\\r\\nIn Hinduism, the Indian peacock is the mount of the Lord Kartikeya, the god of war. A demon king, Surapadman, was split into two by Karthikeya and the merciful lord converted the two parts as an integral part of himself, one becoming a peacock (his mount) and another a rooster adorning his flag. The peacock displays the divine shape of Omkara when it spreads its magnificent plumes into a full-blown circular form.[28] Peacock feathers also adorn the crest of Lord Krishna, an avatar of Lord Vishnu, one of the trimurti. In the Sinhalese zodiac, peacock is the third animal zodiac of the Sinhalese people of Sri Lanka.[29] Peacocks (often a symbol of pride and vanity) were believed to deliberately consume poisonous substances in order to become immune to them, as well as to make the colours of their resplendent plumage all the more vibrant - seeing as so many poisonous flora and fauna are so colourful this idea appears to have merit. The Buddhist deity Mahamayuri is depicted seated on a peacock. Peacocks are seen supporting the throne of Amitabha, the ruby red sunset coloured archetypal Buddha of Infinite Light.\\r\\nAncient Greeks believed that the flesh of peafowl did not decay after death, so it became a symbol of immortality. This symbolism was adopted by early Christianity, thus many early Christian paintings and mosaics show the peacock. The peacock is still used in the Easter season, especially in the east.[30] The 'eyes' in the peacock's tail feathers symbolise the all-seeing Christian God and ÿ in some interpretations ÿ the Church. A peacock drinking from a vase is used as a symbol of a Christian believer drinking from the waters of eternal life. The peacock can also symbolise the cosmos if one interprets its tail with its many 'eyes' as the vault of heaven dotted by the sun, moon, and stars. By Christian adoption of old Persian and Babylonian symbolism, in which the peacock was associated with Paradise and the Tree of Life, the bird is again associated with immortality. In Christian iconography, the peacock is often depicted next to the Tree of Life.[citation needed]\\r\\nThough the peafowl is native to India, in Babylonia and Persia the peacock is seen as a guardian to royalty, and is often seen in engravings upon the thrones of royalty. Nonetheless, using the peacock as the symbol of royalty has an old and distinguished pedigree in India too. Peacocks were believed to be immune to poison, even deliberately consuming poisonous substances which made the resplendent colours of their plumage all the more vibrant. The Buddhist \\"Goddess\\" Mahamayuri is depicted with a peacock as her vehicle. The archetypal Buddha Amitabha, the ruby red sunset coloured Buddha of Infinite Light has peacocks adorning his throne. The first great dynasty unifying the Indian sub-continent in the 3rd century BCE were known as the \\"Maurya\\", lit. \\"of the peacock\\", named after the patriarch Chandragupta Maurya. The word \\"Maurya\\" is derived from Sanskrit \\"Mayura\\" (lit. peacock). The monarchy in Iran is referred to as the Peacock Throne.\\r\\nMelek Taus (Arabic: ????? ????; Persian: ??? ??????; Kurdish: Taw?s Melek?), the \\"Peacock Angel\\", is the Yazidi name for the central figure of their faith. The Yazidi consider Taw?s Melek an emanation of God and a benevolent angel who has redeemed himself from his fall and has become a demiurge who created the cosmos from the cosmic egg. After he repented, he wept for 7,000 years, his tears filling seven jars, which then quenched the fires of hell. In art and sculpture, Taw?s Melek is depicted as a peacock.[31]\\r\\nIn Hellenistic imagery, the Greek goddess Hera's chariot was pulled by peacocks, birds not known to Greeks before the conquests of Alexander. Alexander's tutor, Aristotle, refers to it as \\"the Persian bird\\". One myth states that Hera's servant, the hundred-eyed Argus Panoptes, was instructed to guard the woman-turned-cow, Io. Hera had transformed Io into a cow after learning of Zeus's interest in her. Zeus had the messenger of the gods, Hermes, kill Argus through eternal sleep and free Io. According to Ovid, to commemorate her faithful watchman, Hera had the hundred eyes of Argus preserved forever, in the peacock's tail.[32]\\r\\nAmong Ashkenazi Jews, the golden peacock is a symbol for joy and creativity, with quills from the bird's feathers being a metaphor for a writer's inspiration.[33]\\r\\nThe peacock motif was revived in the Renaissance iconography that unified Hera and Juno, and on which European painters focused.[34]\\r\\nIn 1956, John J. Graham created an abstraction of an 11-feathered peacock logo for American broadcaster NBC. This brightly hued peacock was adopted due to the increase in colour programming. NBC's first colour broadcasts showed only a still frame of the colourful peacock. The emblem made its first on-air appearance on 22 May 1956.[35] The current peacock logo, which has six feathers, debuted in 1986. A stylised peacock in full display is the logo for the Pakistan Television Corporation.\\r\\nIn some cultures, the peacock is a symbol of pride or vanity, due to the way the bird struts and shows off its plumage.\\r\\nLord Kartikeya with his wives in his peacock mount\\r\\n\\"Peacock\\" by Merab Abramishvili\\r\\nIn the 1486 painting Annunciation with St. Emidius by Carlo Crivelli, a peacock is sitting on the roof above the praying Virgin Mary.\\r\\nA peacock served in full plumage (detail of the Allegory of Taste, Hearing and Touch by Jan Brueghel the Elder, 1618)\\r\\nPainting by Abbott Thayer and Richard Meryman for Thayer's 1909 book, wrongly suggesting that the peacock's plumage was camouflage\\r\\nCommon Pea Fowl, John Gould, c.1880 Brooklyn Museum\\r\\nSyrian Bowl with Peacock Motif, c. 1200 Brooklyn Museum\\r\\nDuring the Medieval period, various types of fowl were consumed as food, with the poorer populations (such as serfs) consuming more common birds, such as chicken. However, the more wealthy gentry were privileged to less usual foods, such as swan, and even peafowl were consumed. On a king's table, a peacock would be for ostentatious display as much as for culinary consumption.[36]","input":"What is the young one of peacock called?"},{"output":"approximately 7,000","context":"An astronomer is a scientist in the field of astronomy who concentrates their studies on a specific question or field outside the scope of Earth. They look at stars, planets, moons, comets and galaxies, as well as many other celestial objects  either in observational astronomy, in analyzing the data or in theoretical astronomy. Examples of topics or fields astronomers work on include: planetary science, solar astronomy, the origin or evolution of stars, or the formation of galaxies. There are also related but distinct subjects like physical cosmology which studies the Universe as a whole.\\r\\nAstronomers usually fit into two types: observational and theoretical. Observational astronomers make direct observations of planets, stars and galaxies, and analyze the data. In contrast, theoretical astronomers create and investigate models of things that cannot be observed. Because it takes millions to billions of years for a system of stars or a galaxy to complete a life cycle, astronomers have to observe snapshots of different systems at unique points in their evolution to determine how they form, evolve and die. They use this data to create models or simulations to theorize how different celestial bodies work.\\r\\nThere are further subcategories inside these two main branches of astronomy such as planetary astronomy, galactic astronomy or physical cosmology.\\r\\n\\r\\n\\r\\nHistorically, astronomy was more concerned with the classification and description of phenomena in the sky, while astrophysics attempted to explain these phenomena and the differences between them using physical laws. Today, that distinction has mostly disappeared and the terms \\"astronomer\\" and \\"astrophysicist\\" are interchangeable. Professional astronomers are highly educated individuals who typically have a Ph.D. in physics or astronomy and are employed by research institutions or universities.[1] They spend the majority of their time working on research, although they quite often have other duties such as teaching, building instruments, or aiding in the operation of an observatory.\\r\\nThe number of professional astronomers in the United States is actually quite small. The American Astronomical Society, which is the major organization of professional astronomers in North America, has approximately 7,000 members. This number includes scientists from other fields such as physics, geology, and engineering, whose research interests are closely related to astronomy.[2] The International Astronomical Union comprises almost 10,145 members from 70 different countries who are involved in astronomical research at the Ph.D. level and beyond.[3]\\r\\nContrary to the classical image of an old astronomer peering through a telescope through the dark hours of the night, it is far more common to use a charge-coupled device (CCD) camera to record a long, deep exposure, allowing a more sensitive image to be created because the light is added over time. Before CCDs, photographic plates were a common method of observation. Modern astronomers spend relatively little time at telescopes usually just a few weeks per year. Analysis of observed phenomena, along with making predictions as to the causes of what they observe, takes the majority of observational astronomers' time.\\r\\nAstronomers who serve as faculty spend much of their time teaching undergraduate and graduate classes. Most universities also have outreach programs including public telescope time and sometimes planetariums as a public service to encourage interest in the field.\\r\\nThose who become astronomers usually have a broad background in maths, sciences and computing in high school. Taking courses that teach how to research, write and present papers are also invaluable. In college/university most astronomers get a Ph.D. in astronomy or physics. Keeping in mind how few astronomers there are it is understood that graduate schools in this field are very competitive, so grades are very important. Students must take a Graduate Record Exam in the United States if they wish to be accepted into a US graduate school.\\r\\nWhile there is a relatively low number of professional astronomers, the field is popular among amateurs. Most cities have amateur astronomy clubs that meet on a regular basis and often host star parties. The Astronomical Society of the Pacific is the largest general astronomical society in the world, comprising both professional and amateur astronomers as well as educators from 70 different nations.[4] Like any hobby, most people who think of themselves as amateur astronomers may devote a few hours a month to stargazing and reading the latest developments in research. However, amateurs span the range from so-called \\"armchair astronomers\\" to the very ambitious, who own science-grade telescopes and instruments with which they are able to make their own discoveries and assist professional astronomers in research.","input":"How many astronomers are there in the united states?"},{"output":"a middle-income, mixed economy and emerging market, with expanding manufacturing, financial, service, communications, technology and entertainment sectors","context":"Nigeria is a middle-income, mixed economy and emerging market, with expanding manufacturing, financial, service, communications, technology and entertainment sectors. It is ranked as the 21st-largest economy in the world in terms of nominal GDP, and the 20th-largest in terms of purchasing power parity. It is the largest economy in Africa; its re-emergent manufacturing sector became the largest on the continent in 2013, and it produces a large proportion of goods and services for the West African subcontinent.[15] In addition, the debt-to-GDP ratio is 11 percent, which is 8 percent below the 2012 ratio.[16]\\r\\nPreviously hindered by years of mismanagement, economic reforms of the past decade[when?] have put Nigeria back on track towards achieving its full economic potential. Nigerian GDP at purchasing power parity (PPP) has almost tripled from $170 billion in 2000 to $451 billion in 2012, although estimates of the size of the informal sector (which is not included in official figures) put the actual numbers closer to $630 billion. Correspondingly, the GDP per capita doubled from $1400 per person in 2000 to an estimated $2,800 per person in 2012 (again, with the inclusion of the informal sector, it is estimated that GDP per capita hovers around $3,900 per person). (Population increased from 120 million in 2000 to 160 million in 2010). These figures were to be revised upwards by as much as 80% when metrics were to be recalculated subsequent to the rebasing of its economy in April 2014.[needs update][17]\\r\\nAlthough oil revenues contribute 2/3 of state revenues[18], oil only contributes about 9% to the GDP. Nigeria produces only about 2.7% of the world's oil supply (in comparison, Saudi Arabia produces 12.9%, Russia produces 12.7% and the United States produces 8.6%).[19] Although the petroleum sector is important, as government revenues still heavily rely on this sector, it remains a small part of the country's overall economy.\\r\\nThe largely subsistence agricultural sector has not kept up with rapid population growth, and Nigeria, once a large net exporter of food, now[when?] imports some of its food products, though mechanization has led to a resurgence in manufacturing and exporting of food products, and the move towards food sufficiency. In 2006, Nigeria successfully convinced the Paris Club to let it buy back the bulk of its debts owed to them for a cash payment of roughly US$12 billion.[20]\\r\\nAccording to a Citigroup report published in February 2011, Nigeria will get the highest average GDP growth in the world between 2010 and 2050. Nigeria is one of two countries from Africa among 11 Global Growth Generators countries.[21]\\r\\n\\r\\n\\r\\nIn 2014, Nigeria changed its economic analysis to account for rapidly growing contributors to its GDP, such as telecommunications, banking, and its film industry.[22]\\r\\nIn 2005, Nigeria achieved a milestone agreement with the Paris Club of lending nations to eliminate all of its bilateral external debt. Under the agreement, the lenders will forgive most of the debt, and Nigeria will pay off the remainder with a portion of its energy revenues. Outside of the energy sector, Nigeria's economy is highly inefficient. Moreover, human capital is underdevelopedNigeria ranked 151 out of countries in the United Nations Development Index in 2004and non-energy-related infrastructure is inadequate.\\r\\nFrom 2003 to 2007, Nigeria attempted to implement an economic reform program called the National Economic Empowerment Development Strategy (NEEDS). The purpose of the NEEDS was to raise the country's standard of living through a variety of reforms, including macroeconomic stability, deregulation, liberalization, privatization, transparency, and accountability.\\r\\nThe NEEDS addressed basic deficiencies, such as the lack of freshwater for household use and irrigation, unreliable power supplies, decaying infrastructure, impediments to private enterprise, and corruption. The government hoped that the NEEDS would create 7 million new jobs, diversify the economy, boost non-energy exports, increase industrial capacity utilization, and improve agricultural productivity. A related initiative on the state level is the State Economic Empowerment Development Strategy (SEEDS).\\r\\nA longer-term economic development program is the United Nations (UN)-sponsored National Millennium Goals for Nigeria. Under the program, which covers the years from 2000 to 2015, Nigeria is committed to achieving a wide range of ambitious objectives involving poverty reduction, education, gender equality, health, the environment, and international development cooperation. In an update released in 2004, the UN found that Nigeria was making progress toward achieving several goals but was falling short on others.\\r\\nSpecifically, Nigeria had advanced efforts to provide universal primary education, protect the environment, and develop a global development partnership.\\r\\nA prerequisite for achieving many of these worthwhile objectives is curtailing endemic corruption, which stymies development and taints Nigeria's business environment. President Olusegun Obasanjo's campaign against corruption, which includes the arrest of officials accused of misdeeds and recovering stolen funds, has won praise from the World Bank. In September 2005, Nigeria, with the assistance of the World Bank, began to recover US$458 million of illicit funds that had been deposited in Swiss banks by the late military dictator Sani Abacha, who ruled Nigeria from 1993 to 1998. However, while broad-based progress has been slow, these efforts have begun to become evident in international surveys of corruption. In fact, Nigeria's ranking has consistently improved since 2001 ranking 147 out of 180 countries in Transparency International's 2007 Corruption Perceptions Index.\\r\\nThis is a chart of trend of gross domestic product of Nigeria at market prices estimated[23] by the International Monetary Fund with figures in USD billions. Figures before 2000 are backwards projections from the 2000ÿ2012 numbers, based on historical growth rates, and should be replaced when data becomes available. The figure for 2014 is derived from a rebasing of economical activity earlier in the year.\\r\\nNOTES:\\r\\nThe US dollar exchange rate is an estimated average of the official rate throughout a year, and does not reflect the parallel market rate at which the general population accesses foreign exchange. This rate ranged from a high of 520 in March 2017 to a low of 350 in August 2017, due to a scarcity of forex (oil earnings had dropped by half), and to speculative activity as alleged by the Central Bank. All the while the official rate was pegged at 360.\\r\\nPer capita income (as?% of USA) is calculated using data from estimates in the PPP link above, and from census estimates, based on growth rates between census periods. For instance 2017 GDPs were 1,125 Billion (Nigeria) vs. 19,417 Billion (USA) and populations were estimated at 320 million vs 190 million. The ratio is therefore (1125/19417) / (190/320), which roughly comes to 0.0975. These are estimates and are intended to get a feel for the relative wealth and standard of living, as well as the market potential of its middle class.\\r\\n\\r\\nThis is a chart of trend of the global ranking of the Nigerian economy, in comparison with other countries of the world, derived from the historical List of countries by GDP (PPP).\\r\\n\\r\\nThis chart shows the variance in the parallel exchange rate at which the Dollar can be obtained with Naira in Lagos, with \\"Best\\" being cheaper for a Nigerian (i.e. stronger Naira).[24] [25]\\r\\nFor purchasing power parity comparisons, the US dollar is exchanged at 1 USD to 314.27 Nigerian Naira (as of 2017).[26]\\r\\nCurrent GDP per capita of Nigeria expanded 132% in the sixties reaching a peak growth of 283% in the seventies. But this proved unsustainable and it consequently shrank by 66% in the Eighties. In the Nineties, diversification initiatives finally took effect and decadal growth was restored to 10%.\\r\\nDue to inflation, per capita GDP today remains lower than in 1960 when Nigeria declared independence. About 33% of the population lives on less than US$2 per day.[27]\\r\\nIn 2012, the GDP was composed of the following sectors: agriculture: 40%; services: 30%; manufacturing: 15%; oil: 14%.[3] By 2015, the GDP was composed of the following sectors: agriculture: 18%; services: 55%; manufacturing: 16%; oil: 8% [4]\\r\\nIn 2005 Nigeria's inflation rate was an estimated 15.6%. Nigeria's goal under the National Economic Empowerment Development Strategy (NEEDS) program is to reduce inflation to the single digits. By 2015, Nigeria's inflation stood at 9%. In 2005, the federal government had expenditures of US$13.54 billion but revenues of only US$12.86 billion, resulting in a budget deficit of 5%. By 2012, expenditures stood at $31.61 billion, while revenues was $54.48 billion.\\r\\nNigeria ranks sixth worldwide and first in Africa in farm output. The sector accounts for about 18% of GDP and almost one-third of employment. Nigeria has 19 million head of cattle, the largest in Africa.[29] Though Nigeria is no longer a major exporter, due to local consumer boom, it is still a major producer of many agricultural products, including: cocoa, groundnuts (peanuts), rubber, and palm oil. Cocoa production, mostly from obsolete varieties and overage trees has increased from around 180,000 tons annually to 350,000 tons.\\r\\nMajor agricultural products include cassava (tapioca), corn, cocoa, millet, palm oil, peanuts, rice, rubber, sorghum, and yams. In 2003, livestock production, in order of metric tonnage, featured eggs, milk, beef and veal, poultry, and pork, respectively. In the same year, the total fishing catch was 505.8 metric tons. Roundwood removals totaled slightly less than 70 million cubic meters, and sawnwood production was estimated at 2 million cubic meters. The agricultural sector suffers from extremely low productivity, reflecting reliance on antiquated methods. Agriculture has failed to keep pace with Nigeria's rapid population growth, so that the country, which once exported food, now imports a significant amount of food to sustain itself. However, efforts are being made towards making the country food sufficient again.\\r\\nNigeria's proven oil reserves are estimated to be 35 billion barrels (5.6G109?m3); natural gas reserves are well over 100?trillion cubic feet (2,800?km3). Nigeria is a member of the Organization of Petroleum Exporting Countries (OPEC). The types of crude oil exported by Nigeria are Bonny light oil, Forcados crude oil, Qua Ibo crude oil and Brass River crude oil. Poor corporate relations with indigenous communities, vandalism of oil infrastructure, severe ecological damage, and personal security problems throughout the Niger Delta oil-producing region continue to plague Nigeria's oil sector.\\r\\nEfforts are underway[when?] to reverse these troubles. A new entity, the Niger Delta Development Commission (NDDC), has been created to help catalyze economic and social development in the region. The U.S. remains Nigeria's largest buyer of crude oil, accounting for 40% of the country's total oil exports; Nigeria provides about 10% of overall U.S. oil imports and ranks as the fifth-largest source for U.S. imported oil.\\r\\nThe United Kingdom is Nigeria's largest trading partner followed by the United States. Although the trade balance overwhelmingly favors Nigeria, thanks to oil exports, a large portion of U.S. exports to Nigeria is believed to enter the country outside of the Nigerian government's official statistics, due to importers seeking to avoid Nigeria's tariffs. To counter smuggling and under-invoicing by importers, in May 2001, the Nigerian government instituted a full inspection program for all imports, and enforcement has been sustained.\\r\\nOn the whole, Nigerian high tariffs and non-tariff barriers are gradually being reduced,[when?] but much progress remains to be made.[according to whom?] The government also has been encouraging the expansion of foreign investment, although the country's investment climate remains daunting to all but the most determined.[according to whom?] The stock of U.S. investment is nearly $7 billion, mostly in the energy sector. Exxon Mobil and Chevron are the two largest U.S. corporations in offshore oil and gas production. Significant exports of liquefied natural gas started in late 1999 and are slated to expand as Nigeria seeks to eliminate gas flaring by 2008.\\r\\nThe pump price of P.M.S. currently[when?] stands at around ?145 at fueling stations across Nigeria. An initial increase in the price of petrol (Premium Motor Spirit) from around ?65 to ?140 triggered by the removal of fuel subsidies on January 1, 2012, triggered a total strike and massive protests across the country. Then President Goodluck Ebele Jonathan later reached an agreement with the Nigerian Labour Congress and reduced the pump price to 97 naira. The pump price was further reduced by 10 naira to 87 naira in the run-up to the 2015 general elections. However, after the elections of Muhammadu Buhari, the fuel subsidies was removed again, and the pump price increased again, despite the fall in oil price.\\r\\nSince the fall in oil prices in 2015 and 2016, the government exchange rate policy has limited devaluation of the naira due to inflation concerns by the President Muhammadu Buhari.[30][31]\\r\\nNigeria ranks 27th worldwide and first in Africa in services' output.\\r\\nSince undergoing severe distress in the mid-1990s, Nigeria's banking sector has witnessed significant growth over the last few years[when?] as new banks enter the financial market.\\r\\nPrivate sector-led economic growth remains stymied by the high cost of doing business in Nigeria, including the need to duplicate essential infrastructure, the lack of effective due process, and nontransparent economic decision making, especially in government contracting. While corrupt practices are endemic, they are generally less flagrant than during military rule, and there are signs of improvement. Meanwhile, since 1999 the Nigerian Stock Exchange has enjoyed strong performance, although equity as a means to foster corporate growth is being more utilized by Nigeria's private sector.[according to whom?]\\r\\nNigeria's publicly owned transportation infrastructure is a major constraint to economic development. Principal ports are at Lagos (Apapa and Tin Can Island), Port Harcourt (Onne), and Calabar.\\r\\nExtensive road repairs and new construction activities are gradually being implemented as state governments, in particular, spend their portions of enhanced government revenue allocations.\\r\\nFive of Nigeria's airports (Lagos, Kano, Port Harcourt, Enugu and Abuja) currently ply international destinations. Government-owned Nigeria Airways ceased operations in December 2002. Virgin Nigeria Airways started operations in 2005 as a replacement and serves domestic and international routes. However, Virgin Nigeria also stopped operations in 2012. Also, The Nigerian Airforce began a new airline called United Nigeria, with a Boeing 737-500 in 2013. There are several domestic private Nigerian carriers, and air service among Nigeria's cities is generally dependable.\\r\\nElectricity ÿ production: 18.89 billion kWh (2009)\\r\\nElectricity ÿ production by source:\\r\\nfossil fuel: 61.69%\\r\\nhydro: 38.31%\\r\\nnuclear: 0%\\r\\nother: <.1% (1998)\\r\\nElectricity - consumption: 17.66 billion kWh (2009)\\r\\nElectricity - exports: 40 million kWh (2003)\\r\\nElectricity - imports: 0 kWh (1998)\\r\\nOil - production: 2.35 million barrels per day (374G10^3?m3/d) (July 2006 est.)\\r\\nOil - consumption: 310,000?bbl/d (49,000?m3/d) (2003 est.)\\r\\nA major source of foreign exchange earnings for Nigeria are remittances sent home by Nigerians living abroad.[32] In 2014, 17.5 million Nigerians lived in foreign countries, with the UK and the USA having more than 2 million Nigerians each.[32]\\r\\nAccording to the International Organization for Migration, Nigeria witnessed a dramatic increase in remittances sent home from overseas Nigerians, going from USD 2.3 billion in 2004 to 17.9 billion in 2007, representing 6.7% of GDP. The United States accounts for the largest portion of official remittances, followed by the United Kingdom, Italy, Canada, Spain and France. On the African continent, Egypt, Equatorial Guinea, Chad, Libya and South Africa are important source countries of remittance flows to Nigeria, while China is the biggest remittance-sending country in Asia.\\r\\nIn 2015, Nigeria had a labour force of 74 million. In 2003, the unemployment rate was 10.8% overall; by 2015, unemployment stood at 6.4%.[33]\\r\\nSince 1999, the Nigerian Labor Congress (NLC) a union umbrella organization, has called six general strikes to protest domestic fuel price increases. However, in March 2005 the government introduced legislation ending the NLC's monopoly over union organizing. In December 2005, the NLC was lobbying for an increase in the minimum wage for federal workers. The existing minimum wage, which was introduced six years earlier but has not been adjusted since, has been whittled away by inflation to only US$42.80 per month.\\r\\nAccording to the International Organization for Migration, the number of immigrants residing in Nigeria has more than doubled in recent decades ÿ from 477,135 in 1991 to 971,450 in 2005. The majority of immigrants in Nigeria (74%) are from neighbouring Economic Community of West African States (ECOWAS), and that this number has increased considerably over the last decade, from 63% in 2001 to 97% in 2005.\\r\\nThe Human Development Index (HDI) shows in 2012 that Nigeria is ranked 156 with the value of 0.459 among 187 countries. As of 2015, Nigeria's HDI is ranked 152nd at 0.514. The comparative value for Sub-Saharan Africa is 0.475, 0.910 for the US,[34] and 0.694 for the world average.\\r\\nThe value for the education index is 0.457, compared to the average in the US of 0.939. The expected years of schooling in Nigeria is 9.0 (16.00 in the US), while the mean years of schooling for adults over 25 years is 5.2 years (12.4 years in the US). Additionally, Nigeria is also facing a relatively high inequality, worsening the problem regarding the formation of human capital. The income distribution for the poorest (bottom 10%) is 1.6% while it is 40.8% for the richest (top 10%). Among 114 countries the income distribution places Nigeria respectively in 94th position for the poorest and 17th for the richest.[citation needed]\\r\\nIn the light of highly expansionary public sector fiscal policies in 2001, the government sought ways to head off higher inflation, leading to the implementation of stronger monetary policies by the Central Bank of Nigeria (CBN) and underspending of budgeted amounts. As a result of the CBN's efforts, the official exchange rate for the Naira has stabilized at about 112 Naira to the dollar. The combination of CBN's efforts to prop up the value of the Naira and excess liquidity resulting from government spending led the currency to be discounted by around 20% on the parallel (non-official) market.\\r\\nA key condition of the Stand-by Arrangement has been closure of the gap between the official and parallel market exchange rates. The Inter Bank Foreign Exchange Market (IFEM) is closely tied to the official rate. Under IFEM, banks, oil companies, and the CBN can buy or sell their foreign exchange at government-influenced rates. Much of the informal economy, however, can only access foreign exchange through the parallel market. Companies can hold domiciliary accounts in private banks, and account holders have unfettered use of the funds.\\r\\nExpanded government spending also has led to upward pressure on consumer prices. Inflation which had almost disappeared in April 2000 reached 14.5% by the end of the year and 18.7% in August 2001. In 2000, high oil prices resulted in government revenue of over $16 billion, about double the 1999 level. State and local governments demanded access to this \\"windfall\\" revenue, creating a tug-of-war between the federal government, which sought to control spending, and state governments desiring augmented budgets, preventing the government from making provision for periods of lower oil prices.\\r\\nIn 2016, the black market exchange rate of the Naira was about 60% above the official rate. The central bank releases about $200 million each week at the official exchange rate. However, some companies cite that budgets now include a 30% premium to be paid to central bank officials to get dollars.[30]\\r\\nThe Obasanjo government supported \\"private-sector\\" led, \\"market oriented\\" economic growth and began extensive economic reform efforts. Although the government's anti-corruption campaign was left wanting, progress in injecting transparency and accountability into economic decision-making was notable. The dual exchange rate mechanism formally abolished in the 1999 budget remains in place in actuality.\\r\\nDuring 2000 the government's privatization program showed signs of life and real promise with successful turnover to the private sector of state-owned banks, fuel distribution companies, and cement plants. However, the privatization process has slowed somewhat as the government confronts key parastatals such as the state telephone company NITEL and Nigerian Airways. The successful auction of GSM telecommunications licenses in January 2001 has encouraged investment in this vital sector.\\r\\nAlthough the government has been stymied so far in its desire to deregulate downstream petroleum prices, state refineries, almost paralyzed in 2000, are producing at much higher capacities. By August 2001, gasoline lines disappeared throughout much of the country. The government still intends to pursue deregulation despite significant internal opposition, particularly from the Nigeria Labour Congress. To meet market demand the government incurs large losses importing gasoline to sell at subsidized prices.\\r\\nNigeria's foreign economic relations revolve around its role in supplying the world economy with oil and natural gas, even as the country seeks to diversify its exports, harmonize tariffs in line with a potential customs union sought by the Economic Community of West African States (ECOWAS), and encourage inflows of foreign portfolio and direct investment. In October 2005, Nigeria implemented the ECOWAS common external tariff, which reduced the number of tariff bands.\\r\\nPrior to this revision, tariffs constituted Nigeria's second largest source of revenue after oil exports. In 2005 Nigeria achieved a major breakthrough when it reached an agreement with the Paris Club to eliminate its bilateral debt through a combination of write-downs and buybacks. Nigeria joined the Organization of the Petroleum Exporting Countries in July 1971 and the World Trade Organization in January 1995.\\r\\nIn 2005, Nigeria imported about US$26 billion of goods. In 2004 the leading sources of imports were China (9.4%), the United States (8.4%), the United Kingdom (7.8%), the Netherlands (5.9%), France (5.4%), Germany (4.8%), and Italy (4%). Principal imports were manufactured goods, machinery and transport equipment, chemicals, and food and live animals.\\r\\nIn 2005, Nigeria exported about US$52 billion of goods. In 2004, the leading destinations for exports were the United States (47.4%), Brazil (10.7%), and Spain (7.1%). In 2004 oil accounted for 95% of merchandise exports, and cocoa and rubber accounted for almost 60% of the remainder.\\r\\nIn 2005, Nigeria posted a US$26 billion trade surplus, corresponding to almost 20% of gross domestic product. In 2005, Nigeria achieved a positive current account balance of US$9.6 billion. The Nigerian currency is the naira (NGN). As of mid-June 2006, the exchange rate was about US$1=NGN128.4. In recent years, Nigeria has expanded its trade relations with other developing countries such as India. Nigeria is the largest African crude oil supplier to India  it annually exports 400,000 barrels per day (64,000?m3/d) to India valued at US$10 billion annually.\\r\\nIndia is the largest purchaser of Nigeria's oil which fulfills 20% to 25% of India's domestic oil demand. Indian oil companies are also involved in oil drilling operations in Nigeria and have plans to set up refineries there.[35]\\r\\nThe trade volume between Nigeria and the United Kingdom rose by 35% from USD6.3 billion in 2010 to USD8.5 billion in 2011.[36]\\r\\nIn 2012, Nigeria's external debt was an estimated $5.9 billion and N5.6 trillion domestic - putting total debt at $44 billion.[37]\\r\\nIn April 2006, Nigeria became the first African country to fully pay off its debt owed to the Paris Club. This was structured as a debt writeoff of approximately $18 billion and a cash payment of approximately $12 billion.\\r\\nIn 2012, Nigeria received a net inflow of US$85.73 billion of foreign direct investment (FDI), much of which came from Nigerians in the diaspora. Most FDI is directed toward the energy and banking sectors. Any public designed to encourage inflow of foreign capital is capable of generating employment opportunities within the domestic economy. The Nigerian Enterprises Promotion (NEP) Decree of 1972 (revised in 1977) was intended to reduce foreign investment in the Nigerian economy. This type of policy is not relevant in an economy with a rapidly growing force like Nigeria.\\r\\nAlthough one may accept the rationale for the promulgation of that decree at that time i.e. to promote indigenous entrepreneurship. But the decree or any exchange control policy that has the potential to discourage foreign investment will not be relevant under the present economic dispensations. The abrogation of the NEP decree was therefore a step in the right direction.\\r\\nFurthermore, another reason for the low level of foreign investment in Nigeria is political instability. The various coups and counter coups since 1966, the discontentment and politically motivated riots following the long-drawn and inconclusive political engineering of the Babaginda Military Administration, all combined to create an environment not conducive to foreign investment.\\r\\nForeign direct investment (FDI) is arguably an important source of employment opportunities for developing countries like Nigeria. As a consequence, it is imperative that the federal government promotes a healthy private sector that can earn a reasonable rate of return.\\r\\nDeveloping countries that wish to attract FDI flows should consider measures such as establishing a transparent legal framework that does not discriminate between local and foreign investors; adopting liberal foreign exchange regime (e.g., a regime without large gaps between official and market rates); creating simple,investor-friendly regulations and institutions and effectively administering them.\\r\\nTherefore, the convertibility of naira, the relaxation of the control on remittance of profits and technical fees and the abrogation of the Exchange Control Act of 1962 and the Nigerian Enterprises Promotion Decree of 1989 as spelt out in 1995 Budget are the kind of reforms that can promote the inflow of foreign direct investment a politically stable environment is also of immense importance.\\r\\nAlthough Nigeria must grapple with its decaying infrastructure and a poor regulatory environment, the country possesses many positive attributes for carefully targeted investment and will expand as both a regional and international market player. Profitable niche markets outside the energy sector, like specialized telecommunication providers, have developed under the government's reform program. There is a growing Nigerian consensus that foreign investment is essential to realizing Nigeria's vast but squandered potential. European investments are increasing, especially since Belgian consultancy companies such as Genco are exploring the Nigerian market.\\r\\nCompanies interested in long-term investment and joint ventures, especially those that use locally available raw materials, will find opportunities in the large national market. However, to improve prospects for success, potential investors must educate themselves extensively on local conditions and business practices, establish a local presence, and choose their partners carefully. The Nigerian Government is keenly aware that sustaining democratic principles, enhancing security for life and property, and rebuilding and maintaining infrastructure are necessary for the country to attract foreign investment.\\r\\nThe stock market capitalisation of listed companies in Nigeria was valued at $97.75 billion on 15 February 2008 by the Nigerian Stock Exchange.\\r\\nThe Swiss foreign ministry says it has done all it can to ensure that funds stolen by the late Nigerian dictator Sani Abacha were used properly in his homeland. The authorities were responding to allegations that $200 million (SFr240 million) of $700 million handed back by the Swiss Banks to Nigeria had been misappropriated.\\r\\nHousehold income or consumption by percentage share:r\\r\\nlowest 10%: 2.6%\\r\\nhighest 10%: 35.8% (1996ÿ97)\\r\\nIndustries: crude oil, coal, tin, columbite, palm oil, peanuts, cotton, rubber, wood, hides and skins, textiles, cement and other construction materials, food products, footwear, chemicals, fertilizer, printing, ceramics, steel, small commercial ship construction and repair\\r\\nIndustrial production growth rate: 4.7% (2010 est.)\\r\\nAgriculture ÿ products: cocoa, peanuts, palm oil, maize, rice, sorghum, millet, cassava (tapioca), yams, rubber; cattle, sheep, goats, pigs; timber; fish\\r\\nExchange rates: Naira (NGN) per US$1 ÿ 157.3 (2012) 149.5 (2009), 120 (2006), 128 (2005), 132.89 (2004), 129.22 (2003), 120.58 (2002), 111.23 (2001)\\r\\n1. All twenty-eight member states of the European Union are also members of the WTO in their own right:\\r\\n\\r\\n2. Special administrative regions of the People's Republic of China, participates as \\"Hong Kong, China\\" and \\"Macao China\\".\\r\\n3. Officially the Republic of China, participates as \\"Separate Customs Territory of Taiwan, Penghu, Kinmen and Matsu\\", and \\"Chinese Taipei\\" in short.","input":"What is the structure of the nigerian economy?"},{"output":"Green National Committee","context":"\\r\\n\\r\\nThe Green Party of the United States (GPUS) is a green federation of political parties in the United States.[7] The party promotes green politics, specifically environmentalism, nonviolence, social justice, participatory, grassroots democracy, gender equality, LGBT rights, anti-war and anti-racism. On the political spectrum, the party is generally seen as left-wing.[1]\\r\\n\\r\\nThe GPUS was founded in 2001 as the evolution of the Association of State Green Parties (ASGP), which was formed in 1996.[8] After its founding, the GPUS soon became the primary national green organization in the country, eclipsing the Greens/Green Party USA (G/GPUSA), which formed in 1991 out of the Green Committees of Correspondence (CoC), a collection of local green groups active since 1984.[9] The ASGP had increasingly distanced itself from the G/GPUSA in the late 1990s.[10]\\r\\n\\r\\nThe Greens gained widespread public attention during the 2000 presidential election, when the ticket composed of Ralph Nader and Winona LaDuke won 2.7% of the popular vote. Nader was vilified by many Democrats and even some Greens, who accused him of spoiling the election for Al Gore, the Democratic candidate.[11] The degree of Nader's impact on the 2000 election remains controversial.[12][13]\\r\\n\\r\\nThe GPUS has had several members elected into state legislatures, including in California, Maine and Arkansas. In September 2017, independent Ralph Chapman, member of the Maine House of Representatives, switched his affiliation to the Green Party.[14] A number of Greens around the United States hold positions on the municipal level, including on school boards, city councils and as mayors.\\r\\n\\r\\nThe political movement that began in 1985 as the decentralized Committees of Correspondence[15] evolved into a more centralized structure by 1990, opening a national clearinghouse and forming governing bodies, bylaws and a platform as the Green Committees of Correspondence (GCoC) and by 1990 simply The Greens. The organization conducted grassroots organizing efforts, educational activities and electoral campaigns.\\r\\n\\r\\nInternal divisions arose between members who saw electoral politics as ultimately corrupting and supported the notion of an \\"anti-party party\\" formed by Petra Kelly and other leaders of the Greens in Germany[16] vs. those who saw electoral strategies as a crucial engine of social change. A struggle for the direction of the organization culminated a \\"compromise agreement\\", ratified in 1990 at the Greens National Congress in Elkins, West Virginia and in which both strategies would be accommodated within the same 527 political organization renamed the Greens/Green Party USA (G/GPUSA). It was recognized by the FEC as a national political party in 1991.\\r\\n\\r\\nThe compromise agreement subsequently collapsed and two Green party organizations have co-existed in the United States since. The Green Politics Network was organized in 1990 and the National Association of Statewide Green Parties formed by 1994. Divisions between those pressing to break onto the national political stage and those aiming to grow roots at the local level continued to widen during the 1990s. The Association of State Green Parties (ASGP) encouraged and backed Nader's presidential runs in 1996 and 2000. By 2001, the push to separate electoral activity from the G/GPUSA issue-based organizing led to the Boston Proposal and subsequent rise of the Green Party of the United States.  The G/GPUSA lost most of its affiliates in the following months and dropped its FEC national party status in 2005.\\r\\n\\r\\nIn 2016, Mark Salazar set a new record for a Green Party nominee for Congress. Running in the Arizona 8th district against incumbent Republican Congressman Trent Franks, Salazar received 93,954 votes or 31.43%.[17]\\r\\n\\r\\nThe GPUS follows the ideals of green politics, which are based on the Four Pillars, namely ecological wisdom, social justice, grassroots democracy and nonviolence.[18]\\r\\n\\r\\nThe Ten Key Values, which expand upon the Four Pillars, are as follows:\\r\\n\\r\\nPeter Camejo was quoted in 2002 as claiming that he was a watermelongreen on the outside, but red on the inside.[19] In January 2004, he initiated the Avocado Declaration, which compares Greens to avocados. \\"An avocado is Green on the outside and Green on the inside\\".[20] The Declaration goes on to explain that Greens have a vital role in bringing democracy to the otherwise undemocratic two party system of the United States; that the Greens have a unique and independent identity as a third party, which cannot be subsumed into the Republican or Democratic parties; and that they cannot be dismissed by Republican or Democratic critics by implying that they are merely socialists or communists.\\r\\n\\r\\nThe Green Party does not accept donations from corporations, political action committees (PACs), 527(c) organizations or soft money. The party's platforms and rhetoric harshly criticize corporate influence and control over government, media and society at large.[21]\\r\\n\\r\\nThe party supports the implementation of a single-payer healthcare system. They have also called for contraception and abortion procedures to be available on demand.[22]\\r\\n\\r\\nThe Green Party calls for proving tuition-free college at public universities and vocational schools, increasing funding for after-school and daycare programs, cancelling all student loan debt, and repealing the No Child Left Behind Act. They are strongly against the dissolution of public schools and the privatization of education.[23]\\r\\n\\r\\nThe party favors the abolition of the death penalty, repeal of three-strikes laws, banning of private prisons, legalization of marijuana, and decriminalization of other drugs.[24]\\r\\n\\r\\nThe Green Party advocates for \\"complete and full\\" reparations to the African American community, as well the removal of the Confederate flag from all government buildings. [25]\\r\\n\\r\\nThe party supports same-sex marriage, the right of access to medical and surgical treatment for sex reassignment, and withdrawing foreign aid to countries with poor LGBT+ rights records.[26]\\r\\n\\r\\nThe Green Party calls on the United States to join the International Criminal Court, and sign the Comprehensive Nuclear-Test-Ban Treaty and Non-Proliferation Treaty. Additionally, it supports cutting the defense budget in half as well as prohibiting all arms sales to foreign countries.[27]\\r\\n\\r\\nGreens support the Joint Comprehensive Plan of Action.\\r\\n\\r\\nThe Green Party advocates for the right of return, cutting all U.S. aid to Israel, and a one-state solution. It has also expressed support for the international Boycott, Divestment and Sanctions (BDS) movement.\\r\\n\\r\\nThe Green Party has two national committees recognized by the Federal Election Commission (FEC):\\r\\n\\r\\nThe GNC is composed of delegates elected by affiliated state parties. The state parties also appoint delegates to serve on the various standing committees of the GNC. The National Committee elects a steering committee of seven co-chairs, a secretary and a treasurer to oversee daily operations. The National Committee performs most of its business online, but it also holds an annual national meeting to conduct business in person.\\r\\n\\r\\nFive Identity Caucuses have achieved representation on the GNC:\\r\\n\\r\\nOther caucuses have worked toward formal recognition by the GNC:\\r\\n\\r\\nThe Green Party has its strongest popular support on the Pacific Coast, Upper Great Lakes, and Northeast, as reflected in the geographical distribution of Green candidates elected.[37] As of  June 2007[update], Californians have elected 55 of the 226 office-holding Greens nationwide. Other states with high numbers of Green elected officials include Pennsylvania (31), Wisconsin (23), Massachusetts (18) and Maine (17). Maine has the highest per capita number of Green elected officials in the country and the largest Green registration percentage with more than 29,273 Greens comprising 2.95% of the electorate as of  November 2006[update].[38] Madison, Wisconsin is the city with the most Green elected officials (8), followed by Portland, Maine (7).\\r\\n\\r\\nThe 2016 presidential campaign of Jill Stein got substantive support from counties and precincts with a high percentage of Native American population. For instance, in Sioux County (North Dakota, 84,1% Native American), Stein gained her best county-wide result: 10.4% of the votes. In Rolette County (also North Dakota, 77% Native American), she got 4.7% of the votes. Other majority Native American counties where Stein did above state average are Menominee (WI), Roosevelt (MN) and several precincts in Alaska.[39][40]\\r\\n\\r\\nIn 2005, the Green Party had 305,000 registered members in states allowing party registration and tens of thousands of members and contributors in the rest of the country.[41] One challenge that the Green Party (as well as other third parties) faces is the difficulty of overcoming ballot access laws in many states.\\r\\n\\r\\nThe following is a list of accredited state parties which comprise the Green Party of the United States.[42]\\r\\n\\r\\nIn addition, the Green Party has a chapter in the United States Virgin Islands.[43] The Green Party does not currently have active state chapters in The Dakotas, Utah or Vermont.\\r\\n\\r\\nAs of  October?2016[update], 143 officeholders in the United States were affiliated with the Green Party, the majority of them in California, several in Illinois, Connecticut, Maine, Massachusetts, Oregon, Pennsylvania, and Wisconsin, with five or fewer in ten other states.[44] These included one mayor and one deputy mayor and fourteen county or city commissioners (or equivalent). The remainder were members of school boards, clerks and other local administrative bodies and positions.[44]\\r\\n\\r\\nSeveral Green Party members have been elected to state-level office, though not always as affiliates of the party. John Eder was elected to the Maine House of Representatives, re-elected in 2004, but defeated in 2006. Audie Bock was elected to the California State Assembly in 1999, but switched her registration to independent seven months later[45] running as such in the 2000 election.[46] Richard Carroll was elected to the Arkansas House of Representatives in 2008, but switched parties to become a Democrat five months after his election.[47] Fred Smith was elected to the Arkansas House of Representatives in 2012,[48] but re-registered as a Democrat in 2014.[49] In 2010, former Green Party leader Ben Chipman was elected to the Maine House of Representatives as an unenrolled candidate and was re-elected in 2012 and 2014.[50]\\r\\n\\r\\nGayle McLaughlin was twice elected mayor of Richmond, California, defeating two Democrats in 2006[51] and then reelected in 2010; and elected to City Council in 2014 after completing her second term as mayor.[52] With a population of over 100,000 people, it was the largest American city with a Green mayor. Fairfax, California; Arcata, California; Sebastopol, California; and New Paltz, New York are the only towns in the United States to have had a Green Party majority in their town councils. Twin Ridges Elementary in Nevada County, California held the first Green Party majority school board in the United States.[53]\\r\\n\\r\\nOn September 21, 2017, Ralph Chapman, a member of the Maine House of Representatives, switched his party registration from unaffiliated to Green, providing the Green Party with their first state-level representative since 2014.[14] Henry John Bear became a member of the Green Party in the same year as Chapman, giving the Maine Green Independent Party and GPUS its second currently-serving state representative, though Bear is a nonvoting tribal member of the Maine House of Representatives.\\r\\n\\r\\nNo nominee of the Green Party has been elected to office in the federal government.\\r\\n\\r\\nThe Green National Convention is scheduled in presidential election years and the Annual National Meeting is scheduled in other years. The Green National Committee conducts business online between these in-person meetings.\\r\\n\\r\\nIn the early decades of Green organizing in the United States, the prevailing American system of money-dominated elections was universally rejected by Greens, so that some Greens were reluctant to have Greens participate in the election system at all because they deemed the campaign finance system inherently corrupt. Other Greens felt strongly that the Green Party should develop in the electoral arena and many of these Greens felt that adopting an alternative model of campaign finance, emphasizing self-imposed contribution limits, would present a wholesome and attractive contrast to the odious campaign finance practices of the money-dominated major parties.\\r\\n\\r\\nOver the years, some state Green parties have come to place less emphasis on the principle of self-imposed limits than they did in the past. Nevertheless, it is safe to say that Green Party fundraising (for candidates' campaigns and for the party itself) still tends to rely on relatively small contributions and that Greens generally decry not only the rise of the Super PACs, but also the big-money system, which some Greens criticize as plutocracy.\\r\\n\\r\\nSome Greens feel that the Green Party's position should be simply to follow the laws and regulations of campaign finance.[67] Other Greens argue that it would injure the Green Party not to practice a principled stand against the anti-democratic influence of money in the political process. Candidates for office, like Jill Stein, the 2012 and 2016 Green Party nominee for the President of the United States, typically rely on smaller donations to fund their campaigns.[68]","input":"Who is the chair of the green party?"},{"output":"Cragside, England","context":"A power station, also referred to as a power plant or powerhouse and sometimes generating station or generating plant, is an industrial facility for the generation of electric power. Most power stations contain one or more generators, a rotating machine that converts mechanical power into electrical power. The relative motion between a magnetic field and a conductor creates an electrical current. The energy source harnessed to turn the generator varies widely. Most power stations in the world burn fossil fuels such as coal, oil, and natural gas to generate electricity. Others use nuclear power, but there is an increasing use of cleaner renewable sources such as solar, wind, wave and hydroelectric.\\r\\n\\r\\n\\r\\nIn 1868 a hydro electric power station was designed and built by Lord Armstrong at Cragside, England. It used water from lakes on his estate to power Siemens dynamos. The electricity supplied power to lights, heating, produced hot water, ran an elevator as well as labor-saving devices and farm buildings.[1]\\r\\nIn the early 1870s Belgian inventor Znobe Gramme invented a generator powerful enough to produce power on a commercial scale for industry.[2]\\r\\nIn the autumn of 1882, a central station providing public power was built in Godalming, England. It was proposed after the town failed to reach an agreement on the rate charged by the gas company, so the town council decided to use electricity. It used hydroelectric power for street lighting and household lighting. The system was not a commercial success and the town reverted to gas.[3]\\r\\nIn 1882 the world's first coal-fired public power station, the Edison Electric Light Station, was built in London, a project of Thomas Edison organized by Edward Johnson. A Babcock & Wilcox boiler powered a 125-horsepower steam engine that drove a 27-ton generator. This supplied electricity to premises in the area that could be reached through the culverts of the viaduct without digging up the road, which was the monopoly of the gas companies. The customers included the City Temple and the Old Bailey. Another important customer was the Telegraph Office of the General Post Office, but this could not be reached though the culverts. Johnson arranged for the supply cable to be run overhead, via Holborn Tavern and Newgate.[4]\\r\\nIn September 1882 in New York, the Pearl Street Station was established by Edison to provide electric lighting in the lower Manhattan Island area. The station ran until destroyed by fire in 1890. The station used reciprocating steam engines to turn direct-current generators. Because of the DC distribution, the service area was small, limited by voltage drop in the feeders. The War of Currents eventually resolved in favor of AC distribution and utilization, although some DC systems persisted to the end of the 20th century. DC systems with a service radius of a mile (kilometer) or so were necessarily smaller, less efficient of fuel consumption, and more labor-intensive to operate than much larger central AC generating stations.\\r\\nAC systems used a wide range of frequencies depending on the type of load; lighting load using higher frequencies, and traction systems and heavy motor load systems preferring lower frequencies. The economics of central station generation improved greatly when unified light and power systems, operating at a common frequency, were developed. The same generating plant that fed large industrial loads during the day, could feed commuter railway systems during rush hour and then serve lighting load in the evening, thus improving the system load factor and reducing the cost of electrical energy overall. Many exceptions existed, generating stations were dedicated to power or light by the choice of frequency, and rotating frequency changers and rotating converters were particularly common to feed electric railway systems from the general lighting and power network.\\r\\nThroughout the first few decades of the 20th century central stations became larger, using higher steam pressures to provide greater efficiency, and relying on interconnections of multiple generating stations to improve reliability and cost. High-voltage AC transmission allowed hydroelectric power to be conveniently moved from distant waterfalls to city markets. The advent of the steam turbine in central station service, around 1906, allowed great expansion of generating capacity. Generators were no longer limited by the power transmission of belts or the relatively slow speed of reciprocating engines, and could grow to enormous sizes. For example, Sebastian Ziani de Ferranti planned what would have been the largest reciprocating steam engine ever built for a proposed new central station, but scrapped the plans when turbines became available in the necessary size. Building power systems out of central stations required combinations of engineering skill and financial acumen in equal measure. Pioneers of central station generation include George Westinghouse and Samuel Insull in the United States, Ferranti and Charles Hesterman Merz in UK, and many others.\\r\\nIn thermal power stations, mechanical power is produced by a heat engine that transforms thermal energy, often from combustion of a fuel, into rotational energy. Most thermal power stations produce steam, so they are sometimes called steam power stations. Not all thermal energy can be transformed into mechanical power, according to the second law of thermodynamics; therefore, there is always heat lost to the environment. If this loss is employed as useful heat, for industrial processes or district heating, the power plant is referred to as a cogeneration power plant or CHP (combined heat-and-power) plant. In countries where district heating is common, there are dedicated heat plants called heat-only boiler stations. An important class of power stations in the Middle East uses by-product heat for the desalination of water.\\r\\nThe efficiency of a thermal power cycle is limited by the maximum working fluid temperature produced. The efficiency is not directly a function of the fuel used. For the same steam conditions, coal-, nuclear- and gas power plants all have the same theoretical efficiency. Overall, if a system is on constantly (base load) it will be more efficient than one that is used intermittently (peak load). Steam turbines generally operate at higher efficiency when operated at full capacity.\\r\\nBesides use of reject heat for process or district heating, one way to improve overall efficiency of a power plant is to combine two different thermodynamic cycles in a combined cycle plant. Most commonly, exhaust gases from a gas turbine are used to generate steam for a boiler and a steam turbine. The combination of a \\"top\\" cycle and a \\"bottom\\" cycle produces higher overall efficiency than either cycle can attain alone.\\r\\nPower plants that can be dispatched (scheduled) to provide energy to a system include:\\r\\nNon-dispatchable plants include such sources as wind and solar energy; while their long-term contribution to system energy supply is predictable, on a short-term (daily or hourly) base their energy must be used as available since generation cannot be deferred. Contractual arrangements (\\"take or pay\\") with independent power producers or system interconnections to other networks may be effectively non-dispatchable.\\r\\nAll thermal power plants produce waste heat energy as a byproduct of the useful electrical energy produced. The amount of waste heat energy equals or exceeds the amount of energy converted into useful electricity. Gas-fired power plants can achieve as much as 65 percent conversion efficiency, while coal and oil plants achieve around 30 to 49 percent. The waste heat produces a temperature rise in the atmosphere, which is small compared to that produced by greenhouse-gas emissions from the same power plant. Natural draft wet cooling towers at many nuclear power plants and large fossil fuel-fired power plants use large hyperboloid chimney-like structures (as seen in the image at the right) that release the waste heat to the ambient atmosphere by the evaporation of water.\\r\\nHowever, the mechanical induced-draft or forced-draft wet cooling towers in many large thermal power plants, nuclear power plants, fossil-fired power plants, petroleum refineries, petrochemical plants, geothermal, biomass and waste-to-energy plants use fans to provide air movement upward through downcoming water, and are not hyperboloid chimney-like structures. The induced or forced-draft cooling towers are typically rectangular, box-like structures filled with a material that enhances the mixing of the upflowing air and the downflowing water.[8][9]\\r\\nIn areas with restricted water use, a dry cooling tower or directly air-cooled radiators may be necessary, since the cost or environmental consequences of obtaining make-up water for evaporative cooling would be prohibitive. These coolers have lower efficiency and higher energy consumption to drive fans, compared to a typical wet, evaporative cooling tower.\\r\\nElectric companies often prefer to use cooling water from the ocean, a lake, or a river, or a cooling pond, instead of a cooling tower. This single pass or once-through cooling system can save the cost of a cooling tower and may have lower energy costs for pumping cooling water through the plant's heat exchangers. However, the waste heat can cause thermal pollution as the water is discharged. Power plants using natural bodies of water for cooling are designed with mechanisms such as fish screens, to limit intake of organisms into the cooling machinery. These screens are only partially effective and as a result billions of fish and other aquatic organisms are killed by power plants each year.[10][11] For example, the cooling system at the Indian Point Energy Center in New York kills over a billion fish eggs and larvae annually.[12]\\r\\nA further environmental impact is that aquatic organisms which adapt to the warmer discharge water may be injured if the plant shuts down in cold weather.\\r\\nWater consumption by power stations is a developing issue.[13]\\r\\nIn recent years, recycled wastewater, or grey water, has been used in cooling towers. The Calpine Riverside and the Calpine Fox power stations in Wisconsin as well as the Calpine Mankato power station in Minnesota are among these facilities.\\r\\nPower stations can also generate electrical energy from renewable energy sources.\\r\\nIn a hydroelectric power station water flows through turbines using hydropower to generate hydroelectricity. Power is captured from the gravitational force of water falling through penstocks to water turbines connected to generators. The amount of power available is a combination of height and flow. A wide range of Dams may be built to raise the water level, and create a lake for storing water. Hydropower is produced in 150 countries, with the Asia-Pacific region generating 32 percent of global hydropower in 2010. China is the largest hydroelectricity producer, with 721 terawatt-hours of production in 2010, representing around 17 percent of domestic electricity use.\\r\\nSolar energy can be turned into electricity either directly in solar cells, or in a concentrating solar power plant by focusing the light to run a heat engine.\\r\\nA solar photovoltaic power plant converts sunlight into direct current electricity using the photoelectric effect. Inverters change the direct current into alternating current for connection to the electrical grid. This type of plant does not use rotating machines for energy conversion.\\r\\nSolar thermal power plants are another type of solar power plant. They use either parabolic troughs or heliostats to direct sunlight onto a pipe containing a heat transfer fluid, such as oil. The heated oil is then used to boil water into steam, which turns a turbine that drives an electrical generator. The central tower type of solar thermal power plant uses hundreds or thousands of mirrors, depending on size, to direct sunlight onto a receiver on top of a tower. Again, the heat is used to produce steam to turn turbines that drive electrical generators.\\r\\nWind turbines can be used to generate electricity in areas with strong, steady winds, sometimes offshore. Many different designs have been used in the past, but almost all modern turbines being produced today use a three-bladed, upwind design. Grid-connected wind turbines now being built are much larger than the units installed during the 1970s. They thus produce power more cheaply and reliably than earlier models. With larger turbines (on the order of one megawatt), the blades move more slowly than older, smaller, units, which makes them less visually distracting and safer for birds.\\r\\nMarine energy or marine power (also sometimes referred to as ocean energy or ocean power) refers to the energy carried by ocean waves, tides, salinity, and ocean temperature differences. The movement of water in the worlds oceans creates a vast store of kinetic energy, or energy in motion. This energy can be harnessed to generate electricity to power homes, transport and industries.\\r\\nThe term marine energy encompasses both wave power  power from surface waves, and tidal power  obtained from the kinetic energy of large bodies of moving water. Offshore wind power is not a form of marine energy, as wind power is derived from the wind, even if the wind turbines are placed over water.\\r\\nThe oceans have a tremendous amount of energy and are close to many if not most concentrated populations. Ocean energy has the potential of providing a substantial amount of new renewable energy around the world.[14]\\r\\nSalinity gradient energy is called pressure-retarded osmosis. In this method, seawater is pumped into a pressure chamber that is at a pressure lower than the difference between the pressures of saline water and fresh water. Freshwater is also pumped into the pressure chamber through a membrane, which increases both the volume and pressure of the chamber. As the pressure differences are compensated, a turbine is spun creating energy. This method is being specifically studied by the Norwegian utility Statkraft, which has calculated that up to 25 TWh/yr would be available from this process in Norway. Statkraft has built the world's first prototype osmotic power plant on the Oslo fiord which was opened on November 24, 2009.\\r\\nBiomass energy can be produced from combustion of waste green material to heat water into steam and drive a steam turbine. Bioenergy can also be processed through a range of temperatures and pressures in gasification, pyrolysis or torrefaction reactions. Depending on the desired end product, these reactions create more energy-dense products (syngas, wood pellets, biocoal) that can then be fed into an accompanying engine to produce electricity at a much lower emission rate when compared with open burning.\\r\\nIt is possible to store energy and produce the electricity at a later time like in Pumped-storage hydroelectricity, Thermal energy storage, Flywheel energy storage, Battery storage power station and so on.\\r\\nThe worlds largest form of storage for excess electricity, pumped-storage is a reversible hydroelectric plant. They are a net consumer of energy but provide storage for any source of electricity, effectively smoothing peaks and troughs in electricity supply and demand. Pumped storage plants typically use \\"spare\\" electricity during off peak periods to pump water from a lower reservoir to an upper reservoir. Because the pumping takes place \\"off peak\\", electricity is less valuable than at peak times. This less valuable \\"spare\\" electricity comes from uncontrolled wind power and base load power plants such as coal, nuclear and geothermal, which still produce power at night even though demand is very low. During daytime peak demand, when electricity prices are high, the storage is used for peaking power, where water in the upper reservoir is allowed to flow back to a lower reservoir through a turbine and generator. Unlike coal power stations, which can take more than 12 hours to start up from cold, a hydroelectric generator can be brought into service in a few minutes, ideal to meet a peak load demand. Two substantial pumped storage schemes are in South Africa, Palmiet Pumped Storage Scheme and another in the Drakensberg, Ingula Pumped Storage Scheme.\\r\\nThe power generated by a power station is measured in multiples of the watt, typically megawatts (106 watts) or gigawatts (109 watts). Power stations vary greatly in capacity depending on the type of power plant and on historical, geographical and economic factors. The following examples offer a sense of the scale.\\r\\nMany of the largest operational onshore wind farms are located in the USA. As of 2011, the Roscoe Wind Farm is the second largest onshore wind farm in the world, producing 781.5 MW of power, followed by the Horse Hollow Wind Energy Center (735.5 MW). As of July 2013, the London Array in United Kingdom is the largest offshore wind farm in the world at 630 MW, followed by Thanet Offshore Wind Project in United Kingdom at 300 MW.\\r\\nAs of 2015[update], the largest photovoltaic (PV) power plants in the world are led by Longyangxia Dam Solar Park in China, rated at 850 megawatts.\\r\\nSolar thermal power stations in the U.S. have the following output:\\r\\nLarge coal-fired, nuclear, and hydroelectric power stations can generate hundreds of megawatts to multiple gigawatts. Some examples:\\r\\nGas turbine power plants can generate tens to hundreds of megawatts. Some examples:\\r\\nThe rated capacity of a power station is nearly the maximum electrical power that that power station can produce. Some power plants are run at almost exactly their rated capacity all the time, as a non-load-following base load power plant, except at times of scheduled or unscheduled maintenance.\\r\\nHowever, many power plants usually produce much less power than their rated capacity.\\r\\nIn some cases a power plant produces much less power than its rated capacity because it uses an intermittent energy source. Operators try to pull maximum available power from such power plants, because their marginal cost is practically zero, but the available power varies widelyin particular, it may be zero during heavy storms at night.\\r\\nIn some cases operators deliberately produce less power for economic reasons. The cost of fuel to run a load following power plant may be relatively high, and the cost of fuel to run a peaking power plant is even higherthey have relatively high marginal costs. Operators keep power plants turned off (\\"operational reserve\\") or running at minimum fuel consumption[citation needed] (\\"spinning reserve\\") most of the time. Operators feed more fuel into load following power plants only when the demand rises above what lower-cost plants (i.e., intermittent and base load plants) can produce, and then feed more fuel into peaking power plants only when the demand rises faster than the load following power plants can follow.\\r\\nThe term Power station is generally limited to those able to be despatched by a system operator (i.e. the system operator can, by one means or another, alter the planned output of the generating facility).[16][17][18]\\r\\nThe power station operator has several duties in the electricity-generating facility. Operators are responsible for the safety of the work crews that frequently do repairs on the mechanical and electrical equipment. They maintain the equipment with periodic inspections and log temperatures, pressures and other important information at regular intervals. Operators are responsible for starting and stopping the generators depending on need. They are able to synchronize and adjust the voltage output of the added generation with the running electrical system, without upsetting the system. They must know the electrical and mechanical systems in order to troubleshoot solve/fix problems in the facility and add to the reliability of the facility. Operators must be able to respond to an emergency and know the procedures in place to deal with it.","input":"Where was the first power station built and why?"},{"output":"from June 18, 2004, to August 19, 2006, for two seasons","context":"Phil of the Future is an American science fiction sitcom that originally aired on Disney Channel from June 18, 2004, to August 19, 2006, for two seasons. The series was created by Tim Maile and Douglas Tuber and produced by 2121 Productions, a part of Brookwell McNamara Entertainment. It follows a family from the future that gets stranded in the 21st century when their time machine breaks down. The series began airing again (as reruns) as part of Disney Replay. It also aired in select countries such as Canada (Family Channel).\\r\\n\\r\\n\\r\\nPhil of the Future's working title was The Out of Timers.[citation needed] The title theme song for Phil of the Future was written by John Adair and Steve Hampton. It was sung by Loren Ellis and The Drew Davis Band, who also sang the theme song of The Suite Life of Zack & Cody.\\r\\nA DVD of the show titled Gadgets & Gizmos was released on August 16, 2005.[1] It contains four episodes including a never-before-seen episode.\\r\\nA video game based on Phil of the Future was released for the Game Boy Advance on August 22, 2006. In the game, Pim uses a cloning machine to create clones of a pet from the future called \\"Blahs\\" and it is up to Phil to stop them.[2]","input":"When did phil of the future first air?"},{"output":"1951ÿ52","context":"Jawaharlal Nehru\\r\\nINC\\r\\nJawaharlal Nehru\\r\\nINC\\r\\nThe Indian general election of 1951ÿ52 elected the first Lok Sabha since India became independent in August 1947.[1][2][3] Until this point, the Indian Constituent Assembly had served as an interim legislature. See the 'Durations' section below to find the time-range associated with these elections.\\r\\nThe Indian National Congress (INC) won a landslide victory, winning 364 of the 489 seats and 45% of the total votes polled. This was over four times as many votes as the second-largest party. Jawaharlal Nehru became the first democratically elected Prime Minister of the country. In the first Lok Sabha polls held in 1951, India had around 173 million voters, out of an overall population of about 360 million.[4] Voter turnout was 45.7%.[5] In the 1952 Indian general election, Ravi Narayana Reddy from Telangana stood for the People's Democratic Front, (a pseudonym for the banned Communist Party of India), and polled more votes than Jawaharlal Nehru and also the first one entering in the parliament in independent India.[3]\\r\\n\\r\\n\\r\\nBefore Independent India went to the polls, two former cabinet colleagues of Nehru established separate political parties to challenge the INC's supremacy. While Shyama Prasad Mookerjee went on to found the Jana Sangh in October 1951, Dalit leader Dr. B. R. Ambedkar revived the Scheduled Castes Federation (which was later named the Republican Party). Other parties which started coming to the forefront included the Kisan Mazdoor Praja Parishad, whose prime mover was Acharya Kripalani; the Socialist Party, which had Ram Manohar Lohia and Jayaprakash Narayan's leadership to boast of; and the Communist Party of India. However, these smaller parties were unable to make an electoral stand against the Indian National Congress.\\r\\nThe first general elections, which were conducted for 489 seats in 401 constituencies, represented 25 Indian states. At that time, there were 314 constituencies with one seat, 86 with two seats and one with three seats.[6] The multi-seat constituencies were abolished in the 1960s. There were also 2 nominated Anglo-Indian members.\\r\\nScheduled Caste leader B. R. Ambedkar was defeated in the Bombay (North Central) (reserved seat) constituency as Scheduled Castes Federation candidate by his little-known former assistant and Congress Candidate Narayan Sadoba Kajrolkar, who polled 1,38,137 votes compared to Ambedkar's 1,23,576 votes.[7]:156 Dr Ambedkar then entered the parliament as a Rajya Sabha member. He contested by-poll from Bhandara in 1954 to try to enter Lok Sabha but again lost to Mr Borkar of Congress.\\r\\nAcharya Kripalani lost from Faizabad in UP as KMPP candidate, but his wife Sucheta Kripalani defeated the Congress candidate Manmohini Sahgal in Delhi.[8]\\r\\nThe Speaker of the first Lok Sabha was Ganesh Vasudev Mavalankar. The first Lok Sabha also witnessed 677 sittings (3,784 hours), the highest recorded count of the number of sittings. The Lok Sabha lasted its full term from 17 April 1952 until 4 April 1957.\\r\\nWhile Indian Government's official websites and official documents assign the year 1951 to these polls, it is a misrepresentation because all territories except Himachal Pradesh and Jammu & Kashmir voted in FebruaryÿMarch 1952; no polls were held for Lok Sabha seats in Kashmir until 1967, and only Himachal Pradesh voted in 1951 for the first Lok Sabha because weather tends to be inclement in February and March, heavy snow impending free movement.[9] The rest of the India voted only in FebruaryÿMarch 1952 for the Lok Sabha and Vidhan Sabha elections. Polling was held between 25 October 1951 and 27 March 1952. The very first votes of the election were cast in the tehsil (district) of Chini in Himachal Pradesh.[7]","input":"When was the first general election held in india?"},{"output":"Ventricular diastole","context":"Diastole /da???st?li?/ is that part of the cardiac cycle during which the heart refills with blood after the emptying done during systole (contraction). Ventricular diastole is the period during which the two ventricles are relaxing from the contortions of contraction, then dilating and filling; atrial diastole is the period during which the two atria likewise are relaxing, dilating, and filling. The term diastole originates from the Greek word Ѵϫ, meaning dilation.[1]\\r\\n\\r\\n\\r\\nFor a healthy human heart the entire cardiac cycle typically runs less than one second. That is, for a typical heart rate of 75 beats per minute (bpm), the cycle requires 0.3 sec in ventricular systole (contraction)pumping blood to all body systems from the two ventricles; and 0.5 sec in diastole (dilation), re-filling the four chambers of the heart, for a total time of 0.8 sec to complete the entire cycle.[2]\\r\\nDuring early ventricular diastole, pressure in the two ventricles begins to drop from the peak reached during systole. When pressure in the left ventricle falls below that in the left atrium the mitral valve opens due to a negative pressure differential (suction) between the two chambers, causing blood in the atrium (accumulated during atrial diastole) to flow into the ventricle (see graphic at top). Likewise, the same phenomenon runs simultaneously in the right ventricle and right atrium through the tricuspid valve.\\r\\nThe ventricular filling flow (or flow from the atria into the ventricles) has an early (E) diastolic component caused by ventricular suction, and then a late one created by atrial systole (A). The E/A ratio is used as a diagnostic measure as it's diminishment indicates probable diastolic dysfunction.[3]\\r\\nEarly diastole is a suction mechanism between the atrial and ventricular chambers.[4] Then, in late ventricular diastole, the two atrial chambers contract (atrial systole), causing blood pressure in both atria to increase and forcing additional blood flow into the ventricles. This beginning of the atrial systole is known as the atrial kicksee Wiggers diagram. The atrial kick does not supply the larger amount of flow (during the cardiac cycle) as about 80 per cent of the collected blood volume flows into the ventricles during the active suction period.[5]\\r\\nAt the beginning of the cardiac cycle, all four chambers of the hearttwo atria and two ventriclesare in relaxation and dilation, or diastole. The atria are filling with separate blood volumes returning to the right atrium (from the vena cavae) and to the left atrium (from the lungs). After chamber and back pressures equalize, the mitral and tricuspid valves open, and the 'return' blood flows are passed through the atria into the ventricles. When the ventricles have completed most of their filling, the atria begin to contract (atrial systole), forcing blood under pressure into the ventricles. Now the ventricles start to contract, and as pressures within the ventricles rise, the mitral and tricuspid valves are closed.\\r\\nAs pressures within the ventricles continue to rise, they exceed the 'back pressures' in the aorta trunk and the pulmonary arteries trunk, and the aortic and pulmonary valves openand now blood is ejected from the heart. Ejection causes pressure within the ventricles to fall, and, simultaneously, the atria begin to refill (atrial diastole). Finally, pressures within the ventricles fall below the back pressures in the trunks of the aorta and the pulmonary arteries, and the aortic and pulmonary valves close. The ventricles start to relax, the mitral and tricuspid valves begin to open, and the cycle begins again.[6]\\r\\nIn summary, when the ventricles are in systole and contracting, the atria are relaxed and collecting blood as 'return' flows. When, in late diastole, the ventricles become fully dilated, the atria begin to contract, pumping blood to the ventricles. The atria feed a steady supply of blood to the ventricles, thereby serving as a reservoir to the ventricles and ensuring that these pumps never run dry.[7] This coordination ensures that blood is pumped and circulated efficiently throughout the body.[8]\\r\\nHere the adjective \\"diastolic\\" refers to the function (the \\"diastolic function\\") of filling the heart with blood between muscle contractions; it describes that portion of the cardiac cycle opposite to contraction. The term is more commonly known as one of the two main components for measuring blood pressurenamely, \\"diastolic pressure\\" refers to the lowest pressure in the arterial bloodstream occurring during each heartbeat. (The other component is \\"systolic pressure\\", which is the highest arterial pressure during each heartbeat.)\\r\\nWhen blood pressure is stated for medical purposes, it is usually written with the systolic and diastolic pressures separated by a slash, for example, 120/80?mmHg. This clinical notation is not a mathematical figure for a fraction or ratio, nor a display of a numerator over a denominator. Rather it is a medical notation showing the two clinically significant pressures involved (i.e., systolic-slash-diastolic, or 120/80). It is often shown followed by a third number, the value of the heart rate (in beats per minute), which typically is measured jointly with blood pressure readings.\\r\\nExamining diastolic function during a cardiac stress test is a good way to test for heart failure with preserved ejection fraction.[9]\\r\\nClassification of Blood Pressure in Adults[10]\\r\\nBrain natriuretic peptide (BNP) is a cardiac neurohormone secreted from ventricular myocytes (ventricular muscle cells) at the end of diastolethis in response to the normal, or sub-normal (as the case may be), stretching of cardiomyocytes (heart muscle cells) during systole. Elevated levels of BNP indicate excessive natriuresis (excretion of sodium to the urine) and decline of ventricular function, especially during diastole. Increased BNP concentrations have been found in patients who experience diastolic heart failure.[11]\\r\\nImpaired diastolic function can result from the decreased compliance of ventricular myocytes, and thus the ventricles, which means the heart muscle does not stretch as much as needed during filling.[2] This will result in a reduced end diastolic volume (EDV) and, according to the Frank-Starling mechanism, a reduced EDV will lead to a reduced stroke volume, thus a reduced cardiac output. Over time, decreased cardiac output will diminish the ability of the heart to circulate blood efficiently throughout the body.","input":"What is the relaxed state of the ventricle called?"},{"output":"between 1975 and 1985","context":"Port Authority of New York and New Jersey\\r\\nThe original World Trade Center was a large complex of seven buildings in Lower Manhattan, New York City, United States. It featured the landmark twin towers, which opened on April 4, 1973, and were destroyed as a result of the September 11 attacks in 2001. At the time of their completion, the \\"Twin Towers\\"the original 1 World Trade Center, at 1,368 feet (417?m); and 2 World Trade Center, at 1,362 feet (415.1?m)were the tallest buildings in the world. The other buildings in the complex included the Marriott World Trade Center (3 WTC), 4 WTC, 5 WTC, 6 WTC, and 7 WTC. All these buildings were built between 1975 and 1985, with a construction cost of $400?million ($2,300,000,000 in 2014 dollars).[4] The complex was located in New York City's Financial District and contained 13,400,000 square feet (1,240,000?m2) of office space.[5][6]\\r\\nThe World Trade Center experienced a fire on February 13, 1975,[7] a bombing on February 26, 1993,[8] and a robbery on January 14, 1998.[9] In 1998, the Port Authority decided to privatize the World Trade Center, leasing the buildings to a private company to manage, and awarded the lease to Silverstein Properties in July 2001.[10]\\r\\nOn the morning of September 11, 2001, Al-Qaeda-affiliated hijackers flew two Boeing 767 jets into the North and South Towers within minutes of each other; two hours later, both had collapsed. The attacks killed 2,606 people in and within the vicinity of the towers, as well as all 157 on board the two aircraft.[11] Falling debris from the towers, combined with fires that the debris initiated in several surrounding buildings, led to the partial or complete collapse of all the other buildings in the complex and caused catastrophic damage to ten other large structures in the surrounding area. The cleanup and recovery process at the World Trade Center site took eight months,[12][13] during which time what remained of the other World Trade Center buildings was demolished.\\r\\nThe World Trade Center complex was rebuilt over a span of more than a decade. The site is being rebuilt with six new skyscrapers, while a memorial to those killed in the attacks and a new rapid transit hub have both opened. One World Trade Center, the tallest building in the United States, is the lead building for the new complex, reaching more than 100 stories[14] upon its completion in November 2014.[15]\\r\\n\\r\\n\\r\\nThe western portion of the World Trade Center site was originally under the Hudson River, with the shoreline in the vicinity of Greenwich Street. It was on this shoreline close to the intersection of Greenwich and the former Dey Street that Dutch explorer Adriaen Block's ship, the Tyger, burned to the waterline in November 1613, stranding Block and his crew and forcing them to overwinter on the island. They built the first European settlement in Manhattan. The remains of the ship were buried under landfill when the shoreline was extended starting in 1797, and were discovered during excavation work in 1916. The remains of a second ship from the eighteenth century were discovered in 2010 during excavation work at the site. The ship, believed to be a Hudson River sloop, was found just south of where the Twin Towers used to stand, about 20 feet below the surface.[16]\\r\\nLater, the area became Radio Row. New York City's Radio Row, which existed from 1921 to 1966, was a warehouse district on the Lower West Side in the Financial District. Harry Schneck opened City Radio on Cortlandt Street in 1921, and eventually the area held several blocks of electronics stores, with Cortlandt Street as its central axis. The used radios, war surplus electronics (e.g., ARC-5 radios), junk, and parts often piled so high they would spill out onto the street, attracting collectors and scroungers. According to a business writer, it also was the origin of the electronic component distribution business.[17]\\r\\nThe idea of establishing a World Trade Center in New York City was first proposed in 1943. The New York State Legislature passed a bill authorizing New York Governor Thomas E. Dewey to begin developing plans for the project[18] but the plans were put on hold in 1949.[19] During the late 1940s and 1950s, economic growth in New York City was concentrated in Midtown Manhattan. To help stimulate urban renewal in Lower Manhattan, David Rockefeller suggested that the Port Authority build a World Trade Center in Lower Manhattan.[20]\\r\\nPlans for the use of eminent domain to remove the shops in Radio Row bounded by Vesey, Church, Liberty, and West Streets began in 1961 when the Port Authority of New York and New Jersey was deciding to build the world's first world trade center. They had two choices: the east side of Lower Manhattan, near the South Street Seaport; and the west side, near the H&M station, Hudson Terminal.[21](p56) Initial plans, made public in 1961, identified a site along the East River for the World Trade Center.[22] As a bi-state agency, the Port Authority required approval for new projects from the governors of both New York and New Jersey. New Jersey Governor Robert B. Meyner objected to New York getting a $335?million project.[23] Toward the end of 1961, negotiations with outgoing New Jersey Governor Meyner reached a stalemate.[24]\\r\\nAt the time, ridership on New Jersey's Hudson and Manhattan Railroad (H&M) had declined substantially from a high of 113?million riders in 1927 to 26?million in 1958 after new automobile tunnels and bridges had opened across the Hudson River.[25] In a December 1961 meeting between Port Authority director Austin J. Tobin and newly elected New Jersey Governor Richard J. Hughes, the Port Authority offered to take over the Hudson & Manhattan Railroad to have it become the Port Authority Trans-Hudson (PATH). The Port Authority also decided to move the World Trade Center project to the Hudson Terminal building site on the west side of Lower Manhattan, a more convenient location for New Jersey commuters arriving via PATH.[24] With the new location and Port Authority acquisition of the H&M Railroad, New Jersey agreed to support the World Trade Center project.[26] In compensation for Radio Row business owners' displacement, the PANYNJ gave each business $3,000 each, without regard to how long the business had been there or how prosperous the business was.[21](p68) After the area had been purchased for the World Trade Center in March 1964,[27] Radio Row was demolished starting in March 1965.[28] It was completely demolished by 1966.[29]\\r\\nApproval was also needed from New York City Mayor John Lindsay and the New York City Council. Disagreements with the city centered on tax issues. On August 3, 1966, an agreement was reached that the Port Authority would make annual payments to the City in lieu of taxes for the portion of the World Trade Center leased to private tenants.[30] In subsequent years, the payments would rise as the real estate tax rate increased.[31]\\r\\nOn September 20, 1962, the Port Authority announced the selection of Minoru Yamasaki as lead architect and Emery Roth & Sons as associate architects.[32] Yamasaki devised the plan to incorporate twin towers; Yamasaki's original plan called for the towers to be 80 stories tall,[33] but to meet the Port Authority's requirement for 10,000,000 square feet (930,000?m2) of office space, the buildings would each have to be 110 stories tall.[34]\\r\\nYamasaki's design for the World Trade Center, unveiled to the public on January 18, 1964, called for a square plan approximately 208 feet (63?m) in dimension on each side.[33][35] The buildings were designed with narrow office windows 18 inches (46?cm) wide, which reflected Yamasaki's fear of heights as well as his desire to make building occupants feel secure.[36] Yamasaki's design included building facades sheathed in aluminum-alloy.[37] The World Trade Center was one of the most-striking American implementations of the architectural ethic of Le Corbusier, and it was the seminal expression of Yamasaki's gothic modernist tendencies.[38]\\r\\nA major limiting factor in building height is the issue of elevators; the taller the building, the more elevators are needed to service the building, requiring more space-consuming elevator banks.[34] Yamasaki and the engineers decided to use a new system with two \\"sky lobbies\\"floors where people could switch from a large-capacity express elevator to a local elevator that goes to each floor in a section. This system, inspired by the local-express train operation that the New York City Subway system used,[39] allowed the design to stack local elevators within the same elevator shaft. Located on the 44th and 78th floors of each tower, the sky lobbies enabled the elevators to be used efficiently, increasing the amount of usable space on each floor from 62 to 75 percent by reducing the number of elevator shafts.[40][41] Altogether, the World Trade Center had 95 express and local elevators.[42]\\r\\nThe structural engineering firm Worthington, Skilling, Helle & Jackson worked to implement Yamasaki's design, developing the tube-frame structural system used in the twin towers. The Port Authority's Engineering Department served as foundation engineers, Joseph R. Loring & Associates as electrical engineers, and Jaros, Baum & Bolles (JB&B) as mechanical engineers. Tishman Realty & Construction Company was the general contractor on the World Trade Center project. Guy F. Tozzoli, director of the World Trade Department at the Port Authority, and Rino M. Monti, the Port Authority's Chief Engineer, oversaw the project.[43] As an interstate agency, the Port Authority was not subject to local laws and regulations of the City of New York, including building codes. Nonetheless, the structural engineers of the World Trade Center ended up following draft versions of the new 1968 building codes.[44]\\r\\nThe tube-frame design, earlier introduced by Fazlur Khan, was a new approach that allowed more open floor plans than the traditional design that distributed columns throughout the interior to support building loads. The World Trade Center towers used high-strength, load-bearing perimeter steel columns called Vierendeel trusses that were spaced closely together to form a strong, rigid wall structure, supporting virtually all lateral loads such as wind loads, and sharing the gravity load with the core columns. The perimeter structure containing 59 columns per side was constructed with extensive use of prefabricated modular pieces, each consisting of three columns, three stories tall, connected by spandrel plates.[45] The spandrel plates were welded to the columns to create the modular pieces off-site at the fabrication shop.[46] Adjacent modules were bolted together with the splices occurring at mid-span of the columns and spandrels. The spandrel plates were located at each floor, transmitting shear stress between columns, allowing them to work together in resisting lateral loads. The joints between modules were staggered vertically, so that the column splices between adjacent modules were not at the same floor.[44]\\r\\nThe core of the towers housed the elevator and utility shafts, restrooms, three stairwells, and other support spaces. The core of each tower was a rectangular area 87 by 135?feet (27 by 41?m) and contained 47 steel columns running from the bedrock to the top of the tower. The large, column-free space between the perimeter and core was bridged by prefabricated floor trusses. The floors supported their own weight as well as live loads, providing lateral stability to the exterior walls and distributing wind loads among the exterior walls.[47] The floors consisted of 4 inches (10?cm) thick lightweight concrete slabs laid on a fluted steel deck. A grid of lightweight bridging trusses and main trusses supported the floors.[48] The trusses connected to the perimeter at alternate columns and were on 6?foot 8?inch (2.03?m) centers. The top chords of the trusses were bolted to seats welded to the spandrels on the exterior side and a channel welded to the core columns on the interior side. The floors were connected to the perimeter spandrel plates with viscoelastic dampers that helped reduce the amount of sway felt by building occupants.\\r\\nHat trusses (or \\"outrigger truss\\") located from the 107th floor to the top of the buildings were designed to support a tall communication antenna on top of each building.[48] Only 1?WTC (north tower) actually had an antenna fitted; it was added in 1978.[49] The truss system consisted of six trusses along the long axis of the core and four along the short axis. This truss system allowed some load redistribution between the perimeter and core columns and supported the transmission tower.[48]\\r\\nThe tube frame design, using steel core and perimeter columns protected with sprayed-on fire resistant material, created a relatively lightweight structure that would sway more in response to the wind compared to traditional structures, such as the Empire State Building that have thick, heavy masonry for fireproofing of steel structural elements.[21](p138) During the design process, wind tunnel tests were done to establish design wind pressures that the World Trade Center towers could be subjected to and structural response to those forces.[50] Experiments also were done to evaluate how much sway occupants could comfortably tolerate; however, many subjects experienced dizziness and other ill effects.[21](p139ÿ144) One of the chief engineers Leslie Robertson worked with Canadian engineer Alan G. Davenport to develop viscoelastic dampers to absorb some of the sway. These viscoelastic dampers, used throughout the structures at the joints between floor trusses and perimeter columns along with some other structural modifications, reduced the building sway to an acceptable level.[21](p160ÿ167)\\r\\nIn March 1965, the Port Authority began acquiring property at the World Trade Center site.[27] Demolition work began on March 21, 1966, to clear thirteen square blocks of low rise buildings in Radio Row for construction of the World Trade Center.[28] Groundbreaking for the construction of the World Trade Center took place on August 5, 1966.[51]\\r\\nThe site of the World Trade Center was located on landfill with the bedrock located 65 feet (20?m) below.[52] To construct the World Trade Center, it was necessary to build a \\"bathtub\\" with a slurry wall around the West Street side of the site, to keep water from the Hudson River out.[53] The slurry method selected by Port Authority's chief engineer, John M. Kyle, Jr., involved digging a trench, and as excavation proceeded, filling the space with a \\"slurry\\" mixture composed of bentonite and water, which plugged holes and kept groundwater out. When the trench was dug out, a steel cage was inserted and concrete was poured in, forcing the \\"slurry\\" out. It took fourteen months for the slurry wall to be completed. It was necessary before excavation of material from the interior of the site could begin.[54] The 1,200,000 cubic yards (920,000?m3) of material excavated were used (along with other fill and dredge material) to expand the Manhattan shoreline across West Street to form Battery Park City.[55][56]\\r\\nIn January 1967, the Port Authority awarded $74?million in contracts to various steel suppliers, and Karl Koch was hired to erect the steel.[57] Tishman Realty & Construction was hired in February 1967 to oversee construction of the project.[58] Construction work began on the North Tower in August 1968; construction on the South Tower was under way by January 1969.[59] The original Hudson Tubes, carrying PATH trains into Hudson Terminal, remained in service as elevated tunnels during the construction process until 1971 when a new PATH station opened.[60]\\r\\nThe topping out ceremony of 1?WTC (North Tower) took place on December 23, 1970, while 2?WTC's ceremony (South Tower) occurred later on July 19, 1971.[59] The first tenants moved into the North Tower on December 15, 1970;[1] the South Tower accepted tenants in January 1972.[61] When the World Trade Center twin towers were completed, the total costs to the Port Authority had reached $900?million.[62] The ribbon cutting ceremony was on April 4, 1973.[63]\\r\\nIn addition to the twin towers, the plan for the World Trade Center complex included four other low-rise buildings, which were built in the early 1970s. The 47-story 7?World Trade Center building was added in the 1980s, to the north of the main complex. Altogether, the main World Trade Center complex occupied a 16-acre (65,000?m2) superblock.[64]\\r\\nPlans to build the World Trade Center were controversial. The site for the World Trade Center was the location of Radio Row, home to hundreds of commercial and industrial tenants, property owners, small businesses, and approximately 100 residents, many of whom fiercely resisted forced relocation.[65] A group of small businesses affected sought an injunction challenging the Port Authority's power of eminent domain.[66] The case made its way through the court system to the United States Supreme Court; the Court refused to accept the case.[67]\\r\\nPrivate real estate developers and members of the Real Estate Board of New York, led by Empire State Building owner Lawrence A. Wien, expressed concerns about this much \\"subsidized\\" office space going on the open market, competing with the private sector when there was already a glut of vacancies.[68][69] The World Trade Center itself was not rented out completely until after 1979 and then only due to the fact that the complex's subsidy by the Port Authority made rents charged for its office space relatively cheaper than that of comparable office space in other buildings.[70] Others questioned whether the Port Authority should have taken on a project described by some as a \\"mistaken social priority\\".[71]\\r\\nThe World Trade Center design brought criticism of its aesthetics from the American Institute of Architects and other groups.[37][72] Lewis Mumford, author of The City in History and other works on urban planning, criticized the project and described it and other new skyscrapers as \\"just glass-and-metal filing cabinets\\".[73] The Twin Towers were described as looking similar to \\"the boxes that the Empire State Building and the Chrysler Building came in\\".[74] The twin towers' narrow office windows, only 18 inches (46?cm) wide and framed by pillars that restricted views on each side to narrow slots, were disliked by many.[36] Activist and sociologist Jane Jacobs also criticized plans for the WTC's construction, arguing that the waterfront should be kept open for New Yorkers to enjoy.[75]\\r\\nThe trade center's \\"superblock\\", replacing a more traditional, dense neighborhood, was regarded by some critics as an inhospitable environment that disrupted the complicated traffic network typical of Manhattan. For example, in his book The Pentagon of Power, Lewis Mumford denounced the center as an \\"example of the purposeless giantism and technological exhibitionism that are now eviscerating the living tissue of every great city\\".[76]\\r\\nFor many years, the immense Austin J. Tobin Plaza was often beset by brisk winds at ground level owing to the Venturi effect between the two towers.[77] In fact, some gusts were so high that pedestrian travel had to be aided by ropes.[78] In 1999, the outdoor plaza reopened after undergoing $12?million renovations, which involved replacing marble pavers with gray and pink granite stones, adding new benches, planters, new restaurants, food kiosks and outdoor dining areas.[79]\\r\\nOn a typical weekday 50,000 people worked in the towers[80] with another 200,000 passing through as visitors.[81] The complex was so large that it had its own zip code: 10048.[82] The towers offered expansive views from the observation deck atop the South Tower and the Windows on the World restaurant on top of the North Tower. The Twin Towers became known worldwide, appearing in numerous movies and television shows as well as on postcards and other merchandise, and became seen as a New York icon, in the same league as the Empire State Building, Chrysler Building and the Statue of Liberty.[83]\\r\\nOne World Trade Center and Two World Trade Center, commonly the Twin Towers, the idea of which was brought up by Minoru Yamasaki, were designed as framed tube structures, which provided tenants with open floor plans, uninterrupted by columns or walls.[84][85] They were the main buildings of the World Trade Center.[59] The North Tower (One World Trade Center), the tallest building in the world at 1,368 feet (417?m) by the time of its completion, began construction in 1966 with the South Tower (2 World Trade Center);[86] extensive use of prefabricated components helped to speed up the construction process, and the first tenants moved into the North Tower in December 1970, while it was still under construction.[87][88] When completed in 1973, the South Tower, Two World Trade Center (the South Tower) became the second tallest building in the world at 1,362 feet (415?m); the South Tower's rooftop observation deck was 1,362?ft (415?m) high and its indoor observation deck was 1,310?ft (400?m) high.[89] Each tower stood over 1,350 feet (410?m) high, and occupied about 1 acre (4,000?m2) of the total 16 acres (65,000?m2) of the site's land. During a press conference in 1973, Yamasaki was asked, \\"Why two 110-story buildings? Why not one 220-story building?\\" His tongue-in-cheek response was: \\"I didn't want to lose the human scale.\\"[90]\\r\\nWhen completed in 1972, 1?World Trade Center became the tallest building in the world for two years, surpassing the Empire State Building after a 40-year reign. The North Tower stood 1,368 feet (417?m) tall and featured a telecommunications antenna or mast that was added at the top of the roof in 1978 and stood 362 feet (110?m) tall. With the 362-foot (110?m)-tall antenna/mast, the highest point of the North Tower reached 1,730 feet (530?m).[89] Chicago's Sears Tower, finished in May 1973, reached 1,450 feet (440?m) at the rooftop.[91] Throughout their existence, the WTC towers had more floors (at 110) than any other building.[89] This number was not surpassed until the advent of the Burj Khalifa, which opened in 2010.[92][93]\\r\\nAlthough most of the space in the World Trade Center complex was off-limits to the public, the South Tower featured an indoor and outdoor public observation area called Top of the World Trade Center Observatories on its 107th and 110th floors. Visitors would pass through security checks added after the 1993 World Trade Center bombing,[95] then were sent to the 107th floor indoor observatory at a height of 1,310 feet (400?m). The columns on each face of the building were narrowed on this level to allow 28?inches of glass between them. The Port Authority renovated the observatory in 1995, then leased it to Ogden Entertainment to operate. Attractions added to the observation deck included a simulated helicopter ride around the city. The 107th floor food court was designed with a subway car theme and featured Sbarro and Nathan's Famous Hot Dogs.[96][97] Weather permitting, visitors could take two short escalator rides up from the 107th floor viewing area to an outdoor viewing platform on the 110th floor at a height of 1,377?ft (420?m).[98] On a clear day, visitors could see up to 50 miles (80?km).[96] An anti-suicide fence was placed on the roof itself, with the viewing platform set back and elevated above it, requiring only an ordinary railing and leaving the view unobstructed, unlike the observation deck of the Empire State Building.[97]\\r\\nThe North Tower had a restaurant on its 106th and 107th floors called Windows on the World, which opened in April 1976. The restaurant was developed by Joe Baum at a cost of more than $17?million.[99] Aside from the main restaurant, two offshoots were located at the top of the North Tower: \\"Hors d'Oeuvrerie\\" (offered a Danish smorgasbord during the day and sushi in the evening) and \\"Cellar in the Sky\\" (a small wine bar).[100] Windows on the World also had a wine school program run by Kevin Zraly. Windows on the World was closed following the 1993 World Trade Center bombing.[99] Upon reopening in 1996, Hors d'Oeuvrerie and Cellar in the Sky were replaced with the \\"Greatest Bar on Earth\\" and \\"Wild Blue\\".[100] In 2000, its last full year of operation, Windows on the World reported revenues of $37?million, making it the highest-grossing restaurant in the United States.[101] The Skydive Restaurant, opened in 1976 on the 44th floor of the North Tower, was also operated by Windows on the World restaurant, but served only lunch.\\r\\nFive smaller buildings stood around the 16 acres (65,000?m2) block. One was the 22-floor hotel, which opened in 1981 as the Vista Hotel, and in 1995 became the Marriott World Trade Center (3?WTC) at the southwest corner of the site. Three low-rise buildings (4?WTC, 5?WTC, and 6?WTC) in the same hollow tube design as the towers also stood around the plaza. 6?World Trade Center, at the northwest corner, housed the United States Customs Service and the U.S. Commodities Exchange. 5?World Trade Center was located at the northeast corner above the PATH station and 4?World Trade Center was at the southeast corner. In 1987, a 47-floor office building called 7 World Trade Center was built north of the block. Beneath the World Trade Center complex was an underground shopping mall, which in turn had connections to various mass transit facilities including the New York City Subway system and the Port Authority's own PATH trains connecting Manhattan to New Jersey.[102]\\r\\nOne of the world's largest gold depositories was stored underneath the World Trade Center, owned by a group of commercial banks. The 1993 bombing detonated close to the vault.[103] Seven weeks after the September 11 attacks, $230?million in precious metals was removed from basement vaults of 4?WTC, which included 3,800 100-Troy-ounce 24 carat gold bars and 30,000 1,000-ounce silver bars.[104]\\r\\nOn February 13, 1975, a three-alarm fire broke out on the 11th floor of the North Tower. Fire spread through the tower to the 9th and 14th floors by igniting the insulation of telephone cables in a utility shaft that ran vertically between floors. Areas at the furthest extent of the fire were extinguished almost immediately and the original fire was put out in a few hours. Most of the damage was concentrated on the 11th floor, fueled by cabinets filled with paper, alcohol-based fluid for office machines, and other office equipment. Fireproofing protected the steel and there was no structural damage to the tower. In addition to damage caused by the fire on the 9th ÿ 14th floors, water from the extinguishing of the fires damaged a few floors below. At that time, the World Trade Center had no fire sprinkler systems.[7]\\r\\nThe first terrorist attack on the World Trade Center occurred on February 26, 1993, at 12:17?p.m. A Ryder truck filled with 1,500 pounds (680?kg) of explosives, planted by Ramzi Yousef, detonated in the underground garage of the North Tower.[8] The blast opened a 100?foot (30?m) hole through five sublevels with the greatest damage occurring on levels B1 and B2 and significant structural damage on level B3.[105] Six people were killed and 1,042 others were injured during escape attempts complicated by smoke infiltration from the base of the building up to the 93rd floor of both towers. Many people inside the North Tower were forced to walk down darkened stairwells that contained no emergency lighting, some taking two hours or more to reach safety.[106][107]\\r\\nYousef fled to Pakistan after the bombing but was arrested in Islamabad in February 1995, and was extradited back to the United States to face trial.[108] Sheikh Omar Abdel Rahman was convicted in 1996 for involvement in the bombing and other plots.[109] Yousef and Eyad Ismoil were convicted in November 1997 for their carrying out the bombing.[110] Four others had been convicted in May 1994 for their involvement in the 1993 bombing.[111] According to a presiding judge, the conspirators' chief aim at the time of the attack was to destabilize the north tower and send it crashing into the south tower, toppling both landmarks.[112]\\r\\nFollowing the bombing, floors that were blown out needed to be repaired to restore the structural support they provided to columns.[113] The slurry wall was in peril following the bombing and loss of the floor slabs that provided lateral support against pressure from Hudson River water on the other side. The refrigeration plant on sublevel B5, which provided air conditioning to the entire World Trade Center complex, was heavily damaged.[114] After the bombing, the Port Authority installed photoluminescent markings in the stairwells.[115] The fire alarm system for the entire complex needed to be replaced because critical wiring and signaling in the original system was destroyed.[116] As a memorial to the victims of the bombing of the tower, a reflecting pool was installed with the names of those who had been killed in the blast.[117] However, the memorial was destroyed following the September 11 attacks. Names of the victims of the 1993 bombing are included in the National September 11 Memorial & Museum.\\r\\nIn January 1998, Mafia member Ralph Guarino, who had gained maintenance access to the World Trade Center, arranged a three-man crew for a heist that netted over $2?million from a Brinks delivery to the eleventh floor of the World Trade Center.[9]\\r\\nIn 1974, French high wire acrobatic performer Philippe Petit walked between the towers on a tightrope, as shown in the documentary film Man on Wire (2008), based on Petit's book To Reach the Clouds: My High Wire Walk Between the Twin Towers (2002)[118] (released in paperback with the title Man on Wire (2008)[119]) and depicted in the feature film The Walk (2015).[21]:219 Petit walked between the towers eight times on a steel cable.[120]\\r\\nIn 1977, Brooklyn toymaker George Willig scaled the exterior of the South Tower (2 WTC).[121]\\r\\nIn 1983, on Memorial Day, high-rise firefighting and rescue advocate Dan Goodwin successfully climbed the outside of the North Tower (1 WTC). His stunt was meant to call attention to the inability to rescue people potentially trapped in the upper floors of skyscrapers.[122][123]\\r\\nThe 1995 PCA world chess championship was played on the 107th floor of the South Tower.[124]\\r\\nIn 1998, the Port Authority approved plans to privatize the World Trade Center.[125] In 2001, the Port Authority sought to lease the World Trade Center to a private entity. Bids for the lease came from Vornado Realty Trust, a joint bid between Brookfield Properties Corporation and Boston Properties,[126] and a joint bid by Silverstein Properties and The Westfield Group.[10] By privatizing the World Trade Center, it would be added to the city's tax rolls[10] and provide funds for other Port Authority projects.[127] On February 15, 2001, the Port Authority announced that Vornado Realty Trust had won the lease for the World Trade Center, paying $3.25?billion for the 99-year lease.[128] Vornado outbid Silverstein by $600?million though Silverstein upped his offer to $3.22?billion. However, Vornado insisted on last minute changes to the deal, including a shorter 39-year lease, which the Port Authority considered nonnegotiable.[129] Vornado later withdrew and Silverstein's bid for the lease to the World Trade Center was accepted on April 26, 2001,[130] and closed on July 24, 2001.[131]\\r\\nOn September 11, 2001, Islamist terrorists hijacked American Airlines Flight 11 and crashed it into the northern fa?ade of the North Tower at 8:46:40?a.m., the aircraft striking between the 93rd and 99th floors. Seventeen minutes later, at 9:03:11?a.m., a second group crashed the similarly hijacked United Airlines Flight 175 into the southern facade of the South Tower, striking it between the 77th and 85th floors.[132] The damage caused to the North Tower by Flight 11 destroyed any means of escape from above the impact zone, trapping 1,344 people.[133] Flight 175 had a much more off-centered impact compared to Flight 11, and a single stairwell was left intact; however, only a few people managed to pass through it successfully before the tower collapsed. Although the South Tower was struck lower than the North Tower, thus affecting more floors, a smaller number, fewer than 700, were killed instantly or trapped.[134]\\r\\nAt 9:59?a.m., the South Tower collapsed after burning for approximately 56 minutes. The fire caused steel structural elements, already weakened from the plane impact, to fail. The North Tower collapsed at 10:28?a.m., after burning for approximately 102?minutes.[135] At 5:20?p.m.[136] on September 11, 2001, 7 World Trade Center started to collapse with the crumble of the east penthouse, and it collapsed completely at 5:21?p.m.[136] owing to uncontrolled fires causing structural failure.[137]\\r\\nThe 3 World Trade Center, a Marriott hotel, was destroyed during the collapse of the two towers. The three remaining buildings in the WTC plaza were extensively damaged by debris and later were demolished.[138] The Deutsche Bank Building across Liberty Street from the World Trade Center complex was later condemned owing to the uninhabitable toxic conditions inside; it was deconstructed, with work completed in early 2011.[139][140] The Borough of Manhattan Community College's Fiterman Hall at 30 West Broadway was also condemned owing to extensive damage in the attacks and is slated for deconstruction.[141]\\r\\nIn the immediate aftermath of the attacks, media reports suggested that tens of thousands might have been killed in the attacks, as over 50,000 people could be inside the World Trade Center, although the National Institute of Standards and Technology (NIST) estimated that approximately 17,400 occupants were in the towers at the time of the attacks.[142] Ultimately, 2,753 death certificates (excluding those for hijackers) were filed relating to the 9/11 attacks in New York City, including one filed for Felicia Dunn-Jones, who was added to the official death toll in May 2007; Dunn-Jones died five months later from a lung condition linked to exposure to dust during the collapse of the World Trade Center.[143] Three other victims were then added to the official death toll by the city medical examiner's office: Dr. Sneha Anne Philip, who was last seen the day before the attacks; Leon Heyward, a man who developed lymphoma and subsequently died in 2008 as a result of dust ingestion during the events following the attacks to the Twin Towers;[144][145] and Jerry Borg, who died in December 2010 of pulmonary sarcoidosis determined in June 2011 to be the result of dust from the attacks.[11] 2,192 civilians died in and around the World Trade Center, including 658 employees of Cantor Fitzgerald L.P. (an investment bank on the 101stÿ105th floors of One World Trade Center),[146] 295 employees of Marsh & McLennan Companies (which was located immediately below Cantor Fitzgerald on floors 93ÿ101, the location of Flight 11's impact), and 175 employees of Aon Corporation.[147] In addition to the civilian deaths, 343 New York City Fire Department (FDNY) firefighters were killed in the attacks, as well as 71 law enforcement officers, including 37 members of the Port Authority Police Department (PAPD) and 23 members of the New York City Police Department (NYPD).[148][149][150] Ten years after the attacks, remains of only 1,629 victims had been identified.[151] Of all the people who were still in the towers when they collapsed, only 20 were pulled out alive.[152]\\r\\nOver the following years, plans were created for the reconstruction of the World Trade Center. The Lower Manhattan Development Corporation (LMDC), established in November 2001 to oversee the rebuilding process,[153] organized competitions to select a site plan and memorial design.[154] Memory Foundations, designed by Daniel Libeskind, was selected as the master plan;[155] however, substantial changes were made to the design.[156]\\r\\nThe first new building at the site was 7?WTC, which opened in May 2006.[157] The memorial section of the National September 11 Memorial & Museum opened on September 11, 2011[158] and the museum opened in May 2014.[159] 1?WTC opened on November 3, 2014;[15] 4?WTC opened on November 13, 2013;[160] and 3?WTC is under construction and expected to open in 2017.[161][162] As of November 2013[update], according to an agreement made with Silverstein Properties Inc., the new 2?WTC will not be built to its full height until sufficient leasing is established to make the building financially viable.[163] In Summer 2015, Silverstein Properties revealed plans for a redesigned Tower 2 with News Corp as the core tenant; the Bjarke Ingels-designed structure was expected to be finished by 2020.[164] 5?WTC will be developed by the Port Authority of New York and New Jersey, but, as of February 2014, a schedule was not confirmed.[165]","input":"When was the old world trade center built?"},{"output":"Carmel-by-the-Sea, California","context":"Mission San Carlos Borromeo del ro Carmelo, also known as the Carmel Mission or Mission Carmel, is a Roman Catholic mission church in Carmel-by-the-Sea, California. It is on the National Register of Historic Places and a U.S. National Historic Landmark. The mission was the headquarters of the Alta California missions headed by Saint Junpero Serra from 1770 until his death in 1784. It was also the seat of the second presidente, Father Fermin Francisco de Lasuen.\\r\\nThe mission buildings and lands were secularized by the Mexican government in 1833, and had fallen into disrepair by the mid-19th century. They were partially restored beginning in 1884.[13][14] In 1886 it was transferred from the Franciscans to the local diocese and has continued as a parish church since then. It is the only one of the California Missions to have its original bell tower dome.[citation needed]\\r\\n\\r\\n\\r\\nMission Carmel is the second mission built by Franciscan missionaries in Upper California. It was first established as Mission San Carlos Borromeo in Monterey, California near the native village of Tamo on June 3, 1770. It was named for Carlo Borromeo, Archbishop of Milan, Italy. It was the site of the first Christian confirmation in Alta California.[5] When the mission moved, the original building continued to operate as the Royal Presidio chapel and later became the current Cathedral of San Carlos Borromeo.\\r\\nPedro Fages, who served as military governor of Alta California between 1770 and 1774, kept his headquarters at the Presidio of Monterey, the capital of Alta California. He worked his men very harshly and was seen as a tyrant. Serra intervened on behalf of Fages' soldiers, and the two men did not get along.[15][16] The soldiers raped the Indian woman and kept them as concubines.[15] Serra wanted to put some distance between the missions neophytes and Fages' soldiers.\\r\\nSerra found that the land near the mouth of the Carmel River was better suited for farming.[17] In May 1771, Spain's viceroy approved Serra's petition to relocate the mission to its current location near the Carmelo River.[notes 1] The relocated mission was renamed Mission San Carlos Borromeo del Ro Carmelo.\\r\\nAfter the Carmel mission was moved to Carmel Valley, the Franciscans began to baptize some natives.[18] By the end of 1771, the population of mission was 15 with an additional 22 baptized Indians, out of a total population of northern California of 60.[17] Farming was not very productive and for several years the mission was dependent upon the arrival of supply ships.[17] Historian Jame Culleton wrote in 1950, \\"The summer of '73 came without bringing the supply ship. Neither Carmel nor Monterey was anything like self-supporting.\\"[17] To improve baptismal rates, they sought to convert key members of the Esselen and Rumsen tribes, including chiefs. This persuaded some Indians to follow them to the mission.\\r\\nThe Esselen and Ohlone Indians who lived near the mission were baptized and then forcibly relocated and conscripted into forced labor as plowmen, shepherds, cattle herders, blacksmiths, and carpenters on the mission. Disease, starvation, overwork, and torture decimated these tribes.[19]:114 Native neophyte laborers made the adobe bricks, roof tiles and tools needed to build the mission. In the beginning, the mission relied on bear meat from Mission San Antonio de Padua and supplies brought by ship from Mission San Diego de Alcal. In 1794, the population reached its peak of 927, but by 1823 the total had dwindled to 381.[citation needed] There was extensive \\"comingling of the Costanoan with peoples of different linguistic and cultural background during the mission period.\\"[18]\\r\\nOn November 20, 1818, French privateer Hip܇lito Bouchard raided the nearby Monterey Presidio before moving on to other Spanish installations in the south.\\r\\nThe mission was secularized by the newly independent Mexican government in August 1833 with the stipulation that half the mission lands would be awarded to the native people. This purpose was largely subverted however. The priests could not maintain the missions without the Indians' forced labor and they were soon abandoned. The Indians drifted from the mission. Some found work on farms and ranches in exchange for room and board.\\r\\nThe Carmel Mission was in ruins when the Roman Catholic Church regained control of the mission building and the land in 1863. In 1884 Father Angel Casanova began to restore the mission building. In 1931 Monsignor Philip Scher appointed Harry Downie as the curator in charge of mission restoration. Two years later, the church transferred the mission from the Franciscans to the local diocese and it became a regular parish church. In 1960, the mission was designated as a minor basilica by Pope John XXIII. Downie labored for virtually the rest of his life to restore the mission, ancillary buildings and walls, and the grounds. In 1987, Pope John Paul II visited the mission as part of his U.S. tour.[20][21]\\r\\n\\"Mission Carmel\\", as it came to be known, was Serra's favorite[22] and, because it was close to Monterey, the capital of Alta California, he chose it as his headquarters. When he died on August 28, 1784, he was interred beneath the chapel floor.\\r\\nAs a result of Downie's dedicated efforts to restore the buildings, the Carmel mission church is one of the most authentically restored of all the mission churches in California. Mission Carmel has been designated a National Historic Landmark by the National Park Service. It is an active parish church of the Roman Catholic Diocese of Monterey.[23]\\r\\nIn addition to its activity as a place of worship, Mission Carmel also hosts concerts, art exhibits, lectures and numerous other community events. In 1986, then-pastor Monsignor Eamon MacMahon acquired a magnificent Casavant Frres organ complete with horizontal trumpets. Its hand-painted casework is decorated with elaborate carvings and statuary reflecting the Spanish decorative style seen on the main altar.\\r\\nThe mission also serves as a museum, preserving its own history and the history of the area. There are four specific museum galleries: the Harry Downie Museum, describing restoration efforts; the Munras Family Heritage Museum, describing the history of one of the most important area families; the Jo Mora Chapel Gallery, hosting rotating art exhibits as well as the monumental bronze and travertine cenotaph (1924) sculpted by Jo Mora;[24] and the Convento Museum, which holds the cell Serra lived and died in, as well as interpretive exhibits. At one end of the museum is a special chapel room containing some of the vestments used by Serra.[25][26][27]\\r\\nThe mission grounds are also the location of the Junipero Serra School, a private Catholic school for kindergartners through 8th grade.[28]\\r\\nSeveral notable people are buried in the church and churchyard.\\r\\nAsistencias\\r\\nEstancias\\r\\nSee also","input":"Where was mission san carlos borromeo de carmelo located?"},{"output":"magnetos","context":"A dynamo is an electrical generator that produces direct current with the use of a commutator. Dynamos were the first electrical generators capable of delivering power for industry, and the foundation upon which many other later electric-power conversion devices were based, including the electric motor, the alternating-current alternator, and the rotary converter. Today, the simpler alternator dominates large scale power generation, for efficiency, reliability and cost reasons. A dynamo has the disadvantages of a mechanical commutator. Also, converting alternating to direct current using power rectification devices (vacuum tube or more recently solid state) is effective and usually economical.\\r\\n\\r\\n\\r\\nThe word dynamo (from the Greek word dynamis(Ѵ?ϫ?), meaning force or power) was originally another name for an electrical generator, and still has some regional usage as a replacement for the word generator. The word \\"dynamo\\" was coined by Werner von Siemens in 1882.[1] The original \\"dynamo principle\\" of W. Siemens meant only the direct current generators which use exclusively the self-excitation (self-induction) principle to generate DC power. The earlier DC generators which used permanent magnets were not considered \\"dynamo electric machines\\".[2] The invention of the Dynamo principle (self-induction) was a huge technological leap over the old traditional permanent magnet based DC generators. The discovery of the dynamo principle made industrial scale electric power generation technically and economically feasible. After the invention of the alternator and that alternating current can be used as a power supply, the word dynamo became associated exclusively with the commutated direct current electric generator, while an AC electrical generator using either slip rings or rotor magnets would become known as an alternator.\\r\\nA small electrical generator built into the hub of a bicycle wheel to power lights is called a hub dynamo, although these are invariably AC devices,[citation needed] and are actually magnetos.\\r\\nThe electric dynamo uses rotating coils of wire and magnetic fields to convert mechanical rotation into a pulsing direct electric current through Faraday's law of induction. A dynamo machine consists of a stationary structure, called the stator, which provides a constant magnetic field, and a set of rotating windings called the armature which turn within that field. Due to Faraday's law of induction the motion of the wire within the magnetic field creates an electromotive force which pushes on the electrons in the metal, creating an electric current in the wire. On small machines the constant magnetic field may be provided by one or more permanent magnets; larger machines have the constant magnetic field provided by one or more electromagnets, which are usually called field coils.\\r\\nThe commutator is needed to produce direct current. When a loop of wire rotates in a magnetic field, the magnetic flux through it, and thus the potential induced in it, reverses with each half turn, generating an alternating current. However, in the early days of electric experimentation, alternating current generally had no known use. The few uses for electricity, such as electroplating, used direct current provided by messy liquid batteries. Dynamos were invented as a replacement for batteries. The commutator is essentially a rotary switch. It consists of a set of contacts mounted on the machine's shaft, combined with graphite-block stationary contacts, called \\"brushes\\", because the earliest such fixed contacts were metal brushes. The commutator reverses the connection of the windings to the external circuit when the potential reverses, so instead of alternating current, a pulsing direct current is produced.\\r\\nThe earliest dynamos used permanent magnets to create the magnetic field. These were referred to as \\"magneto-electric machines\\" or magnetos.[3] However, researchers found that stronger magnetic fields, and so more power, could be produced by using electromagnets (field coils) on the stator.[4] These were called \\"dynamo-electric machines\\" or dynamos.[3] The field coils of the stator were originally separately excited by a separate, smaller, dynamo or magneto. An important development by Wilde and Siemens was the discovery (by 1866) that a dynamo could also bootstrap itself to be self-excited, using current generated by the dynamo itself. This allowed the growth of a much more powerful field, thus far greater output power.\\r\\nSelf-excited direct current dynamos commonly have a combination of series and parallel (shunt) field windings which are directly supplied power by the rotor through the commutator in a regenerative manner. They are started and operated in a manner similar to modern portable alternating current electric generators, which are not used with other generators on an electric grid.\\r\\nThere is a weak residual magnetic field that persists in the metal frame of the device when it is not operating, which has been imprinted onto the metal by the field windings. The dynamo begins rotating while not connected to an external load. The residual magnetic field induces a very small electrical current into the rotor windings as they begin to rotate. Without an external load attached, this small current is then fully supplied to the field windings, which in combination with the residual field, cause the rotor to produce more current. In this manner the self-exciting dynamo builds up its internal magnetic fields until it reaches its normal operating voltage. When it is able to produce sufficient current to sustain both its internal fields and an external load, it is ready to be used.\\r\\nA self-excited dynamo with insufficient residual magnetic field in the metal frame will not be able to produce any current in the rotor, regardless of what speed the rotor spins. This situation can also occur in modern self-excited portable generators, and is resolved for both types of generators in a similar manner, by applying a brief direct current battery charge to the output terminals of the stopped generator. The battery energizes the windings just enough to imprint the residual field, to enable building up the current. This is referred to as flashing the field.\\r\\nBoth types of self-excited generator, which have been attached to a large external load while it was stationary, will not be able to build up voltage even if the residual field is present. The load acts as an energy sink and continuously drains away the small rotor current produced by the residual field, preventing magnetic field buildup in the field coil.\\r\\nThe operating principle of electromagnetic generators was discovered in the years 1831ÿ1832 by Michael Faraday. The principle, later called Faraday's law, is that an electromotive force is generated in an electrical conductor which encircles a varying magnetic flux.\\r\\nHe also built the first electromagnetic generator, called the Faraday disk, a type of homopolar generator, using a copper disc rotating between the poles of a horseshoe magnet. It produced a small DC voltage. This was not a dynamo in the current sense, because it did not use a commutator.\\r\\nThis design was inefficient, due to self-cancelling counterflows of current in regions of the disk that were not under the influence of the magnetic field. While current was induced directly underneath the magnet, the current would circulate backwards in regions that were outside the influence of the magnetic field. This counterflow limited the power output to the pickup wires, and induced waste heating of the copper disc. Later homopolar generators would solve this problem by using an array of magnets arranged around the disc perimeter to maintain a steady field effect in one current-flow direction.\\r\\nAnother disadvantage was that the output voltage was very low, due to the single current path through the magnetic flux. Faraday and others found that higher, more useful voltages could be produced by winding multiple turns of wire into a coil. Wire windings can conveniently produce any voltage desired by changing the number of turns, so they have been a feature of all subsequent generator designs, requiring the invention of the commutator to produce direct current.\\r\\nThe first dynamo based on Faraday's principles was built in 1832 by Hippolyte Pixii, a French instrument maker. It used a permanent magnet which was rotated by a crank. The spinning magnet was positioned so that its north and south poles passed by a piece of iron wrapped with insulated wire.\\r\\nPixii found that the spinning magnet produced a pulse of current in the wire each time a pole passed the coil. However, the north and south poles of the magnet induced currents in opposite directions. To convert the alternating current to DC, Pixii invented a commutator, a split metal cylinder on the shaft, with two springy metal contacts that pressed against it.\\r\\nThis early design had a problem: the electric current it produced consisted of a series of \\"spikes\\" or pulses of current separated by none at all, resulting in a low average power output. As with electric motors of the period, the designers did not fully realize the seriously detrimental effects of large air gaps in the magnetic circuit.\\r\\nAntonio Pacinotti, an Italian physics professor, solved this problem around 1860 by replacing the spinning two-pole axial coil with a multi-pole toroidal one, which he created by wrapping an iron ring with a continuous winding, connected to the commutator at many equally spaced points around the ring; the commutator being divided into many segments. This meant that some part of the coil was continually passing by the magnets, smoothing out the current.[5]\\r\\nThe Woolrich Electrical Generator of 1844, now in Thinktank, Birmingham Science Museum, is the earliest electrical generator used in an industrial process.[6] It was used by the firm of Elkingtons for commercial electroplating.[7][8][9]\\r\\nIndependently of Faraday, the Hungarian Anyos Jedlik started experimenting in 1827 with the electromagnetic rotating devices which he called electromagnetic self-rotors. In the prototype of the single-pole electric starter, both the stationary and the revolving parts were electromagnetic.\\r\\nAbout 1856 he formulated the concept of the dynamo about six years before Siemens and Wheatstone but did not patent it as he thought he was not the first to realize this. His dynamo used, instead of permanent magnets, two electromagnets placed opposite to each other to induce the magnetic field around the rotor.[10][11] It was also the discovery of the principle of dynamo self-excitation,[12] which replaced permanent magnet designs.\\r\\nThe dynamo was the first electrical generator capable of delivering power for industry. The modern dynamo, fit for use in industrial applications, was invented independently by Sir Charles Wheatstone, Werner von Siemens and Samuel Alfred Varley. Varley took out a patent on 24 December 1866, while Siemens and Wheatstone both announced their discoveries on 17 January 1867, the latter delivering a paper on his discovery to the Royal Society.\\r\\nThe \\"dynamo-electric machine\\" employed self-powering electromagnetic field coils rather than permanent magnets to create the stator field.[13] Wheatstone's design was similar to Siemens', with the difference that in the Siemens design the stator electromagnets were in series with the rotor, but in Wheatstone's design they were in parallel.[14] The use of electromagnets rather than permanent magnets greatly increased the power output of a dynamo and enabled high power generation for the first time. This invention led directly to the first major industrial uses of electricity. For example, in the 1870s Siemens used electromagnetic dynamos to power electric arc furnaces for the production of metals and other materials.\\r\\nThe dynamo machine that was developed consisted of a stationary structure, which provides the magnetic field, and a set of rotating windings which turn within that field. On larger machines the constant magnetic field is provided by one or more electromagnets, which are usually called field coils.\\r\\nZnobe Gramme reinvented Pacinotti's design in 1871 when designing the first commercial power plants operated in Paris. An advantage of Gramme's design was a better path for the magnetic flux, by filling the space occupied by the magnetic field with heavy iron cores and minimizing the air gaps between the stationary and rotating parts. The Gramme dynamo was one of the first machines to generate commercial quantities of power for industry.[15] Further improvements were made on the Gramme ring, but the basic concept of a spinning endless loop of wire remains at the heart of all modern dynamos.[16]\\r\\nCharles F. Brush assembled his first dynamo in the summer of 1876 using a horse-drawn treadmill to power it. Brush's design modified the Gramme dynamo by shaping the ring armature like a disc rather than a cylinder shape. The field electromagnets were also positioned on the sides of the armature disc rather than around the circumference.[17][18]\\r\\nAfter dynamos and motors were found to allow easy conversion back and forth between mechanical or electrical power, they were combined in devices called rotary converters, rotating machines whose purpose was not to provide mechanical power to loads but to convert one type of electric current into another, for example DC into AC. They were multi-field single-rotor devices with two or more sets of rotating contacts (either commutators or sliprings, as required), one to provide power to one set of armature windings to turn the device, and one or more attached to other windings to produce the output current.\\r\\nThe rotary converter can directly convert, internally, any type of electric power into any other. This includes converting between direct current (DC) and alternating current (AC), three phase and single phase power, 25?Hz AC and 60?Hz AC, or many different output voltages at the same time. The size and mass of the rotor was made large so that the rotor would act as a flywheel to help smooth out any sudden surges or dropouts in the applied power.\\r\\nThe technology of rotary converters was replaced in the early 20th century by mercury-vapor rectifiers, which were smaller, did not produce vibration and noise, and required less maintenance. The same conversion tasks are now performed by solid state power semiconductor devices. Rotary converters remained in use in the West Side IRT subway in Manhattan into the late 1960s, and possibly some years later. They were powered by 25?Hz AC, and provided DC at 600 volts for the trains.\\r\\nDynamos, usually driven by steam engines, were widely used in power stations to generate electricity for industrial and domestic purposes. They have since been replaced by alternators.\\r\\nLarge industrial dynamos with series and parallel (shunt) windings can be difficult to use together in a power plant, unless either the rotor or field wiring or the mechanical drive systems are coupled together in certain special combinations. Without taking care to configure the dynamos in this manner, their combined power outputs can become imbalanced in various ways such that one dynamo drives the others as a motor, leading to out of control racing and overspeed. [19]\\r\\nThis can cause serious damage to the dynamos and power plant, potentially resulting in centrifugal warping and collision of the rotor windings with the field coils, shaft bearing overspeed and failure, and damage to the driving engine, the engine governor, or the rotary power transmission system.\\r\\nDynamos were used in motor vehicles to generate electricity for battery charging. An early type was the third-brush dynamo. They have, again, been replaced by alternators.\\r\\nDynamos still have some uses in low power applications, particularly where low voltage DC is required, since an alternator with a semiconductor rectifier can be inefficient in these applications.\\r\\nHand cranked dynamos are used in clockwork radios, hand powered flashlights, mobile phone rechargers, and other human powered equipment to recharge batteries.","input":"What type of device is a bicycle dynamo?"},{"output":"August 14, 2006","context":"Abigail \\"Abby\\" Cadabby, mostly referred to as just Abby, is a 4-year-old character in the PBS children's television show Sesame Street. She is a Muppet. In 2006, Abby made her debut in the first episode of Sesame Streets 37th season, when she moved into the neighborhood and met some of the Street's residents. On the day of her debut, her wand broke; Big Bird told her to take her wand to the Fix-It Shop where Maria would fix it. Season 40 features her CGI-animated segment \\"Abby's Flying Fairy School\\".\\r\\nHer name is a pun of the magic word Abracadabra. Abby's magical powers are limited to popping in and out of thin air, floating when she's happy, and turning things into pumpkins. Although familiar with the world of fairy tales, Abby is astounded by such basic learning skills as drawing letters or counting, prompting her catchphrase \\"That's so magical!\\" She frequently uses her wand cell phone to call her mommy. When she's asked to return home, she says that she's \\"gotta poof.\\" She can speak a language called Dragonfly and is teaching Rosita the language, while Rosita teaches her Spanish. Along with Baby Bear, Abby begins attending school in a 2006 episode at the Storybook Community School, where Mrs. Goose is the teacher and other fairy tale characters like Hansel and Gretel are her classmates.\\r\\nTony Geiss conceptualized Abby as a way to simultaneously introduce a major female character to the show and add someone from a different culture, without \\"having consciously to introduce somebody from Indonesia or India.\\" Abby's design is an intentional departure from the typical Muppet look because she's not originally from Sesame Street. The implication is that the fairies in her old neighborhood look like her.\\r\\nAbby's likeness has been adapted for a 43-foot balloon which premiered in the 2007 Macy's Thanksgiving Day Parade, a full-body costume character for stage appearances and a number of merchandise items.\\r\\nIn 2008, Abby was added to the cast of Plaza Ssamo, the Mexican co-production of Sesame Street, appearing in new segments where she tries to perform magic tricks with various ordinary objects. In 2009, she became the host of 3, 2, 1 Vamos!, a Latin American pre-school programming block, which first aired in English in 2010, on Canadian television.\\r\\n\\r\\n\\r\\nAbby Cadabby was officially announced in TV Guide, months before the 37th season debut of Sesame Street, and a press kit was issued soon after.[2] Rumors had floated on the Internet before that, with an anonymous Sesame Workshop or Muppet insider revealing the addition on Muppet Wiki.[citation needed]\\r\\nShe made her Street debut on August 14, 2006.\\r\\nAbby was scheduled to be interviewed 10 August 2006 on The Today Show on NBC; \\"NBC Special Report\\" coverage of the 2006 transatlantic aircraft plot pre-empted the interview. Abby appeared for a short chat on 14 August, a few short minutes with Bobbie Thomas and Hoda Kotb before the weather. The scheduled \\"satellite-tour\\" of local stations across America went on later that day.[3]\\r\\nAbby was to be featured as ABC World News \\"Person of the Week\\" on 11 August 2006; Hurricane Katrina/2006 Israel-Lebanon conflict dog rescuer Linda Nealon pre-empted her.[4] Cadabby was featured on the 18 August broadcast, with the teaser, \\"Femininity comes to Sesame Street\\".[5] She was touted as the first 3-year-old \\"Person of The Week\\".\\r\\nAbby appeared on the August 11 edition of All Things Considered on NPR. [6]\\r\\nIn 2007, Abby had her own balloon made after her in the 2007 Macy's Thanksgiving Day Parade. In 2008, Abby starred as 'Alice' in the Sesame Street direct-to-DVD film Abby in Wonderland that adapts Lewis Carroll's Alice in Wonderland. Abby is also now featured in the 2007-2009 tour of Sesame Street Live show When Elmo Grows Up and also on the 2012-2013 leg of the \\"Elmo Makes Music\\" tour.\\r\\nIn 2014's direct-to-DVD film \\"Elmo's Super Numbers\\", Abby appears at NumberCon as \\"One-Da Woman\\", a numeric spoof and pun of the DC comic book female superhero, Wonder Woman.\\r\\nAbby is the host of Sesame Street's autism initiative.\\r\\nThe animation studio SpeakEasy FX, founded by director Scott Stewart, produces the \\"Abby's Flying Fairy School\\" series.[7]","input":"When did abby cadabby first appear on sesame street?"},{"output":"Trench warfare","context":"","input":"What type of warfare was used in wwi?"},{"output":"petroleum","context":"\\r\\n\\r\\nWith an economy worth $93.45 billion[18] $298.310 billion PPP[2] and a per capita GDP of about $4,310,[19] $13,480 PPP as of 2018[20] Sri Lanka has mostly had strong growth rates in recent years. The Sri Lankan economy has seen robust annual growth at 6.4 percent over the course of the 2003-2012 period, well above its regional peers. In GDP per capita terms, it is ahead of other countries in the South Asian region. Since the end of the three-decade civil war, Sri Lanka has begun focusing on long-term strategic and structural development challenges as it strives to transition to an upper middle income country.\\r\\n\\r\\nThe main economic sectors of the country are tourism, tea export, apparel, textile, rice production and other agricultural products. In addition to these economic sectors, overseas employment contributes highly in foreign exchange: 90% of expatriate Sri Lankans reside in the Middle East.[21]\\r\\n\\r\\nSri Lanka has met the Millennium Development Goal (MDG) target of halving extreme poverty and is on track to meet most of the other MDGs, outperforming other South Asian countries. Sri Lanka experienced a major decline in poverty between 2002 and 2009 ÿ from 23 percent to 9 percent of the population. Despite this pockets of poverty continue to exist. An estimated 9 percent of Sri Lankans who are no longer classified as poor live within 20 percent of the poverty line and are, thus, vulnerable to shocks which could cause them to fall back into poverty.[22]\\r\\n\\r\\nSri Lanka has one of the lowest tax-to-GDP ratios in the world, and creating jobs for the bottom 40% has become a challenge. Sri Lanka also faces a challenges in social inclusion, governance and sustainability.[23]\\r\\n\\r\\nAccording to government policies and economic reforms stated by Prime Minister and Minister of National Policy and economic affairs Ranil Wickremesinghe, Sri Lanka plans to create a knowledge-based social market economy and an export-oriented economy as well as the Western Region Megapolis a Megapolis in the western province to promote economic growth. The creation of several business and technology development areas island-wide specialised in various sectors, as well as tourism zones are also being planned.[24][25][26][27]\\r\\n\\r\\nSince becoming independent from Britain in February 1948, the economy of the country has been affected by natural disasters such as the 2004 Indian Ocean earthquake and a number of insurrections, such as the 1971, the 1987ÿ89 and the 1983ÿ2009 civil war. Between 1977 and 1994 the country came under UNP rule in which under President J.R Jayawardana Sri Lanka began to shift away from a socialist orientation in 1977. Since then, the government has been deregulating, privatizing, and opening the economy to international competition. In 2001, Sri Lanka faced bankruptcy, with debt reaching 101% of GDP. The impending currency crisis was averted after the country reached a hasty ceasefire agreement with the LTTE and brokered substantial foreign loans. After 2004 the UPFA government has concentrated on mass production of goods for domestic consumption such as rice, grain and other agricultural products.[28]\\r\\nhowever twenty five years of civil war slowed economic growth,[citation needed] diversification and liberalisation, and the political group Janatha Vimukthi Peramuna (JVP) uprisings, especially the second in the early 1980s, also caused extensive upheavals.[29]\\r\\n\\r\\nFollowing the quelling of the JVP insurrection, increased privatization, economic reform, and a stress on export-oriented growth helped improve the economic performance, increasing GDP growth to 7% in 1993.\\r\\n\\r\\nEconomic growth has been uneven in the ensuing years as the economy faced a multitude of global and domestic economic and political challenges. Overall, average annual GDP growth was 5.2% over 1991ÿ2000.\\r\\n\\r\\nIn 2001, however, GDP growth was negative 1.4%--the first contraction since independence. The economy was hit by a series of global and domestic economic problems and affected by terrorist attacks in Sri Lanka and the United States. The crises also exposed the fundamental policy failures and structural imbalances in the economy and the need for reforms. The year ended in parliamentary elections in December, which saw the election of United National Party to Parliament, while Sri Lanka Freedom Party retained the Presidency.\\r\\n\\r\\nDuring the short lived peace process from 2002 to 2004, the economy benefited from lower interest rates, a recovery in domestic demand, increased tourist arrivals, a revival of the stock exchange, and increased foreign direct investment (FDI). In 2002, the economy experienced a gradual recovery. During this period Sri Lanka has been able to reduce defense expenditures and begin to focus on getting its large, public sector debt under control. In 2002, economic growth reached 4%, aided by strong service sector growth. The agricultural sector of the economy staged a partial recovery. Total FDI inflows during 2002 were about $246 million[30]\\r\\n\\r\\nThe Mahinda Rajapakse government halted the privatization process and launched several new companies as well as re-nationalising previous state owned enterprises, one of which the courts declared that privatizationis null and void.[31] Some state-owned corporations became overstaffed and less efficient, making huge losses with series of frauds being uncovered in them and neopotism rising.[32] During this time EU revoked GSP plus preferential tariffs from Sri Lanka due to alleged human rights violations, which cost about USD 500 million a year.[33][34]\\r\\n\\r\\nThe resumption of the civil-war in 2005 led to a steep increase defense expenditures. The increased violence and lawlessness also prompted some donor countries to cut back on aid to the country.[1][2].\\r\\n\\r\\nA sharp rise in world petroleum prices combined with economic fallout from the civil war led to inflation that peaked 20%. However, as the civil war ended in May 2009 the economy started to grow at a higher rate of 8.0% in the year 2010 and reached 9.1% in 2012 mostly due to the boom in non-tradable sectors. However the boom didn't last and the GDP growth for 2013 fell to 3.4% in 2013 and only slightly recovered to 4.5% in 2014.[35][36][37][38]\\r\\n\\r\\nIn 2016 the government succeeded in lifting an EU ban on Sri Lankan fish products which resulted in fish exports to EU rising by 200% and in 2017 improving human rights conditions resulted in the European Commission proposing to restore GSP plus facility to Sri Lanka.[25][26][39][40] Sri Lanka's tax revenues per GDP also increased from 10% in 2014 which was the lowest in nearly two decades to 12.3% in 2015[41] Despite reforms, Sri Lanka was listed among countries with the highest risk for investors by Bloomberg.[42]\\r\\n\\r\\n\\r\\nThe chart below summarizes the trend of Sri Lanka's gross domestic product at market prices.[43] by the International Monetary Fund with figures in millions of Sri Lankan Rupees.\\r\\n\\r\\nFor purchasing power parity comparisons, the US Dollar is exchanged at 113.4 Sri Lankan Rupees only.\\r\\n\\r\\nIn 1977, Colombo abandoned statist economic policies and its import substitution trade policy for market-oriented policies and export-oriented trade.\\r\\n\\r\\nSri Lanka's most dynamic industries now are food processing, textiles and apparel, food and beverages, telecommunications, and insurance and banking.\\r\\n\\r\\nBy 1996 plantation crops made up only 20% of exports (compared with 93% in 1970), while textiles and garments accounted for 63%. GDP grew at an annual average rate of 5.5% throughout the 1990s until a drought and a deteriorating security situation lowered growth to 3.8% in 1996.\\r\\n\\r\\nThe economy rebounded in 1997ÿ98 with growth of 6.4% and 4.7% ÿ but slowed to 3.7% in 1999. For the next round of reforms, the central bank of Sri Lanka recommends that Colombo expand market mechanisms in nonplantation agriculture, dismantle the government's monopoly on wheat imports, and promote more competition in the financial sector.\\r\\n\\r\\nPre 2009 there was a continuing cloud over the economy the civil war and fighting between the Government of Sri Lanka and LTTE. However the war ended with a resounding victory for the Sri Lankan Government on 19 May 2009 with the total elimination of LTTE.\\r\\n\\r\\n2018E: 4.5%, 2019E: 4.8%\\r\\n\\r\\nFollowing a real GDP expansion of 3.1% in 2017, the economy is however anticipated to be in the \\"negative output gap\\" territory in 2018E and 2019E (i.e. below its potential output of 5.25% - estimated by the IMF)\\r\\n\\r\\nGlobal Environment not likely to assist in reducing the Negative Output Gap of Sri Lanka\\r\\n\\r\\nEven though the CBSL tried to adopt a soft monetary policy by early April 2018 to reduce the output gap of the Sri Lankan economy, the chances of the Regulator opting for further policy interest rate cuts seem to be limited at this juncture, given hawkish approach of the FED in the near term (as there could be a potential capital flight from domestic markets as already seen with the domestic fixed income market's foreign holding). The expected FED rate (at least more than a couple in 2H2018E) hikes are anticipated to continue to reduce foreign participation in the domestic fixed income markets as seen during previous rate hikes of the FED.\\r\\n\\r\\nClimatic Change Impact on South Asia to Hinder Agri allied output in the Short to Medium Term\\r\\n\\r\\nAnother reason why the Sri Lankan (real GDP) output is likely to be low in the near to medium term is low absolute output expected from Agriculture and Agri-allied manufacturing processes. There was a flood in May 2018 (followed by floods in May 2016 and May 2017), and based on the World Banks recent Annual Reports, world climatic change related impact may likely result in annual floods and / or droughts for the South Asian region in the near to medium term (that includes Sri Lanka).\\r\\n\\r\\nInvestment Environment and Sustainable Revenue allied Limitations\\r\\n\\r\\nSri Lanka's investment to GDP ratio (average for past five years) hovers ~ 31?% which is made up of 24% Private Sector Investment, 5% Public Investment (or Government Capex) and 2% via Foreign Direct Investment (FDIs). For an economy to reach a sustainable productive capacity (or real GDP growth rate) equivalent to 6% - 7%, the investment to GDP ratio has to at least increase to the level of ~34% - 35%, which requires an increase in the said investments level. Given Sri Lanka's natural savings rate of ~23% - 24%, private sector investment may continue to remain at 24% levels at least in the near term. Given Government of Sri Lanka's (GoSL) commitment to contain fiscal deficits in the near term, Public investment in GoSL' s best case will be at ~5% level in the short to medium term. This leaves room only for one area of focus in the near term, if Sri Lanka needs to increase its potential output level, which is FDIs. For this to improve, few indices like the \\"doing business index (Sri Lanka stood at 111 for 2018 which deteriorated its rank from 85th position during 2014)\\", the overall tariff structure & allied reforms and infrastructure project execution pace (to increase economic efficiency) play a key role. However, increasing FDI level to 5% - 6% in the medium term may require fast pacing the matters mentioned above.\\r\\n\\r\\nFrom a sustainable revenue point of view, Sri Lanka has continued to perform low, given its declining Export to GDP ratio, which was at 33% in 2000, that deteriorated to 15% in 2014 which has now (by end of 2017) further deteriorated to 12%, emphasizing the fact that policies by the successive Governments during past few decades have not helped in increasing the countrys sustainable revenue. A classic example for this is, during 1992, Sri Lankas absolute Export Value was on par with countries like Vietnam and Bangladesh (at US$2bn), which has only grown to a shade below US$12bn by end of 2017 (compared to Vietnams US$214bn and Bangladeshs US$36bn for 2017)\\r\\n\\r\\nHead line CCPI and NCPI will hover at 6% by 30 June 2019\\r\\n\\r\\nEven though the Government did not adopt the fuel pricing formula, the Government took action to increase domestically controlled Petrol and Diesel prices, that will eventually increase non-financial SOE gains (which were in the red zone up to 2015) in the near term. This reform (even though, it is a spot action) and the anticipated adoption of the electricity pricing reform or a similar adjustment to electricity prices by end of September 2018 (as agreed with IMF's reform agenda for Sri Lanka), may transfer Ceylon Petroleum Corporation - CPC and the Ceylon Electricity Board - CEB (two of the top three loss making SOEs ÿ other being Sri Lankan air lines) in to the green zone (profits), in SOE financials.\\r\\n\\r\\nThese changes may however, result in upward inflationary pressures in the short to medium term which will also be further slightly stimulated by supply side shocks anticipated via adverse weather related mishaps in the near term.\\r\\n\\r\\nHowever, given the tighter monetary approach in the world economy, commodity prices may likely ease at least commencing 2019, which will eventually help sustain imported inflation to the country. As a result inflation in Sri Lanka could be possibly contained at around 6% by mid-2019.\\r\\n\\r\\n12 Month T bill to be at 10% by 30 June 2019\\r\\n\\r\\nThe CBSL has reduced its T bill holding significantly from April 2017 to date reversing any monetary stimulated inflationary actions. Thus the resultant liquidity levels in the money market broadly reflects natural market conditions compared to the market that was there an year ago, which reflected more realistic banking sector interest rates as of June 2018.\\r\\n\\r\\nPrivate sector credit growth declined from high levels of 29% YoY in July 2016 to 15% YoY levels in 1Q2018. One of the main features of this decline includes a significant reduction in the uptake of credit from the Corporates, which is depicted by the declining Prime - Net Interest Margins (Prime - NIMs) since September 2016. Prime - NIMs declined from 5.2% in Sep 2016 to 2.2% in May 2018, which is a 300bps reduction in corporate lending business for the local banking system. However, retail margins in the economy declined only 30bps to 5% during the same period, which confirms the tilt of credit towards less credit worthy from high credit rated corporates (pausing possible NPA threats for the banking system in the near term).\\r\\n\\r\\nGiven the changes taking place in the private credit space (i.e the retail tilt), and provided the CBSL's recent policy rate cut in April 2018, credit growth may still continue to move either horizontally (i.e. at 15% level) or continue to reduce slightly given anticipated near term inflationary pressures, as the consumption led borrowings may also tend to decline on account of anticipated reduction in near term disposable income. This will however not add any excessive upward pressures on interest rates (including 12 month T bill yields) especially during 2H2018E. As a result 12 month Treasury bill yields may in fact slightly decline from its June 2018 --> 9.4% to 9% levels by end of 2018E. However, given the International Sovereign Bond (ISB) bullet payments >US$3bn p.a. commencing from 2019E may likely add some upward pressure on interest rates, resulting in the 12 month T bill yields rising to at least 10% by 30 June 2019.\\r\\n\\r\\nUS$ / LKR spot mid-rate anticipated at Rs.166.1 as at end June 2019\\r\\n\\r\\nThe CBSL became a net purchaser from the Forex market since March 2017, and thus managed to increase its Gross Official Foreign Reserve to ~US$10bn as at mid June 2018, as a result of its market driven ER strategy.\\r\\n\\r\\nHowever, given the relatively high elasticity1 of Sri Lanka's exports (e.g. Tea, Textile etc.) and relative high inelasticity1 of Sri Lanka's imports (such as crude oil etc.), and given near term ISB bullet payment requirements, The LKR is anticipated to depreciate -4.1% YoY to Rs.166.1/US$ as at end June 2019\\r\\n\\r\\n1?: MarshallÿLerner condition applicable to Sri Lanka\\r\\n\\r\\nThe author of this segment (Sanjeewa Fernando) is a Lecturer at the University of Colombo for Master of Financial Economics\\r\\n\\r\\nIn the recent past, the Sri Lankan Government has identified some key focal areas to address the external imbalances of the economy, especially with regard to reducing its high trade deficit (~15% of GDP for 2012) in order to make the economy comply with the MarshallÿLerner condition. Sri Lanka's oil import bill accounts for an estimated 27% of total imports while its pro-growth policies have resulted in an investment goods import component of 24% of total imports. These inelastic import components have led to Sri Lanka's Export goods price elasticity + Import goods price elasticity totalling less than 1, resulting in the country not complying with the MarshallÿLerner condition.\\r\\n\\r\\nSome of the suggested proposals include:\\r\\n\\r\\n[3]\\r\\n\\r\\nThe Central Bank of Sri Lanka is the monetary authority of Sri Lanka and was established in 1950. The Central Bank is responsible for the conduct of monetary policy in the country and also has supervisory powers over the financial system.[44]\\r\\n\\r\\nThe Colombo Stock Exchange (CSE) is the main stock exchange in Sri Lanka. It is one of the most modern exchanges in South Asia, providing a fully automated trading platform. The vision of the CSE is to contribute to the wealth of the nation by creating value through securities. The headquarters of the CSE have been located at the World Trade Center Towers [4] in Colombo since 1995 and it also has branches across the country in Kandy, Matara, Kurunegala, Negombo and Jaffna.[45] In 2009, after the 30 years long civil war came to an end, the CSE was the best performing stock exchange in the world.\\r\\n\\r\\nMost Sri Lankan cities and towns are connected by the Sri Lanka Railways, the state-run railway operator. The Sri Lanka Transport Board is the state-run agency responsible for operating public bus services across the island.\\r\\n\\r\\nThe government has launched several highway projects to bolster the economy and national transport system, including the Colombo-Katunayake Expressway, the Colombo-Kandy (Kadugannawa) Expressway, the Colombo-Padeniya Expressway and the Outer Circular Highway to ease Colombo's traffic congestion. The government sponsored Road Development Authority (RDA) has been involved in several large-scale projects all over the island in an attempt to improve the road network in Sri Lanka. Sri Lanka's commercial and economic centres, primarily the capitals of the nine provinces are connected by the \\"A-Grade\\" roads which are categorically organised and marked. Furthermore, \\"B-Grade\\" roads, also paved and marked, connect district capitals within provinces. The grand total of A, B and E grade roads are estimated at 12,379.49?km.[46]\\r\\n\\r\\nThe energy policy is governed by the Ministry of Power and Energy, while the production and retailing of electricity is carried out by the Ceylon Electricity Board. Policy recommendations and planning comes under the oversight of the Public Utilities Commission of Sri Lanka. Energy in Sri Lanka is mostly generated by hydroelectric power stations in the Central Province.[47][48]\\r\\n\\r\\nSri Lanka has a well established education system which has successfully created vast supply of skilled labor. The Sri Lanka's population has a literacy rate of 92%, higher than that expected for a third world country; it has the highest literacy rate in South Asia and overall, one of the highest literacy rates in Asia.[49] Information technology literacy of the urban sector population is also satisfactory at 39.9 percent and people around the country use web based job boards to find skilled employment together with other sources such as news papers and government gazette. In Sri Lanka all persons above age limit 15 years and above of either gender are identified as working age population.[50] In the fourth quarter of 2017, Sri Lanka had an unemployment rate of 4.2 percent[51] and is shown to reduce gradually over the years.\\r\\n\\r\\nTourism is one of the main industries in Sri Lanka. Major tourist attractions are focused around the islands famous beaches located in the southern and the eastern parts of the country and ancient heritage sites located in the interior of the country and resorts located in the mountainous regions of the country.[52][53] Also, due to precious stones such as rubies and sapphires being frequently found and mined in Ratnapura and its surrounding areas, they are a major tourist attraction.[54]\\r\\n\\r\\nThe 2004 Indian Ocean Tsunami[55] and the past civil war have reduced the tourist arrivals, however the number of tourists visiting have been recently increasing, beginning in early 2008.[56] March 2008 by 8.6% and Sri Lanka attracted 1,003,000 tourists in 2012 according to the Central Bank of Sri Lanka's 2013 roadmap.[57]\\r\\n\\r\\nThe tea industry, operating under the Ministry of Public Estate Management and Development, is one of the main industries in Sri Lanka. It became the world's leading exporter in 1995 with a 23% share of global tea export, higher than Kenya's 22% share. The central highlands of the country have a low temperature climate throughout the year and annual rainfall and the humidity levels that are suitable for growing tea. The industry was introduced to the country in 1867 by James Taylor, a British planter who arrived in 1852.[58]\\r\\n\\r\\nRecently, Sri Lanka has become one of the countries exporting fair trade tea to the UK and other countries. It is believed that such projects could reduce rural poverty.[59][60]\\r\\n\\r\\nThe apparel industry of the Sri Lanka mainly exports to the United States and Europe. Europe increasingly relies on Sri Lankan textiles due to the high cost of labour in Europe.[citation needed] There are about 900 factories throughout country serving companies such as Victoria's Secret, Liz Claiborne and Tommy Hilfiger.[61]\\r\\n\\r\\nThe agricultural sector of the country produces mainly rice, coconut and grain, largely for domestic consumption and occasionally for export. The tea industry which has existed since 1867 is not usually regarded as part of the agricultural sector, which is mainly focused on export rather than domestic use in the country.[62]\\r\\n\\r\\nExport revenue of Sri Lankan IT sector is estimated to be USD 720 million in 2013.[63][64]\\r\\n\\r\\nSri Lanka is known for producing a variety of gemstones, including chrysoberyl, corundum, garnet, ruby, spinel, and tourmaline, and is a leading producer of the Ceylon Blue sapphire. The best known areas for gemstone mining in Sri Lanka were Balangoda, Elahera, Kamburupitiya, Moneragala, Okkampitiya, and Ratnapura. In addition Sri Lanka has a variety of industrial minerals, which include ball clay, kaolin, and other clays, calcite, dolomite, feldspar, graphite, limestone, Ilmenite, mica, rutile mineral sands, phosphate rock, quartz, zircon, dolomite and silica sand. Pulmoddai beach sand deposit is the most important non-ferrous mineral reserve in Sri Lanka as well as one of the world's most richest mineral sand deposits with heavy mineral concentrates of 50% to 60% and contain manyminerals including titanium.[65][66][67]\\r\\n\\r\\nSri Lanka is specially for its highly valued and high-purity vein graphite. As of 2014, graphite was produced at the two largest graphite mines in Sri Lanka, the Bogala and the Kahatagaha Mines. Major investors in graphite mining are Graphite Lanka Ltd., Bogala Graphite Lanka Plc, Bora Bora Resources Ltd. (BBR) of Australia, MRL Corp. Ltd. of Australia, and Saint Jean Carbon Inc. of Canada.[68][65]\\r\\n\\r\\nSri Lanka has developed several multi-national companies and international brands. The most notable companies include:\\r\\n\\r\\nMAS Holdings\\r\\n\\r\\nLAUGFS Holdings\\r\\n\\r\\nAitken Spence\\r\\n\\r\\nDilmah\\r\\n\\r\\nExports to the United States, Sri Lanka's most important market, were valued at $1.8 billion in 2002, or 38% of total exports. For many years, the United States has been Sri Lanka's largest market for garments, receiving more than 63% of the country's total garment exports. India is Sri Lanka's largest supplier, with imports worth $835 million in 2002. Japan, traditionally Sri Lanka's largest supplier, was its fourth-largest in 2002 with exports of $355 million. Other important suppliers include Hong Kong, Singapore, Taiwan, and South Korea. The United States is the 10th-largest supplier to Sri Lanka; US imports amounted to $218 million in 2002, according to Central Bank trade data.\\r\\n\\r\\nA new port is being built in Hambantota in Southern Sri Lanka, funded by the Chinese government as a part of the Chinese aid to Sri Lanka. This will ease the congestion in Sri Lankan ports, particularly in Colombo. In 2009, 4456 ships visited Sri Lankan ports.\\r\\n\\r\\nSri Lanka had applied for credit ratings from international agencies in its efforts to apply for loans from international markets in 2005 after the election of Mahinda Rajapakse as president. Standard and Poor's has rated Sri Lanka a \\"B+\\" speculative rating, four grades below investment grade. Fitch has rated Sri Lanka with \\"BB-\\" which is three grades below investment grade. Standard and Poor's maintains Sri Lanka is constrained by providing widespread subsidies, a bloated public sector, transfers to loss-making state enterprises, and high interest local and international burdens [5]. Standard and Poor's estimates public sector debt has reached 95% of GDP [6], in comparison to CIA estimates of 89% of GDP [7]. Sri Lanka in mid-2007 sought to borrow $500 million from international markets to shore up the deteriorating exchange rate and reduce pressure on repayment of the domestic debt market [8]. The head of the opposition UNP, Ranil Wickremasinghe has warned that such intense borrowing is unsustainable and will not repay these loans if elected to power [9].\\r\\n\\r\\nSri Lanka is highly dependent on foreign assistance, and several high-profile assistance projects were launched in 2003. The most significant of these resulted from an aid conference in Tokyo in June 2003; pledges at the summit, which included representatives from the International Monetary Fund, World Bank, Asian Development Bank, Japan, the European Union and the United States, totalled $4.5 billion.\\r\\n\\r\\nDuring the past few years, the country's debt has soared as it was developing its infrastructure to the point of near bankruptcy which required a bailout from the International Monetary Fund (IMF). \\"Without an IMF loan, Sri Lanka would have been in a precarious position,\\" in May 2016 according to Krystal Tan, an Asia economist at Capital Economics who added, \\"foreign exchange reserves only covered around 80 percent of short-term external debt.\\"[69] The IMF had agreed to provide a $1.5 billion bailout loan in April 2016 after Sri Lanka provided a set of criteria intended to improve its economy.\\r\\n\\r\\nBy the fourth quarter of 2016 the debt was estimated to be $64.9 billion. Additional debt had been incurred in the past by state-owned organizations and this was said to be at least $9.5 billion. Since early 2015, domestic debt increased by 12 percent and external debt by 25 percent.[70]\\r\\n\\r\\nIn late 2016 the World Bank provided US$100 million in financing and the Japan International Cooperation Agency provided a US$100M loan, both intended to \\"provide budget financing and to support reforms in competitiveness, transparency, public sector and fiscal management\\", according to the World Bank. The bank also reported that the country's government had agreed that there was a need for reforms \\"in the areas of fiscal operations, competitiveness and governance\\" and if fully implemented, \\"these could help the country reach Upper Middle Income status in the medium term\\" according to the bank.[71]\\r\\n\\r\\nIn November 2016, the International Monetary Fund reported that it would disburse a higher amount than the US$150 million originally planned, a full US$162.6 million (SDR 119.894 million), to Sri Lanka. The agency's evaluation was cautiously optimistic about the future: \\"While inflation has abated, credit growth remains strong. The central bank indicates its readiness to tighten the monetary policy stance further if inflationary pressures resurge or credit growth persists. The authorities intend to continue building up reserves through outright purchases while allowing for greater exchange rate flexibility. The banking sector is currently well capitalized. Steps are being taken to find a resolution mechanism for the distressed financial institutions. Going forward, there is a need to strengthen the supervisory and regulatory framework, and identify and mitigate vulnerabilities in the financial sector, particularly with regard to non-banks and state-owned banks.\\"[72]\\r\\n\\r\\nAs part of the debt management program the Sri lankan government carried out several reforms which including the implementation of a new Inland Revenue Act as well as an automatic fuel pricing formula. Tax reforms also increased VAT rates and narrowed exemptions and the third review by the IMF noted that performance was on track regarding fiscal consolidation, revenue mobilization, monetary policy management, and reserves accumulation. In the fourth review in June 2018 the IMF claimed that \\" Sri Lanka has made important progress under its Fund-supported program.\\" but stressed the need of further progress with revenue-based fiscal consolidation and a prudent monetary policy with sustained efforts to build up international reserves. In 2018 China extended a loan of $1.25 Billion consisting of a below market rate syndicated loan and smaller Panda bond to bailout Sri Lanka.[73][74][75]","input":"What are the main imports of sri lanka?"},{"output":"thirty-three","context":"The Ancient and Accepted Scottish Rite of Freemasonry (the Northern Masonic Jurisdiction in the United States often omits the and, while the English Constitution in the United Kingdom omits the Scottish), commonly known as simply the Scottish Rite (or, in England and Australia, as the Rose Croix[1] although this is only one of its degrees), is one of several Rites of Freemasonry. A Rite is a progressive series of degrees conferred by various Masonic organizations or bodies, each of which operates under the control of its own central authority. In the Scottish Rite the central authority is called a Supreme Council.\\r\\n\\r\\nThe Scottish Rite is one of the appendant bodies of Freemasonry that a Master Mason may join for further exposure to the principles of Freemasonry.[2][3]  It is also concordant, in that some of its degrees relate to the degrees of Symbolic (Craft) Freemasonry. In England and some other countries, while the Scottish Rite is not accorded official recognition by the Grand Lodge, there is no prohibition against a Freemason electing to join it. In the United States, however, the Scottish Rite is officially recognized by Grand Lodges as an extension of the degrees of Freemasonry.  The Scottish Rite builds upon the ethical teachings and philosophy offered in the Craft (or Blue) Lodge, through dramatic presentation of the individual degrees.\\r\\n\\r\\nThere are records of lodges conferring the degree of \\"Scots Master\\" or \\"Scotch Master\\" as early as 1733.[4][5][6] A lodge at Temple Bar in London is the earliest such lodge on record. Other lodges include a lodge at Bath in 1735, and the French lodge, St. George de l'Observance No. 49 at Covent Garden in 1736. The references to these few occasions indicate that these were special meetings held for the purpose of performing unusual ceremonies, probably by visiting Freemasons.[7]:5  The Copiale cipher, dating from the 1740s[8] says, \\"The rank of a Scottish master is an entirely new invention...\\"[9]\\r\\n\\r\\nThe seed of the myth of Stuart Jacobite influence on the higher degrees may have been a careless and unsubstantiated remark made by John Noorthouk in the 1784 Book of Constitutions of the Premier Grand Lodge of London. It was stated, without support, that King Charles II (older brother and predecessor to James II) was made a Freemason in the Netherlands during the years of his exile (1649ÿ60). However, there were no documented lodges of Freemasons on the continent during those years. The statement may have been made to flatter the fraternity by claiming membership for a previous monarch. This folly was then embellished by John Robison (1739ÿ1805), a professor of Natural Philosophy at the University of Edinburgh, in an anti-Masonic work published in 1797. The lack of scholarship exhibited by Robison in that work caused the Encyclop?dia Britannica to denounce it.[10]\\r\\n\\r\\nA German bookseller and Freemason, living in Paris, working under the assumed name of C. Lenning, embellished the story further in a manuscript titled \\"Encyclopedia of Freemasonry\\" probably written between 1822 and 1828 at Leipzig. This manuscript was later revised and published by another German Freemason named Friedrich Mossdorf (1757ÿ1830).[11] Lenning stated that King James II of England, after his flight to France in 1688, resided at the Jesuit College of Clermont, where his followers fabricated certain degrees for the purpose of carrying out their political ends.[12]\\r\\n\\r\\nBy the mid-19th century, the story had gained currency. The well-known English Masonic writer, Dr. George Oliver (1782ÿ1867), in his Historical Landmarks, 1846, carried the story forward and even claimed that King Charles II was active in his attendance at meetingsan obvious invention, for if it had been true, it would not have escaped the notice of the historians of the time. The story was then repeated by the French writers Jean-Baptiste Ragon (1771ÿ1862) and Emmanuel Rebold, in their Masonic histories. Rebold's claim that the high degrees were created and practiced in Lodge Canongate Kilwinning[13] at Edinburgh are entirely false.[14]\\r\\n\\r\\nJames II died in 1701[15][16]at the Palace of St. Germain en Laye, and was succeeded in his claims to the British throne by his son, James Francis Edward Stuart (1699ÿ1766), the Chevalier St. George, better known as \\"the Old Pretender\\", but recognized as James III by the French King Louis XIV. He was succeeded in his claim by Charles Edward Stuart (\\"Bonnie Prince Charles\\"), also known as \\"the Young Pretender\\", whose ultimate defeat at the Battle of Culloden in 1746 effectively put an end to any serious hopes of the Stuarts regaining the British crowns.\\r\\n\\r\\nThe natural confusion between the names of the Jesuit College of Clermont, and the short-lived Masonic Chapter of Clermont, a Masonic body that controlled a few high degrees during its brief existence, only served to add fuel to the myth of Stuart Jacobite influence in Freemasonry's high degrees. However, the College and the Chapter had nothing to do with each other. The Jesuit College was located at Clermont, whereas the Masonic Chapter was not. Rather, it was named \\"Clermont\\" in honor of the French Grand Master, the Comte de Clermont (Louis de Bourbon, Comte de Clermont) (1709ÿ1771), and not because of any connection with the Jesuit College of Clermont.[17]\\r\\n\\r\\nA French trader, by the name of Estienne Morin, had been involved in high-degree Masonry in Bordeaux since 1744 and, in 1747, founded an \\"cossais\\" lodge (Scots Masters Lodge) in the city of Le Cap Fran?ais, on the north coast of the French colony of Saint-Domingue (now Haiti). Over the next decade, high-degree Freemasonry was carried by French men to other cities in the Western hemisphere. The high-degree lodge at Bordeaux warranted or recognized seven cossais lodges there. In Paris in the year 1761, a patent was issued to Estienne Morin, dated 27 August, creating him \\"Grand Inspector for all parts of the New World\\". This Patent was signed by officials of the Grand Lodge at Paris and appears to have originally granted him power over the craft lodges only, and not over the high, or \\"cossais\\", degree lodges. Later copies of this Patent appear to have been embellished, probably by Morin, to improve his position over the high-degree lodges in the West Indies.[7]:31-45\\r\\n\\r\\nEarly writers long believed that a \\"Rite of Perfection\\" consisting of 25 degrees, the highest being the \\"Sublime Prince of the Royal Secret\\", and being the predecessor of the Scottish Rite, had been formed in Paris by a high-degree council calling itself \\"The Council of Emperors of the East and West\\". The title \\"Rite of Perfection\\" first appeared in the Preface to the \\"Grand Constitutions of 1786\\", the authority for which is now known to be faulty.[7]:75ÿ84 It is now generally accepted that this Rite of twenty-five degrees was compiled by Estienne Morin and is more properly called \\"The Rite of the Royal Secret\\", or \\"Morin's Rite\\".[7]:37 However, it was known as \\"The Order of Prince of the Royal Secret\\" by the founders of the Scottish Rite, who mentioned it in their \\"Circular throughout the two Hemispheres\\"[18] or \\"Manifesto\\", issued on December 4, 1802.[19]\\r\\n\\r\\nMorin returned to the West Indies in 1762 or 1763, to Saint-Domingue. Based on his new Patent, he assumed powers to constitute lodges of all degrees, spreading the high degrees throughout the West Indies and North America. Morin stayed in Saint-Domingue until 1766, when he moved to Jamaica. At Kingston, Jamaica, in 1770, Morin created a \\"Grand Chapter\\" of his new Rite (the Grand Council of Jamaica). Morin died in 1771 and was buried in Kingston.[20]:16\\r\\n\\r\\nHenry Andrew Francken, a naturalized French subject of Dutch origin, was most important in assisting Morin in spreading the degrees in the New World. Morin appointed him Deputy Grand Inspector General (DGIG) as one of his first acts after returning to the West Indies. Francken worked closely with Morin and, in 1771, produced a manuscript book giving the rituals for the 15th through the 25th degrees. Francken produced at least four such manuscripts. In addition to the 1771 manuscript, there is a second which can be dated to 1783; a third manuscript, of uncertain date, written in Franckens handwriting, with the rituals 4ÿ25, which was found in the archives of the Provincial Grand Lodge of Lancashire in Liverpool in approximately 1984; and a fourth, again of uncertain date, with rituals 4ÿ24, which was known to have been given by H. J. Whymper to the District Grand Lodge of the Punjab and rediscovered about 2010.[21] Additionally, there is a French manuscript dating from 1790-1800 which contains the 25 degrees of the Order of the Royal Secret with additional detail, as well as three other Hauts Grades rituals; its literary structure suggests it is derived from a common source as the Francken Manuscripts.[22]\\r\\n\\r\\nA Loge de Parfaits d' cosse was formed on 12 April 1764 at New Orleans, becoming the first high-degree lodge on the North American continent. Its life, however, was short, as the Treaty of Paris (1763) ceded New Orleans to Spain, and the Catholic Spanish crown had been historically hostile to Freemasonry. Documented Masonic activity ceased for a time. It did not return to New Orleans until the late 1790s, when French refugees from the revolution in Saint-Domingue settled in the city.[20]:16\\r\\n\\r\\nFrancken traveled to New York in 1767 where he granted a Patent, dated 26 December 1767, for the formation of a Lodge of Perfection at Albany, which was called \\"Ineffable Lodge of Perfection\\". This marked the first time the Degrees of Perfection (the 4th through the 14th) were conferred in one of the Thirteen British colonies in North America. This Patent, and the early minutes of the Lodge, are still extant and are in the archives of Supreme Council, Northern Jurisdiction.[20]:16 (The minutes of Ineffable Lodge of Perfection reveal that it ceased activity on December 5, 1774. It was revived by Giles Fonda Yates about 1820 or 1821, and came under authority of the Supreme Council, Southern Jurisdiction until 1827. That year it was transferred to the Supreme Council, Northern Jurisdiction.)\\r\\n\\r\\nWhile in New York City, Francken also communicated the degrees to Moses Michael Hays, a Jewish businessman, and appointed him as a Deputy Inspector General. In 1781, Hays made eight Deputy Inspectors General, four of whom were later important in the establishment of Scottish Rite Freemasonry in South Carolina:\\r\\n?_ Isaac Da Costa, Sr., D.I.G. for South Carolina;\\r\\n?_ Abraham Forst, D.I.G. for Virginia;\\r\\n?_ Joseph M. Myers, D.I.G. for Maryland;\\r\\n?_ and Barend M. Spitzer, D.I.G. for Georgia.\\r\\nDa Costa returned to Charleston, South Carolina, where he established the \\"Sublime Grand Lodge of Perfection\\" in February 1783. After Da Costa's death in November 1783, Hays appointed Myers as Da Costa's successor. Joined by Forst and Spitzer, Myers created additional high-degree bodies in Charleston.[20]:16-17\\r\\n\\r\\nPhysician Hyman Isaac Long from the island of Jamaica, who settled in New York City, went to Charleston in 1796 to appoint eight French men; he had received his authority through Spitzer. These men had arrived as refugees from Saint-Domingue, where the slave revolution was underway that would establish Haiti as an independent republic in 1804. They organized a Consistory of the 25th Degree, or \\"Princes of the Royal Secret,\\" which Masonic historian Brigadier ACF Jackson says became the first Supreme Council of the Scottish Rite.[7]:66-68 According to Fox, by 1801, the Charleston bodies were the only extant bodies of the Rite in North America.[20]:16-17\\r\\n\\r\\nAlthough most of the thirty-three degrees of the Scottish Rite existed in parts of previous degree systems,[23] the Scottish Rite did not come into being until the formation of the Mother Supreme Council at Charleston, South Carolina, in May 1801. The Founding Fathers of the Scottish Rite who attended became known as \\"The Eleven Gentlemen of Charleston\\".\\r\\n\\r\\nSubsequently, other Supreme Councils were formed in Saint-Domingue (now Haiti) in 1802, in France in 1804, in Italy in 1805, and in Spain in 1811.[24]\\r\\n\\r\\nOn May 1, 1813, an officer from the Supreme Council at Charleston initiated several New York Masons into the Thirty-third Degree and organized a Supreme Council for the \\"Northern Masonic District and Jurisdiction\\". On May 21, 1814 this Supreme Council reopened and proceeded to \\"nominate, elect, appoint, install and proclaim in due, legal and ample form\\" the elected officers \\"as forming the second Grand and Supreme Council...\\". Finally, the charter of this organization (written January 7, 1815) added, We think the Ratification ought to be dated 21st day May 5815.\\"[25]\\r\\n\\r\\nOfficially, the Supreme Council, 33, N.M.J. dates itself from May 15, 1867. This was the date of the \\"Union of 1867\\", when it merged with the competing Cerneau \\"Supreme Council\\" in New York. The current Ancient and Accepted Scottish Rite, Northern Masonic Jurisdiction of the United States, was thus formed.[26]\\r\\n\\r\\nBorn in Boston, Massachusetts on December 29, 1809, Albert Pike is asserted within the Southern Jurisdiction as the man most responsible for the growth and success of the Scottish Rite from an obscure Masonic Rite in the mid-19th century to the international fraternity that it became. Pike received the 4th through the 32nd Degrees in March 1853[27][28] from Albert Mackey, in Charleston, South Carolina, and was appointed Deputy Inspector for Arkansas that same year.\\r\\n\\r\\nAt this point, the degrees were in a rudimentary form, and often included only a brief history and legend of each degree, as well as other brief details which usually lacked a workable ritual for their conferral. In 1855, the Supreme Council appointed a committee to prepare and compile rituals for the 4th through the 32nd Degrees. That committee was composed of Albert G. Mackey, John H. Honour, William S. Rockwell, Claude P. Samory, and Albert Pike. Of these five committee members, Pike did all the work of the committee.\\r\\n\\r\\nIn 1857 Pike completed his first revision of the 4-32 ritual, and printed 100 copies. This revision, which Mackey dubbed the \\"Magnum Opus\\", was never adopted by the Supreme Council. According to Arturo de Hoyos, 33, the Scottish Rite's Grand Historian, the Magnum Opus became the basis for future ritual revisions.[29]\\r\\n\\r\\nIn March 1858, Pike was elected a member of the Supreme Council for the Southern Jurisdiction of the United States, and in January 1859 he became its Grand Commander. The American Civil War interrupted his work on the Scottish Rite rituals. About 1870 he, and the Supreme Council, moved to Washington, DC. In 1884 his revision of the rituals was complete.\\r\\n\\r\\nScottish Rite Grand Archivist and Grand Historian de Hoyos[30] created the following chart of Pike's ritual revisions:\\r\\n\\r\\n\\r\\n(manuscripts only)\\r\\n\\r\\n\\r\\nPike also wrote lectures about all the degrees, which were published in 1871 under the title Morals and Dogma of the Ancient and Accepted Scottish Rite of Freemasonry.[31]\\r\\n\\r\\nIn 2000 the Southern Jurisdiction revised its ritual.  The current ritual is based upon Pike's, but with some significant differences.\\r\\n\\r\\nThe thirty-three degrees of the Scottish Rite are conferred by several controlling bodies. The first of these is the Craft Lodge, which confers the Entered Apprentice, Fellowcraft, and Master Mason degrees. Craft lodges operate under the authority of national Grand Lodges, not the Scottish Rite. Attainment of the third Masonic degree, that of a Master Mason, represents the attainment of the highest rank in all of Masonry.[32][33] Additional degrees such as those of the AASR are sometimes referred to as appendant degrees, even where the degree numbering might imply a hierarchy. They represent a lateral movement in Masonic education rather than an upward movement, and are degrees of instruction rather than rank.[34]\\r\\n\\r\\nThe AASR does have its own distinctive versions of the Craft rituals, but most lodges throughout the English-speaking world do not confer them. However, there are a handful of lodges in New Orleans and several other major U.S. cities that have traditionally conferred the Scottish Rite version of these degrees,[35] as do most Lodges under the jurisdiction of the Grande Loge Nationale Fran?aise[36]\\r\\n\\r\\nAccording to Masonic historian Alain Bernheim, Belgian Masonic scholar Pierre No?l demonstrated in a 2002 paper that the AASR Craft degrees derived from the French translation of the Masonic expos Three Distinct Knocks, issued in London in 1760.[37]\\r\\n\\r\\nIn 2000, the Southern Jurisdiction in the United States completed a revision of its ritual scripts. In 2004, the Northern Jurisdiction in the United States rewrote and reorganized its degrees.[38]  Further changes have occurred in 2006.[39] The current titles of the degrees and their arrangement in the Southern Jurisdiction remains substantially unchanged from the beginning.\\r\\n\\r\\nThe list of degrees for the Supreme Councils of Australia, England and Wales, and most other jurisdictions largely agrees with that of the Southern Jurisdiction of the U.S.  However, the list of degrees for the Northern Jurisdiction of the United States is now somewhat different and is given in the table below.  The list of degrees of the Supreme Council of Canada reflects a mixture of the two, with some unique titles as well:\\r\\n\\r\\nThe Ancient and Accepted Scottish Rite in each country is governed by a Supreme Council.[46] There is no international governing body; each Supreme Council in each country is sovereign unto itself in its own jurisdiction.\\r\\n\\r\\nIn Canada, whose Supreme Council was warranted in 1874 by that of England and Wales, the Rite is known as Ancient and Accepted Scottish Rite.  The council is called \\"Supreme Council 33 Ancient and Accepted Scottish Rite of Freemasonry of Canada\\". Canada's Supreme Council office is located at 4 Queen Street South in Hamilton, Ontario. There are 45 local units or \\"Valleys\\" across Canada.[47]\\r\\n\\r\\nWhen Comte de Grasse-Tilly returned to France in 1804, he worked to establish the Ancient and Accepted Scottish Rite there. He founded the first Supreme Council in France[48] that same year.\\r\\n\\r\\nThe Grand Orient of France signed a treaty of union in December 1804 with the Supreme Council of the 33rd Degree in France; the treaty declared that \\"the Grand Orient united to itself\\"[49] the Supreme Council in France. This accord was applied until 1814. Thanks to this treaty, the Grand Orient of France took ownership, as it were, of the Scottish Rite.\\r\\n\\r\\nFrom 1805 to 1814, the Grand Orient of France administered the first 18 degrees of the Rite, leaving the Supreme Council of France to administer the last 15. In 1815, five of the leaders of the Supreme Council founded the Suprme Conseil des Rites within the Grand Orient of France. The original Supreme Council of France fell dormant from 1815 to 1821.[50]\\r\\n\\r\\nThe Suprme Conseil des Isles d'Amrique (founded in 1802 by Grasse-Tilly and revived around 1810 by his father-in-law Delahogue, who had also returned from the United States) breathed new life into the Supreme Council for the 33rd Degree in France. They merged into a single organization: the Supreme Council of France. This developed as an independent and sovereign Masonic power. It created symbolic lodges (those composed of the first three degrees, which otherwise would be federated around a Grand Lodge or a Grand Orient).\\r\\n\\r\\nIn 1894, the Supreme Council of France created the Grand Lodge of France. It became fully independent in 1904, when the Supreme Council of France ceased chartering new lodges.[51] The Supreme Council of France still considers itself the overseer of all 33 degrees of the Rite. Relations between the two structures remain close, as shown by their organizing two joint meetings a year. \\r\\n\\r\\nIn 1964, the Sovereign Grand Commander Charles Riandey, along with 400 to 500 members,[52] left the jurisdiction of the Supreme Council of France and joined the Grande Loge Nationale Fran?aise. Because of his resignation and withdrawal of hundreds of members, there was no longer a Supreme Council of France. Riandey then reinitiated the 33 degrees of the rite in Amsterdam.[53] With the support of the Supreme Council of the Southern Jurisdiction of the United States, he founded a new Supreme Council in France, called the Suprme Conseil pour la France. This was the only one to be recognized by the Supreme Councils of the United States after it was designated in 1970 as the sole authority of the Scottish Rite for France by the Supreme Council of the Southern Jurisdiction (the oldest Supreme Council in the world) at the Barranquilla conference.\\r\\n\\r\\nFrance has three different and arguably legitimate Supreme Councils:\\r\\n\\r\\nThe Ancient and Accepted Scottish Rite was established in Romania in 1881, a year after the National Grand Lodge of Romania was founded.  On 27 December 1922, the Supreme Council of Scottish Rite of Romania, received the recognition of the Supreme Council of France in 1922, and recognition from the Supreme Council, Southern Jurisdiction of the United States in 1925.\\r\\n\\r\\nBetween 1948 ÿ 1989 all of Romanian Freemasonry, including the Ancient and Accepted Scottish Rite of Romania, was banned by the Communist regime.\\r\\n\\r\\nThe Supreme Council of the Ancient and Accepted Scottish Rite of Romania was reconsecrated in 1993.[54]\\r\\n\\r\\nIn England and Wales, whose Supreme Council was warranted by that of the Northern Jurisdiction of the USA (in 1845),[55] the Rite is known colloquially as the \\"Rose Croix\\" or more formally as \\"The Ancient and Accepted Rite for England and Wales and its Districts and Chapters Overseas\\" (continental European jurisdictions retain the \\"cossais\\"). England and Wales are divided into Districts, which administer the Rose Croix Chapters within their District; many degrees are conferred in name only, and degrees beyond the 18 are conferred only by the Supreme Council itself.\\r\\n\\r\\nAll candidates for membership must profess the Trinitarian Christian faith and have been Master masons for at least one year.[56]\\r\\n\\r\\nIn England and Wales, the candidate is perfected in the 18th degree with the preceding degrees awarded in name only.  Continuing to the 30th degree is restricted to those who have served in the chair of the Chapter.  Elevation beyond the 30th degree is as in Scotland.\\r\\n\\r\\nIn Scotland, candidates are perfected in the 18th degree, with the preceding degrees awarded in name only. A minimum of a two-year interval is required before continuing to the 30th degree, again with the intervening degrees awarded by name only.  Elevation beyond that is by invitation only, and numbers are severely restricted.[57]\\r\\n\\r\\nIn the United States of America there are two Supreme Councils: one in Washington, D.C. (which controls the Southern Jurisdiction), and one in Lexington, Massachusetts (which controls the Northern Masonic Jurisdiction).[58] They each have particular characteristics that make them different.\\r\\n\\r\\nIn the United States, members of the Scottish Rite can be elected to receive the 33 by the Supreme Council.[59] It is conferred on members who have made major contributions to society or to Masonry in general.\\r\\n\\r\\nBased in Washington, D.C., the Southern Jurisdiction (often referred to as the \\"Mother Supreme Council of the World\\") was founded in Charleston, South Carolina, in 1801. It oversees the Scottish Rite in 35 states, which are referred to as Orients, and local bodies, which are called Valleys.[60][61][62]\\r\\n\\r\\nIn the Southern Jurisdiction of the United States, the Supreme Council consists of no more than 33 members and is presided over by a Grand Commander. Other members of the Supreme Council are called \\"Sovereign Grand Inspectors General\\" (S.G.I.G.), and each is the head of the Rite in his respective Orient (or state). Other heads of the various Orients who are not members of the Supreme Council are called \\"Deputies of the Supreme Council\\". The Supreme Council of the Southern Jurisdiction meets every odd year during the month of August at the House of the Temple, Scottish Rite of Freemasonry Southern Jurisdiction Headquarters, in Washington, D.C. During this conference, closed meetings between the Grand Commander and the S.G.I.G.'s are held, and many members of the fraternity from all over the world attend the open ceremony on the 5th of 6 council meeting days.\\r\\n\\r\\nIn the Southern Jurisdiction, a member who has been a 32 Scottish Rite Mason for 46 months or more is eligible to be elected to receive the \\"rank and decoration\\" of Knight Commander of the Court of Honour (K.C.C.H.) in recognition of outstanding service. After 46 months as a K.C.C.H. he is then eligible to be elected to the 33rd degree, upon approval of the Supreme Council and Grand Commander.[63]\\r\\n\\r\\nThe Lexington, Massachusetts-based Northern Masonic Jurisdiction, formed in 1813, oversees the bodies in fifteen states: Connecticut, Delaware, Illinois, Indiana, Maine, Massachusetts, Michigan, New Jersey, New Hampshire, New York, Ohio, Pennsylvania, Rhode Island, Wisconsin and Vermont.  The Northern Jurisdiction is only divided into Valleys, not Orients.[64]  Each Valley has up to four Scottish Rite bodies, and each body confers a set of degrees.\\r\\n\\r\\nIn the Northern Jurisdiction, the Supreme Council consists of no more than 66 members. Those who are elected to membership on the Supreme Council are then designated \\"Active.\\" In the Northern Jurisdiction\\r\\nall recipients of the 33rd Degree are honorary members of the Supreme Council, and all members are referred to as a \\"Sovereign Grand Inspectors General.\\"  The head of the Rite in each State of the Northern Jurisdiction is called a \\"Deputy of the Supreme Council.\\"  Thus the highest ranking Scottish Rite officer in Ohio, is titled, \\"Deputy for Ohio\\",  and so forth for each state. Additionally, each Deputy has one or more \\"Actives\\" to assist him in the administration of the state. Active members of the Supreme Council who have served faithfully for ten years, or reach the age of 75, may be designated \\"Active, Emeritus\\". The Northern Jurisdiction Supreme Council meets yearly, in the even years by an executive session, and in the odd years, with the full membership invited.  The 33rd Degree is conferred on the odd years at the Annual Meeting.\\r\\n\\r\\nIn the Northern Jurisdiction, there is a 46-month requirement for eligibility to receive the 33rd degree, and while there is a Meritorious Service Award (as well as a Distinguished Service Award), they are not required intermediate steps towards the 33.","input":"How many degrees did the scottish rite confer by name at its founding?"},{"output":"Nirmala Sitharaman","context":"\\r\\n\\r\\nNirmala Sitharaman (born 18 August 1959) is an Indian politician of the Bharatiya Janata Party, currently serving as the Minister of Defence in the Narendra Modi government. She is also a member of the Rajya Sabha for Karnataka.\\r\\n\\r\\nSitharaman has formerly served as the Minister of State for Finance and Corporate Affairs under the Ministry of Finance and the Minister for Commerce and Industry with independent charge. Prior to that, she has served as a national spokesperson for the BJP.[5]\\r\\n\\r\\nNirmala was born in Madurai, Tamil Nadu to Narayanan and Savitri. Her father, Narayanan Sitaraman, hailed from Musiri, Tiruchirappalli district, while her mother's family had its roots in Thiruvenkadu, Thanjavur District and Salem district. Her father was an employee of Indian Railways and hence she spent her childhood in various parts of the state. She had her schooling  from Madras and Tiruchirappalli.[6] She obtained a BA in economics at the Seethalakshmi Ramaswamy College in Tiruchirapalli. And her Master's degree in Economics from Jawaharlal Nehru University, Delhi.[7]\\r\\n\\r\\nShe met her husband Parakala Prabhakar, from Narsapuram Andhra Pradesh, while studying at the Jawaharlal Nehru University. While Nirmala leaned towards the BJP, her husband was from a pro-Congress family.[8] He served as a communications advisor to the Andhra Pradesh Chief Minister, Chandrababu Naidu.[9][10]\\r\\n\\r\\nNirmala Sitharaman joined the BJP in 2008 and has served as a spokesperson for the party. In 2014, she was inducted in Narendra Modi's cabinet as a junior minister.\\r\\n\\r\\nIn May 2016, she was one of the 12 candidates nominated by the BJP to contest the Rajya Sabha elections due on 11 June 2016. She successfully contested her seat from Karnataka.[11] On 3 September 2017, she was elevated to Defence Minister, being only the second woman after Indira Gandhi to hold the Defence Minister post.[12][13]\\r\\n\\r\\nOutside the political sphere, Sitharaman has served as an assistant to Economist in the Agricultural Engineers Association in the UK. During her stay in the UK, she has also served as a Senior Manager (R&D)[10] for Price Waterhouse and briefly at the BBC World Service.[14] She has also served as a member of National Commission for Women.[15] In 2017, she was one of the founding directors of Pranava school in Hyderabad.[16]","input":"Who is the present defence minister of karnataka?"},{"output":"Ken Thompson","context":"Unix (/?ju?n?ks/; trademarked as UNIX) is a family of multitasking, multiuser computer operating systems that derive from the original AT&T Unix, development starting in the 1970s at the Bell Labs research center by Ken Thompson, Dennis Ritchie, and others.[3]\\r\\nInitially intended for use inside the Bell System, AT&T licensed Unix to outside parties from the late 1970s, leading to a variety of both academic and commercial variants of Unix from vendors such as the University of California, Berkeley (BSD), Microsoft (Xenix), IBM (AIX) and Sun Microsystems (Solaris). AT&T finally sold its rights in Unix to Novell in the early 1990s, which then sold its Unix business to the Santa Cruz Operation (SCO) in 1995,[4] but the UNIX trademark passed to the industry standards consortium The Open Group, which allows the use of the mark for certified operating systems compliant with the Single UNIX Specification (SUS). Among these is Apple's macOS,[5] which is the Unix version with the largest installed base as of 2014.\\r\\nFrom the power user's or programmer's perspective, Unix systems are characterized by a modular design that is sometimes called the \\"Unix philosophy\\", meaning that the operating system provides a set of simple tools that each perform a limited, well-defined function,[6] with a unified filesystem as the main means of communication[3] and a shell scripting and command language to combine the tools to perform complex workflows. Aside from the modular design, Unix also distinguishes itself from its predecessors as the first portable operating system: almost the entire operating system is written in the C programming language[7] that allowed Unix to reach numerous platforms.\\r\\nMany Unix-like operating systems have arisen over the years, of which Linux is the most popular, having displaced SUS-certified Unix on many server platforms since its inception in the early 1990s. Android, the most widely used mobile operating system in the world, is in turn based on Linux.\\r\\n\\r\\n\\r\\nUnix was originally meant to be a convenient platform for programmers developing software to be run on it and on other systems, rather than for non-programmer users.[8][9] The system grew larger as the operating system started spreading in academic circles, as users added their own tools to the system and shared them with colleagues.[10]\\r\\nUnix was designed to be portable, multi-tasking and multi-user in a time-sharing configuration. Unix systems are characterized by various concepts: the use of plain text for storing data; a hierarchical file system; treating devices and certain types of inter-process communication (IPC) as files; and the use of a large number of software tools, small programs that can be strung together through a command-line interpreter using pipes, as opposed to using a single monolithic program that includes all of the same functionality. These concepts are collectively known as the \\"Unix philosophy\\". Brian Kernighan and Rob Pike summarize this in The Unix Programming Environment as \\"the idea that the power of a system comes more from the relationships among programs than from the programs themselves\\".[11]\\r\\nBy the early 1980s users began seeing Unix as a potential universal operating system, suitable for computers of all sizes.[12][13] The Unix environment and the clientÿserver program model were essential elements in the development of the Internet and the reshaping of computing as centered in networks rather than in individual computers.\\r\\nBoth Unix and the C programming language were developed by AT&T and distributed to government and academic institutions, which led to both being ported to a wider variety of machine families than any other operating system.\\r\\nUnder Unix, the operating system consists of many utilities along with the master control program, the kernel. The kernel provides services to start and stop programs, handles the file system and other common \\"low-level\\" tasks that most programs share, and schedules access to avoid conflicts when programs try to access the same resource or device simultaneously. To mediate such access, the kernel has special rights, reflected in the division between user space and kernel space.\\r\\nThe microkernel concept was introduced in an effort to reverse the trend towards larger kernels and return to a system in which most tasks were completed by smaller utilities. In an era when a standard computer consisted of a hard disk for storage and a data terminal for input and output (I/O), the Unix file model worked quite well, as most I/O was linear. However, modern systems include networking and other new devices. As graphical user interfaces developed, the file model proved inadequate to the task of handling asynchronous events such as those generated by a mouse. In the 1980s, non-blocking I/O and the set of inter-process communication mechanisms were augmented with Unix domain sockets, shared memory, message queues, and semaphores. In microkernel implementations, functions such as network protocols could be moved out of the kernel, while conventional (monolithic) Unix implementations have network protocol stacks as part of the kernel.\\r\\nThe pre-history of Unix dates back to the mid-1960s when the Massachusetts Institute of Technology, Bell Labs, and General Electric were developing an innovative time-sharing operating system called Multics for the GE-645 mainframe.[14] Multics introduced many innovations, but had many problems. Frustrated by the size and complexity of Multics but not by the aims, Bell Labs slowly pulled out of the project. Their last researchers to leave Multics, Ken Thompson, Dennis Ritchie, M. D. McIlroy, and J. F. Ossanna,[15] decided to redo the work on a much smaller scale.\\r\\nThe new operating system was initially without organizational backing, and also without a name. At this stage, the new operating system was a singletasking operating system,[15] not a multitasking one such as Multics. The name Unics (Uniplexed Information and Computing Service, pronounced as \\"eunuchs\\"), a pun on Multics (Multiplexed Information and Computer Services), was initially suggested for the project in 1970. Brian Kernighan claims the coining for himself, and adds that \\"no one can remember\\" who came up with the final spelling Unix.[16] Dennis Ritchie,[15] Doug McIlroy,[1] and Peter G. Neumann[17] also credit Kernighan.\\r\\nIn 1972, Unix was rewritten in the C programming language.[18] The migration from assembly to the higher-level language C resulted in much more portable software,[19] requiring only a relatively small amount of machine-dependent code to be replaced when porting Unix to other computing platforms. Bell Labs produced several versions of Unix that are collectively referred to as Research Unix. In 1975, the first source license for UNIX was sold to Donald B. Gillies at the University of Illinois Department of Computer Science.[20] UIUC graduate student Greg Chesson (who had worked on the UNIX kernel at Bell Labs) was instrumental in negotiating the terms of this license.[21]\\r\\nDuring the late 1970s and early 1980s, the influence of Unix in academic circles led to large-scale adoption of Unix (BSD and System?V) by commercial startups, including Sequent, HP-UX, Solaris, AIX, and Xenix. In the late 1980s, AT&T Unix System Laboratories and Sun Microsystems developed System?V Release?4 (SVR4), which was subsequently adopted by many commercial Unix vendors.\\r\\nIn the 1990s, Unix-like systems grew in popularity as Linux and BSD distributions were developed through collaboration by a worldwide network of programmers. In 2000, Apple released Darwin, also a Unix-like system, which became the core of the Mac OS X operating system, later renamed macOS.[22]\\r\\nUnix operating systems are widely used in modern servers, workstations, and mobile devices.[23]\\r\\nBeginning in the late 1980s, an open operating system standardization effort now known as POSIX provided a common baseline for all operating systems; IEEE based POSIX around the common structure of the major competing variants of the Unix system, publishing the first POSIX standard in 1988. In the early 1990s, a separate but very similar effort was started by an industry consortium, the Common Open Software Environment (COSE) initiative, which eventually became the Single UNIX Specification (SUS) administered by The Open Group. Starting in 1998, the Open Group and IEEE started the Austin Group, to provide a common definition of POSIX and the Single UNIX Specification, which, by 2008, had become the Open Group Base Specification.\\r\\nIn 1999, in an effort towards compatibility, several Unix system vendors agreed on SVR4's Executable and Linkable Format (ELF) as the standard for binary and object code files. The common format allows substantial binary compatibility among Unix systems operating on the same CPU architecture.\\r\\nThe Filesystem Hierarchy Standard was created to provide a reference directory layout for Unix-like operating systems, and has mainly been used in Linux.\\r\\nThe Unix system is composed of several components that were originally packaged together. By including the development environment, libraries, documents and the portable, modifiable source code for all of these components, in addition to the kernel of an operating system, Unix was a self-contained software system. This was one of the key reasons it emerged as an important teaching and learning tool and has had such a broad influence.[according to whom?]\\r\\nThe inclusion of these components did not make the system large?ÿ  the original V7 UNIX distribution, consisting of copies of all of the compiled binaries plus all of the source code and documentation occupied less than 10?MB and arrived on a single nine-track magnetic tape. The printed documentation, typeset from the on-line sources, was contained in two volumes.\\r\\nThe names and filesystem locations of the Unix components have changed substantially across the history of the system. Nonetheless, the V7 implementation is considered by many[who?] to have the canonical early structure:\\r\\nThe Unix system had significant impact on other operating systems. It achieved its reputation by its interactivity, by providing the software at a nominal fee for educational use, by running on inexpensive hardware, and by being easy to adapt and move to different machines. Unix was originally written in assembly language (which had been thought necessary for system implementations on early computers), but was soon rewritten in C, a high-level programming language.[24] Although this followed the lead of Multics and Burroughs, it was Unix that popularized the idea.\\r\\nUnix had a drastically simplified file model compared to many contemporary operating systems: treating all kinds of files as simple byte arrays. The file system hierarchy contained machine services and devices (such as printers, terminals, or disk drives), providing a uniform interface, but at the expense of occasionally requiring additional mechanisms such as ioctl and mode flags to access features of the hardware that did not fit the simple \\"stream of bytes\\" model. The Plan 9 operating system pushed this model even further and eliminated the need for additional mechanisms.\\r\\nUnix also popularized the hierarchical file system with arbitrarily nested subdirectories, originally introduced by Multics. Other common operating systems of the era had ways to divide a storage device into multiple directories or sections, but they had a fixed number of levels, often only one level. Several major proprietary operating systems eventually added recursive subdirectory capabilities also patterned after Multics. DEC's RSX-11M's \\"group, user\\" hierarchy evolved into VMS directories, CP/M's volumes evolved into MS-DOS 2.0+ subdirectories, and HP's MPE group.account hierarchy and IBM's SSP and OS/400 library systems were folded into broader POSIX file systems.\\r\\nMaking the command interpreter an ordinary user-level program, with additional commands provided as separate programs, was another Multics innovation popularized by Unix. The Unix shell used the same language for interactive commands as for scripting (shell scripts?ÿ there was no separate job control language like IBM's JCL). Since the shell and OS commands were \\"just another program\\", the user could choose (or even write) his own shell. New commands could be added without changing the shell itself. Unix's innovative command-line syntax for creating modular chains of producer-consumer processes (pipelines) made a powerful programming paradigm (coroutines) widely available. Many later command-line interpreters have been inspired by the Unix shell.\\r\\nA fundamental simplifying assumption of Unix was its focus on newline-delimited text for nearly all file formats. There were no \\"binary\\" editors in the original version of Unix?ÿ the entire system was configured using textual shell command scripts. The common denominator in the I/O system was the byte?ÿ unlike \\"record-based\\" file systems. The focus on text for representing nearly everything made Unix pipes especially useful, and encouraged the development of simple, general tools that could be easily combined to perform more complicated ad hoc tasks. The focus on text and bytes made the system far more scalable and portable than other systems. Over time, text-based applications have also proven popular in application areas, such as printing languages (PostScript, ODF), and at the application layer of the Internet protocols, e.g., FTP, SMTP, HTTP, SOAP, and SIP.\\r\\nUnix popularized a syntax for regular expressions that found widespread use. The Unix programming interface became the basis for a widely implemented operating system interface standard (POSIX, see above). The C programming language soon spread beyond Unix, and is now ubiquitous in systems and applications programming.\\r\\nEarly Unix developers were important in bringing the concepts of modularity and reusability into software engineering practice, spawning a \\"software tools\\" movement. Over time, the leading developers of Unix (and programs that ran on it) established a set of cultural norms for developing software, norms which became as important and influential as the technology of Unix itself; this has been termed the Unix philosophy.\\r\\nThe TCP/IP networking protocols were quickly implemented on the Unix versions widely used on relatively inexpensive computers, which contributed to the Internet explosion of worldwide real-time connectivity, and which formed the basis for implementations on many other platforms.\\r\\nThe Unix policy of extensive on-line documentation and (for many years) ready access to all system source code raised programmer expectations, and contributed to the 1983 launch of the free software movement.\\r\\nIn 1983, Richard Stallman announced the GNU (short for \\"GNU's Not Unix\\") project, an ambitious effort to create a free software Unix-like system; \\"free\\" in the sense that everyone who received a copy would be free to use, study, modify, and redistribute it. The GNU project's own kernel development project, GNU Hurd, had not produced a working kernel, but in 1991 Linus Torvalds released the Linux kernel as free software under the GNU General Public License. In addition to their use in the Linux operating system, many GNU packages?ÿ such as the GNU Compiler Collection (and the rest of the GNU toolchain), the GNU C library and the GNU core utilities?ÿ have gone on to play central roles in other free Unix systems as well.\\r\\nLinux distributions, consisting of the Linux kernel and large collections of compatible software have become popular both with individual users and in business. Popular distributions include Red Hat Enterprise Linux, Fedora, SUSE Linux Enterprise, openSUSE, Debian GNU/Linux, Ubuntu, Linux Mint, Mandriva Linux, Slackware Linux, and Gentoo.\\r\\nA free derivative of BSD Unix, 386BSD, was released in 1992 and led to the NetBSD and FreeBSD projects. With the 1994 settlement of a lawsuit brought against the University of California and Berkeley Software Design Inc. (USL v. BSDi) by UNIX Systems Laboratories, it was clarified that Berkeley had the right to distribute BSD Unix for free, if it so desired. Since then, BSD Unix has been developed in several different product branches, including OpenBSD and DragonFly BSD.\\r\\nLinux and BSD are increasingly filling the market needs traditionally served by proprietary Unix operating systems, as well as expanding into new markets such as the consumer desktop and mobile and embedded devices. Because of the modular design of the Unix model, sharing components is relatively common; consequently, most or all Unix and Unix-like systems include at least some BSD code, and some systems also include GNU utilities in their distributions.\\r\\nIn a 1999 interview, Dennis Ritchie voiced his opinion that Linux and BSD operating systems are a continuation of the basis of the Unix design, and are derivatives of Unix:[25]\\r\\n\\"I think the Linux phenomenon is quite delightful, because it draws so strongly on the basis that Unix provided. Linux seems to be the among the healthiest of the direct Unix derivatives, though there are also the various BSD systems as well as the more official offerings from the workstation and mainframe manufacturers.\\"\\r\\nIn the same interview, he states that he views both Unix and Linux as \\"the continuation of ideas that were started by Ken and me and many others, many years ago.\\"[25]\\r\\nOpenSolaris was the open-source counterpart to Solaris developed by Sun Microsystems, which included a CDDL-licensed kernel and a primarily GNU userland. However, Oracle discontinued the project upon their acquisition of Sun, which prompted a group of former Sun employees and members of the OpenSolaris community to fork OpenSolaris into the illumos kernel. As of 2014, illumos remains the only active open-source System V derivative.\\r\\nIn May 1975, RFC 681 described the development of Network Unix by the Center for Advanced Computation at the University of Illinois. The system was said to \\"present several interesting capabilities as an ARPANET mini-host\\". At the time Unix required a license from Bell Laboratories that at $20,000(US) was very expensive for non-university users, while an educational license cost just $150. It was noted that Bell was \\"open to suggestions\\" for an ARPANET-wide license.\\r\\nSpecific features found beneficial were the local processing facilities, compilers, editors, a document preparation system, efficient file system and access control, mountable and unmountable volumes, unified treatment of peripherals as special files, integration of the network control program (NCP) within the Unix file system, treatment of network connections as special files that can be accessed through standard Unix I/O calls, closing of all files on program exit, and the decision to be \\"desirable to minimize the amount of code added to the basic Unix kernel\\".\\r\\nIn October 1993, Novell, the company that owned the rights to the Unix System V source at the time, transferred the trademarks of Unix to the X/Open Company (now The Open Group),[26] and in 1995 sold the related business operations to Santa Cruz Operation (SCO).[27] Whether Novell also sold the copyrights to the actual software was the subject of a 2006 federal lawsuit, SCO v. Novell, which Novell won. The case was appealed, but on August 30, 2011, the United States Court of Appeals for the Tenth Circuit affirmed the trial decisions, closing the case.[28] Unix vendor SCO Group Inc. accused Novell of slander of title.\\r\\nThe present owner of the trademark UNIX is The Open Group, an industry standards consortium. Only systems fully compliant with and certified to the Single UNIX Specification qualify as \\"UNIX\\" (others are called \\"Unix-like\\").\\r\\nBy decree of The Open Group, the term \\"UNIX\\" refers more to a class of operating systems than to a specific implementation of an operating system; those operating systems which meet The Open Group's Single UNIX Specification should be able to bear the UNIX 98 or UNIX 03 trademarks today, after the operating system's vendor pays a substantial certification fee and annual trademark royalties to The Open Group.[29] Systems licensed to use the UNIX trademark include AIX,[30] HP-UX,[31] Inspur K-UX,[32] IRIX,[33] Solaris,[34] Tru64 UNIX (formerly \\"Digital UNIX\\", or OSF/1),[35] macOS,[36] and a part of z/OS.[37] Notably, Inspur K-UX is a Linux distribution certified as UNIX?03 compliant.[38][39]\\r\\nSometimes a representation like Un*x, *NIX, or *N?X is used to indicate all operating systems similar to Unix. This comes from the use of the asterisk (*) and the question mark characters as wildcard indicators in many utilities. This notation is also used to describe other Unix-like systems that have not met the requirements for UNIX branding from the Open Group.\\r\\nThe Open Group requests that UNIX is always used as an adjective followed by a generic term such as system to help avoid the creation of a genericized trademark.\\r\\nUnix was the original formatting, but the usage of UNIX remains widespread because it was once typeset in small caps (Unix). According to Dennis Ritchie, when presenting the original Unix paper to the third Operating Systems Symposium of the American Association for Computing Machinery (ACM), \\"we had a new typesetter and troff had just been invented and we were intoxicated by being able to produce small caps.\\"[40] Many of the operating system's predecessors and contemporaries used all-uppercase lettering, so many people wrote the name in upper case due to force of habit. It is not an acronym.[41]\\r\\nTrademark names can be registered by different entities in different countries and trademark laws in some countries allow the same trademark name to be controlled by two different entities if each entity uses the trademark in easily distinguishable categories. The result is that Unix has been used as a brand name for various products including book shelves, ink pens, bottled glue, diapers, hair driers and food containers.[42]\\r\\nSeveral plural forms of Unix are used casually to refer to multiple brands of Unix and Unix-like systems. Most common is the conventional Unixes, but Unices, treating Unix as a Latin noun of the third declension, is also popular. The pseudo-Anglo-Saxon plural form Unixen is not common, although occasionally seen. Sun Microsystems, developer of the Solaris variant, has asserted that the term Unix is itself plural, referencing its many implementations.[43]","input":"Who is the founder of unix operating system?"},{"output":"September 2013","context":"The National Curriculum for England was first introduced by the Education Reform Act of 1988. At the time of its introduction the legislation applied to both England and Wales. However, education later became a devolved matter for the Welsh government. The current statutory National Curriculum dates from 2014 at which point it was introduced to most year groups across primary and secondary education. Some element were introduced in September 2015. The National Curriculum sets out the content matter which must be taught in a number of subjects in \\"local authorityÿmaintained schools\\".[a]\\r\\n\\r\\n\\r\\nThere are two main aims presented in the statutory documentation for the National Curriculum, stating:\\r\\nThese aims set out to support the statutory duties of schools to offer a curriculum which is balanced and broadly based and which promotes the spiritual, moral, cultural, mental and physical development of pupils at the school and of society, while preparing pupils at the school for the opportunities, responsibilities and experiences of later life, as set out in the Education Act 2002.[1]\\r\\nThe National Curriculum is set out for all year groups for pupils aged between 5 and 16. Within these ages, the curriculum is structured into four Key Stages, for each of which a prescribed list of subjects must be taught. The table below sets out the statutory list of subjects to be taught at each Key Stage:[1]\\r\\nFor each of the statutory curriculum subjects, the Secretary of State for Education is required to set out a Programme of Study which outlines the content and matters which must be taught in those subjects at the relevant Key Stages.[2] The most recently-published National Curriculum was introduced into schools in September 2014.\\r\\nIn addition, children in all Key Stages must be provided with a curriculum of Religious Education, and for pupils in Key Stages 3 and 4 a curriculum of Sex and Relationships Education must also be provided. At Key Stage 4, although some subjects are not compulsory for all students, provision must be made to allow all students to access the arts (comprising art and design, music, dance, drama and media arts), design and technology, the humanities (comprising geography and history) and a modern foreign language.[1]\\r\\nThe first statutory National Curriculum was introduced by the Education Reform Act 1988 by Kenneth Baker.[3] The Programmes of Study were drafted and published in 1988 and 1989, with the first teaching of some elements of the new curriculum beginning in September 1989.\\r\\nA review of the National Curriculum in 1994, led by Ron Dearing was sought in order to find ways to 'slim down' the over-detailed curriculum. The final report set out the need to reduce the volume of statutory content, particularly at lower key stages, as well as recommending changes to methods of assessment.[4] Consequently, an updated National Curriculum was published in 1995 which saw a considerable reduction in the content of the curriculum and a simplification in line with Dearing's recommendations.\\r\\nWhen a new Labour government took office in 1997, its focus on English and Maths led to a decision to disapply the statutory Programmes of Study for the foundation subjects from September 1998, to allow schools to spend more time teaching literacy and numeracy.[5] The Secretary of State, David Blunkett later announced another overhaul of the National Curriculum, particularly at primary level, to reduce the content in foundation subjects allowing more time to be spent on the core subjects of English, Maths and Science.[6] A new National Curriculum was published in 1999, for first teaching in September 2000.[7]\\r\\nFurther changes were announced in 2007 for the statutory curriculum for Key Stages 3 and 4, which again focussed on removing some content from the documentation, while also adding some additional element, with the intended aim of additional flexibility for schools.[8] These changes were introduced in September 2008, and were swiftly followed by proposed changes to the primary curriculum, based on a review to be led by Jim Rose. The review proposed replacing the 10 statutory subjects in Key Stages 1 and 2 with 6 broader 'areas of learning', such as \\"understanding English, communication and languages\\" and \\"human, social and environmental understanding\\".[9]\\r\\nHowever, following the change of government in 2010, the plans for this change - proposed to begin in September 2011 - were abandoned,[10] with schools advised to continue to follow the 2000 curriculum pending review. An expert review panel was commissioned in 2010 to report on a framework for a new National Curriculum. The review was led by Tim Oates and reported in December 2011. It suggested significant changes to the structure of the National Curriculum, including dividing Key Stage 2 into two shorter (two-year) phases.[11]\\r\\nIn 2013, the government produced a draft National Curriculum, followed by a final version in September 2013, for first teaching in September 2014. Due to the short timescales for introduction, the curriculum was introduced only for certain subjects and year groups in 2014, with the core subjects in Years 2 and 6 (the final years of Key Stages 2 and 4) only becoming statutory in September 2015, to allow time for the introduction of new testing arrangements at the end of the Key Stages. Similarly, core subjects at Key Stage 4 are to be introduced on a year-by-year basis starting in September 2015 for English and Maths, and September 2016 for Science.[12]","input":"When was the most recent national curriculum published?"},{"output":"the KansasÿNebraska Act","context":"","input":"What event spurred the creation of the republican party?"},{"output":"The Kids of Degrassi Street, Degrassi Junior High, Degrassi High, Degrassi: The Next Generation and Degrassi: Next Class","context":"Degrassi is a Canadian drama franchise with over 621 episodes across all incarnations. It follows the lives of a group of teenagers who lived on or near De Grassi Street in Toronto, Ontario. The five main series are The Kids of Degrassi Street, Degrassi Junior High, Degrassi High, Degrassi: The Next Generation and Degrassi: Next Class.\\r\\nThe early Degrassi series were produced by the small production company owned by Kit Hood and Linda Schuyler, Playing With Time Inc. The recent version of Degrassi, produced by Epitome Pictures, aired on MTV Canada and on VRAK.TV (dubbed in French) in Canada and was simultaneously broadcast on TeenNick (and in syndication) in the United States. Degrassi: The Next Generations's eighth season aired on BBC Switch in the United Kingdom.\\r\\nThe second incarnation of The Next Generation, Degrassi: Next Class, is jointly produced by Netflix and Epitome (a subsidiary of DHX Media). The show began airing in January 2016 on Netflix internationally and on Family (F2N) in Canada.\\r\\n\\r\\n\\r\\nThe Kids of Degrassi Street, created by Linda Schuyler, was the first in the Degrassi franchise. It originally spawned from four short films: Ida Makes a Movie, Cookie Goes to the Hospital, Irene Moves In, and Noel Buys a Suit, which aired as after-school specials on CBC Television in 1979, 1980, 1981, and 1982, respectively.[1] The series continued from 1982 to 1986. Many actors from The Kids of Degrassi Street, including Neil Hope, Stacie Mistysyn, Anais Granofsky, and Sarah Charlesworth, would go on to appear in Degrassi Junior High and Degrassi High; however, their names and families were changed.[2] The show dealt with age-appropriate issues such as bad luck chain letters, honesty, divorce, and even death.[3]\\r\\nDegrassi Junior High aired for 42 episodes from 1987 to 1989. Later, much of the cast continued over into the spin-off series, Degrassi High, with some extra cast members and a new high school. Degrassi High aired on CBC and PBS for two years from 1989 until 1991. These series are often compared to Saved by the Bell and Beverly Hills, 90210, the latter of which began airing in the United States at the same time, except 90210 used actors who were in their twenties to play teenagers, whereas Degrassi used people who were the same age they were playing.[4] As with Saved by the Bell, Degrassi High follows teenagers going through everyday normal teen social issues, but problems are not solved within the episode; some plot-lines often continue through multiple episodes.\\r\\nA few months after the end of Degrassi High, a 90-minute made-for-TV film entitled School's Out was produced, which concluded the series. It sparked controversy and anger amongst fans and critics[citation needed] for the unusual characterization of familiar characters and infamous scenes of sexuality and coarse language. U.S. viewers saw a toned-down version in 1993, which did not feature the profanity Canadian viewers heard (WGBH released the uncensored version of the film onto video). A six-part documentary series entitled Degrassi Talks aired soon after.\\r\\nHood and Schuyler subsequently worked on a similar series, Liberty Street, which applied the Degrassi format to a series about people in their twenties living on their own for the first time. Pat Mastroianni, one of the most famous actors from the Degrassi series, appeared in Liberty Street as well, although playing a different character.\\r\\nIn 2001, the Degrassi series was revived by Stephen Stohn as Degrassi: The Next Generation. During the series Degrassi Junior High Christine Nelson gave birth to a baby girl named Emma, who became the lead character of the fourth show. This Degrassi series deals with issues that many teenagers must face in high school. It has had a successful run thus far and has grown its own distinct cult following amongst teenagers and adults alike. This series was broadcast on CTV, MuchMusic, MTV, and currently Family Channel. Outside Canada, it was rebroadcast to the United States on the cable channel TeenNick (The N prior to 2010) from 2002 to 2015, and also on MTV, BET and SOAPnet (until 2014), and to the Netherlands on Z@PP, to Brazil on the cable channel Multishow, to Australia on ABC3 and Nickelodeon, to Mexico, Peru, Venezuela and Chile on the cable channel MTV Latin America, and to Poland on the Canal+'s channel ZigZap.\\r\\nThis newer version of Degrassi has thus far dealt with more topics including online predators, suicide, censorship, gangs, self-harm, school shootings, imprisonment, rape, abuse, drugs, drinking, and murder, displaying the many challenges teenagers face in high school and the early years of college.\\r\\nOn 15 January 2009, Program Partners, a subsidiary of Sony Pictures Television, announced that they have acquired the syndication rights to the show, which will start showing daily on local stations in the US during the early evening fringe hours (between 5 and 7 pm) beginning in September.[5] One of the reasons of the program's sale in syndication is that its programming content complies with federal E/I programming requirements.\\r\\nThe broadcast company put together the first Degrassi: The Next Generation film, titled Degrassi Goes Hollywood in 2009, to end the eighth season. Season 9 finished 16 July 2010 with another two-hour film, titled Degrassi Takes Manhattan.\\r\\nSeason 10 premiered 19 July 2010, and marked a change in production style to a telenovela/soap opera format, and for the first time, episodes airing in Canada and the United States on the same day. \\"The Next Generation\\" was also dropped from the title, which became simply Degrassi.\\r\\nDegrassi: Next Class is the second incarnation of The Next Generation but is also considered its own show. After TeenNick and MTV Canada dropped the series, the show was picked up by Netflix and Family Channel. This \\"reboot\\" of the series was initially set to be the fifteenth season of \\"The Next Generation\\" (as casting calls were made for the fifteenth season) but ultimately Netflix and Epitome decided to start it off as a new show, to not confuse new viewers that would watch it on Netflix.\\r\\nSeason one was released on Netflix January 15, 2016, and started airing January 4, 2016 on Family's new teen programming block, F2N. Fourteen cast members from season 14 of Degrassi will also reprise their roles.[6][7][8][8][9][10][11]","input":"What is the order of the degrassi series?"},{"output":"April 1793","context":"The Committee of Public Safety (French: Comit de salut public)created in April 1793 by the National Convention and then restructured in July 1793formed the de facto executive government in France during the Reign of Terror (1793ÿ94), a stage of the French Revolution. The Committee of Public Safety succeeded the previous Committee of General Defence (established in January 1793) and assumed its role of protecting the newly established republic against foreign attacks and internal rebellion. As a wartime measure, the Committeecomposed at first of nine, and later of twelve, memberswas given broad supervisory powers over military, judicial, and legislative efforts. It was formed as an administrative body to supervise and expedite the work of the executive bodies of the Convention and of the government ministers appointed by the Convention. As the Committee tried to meet the dangers of a coalition of European nations and counter-revolutionary forces within the country, it became more and more powerful.\\r\\nIn July 1793, following the defeat at the Convention of the Girondins, the prominent leader of the radical Jacobin, Maximilien Robespierre, was added to the Committee. The power of the Committee peaked between August 1793 and July 1794. In December 1793, the Convention formally conferred executive power upon the Committee.\\r\\nThe execution of Robespierre in July 1794 represented a reactionary period against the Committee of Public Safety. This is known as the Thermidorian Reaction, as Robespierre's fall from power occurred during the Revolutionary calendar month of Thermidor. The Committee's influence diminished,[1] and it was disestablished in 1795.\\r\\n\\r\\n\\r\\nOn 5 April 1793, the French military commander and former minister of war General Charles Fran?ois Dumouriez defected to Austria, following the publication of an incendiary letter in which he threatened to march his army on the city of Paris if the National Convention did not accede to his leadership. News of his defection caused alarm in Paris, where imminent defeat by the Austrians and their allies was feared. A widespread belief held that revolutionary France was in immediate peril, threatened not only by foreign armies and by recent anti-revolutionary revolts in the Vende, but also by foreign agents who plotted the destruction of the nation from within.[2]\\r\\nThe betrayal of the revolutionary government by Dumouriez lent greater credence to this belief. In light of this threat, the Girondin leader Maximin Isnard proposed the creation of a nine-member Committee of Public Safety. Isnard was supported in this effort by Georges Danton, who declared, \\"This Committee is precisely what we want, a hand to grasp the weapon of the Revolutionary Tribunal.\\"[2]\\r\\nThe Committee was formally created on 6 April 1793. Closely associated with the leadership of Danton, it was initially known as \\"the Danton Committee\\".[3] Danton steered the Committee through the 31 May and 2 June 1793 journes that resulted in the fall of the Girondins, and through the intensifying war in the Vende. However, when the Committee was recomposed on 10 July, Danton was not included. Nevertheless, he continued to support the centralization of power by the Committee.[4]\\r\\nOn 27 July 1793, Maximilien Robespierre was elected to the Committee. At this time, the Committee was entering a more powerful and active phase, which would see it become a de facto dictatorship alongside its powerful partner, the Committee of General Security. The role of the Committee of Public Safety included the governance of the war (including the appointment of generals), the appointing of judges and juries for the Revolutionary Tribunal,[5] the provisioning of the armies and the public, the maintenance of public order, and oversight of the state bureaucracy.[6]\\r\\nThe Committee was also responsible for interpreting and applying the decrees of the National Convention, and thus for implementing some of the most stringent policies of the Terrorfor instance, the leve en masse, passed on 23 August 1793, the Law of Suspects, passed on 17 September 1793, and the Law of the Maximum, passed on 29 September 1793. The broad and centralized powers of the Committee were codified by the Law of 14 Frimaire (also known as the Law of Revolutionary Government) on 4 December 1793.[citation needed]\\r\\nOn 5 December 1793, journalist Camille Desmoulins began publishing Le Vieux Cordelier, a newspaper initially aimedwith the approval of Robespierre and the Committee of Public Safety[7]at the ultra-revolutionary Hbertist faction, whose extremist demands, anti-religious fervor, and propensity for sudden insurrections were problematic for the Committee. However, Desmoulins quickly turned his pen against the Committee of Public Safety and the Committee of General Security, comparing their reign to that of the Roman tyrants chronicled by Tacitus, and expounding the \\"indulgent\\" views of the Dantonist faction.[citation needed]\\r\\nConsequently, though the Hbertists were arrested and executed in March 1794, the Committee of Public Safety and the Committee of General Security ensured that Desmoulins and Danton were also arrested. Hrault de Schellesa friend and ally of Dantonwas expelled from the Committee of Public Safety, arrested, and tried alongside them. On 5 April 1794, the Dantonists went to the guillotine.[8]\\r\\nThe elimination of the Hbertists and the Dantonists made evident the strength of the committees, as had their ability to control and silence opposition. The creation, in March 1794, of a \\"General Police Bureau\\"reporting nominally to the Committee of Public Safety, but more often directly to Robespierre and his closest ally, Louis Antoine de Saint-Justserved to increase the power of the Committee of Public Safety, and of Robespierre himself.\\r\\nThe Law of 22 Prairial, proposed by the Committee of Public Safety and enacted on 10 June 1794, went further in establishing the iron control of the Revolutionary Tribunal and, above it, the Committees of Public Safety and General Security. The law enumerated various forms of public enemies, made mandatory their denunciation, and severely limited the legal recourse available to those accused. The punishment for all crimes under the Law of 22 Prairal was death. From the initiation of this law to the fall of Robespierre on 27 July, more people were condemned to death than in the entire previous history of the Revolutionary Tribunal.[9]\\r\\nHowever, even as the Terror reached its height, and with it the Committee's political power, discord was growing within the revolutionary government. Members of the Committee of General Security resented the autocratic behavior of the Committee of Public Safety, and particularly the encroachment of the General Police Bureau upon their own brief.[10] Arguments within the Committee of Public Safety itself had grown so violent that it relocated its meetings to a more private room to preserve the illusion of agreement.[11] Robespierre, a fervent supporter of the theistic Cult of the Supreme Being, found himself frequently in conflict with anti-religious Committee members Collot d'Herbois and Billaud-Varenne. Moreover, Robespierre's increasingly extensive absences from the Committee due to illness (he all but ceased to attend meetings in June 1794) created the impression that he was isolated and out of touch.\\r\\nWhen it became evident, in mid-July 1794, that Robespierre and Saint-Just were planning to strike against their political opponents Joseph Fouch, Jean-Lambert Tallien, and Marc-Guillaume Alexis Vadier (the latter two of whom were members of the Committee of General Security), the fragile truce within the government was dissolved. Saint-Just and his fellow Committee of Public Safety member Barre attempted to keep the peace between the Committees of Public Safety and General Security; however, on 26 July, Robespierre delivered a speech to the National Convention in which he emphasized the need to \\"purify\\" the Committees and \\"crush all factions.\\"[12] In a speech to the Jacobin Club that night, he attacked Collot d'Herbois and Billaud-Varenne, who had refused to allow the printing and distribution of his speech to the Convention.\\r\\nOn the following day, 27 July 1794 (or 9 Thermidor according to the Revolutionary calendar), Saint-Just began to deliver a speech to the Convention in which he had planned to denounce Collot d'Herbois, Billaud-Varenne, and other members of the Committee of Public Safety. However, he was almost immediately interrupted by Tallien and by Billaud-Varenne, who accused Saint-Just of intending to \\"murder the Convention.\\"[13] Barre, Vadier, and Stanislas Frron joined the accusations against Saint-Just and Robespierre. The arrest of Robespierre, his brother Augustin, and Saint-Just was ordered, along with that of their supporters, Philippe Le Bas and Georges Couthon.\\r\\nA period of intense civil unrest ensued, during which the members of the Committees of Public Safety and General Security were forced to seek refuge in the Convention. The Robespierre brothers, Saint-Just, Le Bas, and Couthon ensconced themselves in the H?tel de Ville, attempting to incite an insurrection. Ultimately, faced with defeat and arrest, Le Bas committed suicide. Saint-Just, Couthon, and Maximilien and Augustin Robespierre were arrested and guillotined on 28 July.[14]\\r\\nThe ensuing period of upheaval, dubbed the Thermidorian Reaction, saw the repeal of many of the Terror's most unpopular laws and the reduction in power of the Committees of General Security and Public Safety. The Committees ceased to exist under the Constitution of the Year III (1795), which marked the beginning of the Directory.[citation needed]\\r\\nThe Committee was initially composed of nine members, all selected by the National Convention for one month at a time, without term limits. Its first members, instated on 6 April 1793, were as follows, in order of election.\\r\\nAfter Robespierre's election to the Committee on 27 July 1793, the Committee increased its membership to twelve. The list below represents the Committee's membership.\\r\\nApril 1793 ÿ July 1793:\\r\\nJuly 1793 ÿ July 1794:","input":"When did the committee of public safety start?"},{"output":"non-neuronal cells that maintain homeostasis, form myelin, and provide support and protection for neurons in the central and peripheral nervous systems","context":"Neuroglia, also called glial cells, or simply glia are non-neuronal cells that maintain homeostasis, form myelin, and provide support and protection for neurons in the central and peripheral nervous systems.[1] In the central nervous system, glial cells include oligodendrocytes, astrocytes, ependymal cells and microglia, and in the peripheral nervous system glial cells include Schwann cells and satellite cells. The term derives from Greek ?ϫ and ?ϫ \\"glue\\"; pronounced in English as either /?li??/ or /?la??/\\r\\nAs the Greek name implies, glia are commonly known as the glue of the nervous system; however, this is not fully accurate. Neuroscience currently identifies four main functions of glial cells:\\r\\nGlia were discovered in 1856, by the pathologist Rudolf Virchow in his search for a \\"connective tissue\\" in the brain.[2] For over a century, it was believed that the neuroglia did not play any role in neurotransmission. However 21st century neuroscience has recognized that glial cells do have some effects on certain physiological processes like breathing,[3][4] and in assisting the neurons to form synaptic connections between each other.[5]\\r\\n\\r\\n\\r\\nDerived from ectodermal tissue.\\r\\nThe most abundant type of macroglial cell in the CNS,[6] astrocytes (also called astroglia) have numerous projections that link neurons to their blood supply while forming the blood-brain barrier. They regulate the external chemical environment of neurons by removing excess potassium ions, and recycling neurotransmitters released during synaptic transmission. Astrocytes may regulate vasoconstriction and vasodilation by producing substances such as arachidonic acid, whose metabolites are vasoactive.\\r\\nAstrocytes signal each other using ATP. The gap junctions (also known as electrical synapses) between astrocytes allow the messenger molecule IP3 to diffuse from one astrocyte to another. IP3 activates calcium channels on cellular organelles, releasing calcium into the cytoplasm. This calcium may stimulate the production of more IP3 and cause release of ATP through channels in the membrane made of pannexins. The net effect is a calcium wave that propagates from cell to cell. Extracellular release of ATP, and consequent activation of purinergic receptors on other astrocytes, may also mediate calcium waves in some cases.\\r\\nIn general, there are two types of astrocytes, protoplasmic and fibrous, similar in function but distinct in morphology and distribution. Protoplasmic astrocytes have short, thick, highly branched processes and are typically found in gray matter. Fibrous astrocytes have long, thin, less branched processes and are more commonly found in white matter.\\r\\nIt has recently been shown that astrocyte activity is linked to blood flow in the brain, and that this is what is actually being measured in fMRI.[7] They also have been involved in neuronal circuits playing an inhibitory role after sensing changes in extracellular calcium.[8]\\r\\nOligodendrocytes are cells that coat axons in the central nervous system (CNS) with their cell membrane, forming a specialized membrane differentiation called myelin, producing the so-called myelin sheath. The myelin sheath provides insulation to the axon that allows electrical signals to propagate more efficiently.[9]\\r\\nEpendymal cells, also named ependymocytes, line the spinal cord and the ventricular system of the brain. These cells are involved in the creation and secretion of cerebrospinal fluid (CSF) and beat their cilia to help circulate the CSF and make up the blood-CSF barrier. They are also thought to act as neural stem cells.[10]\\r\\nRadial glia cells arise from neuroepithelial cells after the onset of neurogenesis. Their differentiation abilities are more restricted than those of neuroepithelial cells. In the developing nervous system, radial glia function both as neuronal progenitors and as a scaffold upon which newborn neurons migrate. In the mature brain, the cerebellum and retina retain characteristic radial glial cells. In the cerebellum, these are Bergmann glia, which regulate synaptic plasticity. In the retina, the radial Mller cell is the glial cell that spans the thickness of the retina and, in addition to astroglial cells,[11] participates in a bidirectional communication with neurons.[12]\\r\\nSimilar in function to oligodendrocytes, Schwann cells provide myelination to axons in the peripheral nervous system (PNS). They also have phagocytotic activity and clear cellular debris that allows for regrowth of PNS neurons.[13]\\r\\nSatellite glial cells are small cells that surround neurons in sensory, sympathetic, and parasympathetic ganglia.[14] These cells help regulate the external chemical environment. Like astrocytes, they are interconnected by gap junctions and respond to ATP by elevating intracellular concentration of calcium ions. They are highly sensitive to injury and inflammation, and appear to contribute to pathological states, such as chronic pain.[15]\\r\\nAre found in the intrinsic ganglia of the digestive system. They are thought to have many roles in the enteric system, some related to homeostasis and muscular digestive processes.[16]\\r\\nMicroglia are specialized macrophages capable of phagocytosis that protect neurons of the central nervous system.[17] They are derived from the earliest wave of mononuclear cells that originate in yolk sac blood islands early in development, and colonize the brain shortly after the neural precursors begin to differentiate.[18]\\r\\nThese cells are found in all regions of the brain and spinal cord. Microglial cells are small relative to macroglial cells, with changing shapes and oblong nuclei. They are mobile within the brain and multiply when the brain is damaged. In the healthy central nervous system, microglia processes constantly sample all aspects of their environment (neurons, macroglia and blood vessels). In a healthy brain, microglia direct the immune response to brain damage and play an important role in the inflammation that accompanies the damage. Many diseases and disorders are associated with deficient microglia, such as, Alzheimer's disease, Parkinson's disease, and ALS.\\r\\nPituicytes from the posterior pituitary are glia cells with characteristics in common to astrocytes.[19] Tanycytes in the median eminence of the hypothalamus are a type of ependymal cell that descend from radial glia and line the base of the third ventricle.[20]\\r\\nIn general, neuroglial cells are smaller than neurons; there are about 86 billion neurons and 85 billion \\"nonneuronal\\" (glial) cells in the human male brain. Glial cells make up about half the total volume of the brain and spinal cord.[21][22]^ The ratio differs from one part of the brain to another. The glia/neuron ratio in the cerebral cortex is 3.72 (60.84 billion glia (72%); 16.34 billion neurons), while that of the cerebellum is only 0.23 (16.04 billion glia; 69.03 billion neurons). The ratio in the cerebral cortex gray matter is 1.48 and for the combined gray and white matter is 3.76.[21] The ratio of the basal ganglia, diencephalon and brainstem combined is 11.35.[21]\\r\\nMost cerebral cortex glia are oligodendrocytes (75.6%); astrocytes account for 17.3% and microglia for 6.5%.[23]\\r\\nMost glia are derived from ectodermal tissue of the developing embryo, in particular the neural tube and crest. The exception is microglia, which are derived from hemopoietic stem cells. In the adult, microglia are largely a self-renewing population and are distinct from macrophages and monocytes, which infiltrate the injured and diseased CNS.\\r\\nIn the central nervous system, glia develop from the ventricular zone of the neural tube. These glia include the oligodendrocytes, ependymal cells, and astrocytes. In the peripheral nervous system, glia derive from the neural crest. These PNS glia include Schwann cells in nerves and satellite glial cells in ganglia.\\r\\nCurrent research involving glial cells in the human cochlea proposes that these cells are the common precursor to both mature Schwann cells and satellite glial cells. Additionally, the peripheral glial cells located along the peripheral processes expressed NGFR, indicating a phenotype distinct from the peripheral glial cells located along the central processes.[24]\\r\\nGlia retain the ability to undergo cell division in adulthood, whereas most neurons cannot. The view is based on the general deficiency of the mature nervous system in replacing neurons after an injury, such as a stroke or trauma, while very often there is a profound proliferation of glia, or gliosis near or at the site of damage. However, detailed studies found no evidence that 'mature' glia, such as astrocytes or oligodendrocytes, retain the ability of mitosis. Only the resident oligodendrocyte precursor cells seem to keep this ability after the nervous system matures. On the other hand, there are a few regions in the mature nervous system, such as the dentate gyrus of the hippocampus and the subventricular zone, where generation of new neurons can be observed.[25]\\r\\nGlial cells are known to be capable of mitosis. By contrast, scientific understanding of whether neurons are permanently post-mitotic,[26] or capable of mitosis,[27][28][29] is still developing. In the past, glia had been considered[by whom?] to lack certain features of neurons. For example, glial cells were not believed to have chemical synapses or to release transmitters. They were considered to be the passive bystanders of neural transmission. However, recent studies have shown this to be untrue.[30]\\r\\nSome glial cells function primarily as the physical support for neurons. Others regulate the internal environment of the brain, especially the fluid surrounding neurons and their synapses, and nutrify neurons. During early embryogenesis, glial cells direct the migration of neurons and produce molecules that modify the growth of axons and dendrites.\\r\\nGlia are also crucial in the development of the nervous system and in processes such as synaptic plasticity and synaptogenesis. Glia have a role in the regulation of repair of neurons after injury. In the central nervous system (CNS), glia suppress repair. Glial cells known as astrocytes enlarge and proliferate to form a scar and produce inhibitory molecules that inhibit regrowth of a damaged or severed axon. In the peripheral nervous system (PNS), glial cells known as Schwann cells promote repair. After axonal injury, Schwann cells regress to an earlier developmental state to encourage regrowth of the axon. This difference between the CNS and the PNS, raises hopes for the regeneration of nervous tissue in the CNS. For example, a spinal cord may be able to be repaired following injury or severance. Schwann cells are also known as neuri-lemmocytes. These cells envelop nerve fibers of the PNS by winding repeatedly around a nerve fiber with the nucleus inside of it. This process creates a myelin sheath, which not only aids in conductivity but also assists in the regeneration of damaged fibers.\\r\\nOligodendrocytes are another type of glial cell of the CNS. These dendrocytes resemble an octopus bulbous body and contain up to fifteen arm-like processes. Each arm reaches out to a nerve fiber and spirals around it, creating a myelin sheath. This myelin sheath insulates the nerve fiber from the extracellular fluid as well as speeds up the signal conduction in the nerve fiber.[31]\\r\\nRecent research indicates that glial cells of the hippocampus and cerebellum participate in synaptic transmission, regulate the clearance of neurotransmitters from the synaptic cleft, and release gliotransmitters such as ATP, which modulate synaptic function.[32]\\r\\nAstrocytes are crucial in clearance of neurotransmitters from within the synaptic cleft, which provides distinction between arrival of action potentials and prevents toxic build-up of certain neurotransmitters such as glutamate (excitotoxicity). It is also thought that glia play a role in many neurological diseases, including Alzheimer's disease.[33] Furthermore, at least in vitro, astrocytes can release gliotransmitter glutamate in response to certain stimulation. Another unique type of glial cell, the oligodendrocyte precursor cells or OPCs, have very well-defined and functional synapses from at least two major groups of neurons.[34] The only notable differences between neurons and glial cells are neurons' possession of axons and dendrites, and capacity to generate action potentials.\\r\\nWhile glial cells in the PNS frequently assist in regeneration of lost neural functioning, loss of neurons in the CNS does not result in a similar reaction from neuroglia.[13] In the CNS, regrowth will only happen if the trauma was mild, and not severe.[35] When severe trauma presents itself, the survival of the remaining neurons becomes the optimal solution. However, some studies investigating the role of glial cells in Alzheimer's Disease are beginning to contradict the usefulness of this feature, and even claim it can \\"exacerbate\\" the disease.[36] In addition to impacting the potential repair of neurons in Alzheimer's Disease, scarring and inflammation from glial cells have been further implicated in the degeneration of neurons caused by Amyotrophic lateral sclerosis.[37]\\r\\nIn addition to neurodegenerative diseases, a wide range of harmful exposure, such as hypoxia, or physical trauma, can lead to the end result of physical damage to the CNS.[35] Generally, when damage occurs to the CNS, glial cells cause Apoptosis among the surrounding cellular bodies.[35] Then, there is a large amount of microglial activity, which results in inflammation, and finally, there is a heavy release of growth inhibiting molecules.[35]\\r\\nGlia were first described in 1856 by the pathologist Rudolf Virchow in a comment to his 1846 publication on connective tissue. A more detailed description of glial cells was provided in the 1858 book Cellular Pathology by the same author.[38]\\r\\nWhen markers for different types of cells were analyzed, Einstein's brain was discovered to contain significantly more glia than normal brains in the left angular gyrus, an area thought to be responsible for mathematical processing and language.[39]\\r\\nThe ratio of glia to neurons increases with our definition of intelligence. Not only does the ratio of glia to neurons increase through evolution, but so does the size of the glia. Astroglial cells in the human have a volume 27 times greater than the same cells in the mouse's brain.[40]\\r\\nThese important scientific findings may begin to shift the neuron-specific perspective into a more holistic view of the brain which encompasses the glial cells as well. The glia's importance is becoming ever more clear as time goes on and new research is conducted. For the vast part of the last century, scientists have written off glial cells to be nothing more than the structure and foundations that hold the neurons in place. But now, there is direct evidence that correlates the number of glial cells in the brain with the amount of intelligence that any given species possesses.[41] Future research will begin to shed light on the mysterious, yet increasingly crucial, role of glial cells.","input":"What are neuroglia and what is their function?"},{"output":"James Clark Ross","context":"The Ross Ice Shelf is the largest ice shelf of Antarctica (as of 2013 an area of roughly 500,809 square kilometres (193,363?sq?mi)[1] and about 800 kilometres (500?mi) across: about the size of France).[2] It is several hundred metres thick. The nearly vertical ice front to the open sea is more than 600 kilometres (370?mi) long, and between 15 and 50 metres (50 and 160?ft) high above the water surface.[3] Ninety percent of the floating ice, however, is below the water surface.\\r\\nMost of Ross Ice Shelf is in the Ross Dependency claimed by New Zealand. It floats in, and covers, a large southern portion of the Ross Sea and the entire Roosevelt Island located in the west of the Ross Sea.\\r\\nThe ice shelf is named after Captain Sir James Clark Ross, who discovered it on 28 January 1841. It was originally called The Barrier, with various adjectives including Great Ice Barrier, as it prevented sailing further south. Ross mapped the ice front eastward to 160W. In 1947, the US Board on Geographic Names applied the name Ross Shelf Ice to this feature and published it in the original US Antarctic Gazetteer. In January 1953 the name was changed to Ross Ice Shelf; that name was published in 1956.[4][5]\\r\\n\\r\\n\\r\\nOn January 5, 1841, the British Admiralty's Ross expedition in the Erebus and the Terror, three-masted ships with specially strengthened wooden hulls, was going through the pack ice of the Pacific near Antarctica in an attempt to determine the position of the South Magnetic Pole. Four days later, they found their way into open water and were hoping that they would have a clear passage to their destination. But on 11 January, the men were faced with an enormous mass of ice.\\r\\nSir James Clark Ross, the expedition's commander, remarked: \\"Well, there's no more chance of sailing through that than through the cliffs of Dover\\". Ross, who in 1831 had located the North Magnetic Pole, spent the next two years vainly searching for a sea passage to the South Pole; later, his name was given to the ice shelf and the sea surrounding it. Two volcanoes in the region were named by Ross for his vessels.[6]\\r\\nFor early Antarctic explorers seeking to reach the South Pole, the Ross Ice Shelf became a starting area. In a first exploration of the area by the Discovery Expedition in 1901-1904, Robert Falcon Scott made a significant study of the shelf and its surroundings from his expedition's base on Ross Island. By measurement of calved ice bergs and their buoyancy, he estimated the ice sheet to be on average 900 feet thick; the undisturbed morphology of the ice sheet and its inverted temperature profile led him to conclude it was floating on water; and measurements in 1902-1903 showed it had advanced 608 yards northwards in 13.5 months.[7] The findings were presented at a lecture entitled \\"Universitas Antarctica!\\" given 7 June 1911 and were published in the account of Scott's second expedition (the Terra Nova Expedition of 1910-1913).[8]\\r\\nBoth Roald Amundsen and Scott crossed the shelf to reach the Pole in 1911. Amundsen wrote: \\"Along its outer edge the Barrier shows an even, flat surface; but here, inside the bay, the conditions were entirely different. Even from the deck of the Fram we were able to observe great disturbances of the surface in every direction; huge ridges with hollows between them extended on all sides. The greatest elevation lay to the south in the form of a lofty, arched ridge, which we took to be about 500 feet [150?m] high on the horizon. But it might be assumed that this ridge continued to rise beyond the range of vision\\".\\r\\nThe next day, the party made its first steps on the Barrier. \\"After half an hours march we were already at the first important pointthe connection between the sea-ice and the Barrier. This connection had always haunted our brains. What would it be like? A high, perpendicular face of ice, up which we should have to haul our things laboriously with the help of tackles? Or a great and dangerous fissure, which we should not be able to cross without going a long way round? We naturally expected something of the sort. This mighty and terrible monster would, of course, offer resistance in some form or other,\\" he wrote.\\r\\n\\"The mystic Barrier! All accounts without exception, from the days of Ross to the present time, had spoken of this remarkable natural formation with apprehensive awe. It was as though one could always read between the lines the same sentence: 'Hush, be quiet! the mystic Barrier!'\\r\\n\\"One, two, three, and a little jump, and the Barrier was surmounted!\\"[9]\\r\\nIce shelves are thick plates of ice, formed continuously by glaciers, that float atop an ocean. The presence of the shelves acts as \\"brakes\\" for the glaciers. These shelves serve another important purpose -- \\"they moderate the amount of melting that occurs on the glaciers' surfaces. Once their ice shelves are removed, the glaciers increase in speed due to meltwater percolation and/or a reduction of braking forces, and they may begin to dump more ice into the ocean than they gather as snow in their catchments. Glacier ice speed increases are already observed in Peninsula areas where ice shelves disintegrated in prior years.\\"[10]\\r\\nThe Ross Ice Shelf is one of many such shelves. It reaches into Antarctica from the north, and covers an area of about 520,000?km2 (200,000?sq?mi), nearly the size of France.[2][3] The ice mass is about 800?km (500?mi) wide and 970?km (600?mi) long. In some places, namely its southern areas, the ice shelf can be almost 750?m (2,450?ft) thick. The Ross Ice Shelf pushes out into the sea at between 1.5 and 3?m (5 and 10?ft) a day. Other glaciers gradually add bulk to it. At the same time, the freezing of seawater below the ice mass increases the thickness of the ice from 40 to 50?cm (16 to 20?in)[when?]. Sometimes, fissures and cracks may cause part of the shelf to break off; the largest known is about 31,000?km2 (12,000?sq?mi), that is, slightly larger than Belgium.[11] Iceberg B-15, the world's largest recorded iceberg, was calved from the Ross Ice Shelf during March 2000.\\r\\nScientists have long been intrigued by the shelf and its composition. In fact, many scientific teams researching the Antarctic have made camps on or adjacent to the Ross Ice Shelf. This includes McMurdo Station[12] One major effort was a series of studies conducted in 1957 and 1958, which were continued during the 1960ÿ61 season. The efforts involved an international team of scientists. Some parties explored the glaciers and others the valleys on the ice shelf.[13]\\r\\nFrom 1967 to 1972 the Scott Polar Research Institute reported extensive observations using radio echo sounding. The technique allowed measurements to be taken from the air; allowing a criss cross track of 35,000?km to be covered; compared with a 3,000?km track from previous seismic sounding on the ground.[14] More detailed surveys were executed between 1973 and 1978.\\r\\nA significant scientific endeavor called the Ross Ice Shelf Project was launched with a plan of drilling into the shelf to sample the biomass in the area and make other determinations about the shelf and its relationship to the sea floor. The Project included surface glaciological observations as well as drilling, and the glaciological portion started during the planning phase of the drilling.[15] The drilling portion of the project was to have begun during 1974, but the actual drilling was delayed until 1976. Finally, in 1977, the scientists were able to drill successfully through the ice, making a hole that could be sampled every few days for three weeks. The team was able to map the sea floor, study the tides, and assess the fish and various other forms of life in the waters. The team also examined the oceanographic and geological conditions as well as the temperature of the ice. They estimated that the base of the shelf was ?2.16 Celsius (27.3?F). They also made other calculations about the fluctuations of the temperatures.[12]\\r\\nThe results of these various projects were published in a series of reports in the 2 February 1979 issue of Science.[12]\\r\\nDuring the 1980s, a network of weather stations were installed to record temperatures on the shelf and throughout the more remote parts of the continent.[16]\\r\\nUniversity of Colorado's National Snow and Ice Data Center has been studying ice shelves and, in 2002, announced that, based on several breakups of ice shelves, including Larsen B, has begun to reassess their stability. Their scientists stated that the temperature of the warmest portion of the shelf is \\"only a few degrees too cool in summer presently to undergo the same kind of retreat process. The Ross Ice Shelf is the main outlet for several major glaciers draining the West Antarctic Ice Sheet, which contains the equivalent of 5 m of sea level rise in its above-sea-level ice.\\" The report added that observations of \\"iceberg calving\\" on the Ross Ice Shelf are, in their opinion, unrelated to its stability.[10]\\r\\nScientific exploration continues to uncover interesting information and the analyses have resulted in some interesting theories being posited and publicized. One such opinion, given in 2006 based on a geological survey, suggested that the ice shelf had collapsed previously, perhaps suddenly, which could well happen again.[17]\\r\\nCoordinates: 8130S 17500W? / ?81.500S 175.000W? / -81.500; -175.000","input":"Who is the ross ice shelf named after?"},{"output":"late 2016","context":"The Handmaid's Tale is an American television series created by Bruce Miller based on the 1985 novel of the same name by Canadian writer Margaret Atwood. It was ordered by streaming service Hulu with a straight-to-series order of 10 episodes, with the production beginning in late 2016.\\r\\nThe first three episodes of the series premiered on April 26, 2017, with the subsequent seven episodes added on a weekly basis every Wednesday. In May 2017, it was renewed for a second season to premiere in 2018. The series garnered extremely positive reviews and won eight Primetime Emmy Awards from thirteen nominations, including Outstanding Drama Series in 2017. It is the first series on a streaming platform to win an \\"Outstanding Series\\" Emmy.[1]\\r\\n\\r\\n\\r\\nIn the near future, human fertility rates collapse as a result of sexually transmitted diseases and environmental pollution.[2] With this chaos in place, the totalitarian, Christian theonomic government of \\"Gilead\\" establishes rule in the former United States in the aftermath of a civil war.[3][4][5] Society is organized by power-hungry leaders along a new, militarized, hierarchical regime of fanaticism and newly created social classes, in which women are brutally subjugated, and by law are not allowed to work, own property, handle money, or read.[5] Worldwide infertility has resulted in the conscription of the few remaining fertile women in Gilead, called handmaids, according to an \\"extremist interpretation\\" of a Biblical account.[5] They are assigned to the homes of the ruling elite, where they must submit to ritualized rape with their male masters in order to become pregnant and bear children for those men and their wives.\\r\\nJune Osborne, renamed Offred (Elisabeth Moss), is the Handmaid assigned to the home of Gileadan Commander Fred Waterford (Joseph Fiennes) and his wife Serena Joy (Yvonne Strahovski). She is subject to the strictest rules and constant scrutiny; an improper word or deed on her part can lead to brutal punishment. Offred, who is named after her male master like all Handmaids, can remember the \\"time before\\", when she was married with a daughter and had her own name and identity, but all she can safely do now is follow the rules of Gilead in the hope that she can someday live free and be reunited with her daughter. The Waterfords, key players in the rise of Gilead, have their own conflicts with the realities of the society they have helped create.\\r\\nA straight-to-series order by Hulu of The Handmaid's Tale was announced in April 2016, with Elisabeth Moss set to star.[15] Based on the 1985 novel of the same name by Margaret Atwood, the series was created by Bruce Miller, who is also an executive producer with Daniel Wilson, Fran Sears, and Warren Littlefield.[15] Atwood serves as consulting producer, giving feedback on some of the areas where the series expands or modernizes the book.[15][16] She also co-wrote every episode[17] and also had a small cameo role in the first episode.[18] Moss is also a producer.[19] In June 2016, Reed Morano was announced as director of the series.[20] Samira Wiley, Max Minghella, and Ann Dowd joined the cast in July 2016.[21][22][23] Joseph Fiennes, Madeline Brewer and Yvonne Strahovski were cast in August 2016,[24][25][26] followed by O. T. Fagbenle and Amanda Brugel in September 2016.[27][28] In October 2016, Ever Carradine joined the cast,[29] and Alexis Bledel was added in January 2017.[30]\\r\\nFilming on the series took place in Toronto, Mississauga, Hamilton, and Cambridge, Ontario, from September 2016 to February 2017.[31][32] The first full trailer of the TV series was released by Hulu on YouTube on March 23, 2017.[33] The series premiered on April 26, 2017.[34]\\r\\nOn May 3, 2017, The Handmaid's Tale was renewed for a second season to premiere in 2018.[35] Moss told the news media that the subsequent episodes will cover further developments in the story, filling in some of the unanswered questions and continuing the narrative already \\"finished\\" in the book.[36] The second season will consist of 13 episodes and will begin filming in fall 2017.[37] Alexis Bledel will return as a series regular. Showrunner Bruce Miller stated that he envisioned ten seasons of the show, stating \\"Well, you know, honestly, when I started, I tried to game out in my head what would ten seasons be like? If you hit a home run, you want energy to go around the bases, you want enough story to keep going, if you can hook the audience to care about these people enough that they're actually crying at the finale.\\"[38]\\r\\nThe first three episodes of the series premiered on April 26, 2017, with the subsequent seven episodes added on a weekly basis.[34][39] In Canada, the series is broadcast by Bravo, beginning with the first two episodes premiering on April 30.[40] In Scandinavia, the series is available on HBO Nordic.[41] In the UK and Ireland, the series premiered on May 28, 2017, and airs every Sunday on Channel 4.[42]\\r\\nIn New Zealand, the series was released on the subscription video on demand service Lightbox on June 8, 2017.[43] In Australia, the series premiered on the TV channel SBS's video streaming service SBS on Demand on July 6, 2017.[44]\\r\\nThe Handmaid's Tale has received high acclaim from television critics. On Metacritic, it has a score of 92 out of 100 based on 40 reviews, indicating \\"universal acclaim\\".[45] The season has a 96% approval rating on Rotten Tomatoes with an average score of 8.63 out of 10 based on 92 reviews. The site's critical consensus is, \\"Haunting and vivid, The Handmaid's Tale is an endlessly engrossing adaptation of Margaret Atwood's dystopian novel that's anchored by a terrific central performance from Elisabeth Moss.\\"[46] Daniel Fienberg of The Hollywood Reporter called it \\"probably the spring's best new show and certainly its most important\\".[47] Jen Chaney of Vulture gave it a highly positive review, and wrote that it is \\"A faithful adaptation of the book that also brings new layers to Atwood's totalitarian, sexist world of forced surrogate motherhood\\" and that \\"this series is meticulously paced, brutal, visually stunning, and so suspenseful from moment to moment that only at the end of each hour will you feel fully at liberty to exhale\\".[48]\\r\\nThere was much debate on whether parallels could be drawn between the series (and by extension, the book it is based on) and American society following Donald Trump's and Mike Pence's election as President of the United States and Vice President of the United States, respectively.[49][50]","input":"When did handmaid's tale go into production?"},{"output":"Timothy Bradley","context":"Manny Pacquiao vs. Timothy Bradley, billed as \\"Perfect Storm\\", was a welterweight title boxing match held on June 9, 2012, at the MGM Grand Garden Arena in Paradise, Nevada.[1] After 12 rounds, Bradley won a highly controversial split decision to take the WBO welterweight title.[2][3]\\r\\n\\r\\n\\r\\nPacquiao came into the fight following a controversial November 2011 majority decision victory in Manny Pacquiao vs. Juan Manuel Marquez III. He brought with him world championships in eight different weight classesa feat unmatched by anyone.[4]\\r\\nAfter fight negotiations between Pacquiao and Floyd Mayweather, Jr., ended without coming to terms, Mayweather opted to challenge junior middleweight titlist Miguel Cotto on May 5. Cotto had also been in the running to face Pacquiao in a rematch, but Pacquiao and Cotto could not agree on the weight for the fight. Pacquiao wanted the 147-pound welterweight limit, which Cotto said was too low for him, and he accepted a fight with Mayweather.[5] Also in the running to face Pacquiao were Lamont Peterson and Juan Manuel Marquez. Ultimately, it was Timothy Bradley who came to terms, agreeing to move up a weight class to challenge Pacquiao for his title.[5]\\r\\nBradley came into the fight with a somewhat lower profile than a typical Pacquiao opponent. In January 2011, he won a 10th-round technical decision in a junior welterweight title unification bout against Devon Alexander.[4] Previously, he had beaten then world-champion Lamont Peterson to claim the title.[4] After the fight with Alexander, Bradley rejected an offer to fight Amir Khan and further unify the 140-pound division. He signed with Top Rank and was given a co-feature slot on the Pacquiao-Marquez III pay-per-view card in November. In his match, Bradley beat former lightweight champion Joel Casamayor in an eight-round fight.[6]\\r\\nPrior to the Pacquiao fight, Bradley was ranked as one of the top 10 pound for pound fighters in the world. In a show of confidence, Bradley had \\"rematch\\" posters printed up, implying he would win the fight.[7]\\r\\nPacquiao entered the fight as a substantial favorite, among both bookmakers and fans. Bradley entered the fight as a 7-2 underdog in the Vegas line. The glove weight was set at 8 ounces each. Midway through the first round, the crowd spontaneously began to chant \\"Manny, Manny, Manny?...\\"[4]\\r\\nThroughout the fight, action was fast-paced, with Pacquiao landing the harder punches.[4] He appeared to take control early, beating Bradley on most exchanges.[7] However, Bradley did make good use of the punches he was able to throw, connecting on some of them.[8] Pacquiao won the first round, according to two of the three judges. Bradley was the more aggressive fighter in the second, backing Pacquiao against the ropes at one point. However, Pacquiao's patience paid off as he countered effectively at the end of the round.[8] Two of the judges awarded the round to Bradley. According to trainer Joel Diaz, Bradley hurt his ankle during the round, but chose to press on.[7] Pacquaio took control in the third round, winning on all three judges' cards. By the end of the round, Bradley \\"looked stunned\\".[8]\\r\\nDuring the fourth and fifth rounds, Bradley seemed to be visibly hurting.[7] In the fourth round, Bradley was cautioned for a low blow, and forced on defense throughout. During the final seconds of the round, Bradley appeared close to going down multiple times.[8] Pacquiao remained on the offensive during the fifth, but Bradley managed to land some counterpunches.[8] All three judges gave the fourth round to Pacquiao, but only one awarded him the fifth.\\r\\nPacquiao dominated the sixth round, trapping Bradley in the corner, and getting off several consecutive unanswered punches.[8] All judges gave Pacquiao the sixth round.\\r\\nThings started to turn in Bradley's favor during the second half of the fight, as Pacquiao struggled to land big punches. Around the seventh round, Bradley began to use his double jab effectively, and avoid Pacquiao's counterpunches.[4] He moved better, and got off his own counterpunches.[7] \\"I got my second wind in the sixth round,\\" Bradley said after the fight.[7] Bradley won the seventh round, according to all three judges, with Pacquiao being backed into the ropes near the end of the round.[8]\\r\\nThe eighth round saw a brief stoppage after Bradley appeared to get caught in the eye by Pacquiao's thumb. After the stoppage, Bradley appeared to be in trouble, but reduced his arm swing, to avoid further counterattacks by Pacquiao.[8] Two judges scored the round in favor of Bradley. In the ninth round, Pacquiao was able to take advantage of Bradley's aggression, and drive him back into the ropes.[8] Bradley left himself too open to counterpunches, and Pacquiao won the round on two of the three judges' cards. Bradley bounced back in the tenth round, winning on all cards, thanks in part to a good head shot midway through the round.[8]\\r\\nDuring the eleventh round, Bradley appeared to be gaining momentum, landing a series of punches, and backing Pacquiao against the ropes. The crowd began to chant Bradley's name.[8] Pacquiao silenced the crowd with a series of punches of his own, but two judges gave the round to Bradley. In the final round, both fighters pushed the action. Bradley landed a strong shot to Pacquiao's jaw in the final minute, and the match ended with a series of wild punches by both parties.[8] Bradley won the final round on all three scorecards.\\r\\nIn the end, Bradley won a highly controversial split decision over Pacquiao. Judges Duane Ford and C. J. Ross scored the fight 115ÿ113 in favor of Bradley, while Jerry Roth scored the fight 115ÿ113 in favor of Pacquiao.[4] It was Pacquiao's first defeat in seven years.[4] For his win, Bradley received a US$5 million checkthe largest prize of his career.[7]\\r\\nSource:[10]\\r\\nFollowing the announcement of the decision, the crowd booed loudly.[12] Pacquiao appeared to be stunned by the result. Fight promoter Bob Arum was irate, and said that he was worried about boxing's credibility in the aftermath of the decision.[7] He also questioned the competence of the judges.[13] Arum also stated that before the scorecards were read out, Bradley told him \\"I tried hard but I couldn't beat the guy.\\"[7][14]A few reporters questioned Bob Arum's sincerity in his comments and hinted that he was in on the bad decisions. One reporter, Tito Garcia of FanvsFan.com, interrupted Arum's press conference by loudly accusing Arum of fixing the fight. Garcia was physically ejected from the press conference.\\r\\nPacquiao said he was shocked by the result, but accepted the decision respectfully, saying \\"I did my best. I guess my best wasn't good enough,\\" adding, \\"He never hurt me with his punches, most of them landed on my arms.\\"[7] Immediately after the fight, Bradley stated, \\"Every round was pretty close. I got to go home and view the tape and see if I really won.\\"[15] He later remarked, \\"I thought I won the fight. I didn't think he was as good as everyone says he was. I didn't feel his power.\\"[7]\\r\\nThe decision was criticized by many commentators. The Associated Press and the Los Angeles Times scored the fight 117ÿ111 in favor of Pacquiao,[7][8] while ESPN and Harold Lederman of HBO both scored the fight 119ÿ109, also in Pacquiao's favor.[16][17] ESPN boxing analyst and longtime trainer Teddy Atlas said that Pacquiao clearly won, and that it was a case of either incompetence or corruption. He added that boxing needs a national commission, so that judges can be accountable for their decisions.[18] According to the AP's count, Pacquiao landed 253 punches to Bradley's 159.[7] Former champion Oscar De La Hoya said that Bradley should have refused the title belt, and Pacquiao had won the fight.[19]\\r\\nHowever, not everyone was upset by the result. Corbin Middlemas of the Australian Broadcasting Corporation said the fight was very close and difficult to score. \\"For all of the sports vices, this [decision] wasn't one of them,\\" he said.[4] The only poor decision would have been to award either fighter victory by a large margin, he added.[4]\\r\\nFollowing the event, Bradley announced that he would welcome a rematch,[7] that of which was also held at the MGM Grand Garden Arena on April 12, 2014. Pacquiao defeated Bradley via unanimous decision and regained the WBO title. Manny Pacquiao vs. Timothy Bradley III then took place almost two years later on April 9, 2016. Pacquiao again defeated Bradley via unanimous decision and knocked him down twice along the way.\\r\\nSource:[20]\\r\\nTotal, 55: Pacquiao 52 | Bradley 3.\\r\\nAfter the decision the WBO ordered a video review of the bout. The five judges on the WBO's committee all scored the fight in Pacquiao's favor  118ÿ110, 117ÿ111, 117ÿ111, 116ÿ112 and 115ÿ113. However, the WBO cannot overturn the result of the fight.[22]\\r\\nThe fight drew 890,000 pay-per-view purchases.[23]","input":"Who won the boxing match pacquiao vs bradley?"},{"output":"August 25, 1939","context":"","input":"When was the wizard of oz first released?"},{"output":"Port of Spain","context":"Coordinates: 103911N 613042W? / ?10.65306N 61.51167W? / 10.65306; -61.51167\\r\\nThe Red House is the seat of Parliament in the Republic of Trinidad and Tobago. The architectural design of the Red House is of Beaux-Arts style. The original building was destroyed in the 1903 water riots and was rebuilt in the year 1907. The Red House is located centrally within the capital city Port of Spain. It is currently used as a meeting place for parliament and elections and for political uses.\\r\\nIn July 1990, the Red House was the site of the Jamaat al Muslimeen coup attempt, during which the Prime Minister and other members of the government were held hostage for six days and 24 people were killed.\\r\\n\\r\\n\\r\\nOn 15 February 1844, then Governor of Trinidad, Sir Henry McLeod, laid the foundation stone for a new government administration building on the western edge of the Woodford Square, then known as Brunswick Square. This building was to be constructed on eight privately owned parcels of land. The then Superintendent of Public Works, Mr. Richard Bridgens designed the building and it was constructed by contractors, Messrs G. de la Sauvagere and A. A. Pierre.\\r\\nThe building was to consist of a north and south blocks and connected by a double archway, much as the Red House of today. The double archway was a necessary architectural feature required by the City Council to keep patent Prince Street, as the buildings were constructed over this street. The Council stipulated that Prince Street should never be closed to the public and, pedestrian and wheeled traffic should pass freely. The southern block completed in 1848 contained the courts of law and the northern to colonial administration offices.\\r\\nThe buildings were slow to complete and, by 1892, they still were unfinished, as indicated by a quote within the newspaper, the Port-of-Spain Gazette:\\r\\n\\"Nothing further had been done to complete the buildings since their erection some fifty years ago. The only attempt to relieve the monotony of the whole is to be seen in the arching of the carriageway through the courtyard which is a perfect skeleton and, like the ruins of Pompeii, is more suggestive of what the buildings must have been than of what they were intended to be.\\"\\r\\nAfter many alterations and additions the buildings were completed at a cost of S15,000.\\r\\nIn 1897, as Trinidad was preparing to celebrate the Diamond Jubilee of Queen Victoria, the buildings were given a coat of red paint, and the public promptly referred to them thereafter as the Red House. This direct ancestor of the present Red House was burnt to the ground on 23 March 1903.\\r\\nOn the day of the fire, while the a new ordinance regarding the distribution of and payment for water in the town was being debated in the Legislative Council, a protest meeting was held in Brunswick Square by the Ratepayer's Association, as there was much public dissatisfaction over certain clauses contained in the ordinance which increased the water rates. At the end of the meeting, the crowds became noisy and stones were thrown, and all the windows of the original Red House were smashed including a stained glass window in the chamber which was erected to commemorate the arrival of Christopher Columbus in Trinidad.\\r\\nWhen a woman was arrested by a policeman, the mob became riotous. Stones were thrown into the Council Chamber and the Members were forced to protect themselves under tables and desks and behind the pillars. Still, the Governor, Sir Cornelius Alfred Moloney, refused to withdraw the Ordinance. When it became known that the lower storey of the building was on fire, the riot act was read, following which the police opened fire on the crowd. Sixteen people were killed and forty-two injured, and the Red House was completely gutted. After the fire, only the shell of the original Red House remained.\\r\\nThe work of rebuilding it began the following year, and the Red House, as it is known today, was erected on the same site. It was opened to the public on 4 February 1907, by Governor Sir Henry Moore Jackson.\\r\\nThe building was designed and built by D. M. Hahn, Chief Draughtsman of the Public Works, at an estimated cost of S7,485. This sum included the gesso work (a mixture of plaster of Paris and glue) in the Legislative Council Chamber and the Justice Hall, which was estimated at S7,200.\\r\\nThe work was completed in 1906. The ceiling is the most striking feature in the Chamber. It is Wedgwood blue with white gesso work and was the work of Messrs. Jackson & Sons, an English firm.\\r\\nThe decorations were made in England in panels, and shipped to Trinidad in crates. An Italian craftsman was sent to install the ceiling.\\r\\nThe entablature and dais at the eastern end were also designed by D. M. Hahn. The columns and entablature are made of purple heart wood, while the panelling is fustic (yellow tinted wood commonly found in South America). The passageway between the two buildings which replaced the double archway, is no longer open to vehicular traffic. The fountain in the centre of the rotunda was designed by D. M. Hahn as a means of cooling and ventilation for the offices, in the days before air-conditioning.\\r\\nThe offices of the early Red House, with the exception of the Governor's office and that of the Colonial Secretary, comprised offices for the Attorney-General, Registrar-General Lands & Surveys Department, Judges' Chambers, the Courts of Justice and the Parliament and Law Libraries, as well as the Legislative Council Chamber, which is now the Parliament Chamber. At present, the building is being restored for the exclusive use of the Parliament.\\r\\nThe Red House today is the second Government building to be known by this name since the newly constructed government offices were built on the same site and given the same name. The name Brunswick Square was changed to Woodford Square during World War I in 1914-1918. The rubble which was removed after the fire was used as landfill for Victoria and Harris Squares.","input":"What capital is home of the red house?"},{"output":"October 2004","context":"The Signature at MGM Grand is a condo-hotel at the MGM Grand Las Vegas resort, in Paradise.\\r\\nIt was built by a partnership between MGM Mirage and Turnberry Associates on the location of the former MGM Grand Adventures Theme Park, though it officially holds an off-strip address of 145 East Harmon Avenue. It features three identical 38-story towers, each consisting of 576 fully furnished units.\\r\\nMost of the condo units were to be privately owned. If a unit owner chooses, he or she may join the rental program, placing their units into the hotel's room inventory to be offered as luxury suites by The Signature when they are not occupied by the owner. The owner, minus fees for hotel maintenance, marketing, and upkeep, receives a split of the rental rate of the owner's unit.\\r\\nDeveloper Turnberry/MGM Grand Towers filed for chapter 11 bankruptcy following the 2008 Great Recession. Owners of 545 condo units in turn sued the developer for fraud, alleging units were rented out at less than luxury rates.[2]\\r\\n\\r\\n\\r\\nOn December 20, 2003, MGM Resorts International and Turnberry Associates announced the formation of a partnership to build the luxury condo hotel, stating that they would build up to six towers each rising up to 40 stories. The first phase of this project involved the sales of condo units to individuals and investors. During this phase, the project was referred to as The Residences, The Residences at MGM Grand and/or The Residences: A Condo Hotel by Turnberry. The first tower sold out within 90 days, proving that the concept was well received. The second tower was already 50% sold by the time construction began in late 2004.\\r\\nIn October 2004, The Residences celebrated its ground-breaking, making it the first high-rise condo-hotel project overlooking the strip to begin construction. Other ventures of this type have begun since in Las Vegas, with many of them failing to obtain necessary financing and/or support. On May 12, 2006, the first tower opened for occupancy. The successful opening of The Signature at MGM Grand appears to be aided by its connection to the resources and amenities of The MGM Grand Hotel & Casino.\\r\\nEach tower has a private pool with a hot tub. Tower 1 has a cocktail lounge. Towers 1 and 3 have a workout room for the guests of all three towers. Starbucks is available at Tower 1. Signature guests have full access to all MGM Grand amenities as well.\\r\\nThe Signature is connected to the main MGM Grand Casino via several moving walkways that traverse through two of the 3 towers (Tower 1 & 2). Tower 3 is accessible via a hallway between tower 2 and 3. Time to the main casino to the furthest tower (Tower 3) is about 8 minutes for the average walker.\\r\\nThere is no self parking on the Signature property, all parking is by valet. Each tower has valet parking on the ground level.\\r\\nHarmon Ave East\\r\\nThe Signature at MGM Grand, Las Vegas\\r\\nFile:The Signature at MGM Grand Tower","input":"When was the signature at mgm grand built?"},{"output":"Pregnenolone","context":"The corpus luteum (Latin for \\"yellow body\\"; plural corpora lutea) is a temporary endocrine structure in female ovaries and is involved in the production of relatively high levels of progesterone, moderate levels of estradiol, inhibin A and small amounts of estrogen.[1][2] It is the remains of the ovarian follicle that has released a mature ovum during a previous ovulation.[3]\\r\\n\\r\\nThe corpus luteum is colored as a result of concentrating carotenoids (including lutein) from the diet and secretes a moderate amount of estrogen to inhibit further release of gonadotropin-releasing hormone (GnRH) and thus secretion of luteinizing hormone (LH) and follicle-stimulating hormone (FSH). A new corpus luteum develops with each menstrual cycle.[citation needed]\\r\\n\\r\\nThe corpus luteum develops from an ovarian follicle during the luteal phase of the menstrual cycle or oestrous cycle, following the release of a secondary oocyte from the follicle during ovulation. The follicle first forms a corpus hemorrhagicum before it becomes a corpus luteum, but the term refers to the visible collection of blood, left after rupture of the follicle, that secretes progesterone. While the oocyte (later the zygote if fertilization occurs) traverses the Fallopian tube into the uterus, the corpus luteum remains in the ovary.\\r\\n\\r\\nThe corpus luteum is typically very large relative to the size of the ovary; in humans, the size of the structure ranges from under 2?cm to 5?cm in diameter.[4]\\r\\n\\r\\nIts cells develop from the follicular cells surrounding the ovarian follicle.[5] The follicular theca cells luteinize into small luteal cells (thecal-lutein cells) and follicular granulosa cells luteinize into large luteal cells (granulosal-lutein cells) forming the corpus luteum. Progesterone is synthesized from cholesterol by both the large and small luteal cells upon luteal maturation.  Cholesterol-LDL complexes bind to receptors on the plasma membrane of luteal cells and are internalized. Cholesterol is released and stored within the cell as cholesterol ester. LDL is recycled for further cholesterol transport.  Large luteal cells produce more progesterone due to uninhibited/basal levels of protein kinase A (PKA) activity within the cell. Small luteal cells have LH receptors that regulate PKA activity within the cell. PKA actively phosphorylates steroidogenic acute regulatory protein (StAR) and translocator protein to transport cholesterol from the outer mitochondrial membrane to the inner mitochondrial membrane.[6]\\r\\n\\r\\nThe development of the corpus luteum is accompanied by an increase in the level of the steroidogenic enzyme P450scc that converts cholesterol to pregnenolone in the mitochondria.[7] Pregnenolone is then converted to progesterone that is secreted out of the cell and into the blood stream. During the bovine estrous cycle, plasma levels of progesterone increase in parallel to the levels of P450scc and its electron donor adrenodoxin, indicating that progesterone secretion is a result of enhanced expression of P450scc in the corpus luteum.[7]\\r\\n\\r\\nThe mitochondrial P450 system electron transport chain including adrenodoxin reductase and adrenodoxin has been shown to leak electrons leading to the formation of superoxide radical.[8][9] Apparently to cope with the radicals produced by this system and by enhanced  mitochondrial metabolism, the levels of antioxidant enzymes catalase and superoxide dismutase also increase in parallel with the enhanced steroidogenesis in the corpus luteum.[7]\\r\\n\\r\\nLike the previous theca cells, the theca lutein cells lack the aromatase enzyme that is necessary to produce estrogen, so they can only perform steroidogenesis until formation of androgens. The granulosa lutein cells do have aromatase, and use it to produce estrogens, using the androgens previously synthesized by the theca lutein cells, as the granulosa lutein cells in themselves do not have the 17ϫ-hydroxylase or 17,20 lyase to produce androgens.[5]\\r\\nOnce the corpus luteum regresses the remnant is known as corpus albicans.[12]\\r\\n\\r\\nThe corpus luteum is essential for establishing and maintaining pregnancy in females. The corpus luteum secretes progesterone, which is a steroid hormone responsible for the decidualization of the endometrium (its development) and maintenance, respectively. It also produces relaxin, a hormone responsible for softening of the pubic symphysis which helps in parturition.\\r\\n\\r\\nIf the egg is not fertilized, the corpus luteum stops secreting progesterone and decays (after approximately 10 days in humans). It then degenerates into a corpus albicans, which is a mass of fibrous scar tissue.\\r\\n\\r\\nThe uterine lining (endometrium) is expelled through the vagina (in mammals that go through a menstrual cycle). In an estrous cycle, the lining degenerates back to normal size.\\r\\n\\r\\nIf the egg is fertilized and implantation occurs, the syncytiotrophoblast (derived from trophoblast) cells of the blastocyst secrete the hormone human chorionic gonadotropin (hCG, or a similar hormone in other species) by day 9 post-fertilization.\\r\\n\\r\\nHuman chorionic gonadotropin signals the corpus luteum to continue progesterone secretion, thereby maintaining the thick lining (endometrium) of the uterus and providing an area rich in blood vessels in which the zygote(s) can develop. From this point on, the corpus luteum is called the corpus luteum graviditatis.\\r\\n\\r\\nThe introduction of prostaglandins at this point causes the degeneration of the corpus luteum and the abortion of the fetus. However, in placental animals such as humans, the placenta eventually takes over progesterone production and the corpus luteum degrades into a corpus albicans without embryo/fetus loss.\\r\\n\\r\\nLuteal support refers to the administration of medication (generally progestins) for the purpose of increasing the success of implantation and early embryogenesis, thereby complementing the function of the corpus luteum.\\r\\n\\r\\nThe yellow color and name of the corpus luteum, like that of the macula lutea of the retina, is due to its concentration of certain carotenoids, especially lutein. In 1968, a report indicated that beta-carotene was synthesized in laboratory conditions in slices of corpus luteum from cows. However, attempts have been made to replicate these findings, but have not succeeded. The idea is not presently accepted by the scientific community.[13] Rather, the corpus luteum concentrates carotenoids from the diet of the mammal.\\r\\n\\r\\nSimilar structures and functions of the corpus luteum exist in some reptiles.[14] Dairy cattle also follow a similar cycle.[15]\\r\\n\\r\\nOrder of changes in ovary\\r\\n\\r\\nHuman ovary with fully developed corpus luteum\\r\\n\\r\\nLuteinized follicular cyst. H&E stain.","input":"What stimulates the corpus luteum to produce progesterone?"},{"output":"between West Africa and Central Africa, at the border of eastern Nigeria and Cameroon","context":"Bantu peoples is used as a general label for the 300ÿ600 ethnic groups in Africa who speak Bantu languages.[1] They inhabit a geographical area stretching east and southward from Central Africa across the African Great Lakes region down to Southern Africa.[1] Bantu is a major branch of the NigerÿCongo language family spoken by most populations in Africa. There are about 650 Bantu languages by the criterion of mutual intelligibility,[2] though the distinction between language and dialect is often unclear, and Ethnologue counts 535 languages.[3]\\r\\nAround 3,000 years ago, speakers of the Proto-Bantu language group began a millennia-long series of migrations eastward from their homeland between West Africa and Central Africa, at the border of eastern Nigeria and Cameroon.[4] This Bantu expansion first introduced Bantu peoples to central, southern and southeastern Africa, regions they had previously been absent from. The proto-Bantu migrants in the process assimilated and/or displaced a number of earlier inhabitants that they came across, such as Pygmy and Khoisan populations in the centre and south, respectively. They also encountered some Afro-Asiatic outlier groups in the southeast who had been there for centuries, having migrated from Northeast Africa.[5][6]\\r\\nIndividual Bantu groups today often comprise millions of people. Among these are the Shona of Zimbabwe with 14.2 million people; the Luba of the Democratic Republic of the Congo, with over 13.5 million people; the Zulu of South Africa, with over 10 million people; the Tiv of central Nigeria and Cameroon, with almost 10 million people; the Sukuma of Tanzania, with around eight million people; and the Kikuyu of Kenya, with over six million people. Although only around five million individuals speak the Arabic-influenced Swahili language as their mother tongue,[7] it is used as a lingua franca by over 140 million people throughout Southeast Africa.[8] Swahili also serves as one of the official languages of the African Union.\\r\\n\\r\\n\\r\\nThe word Bantu, and its variations, means \\"people\\" or \\"humans\\". The root in Proto-Bantu is reconstructed as *-ntu. Versions of the word Bantu (that is, the root plus the class 2 noun class prefix *ba-) occur in all Bantu languages: for example, as watu in Swahili; bantu in Kikongo; anthu in Chichewa;batu in Lingala; bato in Kiluba; bato in Duala; abanto in Gusii; and? in Kamba and Kikuyu; abantu in Kirundi, Zulu, Xhosa, Runyakitara,[9] and Ganda; wandru in Shingazidja; abantru in Mpondo and Ndebele; b?tfu in Phuthi; bantfu in Swati; banu in Lala; vanhu in Shona and Tsonga; batho in Sesotho, Tswana and Northern Sotho; antu in Meru; andu in Embu; vandu in some Luhya dialects; vhathu in Venda; bhandu in nyakyusa and mbaityo in Tiv.\\r\\nCurrent scholarly understanding places the ancestral proto-Bantu homeland in West Africa near the present-day southwestern border of Nigeria and Cameroon c.?4,000 years ago (2000?B.C.), and regards the Bantu languages as a branch of the NigerÿCongo language family.[13] This view represents a resolution of debates in the 1960s over competing theories advanced by Joseph Greenberg and Malcolm Guthrie, in favor of refinements of Greenberg's theory. Based on wide comparisons including non-Bantu languages, Greenberg argued that Proto-Bantu, the hypothetical ancestor of the Bantu languages, had strong ancestral affinities with a group of languages spoken in Southeastern Nigeria. He proposed that Bantu languages had spread east and south from there, to secondary centers of further dispersion, over hundreds of years.\\r\\nUsing a different comparative method focused more exclusively on relationships among Bantu languages, Guthrie argued for a single Central African dispersal point spreading at a roughly equal rate in all directions. Subsequent research on loanwords for adaptations in agriculture and animal husbandry and on the wider NigerÿCongo language family rendered that thesis untenable. In the 1990s, Jan Vansina proposed a modification of Greenberg's ideas, in which dispersions from secondary and tertiary centers resembled Guthrie's central node idea, but from a number of regional centers rather than just one, creating linguistic clusters.[14]\\r\\nIt is unclear exactly when the spread of Bantu-speakers began from their core area as hypothesized c. 5,000 years ago (3000 B.C.). By 3,500 years ago (1500?B.C.) in the west, Bantu-speaking communities had reached the great Central African rain forest, and by 2,500 years ago (500?B.C.) pioneering groups had emerged into the savannahs to the south, in what are now the Democratic Republic of the Congo, Angola, and Zambia. Another stream of migration, moving east, by 3,000 years ago (1000?B.C.) was creating a major new population center near the Great Lakes of East Africa, where a rich environment supported a dense population. Movements by small groups to the southeast from the Great Lakes region were more rapid, with initial settlements widely dispersed near the coast and near rivers, due to comparatively harsh farming conditions in areas farther from water. Pioneering groups had reached modern KwaZulu-Natal in South Africa by A.D.?300 along the coast, and the modern Northern Province (encompassed within the former province of the Transvaal) by A.D.?500.[15]\\r\\nBefore the expansion of farming and herding peoples, including those speaking Bantu languages, Africa south of the equator was populated by neolithic hunting and foraging peoples. Some of them were ancestral to proto-Khoisan-speaking peoples, whose modern hunter-forager and linguistic descendants, the Khoekhoe and San, occupy the arid regions around the Kalahari desert. The Hadza and Sandawe populations in Tanzania comprise the other modern hunter-forager remnant in Africa of these proto-Khoisan-speaking peoples.\\r\\nOver a period of many centuries, most hunting-foraging peoples were displaced and absorbed by incoming Bantu-speaking communities, as well as by Ubangian, Nilotic, and Sudanic language-speakers in North Central and Eastern Africa. The Bantu expansion was a long series of physical migrations, a diffusion of language and knowledge out into and in from neighboring populations, and a creation of new societal groups involving inter-marriage among communities and small groups moving to communities and small groups moving to new areas.\\r\\nAfter their movements from their original homeland in West Africa, Bantus also encountered in East Africa peoples of Afro-Asiatic (mainly Cushitic) and Nilo-Saharan (mainly Nilotic and Sudanic) ancestral origin. As cattle terminology in use amongst the few modern Bantu pastoralist groups suggests, the Bantu migrants would acquire cattle from their new Cushitic neighbors. Linguistic evidence also indicates that Bantus likely borrowed the custom of milking cattle directly from Cushitic peoples in the area.[16] Later interactions between Bantu and Cushitic peoples resulted in Bantu groups with significant Cushitic ethnic admixture, such as the Tutsi of the African Great Lakes region; and culturo-linguistic influences, such as the Herero herdsmen of southern Africa.[17][18]\\r\\nOn the coastal section of East Africa, another mixed Bantu community developed through contact with Muslim Arab and Persian traders. The Swahili culture that emerged from these exchanges evinces many Arab and Islamic influences not seen in traditional Bantu culture, as do the many Afro-Arab members of the Bantu Swahili people. With its original speech community centered on the coastal parts of Zanzibar, Kenya, and Tanzania ÿ a seaboard referred to as the Swahili Coast ÿ the Bantu Swahili language contains many Arabic loan-words as a result of these interactions.[19]\\r\\nBetween the 14th and 15th centuries, Bantu-speaking states began to emerge in the Great Lakes region in the savannah south of the Central African rain forest. On the Zambezi river, the Monomatapa kings built the famous Great Zimbabwe complex, a civilisation of what are today referred to as the Shona people. From the 16th century onward, the processes of state formation amongst Bantu peoples increased in frequency. This was probably due to denser population (which led to more specialized divisions of labor, including military power, while making emigration more difficult); to increased interaction amongst Bantu-speaking communities with Chinese, European, Indonesian, and Arab traders on the coasts; to technological developments in economic activity; and to new techniques in the political-spiritual ritualization of royalty as the source of national strength and health.[20]\\r\\nKongo youth and adults in Kinshasa, Democratic Republic of the Congo\\r\\nA Kikuyu woman in Kenya\\r\\nA Makua mother and child in Mozambique\\r\\nBubi girls in Equatorial Guinea\\r\\nBetween the 14th and 15th centuries, Bantu states began to emerge in the Great Lakes region in the savanna south of the Central African rain-forest. In Southern Africa on the Zambezi river, the Monomatapa kings built the famous Great Zimbabwe complex, the largest of over 200 such sites in Southern Africa, such as Bumbusi in Zimbabwe and Manyikeni in Mozambique. From the 16th century onward, the processes of state formation among Bantu peoples increased in frequency. Some examples of such Bantu states include: in Central Africa, the Kingdom of Kongo,[21] Lunda Empire,[22] and Luba Empire[23] of Angola, the Republic of Congo, and the Democratic Republic of Congo; in the Great Lakes Region, the Buganda[24] and Karagwe[24] Kingdoms of Uganda and Tanzania; and in Southern Africa, the Mutapa Empire,[25] Rozwi Empire,[26] and the Danamombe, Khami, and Naletale Kingdoms of Zimbabwe and Mozambique.[25]\\r\\nToward the 18th and 19th centuries, the flow of Zanj (Bantu) slaves from Southeast Africa increased with the rise of the Omani Sultanate of Zanzibar, based in Zanzibar, Tanzania. With the arrival of European colonialists, the Zanzibar Sultanate came into direct trade conflict and competition with Portuguese and other Europeans along the Swahili Coast, leading eventually to the fall of the Sultanate and the end of slave trading on the Swahili Coast in the mid-19th century.\\r\\nIn the 1920s, relatively liberal South Africans, missionaries, and the small black intelligentsia began to use the term \\"Bantu\\" in preference to \\"Native\\" and more derogatory terms (such as \\"Kaffir\\") to refer collectively to Bantu-speaking South Africans. After World War II, the National Party governments adopted that usage officially, while the growing African nationalist movement and its liberal allies turned to the term \\"African\\" instead, so that \\"Bantu\\" became identified with the policies of apartheid. By the 1970s this so discredited \\"Bantu\\" as an ethno-racial designation that the apartheid government switched to the term \\"Black\\" in its official racial categorizations, restricting it to Bantu-speaking Africans, at about the same time that the Black Consciousness Movement led by Steve Biko and others were defining \\"Black\\" to mean all racially oppressed South Africans (Africans, Coloureds, and Indians).\\r\\nExamples of South African usages of \\"Bantu\\" include:","input":"What was the original homeland of bantu-speaking farmers?"},{"output":"10 February 1763","context":"The Treaty of Paris, also known as the Treaty of 1763, was signed on 10 February 1763 by the kingdoms of Great Britain, France and Spain, with Portugal in agreement, after Great Britain's victory over France and Spain during the Seven Years' War.\\r\\nThe signing of the treaty formally ended the Seven Years' War, known as the French and Indian War in the North American theatre,[1] and marked the beginning of an era of British dominance outside Europe.[2] Great Britain and France each returned much of the territory that they had captured during the war, but Great Britain gained much of France's possessions in North America. Additionally, Great Britain agreed to protect Roman Catholicism in the New World. The treaty did not involve Prussia and Austria as they signed a separate agreement, the Treaty of Hubertusburg, five days later.\\r\\n\\r\\n\\r\\nDuring the war, Great Britain had conquered the French colonies of Canada, Guadeloupe, Saint Lucia, Dominica, Grenada, Saint Vincent and the Grenadines, and Tobago, the French \\"factories\\" (trading posts) in India, the slave-trading station at Gore, the Sngal River and its settlements, and the Spanish colonies of Manila (in the Philippines) and Havana (in Cuba). France had captured Minorca and British trading posts in Sumatra, while Spain had captured the border fortress of Almeida in Portugal, and Colonia del Sacramento in South America.\\r\\nIn the treaty, most of these territories were restored to their original owners, but not all: Britain made considerable gains.[3] France and Spain restored all their conquests to Britain and Portugal. Britain restored Manila and Havana to Spain, and Guadeloupe, Martinique, Saint Lucia, Gore, and the Indian factories to France.[4] In return, France ceded Canada, Dominica, Grenada, Saint Vincent and the Grenadines, and Tobago to Britain.[5] France also ceded the eastern half of French Louisiana to Britain; that is, the area from the Mississippi River to the Appalachian Mountains.[6]\\r\\nSpain ceded Florida to Britain.[4] France had already secretly given Louisiana to Spain in the Treaty of Fontainebleau (1762). In addition, while France regained its factories in India, France recognized British clients as the rulers of key Indian native states, and pledged not to send troops to Bengal. Britain agreed to demolish its fortifications in British Honduras (now Belize), but retained a logwood-cutting colony there. Britain confirmed the right of its new subjects to practise Catholicism.[7]\\r\\nFrance ceded all of its territory in mainland North America, but retained fishing rights off Newfoundland and the two small islands of Saint Pierre and Miquelon, where its fishermen could dry their catch. In turn France gained the return of its sugar colony, Guadeloupe, which it considered more valuable than Canada.[8] Voltaire had notoriously dismissed Canada as \\"Quelques arpents de neige\\", \\"A few acres of snow\\".[9]\\r\\nThe Treaty of Paris is frequently noted[by whom?] as the point at which France gave Louisiana to Spain.[citation needed] The transfer, however, occurred with the Treaty of Fontainebleau (1762) but was not publicly announced until 1764. The Treaty of Paris was to give Britain the east side of the Mississippi (including Baton Rouge, Louisiana, which was to be part of the British territory of West Florida). New Orleans on the east side remained in French hands (albeit temporarily). The Mississippi River corridor in what is modern day Louisiana was to be reunited following the Louisiana Purchase in 1803 and the AdamsÿOns Treaty in 1819.\\r\\nThe 1763 treaty states in Article VII:\\r\\nVII. French territories on the continent of America; it is agreed, that, for the future, the confines between the dominions of his Britannick Majesty and those of his Most Christian Majesty, in that part of the world, shall be fixed irrevocably by a line drawn along the middle of the River Mississippi, from its source to the river Iberville, and from thence, by a line drawn along the middle of this river, and the lakes Maurepas and Pontchartrain to the sea; and for this purpose, the Most Christian King cedes in full right, and guaranties to his Britannick Majesty the river and port of the Mobile, and everything which he possesses, or ought to possess, on the left side of the river Mississippi, except the town of New Orleans and the island in which it is situated, which shall remain to France, provided that the navigation of the river Mississippi shall be equally free, as well to the subjects of Great Britain as to those of France, in its whole breadth and length, from its source to the sea, and expressly that part which is between the said island of New Orleans and the right bank of that river, as well as the passage both in and out of its mouth: It is farther stipulated, that the vessels belonging to the subjects of either nation shall not be stopped, visited, or subjected to the payment of any duty whatsoever. The stipulations inserted in the IVth article, in favour of the inhabitants of Canada shall also take place with regard to the inhabitants of the countries ceded by this article.\\r\\nWhile the war was fought all over the world, the British began the war over French possessions in North America.[10] After a long debate of the relative merits of Guadeloupe, which produced S6 million a year in sugar, versus Canada which was expensive to keep, Great Britain decided to keep Canada for strategic reasons and return Guadeloupe to France.[11] While the war had weakened France, it was still a European power. British Prime Minister Lord Bute wanted a peace that would not aggravate France towards a second war.[12] This explains why Great Britain agreed to return so much while being in such a strong position.\\r\\nThough the Protestant British feared Roman Catholics, Great Britain did not want to antagonize France through expulsion or forced conversion. Also, it did not want French settlers to leave Canada to strengthen other French settlements in North America.[13] This explains Great Britain's willingness to protect Roman Catholics living in Canada.\\r\\nUnlike Lord Bute, the French Foreign Minister the Duke of Choiseul expected a return to war. However, France needed peace to rebuild.[14] French diplomats believed that without France to keep the Americans in check, the colonists might attempt to revolt.[citation needed] In Canada, France wanted open emigration for those, such as nobility, who would not swear allegiance to the British Crown.[15] Lastly, France required protection for Roman Catholics in North America considering Britain's previous treatment of Roman Catholics under its jurisdiction.\\r\\nThe article states:\\r\\nIV. His Most Christian Majesty renounces all pretensions which he has heretofore formed or might have formed to Nova Scotia or Acadia in all its parts, and guaranties the whole of it, and with all its dependencies, to the King of Great Britain: Moreover, his Most Christian Majesty cedes and guaranties to his said Britannick Majesty, in full right, Canada, with all its dependencies, as well as the island of Cape Breton, and all the other islands and coasts in the gulph and river of St. Lawrence, and in general, every thing that depends on the said countries, lands, islands, and coasts, with the sovereignty, property, possession, and all rights acquired by treaty, or otherwise, which the Most Christian King and the Crown of France have had till now over the said countries, lands, islands, places, coasts, and their inhabitants, so that the Most Christian King cedes and makes over the whole to the said King, and to the Crown of Great Britain, and that in the most ample manner and form, without restriction, and without any liberty to depart from the said cession and guaranty under any pretence, or to disturb Great Britain in the possessions above mentioned. His Britannick Majesty, on his side, agrees to grant the liberty of the Catholick religion to the inhabitants of Canada: he will, in consequence, give the most precise and most effectual orders, that his new Roman Catholic subjects may profess the worship of their religion according to the rites of the Romish church, as far as the laws of Great Britain permit. His Britannick Majesty farther agrees, that the French inhabitants, or others who had been subjects of the Most Christian King in Canada, may retire with all safety and freedom wherever they shall think proper, and may sell their estates, provided it be to the subjects of his Britannick Majesty, and bring away their effects as well as their persons, without being restrained in their emigration, under any pretence whatsoever, except that of debts or of criminal prosecutions: The term limited for this emigration shall be fixed to the space of eighteen months, to be computed from the day of the exchange of the ratification of the present treaty.\\r\\nDuring the negotiations that led to the treaty, a major issue of dispute between Britain and France had been over the status of the fortifications of the French coastal settlement of Dunkirk. The British had long feared that it would be used as a staging post to launch a French invasion of Britain. Under the Treaty of Utrecht in 1713 they had forced France to concede extreme limits on the fortifications there. The 1748 Treaty of Aix-la-Chapelle had allowed more generous terms,[16] and France had constructed greater defences for the town.\\r\\nBy the Treaty Britain forced France to accept the earlier 1713 conditions and demolish the fortifications they had constructed since then.[17] This would be a continuing source of resentment to France, who would eventually have this clause overturned in the 1783 Treaty of Paris which brought an end to the American Revolutionary War.\\r\\nWhen Lord Bute became Prime Minister in 1762, he pushed for a resolution to the war with France and Spain, fearing that Great Britain could not govern all of its newly acquired territories. In what Winston Churchill would later term a policy of \\"appeasement,\\" Bute returned some colonies to Spain and France in the negotiations.[18] Despite a desire for peace, many in the British parliament opposed the return of any gains made during the war. Notable among the opposition was former Prime Minister William Pitt, the Elder, who warned that the terms of the treaty would only lead to further conflicts once France and Spain had time to rebuild. \\"The peace was insecure,\\" he would later say, \\"because it restored the enemy to her former greatness. The peace was inadequate, because the places gained were no equivalent for the places surrendered.\\"[19] The treaty passed 319 votes to 65 opposed.[20]\\r\\nThe Treaty of Paris took no consideration of Great Britain's battered continental ally, Frederick II of Prussia. Frederick would have to negotiate peace terms separately in the Treaty of Hubertusburg. For decades following the Seven Years' War, Frederick II would consider the Treaty of Paris as a British betrayal.\\r\\nThe American colonists were disappointed by the protection of Roman Catholicism in the Treaty of Paris because of their own strong Protestant faith.[21] Some have pointed to this as one reason for the breakdown of AmericanÿBritish relations.[21]\\r\\nThe article provided for unrestrained emigration for 18 months from Canada. However, passage on British ships was expensive.[15] A total of 1,600 people left New France through the Treaty clause, but only 270 French Canadians.[15] Some have claimed that this was part of British policy to limit emigration.[15]\\r\\nArticle IV of the treaty allowed Roman Catholicism to be practised in Canada.[22] George III agreed to allow Catholicism within the laws of Great Britain. In this period, British laws included various Test Acts to prevent governmental, judicial, and bureaucratic appointments from going to Roman Catholics. Roman Catholics were believed to be agents of the Jacobite Pretenders to the throne, who normally resided in France supported by the French regime.[23] This was relaxed in Quebec to some degree, but top positions like governorships were still held by Anglicans.[22]\\r\\nArticle IV has also been cited as the basis for Quebec often having its unique set of laws that are different from the rest of Canada. There was a general constitutional principle in the United Kingdom to allow colonies taken through conquest to continue their own laws.[24] This was limited by royal prerogative, and the monarch could still choose to change the accepted laws in a conquered colony.[24] However, the treaty eliminated this power because by a different constitutional principle, terms of a treaty were considered paramount.[24] In practice, Roman Catholics could become jurors in inferior courts in Quebec and argue based on principles of French law.[25] However, the judge was British and his opinion on French law could be limited or hostile.[25] If the case was appealed to a superior court, neither French law nor Roman Catholic jurors were allowed.[26]\\r\\nMany French residents of what are now Canada's Maritime provinces, called Acadians, were deported during the Great Expulsion (1755ÿ63). After the signing of the peace treaty guaranteed some rights to Roman Catholics, some Acadians returned to Canada. However, they were no longer welcome in English Nova Scotia.[27] They were forced into New Brunswick, which is a bilingual province today as a result of that relocation.[28]\\r\\nMuch land previously owned by France was now owned by Britain, and the French people of Quebec felt great betrayal at the French concession. Commander-in-Chief of the British Jeffrey Amherst noted that, \\"Many of the Canadians consider their Colony to be of utmost consequence to France & cannot be convinced  that their Country has been conceded to Great Britain\\".[29]","input":"When was the treaty of paris 1763 signed?"},{"output":"Richmond, Virginia","context":"Richmond, Virginia, served as the capital of the Confederate States of America for almost the whole of the American Civil War. Not only was Richmond the seat of political power for the Confederacy, it served as a vital source of munitions, armament, weapons, supplies, and manpower for the Confederate States Army and Confederate States Navy, and as such would have been defended at all costs regardless of its political status. The city was less than 100 miles (160?km) from the Union capital in Washington, D.C..\\r\\n\\r\\nDue to its symbolic and strategic importance to the Confederate war effort, it was the target of numerous attempts by the Union Army to seize possession of the capital, most notably during the Peninsula Campaign of 1862 and the Overland Campaign of 1864. Its proximity to the fighting would lead to it becoming a center of hospitals and military prisons. The city finally fell to Union forces on April 3, 1865, with large portions of the city destroyed by fires set during the evacuation.\\r\\n\\r\\nIn the aftermath of the war, numerous monuments, memorials, and museums were erected in the city.\\r\\n\\r\\nIn the 1860 United States Census, Richmond was the 25th largest urban area in the United States, with a population of 37,910.[1][2] The city had been the capital of Virginia since 1780.\\r\\n\\r\\nThe Confederate States of America was formed in early 1861 from the first states to secede from the Union. Montgomery, Alabama, was selected as the Confederate capital.\\r\\n\\r\\nAfter the Confederate Army fired on Fort Sumter in Charleston, South Carolina, on April 12, 1861, beginning the Civil War, additional states seceded.  Virginia voted to secede from the Union on April 17, 1861, and existed briefly thereafter as an independent republic before joining the Confederacy on June 19, 1861. However, on May 8, 1861, in the Confederate Capital City of Montgomery, Alabama, the decision was made to name the City of Richmond, Virginia as the new Capital of the Confederacy.  Shortly thereafter, in recognition of Virginia's strategic importance, the Confederate capital was moved to Richmond.\\r\\n\\r\\nThe Great Seal of the Confederate States of America, adopted April 30, 1863, features a depiction of George Washington based on the Virginia Washington Monument adjacent to the Confederate Capitol building.\\r\\n\\r\\nRichmond remained the capital of the Confederacy until April 2, 1865, at which point the government evacuated and was re-established, albeit briefly, in Danville, Virginia.[3]\\r\\n\\r\\nPositioned on the Fall Line along the James River, the city had ready access to an ample supply of hydropower to run mills and factories.\\r\\n\\r\\nThe Tredegar Iron Works, sprawling along the James River, supplied high-quality munitions to Confederacy during the war. The company also manufactured railroad steam locomotives in the same period. Tredegar is also credited with the production of approximately 10,000 artillery pieces during the war which was about half of the South's total domestic production of artillery between the war years of 1861ÿ1865. The foundry made the 723 tons of armor plating that covered the CSS Virginia (the former USS Merrimack), which fought the first battle between ironclad warships in March 1862. The Tredegar works were adjacent to the Richmond Arsenal, which was recommissioned in the lead-up to the war. On Brown's Island, the Confederate States Laboratory was established to consolidate explosives production to an isolated setting in the eventuality of an accidental explosion.\\r\\n\\r\\nNumerous smaller factories in Richmond produced tents, uniforms, harnesses and leather goods, swords and bayonets, and other war material. As the war progressed, the city's warehouses became the supply and logistical center for much of the Confederate forces within the Eastern Theater.\\r\\n\\r\\nRichmond was also a transportation hub. It was the terminus of five railroads: the Richmond, Fredericksburg, and Potomac Railroad, the Virginia Central Railroad, the Richmond and York River Railroad, the Richmond and Petersburg Railroad, and the Richmond and Danville Railroad, as well as the James River and Kanawha Canal, together with a seaport with access to the Chesapeake Bay and the Atlantic Ocean. At the fall of Richmond in April 1865, all but the Richmond and Danville Railroad and the canal had effectively been cut off by Union forces.\\r\\n\\r\\nIn the late spring of 1862, a large Federal army under Major General George B. McClellan landed on the Virginia Peninsula. McClellan, who had enjoyed early publicity from a series of successes in western Virginia, was assigned the task of seizing and occupying Richmond. His military maneuvers and the resulting battles and engagements became collectively known as the Peninsula Campaign, culminating in the Seven Days Battles.\\r\\n\\r\\nMcClellan's starting base was the Union-held Fort Monroe at the eastern tip of the Peninsula. Efforts to take Richmond by the James River were successfully blocked by Confederate defenses at the Battle of Drewry's Bluff on May 15, about eight miles downstream from Richmond. The Union Army advance was halted shortly outside of the city at the Battle of Seven Pines on May 31 and June 1, 1862 (near the site of what is now Richmond International Airport).\\r\\n\\r\\nOver a period of seven days from June 25 to July 1, 1862, Richmond's defensive line of batteries and fortifications set up under General Robert E. Lee, a daring ride around the Union Army by Confederate cavalry under General J.E.B. Stuart, and an unexpected appearance of General Stonewall Jackson's famous \\"foot cavalry\\" combined to unnerve the ever-cautious McClellan, and he initiated a Union retreat before Richmond.\\r\\n\\r\\nEven as other portions of the South were already falling, the failure of the Peninsula Campaign to take Richmond led to almost three more years of warfare between the states.\\r\\n\\r\\nAs a result of its proximity to the battlefields of the Eastern Theater and its high level of defense, the city processed many casualties of both sides: as home to numerous hospitals (the largest such being Chimborazo Hospital), prisons (notably Libby Prison, Castle Thunder, and Belle Isle), and various cemeteries.\\r\\n\\r\\nOn March 13, 1863, the Confederate Laboratory on Brown's Island was rocked by an explosion that killed dozens of workers.\\r\\n\\r\\nOn April 2, 1863, the city was beset by a large bread riot as housewives could no longer afford very high food prices and broke into stores. The riot was organized by Mary Jackson, a huckster and the mother of a soldier.[4] The militia was called out to end the riot.[5]\\r\\n\\r\\nThe Confederacy hit its high-water mark at the Battle of Gettysburg in July 1863. Subsequent campaigning in the balance of the year failed to bring about a decisive battle, and Richmond residents settled down to the winter of 1863ÿ64 mostly still optimistic about the Confederacy's fortunes.\\r\\n\\r\\nOne of the Civil War's most daring prison breaks, the Libby Prison Escape, took place in February 1864 when more than 100 Federal captives escaped and fled into the night. Fewer than half were recaptured, with the majority reaching Union lines and safety. The city was shaken shortly thereafter by the March 2, 1864 Dahlgren Affair, a failed Union raid on the city.\\r\\n\\r\\nUlysses S. Grant's 1864 Overland Campaign resulted in Robert E. Lee's Confederate army retiring to the vicinity of Richmond and Petersburg, where they checked Grant's progress.\\r\\n\\r\\nAfter a long siege, Grant captured Petersburg and Richmond in early April 1865. As the fall of Petersburg became imminent, on Evacuation Sunday (April 2), President Davis, his Cabinet, and the Confederate defenders abandoned Richmond and fled south on the last open railroad line, the Richmond and Danville.\\r\\n\\r\\nThe retreating soldiers were under orders to set fire to bridges, the armory, and warehouses with supplies as they left. The fire in the largely abandoned city spread out of control, and large parts of Richmond were destroyed, reaching to the very edge of Capitol Square mostly unchecked. The conflagration was not completely extinguished until the mayor and other civilians went to the Union lines east of Richmond on New Market Road (now State Route 5) and surrendered the city the next day. Union troops put out the raging fires in the city. The event became known as the Evacuation Fire. The occupation was overseen by General Godfrey Weitzel and later General Edward Ord.\\r\\n\\r\\nPresident Lincoln, who had been visiting General Grant and staying nearby at City Point, toured the fallen city (April 4ÿ7) by foot and carriage with his young son Tad, and visited the former White House of the Confederacy and the Virginia State Capitol.\\r\\n\\r\\nAbout one week after the evacuation of Richmond, General Robert E. Lee surrendered to Grant on April 9 ending the Battle of Appomattox Courthouse. Within the same week, on the evening of April 14, President Lincoln was assassinated in Washington D.C. by the Confederate sympathizer John Wilkes Booth.\\r\\n\\r\\nRichmond's Hollywood Cemetery is the final burial place of many Civil War notables, including Jefferson Davis, Stuart, former U.S. President and Confederate Congressman John Tyler, Virginia Governors and Confederate Generals Henry A. Wise and William \\"Extra Billy\\" Smith, Tredegar Iron Works owner and Confederate Brigadier General Joseph Reid Anderson, and Major Generals George Pickett, Fitzhugh Lee, Henry Heth, and John Imboden.  A large, stone pyramid dominates the Confederate Soldiers' section, where over 18,000 (many of whom are unknown) Confederates are buried.\\r\\n\\r\\nWar dead were also buried at Oakwood Cemetery, Shockoe Hill Cemetery, and the Cemetery for Hebrew Confederate Soldiers section of the Hebrew Cemetery. Numerous Union dead who were buried at these sites were re-interred after the war to several national cemeteries outside of the city.\\r\\n\\r\\nThe city has a number of markers and monuments commemorating the Civil War and the city's role in the Confederacy. Monument Avenue was laid out in 1887, with a series of monuments at various intersections honoring the city's Confederate heroes. Included (east to west) were J.E.B. Stuart, Robert E. Lee, Jefferson Davis, Stonewall Jackson, and Matthew F. Maury.\\r\\n\\r\\nThe Richmond National Battlefield Park, a unit of the National Park Service, maintains several battlefields from the Peninsula Campaign and subsequent actions. A driving tour through Civil War sites in Richmond and its surrounding counties is maintained by Virginia Civil War Trails. The White House of the Confederacy (part of the private, non-profit Museum of the Confederacy) has been fully restored to its wartime appearance and is open for daily tours.  Immediately next door to the White House, the internationally renowned Museum of the Confederacy houses the largest comprehensive collection of artifacts and personal effects relating to the Confederacy. Other museums include the Virginia Historical Society. A statue of Lincoln, commemorating his visit to the former Confederate capitol, was unveiled in 2003, causing controversy.[6]\\r\\n\\r\\nThe Charlie Daniels Band song, \\"Trudy\\", compares the taking of Richmond by Grant with the narrator saying that he was \\"raking in chips like Grant took Richmond\\" in a poker game.\\r\\n\\r\\nRobbie Robertson / The Band wrote 'The night they drove old Dixie down'.  One lyric refers to, '...by May the 10th (1865) Richmond had fell, it was a time I remember oh so well'. On May 10, 1865, Confederate President Jefferson Davis, fleeing Richmond and having dissolved the Confederate government, was captured by Union forces in Irwinville, Georgia. [7]","input":"What was the confederate capital in the civil war?"},{"output":"Muhammad Iqbal","context":"\\r\\n\\r\\n\\"Sare Jahan se Accha\\" (Urdu: ???? ???? ?? ??????, Hindi: ???? ???? ?? ?????; Sre Jahn? se Acch), formally known as \\"Tarnah-i-Hindi\\" (Urdu: ????? ??????, Hindi: ?????-?-??????; \\"Anthem of the People of India\\"), is an Urdu language patriotic song written for children by poet Muhammad Iqbal in the ghazal style of Urdu poetry.[a] The poem was published in the weekly journal Ittehad on 16 August 1904.[1] Publicly recited by Iqbal the following year at Government College, Lahore, British India (now in Pakistan) it quickly became an anthem of opposition to the British Raj. The song, an ode to Hindustanthe land comprising present-day Bangladesh, India and Pakistan, was later published in 1924 in the Urdu book Bang-i-Dara.[2]\\r\\n\\r\\nThe song has remained popular, especially in India. An abridged version is sung and played frequently as a patriotic song and as a marching song of the Indian Armed Forces.  A satirical version of the same from a 1958 Hindi movie also remains popular.[3]\\r\\n\\r\\nIts popular fast tempo tune was composed by Pandit Ravi Shankar in 1945 based on raga Mishra Pilu,[4] and it was set as a marching tune by Professor Antsher Lobo.[citation needed]\\r\\n\\r\\n???? ???? ?? ???? ???????? ?????\\r\\n?? ?????? ??? ?? ??? ?? ?????? ?????\\r\\n\\r\\n???? ??? ??? ??? ??? ???? ?? ?? ??? ???\\r\\n????? ???? ???? ??? ?? ?? ???? ?????\\r\\n\\r\\n???? ?? ?? ?? ?????? ?????? ????? ??\\r\\n?? ????? ?????? ?? ?????? ?????\\r\\n\\r\\n???? ??? ?????? ??? ?? ?? ?????? ?????\\r\\n???? ?? ?? ?? ?? ?? ???? ???? ?????\\r\\n\\r\\n?? ??? ???? ????! ?? ?? ??? ??? ??? ???\\r\\n???? ??? ????? ?? ?????? ?????\\r\\n\\r\\n???? ???? ?????? ??? ??? ??? ?????\\r\\n???? ??? ??? ??? ?? ???????? ?????\\r\\n\\r\\n????? ? ??? ? ???? ?? ?? ??? ???? ??\\r\\n?? ?? ??? ?? ???? ??? ? ???? ?????\\r\\n\\r\\n??? ??? ?? ?? ???? ???? ???? ?????\\r\\n????? ??? ?? ???? ???? ???? ?????\\r\\n\\r\\n?????! ???? ???? ???? ???? ???? ???\\r\\n????? ??? ??? ?? ???? ???? ?????!?\\r\\n\\r\\n???? ???? ?? ????? ??????????? ????? \\r\\n?? ???????? ??? ???? ?? ???????? ?????\\r\\n\\r\\n??????? ??? ??? ??? ??, ???? ?? ??? ??? ??? \\r\\n???? ???? ???? ?? ??? ?? ???? ?????\\r\\n\\r\\n???? ?? ???? ????, ??????? ????? ?? \\r\\n?? ????? ?????, ?? ?????? ?????\\r\\n\\r\\n???? ??? ????? ??? ???? ??????? ?????? \\r\\n?????? ?? ????? ?? ?? ????-?-???? ?????\\r\\n\\r\\n? ??-?-???-?-????! ?? ??? ??? ??? ?????? \\r\\n???? ???? ?????? ?? ?????? ?????\\r\\n\\r\\n?????? ???? ?????? ??? ??? ??? ???? \\r\\n????? ??? ??, ??? ?? ??????????? ?????\\r\\n\\r\\n?????-?-?????-?-???? ?? ??? ?? ???? ?? \\r\\n?? ?? ??? ?? ????? ???-?-????? ?????\\r\\n\\r\\n??? ??? ?? ?? ????? ????? ???? ????? \\r\\n?????? ??? ?? ?????? ???-?-????? ?????\\r\\n\\r\\n???????! ??? ???? ???? ???? ???? ??? \\r\\n????? ???? ???? ?? ????-?-????? ??????!\\r\\n\\r\\nSre jahn? se acch, Hindositn?[5] hamr\\r\\nHam bulbulen? hain? is kؐ, yih gulsitn?[5] hamr\\r\\n\\r\\nG?h?urbat men? hon? agar ham, raht hai dil wat?an men?\\r\\nSamjho wuhؐn? hamen? bhؐ dil ho jahn? hamr\\r\\n\\r\\nParbat wuh sab se n?ch, hamsyah smn? k\\r\\nWuh santarؐ hamr, wuh psbn? hamr\\r\\n\\r\\nGodؐ men? kheltؐ hain? is kؐ hazron? nadiyn?\\r\\nGuls?h?an hai jin ke dam se ras?h?k-i jann? hamr\\r\\n\\r\\nAi b-i rd-i Gang! wuh din hain? yd tujh ko?\\r\\nUtr tire[6] kinre jab krwn? hamr\\r\\n\\r\\nMaz?hab nahؐn? sikht pas men? bair rakhn\\r\\nHindؐ hain? ham, wat?an hai Hindositn? hamr\\r\\n\\r\\nYnn o-Mi?r o-Rm, sab mi? ga'e jahn? se\\r\\nAb tak magar hai bqؐ, nm o-nis?h?an? hamr\\r\\n\\r\\nKuch bt hai kih hastؐ, mi?tؐ nahؐn? hamrؐ\\r\\n?adiyon? rah hai dus?h?man daur-i zamn? hamr\\r\\n\\r\\nIqbl! ko'ؐ ma?ram apn nahؐn? jahn? men?\\r\\nMa?lm ky kisؐ ko dard-i nihn? hamr!\\r\\n\\r\\nBetter than the entire world, is our Hindustan,\\r\\nWe are its nightingales, and it (is) our garden abode\\r\\n\\r\\nIf we are in an alien place, the heart remains in the homeland,\\r\\nKnow us to be only there where our heart is.\\r\\n\\r\\nThat tallest mountain, that shade-sharer of the sky,\\r\\nIt (is) our sentry, it (is) our watchman\\r\\n\\r\\nIn its lap where frolic thousands of rivers,\\r\\nWhose vitality makes our garden the envy of Paradise.\\r\\n\\r\\nO the flowing waters of the Ganges, do you remember that day\\r\\nWhen our caravan first disembarked on your waterfront?\\r\\n\\r\\nReligion does not teach us to bear animosity among ourselves\\r\\nWe are of Hind, our homeland is Hindustan.\\r\\n\\r\\nIn a world in which ancient Greece, Egypt, and Rome have all vanished\\r\\nOur own attributes (name and sign) live on today.\\r\\n\\r\\nThere is something about our existence for it doesn't get wiped \\r\\nEven though, for centuries, the time-cycle of the world has been our enemy.\\r\\n\\r\\nIqbal! We have no confidant in this world\\r\\nWhat does any one know of our hidden pain?\\r\\n\\r\\nIqbal was a lecturer at the Government College, Lahore at that time, and was invited by a student Lala Har Dayal to preside over a function. Instead of delivering a speech, Iqbal sang \\"Saare Jahan Se Achcha\\".  The song, in addition to embodying yearning and attachment to the land of Hindustan, expressed \\"cultural memory\\" and had an elegiac quality.  In 1905, the 27-year-old Iqbal viewed the future society of the subcontinent as both a pluralistic and composite Hindu-Muslim culture.  Later that year he left for Europe for a three-year sojourn that was to transform him into an Islamic philosopher and a visionary of a future Islamic society.[2]\\r\\n\\r\\nIn 1910, Iqbal wrote another song for children, \\"Tarana-e-Milli\\" (Anthem of the Religious Community), which was composed in the same metre and rhyme scheme as \\"Saare Jahan Se Achcha\\", but which renounced much of the sentiment of the earlier song.[7] The sixth stanza of \\"Saare Jahan Se Achcha\\" (1904), which is often quoted as proof of Iqbal's secular outlook:\\r\\n\\r\\nMaz?hab nahؐn? sikht pas men? bair rakhn\\r\\nHindؐ hain? ham, wat?an hai Hindstn? hamr\\r\\n\\r\\nReligion does not teach us to bear ill-will among ourselves\\r\\nWe are of Hind, our homeland is Hindustan.\\r\\n\\r\\ncontrasted significantly with the first stanza of Tarana-e-Milli (1910) reads:[7]\\r\\n\\r\\nCؐn o-?Arab hamr, Hindstn? hamr\\r\\nMuslim hain? ham, wat?an hai sr jahn? hamr\\r\\n\\r\\nCentral Asia[8] and Arabia are ours, Hindustan is ours\\r\\nWe are Muslims, the whole world is our homeland.[7]\\r\\n\\r\\nIqbal's world view had now changed; it had become both global and Islamic.  Instead of singing of Hindustan, \\"our homeland,\\" the new song proclaimed that \\"our homeland is the whole world.\\"[9] Two decades later, in his presidential address to the Muslim League annual conference in Allahabad in 1930, he supported a separate nation-state in the Muslim majority areas of the sub-continent, an idea that inspired the creation of Pakistan.[10]","input":"Who composed the song sare jahan se achcha?"},{"output":"9,000 BCE","context":"The Fertile Crescent (also known as the \\"cradle of civilization\\") is a crescent-shaped region where agriculture and early human civilizations like the Sumer and Ancient Egypt flourished due to inundations from the surrounding Nile, Euphrates, and Tigris rivers.[1] Technological advances in the region include the development of writing, glass, the wheel, agriculture, and the use of irrigation.\\r\\nModern-day countries with significant territory within the Fertile Crescent are Iraq, Syria, Lebanon, Cyprus, Jordan, Israel, Palestine, Egypt, as well as the southeastern fringe of Turkey and the western fringes of Iran.[2][3] The term is also used in international geopolitics and diplomatic relations.\\r\\n\\r\\n\\r\\nThe term \\"Fertile Crescent\\" was popularized by archaeologist James Henry Breasted in Outlines of European History (1914) and Ancient Times, A History of the Early World (1916).[4][5][6][7][8][9] Breasted wrote:[4]\\r\\nThis fertile crescent is approximately a semicircle, with the open side toward the south, having the west end at the southeast corner of the Mediterranean, the center directly north of Arabia, and the east end at the north end of the Persian Gulf (see map, p. 100). It lies like an army facing south, with one wing stretching along the eastern shore of the Mediterranean and the other reaching out to the Persian Gulf, while the center has its back against the northern mountains. The end of the western wing is Palestine; Assyria makes up a large part of the center; while the end of the eastern wing is Babylonia.\\r\\n\\r\\nThis great semicircle, for lack of a name, may be called the Fertile Crescent.1 It may also be likened to the shores of a desert-bay, upon which the mountains behind look downa bay not of water but of sandy waste, some eight hundred kilometres across, forming a northern extension of the Arabian desert and sweeping as far north as the latitude of the northeast corner of the Mediterranean. This desert-bay is a limestone plateau of some heighttoo high indeed to be watered by the Tigris and Euphrates, which have cut ca?ons obliquely across it. Nevertheless, after the meager winter rains, wide tracts of the northern desert-bay are clothed with scanty grass, and spring thus turns the region for a short time into grasslands. The history of Western Asia may be described as an age-long struggle between the mountain peoples of the north and the desert wanderers of these grasslandsa struggle which is still going onfor the possession of the Fertile Crescent, the shores of the desert-bay.\\r\\n\\r\\n1 There is no name, either geographical or political, which includes all of this great semicircle (see map, p. 100). Hence we are obliged to coin a term and call it the Fertile Crescent.\\r\\nIn current usage, the Fertile Crescent includes Iraq, Kuwait, and surrounding portions of Iran and Turkey, as well as the rest of the Levantine coast of the Mediterranean Sea, Syria, Jordan, Israel, and Lebanon. Water sources include the Jordan River. The inner boundary is delimited by the dry climate of the Syrian Desert to the south. Around the outer boundary are the Anatolian highlands to the north and the Sahara Desert to the west.\\r\\nAs crucial as rivers and marshlands were to the rise of civilization in the Fertile Crescent, they were not the only factor. The area is geographically important as the \\"bridge\\" between Africa and Eurasia, which has allowed it to retain a greater amount of biodiversity than either Europe or North Africa, where climate changes during the Ice Age led to repeated extinction events when ecosystems became squeezed against the waters of the Mediterranean Sea. The Saharan pump theory posits that this Middle Eastern land-bridge was extremely important to the modern distribution of Old World flora and fauna, including the spread of humanity.\\r\\nThe area has borne the brunt of the tectonic divergence between the African and Arabian plates and the converging Arabian and Eurasian plates, which has made the region a very diverse zone of high snow-covered mountains.\\r\\nThe Fertile Crescent had many diverse climates, and major climatic changes encouraged the evolution of many \\"r\\" type annual plants, which produce more edible seeds than \\"K\\" type perennial plants. The region's dramatic variety in elevation gave rise to many species of edible plants for early experiments in cultivation. Most importantly, the Fertile Crescent was home to the eight Neolithic founder crops important in early agriculture (i.e., wild progenitors to emmer wheat, einkorn, barley, flax, chick pea, pea, lentil, bitter vetch), and four of the five most important species of domesticated animalscows, goats, sheep, and pigs; the fifth species, the horse, lived nearby.[10] The Fertile Crescent flora comprises a high percentage of plants that can self-pollinate, but may also be cross-pollinated.[10] These plants, called \\"selfers\\", were one of the geographical advantages of the area because they did not depend on other plants for reproduction.[10]\\r\\nFertile Crescent\\r\\nMesopotamia\\r\\nEgypt\\r\\nPersia\\r\\nAnatolia\\r\\nThe Levant\\r\\nAs well as possessing many sites with the skeletal and cultural remains of both pre-modern and early modern humans (e.g., at Tabun and Es Skhul caves in Israel), later Pleistocene hunter-gatherers, and Epipalaeolithic semi-sedentary hunter-gatherers (the Natufians); the Fertile Crescent is most famous for its sites related to the origins of agriculture. The western zone around the Jordan and upper Euphrates rivers gave rise to the first known Neolithic farming settlements (referred to as Pre-Pottery Neolithic A (PPNA), which date to around 9,000 BCE (and includes sites such as G?bekli Tepe and Jericho).\\r\\nThis region, alongside Mesopotamia (which lies to the east of the Fertile Crescent, between the rivers Tigris and Euphrates), also saw the emergence of early complex societies during the succeeding Bronze Age. There is also early evidence from the region for writing and the formation of hierarchical statelevel societies. This has earned the region the nickname \\"The cradle of civilization\\".\\r\\nIt is in this region where the first libraries appeared, some 5,000 years ago. The oldest known library was found in northern Syria, in the ruins of Ebla, a major commercial center that was destroyed around 1650 BCE.[11]\\r\\nBoth the Tigris and Euphrates start in the Taurus Mountains of what is modern-day Turkey. Farmers in southern Mesopotamia had to protect their fields from flooding each year, except northern Mesopotamia which had just enough rain to make some farming possible. To protect against flooding, they made levees.[12]\\r\\nSince the Bronze Age, the region's natural fertility has been greatly extended by irrigation works, upon which much of its agricultural production continues to depend. The last two millennia have seen repeated cycles of decline and recovery as past works have fallen into disrepair through the replacement of states, to be replaced under their successors. Another ongoing problem has been salination  gradual concentration of salt and other minerals in soils with a long history of irrigation.\\r\\nPrehistoric seedless figs were discovered at Gilgal I in the Jordan Valley, suggesting that fig trees were being planted some 11,400 years ago.[13] Cereals were already grown in Syria as long as 9,000 years ago.[14] Small cats (Felis silvestris) also were domesticated in this region.[15]\\r\\n\\r\\nLinguistically, the Fertile Crescent was a region of great diversity. Historically, Semitic languages generally prevailed in the lowlands, whilst in the mountainous areas to the east and north a number of generally unrelated languages were found including Elamite, Kassite, and Hurro-Urartian. The precise affiliation of these, and their date of arrival, remain topics of scholarly discussion. However, given lack of textual evidence for the earliest era of prehistory, this debate is unlikely to be resolved in the near future.\\r\\nThe evidence which does exist suggests that by the third millennium BCE and into the second, several language groups already existed in the region. These included:[16][17][18][19][20][21]\\r\\nLinks between Hurro-Urartian and Hattic and the indigenous languages of the Caucasus have frequently been suggested, but are not generally accepted.\\r\\nCoordinates: 36N 40E? / ?36N 40E? / 36; 40","input":"When did civilization begin in the fertile crescent?"},{"output":"trade and the spread of the Catholic faith through indigenous conversions","context":"The overseas expansion under the crown of Castile was initiated under the royal authority and first accomplished by the Spanish conquistadores. The Americas were incorporated into the Spanish Empire, with the exception of Brazil and Canada, and the crown created civil and religious structures to administer the region. The motivations for colonial expansion were trade and the spread of the Catholic faith through indigenous conversions.\\r\\nBeginning with the 1492 arrival of Christopher Columbus in the Caribbean and continuing control of vast territory for over three centuries, the Spanish Empire would expand across the Caribbean Islands, half of South America, most of Central America and much of North America (including present day Mexico, Florida and the Southwestern and Pacific Coastal regions of the United States). It is estimated that during the colonial period (1492ÿ1832), a total of 1.86 million Spaniards settled in the Americas and a further 3.5 million immigrated during the post-colonial era (1850ÿ1950); the estimate is 250,000 in the 16th century, and most during the 18th century as immigration was encouraged by the new Bourbon Dynasty.\\r\\nIn the early 19th century, the Spanish American wars of independence resulted in the emancipation of most Spanish colonies in the Americas, except for Cuba and Puerto Rico, which were finally given up in 1898, following the SpanishÿAmerican War, together with Guam and the Philippines in the Pacific. Spain's loss of these last territories politically ended the Spanish rule in the Americas.\\r\\n\\r\\n\\r\\nThe Catholic Monarchs Isabella of Castile, Queen of Castile and her husband King Ferdinand, King of Aragon, pursued a policy of joint rule of their kingdoms and created a single Spanish monarchy. Even though Castile and Aragon were ruled jointly by their respective monarchs, they remained separate kingdoms. The Catholic Monarchs gave official approval for the plans of Genoese mariner Christopher Columbus for a voyage to reach India by sailing West. The funding came from the queen of Castile, so the profits from Spanish expedition flowed to Castile. In the extension of Spanish sovereignty to its overseas territories, authority for expeditions of discovery, conquest, and settlement resided in the monarchy.[1]\\r\\nColumbus made four voyages to the West Indies as the monarchs granted Columbus the governorship of the new territories, and financed more of his trans-Atlantic journeys. He founded La Navidad on the island later named Hispaniola (now divided into Haiti and the Dominican Republic), in what is the present-day Haiti on his first voyage. After its destruction by the indigenous Taino people, the town of Isabella was begun in 1493, on his second voyage. In 1496 his brother, Bartholomew, founded Santo Domingo. By 1500, despite a high death rate, there were between 300 and 1000 Spanish settled in the area. The local Tano people continued to resist, refusing to plant crops and abandoning their Spanish-occupied villages. The first mainland explorations were followed by a phase of inland expeditions and conquest. In 1500 the city of Nueva Cdiz was founded on the island of Cubagua, Venezuela, followed by the founding of Santa Cruz by Alonso de Ojeda in present-day Guajira peninsula. Cuman in Venezuela was the first permanent settlement founded by Europeans in the mainland Americas,[2] in 1501 by Franciscan friars, but due to successful attacks by the indigenous people, it had to be refounded several times, until Diego Hernndez de Serpa's foundation in 1569. The Spanish founded San Sebastian de Uraba in 1509 but abandoned it within the year. There is indirect evidence that the first permanent Spanish mainland settlement established in the Americas was Santa Mara la Antigua del Darin.[3]\\r\\nThe Spanish conquest of Mexico is generally understood to be the Spanish conquest of the Aztec Empire (1519ÿ21) which was the base for later conquests of other regions. Later conquests were protracted campaigns with less spectacular results than the conquest of the Aztecs. The Spanish conquest of Yucatn, the Spanish conquest of Guatemala, the war of Mexico's west, and the Chichimeca War in northern Mexico expanded Spanish control over territory and indigenous populations.[4][5][6] But not until the Spanish conquest of Peru was the conquest of the Aztecs matched in scope by the victory over the Inca empire in 1532.\\r\\nThe Spanish conquest of the Aztec empire was led by Hernn Corts. The victory over the Aztecs was relatively quick, from 1519 to 1521, and aided by his Tlaxcala and other allies from indigenous city-states or altepetl. These polities allied against the Aztec empire, to which they paid tribute following conquest or threat of conquest, leaving the city-states' political hierarchy and social structure in place.\\r\\nThe Spanish conquest of Yucatn was a much longer campaign, from 1551 to 1697, against the Maya peoples in the Yucatn Peninsula of present-day Mexico and northern Central America. Hernn Corts' landing ashore at present day Veracruz and founding the Spanish city there on April 22, 1519 marked the beginning of 300 years of Spanish hegemony over the region. The assertion of royal control over the Kingdom of New Spain and the initial Spanish conquerors took over a decade, with importance of the region meriting the creation of the Viceroyalty of New Spain. Established by Charles V in 1535, the first viceroy was Don Antonio de Mendoza.\\r\\nSpain colonized and exerted control of Alta California through the Spanish missions in California until the Mexican secularization act of 1833.\\r\\nIn 1532 at the Battle of Cajamarca a group of Spaniards under Francisco Pizarro and their indigenous Andean Indian auxiliaries native allies ambushed and captured the Emperor Atahualpa of the Inca Empire. It was the first step in a long campaign that took decades of fighting to subdue the mightiest empire in the Americas. In the following years Spain extended its rule over the Empire of the Inca civilization.\\r\\nThe Spanish took advantage of a recent civil war between the factions of the two brothers Emperor Atahualpa and Huscar, and the enmity of indigenous nations the Incas had subjugated, such as the Huancas, Chachapoyas, and Ca?aris. In the following years the conquistadors and indigenous allies extended control over Greater Andes Region. The Viceroyalty of Per~ was established in 1542. The last Inca stronghold was conquered by the Spanish in 1572.\\r\\nEuropean explorers arrived in Ro de la Plata in 1516. Their first Spanish settlement in this zone was the Fort of Sancti Spiritu established in 1527 next to the Paran River. Buenos Aires, a permanent colony, was established in 1536 and in 1537 Asunci܇n was established in the area that is now Paraguay. Buenos Aires suffered attacks by the indigenous peoples that forced the settlers away, and in 1541 the site was abandoned. A second (and permanent) settlement was established in 1580 by Juan de Garay, who arrived by sailing down the Paran River from Asunci܇n (now the capital of Paraguay). He dubbed the settlement \\"Santsima Trinidad\\" and its port became \\"Puerto de Santa Mara de los Buenos Aires.\\" The city came to be the head of the Governorate of the Ro de la Plata and in 1776 elevated to be the capital of the new Viceroyalty of the Ro de la Plata.\\r\\nBetween 1537 and 1543, six[citation needed] Spanish expeditions entered highland Colombia, conquered the Muisca Confederation, and set up the New Kingdom of Granada (Spanish: Nuevo Reino de Granada). Gonzalo Jimnez de Quesada was the leading conquistador with his brother Hernn second in command.[7] It was governed by the president of the Audiencia of Bogot, and comprised an area corresponding mainly to modern-day Colombia and parts of Venezuela. The conquistadors originally organized it as a captaincy general within the Viceroyalty of Peru. The crown established the audiencia in 1549. Ultimately, the kingdom became part of the Viceroyalty of New Granada first in 1717 and permanently in 1739. After several attempts to set up independent states in the 1810s, the kingdom and the viceroyalty ceased to exist altogether in 1819 with the establishment of Gran Colombia.[8]\\r\\nSpain's administration of its colonies in the Americas was divided into the Viceroyalty of New Spain 1535 (capital, Mexico City), and the Viceroyalty of Peru 1542 (capital, Lima). In the 18th century the additional Viceroyalty of New Granada 1717 (capital, Bogot), and Viceroyalty of Rio de la Plata 1776 (capital, Buenos Aires) were established from portions of the Viceroyalty of Peru. The change diminished the political and economic power of the viceroyalty and opened formal connections between the mining district of Upper Peru and the port of Buenos Aires.\\r\\nThe system of crown rule evolved from the era of the Catholic Monarchs, which established the Council of the Indies, to the establishment of viceroyalties in Mexico and Peru following their conquests in the Hapsburg era, and then into an Intendant system in the eighteenth century as part of the Bourbon Reforms. The reform was aimed at increasing crown control over its colonies, raising more revenue, and promoting greater efficiency.\\r\\nDuring the Napoleonic Peninsular War in Europe between France and Spain, assemblies called juntas were established to rule in the name of Ferdinand VII of Spain. The Libertadores (Spanish and Portuguese for \\"Liberators\\") were the principal leaders of the Spanish American wars of independence. They were predominantly criollos (Americas-born people of European ancestry, mostly Spanish or Portuguese), bourgeois and influenced by liberalism and in some cases with military training in the mother country.\\r\\nIn 1809 the first declarations of independence from Spanish rule occurred in the Viceroyalty of New Granada. The first two were in present-day Bolivia at Sucre (May 25), and La Paz (July 16); and the third in present-day Ecuador at Quito (August 10). In 1810 Mexico declared independence, with the Mexican War of Independence following for over a decade. In 1821 Treaty of C܇rdoba established Mexican independence from Spain and concluded the War. The Plan of Iguala was part of the peace treaty to establish a constitutional foundation for an independent Mexico.\\r\\nThese began a movement for colonial independence that spread to Spain's other colonies in the Americas. The ideas from the French and the American Revolution influenced the efforts. All of the colonies, except Cuba and Puerto Rico, attained independence by the 1820s. The British Empire offered support, wanting to end the Spanish monopoly on trade with its colonies in the Americas.\\r\\nIn 1898, the United States achieved victory in the SpanishÿAmerican War with Spain, ending the Spanish colonial era. Spanish possession and rule of its remaining colonies in the Americas ended in that year with its sovereignty transferred to the United States. The United States took occupation of Cuba, the Philippines, and Puerto Rico. Puerto Rico continues to be a possession of the United States, now officially continues as a self-governing unincorporated territory.\\r\\nIt has been estimated that over 1.86 million Spaniards emigrated to Latin America in the period between 1492 and 1824, with millions more continuing to immigrate following independence.[9]\\r\\nIn Hispaniola, the indigenous Tano pre-contact population before the arrival of Columbus of several hundred thousand had declined to sixty thousand by 1509. Although population estimates vary, Dominican friar Bartolom de las Casas, the \\"Defender of the Indians\\" estimated there were 6 million (6,000,000) Tano and Arawak in the Caribbean at the time of Columbus's arrival in 1492.[citation needed]\\r\\nThe population of the Native Amerindian population in Mexico declined by an estimated 90% (reduced to 1ÿ2.5 million people) by the early 17th century. In Peru the indigenous Amerindian pre-contact population of around 6.5 million declined to 1 million by the early 17th century.[citation needed] The overwhelming cause of the decline in both Mexico and Peru was infectious diseases, although the brutality of the Encomienda also played a significant part in the population decline.\\r\\nOf the history of the indigenous population of California, Sherburne F. Cook (1896ÿ1974) was the most painstakingly careful researcher. From decades of research he made estimates for the pre-contact population and the history of demographic decline during the Spanish and post-Spanish periods. According to Cook, the indigenous Californian population at first contact, in 1769, was about 310,000 and had dropped to 25,000 by 1910. The vast majority of the decline happened after the Spanish period, during the Mexican and US periods of Californian history (1821ÿ1910), with the most dramatic collapse (200,000 to 25,000) occurring in the US period (1846ÿ1910).[10][11][12]","input":"Why did spain establish colonies in north america?"},{"output":"ancient Athens","context":"A democracy is a political system, or a system of decision-making within an institution or organization or a country, in which all members have an equal share of power.[1] Modern democracies are characterized by two capabilities that differentiate them fundamentally from earlier forms of government: the capacity to intervene in their own societies and the recognition of their sovereignty by an international legalistic framework of similarly sovereign states. Democratic government is commonly juxtaposed with oligarchic and monarchic systems, which are ruled by a minority and a sole monarch respectively.\\r\\nDemocracy in its earliest forms is generally associated with the efforts of the ancient Greeks and Romans, who were themselves considered the founders of Western civilization by the 18th century intellectuals who attempted to leverage these early democratic experiments into a new template for post-monarchical political organization.[2] The extent to which these 18th century democratic revivalists succeeded in turning the democratic ideals of the ancient Greeks and Romans into the dominant political institution of the next 300 years is hardly debatable, even if the moral justifications they often employed might be. Nevertheless, the critical historical juncture catalyzed by the resurrection of democratic ideals and institutions fundamentally transformed the ensuing centuries and has dominated the international landscape since the dismantling of the final vestige of empire following the end of the Second World War.\\r\\nModern representative democracies attempt to bridge the gulf between the Hobbesian 'state of nature' and the grip of authoritarianism through 'social contracts' that enshrine the rights of the citizens, curtail the power of the state, and grant agency through the right to vote.[3] While they engage populations with some level of decision-making , they are defined by the premise of distrust in the ability of human populations to make a direct judgement about candidates or decisions on issues.\\r\\n\\r\\n\\r\\nAnthropologists have identified forms of proto-democracy that date back to small bands of hunter-gatherers that predate the establishment of agrarian, settled, societies and still exist virtually unchanged in isolated indigenous groups today. In these groups of generally 50-100 individuals, often tied closely by familial bonds, decisions are reached by consensus or majority and many times without the designation of any specific chief.[3] Given that these dynamics are still alive and well today, it is plausible to assume that democracy in one form or another arises naturally in any well-bonded group or tribe.\\r\\nThese types of democracy are commonly identified as tribalism, or primitive democracy. In this sense, a primitive democracy usually takes shape in small communities or villages when there are face-to-face discussions in a village council or with a leader who has the backing of village elders or other cooperative forms of government.[4] This becomes more complex on a larger scale, such as when the village and city are examined more broadly as political communities. All other forms of rule ÿ including monarchy, tyranny, aristocracy, and oligarchy ÿ have flourished in more urban centers, often those with concentrated populations.[5]\\r\\nThe concepts (and name) of democracy and constitution as a form of government originated in ancient Athens circa 508 B.C. In ancient Greece, where there were many city-states with different forms of government, democracy was contrasted with governance by elites (aristocracy), by one person (monarchy), by tyrants (tyranny), etc.\\r\\nIn recent decades scholars have explored the possibility that advancements toward democratic government occurred somewhere else (i.e. other than Greece) first, as Greece developed its complex social and political institutions long after the appearance of the earliest civilizations in Egypt and the Near East.[6]\\r\\nStudying pre-Babylonian Mesopotamia, Thorkild Jacobsen used Sumerian epic, myth, and historical records to identify what he has called primitive democracy. By this, Jacobsen means a government in which ultimate power rests with the mass of free male citizens, although \\"the various functions of government are as yet little specialised [and] the power structure is loose\\". In early Sumer, kings like Gilgamesh did not hold the autocratic power that later Mesopotamian rulers wielded. Rather, major city-states functioned with councils of elders and \\"young men\\" (likely free men bearing arms) that possessed the final political authority, and had to be consulted on all major issues such as war.[7][8]\\r\\nThe work has gained little outright acceptance. Scholars criticize the use of the word \\"democracy\\" in this context since the same evidence also can be interpreted convincingly to demonstrate a power struggle between primitive monarchy and noble classes, a struggle in which the common people function more like pawns rather than any kind of sovereign authority.[9] Jacobsen conceded that the vagueness of the evidence prohibits the separation between the Mesopotamian democracy from a primitive oligarchy.[10]\\r\\nAnother claim for early democratic institutions comes from the independent \\"republics\\" of India, sanghas and ganas, which existed as early as the 6th century B.C. and persisted in some areas until the 4th century. The evidence for this is scattered, however, and no pure historical source exists for that period. In addition, Diodorusa Greek historian who wrote two centuries after the time of Alexander the Great's invasion of Indiamentions, without offering any detail, that independent and democratic states existed in India.[11] Modern scholars note the word democracy at the time of the 3rd century B.C. and later suffered from degradation and could mean any autonomous state, no matter how oligarchic in nature.[12][13]\\r\\nKey characteristics of the gana seem to include a monarch, usually known by the name raja, and a deliberative assembly. The assembly met regularly. It discussed all major state decisions. At least in some states, attendance was open to all free men. This body also had full financial, administrative, and judicial authority. Other officers, who rarely receive any mention, obeyed the decisions of the assembly. Elected by the gana, the monarch apparently always belonged to a family of the noble class of Kshatriya Varna. The monarch coordinated his activities with the assembly; in some states, he did so with a council of other nobles.[14] The Licchavis had a primary governing body of 7,077 rajas, the heads of the most important families. On the other hand, the Shakyas, Koliyas, Mallas, and Licchavis, during the period around Gautama Buddha, had the assembly open to all men, rich and poor.[15]\\r\\nScholars differ over how best to describe these governments, and the vague, sporadic quality of the evidence allows for wide disagreements. Some emphasize the central role of the assemblies and thus tout them as democracies; other scholars focus on the upper-class domination of the leadership and possible control of the assembly and see an oligarchy or an aristocracy.[16][17] Despite the assembly's obvious power, it has not yet been established whether the composition and participation were truly popular. The first main obstacle is the lack of evidence describing the popular power of the assembly. This is reflected in the Arthashastra, an ancient handbook for monarchs on how to rule efficiently. It contains a chapter on how to deal with the sangas, which includes injunctions on manipulating the noble leaders, yet it does not mention how to influence the mass of the citizensa surprising omission if democratic bodies, not the aristocratic families, actively controlled the republican governments.[18] Another issue is the persistence of the four-tiered Varna class system.[16] The duties and privileges on the members of each particular casterigid enough to prohibit someone sharing a meal with those of another ordermight have affected the roles members were expected to play in the state, regardless of the formality of the institutions. A central tenet of democracy is the notion of shared decision-making power. The absence of any concrete notion of citizen equality across these caste system boundaries leads many scholars to claim that the true nature of ganas and sanghas is not comparable to truly democratic institutions.[17]\\r\\nAncient Greece, in its early period, was a loose collection of independent city states called poleis. Many of these poleis were oligarchies.[19] The most prominent Greek oligarchy, and the state with which democratic Athens is most often and most fruitfully compared, was Sparta. Yet Sparta, in its rejection of private wealth as a primary social differentiator, was a peculiar kind of oligarchy[20] and some scholars note its resemblance to democracy.[21][22][23] In Spartan government, the political power was divided between four bodies: two Spartan Kings (diarchy), gerousia (Council of Gerontes (Elders), including the two kings), the ephors (representatives of the citizens who oversaw the Kings) and the apella (assembly of Spartans).\\r\\nThe two kings served as the head of the government. They ruled simultaneously, but they came from two separate lines. The dual kingship diluted the effective power of the executive office. The kings shared their judicial functions with other members of the gerousia. The members of the gerousia had to be over the age of 60 and were elected for life. In theory, any Spartan over that age could stand for election. However, in practice, they were selected from wealthy, aristocratic families. The gerousia possessed the crucial power of legislative initiative. Apella, the most democratic element, was the assembly where Spartans above the age of 30 elected the members of the gerousia and the ephors, and accepted or rejected gerousia's proposals.[24] Finally, the five ephors were Spartans chosen in apella to oversee the actions of the kings and other public officials and, if necessary, depose them. They served for one year and could not be re-elected for a second term. Over the years, the ephors held great influence on the formation of foreign policy and acted as the main executive body of the state. Additionally, they had full responsibility for the Spartan educational system, which was essential for maintaining the high standards of the Spartan army. As Aristotle noted, ephors were the most important key institution of the state, but because often they were appointed from the whole social body it resulted in very poor men holding office, with the ensuing possibility that they could easily be bribed.[25][26]\\r\\nThe creator of the Spartan system of rule was the legendary lawgiver Lycurgus. He is associated with the drastic reforms that were instituted in Sparta after the revolt of the helots in the second half of the 7th century BCE. In order to prevent another helot revolt, Lycurgus devised the highly militarized communal system that made Sparta unique among the city-states of Greece. All his reforms were directed towards the three Spartan virtues: equality (among citizens), military fitness, and austerity. It is also probable that Lycurgus delineated the powers of the two traditional organs of the Spartan government, the gerousia and the apella.[27]\\r\\nThe reforms of Lycurgus were written as a list of rules/laws called Great Rhetra, making it the world's first written constitution.[28] In the following centuries, Sparta became a military superpower, and its system of rule was admired throughout the Greek world for its political stability.[29] In particular, the concept of equality played an important role in Spartan society. The Spartans referred to themselves as ? (Homoioi, men of equal status). It was also reflected in the Spartan public educational system, agoge, where all citizens irrespective of wealth or status had the same education.[23] This was admired almost universally by contemporaries, from historians such as Herodotus and Xenophon to philosophers such as Plato and Aristotle. In addition, the Spartan women, unlike elsewhere, enjoyed \\"every kind of luxury and intemperance\\" including rights such as the right to inheritance, property ownership, and public education.[30]\\r\\nOverall, the Spartans were remarkably free to criticize their kings and they were able to depose and exile them. However, despite these democratic elements in the Spartan constitution, there are two cardinal criticisms, classifying Sparta as an oligarchy. First, individual freedom was restricted, since as Plutarch writes \\"no man was allowed to live as he wished\\", but as in a \\"military camp\\" all were engaged in the public service of their polis. And second, the gerousia effectively maintained the biggest share of power of the various governmental bodies.[31][32]\\r\\nThe political stability of Sparta also meant that no significant changes in the constitution were made. The oligarchic elements of Sparta became even stronger, especially after the influx of gold and silver from the victories in the Persian Wars. In addition, Athens, after the Persian Wars, was becoming the hegemonic power in the Greek world and disagreements between Sparta and Athens over supremacy emerged. These led to a series of armed conflicts known as the Peloponnesian War, with Sparta prevailing in the end. However, the war exhausted both poleis and Sparta was in turn humbled by Thebes at the Battle of Leuctra in 371 BCE. It was all brought to an end a few years later, when Philip II of Macedon crushed what remained of the power of the factional city-states to his South.\\r\\nAthens is often regarded[i] as the birthplace of democracy and remains an important reference-point for democracy.\\r\\nAthens emerged in the 7th century BCE, like many other poleis, with a dominating powerful aristocracy.[33] However, this domination led to exploitation, creating significant economic, political, and social problems. These problems exacerbated early in the 6th century; and, as \\"the many were enslaved to few, the people rose against the notables\\".[34] At the same time, a number of popular revolutions disrupted traditional aristocracies. This included Sparta in the second half of the 7th century BCE. The constitutional reforms implemented by Lycurgus in Sparta introduced a hoplite state that showed, in turn, how inherited governments can be changed and lead to military victory.[35] After a period of unrest between the rich and poor, Athenians of all classes turned to Solon to act as a mediator between rival factions, and reached a generally satisfactory solution to their problems.[36][37]\\r\\nSolon(c.?638 ÿ c.?558 BC), an Athenian (Greek) of noble descent but moderate means, was a lyric poet and later a lawmaker; Plutarch ranked him as one of the Seven Sages of the ancient world.[37] Solon attempted to satisfy all sides by alleviating the suffering of the poor majority without removing all the privileges of the rich minority.[38] Solon divided the Athenians into four property-classes, with different rights and duties for each. As the Rhetra did in Lycurgian Sparta, Solon formalized the composition and functions of the governmental bodies. All citizens gained the right to attend the Ecclesia (Assembly) and to vote. The Ecclesia became, in principle, the sovereign body, entitled to pass laws and decrees, elect officials, and hear appeals from the most important decisions of the courts.[37] All but those in the poorest group might serve, a year at a time, on a new Boule of 400, which was to prepare the agenda for the Ecclesia. The higher governmental posts, those of the archons (magistrates), were reserved for citizens of the top two income groups. The retired archons became members of the Areopagus (Council of the Hill of Ares), which like the Gerousia in Sparta, was able to check improper actions of the newly powerful Ecclesia. Solon created a mixed timocratic and democratic system of institutions.[39]\\r\\nOverall, Solon devised the reforms of 594 BC to avert the political, economic, and moral decline in archaic Athens and gave Athens its first comprehensive code of law. The constitutional reforms eliminated enslavement of Athenians by Athenians, established rules for legal redress against over-reaching aristocratic archons, and assigned political privileges on the basis of productive wealth rather than of noble birth. Some of Solon's reforms failed in the short term, yet he is often[quantify] credited with having laid the foundations for Athenian democracy.[40][41]\\r\\nEven though the Solonian reorganization of the constitution improved the economic position of the Athenian lower classes, it did not eliminate the bitter aristocratic contentions for control of the archonship, the chief executive post. Peisistratus became tyrant of Athens three times from 561 BCE and remained in power until his death in 527 BCE. His sons Hippias and Hipparchus succeeded him.[42]\\r\\nAfter the fall of tyranny (510 BCE) and before the year 508ÿ507 was over, Cleisthenes proposed a complete reform of the system of government, which later was approved by the popular Ecclesia.[43] Cleisthenes reorganized the population of citizens into ten tribes, with the aim to change the basis of political organization from the family loyalties to political ones,[44] and improve the army's organization.[45] He also introduced the principle of equality of rights for all male citizens, isonomia,[43] by expanding access to power to more citizens.[46] During this period, Athenians first used the word \\"democracy\\" (Greek: Ѵϫ?ϫ ÿ \\"rule by the people\\") to define their new system of government.[47] In the next generation, Athens entered its Golden Age, becoming a great center of literature and art.[48] Greek victories in Persian Wars (499ÿ449 BCE) encouraged the poorest Athenians (who participated in the military campaigns) to demand a greater say in the running of their city. In the late 460s, Ephialtes and Pericles presided over a radicalization of power that shifted the balance decisively to the poorest sections of society, by passing laws which severely limited the powers of the Council of the Areopagus and allowed thetes (Athenians without wealth) to occupy public office.[49] Pericles became distinguished as the Athenians' greatest democratic leader, even though he has been accused of running a political machine.[citation needed] In the following passage, Thucydides recorded Pericles, in the funeral oration, describing the Athenian system of rule:\\r\\nThe Athenian democracy of Cleisthenes and Pericles was based on freedom of citizens(through the reforms of Solon) and on equality of citizens(isonomia) - introduced by Cleisthenes and later expanded by Ephialtes and Pericles. To preserve these principles, the Athenians used lot for selecting officials. Casting lots aimed to ensure that all citizens were \\"equally\\" qualified for office, and to avoid any corruption allotment machines were used.[51] Moreover, in most positions chosen by lot, Athenian citizens could not be selected more than once; this rotation in office meant that no-one could build up a power base through staying in a particular position.[52]\\r\\nThe courts formed another important political institution in Athens; they were composed of a large number of juries with no judges, and they were selected by lot on a daily basis from an annual pool, also chosen by lot. The courts had unlimited power to control the other bodies of the government and its political leaders.[5] Participation by the citizens selected was mandatory,[53] and a modest financial compensation was given to citizens whose livelihood was affected by being \\"drafted\\" to office. The only officials chosen by elections, one from each tribe, were the strategoi (generals), where military knowledge was required, and the treasurers, who had to be wealthy, since any funds revealed to have been embezzled were recovered from a treasurer's private fortune. Debate was open to all present and decisions in all matters of policy were taken by majority vote in the Ecclesia (compare direct democracy), in which all male citizens could participate (in some cases with a quorum of 6000). The decisions taken in the Ecclesia were executed by the Boule of 500, which had already approved the agenda for the Ecclesia. The Athenian Boule was elected by lot every year[54] and no citizen could serve more than twice.[55]\\r\\nOverall, the Athenian democracy was not only direct in the sense that decisions were made by the assembled people, but also directest in the sense that the people through the assembly, boule, and courts of law controlled the entire political process and a large proportion of citizens were involved constantly in the public business.[56] And even though the rights of the individual (probably) were not secured by the Athenian constitution in the modern sense,[ii] the Athenians enjoyed their liberties not in opposition to the government, but by living in a city that was not subject to another power and by not being subjects themselves to the rule of another person.[47]\\r\\nWithin the Athenian democratic environment, many philosophers from all over the Greek world gathered to develop their theories. Socrates (470-399 BCE) was the first to raise the question, further expanded by his pupil Plato (died 348/347), about the relation/position of an individual within a community. Aristotle (384ÿ322 BCE) continued the work of his teacher, Plato, and laid the foundations of political philosophy. The political philosophy developed in Athens was, in the words of Peter Hall, \\"in a form so complete that hardly added anyone of moment to it for over a millennium\\".[57] Aristotle systematically analyzed the different systems of rule that the numerous Greek city-states had and divided them into three categories based on how many ruled: the many (democracy/polity), the few (oligarchy/aristocracy), a single person (tyranny, or today: autocracy/monarchy). For Aristotle, the underlying principles of democracy are reflected in his work Politics:\\r\\nThe Athenian democracy, in its two centuries of life-time, twice voted against its democratic constitution (both times during the crisis at the end of the Pelopponesian War of 431 to 404 BC), establishing first the Four Hundred (in 411 BCE) and second Sparta's puppet rgime of the Thirty Tyrants (in 404 BCE). Both votes took place under manipulation and pressure, but democracy was recovered in less than a year in both cases. Reforms following the restoration of democracy after the overthrow of the Thirty Tyrants removed most law-making authority from the Assembly and placed it in randomly selected law-making juries known as \\"nomothetai\\". Athens restored its democratic constitution again after King Phillip II of Macedon (reigned 359-336 BCE) and later Alexander the Great (reigned 336ÿ323 BCE) unified Greece, but it was politically over-shadowed by the Hellenistic empires. Finally, after the Roman conquest of Greece in 146 BC, Athens was restricted to matters of local administration.\\r\\nHowever, democracy in Athens declined not only due to external powers, but due to its citizens, such as Plato and his student Aristotle. Because of their influential works, after the rediscovery of classics during the Renaissance, Sparta's political stability was praised,[59][60][61] while the Periclean democracy was described as a system of rule where either the less well-born, the mob (as a collective tyrant), or the poorer classes held power.[47] Only centuries afterwards, after the publication of A History of Greece by George Grote from 1846 onwards, did modern political thinkers start to view the Athenian democracy of Pericles positively.[62] In the late 20th century scholars re-examined the Athenian system of rule as a model of empowering citizens and as a \\"post-modern\\" example for communities and organizations alike.[63]\\r\\nEven though Rome is classified as a Republic and not a democracy, its history has helped preserve the concept of democracy over the centuries. The Romans invented the concept of classics and many works from Ancient Greece were preserved.[64] Additionally, the Roman model of governance inspired many political thinkers over the centuries,[65] and today's modern (representative) democracies imitate more the Roman than the Greek models.[66]\\r\\nRome was a city-state in Italy next to powerful neighbors; Etruscans had built city-states throughout central Italy since the 13th century BCE and in the south were Greek colonies. Similar to other city-states, Rome was ruled by a king. However, social unrest and the pressure of external threats led in 510 BCE the last king to be deposed by a group of aristocrats led by Lucius Junius Brutus.[67][68] A new constitution was crafted, but the conflict between the ruling families (patricians) and the rest of the population, the plebeians continued. The plebs were demanding for definite, written, and secular laws. The patrician priests, who were the recorders and interpreters of the statutes, by keeping their records secret used their monopoly against social change. After a long resistance to the new demands, the Senate in 454 BCE sent a commission of three patricians to Greece to study and report on the legislation of Solon and other lawmakers.[67][68] When they returned, the Assembly in 451 BCE chose ten men ÿ a decemviri ÿ to formulate a new code, and gave them supreme governmental power in Rome for two years. This commission, under the supervision of a resolute reactionary, Appius Claudius, transformed the old customary law of Rome into Twelve Tables and submitted them to the Assembly (which passed them with some changes) and they were displayed in the Forum for all who would and could read. The Twelve Tables recognised certain rights and by the 4th century BCE, the plebs were given the right to stand for consulship and other major offices of the state.\\r\\nThe political structure as outlined in the Roman constitution resembled a mixed constitution[69] and its constituent parts were comparable to those of the Spartan constitution: two consuls, embodying the monarchic form; the Senate, embodying the aristocratic form; and the people through the assemblies.[70] The consul was the highest ranking ordinary magistrate.[71] Consuls had power in both civil and military matters. While in the city of Rome, the consuls were the head of the Roman government and they would preside over the Senate and the assemblies. While abroad, each consul would command an army. The Senate passed decrees, which were called senatus consultum and were official advices to a magistrate. However, in practice, it was difficult for a magistrate to ignore the Senate's advice.[71] The focus of the Roman Senate was directed towards foreign policy. Though it technically had no official role in the management of military conflict, the Senate ultimately was the force that oversaw such affairs. Also, it managed Rome's civil administration. The requirements for becoming a senator included having at least 100,000 denarii worth of land, being born of the patrician (noble aristocrats) class, and having held public office at least once before. New Senators had to be approved by the sitting members.[71] The people of Rome through the assemblies had the final say regarding the election of magistrates, the enactment of new laws, the carrying out of capital punishment, the declaration of war and peace, and the creation (or dissolution) of alliances. Despite the obvious power the assemblies had, in practice, the assemblies were the least powerful of the other bodies of government. An assembly was legal only if summoned by a magistrate[71] and it was restricted from any legislative initiative or the ability to debate. And even the candidates for public office as Livy writes \\"levels were designed so that no one appeared to be excluded from an election and yet all of the clout resided with the leading men\\".[72] Moreover, the unequal weight of votes was making a rare practice for asking the lowest classes for their votes.[72][73]\\r\\nRoman stability, in Polybius assessment, was owing to the checks each element put on the superiority of any other: a consul at war, for example, required the cooperation of the Senate and the people if he hoped to secure victory and glory, and could not be indifferent to their wishes. This was not to say that the balance was in every way even: Polybius observes that the superiority of the Roman to the Carthaginian constitution (another mixed constitution) at the time of the Hannibalic War was an effect of the latters greater inclination toward democracy than to aristocracy.[74] Moreover, recent attempts to posit for Rome personal freedom in the Greek sense ÿ eleutheria: living as you like ÿ have fallen on stony ground, since eleutheria (which was an ideology and way of life in the democratic Athens[75]) was anathema in the Roman eyes.[76] Romes core values included order, hierarchy, discipline, and obedience. These values were enforced with laws regulating the private life of an individual. The laws were applied in particular to the upper classes, since the upper classes were the source of Roman moral examples.\\r\\nRome became the ruler of a great Mediterranean empire. The new provinces brought wealth to Italy, and fortunes were made through mineral concessions and enormous slave run estates. Slaves were imported to Italy and wealthy landowners soon began to buy up and displace the original peasant farmers. By the late 2nd century this led to renewed conflict between the rich and poor and demands from the latter for reform of the constitution. The background of social unease and the inability of the traditional republican constitutions to adapt to the needs of the growing empire led to the rise of a series of over-mighty generals, championing the cause of either the rich or the poor, in the last century BCE.\\r\\nOver the next few hundred years, various generals would bypass or overthrow the Senate for various reasons, mostly to address perceived injustices, either against themselves or against poorer citizens or soldiers. One of those generals was Julius Caesar, where he marched on Rome and took supreme power over the republic. Caesar's career was cut short by his assassination at Rome in 44 BCE by a group of Senators including Marcus Junius Brutus. In the power vacuum that followed Caesar's assassination, his friend and chief lieutenant, Marcus Antonius, and Caesar's grandnephew Octavian who also was the adopted son of Caesar, rose to prominence. Their combined strength gave the triumvirs absolute power. However, in 31 BC war between the two broke out. The final confrontation occurred on 2 September 31 BCE, at the naval Battle of Actium where the fleet of Octavian under the command of Agrippa routed Antony's fleet. Thereafter, there was no one left in the Roman Republic who wanted to, or could stand against Octavian, and the adopted son of Caesar moved to take absolute control. Octavian left the majority of Republican institutions intact, though he influenced everything using personal authority and ultimately controlled the final decisions, having the military might to back up his rule if necessary. By 27 BCE the transition, though subtle, disguised, and relying on personal power over the power of offices, was complete. In that year, Octavian offered back all his powers to the Senate, and in a carefully staged way, the Senate refused and titled Octavian Augustus  \\"the revered one\\". He was always careful to avoid the title of rex  \\"king\\", and instead took on the titles of princeps  \\"first citizen\\" and imperator, a title given by Roman troops to their victorious commanders.\\r\\nThe Roman Empire had been born. Once Octavian named Tiberius as his heir, it was clear to everyone that even the hope of a restored Republic was dead. Most likely, by the time Augustus died, no one was old enough to know a time before an Emperor ruled Rome. The Roman Republic had been changed into a despotic rgime, which, underneath a competent and strong Emperor, could achieve military supremacy, economic prosperity, and a genuine peace, but under a weak or incompetent one saw its glory tarnished by cruelty, military defeats, revolts, and civil war.\\r\\nThe Roman Empire was eventually divided between the Western Roman Empire which fell in 476 AD and the Eastern Roman Empire (also called the Byzantine Empire) which lasted until the fall of Constantinople in 1453 AD.\\r\\nMost of the procedures used by modern democracies are very old. Almost all cultures have at some time had their new leaders approved, or at least accepted, by the people; and have changed the laws only after consultation with the assembly of the people or their leaders. Such institutions existed since before the times of the Iliad or of the Odyssey, and modern democracies are often derived from or inspired by them, or what remained of them.\\r\\nNevertheless, the direct result of these institutions was not always a democracy. It was often a narrow oligarchy, as in Venice, or even an absolute monarchy, as in Florence, in the Renaissance period; but during the medieval period guild democracies did evolve.\\r\\nEarly institutions included:\\r\\nHistorian Jack Weatherford has argued that the ideas leading to the United States Constitution and democracy derived from various indigenous peoples of the Americas including the Iroquois. Weatherford claimed this democracy was founded between the years 1000ÿ1450, and lasted several hundred years, and that the U.S. democratic system was continually changed and improved by the influence of Native Americans throughout North America.[85]\\r\\nTemple University professor of anthropology and an authority on the culture and history of the Northern Iroquois Elizabeth Tooker has reviewed these claims and concluded they are myth rather than fact. The idea that North American Indians had a democratic culture is several decades old, but not usually expressed within historical literature. The relationship between the Iroquois League and the Constitution is based on a portion of a letter written by Benjamin Franklin and a speech by the Iroquois chief Canasatego in 1744. Tooker concluded that the documents only indicate that some groups of Iroquois and white settlers realized the advantages of a confederation, and that ultimately there is little evidence to support the idea that eighteenth century colonists were knowledgeable regarding the Iroquois system of governance.[86]\\r\\nWhat little evidence there is regarding this system indicates chiefs of different tribes were permitted representation in the Iroquois League council, and this ability to represent the tribe was hereditary. The council itself did not practice representative government, and there were no elections; deceased chiefs' successors were selected by the most senior woman within the hereditary lineage in consultation with other women in the clan. Decision making occurred through lengthy discussion and decisions were unanimous, with topics discussed being introduced by a single tribe. Tooker concludes that \\"...there is virtually no evidence that the framers borrowed from the Iroquois\\" and that the myth that this was the case is the result of exaggerations and misunderstandings of a claim made by Iroquois linguist and ethnographer J.N.B. Hewitt after his death in 1937.[86]\\r\\nThe Aztecs also practiced elections, but the elected officials elected a supreme speaker, not a ruler.[85]\\r\\nThe notion of a secret ballot, where one is entitled to the privacy of their votes, is taken for granted by most today by virtue of the fact that it is simply considered the norm. However, this practice was highly controversial in the 19th century; it was widely argued that no man would want to keep his vote secret unless he was ashamed of it.[citation needed]\\r\\nThe two earliest systems used were the Victorian method and the South Australian method. Both were introduced in 1856 to voters in Victoria and South Australia. The Victorian method involved voters crossing out all the candidates whom he did not approve of. The South Australian method, which is more similar to what most democracies use today, had voters put a mark in the preferred candidate's corresponding box. The Victorian voting system also was not completely secret, as it was traceable by a special number.\\r\\nThe stone inscriptions in a temple say that ballot elections were held in South India by a method called Kudavolai system. Kudavolai means the ballot sheet of leaf that was put secretly in a pot vessel called \\"kudam\\". The details are found inscribed on the walls of the village assembly hall. Actually, the once village-assembly hall is the present temple. The details show that the village had a secret ballot electoral system and a written Constitution, prescribing the mode of elections.[citation needed]\\r\\nThe end of the First World War was a temporary victory for democracy in Europe, as it was preserved in France and temporarily extended to Germany. Already in 1906 full modern democratic rights, universal suffrage for all citizens was implemented constitutionally in Finland as well as a proportional representation, open list system. Likewise, the February Revolution in Russia in 1917 inaugurated a few months of liberal democracy under Alexander Kerensky until Lenin took over in October. The terrible economic impact of the Great Depression hurt democratic forces in many countries. The 1930s became a decade of dictators in Europe and Latin America.\\r\\nIn 1918 the United Kingdom granted the right to vote to women over 30 who met a property qualification the right to vote, a second one was later passed in 1928 granting women and men equal rights. On August 18, 1920 the Nineteenth Amendment (Amendment XIX) to the United States Constitution was adopted which prohibits the states and the federal government from denying the right to vote to citizens of the United States on the basis of sex. French women got the right to vote in 1944, but did not actually cast their ballot for the first time until April 29, 1945.\\r\\nThe Indian Citizenship Act of 1924 granted full U.S. citizenship to America's indigenous peoples, called \\"Indians\\" in this Act. (The Fourteenth Amendment guarantees citizenship to persons born in the U.S., but only if \\"subject to the jurisdiction thereof\\"; this latter clause excludes certain indigenous peoples.) The act was signed into law by President Calvin Coolidge on 2 June 1924. The act further enfranchised the rights of peoples resident within the boundaries of the United States.\\r\\nWorld War II was ultimately a victory for democracy in Western Europe, where representative governments were established that reflected the general will of their citizens. However, many countries of Central and Eastern Europe became undemocratic Soviet satellite states. In Southern Europe, a number of right-wing authoritarian dictatorships (most notably in Spain and Portugal) continued to exist.\\r\\nJapan had moved towards democracy during the Taish period during the 1920s, but it was under effective military rule in the years before and during World War II. The country adopted a new constitution during the postwar Allied occupation, with initial elections in 1946.\\r\\nWorld War II also planted seeds of democracy outside Europe and Japan, as it weakened, with the exception of the USSR and the United States, all the old colonial powers while strengthening anticolonial sentiment worldwide. Many restive colonies/possessions were promised subsequent independence in exchange for their support for embattled colonial powers during the war.\\r\\nThe aftermath of World War II also resulted in the United Nations' decision to partition the British Mandate into two states, one Jewish and one Arab. On 14 May 1948 the state of Israel declared independence and thus was born the first full democracy in the Middle East. Israel is a representative democracy with a parliamentary system and universal suffrage.[111][112]\\r\\nIndia became a Democratic Republic in 1950 after achieving independence from Great Britain in 1947. After holding its first national elections in 1952, India achieved the status of the world's largest liberal democracy with universal suffrage which it continues to hold today. Most of the former British and French colonies were independent by 1965 and at least initially democratic; those that were formerly part of the British Empire often adopted the Westminster parliamentary system.[113] The process of decolonisation created much political upheaval in Africa and parts of Asia, with some countries experiencing often rapid changes to and from democratic and other forms of government.\\r\\nIn the United States of America, the Voting Rights Act of 1965 and the Civil Rights Act enforced the 15th Amendment. The 24th Amendment ended poll taxing by removing all tax placed upon voting, which was a technique commonly used to restrict the African American vote. The Voting Rights Act also granted voting rights to all Native Americans, irrespective of their home state. The minimum voting age was reduced to 18 by the 26th Amendment in 1971.\\r\\nNew waves of democracy swept across Southern Europe in the 1970s, as a number of right-wing nationalist dictatorships fell from power. Later, in Central and Eastern Europe in the late 1980s, the communist states in the USSR sphere of influence were also replaced with liberal democracies.\\r\\nMuch of Eastern Europe, Latin America, East and Southeast Asia, and several Arab, central Asian and African states, and the not-yet-state that is the Palestinian Authority moved towards greater liberal democracy in the 1990s and 2000s.\\r\\nAn analysis by the U.S. Government funded Freedom House shows that there was not a single liberal democracy with universal suffrage in the world in 1900, but that in 2000, 120 of the world's 192 nations, or 62% were such democracies. They count 25 nations, or 13% of the world's nations with \\"restricted democratic practices\\" in 1900 and 16, or 8% of the world's nations today. They counted 19 constitutional monarchies in 1900, forming 14% of the world's nations, where a constitution limited the powers of the monarch, and with some power devolved to elected legislatures, and none in the present. Other nations had, and have, various forms of non-democratic rule.[115] While the specifics may be open to debate (for example, New Zealand actually enacted universal suffrage in 1893, but is discounted due to a lack of complete sovereignty and certain restrictions on the Mori vote), the numbers are indicative of the expansion of democracy during the twentieth century.\\r\\nIn the 21st century, democracy movements have been seen across the world. In the Arab world, an unprecedented series of major protests occurred with citizens of Egypt, Tunisia, Bahrain, Yemen, Jordan, Syria and other countries across the MENA region demanding democratic rights. This revolutionary wave was given the term Tunisia Effect, as well as the Arab Spring. The Palestinian Authority also took action to address democratic rights.\\r\\nIn Iran, following a highly disputed presidential vote fraught with corruption, Iranian citizens held a major series of protests calling for change and democratic rights (see: the 2009ÿ2010 Iranian election protests and the 2011 Iranian protests). The 2003 US-led invasion of Iraq led to a toppling of Saddam Hussein and a new constitution with free and open elections.[116][citation needed][neutrality is disputed]\\r\\nIn Asia, the country of Burma (also known as Myanmar) had long been ruled by a military junta; however, in 2011, the government changed to allow certain voting rights and released democracy-leader Aung San Suu Kyi from house arrest. However, Burma still will not allow Suu Kyi to run for election and still has major human rights problems and not full democratic rights. However, this was later partialy aborgated with the election of suu kyi's national league for democracy party and her appointment as the de facto leader of Burma (Myanmar) with the title \\"state councellor\\", as she is still not allowed to be president and therefore leads through a figurehead, Htin Kyaw. Human rights, however, have not improved. In Bhutan, in December 2005, the 4th King Jigme Singye Wangchuck announced that the first general elections would be held in 2008, and that he would abdicate the throne in favor of his eldest son. Bhutan is currently undergoing further changes to allow for a constitutional monarchy. In the Maldives, protests and political pressure led to a government reform which allowed democratic rights and presidential elections in 2008. These were however undone by a coup in 2018.\\r\\nNot all movement has been pro-democratic however. In Poland and Hungary, so-called 'illiberal democracies' have taken hold, with the ruling parties in both countries considered by the EU and civil society to be working to undermine democratic governance. Also in Europe, the Spanish government refused to allow a democratic vote on the future of Catalunya, a decision causing months of instability in the region. Meanwhile in Thailand a military junta twice overthrew democratically elected governments and has changed the constitution in order to increase its own power. The authoritarian regime of Han Sen in Cambodia also dissolved the main opposition party and effectively implemented a one-man dictatorship. There are also large parts of the world such as China, Russia, Central and South East Asia, the Middle East and much of Africa which have consolidated authoritarian rule rather seeing it weaken.\\r\\nUnder the influence of the theory of deliberative democracy, there have been several experiments since the start of the new millennium with what are called deliberative fora, places (in real life or in cyber space) where citizens and their representatives assemble to exchange reasons. One type of deliberative forum is called a minpublic: a body of randomly chosen or actively selected citizens that represents the whole population. The use of random selection to form a representative deliberative body is known as sortition. Examples of this are citizens' assemblies and citizens' juries. Citizens' assemblies have been used in Canada (2004, 2006) and the Netherlands (2006) to debate electoral reform, and in Iceland (2009 and 2010) for broader constitutional change.","input":"Where was the worlds first known democracy located?"},{"output":"Thomason College of Civil Engineering","context":"The impulse for creation of centres of technical training came from the British rulers of India, and it arose out of the necessity for the training of overseers for construction and maintenance of public buildings, roads, canals, and ports, and for the training of artisans and craftsmen for the use of instruments, and apparatus needed for the army, the navy, and the survey department. While superintending engineers were mostly recruited from Britain, lower grade craftsmen, artisans and sub-overseers were recruited locally. The necessity to make them more efficient, led to the establishment of industrial schools attached to the Ordnance Factories Board and other engineering establishments.\\r\\nThe first engineering college was established in present day Uttar Pradesh at Roorkee in the year 1847 for the training of Civil Engineers. Thomason College of Civil Engineering as it was called, made use of the large workshops and public buildings there that were erected for the Upper Ganges Canal. The college was converted to the University of Roorkee in 1948 and upgraded to the Indian Institute of Technology, Roorkee in 2001.\\r\\nIn pursuance of the Government policy, three Engineering Colleges were opened by about 1856 in the three Presidencies.\\r\\nIn Bengal, a College called the Calcutta Civil Engineering College was opened at the Writers' Buildings in November 1856. With the establishment of University of Calcutta on 24 January 1857, the college was affiliated to this university in May 1857. In 1865, the college merged with Presidency College, Kolkata and from 1865 to 1869 the college functioned as the Civil Engineering Department of Presidency College. In 1880, the college was shifted to its present campus at Shibpur, Howrah, and was christened the Government College, Howrah, in the premises of Bishop's College. On 12 February 1920 the name was changed to Bengal Engineering College, Shibpur. The word Shibpur was deleted on 24 March 1921 and it became Bengal Engineering College. The college was subsequently rechristened Bengal Engineering and Science University and upgraded to the Indian Institute of Engineering Science and Technology, Shibpur in 2014.\\r\\nIn Bombay, the overseers' school at Pune eventually became the College of Engineering, Pune and affiliated to the erstwhile University of Bombay (now called University of Mumbai) in 1858.\\r\\nIn the Madras Presidency, the industrial school attached to the Gun Carriage Factory became ultimately the College of Engineering, Guindy and was affiliated to the University of Madras in 1858.\\r\\nThe Banaras Engineering College was established at Varanasi in the year 1919. The college was rechristened as the Institute of Technology, Banaras Hindu University in 1968. The college was upgraded to the Indian Institute of Technology (Banaras Hindu University), Varanasi in 2012.\\r\\nThe British also opened Harcourt Butler Technological Institute, Kanpur for chemical sciences in 1921 in the United Provinces, now Uttar Pradesh. it is now called the Harcourt Butler Technical University.\\r\\nIndian School of Mines, Dhanbad was established by British Indian Government on the lines of the Royal School of Mines, London, and was formally opened on 9 December 1926 by Lord Irwin, the then Viceroy of India. The college was subsequently called the Indian School of Mines University for a while before the word University was dropped from its name. The college was upgraded to the Indian Institute of Technology (Indian School of Mines), Dhanbad in 2016.[1]\\r\\nIn 1947 when India became independent, there were 36 institutions for first-degree engineering education, with an annual intake of about 2500 students.[2]\\r\\n|- | JNTU College of Engineering, Ananatpur was started in1946 as extension of College of Engineering Guindy, later became part of SV University and finally part of JNT University. |Anantapur |1946 |}","input":"Which was the first engineering college in india?"},{"output":"speaks of the protagonist who, despite being ignored by her significant other, resolves to love him regardless","context":"\\"Video Games\\" is a song recorded by American singer and songwriter Lana Del Rey for her second studio album and major label debut, Born to Die (2012). It was first released to the Internet on June 29, 2011, was later released on her extended play, Lana Del Rey, and re-released as the lead single from her second studio album, Born to Die on October 10, 2011, through Interscope Records. The song was produced by Robopop while the lyrics were written by Del Rey and Justin Parker. \\"Video Games\\" is a baroque pop ballad that speaks of the protagonist who, despite being ignored by her significant other, resolves to love him regardless.\\r\\n\\"Video Games\\" has received several accolades from critics since its release. The song has been considered Del Rey's \\"breakthrough hit\\" and was a commercial success, peaking at number one in Germany and Luxembourg, reaching a top ten position in Belgium, France, the Netherlands and the United Kingdom and peaked at 91 on Billboard Hot 100 chart and was certified Gold. The song's accompanying music video was directed and edited by Del Rey herself, combining scenes of her performing the song filmed on a webcam with clips of archive footage. The song won an Ivor Novello Award for \\"Best Contemporary Song\\" in 2012 and was nominated for several other awards shortly after release.\\r\\n\\r\\n\\r\\n\\"Video Games\\" is four minutes and 42 seconds long (4:42).[5][6] The song was written and composed by Elizabeth Grant (Lana Del Rey) and Justin Parker in the key of F? minor.[7] Set in time signature of common time with a tempo of 123 beats per minute, Del Rey's vocal range spans from E3 to A4.[7] Lindsey Johnstone of The Scotsman described the song as an \\"ode to being ignored and the exquisite pain of clutching at an illusion of happiness\\".[8] Alexis Petridis of The Guardian praised how Del Rey's vocal performance, against ballooning orchestration and pizzicato strings, overlapped the subdued theme of an aloof, beer-drinking boyfriend figure.[4] Del Rey described her music and day-to-day style as \\"gangsta Nancy Sinatra\\",[3] with contemporary critics noting the song as a doom-filled ballad that unapologetically displays vulnerability.[3]\\r\\nProminent lyrics in the song include, \\"I heard you like the bad girls, honey, is that true?\\";[9] \\"Heaven is a place on earth with you/ Tell me all the things you want to do\\"; and \\"open up a beer... and play a video game?\\"[10] In an interview for British online magazine The Quietus, Del Rey stated that the inspiration for the song came from her ex-boyfriend, commenting, \\"I think we came together because we were both outsiders. It was perfect. But I think with that contentment also comes sadness. There was something heavenly about that life?ÿ we'd go to work and he'd play his video games?ÿ but also it was maybe too regular.\\"[11] The singer also stated that she used lower vocals for \\"Video Games\\" because she felt that the public did not see her as a serious artist.[2] Thematically, the lyrics have been labeled as antifeminist.[12][13] Of the musical composition, MTV lauded the cinematic atmosphere of the song, adoring its feathery violins, echoing electronic thumps, and melancholic crescendos  wound into a dramatic exhibition.[14]\\r\\nFollowing the release of the single, Ian Cohen dubbed the song \\"Best New Music\\" on Pitchfork commenting, \\"on her stirring debut single, New York singer-songwriter Lizzy Grant transforms into the more bombastically named Lana Del Rey and absolutely wallows in it.\\"[10] Digital Spy writer Robert Copsey noted Del Rey \\"combines a near-identical [Nancy Sinatra] vocal with her own fascination for the tragically glamorous? or 'Hollywood sadcore' as she succinctly labels it.\\"[3] Lewis Corner, also of Digital Spy, said, \\"New York-born, London-based Lana Del Rey is currently working on her debut album, but if this newly released preview track/video is anything to go by, it's going to be avant-garde pop at its very best.\\"[15] Alexis Petridis of The Guardian considered \\"Video Games\\" the best song of 2011 and added, \\"it's tempting to say with a song that good it doesn't matter who sings it, but that's not strictly true. As pop divas who collaborate with Eg White go, Lana Del Rey sounds hugely understated.\\"[4]\\r\\nIn a poll performed by NME, readers ranked it as the tenth best song of the year.[16] Maura Johnston of The Village Voice, however, called the song and the accompanying music video overproduced,[6] while New York magazine contributor Amanda Dobbis commented that \\"it's hard to totally separate Del Rey's looks from the criticism that's been bubbling around her.\\"[17] In responses to this controversy, Paste's Luke Larson said, \\"when 'Video Games' first hit the web, people werent freaking out about Lana Del Rey or fake lips or Lizzy Grant. People were freaking out because they had stumbled upon a fresh voice and a beautifully written song and ultimately, does the rest really matter?\\"[18] Krystina Nellis of Drowned in Sound said the single reminded her of something from a David Lynch movie.[19]\\r\\nAdditional, Drowned in Sound wrote of \\"Video Games\\":\\r\\nAbove all the noise of the internet, Video Games still works and is magical; I still have to listen on quiet because whenever I do I become convinced something absolutely terrible is about to crash through the window; it might as well be sung by Laura Palmer and be about the Bobs at the end of your bed. Either way, it is a brilliant, beautifully-executed pop song, a proper shivery, proper classic.[20]\\r\\nThe Village Voice's Pazz & Jop annual critics' poll ranked \\"Video Games\\" at number seven to find the best music of 2011, tied with Britney Spears's \\"Till the World Ends\\". The British magazine NME named \\"Video Games\\" the best single of 2011.\\r\\n\\"Video Games\\" attained commercial success across Europe, reaching number one in Germany and top ten positions in Austria, Belgium, France, Ireland, the Netherlands, Switzerland and the United Kingdom.[21][22][23] The song was also voted number six in Triple J's Hottest 100 songs of 2011.[24]\\r\\nThe music video for \\"Video Games\\" was directed and edited by Del Rey. It features video clips of skateboarders, cartoons and shots from old movies, as well as paparazzi footage of Paz de la Huerta falling down while intoxicated.[17] These are interspersed with shots of Del Rey singing, which she filmed herself using a webcam.[17] When asked if she would change anything about the video's production, Del Rey stated, \\"Had I known so many people were going to watch the video, I'd have put some more effort into it. I would have got my hair and make-up done and tried not to be so pouty, seeing as everyone talks about my face all the time. And I'd have put more of a storyline into it.\\"[2] The singer also revealed that she was \\"trying to look smart and well turned-out, rather than 'sexy' [in the music video]. Of course I wanted to look good, but 'smart' was the primary focus.\\"[11]\\r\\nNew York magazine contributor Amanda Dobbins noted the music video \\"predictably [...] garnered some attention\\" from the public.[17] Rya Backer of MTV questioned Del Rey's originality, saying, \\"it's hard to know what to make of Lana Del Rey at first glance. Is she as Jools Holland once dubbed her an 'Internet phenomenon' worth no more than a few salacious blog posts? Or is she a legitimate chanteuse wrapped in the hyper feminine, yet innately American (and admittedly well-curated) image she's projected in videos for such tracks as 'Video Games' and 'Blue Jeans'?\\"[25] Pitchfork's Ian Cohen commented that the music video \\"fits between surrendering to romance and depression, moving with the elegant wastefulness of the kind of day drunk that's a true privilege of the beautiful, idle class.\\"[10]\\r\\nDirected by Ben Coughlan, Del Rey filmed a second video for the song, titled \\"Video Games (Live At The Premises)\\".[26] Uploaded to her official VEVO account on October 18, 2011, the song features Del Rey in jean shorts and a white T-shirt, singing into a microphone barefoot, while accompanied by a pianist. The Huffington Post wrote that the video was \\"loaded\\", indicating that the title was a contradiction and not, in fact, a live performance whatsoever.[27] Additionally, they wrote: \\"we understand the takedowns of her hype (there is a lot of hype), but that doesn't change the fact that \\"Video Games\\"the tune Lana's singing live this timeis one helluva song.\\"[27] Aside from criticizing her pillowy lips and long fingernails, The Guardian applauded Del Rey for amping up on the coquettish glances and hushed vocals as she devastatingly delivers the line, \\"I heard that you like the bad girls, honey, is that true?\\"[9]\\r\\n\\"Video Games\\" was performed by Del Rey in a number of live appearances, including for MTV Push,[28] and at the Bowery Ballroom, where, according to Eliot Glazer of New York, \\"the polarizing indie hipstress brought her 'gangsta Nancy Sinatra' swag\\".[29] Matthew Perpetua of Rolling Stone commented that, despite Del Rey being nervous and anxious while performing the song live, the singer \\"sang with considerable confidence, though her transitions from husky, come-hither sexuality to bratty, girlish petulance could be rather jarring\\".[30] Del Rey also performed the song on Dutch television program De Wereld Draait Door,[31] on British music television show Later... with Jools Holland,[32] and in an intimate show at Chateau Marmont in Los Angeles, California.[33] On January 14, 2012, Del Rey performed the song on Saturday Night Live (along with \\"Blue Jeans\\"). Her performance soon came under scrutiny and was even criticized by NBC anchor Brian Williams. Williams referred to the performance as \\"one of the worst outings in SNL history\\".[34] Actor Daniel Radcliffe, who hosted the show the night Del Rey performed, quickly came to her defense, stating that criticism toward her was less about the performance and more about \\"her past and her family\\".[35] On February 2, 2012, Del Rey performed the song on The Late Show With David Letterman where according to Rolling Stone she gave a \\"much more controlled and dramatic performance than on SNL.\\"[36] Del Rey performed Video Games live at ECHO Awards in Berlin, March 22, 2012.\\r\\n\\"Video Games\\" has been covered by several artists, including Boy George, TYP, John Mayer (on guitar), Bristeil (in the Belarusian language), Ben Howard, Maverick Sabre, Tyler Hilton, Amanda Palmer, Patrick Stickles of Titus Andronicus, Patrick Wolf, Tom Odell[37] and English rock bands Bombay Bicycle Club and Kasabian.[38][39][40][41] The song was featured in the Ringer \\"If You Ever Want a French Lesson...\\" episode from its first season.[42] It was also featured in the sixth episode of the fifth season of Gossip Girl. A remix of the song was featured in \\"The Dead Don't Stay Dead,\\" an episode of 666 Park Avenue. On February 25, 2012, Masha covered \\"Video Games\\" on her popular YouTube channel.[43]\\r\\nA choral a cappella version of \\"Video Games\\" was used as part of the Royal Shakespeare Company's 2017 production of Othello in the Sam Wanamaker Playhouse, within Shakespeare's Globe, London. Used in place of Desdemona's \\"Weeping Willow\\" song, it was performed by live choir at various points in the production in an Elizabethan style, framing key moments of the play, as well as being used as the opening overture.\\r\\n*sales figures based on certification alone\\r\\n^shipments figures based on certification alone\\r\\nsales+streaming figures based on certification alone","input":"What is lana del rey video games song about?"},{"output":"56ÿ70 days","context":"Vicryl (polyglactin 910) is an absorbable, synthetic, usually braided suture, manufactured by Ethicon Inc., a subsidiary of Johnson and Johnson. A monofilament version is also made for use in ophthalmic practice. It is indicated for soft tissue approximation and ligation. The suture holds its tensile strength for approximately two to three weeks in tissue, and is completely absorbed by hydrolysis within 56ÿ70 days.[1] Vicryl and other polyglycolic-acid sutures may also be treated for more rapid breakdown (\\"Vicryl Rapide\\") in rapidly healing tissues such as mucous membrane, or impregnated with triclosan (\\"Vicryl Plus Antibacterial\\") to provide antimicrobial protection of the suture line. Because vicryl is slow-absorbing and often braided, its use is contraindicated in closure of any cutaneous wound exposed to the air, as it draws moisture from the healing tissue to the skin and allows bacteria and irritants to migrate into the wound. This inevitably leads to high reactivity to the contaminants, poor wound healing, and eventually infection.[2]\\r\\nVicryl is manufactured in Cornelia, GA.\\r\\nAlthough the name \\"Vicryl\\" is a trademark of Ethicon, the term \\"vicryl\\" has been used generically referring to any synthetic absorbable suture made primarily of polyglycolic acid. Other brands of polyglycolic acid suture include PolySyn, Surgicryl, Polysorb and Dexon, all of which are manufactured by different companies. Vicryl is a copolymer of lactide (a cyclic diester of lactic acid) and glycoside. In practice, vicryl comes as braided, dyed or undyed with the following decay schedule: 75% at 2 weeks, 50% at 3 weeks, and 25% at 4 weeks (i.e., the sutures retain that proportion of tensile strength at those dates).","input":"How long does it take for vicryl sutures to dissolve?"},{"output":"June 12, 1981","context":"Raiders of the Lost Ark (also known as Indiana Jones and the Raiders of the Lost Ark) is a 1981 American action adventure film directed by Steven Spielberg, with a screenplay written by Lawrence Kasdan, from a story by George Lucas and Philip Kaufman. It was produced by Frank Marshall for Lucasfilm Ltd., with Lucas and Howard Kazanjian as executive producers. Starring Harrison Ford, it was the first installment in the Indiana Jones film franchise to be released, though it is the second in internal chronological order. It pits Indiana Jones (Ford) against a group of Nazis who are searching for the Ark of the Covenant, which Adolf Hitler believes will make his army invincible. The film co-stars Karen Allen as Indiana's former lover, Marion Ravenwood; Paul Freeman as Indiana's rival, French archaeologist Ren Belloq; John Rhys-Davies as Indiana's sidekick, Sallah; Ronald Lacey as Gestapo agent Arnold Toht; and Denholm Elliott as Indiana's colleague, Marcus Brody.\\r\\nThe film originated from Lucas' desire to create a modern version of the serials of the 1930s and 1940s. Production was based at Elstree Studios, England; but filming also took place in La Rochelle, France, Tunisia, Hawaii, and California from June to September 1980.\\r\\nReleased on June 12, 1981, Raiders of the Lost Ark became the year's top-grossing film and remains one of the highest-grossing films ever made. It was nominated for eight Academy Awards in 1982, including Best Picture, and won four for Best Art Direction, Film Editing, Sound, and Visual Effects with a fifth Academy Award: a Special Achievement Award for Sound Effects Editing. The film's critical and popular success led to three additional films, Indiana Jones and the Temple of Doom (1984), Indiana Jones and the Last Crusade (1989), and Indiana Jones and the Kingdom of the Crystal Skull (2008), with a fifth slated for 2020; the television series The Young Indiana Jones Chronicles (1992ÿ1996), and 15 video games as of 2009.\\r\\nConsidered one of the greatest films ever made,[3][4][5][6] the film also ranked #7 on Empire's list of the 100 greatest movies of all time.[7] In 1999, the film was included in the U.S. Library of Congress' National Film Registry as having been deemed \\"culturally, historically, or aesthetically significant.\\"\\r\\n\\r\\n\\r\\nIn 1936, archaeologist Indiana Jones braves an ancient booby-trapped temple in Peru and retrieves a golden idol. He is confronted by rival archaeologist Ren Belloq and the indigenous Hovito people. Surrounded and outnumbered, Jones surrenders the idol to Belloq and escapes aboard a waiting floatplane.\\r\\nJones returns to his teaching position at Marshall College, a fictitious New England college, where he is interviewed by two Army Intelligence agents. They inform Jones that the Nazis are searching for his old mentor, Abner Ravenwood, under whom Jones studied at the University of Chicago. The Nazis know that Ravenwood is the leading expert on the ancient city of Tanis in Egypt, and that he possesses the headpiece of the Staff of Ra. Jones deduces that the Nazis are searching for the Ark of the Covenant? the Nazis believe that if they acquire the Ark, their armies will become invincible. The Staff of Ra is the key to finding the Well of Souls, a secret chamber in which the Ark is buried.\\r\\nThe agents authorize Jones to recover the Ark to prevent the Nazis from obtaining it. He travels to Nepal and discovers that Ravenwood has died, and the headpiece is in the possession of Ravenwood's daughter Marion. Jones visits Marion at her tavern, where she reveals her bitter feelings toward him from a previous romantic affair. She physically rebuffs his offer to buy the headpiece, and Jones leaves. Shortly after, a group of thugs arrive with their Nazi commander Arnold Toht. Toht threatens Marion to get the headpiece but Jones returns to the bar and fights the Nazis to save Marion. During the fight the bar is set ablaze and the headpiece lands in the flames. Toht severely burns his hand trying to take the hot headpiece and flees the tavern screaming. Jones and Marion escape with the headpiece, and Marion decides to accompany Jones in his search for the Ark so he can repay his debt to her.\\r\\nThe pair travels to Cairo, where they meet up with Jones' friend and skilled digger Sallah. Sallah informs them that Belloq and the Nazis are digging for the Well of Souls with a replica of the headpiece (created from the scar on Toht's hand). They quickly realize the Nazi headpiece is incomplete and that the Nazis are digging in the wrong place. The Nazis kidnap Marion and it appears to Jones that she is killed in an exploding truck. After a confrontation with Belloq in a local bar, Jones and Sallah infiltrate the Nazi dig site and use their staff to correctly locate the Ark. Jones discovers Marion is alive when he finds her bound and gagged in a tent. He refuses to release her out of fear of alerting the Nazis. Jones, Sallah, and a small group of diggers unearth the Well of Souls and acquire the Ark. Belloq and Nazi officer Colonel Dietrich arrive and seize the Ark from Jones, throwing Marion into the Well of Souls with him before sealing it back up. Jones and Marion escape to a local airstrip, where Jones has a fistfight with a Nazi mechanic and destroys the flying wing that was to transport the Ark to Berlin. The panicked Nazis remove the Ark in a truck and set off for Cairo, but Jones catches them and retakes it. He makes arrangements to take the Ark to London aboard a tramp steamer.\\r\\nThe next day, a Nazi U-boat appears and intercepts the ship. Belloq and Dietrich seize the Ark and Marion but cannot locate Jones, who stows away aboard the U-boat and travels with them to an island in the Aegean Sea. Once there, Belloq plans to test the power of the Ark before presenting it to Hitler. Jones reveals himself and threatens to destroy the Ark with a panzerfaust, but Belloq calls his bluff and Jones surrenders rather than destroy such an important historical artifact. The Nazis take Jones and Marion to an area where the Ark will be opened and tie them to a post to observe. Belloq performs a ceremonial opening of the Ark and finds it contains nothing but sand. Suddenly, angelic ghost-like beings emerge from the Ark and Jones cautions Marion to keep her eyes closed and not to look no matter what happens. Belloq and the others look on in astonishment as the apparitions are suddenly revealed to be angels of death. A vortex of flame forms above the Ark and shoots bolts of fiery energy into the gathered Nazi soldiers, killing them all. As Belloq, Toht and Dietrich all scream in terror the Ark turns its fury on them: Dietrich's head shrivels up, Toht's face is melted off his skull and Belloq's head explodes. Flames then engulf the remains of the doomed assembly, save for Jones and Marion, and the pillar of fire rises into the sky. The Ark's lid is blasted high into the air before dropping back down onto the Ark and sealing it. Jones and Marion find their ropes burned off and embrace.\\r\\nIn Washington, D.C., the Army Intelligence agents inform Jones and Marcus Brody that the Ark is someplace safe and will be studied by \\"top men\\". The Ark is shown being stored in a giant government warehouse among countless similar crates.\\r\\nProducer Frank Marshall played a pilot in the airplane fight sequence. The stunt team was ill, so he took the role instead. The result was three days in a hot cockpit, which he joked was over \\"140 degrees\\".[8] Pat Roach plays the Nazi mechanic with whom Jones brawls in this sequence, as well as a massive sherpa who battles Jones in Marion's bar. He had the rare opportunity to be killed twice in one film.[19] Special-effects supervisor Dennis Muren made a cameo as a Nazi spy on the seaplane Jones takes from San Francisco to Manila.[20]\\r\\nIn 1973, George Lucas wrote The Adventures of Indiana Smith.[21] Like Star Wars, which he also wrote, it was an opportunity to create a modern version of the film serials of the 1930s and 1940s.[8] Lucas discussed the concept with Philip Kaufman, who worked with him for several weeks and came up with the Ark of the Covenant as the plot device.[22] Kaufman was told about the Ark by his dentist when he was a child.[23] The project stalled when Clint Eastwood hired Kaufman to direct The Outlaw Josey Wales.[22] Lucas shelved the idea, deciding to concentrate on his outer space adventure that would become Star Wars. In late May 1977, Lucas was in Hawaii, trying to escape the enormous success of Star Wars. Friend and colleague Steven Spielberg was also there, on vacation from work on Close Encounters of the Third Kind. While building a sand castle at the Mauna Kea Beach Hotel,[24] Spielberg expressed an interest in directing a James Bond film. Lucas convinced his friend Spielberg that he had conceived a character \\"better than James Bond\\" and explained the concept of Raiders of the Lost Ark. Spielberg loved it, calling it \\"a James Bond film without the hardware\\",[25] although he told Lucas that the surname 'Smith' was not right for the character. Lucas replied, \\"OK. What about 'Jones'?\\" Indiana was the name of Lucas' Alaskan Malamute, whose habit of riding in the passenger seat as Lucas drove was also the inspiration for Star Wars' Chewbacca.[8] Spielberg was at first reluctant to sign on, as Lucas had told him that he would want Spielberg for an entire trilogy, and Spielberg did not want to work on two more scripts. Lucas told him, however, that he already had the next two movies written, so Spielberg agreed. But when the time came for the first sequel, it was revealed that Lucas had nothing written for either sequel.[8]\\r\\nThe following year, Lucas focused on developing Raiders and the Star Wars sequel The Empire Strikes Back, during which Lawrence Kasdan and Frank Marshall joined the project as screenwriter and producer respectively. Between January 23ÿ27, 1978, for nine hours a day, Lucas, Kasdan, and Spielberg discussed the story and visual ideas. Spielberg came up with Jones being chased by a boulder,[8] which was inspired by Carl Barks' Uncle Scrooge comic \\"The Seven Cities of Cibola\\". Lucas later acknowledged that the idea for the idol mechanism in the opening scene and deadly traps later in the film were inspired by several Uncle Scrooge comics.[26] Lucas came up with a submarine, a monkey giving the Hitler salute, and Marion punching Jones in Nepal.[25] Kasdan used a 100-page transcript of their conversations for his first script draft,[27] which he worked on for six months.[8] Ultimately, some of their ideas were too grand and had to be cut: a mine chase,[28] an escape in Shanghai using a rolling gong as a shield,[29] and a jump from an airplane in a raft, all of which made it into the prequel, Indiana Jones and the Temple of Doom.[8]\\r\\nSpielberg and Lucas disagreed on the character: although Spielberg saw him as a Bondian playboy, Lucas felt the character's academic and adventurer elements made him complex enough.[30] Spielberg had a darker vision of Jones, interpreting him as an alcoholic similar to Humphrey Bogart's character Fred C. Dobbs in The Treasure of the Sierra Madre (1948). This characterization fell away during the later drafts, though elements survive in Jones's reaction when he believes Marion to be dead.[25] Costume designer Deborah Nadoolman credits Secret of the Incas (1954), starring Charlton Heston, as an influence on the development of the character, noting that the crew watched the film together several times. Nadoolman based the look of Ford's costume on that of Heston's, and observed that Indiana is a \\"kinder and gentler\\" Harry Steele.[31]\\r\\nInitially, the film was rejected by every major studio in Hollywood, mostly due to the $20 million budget and the deal Lucas was offering.[32] Eventually Paramount agreed to finance the film, with Lucas negotiating a five-picture deal. By April 1980, Kasdan's fifth draft was produced, and production was getting ready to shoot at Elstree Studios, with Lucas trying to keep costs down.[11] With four illustrators, Raiders of the Lost Ark was Spielberg's most storyboarded film of his career to date, further helping the film economically. He and Lucas agreed on a tight schedule to keep costs down and to follow the \\"quick and dirty\\" feel of the old Saturday matine serials. Special effects were done using puppets, miniature models, animation, and camera trickery.[8] \\"We didn't do 30 or 40 takes; usually only four. It was like silent film--shoot only what you need, no waste\\", Spielberg said. \\"Had I had more time and money, it would have turned out a pretentious movie.\\"[33]\\r\\nPrincipal photography began on June 23, 1980, at La Rochelle, France, with scenes involving the Nazi submarine,[11] which had been rented from the production of Das Boot. The U-boat pen was a real one from World War II.[8] The crew moved to Elstree Studios[11] for the Well of Souls scenes, the opening sequence temple interiors and Marion Ravenwood's bar.[34] The Well of Souls scene required 7,000 snakes. The only venomous snakes were the cobras, but one crew member was bitten on set by a python.[8] The bulk of the snakes' numbers were made up with giant but harmless legless lizards known as Scheltopusiks (Pseudopus apodus) which occur from the Balkan Peninsula of southeastern Europe to Central Asia. Growing to 1.3 m they are the largest legless lizards in the world and are often mistaken for snakes despite some very obvious differences such as the presence of eyelids and external ear openings, which are both absent from all snakes, and a notched rather than forked tongue. In the finished film, during the scene in which Indiana comes face-to-face with the cobra, a reflection in glass screen that protected Ford from the snake was seen,[8] an issue that was corrected in the 2003 digitally-enhanced re-release. Unlike Indiana, neither Ford nor Spielberg has a fear of snakes, but Spielberg said that seeing all the snakes on the set writhing around made him \\"want to puke\\".[8]\\r\\nThe opening scene in the Peruvian jungle was filmed on the island of Kauai, one of the islands of Hawaii, to which Spielberg would return for Jurassic Park. The first shot features a misty view of Kalalea Mountain, the culminating point in Anahola, Hawaii. The \\"temple\\" location is on the Huleia River, on the Kipu Ranch, south from Kaumualii Highway on the east coast, just south of Lihue, the island's main town. Kipu is a working cattle ranch, not generally open to the public.[35] The Peruvian section (but actually filmed in Hawaii) featured live tarantulas of a Mexican species (Brachypelma) on Harrison Ford and Alfred Molina, and are harmless to humans, and in fact of a species which are commonly kept as exotic pets. A fiberglass boulder 22?feet (7?m) in diameter was made for the scene where Indiana escapes from the temple; Spielberg was so impressed by production designer Norman Reynolds' realization of his idea that he gave the boulder a more prominent role in the film and told Reynolds to let the boulder roll another 50 feet (15?m).[36]\\r\\nThe scenes set in Egypt were filmed in Tunisia, and the canyon where Indiana threatens to blow up the Ark was shot in Sidi Bouhlel, just outside Tozeur.[37] The canyon location had been used for the Tatooine scenes from 1977's Star Wars (many of the location crew members were the same for both films[8]) where R2-D2 was attacked by Jawas.[8] The Tanis scenes were filmed in nearby Sedala, a harsh place due to heat and disease. Several cast and crew members fell ill and Rhys-Davies defecated in his costume during one shot.[8] Spielberg averted disease by eating only canned foods from England, but did not like the area and quickly condensed the scheduled six-week shoot to four-and-a-half weeks. Much was improvised: the scene where Marion puts on her dress and attempts to leave Belloq's tent was improvised as was the entire plane fight. During that scene's shooting, a wheel went over Ford's knee and tore his left leg's cruciate ligament, but he refused local medical help and simply put ice on it.[8]\\r\\nThe fight scenes in the town were filmed in Kairouan, while Ford was suffering from dysentery. Stuntman Terry Richards had practiced for weeks with his sword to create the scripted fight scene, choreographing a fight between the swordsman and Jones' whip.[38] However, after filming the initial shots of the scene, after lunch due to Ford's dysentery, Ford and Spielberg agreed to cut the scene down to a gunshot, with Ford saying to Spielberg \\"Let's just shoot the sucker\\".[39] It was later voted in at No.5 on Playboy magazine's list of best all time scenes.[38][40] Most of the truck chase was shot by second unit director Michael D. Moore following Spielberg's storyboards, including Indiana being dragged by the truck (performed by stuntman Terry Leonard), in tribute to a famous Yakima Canutt stunt. Spielberg then filmed all the shots with Ford himself in and around the truck cab.[8] Lucas directed a few other second unit shots, in particular the monkey giving the Nazi salute.[33]\\r\\nThe interior staircase set in Washington, D.C. was filmed in San Francisco's City Hall. The University of the Pacific's campus in Stockton, California, stood in for the exterior of the college where Jones works, while his classroom and the hall where he meets the American intelligence agents was filmed at the Royal Masonic School for Girls in Rickmansworth, Hertfordshire, England, which was again used in The Last Crusade. His home exteriors were filmed in San Rafael, California.[34] Opening sequence exteriors were filmed in Kauai, Hawaii, with Spielberg wrapping in September in 73?days, finishing under schedule in contrast to his previous film, 1941.[11][25] The Washington, D.C. coda, although it appeared in the script's early drafts, was not included in early edits but was added later when it was realized that there was no resolution to Jones' relationship with Marion.[41] Shots of the Douglas DC-3 Jones flies on to Nepal were taken from Lost Horizon, and a street scene was from a shot in The Hindenburg.[33] Filming of Jones boarding a Boeing Clipper flying-boat was complicated by the lack of a surviving aircraft. Eventually, a post-war British Short Solent flying-boat formerly owned by Howard Hughes was located in California and substituted.[42]\\r\\nThe special visual effects for Raiders were provided by Industrial Light & Magic and include: a matte shot to establish the Pan Am flying boat in the water[43] and miniature work to show the plane taking off and flying, superimposed over a map; animation effects for the beam in the Tanis map room; and a miniature car and passengers[44] superimposed over a matte painting for a shot of a Nazi car being forced off a cliff. The bulk of effects shots were featured in the climactic sequence wherein the Ark of the Covenant (which was designed by Brian Muir and Keith Short) is opened and God's wrath is unleashed. This sequence featured animation, a woman to portray a beautiful spirit's face, rod puppet spirits moved through water to convey a sense of floating,[45] a matte painting of the island, and cloud tank effects to portray clouds. The melting of Toht's head was done by exposing a gelatine and plaster model of Ronald Lacey's head to a heat lamp with an under-cranked camera, while Dietrich's crushed head was a hollow model from which air was withdrawn. When the film was originally submitted to the Motion Picture Association of America, it received an R rating because of the scene in which Belloq's head explodes. The filmmakers were able to receive a PG rating when they added a veil of fire over the exploding head scene. (PG-13 rating was not created until 1984.[20]) The firestorm that cleanses the canyon at the finish was a miniature canyon filmed upside down.[45]\\r\\nBen Burtt, the sound effects supervisor, made extensive use of traditional foley work in yet another of the production's throwbacks to days of the Republic serials. He selected a .30-30 Winchester rifle for the sound of Jones' pistol. Sound effects artists struck leather jackets and baseball gloves with a baseball bat to create a variety of punching noises and body blows. For the snakes in the Well of Souls sequence, fingers running through cheese casserole and sponges sliding over concrete were used for the slithering noises. The sliding lid on a toilet cistern provided the sound for the opening of the Ark, and the sound of the boulder in the opening is a car rolling down a gravel driveway in neutral. Burtt also used, as he did in many of his films, the ubiquitous Wilhelm scream when a Nazi falls from a truck. In addition to his use of such time-honored foley work, Burtt also demonstrated the modern expertise honed during his award-winning work on Star Wars. He employed a synthesizer for the sounds of the Ark, and mixed dolphins' and sea lions' screams for those of the spirits within.[46]\\r\\nThe final scene, where the Ark is placed in a vast warehouse, was Spielberg's homage to Citizen Kane, where Kane's lost childhood sled, \\"Rosebud\\", is similarly treated.[47]\\r\\nJohn Williams composed the score for Raiders of the Lost Ark, which was the only score in the series performed by the London Symphony Orchestra, the same orchestra that performed the scores for the Star Wars saga. The score most notably features the well-known \\"Raiders March\\". This piece came to symbolize Indiana Jones and was later used in the scores for the other three films. Williams originally wrote two different candidates for Jones's theme, but Spielberg enjoyed them so much that he insisted that both be used together in what became the \\"Raiders March\\".[48] The alternately eerie and apocalyptic theme for the Ark of the Covenant is also heard frequently in the score, with a more romantic melody representing Marion and, more broadly, her relationship with Jones. The score as a whole received an Oscar nomination for Best Original Score, but lost to the score to Chariots of Fire composed by Vangelis.\\r\\nThe only video game based exclusively on the film is Raiders of the Lost Ark, released in 1982 by Atari for their Atari 2600 console.[49] The first third of the video game Indiana Jones' Greatest Adventures, released in 1994 by JVC for Nintendo's Super Nintendo Entertainment System, is based entirely on the film. Several of the film's sequences are reproduced (the boulder run and the showdown with the Cairo Swordsman among them); however, several inconsistencies with the film are present in the game, such as Nazi soldiers and bats being present in the Well of Souls sequence, for example.[50] The game was developed by LucasArts and Factor 5. In the 1999 game Indiana Jones and the Infernal Machine, a bonus level brings Jones back to the Peruvian temple of the film's opening scene.[51] In 2008, to coincide with the release of Kingdom of the Crystal Skull, Lego released the Lego Indiana Jones linewhich included building sets based on Raiders of the Lost Ark[52]and LucasArts published a video game based on the toyline, Lego Indiana Jones: The Original Adventures, which was developed by Traveller's Tales.[53]\\r\\nMarvel Comics published a comic book adaptation of the film by writer Walt Simonson and artists John Buscema and Klaus Janson. It was published as Marvel Super Special #18[54] and as a three-issue limited series.[55] This was followed with the comic book series The Further Adventures of Indiana Jones which was published monthly from January 1983 through March 1986.\\r\\nIn 1981, Kenner released a 12-inch (30?cm) doll of Indiana Jones, and the following year they released nine action figures of the film's characters, three playsets, as well as toys of the Nazi truck and Jones' horse. They also released a board game. In 1984, miniature metal versions of the characters were released for a role playing game, The Adventures of Indiana Jones, and in 1995 Micro Machines released die-cast toys of the film's vehicles.[56] Hasbro released action figures based on the film, ranging from 3 to 12 inches (7.6 to 30.5?cm), to coincide with Kingdom of the Crystal Skull on May 1, 2008.[57] Later in 2008, and in 2011, two high-end sixth scale (1:6) collectible action figures were released by Sideshow Collectibles, and Hot Toys, Ltd. respectively. A novelization by Ryder Windham was released in April 2008 by Scholastic to tie in with the release of Kingdom of the Crystal Skull. A previous novelization by Scottish author Campbell Armstrong (under the pseudonym Campbell Black) was concurrently released with the film in 1981. A book about the making of the film was also released, written by Derek Taylor.\\r\\nThe film was released on VHS, Betamax and VideoDisc in pan and scan only, and on laserdisc in both pan and scan and widescreen. For its 1999 VHS re-issue, the film was remastered in THX and made available in widescreen. The outer package was retitled Indiana Jones and the Raiders of the Lost Ark for consistency with the film's prequel and sequel. The subsequent DVD release in 2003 features this title as well. The title in the film itself remains unchanged, even in the restored DVD print. In the DVD, two subtle digital revisions were added. First, a connecting rod from the giant boulder to an offscreen guidance track in the opening scene was removed from behind the running Harrison Ford; second, a reflection in the glass partition separating Ford from the cobra in the Well of Souls was removed.[58] Shortly before the theatrical release of Kingdom of the Crystal Skull, Raiders (along with The Temple of Doom and The Last Crusade) was re-released on DVD with additional extra features not included on the previous set on May 13, 2008. The film was released on Blu-ray Disc in September 2012.[59] Previously, only Kingdom of the Crystal Skull had been available on Blu-ray.[citation needed]\\r\\nRaiders of the Lost Ark opened at #14 and grossed $1,673,731 from 267 theaters ($6,269 theater average) during its opening weekend. In total, the IMAX release grossed $3,125,613 domestically.[60] The film, made on an $18 million budget, grossed $384 million worldwide throughout its theatrical releases. In North America, it was by some distance the highest-grossing film of 1981,[61] and remains one of the top twenty-five highest-grossing films ever made when adjusted for inflation.[62] Box Office Mojo estimates that the film sold more than 70 million tickets in the US in its initial theatrical run.[63]\\r\\nThe film was subsequently nominated for nine Academy Awards, including Best Picture, in 1982 and won four (Best Sound, Best Film Editing, Best Visual Effects, and Best Art Direction-Set Decoration (Norman Reynolds, Leslie Dilley, and Michael D. Ford). It also received a Special Achievement Award for Sound Effects Editing. It won numerous other awards, including a Grammy Award and Best Picture at the People's Choice Awards. Spielberg was also nominated for a Golden Globe Award.[64]\\r\\nThe film was highly acclaimed by critics and audiences alike. In his review for The New York Times, Vincent Canby praised the film, calling it, \\"one of the most deliriously funny, ingenious and stylish American adventure movies ever made.\\"[65] Roger Ebert in his review for the Chicago Sun-Times wrote, \\"Two things, however, make Raiders of the Lost Ark more than just a technological triumph: its sense of humor and the droll style of its characters [...] We find ourselves laughing in surprise, in relief, in incredulity at the movie's ability to pile one incident upon another in an inexhaustible series of inventions.\\"[66] He later added it to his list of \\"Great Movies\\".[67] Rolling Stone said the film was \\"the ultimate Saturday action matineeÿa film so funny and exciting it can be enjoyed any day of the week.\\"[68] Bruce Williamson of Playboy claimed: \\"There's more excitement in the first ten minutes of Raiders than any movie I have seen all year. By the time the explosive misadventures end, any movie-goer worth his salt ought to be exhausted.\\"[69] Stephen Klain of Variety also praised the film. Yet, making an observation that would revisit the franchise with its next film, he felt that the film was surprisingly violent and bloody for a PG-rated film.[70]\\r\\nThere were some dissenting voices: Sight & Sound described it as an \\"expensively gift-wrapped Saturday afternoon pot-boiler\\",[71] and New Hollywood champion Pauline Kael, who once contended that she only got \\"really rough\\" on large films that were destined to be hits but were nonetheless \\"atrocious\\",[72] found the film to be a \\"machine-tooled adventure\\" from a pair of creators who \\"think just like the marketing division\\".[73] (Lucas later named a villain, played by Raiders Nazi strongman Pat Roach, in his 1988 fantasy film Willow after Kael.)[72] On Rotten Tomatoes, the film has a 94% \\"Certified Fresh\\" rating with a 9.2/10 average rating. The sites critical consensus stating: \\"Featuring bravura set pieces, sly humor, and white-knuckle action,?Raiders of the Lost Ark?is one of the most consummately entertaining adventure pictures of all time.\\",[74] as well as a 85% rating on Metacritic, indicating \\"universal acclaim\\".[75]\\r\\nFollowing the success of Raiders, a prequel, The Temple of Doom, and two sequels, The Last Crusade and Kingdom of the Crystal Skull, were produced, with a third sequel set for release in 2020.[79] A television series, entitled The Young Indiana Jones Chronicles, was also spun off from this film, and details the character's early years. Numerous other books, comics, and video games have also been produced.\\r\\nIn 1998, the American Film Institute placed the film at number 60 on its top 100 films of the first century of cinema. In 2007, AFI updated the list and placed it at number 66. They also named it as the 10th most thrilling film, and named Indiana Jones as the second greatest hero. In 1999, the film was deemed \\"culturally, historically, or aesthetically significant\\" by the U.S. Library of Congress and selected for preservation in the National Film Registry. Indiana Jones has become an icon, being listed as Entertainment Weekly's third favorite action hero, while noting \\"some of the greatest action scenes ever filmed are strung together like pearls\\" in this film.[80]\\r\\nAn amateur, near shot-for-shot remake was made by Chris Strompolos, Eric Zala, and Jayson Lamb, then children in Ocean Springs, Mississippi. It took the boys seven years to finish, from 1982 to 1989. After production of the film, called Raiders of the Lost Ark: The Adaptation, it was shelved and forgotten until 2003, when it was discovered by Eli Roth[81][82] and acclaimed by Spielberg himself, who congratulated the boys on their hard work and said he looked forward to seeing their names on the big screen.[83] Scott Rudin and Paramount Pictures purchased the trio's life rights with the goal of producing a film based on their adventures making their remake.[84][85]\\r\\nIn 2014, film director Steven Soderbergh published an experimental black-and-white version of the film, with the original soundtrack and dialogue replaced by an electronic soundtrack. Soderbergh said his intention was to encourage viewers to focus on Spielberg's extraordinary staging and editing: \\"This filmmaker forgot more about staging by the time he made his first feature than I know to this day.\\"[86]\\r\\nAssessing the film's legacy in 1997, Bernard Weinraub, film critic for The New York Times, which had initially reviewed the film as \\"deliriously funny, ingenious, and stylish\\",[87] maintained that \\"the decline in the traditional family G-rated film, for 'general' audiences, probably began\\" with the appearance of Raiders of the Lost Ark. \\"Whether by accident or design,\\" found Weinraub, \\"the filmmakers made a comic nonstop action film intended mostly for adults but also for children.\\"[88] Eight years later, in 2005, viewers of Channel 4 rated the film as the 20th-best family film of all time, with Spielberg taking best over-all director honors.[89]\\r\\nOn Empire magazine's list of the 500 Greatest Movies of All Time, Raiders ranked second, beaten only by The Godfather.[90] The film was ranked at number 11 in a list of the 25 best action and war films of all time by The Guardian.[91][92][93]\\r\\nIn conjunction with the Blu-ray release, a limited one-week release in IMAX theaters was announced for September 7, 2012. Steven Spielberg and sound designer Ben Burtt supervised the format conversion. No special effects or other visual elements were altered, but the audio was enhanced for surround sound.[94]\\r\\nIn December 2012, the University of Chicago's admissions department received a package in the mail addressed to Henry Walton Jones, Jr., Indiana Jones' full name. The address on the stamped package was listed for a hall that was the former home of the university's geology and geography department. Inside the manila envelope was a detailed replica journal similar to the one Jones used in the movie, as well as postcards and pictures of Marion Ravenwood. The admissions department posted pictures of the contents on its Internet blog, looking for any information about the package. It was discovered that the package was part of a set to be shipped from Guam to Italy that had been sold on eBay. The package with the journal had fallen out in transit and a postal worker had sent it to the university, as it had a complete address and postage, which turned out to be fake. All contents were from a Guam \\"prop replicator\\" who sells them all over the world. The university will display its replica in the main lobby of the Oriental Institute.[95]","input":"When did raiders of the lost ark premiere?"},{"output":"Deuteronomy 14:24ÿ25","context":"A tithe (/ta?e/; from Old English: teogota \\"tenth\\") is a one-tenth part of something, paid as a contribution to a religious organization or compulsory tax to government.[1] Today, tithes are normally voluntary and paid in cash, cheques, or stocks, whereas historically tithes were required and paid in kind, such as agricultural products. Several European countries operate a formal process linked to the tax system allowing some churches to assess tithes.\\r\\nTraditional Jewish law and practice has included various forms of tithing since ancient times. Orthodox Jews commonly practice ma'aser kesafim (tithing 10% of their income to charity). In modern Israel, Jews continue to follow the laws of agricultural tithing, e.g., ma'aser rishon, terumat ma'aser, and ma'aser sheni.\\r\\nWith respect to Christianity, Jesus Christ taught the Jews that \\"tithing must be done in conjunction with a deep concern for justice, mercy and faithfulness (Matthew 23:23)\\".[2] Tithing was taught at early Christian ecumenical councils, including the Council of Tours in 567, as well as the Synod of Macon in 585, and remains an important doctrine in many Christian denominations, such as the Congregationalist Churches and Methodist Churches.[2]\\r\\n\\r\\n\\r\\nNone of the extant extrabiblical laws of the Ancient Near East deal with tithing, although other secondary documents show that it was a widespread practice in the Ancient Near East.[3] William W. Hallo (1996[4]) recognises comparisons for Israel with its ancient Near Eastern environment, however, as regards tithes, comparisons with other ancient Near Eastern evidence is ambiguous,[5] and Ancient Near Eastern literature provides scant evidence for the practice of tithing and the collection of tithes.[6]\\r\\nListed below are some specific instances of the Mesopotamian tithe, taken from The Assyrian Dictionary of the Oriental Institute of the University of Chicago, Vol. 4 \\"E\\" p. 369:[7]\\r\\nHebrew is a Semitic language, related to Akkadian, the lingua franca of that time.[8]\\r\\nIn Genesis 14:18ÿ20, Abraham, after rescuing Lot, met with Melchizedek. After Melchizedek's blessing, Abraham gave him a tenth of everything he has obtained from battle:\\r\\n\\"Then Melchizedek king of Salem brought out bread and wine. He was priest of God Most High, and he blessed Abram, saying, Blessed be Abram by God Most High, Creator of heaven and earth. And praise be to God Most High, who delivered your enemies into your hand. Then Abram gave him a tenth of everything.\\r\\nIn Genesis 28:12ÿ22, Jacob, after his visionary dream of Jacob's Ladder and receiving a blessing from God, promises God a tenth:\\r\\n\\"Then Jacob awoke from his sleep and said, Surely the Lord is in this place, and I did not know it. And he was afraid and said, How awesome is this place! This is none other than the house of God, and this is the gate of heaven. So early in the morning Jacob took the stone that he had put under his head and set it up for a pillar and poured oil on the top of it. He called the name of that place Bethel, but the name of the city was Luz at the first. Then Jacob made a vow, saying, If God will be with me and will keep me in this way that I go, and will give me bread to eat and clothing to wear, so that I come again to my father's house in peace, then the Lord shall be my God, and this stone, which I have set up for a pillar, shall be God's house. And of all that you give me I will give a full tenth to you.\\r\\nThe tithe is specifically mentioned in the Books of Leviticus, Numbers and Deuteronomy. The tithe system was organized in a three-year cycle, corresponding to the Shemittah-cycle. These tithes were in reality more like taxes for the people of Israel and were mandatory, not optional giving. This tithe was distributed locally \\"within thy gates\\" (Deuteronomy 14:28) to support the Levites and assist the poor.\\r\\nEvery year, Bikkurim, Terumah, Ma'aser Rishon and Terumat Ma'aser were separated from the grain, wine and oil (Deuteronomy 14:22). (As regards other fruit and produce, the Biblical requirement to tithe is a source of debate.) The first tithe is giving of one tenth of agricultural produce (after the giving of the standard terumah) to the Levite (or Aaronic priests). Historically, during the First Temple period, the first tithe was given to the Levites. Approximately at the beginning of the Second Temple construction, Ezra and his Beth din implemented its giving to the kohanim.[9][10]\\r\\nUnlike other offerings which were restricted to consumption within the tabernacle, the second tithe could be consumed anywhere . On years one, two, four and five of the Shemittah-cycle, God commanded the Children of Israel to take a second tithe that was to be brought to the place of the Temple (Deuteronomy 14:23). The owner of the produce was to separate and bring 1/10 of his finished produce to the Old City of Jerusalem after separating Terumah and the first tithe, but if the family lived too far from Jerusalem, the tithe could be redeemed upon coins (Deuteronomy 14:24ÿ25). Then, the Bible required the owner of the redeemed coins to spend the tithe \\"to buy whatever you like: cattle, sheep, wine or other fermented drink, or anything you wish\\" (Deuteronomy 14:26). Implicit in the commandment was an obligation to spend the coins on items meant for human consumption.\\r\\nIn years three and six of the Shemittah-cycle the Israelites set aside the (second) tithe instead as the poor tithe, and it was given to the strangers, orphans, and widows.\\r\\nThe Levites, also known as the Tribe of Levi, were descendants of Levi. They were assistants to the Aaronic priests (who were the children of Aaron and, therefore, a subset of the Tribe of Levi) and did not own or inherit a territorial patrimony (Numbers 18:21-28). Their function in society was that of temple functionaries, teachers and trusted civil servants who supervised the weights and scales and witnessed agreements. The goods donated from the other Israeli tribes were their source of sustenance. They received from \\"all Israel\\" a tithe of food or livestock for support, and in turn would set aside a tenth portion of that tithe (known as the Terumat hamaaser) for the Aaronic priests.\\r\\nAn additional tithe mentioned in the Book of Leviticus (27:32ÿ33) is the cattle tithe, which is to be sacrificed as a korban at the Temple in Jerusalem.\\r\\nLMLK seals may represent the oldest archaeological evidence of tithing. About 10 percent of the storage jars manufactured during Hezekiah's reign (circa 700 BC) were stamped (Grena, 2004, pp.?376ÿ78). See 2 Chronicles 29ÿ31 for a record of this early worship reformation.\\r\\nThe Book of Nehemiah also talks about the collection of tithes to Leviim and distribution of Terumah to the priests: Nehemiah 13:5. People were actually appointed to collect mandatory tithes and place them in specially designated chambers which eventually came to be known as storehouses: Nehemiah 12:44.\\r\\nThe Book of Malachi has one of the most quoted Biblical passages about tithing, directed to the sons of Jacob:\\r\\nFor I, Jehovah, change not; therefore ye, O sons of Jacob, are not consumed. From the days of your fathers ye have turned aside from mine ordinances, and have not kept them. Return unto me, and I will return unto you, saith Jehovah of hosts. But ye say, Wherein shall we return? \\"Will man rob God? Yet you are robbing me. But you say, How have we robbed you? In your tithes and contributions. You are cursed with a curse, for you are robbing me, the whole nation of you. Bring the full tithe into the storehouse, that there may be food in my house. And thereby put me to the test, says the Lord of hosts, if I will not open the windows of heaven for you and pour down for you a blessing until there is no more need. I will rebuke the devourer for you, so that it will not destroy the fruits of your soil, and your vine in the field shall not fail to bear, says the Lord of hosts. Then all nations will call you blessed, for you will be a land of delight, says the Lord of hosts.\\"\\r\\nThe deuterocanonical Book of Tobit provides an example of all three classes of tithes practiced during the Babylonian captivity:\\r\\n\\"I would often go by myself to Jerusalem on religious holidays, as the Law commanded for every Israelite for all time. I would hurry off to Jerusalem and take with me the early produce of my crops, a tenth of my flocks, and the first portion of the wool cut from my sheep. I would present these things at the altar to the priests, the descendants of Aaron. I would give the first tenth of my grain, wine, olive oil, pomegranates, figs, and other fruit to the Levites who served in Jerusalem. For six out of seven years, I also brought the cash equivalent of the second tenth of these crops to Jerusalem where I would spend it every year. I gave this to orphans and widows, and to Gentiles who had joined Israel. In the third year, when I brought and gave it to them, we would eat together according to the instruction recorded in Moses Law, as Deborah my grandmother had taught me...\\"\\r\\nOrthodox Jews continue to follow the laws of Terumah and Ma'aser as well as the custom of tithing 10% of one's earnings to charity (ma'aser kesafim). Due to doubts concerning the status of persons claiming to be Kohanim or Levi'im arising after severe Roman/Christian persecutions and exile, the Hebrew Bible tithe of 10% for the Levites, and \\"tithe of the tithe\\" (Numbers 18:26) of 10% of 10% (1%) for the priests are dealt with in accordance with Jewish Law.[clarification needed] The Mishnah and Talmud contain analysis of the first tithe, second tithe and poor tithe.[11]\\r\\nAnimals are not tithed in the present era when the Temple is not standing.[12]\\r\\nJesus Christ taught that \\"tithing must be done in conjunction with a deep concern for justice, mercy and faithfulness (Matthew 23:23)\\".[2] Many of the ancient and historic Christian Churches, such as the Catholic Church and the Methodist Churches, practice tithing, as it was taught by the Council of Tours in A.D. 567, and in the Council of Macon in A.D. 581, a penalty of excommunication was prescribed for those who did not adhere to this ecclesiastical law.[13] Tithes can be given to the Church at once (as is the custom in many Christian countries with a Church tax), or distributed throughout the year; during the part of Western Christian liturgies known as the offertory, people often place a portion of their tithes (sometimes along with additional offerings) in the collection plate.[14]\\r\\n2 Corinthians 9:7 talks about giving cheerfully, 2 Corinthians 8:12 encourages giving what you can afford, 1 Corinthians 16:1ÿ2 discusses giving weekly (although this is a saved amount for Jerusalem), 1 Timothy 5:17ÿ18 exhorts supporting the financial needs of Christian workers, Acts 11:29 promotes feeding the hungry wherever they may be and James 1:27 states that pure religion is to help widows and orphans.[2]\\r\\nThe Seventh-day Adventist Church teaches in its Fundamental Beliefs that \\"We acknowledge God's ownership by faithful service to Him and our fellow men, and by returning tithes and giving offerings for the proclamation of His gospel and the support of His Church.\\"[2]\\r\\nThe Mennonite Church teaches that \\"tithing as a minimum baseline is one of the principles on which financial giving in this 'first fruits' system is based\\":[2]\\r\\nWe depend on God's gracious gifts for food and clothing, for our salvation, and for life itself. We do not need to hold on tightly to money and possessions, but can share what God has given us. The practice of mutual aid is a part of sharing God's gifts so that no one in the family of faith will be without the necessities of life. Whether through community of goods or other forms of financial sharing, mutual aid continues the practice of Israel in giving special care to widows, orphans, aliens, and others in economic need (Deut. 24:17-22). Tithes and firstfruit offerings were also a part of this economic sharing (Deut. 26; compare Matt. 23:23).[2]\\r\\nThe National Baptist Convention of America teaches that \\"Baptists believe that a proper sense of stewardship begins with the 'tithe'; a presentation of which belongs to Him. 'The tithe is the Lord's.' We have not given as a result of presenting the tithe. Our giving begins with the offering {after we have tithed}.\\"[2]\\r\\nOne of the Commandments of the Church is to to pay tithes to their pastors.[15] The Council of Trent, which was held after the Reformation, taught that tithes are due to God or to religion, and that it is sacrilegious to withold them.[15]\\r\\nThe Church of Jesus Christ of Latter-day Saints (LDS Church) bases its tithing on the following additional scriptures:[16]\\r\\nAnd this shall be the beginning of the tithing of my people. And after that, those who have thus been tithed shall pay one-tenth of all their interest annually; and this shall be a standing law unto them forever, for my holy priesthood, saith the Lord.\\r\\nAnd it was this same Melchizedek to whom Abraham paid tithes; yea, even our father Abraham paid tithes of one-tenth part of all he possessed.\\r\\nTithing is currently defined by the church as payment to the church of one-tenth of one's annual income. Many LDS leaders have made statements in support of tithing.[17] Every Latter-day Saint receives an opportunity once a year to meet with their bishop about their tithing settlement. The payment of tithes is mandatory for members to receive the priesthood or obtain admission to temples.\\r\\nNone of the funds collected from tithing is paid to church officials. The Church of Jesus Christ of Latter-day Saints is a Lay Ministry.[18] The money that is given is used to build and maintain church buildings as well as to further the work of the church.[19] Brigham Young University, a practically all-LDS school, also receives \\"a significant portion\\" of its maintenance and operational costs from LDS tithes.\\r\\nThe Lutheran ChurchÿMissouri Synod teaches that \\"Encourage[s] cheerful, first-fruit, proportionate (including but not limited to tithing) living and giving in all areas of life by Christian stewards\\".[2]\\r\\nThe Book of Discipline states that it is the responsibility of ecclesiastics to educate the local church that tithing is the minimum goal of giving in The United Methodist Church.[2]\\r\\nThe Church of the Nazarene teaches to pay a tithe, although not necessarily the one-tenth under Old Testament law. People pay according to their ability.[20]\\r\\nThe Moravian Church encourages its members to \\"financially support the ministry of the Church toward the goal of tithing.\\"[2] It \\"deem[s] it a sacred responsibility and genuine opportunity to be faithful stewards of all God has entrusted to us: our time, our talents, [and] our financial resources\\".[2]\\r\\nTithing in medieval Eastern Christianity did not spread so widely as in the West. A Constitution of the Emperors Leo I (reigned 457ÿ474) and Anthemius (reigned 467ÿ472) apparently expected believers to make voluntary payments and forbade compulsion.[21]\\r\\nThe Greek Orthodox Archdiocese of America teaches \\"proportionate giving and tithing as normal practices of Christian giving.\\"[2]\\r\\nThe Pentecostal Church of God teaches that \\"We recognize the scriptural duty of all our people, as well as ministers, to pay tithes as unto the Lord. Tithes should be used for the support of active ministry and for the propagation of the Gospel and the work of the Lord in general.\\"[2]\\r\\nThe International Pentecostal Holiness Church likewise instructs the faithful that:[2]\\r\\nOur commitment to Jesus Christ includes stewardship. According to the Bible everything belongs to God. We are stewards of His resources. Our stewardship of possessions begins with the tithe. All our members are expected to return a tenth of all their income to the Lord.[2]\\r\\nThe Book of Order of the Presbyterian Church (USA) states, with respect to the obligation to tithe:[22]\\r\\nGiving has always been a mark of Christian commitment and discipleship. The ways in which a believer uses Gods gifts of material goods, personal abilities, and time should reflect a faithful response to Gods self-giving in Jesus Christ and Christs call to minister to and share with others in the world. Tithing is a primary expression of the Christian discipline of stewardship.[22]\\r\\nThe United Church of Christ, a denomination in the Congregationalist tradition, teaches that:[2]\\r\\nWhen we tithe we place God as our first priority. We trust in God's abundance instead of worrying about not having enough. Tithing churches live out a vision of abundance rather than a mentality of scarcity.[2]\\r\\nThe right to receive tithes was granted to the English churches by King Ethelwulf in 855. The Saladin tithe was a royal tax, but assessed using ecclesiastical boundaries, in 1188. The legal validity of the tithe system was affirmed under the Statute of Westminster of 1285. The Dissolution of the Monasteries led to the transfer of many rights to tithe to secular landowners and the Crown ÿ and tithes could be extinguished until 1577 under an Act of the 37th year of Henry VIII's reign.[23] Adam Smith criticized the system in The Wealth of Nations (1776), arguing that a fixed rent would encourage peasants to farm more efficiently.\\r\\nSee below for a fuller description and history, until the reforms of the 19th century, written by Sir William Blackstone and edited by other learned lawyers of the period.\\r\\nThe system gradually ended with the Tithe Commutation Act 1836, whose long-lasting Tithe Commission replaced them with a commutation payment, land award and/or rentcharges to those paying the commutation payment and took the opportunity to map out (apportion) residual chancel liability where the rectory had been appropriated during the medieval period by a religious house or college. Its records give a snapshot of land ownership in most parishes, the Tithe Files, are a socio-economic history resource. The rolled-up payment of several years' tithe would be divided between the tithe-owners as at the date of their extinction.[24]\\r\\nThis commutation reduced problems to the ultimate payers by effectively folding tithes in with rents however, it could cause transitional money supply problems by raising the transaction demand for money. Later the decline of large landowners led tenants to become freeholders and again have to pay directly; this also led to renewed objections of principle by non-Anglicans. It also kept intact a system of chancel repair liability affecting the minority of parishes where the rectory had been lay-appropriated. The precise land affected in such places hinged on the content of documents such as the content of deeds of merger and apportionment maps.[24]\\r\\nRentcharges in lieu of abolished tithe paid by landowners were converted by a public outlay of money under the Tithe Act 1936 into annuities paid to the state through the Tithe Redemption Commission. Such payments were transferred in 1960 to the Board of Inland Revenue, and those remaining were terminated by the Finance Act 1977.\\r\\nThe Tithe Act 1951 established the compulsory redemption of English tithes by landowners where the annual amounts payable were less than S1 so abolishing the bureaucracy and costs of collecting small sums of money.[citation needed]\\r\\nReceipted 1955 redemption notice for property in East Dundry, just south of Bristol\\r\\nRegistered letter cover\\r\\nRelevant tithe map\\r\\nIn France, the tithes ÿ called \\"la d?me\\" ÿ were a land tax. Originally a voluntary tax, the \\"d?me\\" became mandatory in 1585. In principle, unlike the taille, the \\"d?me\\" was levied on both noble and non-noble lands. The d?me was divided into a number of types, including the \\"grosses d?mes\\" (grains, wine, hay), \\"menues\\" or \\"vertes d?mes\\" (vegetables, poultry), \\"d?mes de charnage\\" (veal, lamb, pork). Although the term \\"d?me\\" comes from the Latin decima [pars] (\\"one tenth\\", with the same origin as that of the U.S. coin, the dime), the \\"d?me\\" rarely reached this percentage and (on the whole) it was closer to 1/13th of the agricultural production.\\r\\nThe \\"d?me\\" was originally meant to support the local parish, but by the 16th century many \\"d?mes\\" went directly to distant abbeys, monasteries, and bishops, leaving the local parish impoverished, and this contributed to general resentment. In the Middle Ages, some monasteries also offered the \\"d?me\\" in homage to local lords in exchange for their protection (see Feudalism) (these are called \\"d?mes infodes\\"), but this practice was forbidden by the Lateran Council of 1179.\\r\\nAll religious taxes were constitutionally abolished in 1790, in the wake of the French Revolution.\\r\\nThere has never been a separate church tax or mandatory tithe on Greek citizens. The state pays the salaries of the clergy of the established Church of Greece, in return for use of real estate, mainly forestry, owned by the church. The remainder of church income comes from voluntary, tax-deductible donations from the faithful. These are handled by each diocese independently.[citation needed]\\r\\nFrom the English Reformation in the 16th century, most Irish people chose to remain Roman Catholic and had by now to pay tithes valued at about 10% of an area's agricultural produce, to maintain and fund the established state church, the Anglican Church of Ireland, to which only a small minority of the population converted. Irish Presbyterians and other minorities like the Quakers and Jews were in the same situation.\\r\\nThe collection of tithes was resisted in the period 1831ÿ36, known as the Tithe War. Thereafter, tithes were reduced and added to rents with the passing of the Tithe Commutation Act in 1836. With the disestablishment of the Church of Ireland by the Irish Church Act 1869, tithes were abolished.\\r\\nWhile the federal government has never collected a church tax or mandatory tithe on its citizens, states collected a tithe into the early 19th century. The United States and its governmental subdivisions also exempt most churches from payment of income tax (under Section 501(c)(3) of the Internal Revenue Code and similar state statutes, which also allows donors to claim the donations as an income tax itemized deduction). Also, churches may be permitted exemption from other state and local taxes such as sales and property taxes, either in whole or in part. Clergy, such as ministers and members of religious orders (who have taken a vow of poverty) may be exempt from federal self-employment tax on income from ministerial services. Income from non-ministerial services are taxable and churches are required to withhold Federal and state income tax from this non-exempt income. They are also required to withhold employee's share of Social Security and Medicare taxes under FICA, and pay the employer's share for the non-exempt income.[25]\\r\\nBoth the tithe (diezmo), a levy of 10% on all agricultural production, and \\"first fruits\\" (primicias), an additional harvest levy, were collected in Spain throughout the medieval and early modern periods for the support of local Catholic parishes.\\r\\nThe tithe crossed the Atlantic with the Spanish Empire; however, the Indians who made up the vast majority of the population in colonial Spanish America were exempted from paying tithes on native crops such as corn and potatoes that they raised for their own subsistence. After some debate, Indians in colonial Spanish America were forced to pay tithes on their production of European agricultural products, including wheat, silk, cows, pigs, and sheep.\\r\\nThe tithe was abolished in several Latin American countries, including Mexico, soon after independence from Spain (which started in 1810). The tithe was abolished in Spain itself in 1841, and in Argentina in 1826.\\r\\nIn Austria a colloquially called church tax (Kirchensteuer, officially called Kirchenbeitrag, i.e. church contribution) has to be paid by members of the Catholic and Protestant Church[which?]. It is levied by the churches themselves and not by the government. The obligation to pay church tax can just be evaded by an official declaration to cease church membership. The tax is calculated on the basis of personal income. It amounts to about 1.1% (Catholic church) and 1.5% (Protestant church).[citation needed]\\r\\nAll members of the Church of Denmark pay a church tax, which varies between municipalities.[26] The tax is generally around 1% of the taxable income.[citation needed]\\r\\nMembers of state churches pay a church tax of between 1% and 2% of income, depending on the municipality. In addition, 2.55% of corporate taxes are distributed to the state churches. Church taxes are integrated into the common national taxation system.[27]\\r\\nGermany levies a church tax, on all persons declaring themselves to be Christians, of roughly 8ÿ9% of their income tax, which is effectively (very much depending on the social and financial situation) typically between 0.2% and 1.5% of the total income. The proceeds are shared amongst Catholic, Lutheran, and other Protestant Churches.[28]\\r\\nThe church tax (Kirchensteuer) actually traces its roots back as far as the Reichsdeputationshauptschluss of 1803. It was reaffirmed in the Concordat of 1933 between Nazi Germany and the Catholic Church. Today its legal basis is article 140 of the Grundgesetz (the German \\"constitution\\") in connection with article 137 of the Weimar constitution. These laws originally merely allowed the churches themselves to tax their members, but in Nazi Germany, collection of church taxes was transferred to the German government. As a result, both the German government and the employer are notified of the religious affiliation of every taxpayer. This system is still in effect today. Mandatory disclosure of religious affiliation to government agencies or employers constituted a violation of the original European data protection directives but is now permitted after the German government obtained an exemption.[28]\\r\\nChurch tax (Kirchensteuer) is compulsory in Germany for those confessing members of a particular religious group. It is deducted at the PAYE level. The duty to pay this tax theoretically starts on the day one is christened. Anyone who wants to stop paying it has to declare in writing, at their local court of law (Amtsgericht) or registry office, that they are leaving the Church. They are then crossed off the Church registers and can no longer receive the sacraments, confession and certain services; Roman Catholic church may deny such as person a burial plot.[28] In addition to the government, the taxpayer also must notify his employer of his religious affiliation (or lack thereof) in order to ensure proper tax withholding.[29]\\r\\nThis opt-out is also used by members of \\"free churches\\" (e.g. Baptists) (non-affiliated to the scheme) to stop paying the church tax, from which the free churches do not benefit, in order to support their own church directly.\\r\\nOriginally the Italian government of Benito Mussolini, under the Lateran treaties of 1929 with the Holy See, paid a monthly salary to Catholic clergymen. This salary was called the congrua. The eight per thousand law was created as a result of an agreement, in 1984, between the Italian Republic and the Holy See.\\r\\nUnder this law Italian taxpayers are able to vote how to partition the 0.8% ('eight per thousand') of the total income tax IRPEF levied by Italy among some specific religious confessions or, alternatively, to a social assistance program run by the Italian State. This declaration is made on the IRPEF form. This vote is not compulsory; the whole amount levied by the IRPEF tax is distributed in proportion to explicit declarations.\\r\\nThe last official statement of Italian Ministry of Finance made in respect of the year 2000 singles out seven beneficiaries: the Italian State, the Catholic Church, the Waldenses, the Jewish Communities, the Lutherans, the Seventh-day Adventist Church and the Assemblies of God in Italy.\\r\\nThe tax was divided up as follows:\\r\\nIn 2000, the Catholic Church raised almost a billion euros, while the Italian State received about ?100 million euros.\\r\\nIn Scotland teinds were the tenths of certain produce of the land appropriated to the maintenance of the Church and clergy. At the Reformation most of the Church property was acquired by the Crown, nobles and landowners. In 1567 the Privy Council of Scotland provided that a third of the revenues of lands should be applied to paying the clergy of the reformed Church of Scotland. In 1925 the system was recast by statute[30] and provision was made for the standardisation of stipends at a fixed value in money. The Court of Session acted as the Teind Court. Teinds were finally abolished by section 56 of the Abolition of Feudal Tenure etc. (Scotland) Act 2000.\\r\\nUntil the year 2000, Sweden had a mandatory church tax, to be paid if one did belong to the Church of Sweden, which had been funneling about $500 million annually to the church. Because of change in legislation, the tax was withdrawn in the year 2000. However, the Swedish government has agreed to continue collecting from individual taxpayers the annual payment that has always gone to the church. But now the tax will be an optional checkoff box on the tax return. The government will allocate the money collected to Catholic, Muslim, Jewish and other faiths as well as the Lutherans, with each taxpayer directing where his or her taxes should go.[citation needed]\\r\\nThere is no official state church in Switzerland; however, all the 26 cantons (states) financially support at least one of the three traditional denominationsRoman Catholic, Old Catholic, or Protestantwith funds collected through taxation. Each canton has its own regulations regarding the relationship between church and state. In some cantons, the church tax (up to 2.3%) is voluntary but in others an individual who chooses not to contribute to church tax may formally have to leave the church. In some cantons private companies are unable to avoid payment of the church tax.[citation needed]\\r\\nExcerpts from Sir William Blackstone, Commentaries on the Laws of England:\\r\\n. . . tithes; which are defined to be the tenth part of the increase, yearly arising and renewing from the profits of lands, the stock upon lands, and the personal industry of the inhabitants:\\r\\n...\\r\\nWe cannot precisely ascertain the time when tithes were first introduced into this country. Possibly they were contemporary with the planting of Christianity among the Saxons, by Augustin the monk, about the end of the fifth century. But the first mention of them, which I have met with in any written English law, is in a constitutional decree, made in a synod held A.D. 786, wherein the payment of tithes in general is strongly enjoined. This canon, or decree, which at first bound not the laity, was effectually confirmed by two kingdoms of the heptarchy, in their parliamentary conventions of estates, respectively consisting of the kings of Mercia and Northumberland, the bishops, dukes, senators, and people. Which was a few years later than the time that Charlemagne established the payment of them in France, and made that famous division of them into four parts?; one to maintain the edifice of the church, the second to support the poor, the third the bishop, and the fourth the parochial clergy.[32]:25\\r\\nAnd upon their first introduction (as hath formerly been observed), though every man was obliged to pay tithes in general, yet he might give them to what priests he pleased; which were called arbitrary consecrations of tithes: or he might pay them into the hands of the bishop, who distributed among his diocesan clergy the revenues of the church, which were then in common. But, when dioceses were divided into parishes, the tithes of each parish were allotted to its own particular minister; first by common consent, or the appointment of lords of manors, and afterwards by the written law of the land.[32]:26 ... it is now universally held, that tithes are due, of common right, to the parson of the parish, unless there be a special exemption. This parson of the parish, we have formerly seen, may be either the actual incumbent, or else the appropriator of the benefice: appropriations being a method of endowing monasteries, which seems to have been devised by the regular clergy, by way of substitution to arbitrary consecrations of tithes.[32]:28\\r\\nWe observed that tithes are due to the parson of common right, unless by special exemption: let us therefore see, thirdly, who may be exempted from the payment of tithes ... either in part or totally, first, by a real composition; or secondly, by custom or prescription.\\r\\nFirst, a real composition is when an agreement is made between the owner of the lands, and the parson or vicar, with the consent of the ordinary and the patron, that such lands shall for the future be discharged from payment of tithes, by reason of some land or other real recompence given to the parson, in lieu and satisfaction thereof. ...\\r\\nSecondly, a discharge by custom or prescription, is where time out of mind such persons or such lands have been, either partially or totally, discharged from the payment of tithes. And this immemorial usage is binding upon all parties, as it is in its nature an evidence of universal consent and acquiescence; and with reason supposes a real composition to have been formerly made. This custom or prescription is either de modo decimandi, or de non decimando.\\r\\nA modus decimandi, commonly called by the simple name of a modus only, is where there is by custom a particular manner of tithing allowed, different from the general law of taking tithes in kind, which are the actual tenth part of the annual increase. This is sometimes a pecuniary compensation, as twopence an acre for the tithe of land?: sometimes it is a compensation in work and labour, as that the parson shall have only the twelfth cock of hay, and not the tenth, in consideration of the owner's making it for him: sometimes, in lieu of a large quantity of crude or imperfect tithe, the parson shall have a less quantity, when arrived to greater maturity, as a couple of fowls in lieu of tithe eggs?; and the like. Any means, in short, whereby the general law of tithing is altered, and a new method of taking them is introduced, is called a modus decimandi, or special manner of tithing.[32]:28ÿ29 ...\\r\\nA prescription de non decimando is a claim to be entirely discharged of tithes, and to pay no compensation in lieu of them. Thus the king by his prerogative is discharged from all tithes. So a vicar shall pay no tithes to the rector, nor the rector to the vicar, for ecclesia decimas non folvit ecclesiae. But these personal to both the king and the clergy?; for their tenant or lessee shall pay tithes of the same land, though in their own occupation it is not tithable. And, generally speaking, it is an established rule, that in lay hands, modus de non decimando non valet. But spiritual persons or corporations, as monasteries, abbots, bishops, and the like, were always capable of having their lands totally discharged of tithes, by various ways: as\\r\\nThe Tithe Barn, Abbotsbury, Dorset (scene of the sheep-shearing in Thomas Hardy's Far from the Madding Crowd)\\r\\nTithe Barn at Bradford on Avon, West Wiltshire\\r\\nInterior of the medieval tithe barn at Pilton, Somerset\\r\\nCoggeshall near Braintree Essex, the timber has been dated to between 1130 and 1270\\r\\nZakt (Arabic: ????? [z?k??h]) or \\"alms giving\\", one of the Five Pillars of Islam, is the giving of a small percentage of one's assets to charity. It serves principally as the welfare contribution to poor and deprived Muslims, although others may have a rightful share. It is the duty of an Islamic state not just to collect zakat but to distribute it fairly as well.\\r\\nZakat is payable on three kinds of assets: wealth, production, and animals. The more well-known zakat on wealth is 2.5% of accumulated wealth, beyond one's personal needs. Production (agricultural, industrial, renting, etc.), is subject to a 10% or 5% zakat (also known as Ushur (????), or \\"one-tenth\\"), using the rule that if both labor and capital are involved, 5% rate is applied, if only one of the two are used for production, then the rate is 10%. For any earnings, that require neither labor nor capital, like finding underground treasure, the rate is 20%. The rules for zakat on animal holdings are specified by the type of animal group and tend to be fairly detailed.[33]\\r\\nMuslims fulfill this religious obligation by giving a fixed percentage of their surplus wealth. Zakat has been paired with such a high sense of righteousness that it is often placed on the same level of importance as performing the five-daily repetitive ritualised prayer (salat).[34] Muslims see this process also as a way of purifying themselves from their greed and selfishness and also safeguarding future business.[34] In addition, Zakat purifies the person who receives it because it saves him from the humiliation of begging and prevents him from envying the rich.[35] Because it holds such a high level of importance the \\"punishment\\" for not paying when able is very severe. In the 2nd edition of the Encyclopaedia of Islam it states, \\"...the prayers of those who do not pay zakat will not be accepted\\".[34] This is because without Zakat a tremendous hardship is placed on the poor which otherwise would not be there. Besides the fear of their prayers not getting heard, those who are able should be practicing this third pillar of Islam because the Quran states that this is what believers should do.[36]\\r\\nNon-Muslims (able-bodied adult males of military age) living in an Islamic state are required to pay Jizya, this exempts them from military service and they do not pay Zakat.\\r\\nDaswandh (Punjabi: ?????), sometimes spelled Dasvandh, is the one tenth part (or 10%) of one's income that should be donated in the name of the God, according to Sikh principles.[37][38]","input":"Where was tithe first recorded in the bible?"},{"output":"2,403","context":"Coordinates: 2122N 15757W? / ?21.367N 157.950W? / 21.367; -157.950\\r\\nMajor Japanese tactical victory; precipitated the entrance of the United States into World War II\\r\\nSoutheast Asia\\r\\nSouthwest Pacific\\r\\nNorth America\\r\\nJapan\\r\\nManchuria\\r\\nThe attack on Pearl Harbor was a surprise military strike by the Imperial Japanese Navy Air Service against the United States naval base at Pearl Harbor, Hawaii Territory, on the morning of December 7, 1941. The attack, also known as the Battle of Pearl Harbor,[9] led to the United States' entry into World War II. The Japanese military leadership referred to the attack as the Hawaii Operation and Operation AI,[10][11] and as Operation Z during its planning.[12]\\r\\nJapan intended the attack as a preventive action to keep the U.S. Pacific Fleet from interfering with military actions they planned in Southeast Asia against overseas territories of the United Kingdom, the Netherlands, and the United States. Over the next seven hours there were coordinated Japanese attacks on the U.S.-held Philippines, Guam and Wake Island and on the British Empire in Malaya, Singapore, and Hong Kong.[13]\\r\\nThe attack commenced at 7:48?a.m. Hawaiian Time (18:18 UTC).[14] The base was attacked by 353[15] Imperial Japanese aircraft (including fighters, level and dive bombers, and torpedo bombers) in two waves, launched from six aircraft carriers.[15] All eight U.S. Navy battleships were damaged, with four sunk. All but the USS?Arizona were later raised, and six were returned to service and went on to fight in the war. The Japanese also sank or damaged three cruisers, three destroyers, an anti-aircraft training ship,[nb 4] and one minelayer. 188 U.S. aircraft were destroyed; 2,403 Americans were killed and 1,178 others were wounded.[17] Important base installations such as the power station, dry dock, shipyard, maintenance, and fuel and torpedo storage facilities, as well as the submarine piers and headquarters building (also home of the intelligence section), were not attacked. Japanese losses were light: 29 aircraft and five midget submarines lost, and 64 servicemen killed. One Japanese sailor, Kazuo Sakamaki, was captured.\\r\\nThe surprise attack came as a profound shock to the American people and led directly to the American entry into World War II in both the Pacific and European theaters. The following day, December 8, the United States declared war on Japan,[18][19] and several days later, on December 11, Germany and Italy declared war on the U.S. The U.S. responded with a declaration of war against Germany and Italy. Domestic support for non-interventionism, which had been fading since the Fall of France in 1940,[20] disappeared.[19]\\r\\nThere were numerous historical precedents for unannounced military action by Japan, but the lack of any formal warning, particularly while negotiations were still apparently ongoing, led President Franklin D. Roosevelt to proclaim December 7, 1941, \\"a date which will live in infamy\\". Because the attack happened without a declaration of war and without explicit warning, the attack on Pearl Harbor was later judged in the Tokyo Trials to be a war crime.[21][22]\\r\\nWar between Japan and the United States had been a possibility that each nation had been aware of (and developed contingency plans for) since the 1920s, though tensions did not begin to grow seriously until Japan's 1931 invasion of Manchuria. Over the next decade, Japan continued to expand into China, leading to all-out war between those countries in 1937. Japan spent considerable effort trying to isolate China and achieve sufficient resource independence to attain victory on the mainland; the \\"Southern Operation\\" was designed to assist these efforts.[23]\\r\\nFrom December 1937, events such as the Japanese attack on USS Panay, the Allison incident, and the Nanking Massacre (the International Military Tribunal of the Far East concluded that more than 200,000 Chinese non-combatants were killed in indiscriminate massacres, though other estimates have ranged from 40,000 to more than 300,000) swung public opinion in the West sharply against Japan. Fearing Japanese expansion,[24] the United States, the United Kingdom, and France provided loan assistance for war supply contracts to China.\\r\\nIn 1940, Japan invaded French Indochina in an effort to control supplies reaching China. The United States halted shipments of airplanes, parts, machine tools, and aviation gasoline to Japan, which was perceived by Japan as an unfriendly act.[nb 5] The U.S. did not stop oil exports to Japan at that time in part because prevailing sentiment in Washington was that such an action would be an extreme step that Japan would likely consider a provocation, given Japanese dependence on U.S. oil.[26][27]\\r\\nEarly in 1941, President Franklin D. Roosevelt moved the Pacific Fleet to Hawaii from its previous base in San Diego and ordered a military buildup in the Philippines in the hope of discouraging Japanese aggression in the Far East. Because the Japanese high command was (mistakenly) certain that any attack on the UK's Southeast Asian colonies, including Singapore,[28] would bring the U.S. into war, a devastating preventive strike appeared to be the only way to avoid U.S. naval interference.[29] An invasion of the Philippines was also considered necessary by Japanese war planners. The U.S. War Plan Orange had envisioned defending the Philippines with a 40,000-man elite force. This was opposed by Douglas MacArthur, who felt that he would need a force ten times that size, and was never implemented.[30] By 1941, U.S. planners anticipated abandonment of the Philippines at the outbreak of war and orders to that effect were given in late 1941 to Admiral Thomas Hart, commander of the Asiatic Fleet.[31]\\r\\nThe U.S. ceased oil exports to Japan in July 1941, following Japanese expansion into French Indochina after the Fall of France, in part because of new American restrictions on domestic oil consumption.[32] This in turn caused the Japanese to proceed with plans to take the Dutch East Indies, an oil-rich territory.[nb 6] On August 17, Roosevelt warned Japan that the U.S. was prepared to take steps against Japan if it attacked \\"neighboring countries\\".[34] The Japanese were faced with the option of either withdrawing from China and losing face or seizing and securing new sources of raw materials in the resource-rich, European-controlled colonies of Southeast Asia.\\r\\nJapan and the U.S. engaged in negotiations during the course of 1941 in an effort to improve relations. During these negotiations, Japan offered to withdraw from most of China and Indochina when peace was made with the Nationalist government, adopt an independent interpretation of the Tripartite Pact, and not to discriminate in trade provided all other countries reciprocated. Washington rejected these proposals. Japanese Prime Minister Konoye then offered to meet with Roosevelt, but Roosevelt insisted on coming to an agreement before any meeting.[35] The U.S. ambassador to Japan repeatedly urged Roosevelt to accept the meeting, warning that it was the only way to preserve the conciliatory Konoye government and peace in the Pacific.[36] His recommendation was not acted upon. The Konoye government collapsed the following month when the Japanese military refused to agree to the withdrawal of all troops from China.[37]\\r\\nJapan's final proposal, on November 20, offered to withdraw their forces from southern Indochina and not to launch any attacks in Southeast Asia provided that the U.S., the UK, and the Netherlands ceased aiding China and lifted their sanctions against Japan.[37] The American counter-proposal of November 26 (November 27 in Japan) (the Hull note) required Japan to evacuate all of China without conditions and conclude non-aggression pacts with Pacific powers. However the day before the Hull Note was delivered, on November 26 in Japan, the main Japanese attack fleet left port for Pearl Harbor.\\r\\nPreliminary planning for an attack on Pearl Harbor to protect the move into the \\"Southern Resource Area\\" (the Japanese term for the Dutch East Indies and Southeast Asia generally) had begun very early in 1941 under the auspices of Admiral Isoroku Yamamoto, then commanding Japan's Combined Fleet.[38] He won assent to formal planning and training for an attack from the Imperial Japanese Navy General Staff only after much contention with Naval Headquarters, including a threat to resign his command.[39] Full-scale planning was underway by early spring 1941, primarily by Rear Admiral Rynosuke Kusaka, with assistance from Captain Minoru Genda and Yamamoto's Deputy Chief of Staff, Captain Kameto Kuroshima.[40] The planners studied the 1940 British air attack on the Italian fleet at Taranto intensively.[nb 7][nb 8]\\r\\nOver the next several months, pilots were trained, equipment was adapted, and intelligence was collected. Despite these preparations, Emperor Hirohito did not approve the attack plan until November 5, after the third of four Imperial Conferences called to consider the matter.[43] Final authorization was not given by the emperor until December 1, after a majority of Japanese leaders advised him the \\"Hull Note\\" would \\"destroy the fruits of the China incident, endanger Manchukuo and undermine Japanese control of Korea.\\"[44]\\r\\nBy late 1941, many observers believed that hostilities between the U.S. and Japan were imminent. A Gallup poll just before the attack on Pearl Harbor found that 52% of Americans expected war with Japan, 27% did not, and 21% had no opinion.[45] While U.S. Pacific bases and facilities had been placed on alert on many occasions, U.S. officials doubted Pearl Harbor would be the first target; instead, they expected the Philippines would be attacked first. This presumption was due to the threat that the air bases throughout the country and the naval base at Manila posed to sea lanes, as well as to the shipment of supplies to Japan from territory to the south.[46] They also incorrectly believed that Japan was not capable of mounting more than one major naval operation at a time.[47]\\r\\nThe Japanese attack had several major aims. First, it intended to destroy important American fleet units, thereby preventing the Pacific Fleet from interfering with Japanese conquest of the Dutch East Indies and Malaya and to enable Japan to conquer Southeast Asia without interference. Second, it was hoped to buy time for Japan to consolidate its position and increase its naval strength before shipbuilding authorized by the 1940 Vinson-Walsh Act erased any chance of victory.[48][49] Third, to deliver a blow to America's ability to mobilize its forces in the Pacific, battleships were chosen as the main targets, since they were the prestige ships of any navy at the time.[48] Finally, it was hoped that the attack would undermine American morale such that the U.S. government would drop its demands contrary to Japanese interests, and would seek a compromise peace with Japan.[50][51]\\r\\nStriking the Pacific Fleet at anchor in Pearl Harbor carried two distinct disadvantages: the targeted ships would be in very shallow water, so it would be relatively easy to salvage and possibly repair them; and most of the crews would survive the attack, since many would be on shore leave or would be rescued from the harbor. A further important disadvantagethis of timing, and known to the Japanesewas the absence from Pearl Harbor of all three of the U.S. Pacific Fleet's aircraft carriers (Enterprise, Lexington, and Saratoga). IJN top command was so imbued with Admiral Mahan's \\"Decisive battle\\" doctrineespecially that of destroying the maximum number of battleshipsthat, despite these concerns, Yamamoto decided to press ahead.[52][page?needed]\\r\\nJapanese confidence in their ability to achieve a short, victorious war also meant other targets in the harbor, especially the navy yard, oil tank farms, and submarine base, were ignored, sinceby their thinkingthe war would be over before the influence of these facilities would be felt.[53]\\r\\nOn November 26, 1941, a Japanese task force (the Striking Force) of six aircraft carriersAkagi, Kaga, Sry, Hiry, Shkaku, and Zuikakudeparted northern Japan en route to a position northwest of Hawaii, intending to launch its 408 aircraft to attack Pearl Harbor: 360 for the two attack waves and 48 on defensive combat air patrol (CAP), including nine fighters from the first wave.\\r\\nThe first wave was to be the primary attack, while the second wave was to attack carriers as its first objective and cruisers as its second, with battleships as the third target.[54] The first wave carried most of the weapons to attack capital ships, mainly specially adapted Type 91 aerial torpedoes which were designed with an anti-roll mechanism and a rudder extension that let them operate in shallow water.[55] The aircrews were ordered to select the highest value targets (battleships and aircraft carriers) or, if these were not present, any other high value ships (cruisers and destroyers). First wave dive bombers were to attack ground targets. Fighters were ordered to strafe and destroy as many parked aircraft as possible to ensure they did not get into the air to intercept the bombers, especially in the first wave. When the fighters' fuel got low they were to refuel at the aircraft carriers and return to combat. Fighters were to serve CAP duties where needed, especially over U.S. airfields.[citation needed]\\r\\nBefore the attack commenced, two reconnaissance aircraft launched from cruisers Chikuma and Tone were sent to scout over Oahu and Maui and report on U.S. fleet composition and location. Reconnaissance aircraft flights risked alerting the U.S.,[56] and were not necessary. U.S. fleet composition and preparedness information in Pearl Harbor was already known due to the reports of the Japanese spy Takeo Yoshikawa. A report of the absence of the U.S. fleet in Lahaina anchorage off Maui was received from the fleet submarine I-72.[57] Another four scout planes patrolled the area between the Japanese carrier force (the Kid Butai) and Niihau, to detect any counterattack.[58]\\r\\nFleet submarines I-16, I-18, I-20, I-22, and I-24 each embarked a Type A midget submarine for transport to the waters off Oahu.[59] The five I-boats left Kure Naval District on November 25, 1941.[60] On December 6, they came to within 10?nmi (19?km; 12?mi) of the mouth of Pearl Harbor[61] and launched their midget subs at about 01:00[clarification needed] on December 7.[62] At 03:42[63] Hawaiian Time, the minesweeper Condor spotted a midget submarine periscope southwest of the Pearl Harbor entrance buoy and alerted the destroyer Ward.[64] The midget may have entered Pearl Harbor. However, Ward sank another midget submarine at 06:37[64][nb 9] in the first American shots in the Pacific Theater. A midget submarine on the north side of Ford Island missed the seaplane tender Curtiss with her first torpedo and missed the attacking destroyer Monaghan with her other one before being sunk by Monaghan at 08:43.[64]\\r\\nA third midget submarine, Ha-19, grounded twice, once outside the harbor entrance and again on the east side of Oahu, where it was captured on December 8.[66] Ensign Kazuo Sakamaki swam ashore and was captured by Hawaii National Guard Corporal David Akui, becoming the first Japanese prisoner of war.[nb 10] A fourth had been damaged by a depth charge attack and was abandoned by its crew before it could fire its torpedoes.[67] Japanese forces received a radio message from a midget submarine at 00:41 on December 8 claiming damage to one or more large warships inside Pearl Harbor.[68]\\r\\nIn 1992, 2000, and 2001, Hawaii Undersea Research Laboratory's submersibles found the wreck of the fifth midget submarine lying in three parts outside Pearl Harbor. The wreck was in the debris field where much surplus U.S. equipment was dumped after the war, including vehicles and landing craft. Both of its torpedoes were missing. This correlates with reports of two torpedoes fired at the light cruiser St. Louis at 10:04 at the entrance of Pearl Harbor, and a possible torpedo fired at destroyer Helm at 08:21.[69]\\r\\nThe attack took place before any formal declaration of war was made by Japan, but this was not Admiral Yamamoto's intention. He originally stipulated that the attack should not commence until thirty minutes after Japan had informed the United States that peace negotiations were at an end.[citation needed] However, the attack began before the notice could be delivered. Tokyo transmitted the 5000-word notification (commonly called the \\"14-Part Message\\") in two blocks to the Japanese Embassy in Washington. Transcribing the message took too long for the Japanese ambassador to deliver it on schedule; in the event, it was not presented until more than an hour after the attack began. (In fact, U.S. code breakers had already deciphered and translated most of the message hours before he was scheduled to deliver it.)[70] The final part is sometimes described as a declaration of war. While it was viewed by a number of senior U.S government and military officials as a very strong indicator negotiations were likely to be terminated[71] and that war might break out at any moment,[72] it neither declared war nor severed diplomatic relations. A declaration of war was printed on the front page of Japan's newspapers in the evening edition of December 8,[73] but not delivered to the U.S. government until the day after the attack.\\r\\nFor decades, conventional wisdom held that Japan attacked without first formally breaking diplomatic relations only because of accidents and bumbling that delayed the delivery of a document hinting at war to Washington. In 1999, however, Takeo Iguchi, a professor of law and international relations at International Christian University in Tokyo, discovered documents that pointed to a vigorous debate inside the government over how, and indeed whether, to notify Washington of Japan's intention to break off negotiations and start a war, including a December 7 entry in the war diary saying, \\"[O]ur deceptive diplomacy is steadily proceeding toward success.\\" Of this, Iguchi said, \\"The diary shows that the army and navy did not want to give any proper declaration of war, or indeed prior notice even of the termination of negotiations?... and they clearly prevailed.\\"[74][75]\\r\\nIn any event, even if the Japanese had decoded and delivered the 14-Part Message before the beginning of the attack, it would not have constituted either a formal break of diplomatic relations or a declaration of war. The final two paragraphs of the message read:\\r\\nThus the earnest hope of the Japanese Government to adjust Japanese-American relations and to preserve and promote the peace of the Pacific through cooperation with the American Government has finally been lost.\\r\\nThe first attack wave of 183 planes was launched north of Oahu, led by Commander Mitsuo Fuchida.[77] Six planes failed to launch due to technical difficulties.[58] It included:[nb 11]\\r\\nAs the first wave approached Oahu, it was detected by the U.S. Army SCR-270 radar at Opana Point near the island's northern tip. This post had been in training mode for months, but was not yet operational.[80] The operators, Privates George Elliot Jr. and Joseph Lockard, reported a target.[81] But Lieutenant Kermit A. Tyler, a newly assigned officer at the thinly manned Intercept Center, presumed it was the scheduled arrival of six B-17 bombers from California. The Japanese planes were approaching from a direction very close (only a few degrees difference) to the bombers,[82] and while the operators had never seen a formation as large on radar, they neglected to tell Tyler of its size.[83] Tyler, for security reasons, could not tell the operators of the six B-17s that were due (even though it was widely known).[83]\\r\\nAs the first wave planes approached Oahu, they encountered and shot down several U.S. aircraft. At least one of these radioed a somewhat incoherent warning. Other warnings from ships off the harbor entrance were still being processed or awaiting confirmation when the attacking planes began bombing and strafing. Nevertheless, it is not clear any warnings would have had much effect even if they had been interpreted correctly and much more promptly. The results the Japanese achieved in the Philippines were essentially the same as at Pearl Harbor, though MacArthur had almost nine hours warning that the Japanese had already attacked Pearl Harbor.\\r\\nThe air portion of the attack began at 7:48?a.m. Hawaiian Time[14] (3:18?a.m. December 8 Japanese Standard Time, as kept by ships of the Kido Butai),[84][nb 12] with the attack on Kaneohe. A total of 353[15] Japanese planes in two waves reached Oahu. Slow, vulnerable torpedo bombers led the first wave, exploiting the first moments of surprise to attack the most important ships present (the battleships), while dive bombers attacked U.S. air bases across Oahu, starting with Hickam Field, the largest, and Wheeler Field, the main U.S. Army Air Forces fighter base. The 171 planes in the second wave attacked the Army Air Forces' Bellows Field near Kaneohe on the windward side of the island, and Ford Island. The only aerial opposition came from a handful of P-36 Hawks, P-40 Warhawks, and some SBD Dauntless dive bombers from the carrier Enterprise.[nb 13]\\r\\nIn the first wave attack, about eight of the forty-nine 800?kg (1760?lb) armor-piercing bombs dropped hit their intended battleship targets. At least two of those bombs broke up on impact, another detonated before penetrating an unarmored deck, and one was a dud. Thirteen of the forty torpedoes hit battleships, and four torpedoes hit other ships.[85] Men aboard U.S. ships awoke to the sounds of alarms, bombs exploding, and gunfire, prompting bleary-eyed men to dress as they ran to General Quarters stations. (The famous message, \\"Air raid Pearl Harbor. This is not drill.\\",[nb 14] was sent from the headquarters of Patrol Wing Two, the first senior Hawaiian command to respond.) The defenders were very unprepared. Ammunition lockers were locked, aircraft parked wingtip to wingtip in the open to prevent sabotage,[86] guns unmanned (none of the Navy's 5\\"/38s, only a quarter of its machine guns, and only four of 31 Army batteries got in action).[86] Despite this low alert status, many American military personnel responded effectively during the attack.[nb 15] Ensign Joe Taussig Jr., aboard Nevada, commanded the ship's antiaircraft guns and was severely wounded, but continued to be on post. Lt. Commander F. J. Thomas commanded Nevada in the captain's absence and got her under way until the ship was grounded at 9:10?a.m.[87] One of the destroyers, Aylwin, got underway with only four officers aboard, all ensigns, none with more than a year's sea duty; she operated at sea for 36 hours before her commanding officer managed to get back aboard.[88] Captain Mervyn Bennion, commanding West Virginia, led his men until he was cut down by fragments from a bomb which hit Tennessee, moored alongside.\\r\\nThe second planned wave consisted of 171 planes: 54 B5Ns, 81 D3As, and 36 A6Ms, commanded by Lieutenant-Commander Shigekazu Shimazaki.[78] Four planes failed to launch because of technical difficulties.[58] This wave and its targets comprised:[78]\\r\\nThe second wave was divided into three groups. One was tasked to attack Kne?ohe, the rest Pearl Harbor proper. The separate sections arrived at the attack point almost simultaneously from several directions.\\r\\nNinety minutes after it began, the attack was over. 2,008 sailors were killed and 710 others wounded; 218 soldiers and airmen (who were part of the Army until the independent U.S. Air Force was formed in 1947) were killed and 364 wounded; 109 marines were killed and 69 wounded; and 68 civilians were killed and 35 wounded. In total, 2,403 Americans died and 1,178 were wounded.[89] Eighteen ships were sunk or run aground, including five battleships.[4][5] All of the Americans killed or wounded during the attack were non-combatants, given the fact there was no state of war when the attack occurred.[21][22][90]\\r\\nOf the American fatalities, nearly half were due to the explosion of Arizona's forward magazine after it was hit by a modified 16-inch (410?mm) shell.[nb 16]\\r\\nAlready damaged by a torpedo and on fire amidships, Nevada attempted to exit the harbor. She was targeted by many Japanese bombers as she got under way and sustained more hits from 250?lb (113?kg) bombs, which started further fires. She was deliberately beached to avoid blocking the harbor entrance.\\r\\nCalifornia was hit by two bombs and two torpedoes. The crew might have kept her afloat, but were ordered to abandon ship just as they were raising power for the pumps. Burning oil from Arizona and West Virginia drifted down on her, and probably made the situation look worse than it was. The disarmed target ship Utah was holed twice by torpedoes. West Virginia was hit by seven torpedoes, the seventh tearing away her rudder. Oklahoma was hit by four torpedoes, the last two above her belt armor, which caused her to capsize. Maryland was hit by two of the converted 16\\" shells, but neither caused serious damage.\\r\\nAlthough the Japanese concentrated on battleships (the largest vessels present), they did not ignore other targets. The light cruiser Helena was torpedoed, and the concussion from the blast capsized the neighboring minelayer Oglala. Two destroyers in dry dock, Cassin and Downes were destroyed when bombs penetrated their fuel bunkers. The leaking fuel caught fire; flooding the dry dock in an effort to fight fire made the burning oil rise, and both were burned out. Cassin slipped from her keel blocks and rolled against Downes. The light cruiser Raleigh was holed by a torpedo. The light cruiser Honolulu was damaged, but remained in service. The repair vessel Vestal, moored alongside Arizona, was heavily damaged and beached. The seaplane tender Curtiss was also damaged. The destroyer Shaw was badly damaged when two bombs penetrated her forward magazine.[91]\\r\\nOf the 402[15] American aircraft in Hawaii, 188 were destroyed and 159 damaged,[15] 155 of them on the ground. Almost none were actually ready to take off to defend the base. Eight Army Air Forces pilots managed to get airborne during the attack[92] and six were credited with downing at least one Japanese aircraft during the attack: 1st Lt. Lewis M. Sanders, 2nd Lt. Philip M. Rasmussen, 2nd Lt. Kenneth M. Taylor, 2nd Lt. George S. Welch, 2nd Lt. Harry W. Brown, and 2nd Lt. Gordon H. Sterling Jr. Sterling was shot down by Lt. Fujita over Kaneohe Bay and is listed as Body Not Recovered (not Missing In Action). Lt. John L. Dains was killed by friendly fire returning from a victory over Kaawa.[93][94] Of 33 PBYs in Hawaii, 24 were destroyed, and six others damaged beyond repair. (The three on patrol returned undamaged.) Friendly fire brought down some U.S. planes on top of that, including five from an inbound flight from Enterprise. Japanese attacks on barracks killed additional personnel.\\r\\nAt the time of the attack, nine civilian aircraft were flying in the vicinity of Pearl Harbor. Of these, three were shot down.[89]\\r\\nFifty-five Japanese airmen and nine submariners were killed in the attack, and one was captured. Of Japan's 414[78] available planes, 29 were lost during the battle (nine in the first attack wave, 20 in the second),[95][nb 17] with another 74 damaged by antiaircraft fire from the ground.\\r\\nSeveral Japanese junior officers including Fuchida and Genda urged Nagumo to carry out a third strike in order to destroy as much of Pearl Harbor's fuel and torpedo[nb 18] storage, maintenance, and dry dock facilities as possible.[96] Genda, who had unsuccessfully advocated for invading Hawaii after the air attack, believed that without an invasion three strikes were necessary to disable the base as much as possible.[97] The captains of the other five carriers in the task force reported they were willing and ready to carry out a third strike.[98] Military historians have suggested the destruction of these shore facilities would have hampered the U.S. Pacific Fleet far more seriously than the loss of its battleships.[99] If they had been wiped out, \\"serious [American] operations in the Pacific would have been postponed for more than a year\\";[100] according to Admiral Chester W. Nimitz, later Commander in Chief of the Pacific Fleet, \\"it would have prolonged the war another two years.\\"[101] Nagumo, however, decided to withdraw for several reasons:\\r\\nAt a conference aboard Yamato the following morning, Yamamoto initially supported Nagumo.[106] In retrospect, sparing the vital dockyards, maintenance shops, and oil depots meant the U.S. could respond relatively quickly to Japanese activities in the Pacific. Yamamoto later regretted Nagumo's decision to withdraw and categorically stated it had been a great mistake not to order a third strike.[108]\\r\\nSeventeen ships were damaged or lost in the attack, of which fourteen were repaired and returned to service.[109]\\r\\nAfter a systematic search for survivors, formal salvage operations began. Captain Homer N. Wallin, Material Officer for Commander, Battle Force, U.S. Pacific Fleet, was immediately ordered to lead salvage operations. \\"Within a short time I was relieved of all other duties and ordered to full time work as Fleet Salvage Officer.\\"[111][nb 19]\\r\\nAround Pearl Harbor, divers from the Navy (shore and tenders), the Naval Shipyard, and civilian contractors (Pacific Bridge and others) began work on the ships that could be refloated. They patched holes, cleared debris, and pumped water out of ships. Navy divers worked inside the damaged ships. Within six months, five battleships and two cruisers were patched or refloated so they could be sent to shipyards in Pearl Harbor and on the mainland for extensive repair.\\r\\nIntensive salvage operations continued for another year, a total of some 20,000 man-hours under water.[113] Oklahoma, while successfully raised, was never repaired, and capsized while under tow to the mainland in 1947. Arizona and the target ship Utah were too heavily damaged for salvage, though much of their armament and equipment was removed and put to use aboard other vessels. Today, the two hulks remain where they were sunk,[114] with Arizona becoming a war memorial.\\r\\nIn the wake of the attack, 15 Medals of Honor, 51 Navy Crosses, 53 Silver Stars, four Navy and Marine Corps Medals, one Distinguished Flying Cross, four Distinguished Service Crosses, one Distinguished Service Medal, and three Bronze Star Medals were awarded to the American servicemen who distinguished themselves in combat at Pearl Harbor.[115] Additionally, a special military award, the Pearl Harbor Commemorative Medal, was later authorized for all military veterans of the attack.\\r\\nThe day after the attack, Roosevelt delivered his famous Infamy Speech to a Joint Session of Congress, calling for a formal declaration of war on the Empire of Japan. Congress obliged his request less than an hour later. On December 11, Germany and Italy declared war on the United States, even though the Tripartite Pact did not require it.[nb 20] Congress issued a declaration of war against Germany and Italy later that same day. The UK actually declared war on Japan nine hours before the U.S. did, partially due to Japanese attacks on Malaya, Singapore and Hong Kong, and partially due to Winston Churchill's promise to declare war \\"within the hour\\" of a Japanese attack on the United States.[116]\\r\\nThe attack was an initial shock to all the Allies in the Pacific Theater. Further losses compounded the alarming setback. Japan attacked the Philippines hours later (because of the time difference, it was December 8 in the Philippines). Only three days after the attack on Pearl Harbor, the battleships Prince of Wales and Repulse were sunk off the coast of Malaya, causing British Prime Minister Winston Churchill later to recollect \\"In all the war I never received a more direct shock. As I turned and twisted in bed the full horror of the news sank in upon me. There were no British or American capital ships in the Indian Ocean or the Pacific except the American survivors of Pearl Harbor who were hastening back to California. Over this vast expanse of waters Japan was supreme and we everywhere were weak and naked\\".[117]\\r\\nThroughout the war, Pearl Harbor was frequently used in American propaganda.[118]\\r\\nOne further consequence of the attack on Pearl Harbor and its aftermath (notably the Niihau incident) was that Japanese American residents and citizens were relocated to nearby Japanese-American internment camps. Within hours of the attack, hundreds of Japanese American leaders were rounded up and brought to high-security camps such as Sand Island at the mouth of Honolulu harbor and Kilauea Military Camp on the island of Hawaii.[119][120] Eventually, more than 110,000 Japanese Americans, nearly all who lived on the West Coast, were forced into interior camps, but in Hawaii, where the 150,000-plus Japanese Americans composed over one-third of the population, only 1,200 to 1,800 were interned.[121][122][123]\\r\\nThe attack also had international consequences. The Canadian province of British Columbia, bordering the Pacific Ocean, had long had a large population of Japanese immigrants and their Japanese Canadian descendants. Pre-war tensions were exacerbated by the Pearl Harbor attack, leading to a reaction from the Government of Canada. On February 24, 1942, Order-in-Council P.C. no. 1486 was passed under the War Measures Act allowing for the forced removal of any and all Canadians of Japanese descent from British Columbia, as well as the prohibiting from them returning to the province. On 4 March, regulations under the Act were adopted to evacuate Japanese-Canadians.[124] As a result, 12,000 were interned in interior camps, 2,000 were sent to road camps and another 2,000 were forced to work in the prairies at sugar beet farms.[125]\\r\\nThe Japanese planners had determined that some means was required for rescuing fliers whose aircraft were too badly damaged to return to the carriers. The island of Niihau, only 30 minutes flying time from Pearl Harbor, was designated as the rescue point.\\r\\nThe Zero flown by Petty Officer Shigenori Nishikaichi of Hiryu was damaged in the attack on Wheeler, so he flew to the rescue point on Niihau. The aircraft was further damaged on landing. Nishikaichi was helped from the wreckage by one of the native Hawaiians, who, aware of the tension between the United States and Japan, took the pilot's maps and other documents. The island's residents had no telephones or radio and were completely unaware of the attack on Pearl Harbor. Nishikaichi enlisted the support of three Japanese-American residents in an attempt to recover the documents. During the ensuing struggles, Nishikaichi was killed and a Hawaiian civilian was wounded; one collaborator committed suicide, and his wife and the third collaborator were sent to prison.\\r\\nThe ease with which the local ethnic Japanese residents had apparently gone to the assistance of Nishikaichi was a source of concern for many, and tended to support those who believed that local Japanese could not be trusted.[126]\\r\\nAdmiral Hara Tadaichi summed up the Japanese result by saying, \\"We won a great tactical victory at Pearl Harbor and thereby lost the war.\\"[127] To a similar effect, see Isoroku Yamamoto's alleged \\"sleeping giant\\" quote.\\r\\nWhile the attack accomplished its intended objective, it turned out to be largely unnecessary. Unbeknownst to Yamamoto, who conceived the original plan, the U.S. Navy had decided as far back as 1935 to abandon 'charging' across the Pacific towards the Philippines in response to an outbreak of war (in keeping with the evolution of Plan Orange).[29] The U.S. instead adopted \\"Plan Dog\\" in 1940, which emphasized keeping the IJN out of the eastern Pacific and away from the shipping lanes to Australia, while the U.S. concentrated on defeating Nazi Germany.[128]\\r\\nFortunately for the United States, the American aircraft carriers were untouched by the Japanese attack; otherwise the Pacific Fleet's ability to conduct offensive operations would have been crippled for a year or more (given no diversions from the Atlantic Fleet). As it was, the elimination of the battleships left the U.S. Navy with no choice but to rely on its aircraft carriers and submarinesthe very weapons with which the U.S. Navy halted and eventually reversed the Japanese advance. While six of the eight battleships were repaired and returned to service, their relatively low speed and high fuel consumption limited their deployment, and they served mainly in shore bombardment roles (their only major action being the Battle of Surigao Strait in October 1944). A major flaw of Japanese strategic thinking was a belief that the ultimate Pacific battle would be fought by battleships, in keeping with the doctrine of Captain Alfred Thayer Mahan. As a result, Yamamoto (and his successors) hoarded battleships for a \\"decisive battle\\" that never happened.[129]\\r\\nThe Japanese confidence in their ability to achieve a short, victorious war meant that they neglected Pearl Harbor's navy repair yards, oil tank farms, submarine base, and old headquarters building.[53] All of these targets were omitted from Genda's list, yet they proved more important than any battleship to the American war efforts in the Pacific. The survival of the repair shops and fuel depots allowed Pearl Harbor to maintain logistical support to the U.S. Navy's operations,[130][131] such as the Battles of Coral Sea and Midway. It was submarines that immobilized the Imperial Japanese Navy's heavy ships and brought Japan's economy to a virtual standstill by crippling the transportation of oil and raw materials: by the end of 1942, import of raw materials was cut to half of what it had been, \\"to a disastrous ten million tons\\", while oil import \\"was almost completely stopped\\".[nb 21] Lastly, the basement of the Old Administration Building was the home of the cryptanalytic unit which contributed significantly to the Midway ambush and the Submarine Force's success.[132]\\r\\nEver since the Japanese attack, there has been debate as to how and why the United States had been caught unaware, and how much and when American officials knew of Japanese plans and related topics. Military officers including Gen. Billy Mitchell had pointed out the vulnerability of Pearl to air attack. At least two Naval War games, one in 1932 and another in 1936, proved that Pearl was vulnerable to such an attack. Admiral James Richardson was removed from command shortly after protesting President Roosevelt's decision to move the bulk of the Pacific fleet to Pearl Harbor.[133] [134]The decisions of military and political leadership to ignore these warnings has contributed to conspiracy theories. Several writers, including journalist Robert Stinnett and former United States rear admiral Robert Alfred Theobald, have argued that various parties high in the U.S. and British governments knew of the attack in advance and may even have let it happen or encouraged it in order to force the U.S. into war via the so-called \\"back door\\". However, this conspiracy theory is rejected by mainstream historians.[135][136][137][138][nb 22]\\r\\nInformational notes\\r\\nCitations\\r\\nBibliography\\r\\nFurther reading\\r\\nAccounts\\r\\nMedia\\r\\nHistorical documents","input":"How many american lives lost at pearl harbor?"},{"output":"a single six-year term","context":"\\r\\n\\r\\nThe President of Mexico\\r\\n(Spanish: Presidente de Mxico), officially known as the President of the United Mexican States (Spanish: Presidente de los Estados Unidos Mexicanos)[2], is the head of state and government of Mexico. Under the Constitution, the president is also the Supreme Commander of the Mexican armed forces. The current President is Enrique Pe?a Nieto, who took office on December 1, 2012. Andrs Manuel L܇pez Obrador won the presidential election on July 1, 2018 to become President-elect.\\r\\n\\r\\nCurrently, the office of the President is considered to be revolutionary, in that the powers of office are derived from the Revolutionary Constitution of 1917. Another legacy of the Revolution is its ban on re-election.  Mexican presidents are limited to a single six-year term, called a sexenio.  No one who has held the post, even on a caretaker basis, is allowed to run or serve again. The constitution and the office of the President closely follow the presidential system of government.\\r\\n\\r\\nChapter III of Title III of the Constitution deals with the executive branch of government and sets forth the powers of the president, as well as the qualifications for the office.  He is vested with the \\"supreme executive power of the Union\\".\\r\\n\\r\\nTo be eligible to serve as president, Article 82 of the Constitution specifies that the following requirements must be met:\\r\\n\\r\\nThe ban on any sort of presidential re-election dates back to the aftermath of the Porfiriato and the end of the Mexican Revolution. It is so entrenched in Mexican politics that it has remained in place even as it was relaxed for other offices. In 2014, the constitution was amended to allow Deputies and Senators to run for a second consecutive term.  Previously, Deputies and Senators were barred from successive re-election. However, the president remained barred from re-election, even if it is nonsuccessive.\\r\\n\\r\\nThe presidential term was set at four years from 1821 to 1928, and has been set at six years since 1928. The president is elected by direct, popular, universal suffrage.  Whoever wins a simple plurality of the national vote is elected; there is no runoff election.\\r\\n\\r\\nThe most recent former President, Felipe Calder܇n, won with 36.38% of the votes in the 2006 general election, finishing only 0.56 percent above his nearest rival, Andrs Manuel L܇pez Obrador (who contested the official results). Former President Vicente Fox was elected with a plurality of 43% of the popular vote, Ernesto Zedillo won 48% of the vote, and his predecessor Carlos Salinas won with a majority of 50%. The current president, Enrique Pe?a Nieto won 38% of the popular vote.[3]\\r\\n\\r\\nThe history of Mexico has not been a peaceful one. After the fall of dictator Porfirio Daz in 1910 because of the Mexican Revolution, there was no stable government until 1929, when all the revolutionary leaders united in one political party:  the National Revolutionary Party, which later changed its name to the Party of the Mexican Revolution, and is now the Institutional Revolutionary Party (Spanish: Partido Revolucionario Institucional).  From then until 1988, the PRI ruled Mexico as a virtual one-party state.\\r\\n\\r\\nToward the end of his term, the incumbent president in consultation with party leaders, selected the PRI's candidate in the next election in a procedure known as \\"the tap of the finger\\" (Spanish: el dedazo). Until 1988, the PRI's candidate was virtually assured of election, winning by margins well over 70 percent of the voteresults that were usually obtained by massive electoral fraud. In 1988, however, the PRI ruptured and the dissidents formed the National Democratic Front with rival center-left parties (now the PRD). Discontent with the PRI, and the popularity of the Front's candidate Cuauhtmoc Crdenas led to worries that PRI candidate Carlos Salinas de Gortari would not come close to a majority, and might actually be defeated. While the votes were being counted, the tabulation system mysteriously shut down. The government declared Salinas the winner, leading to stronger than ever allegations of electoral fraud.[4]\\r\\n\\r\\nThe PRI enacted a strict internal discipline and government presence in the country, and electoral fraud became common. After the country regained its peace, this pattern of fraud continued, with the opposition losing every election until the later part of the 20th century. The first presidential election broadly considered legitimate was the one held in 1994, when the PRI's Ernesto Zedillo took office, and in his term several reforms were enacted to ensure fairness and transparency in elections. Partly as a consequence of these reforms, the 1997 federal congressional election saw the first opposition Chamber of Deputies ever, and the 2000 elections saw Vicente Fox of a PAN/PVEM alliance become the first opposition candidate to win an election since 1911. This historical defeat was accepted on election night by the PRI in the voice of President Zedillo; while this calmed fears of violence, it also fueled questions about the role of the president in the electoral process and to whom the responsibility of conceding defeat should fall in a democratic election.\\r\\n\\r\\nThe role of unions in the new balance of power and future elections is documented in works like historian Enrique Krauze's Analysis of the Corporative System.\\r\\n\\r\\nAfter the presidential election, political parties may issue challenges to the election. These challenges are heard by the Electoral Tribunal of the Federal Judicial Power; after it has heard and ruled on them, the Tribunal must either declare the election invalid, or certify the results of the elections in accordance to their rulings. Once the Tribunal declares the election valid, it issues a \\"Certificate of Majority\\" (Constancia de Mayora) to the candidate who obtained a plurality. That candidate then becomes President-elect. The final decision is made in September, two months after the election is carried out.[5]\\r\\n\\r\\nThe 1917 Constitution borrowed heavily from the Constitution of the United States, providing for a clear separation of powers while giving the president wider powers than his American counterpart.  However, this has only recently become the case in practice.\\r\\n\\r\\nFor the first 71 years after the enactment of the 1917 Constitution, the president exercised nearly absolute control over the country.  Much of this power came from the de facto monopoly status of the PRI.  As mentioned above, he effectively chose his successor as president by personally nominating the PRI's candidate in the next election.  In addition, the unwritten rules of the PRI allowed him to designate party officials and candidates all the way down to the local level. He thus had an important (but not exclusive) influence over the political life of the country (part of his power had to be shared with unions and other groups, but as an individual he had no peers). This, and his constitutional powers, made some political commentators describe the president as a six-year dictator, and to call this system an \\"imperial presidency\\". The situation remained largely unchanged until the early 1980s, when a grave economic crisis created discomfort both in the population and inside the party, and the president's power was no longer absolute but still impressive.\\r\\n\\r\\nAn important characteristic of this system is that the new president was effectively chosen by the old one (since the PRI candidate was assured of election) but once he assumed power, the old one lost all power and influence (\\"no reelection\\" is a cornerstone of Mexican politics).  In fact, tradition called for the incumbent president to fade into the background during the campaign to elect his successor.  This renewed command helped maintain party discipline and avoided the stagnation associated with a single man holding power for decades, prompting Peruvian novelist Mario Vargas Llosa to call Mexico's political system \\"the perfect dictatorship\\", since the president's powers were cloaked by democratic practice.\\r\\n\\r\\nWith the democratic reforms of recent years and fairer elections, the president's powers have been limited in fact as well as in name. Vargas Llosa, during the Fox administration, called this new system \\"The Imperfect Democracy\\". The current rights and powers of the president of Mexico are established, limited and enumerated by Article 89 of the Constitution which include the following:\\r\\n\\r\\nA decree is a legislative instrument that has an expiration date and that is issued by one of the three branches of government. Congress may issue decrees, and the President may issue decrees as well. However, they have all the power of laws, but cannot be changed except by the power that issued them. Decrees are very limited in their extent. One such decree is the federal budget, which is issued by Congress. The president's office may suggest a budget, but at the end of the day, it is Congress that decrees how to collect taxes and how to spend them. A Supreme Court ruling on Vicente Fox's veto of the 2004 budget suggests that the President may have the right to veto decrees from Congress.\\r\\n\\r\\nSince 1997, the Congress has been plural, usually with opposition parties having a majority. Major reforms (tax, energy) have to pass by Congress, and the ruling President usually found his efforts blocked: the PRI's Zedillo by opposing PAN/PRD congressmen, and later the PAN's Fox by the PRI and PRD. The PAN would push the reforms it denied to the PRI and vice versa. This situation, novel in a country where Congress was +90% dominated by the president's party for most of the century, has led to a legal analysis of the president's power. Formerly almost a dictator (because of PRI's party discipline), the current times show the president's power as somewhat limited. In 2004, President Fox threatened to veto the budget approved by Congress, claiming the budget overstepped his authority to lead the country, only to learn no branch of government had the power to veto a decree issued by another branch of government (although a different, non jurisprudence-setting ruling stated he could return the budget with observations).\\r\\n\\r\\nUpon taking office, the President raises his/her right arm to shoulder-level and takes the following oath:\\r\\n\\r\\nProtesto guardar y hacer guardar la Constituci܇n Poltica de los Estados Unidos Mexicanos y las leyes que de ella emanen, y desempe?ar leal y patri܇ticamente el cargo de Presidente de la Rep~blica que el pueblo me ha conferido, mirando en todo por el bien y prosperidad de la Uni܇n; y si as no lo hiciere que la Naci܇n me lo demande.\\r\\n\\r\\nTranslation:\\r\\n\\r\\nI affirm to follow and uphold the Political Constitution of the United Mexican States and the laws that emanate from it, and to perform the office of President of the Republic which the people have conferred upon me with loyalty and patriotism, in all actions looking after the good and prosperity of the Union; and if I do not fulfill these obligations, may the Nation demand it of me.\\r\\n\\r\\nThe Mexican Presidential sash has the colors of the Mexican flag in three bands of equal width, with red on top, white in the center, and green on the bottom, worn from right shoulder to left waist; it also includes the National Seal, in gold thread, to be worn chest-high.  During the swearing-in ceremony of a newly elected President, the outgoing President turns in the sash to the current President of Congress, who in turn gives it to the new President after the latter has sworn the oath of office. The sash is the symbol of the Executive Federal Power, and may only be worn by the current President.\\r\\n\\r\\nAccording to Article 35 of the Law on the National Arms, Flag, and Anthem, the President must wear the sash at the swearing-in ceremony, when he makes his annual State of the Union report to Congress, during the commemoration of the Grito de Dolores on September 15 of each year, and when he receives the diplomatic credentials of accredited foreign ambassadors and ministers. He is also expected to wear it \\"in those official ceremonies of greatest solemnity\\". The sash is worn from right shoulder to left hip, and should be worn underneath the coat except during the swearing-in ceremony, when both the out-going and incoming president wear it over their coat (Article 36).\\r\\n\\r\\nIn addition to the Presidential Sash, each president receives a Presidential Flag; the flag has imprinted the words Estados Unidos Mexicanos in golden letters and the national coat of arms also in gold.\\r\\n\\r\\nThe President's official residence and main workplace is Los Pinos, located inside the Bosque de Chapultepec (Chapultepec Park). The President has the right to use this residence for the six-year term of office.\\r\\n\\r\\nThe National Palace, a building facing the Mexico City Z܇calo, is officially the seat of the Executive Power, but is used only for ceremonies or national holidays such as Independence Day or Revolution Day. Some areas of the historic building are open to the public, and others hold some government offices.\\r\\n\\r\\nThe President also has the use of Chapultepec Castle, formerly an Imperial palace of the Second Mexican Empire, and afterwards the official residence of Mexican Presidents until the Presidency of Lzaro Crdenas.\\r\\n\\r\\nArticle 84 of the Mexican Constitution states that \\"in case of absolute absence of a President\\" the following should happen:\\r\\n\\r\\nNo person who has already served as President, whether elected, Provisional, Interim, or Substitute, can be designated as Provisional, Interim, or Substitute President.\\r\\n\\r\\nThe designation of the Secretary of the Interior as the immediate successor dates to August 2012, when the changes to the Constitution were published in the Official Diary.\\r\\n\\r\\nThe succession provisions have come into play only twice since the current constitution was enacted. In 1928, after the assassination of president-elect lvaro Obreg܇n, Congress appointed Emilio Portes Gil as Interim President; Portes Gil served in the position for 14 months while new elections were called. Pascual Ortiz Rubio was elected President in the special elections that followed in 1930, but he resigned in 1932. Abelardo L. Rodrguez was then appointed Interim President to fill out the remainder of Ortiz Rubio's term (under current law Rodrguez would be Substitute President, but at the time there was no distinction between Interim, Substitute, and Provisional Presidents).\\r\\n\\r\\nThere are five living former presidents. The most recent former president to die was Miguel de la Madrid (1982ÿ1988), on 1 April 2012.\\r\\n\\r\\nFormer presidents of Mexico continue to carry the title \\"President\\" until death but are rarely referred by it; they are commonly called ex-Presidents. They are also given protection by the Estado Mayor Presidencial. Former presidents are also given a lifelong pension, which they can refuse, as in the case of Ernesto Zedillo.\\r\\n\\r\\nContrary to what happens in many other countries, former presidents of Mexico do not continue to be important national figures once out of office, and usually lead a discreet life. This is partly because they do not want to interfere with the government of the new president and partly because they may not have a good public image. This tradition can be traced back to the presidency of Lzaro Crdenas.  Former president Plutarco Elas Calles had personally selected Crdenas as his successor, and had hoped to control things from behind the scenes as he had for the last five years.  However, when Crdenas showed he was going to rule in fact as well as in name, Calles publicly criticized him, prompting Crdenas to have Calles escorted out of the country by military police. Crdenas himself remained silent on the policies of his successor Manuel vila Camacho, establishing a tradition that former presidents do not interfere with their successors.\\r\\n\\r\\nFor example, Ernesto Zedillo holds important offices in the United Nations and in the private sector, but outside of Mexico. It is speculated he lives in a self-imposed exile to avoid the hatred of some of his fellow members of the PRI for having acknowledged the PRI's defeat in the 2000 presidential election. Carlos Salinas also lived in a self-imposed exile in Ireland, but returned to Mexico. He campaigned intensely to have his brother, Ra~l Salinas, freed after he was jailed in the early days of Zedillo's term, accused of drug trafficking and planning the assassination of Jos Francisco Ruiz Massieu. Carlos Salinas also wrote a book on neo-liberal Mexico, secured a position with the Dow Jones Company in the United States, and worked as a professor at several prestigious universities in that country. Felipe Calder܇n was given a contract to work as a professor for Harvard University in 2013, but he returned to Mexico in 2014. It was rumored that he would look after the then newly created Humanist Party;[6] this fact was eventually denied by his wife.[7]\\r\\n\\r\\nAlong with Felipe Calder܇n, two other surviving former presidents, Luis Echeverra and Vicente Fox, still live in Mexico. On June 30, 2006, Echeverra was placed under house arrest under charges of genocide for his role as Secretary of the Interior during the 1968 Tlatelolco massacre.[8] The house arrest was lifted in 2009.","input":"How many terms can the president of mexico serve?"},{"output":"23 July 1997","context":"","input":"When did myanmar became a member of asean?"},{"output":"Historia Regum Britanniae","context":"King Arthur is a legendary British leader who, according to medieval histories and romances, led the defence of Britain against Saxon invaders in the late 5th and early 6th centuries AD. The details of Arthur's story are mainly composed of folklore and literary invention, and his historical existence is debated and disputed by modern historians.[2] The sparse historical background of Arthur is gleaned from various sources, including the Annales Cambriae, the Historia Brittonum, and the writings of Gildas. Arthur's name also occurs in early poetic sources such as Y Gododdin.[3]\\r\\nArthur is a central figure in the legends making up the Matter of Britain. The legendary Arthur developed as a figure of international interest largely through the popularity of Geoffrey of Monmouth's fanciful and imaginative 12th-century Historia Regum Britanniae (History of the Kings of Britain).[4] In some Welsh and Breton tales and poems that date from before this work, Arthur appears either as a great warrior defending Britain from human and supernatural enemies or as a magical figure of folklore, sometimes associated with the Welsh Otherworld, Annwn.[5] How much of Geoffrey's Historia (completed in 1138) was adapted from such earlier sources, rather than invented by Geoffrey himself, is unknown.\\r\\nAlthough the themes, events and characters of the Arthurian legend varied widely from text to text, and there is no one canonical version; Geoffrey's version of events often served as the starting point for later stories. Geoffrey depicted Arthur as a king of Britain who defeated the Saxons and established an empire over Britain, Ireland, Iceland, Norway and Gaul. Many elements and incidents that are now an integral part of the Arthurian story appear in Geoffrey's Historia, including Arthur's father Uther Pendragon, the wizard Merlin, Arthur's wife Guinevere, the sword Excalibur, Arthur's conception at Tintagel, his final battle against Mordred at Camlann, and final rest in Avalon. The 12th-century French writer Chrtien de Troyes, who added Lancelot and the Holy Grail to the story, began the genre of Arthurian romance that became a significant strand of medieval literature. In these French stories, the narrative focus often shifts from King Arthur himself to other characters, such as various Knights of the Round Table. Arthurian literature thrived during the Middle Ages but waned in the centuries that followed until it experienced a major resurgence in the 19th century. In the 21st century, the legend lives on, not only in literature but also in adaptations for theatre, film, television, comics and other media.\\r\\n\\r\\n\\r\\nThe historical basis for the King Arthur legend has long been debated by scholars. One school of thought, citing entries in the Historia Brittonum (History of the Britons) and Annales Cambriae (Welsh Annals), sees Arthur as a genuine historical figure, a Romano-British leader who fought against the invading Anglo-Saxons some time in the late 5th to early 6th century. The Historia Brittonum, a 9th-century Latin historical compilation attributed in some late manuscripts to a Welsh cleric called Nennius, contains the first datable mention of King Arthur, listing twelve battles that Arthur fought. These culminate in the Battle of Badon, where he is said to have single-handedly killed 960 men. Recent studies, however, question the reliability of the Historia Brittonum.[7]\\r\\nThe other text that seems to support the case for Arthur's historical existence is the 10th-century Annales Cambriae, which also link Arthur with the Battle of Badon. The Annales date this battle to 516ÿ518, and also mention the Battle of Camlann, in which Arthur and Medraut (Mordred) were both killed, dated to 537ÿ539. These details have often been used to bolster confidence in the Historia's account and to confirm that Arthur really did fight at Badon. Problems have been identified, however, with using this source to support the Historia Brittonum's account. The latest research shows that the Annales Cambriae was based on a chronicle begun in the late 8th century in Wales. Additionally, the complex textual history of the Annales Cambriae precludes any certainty that the Arthurian annals were added to it even that early. They were more likely added at some point in the 10th century and may never have existed in any earlier set of annals. The Badon entry probably derived from the Historia Brittonum.[8]\\r\\nThis lack of convincing early evidence is the reason many recent historians exclude Arthur from their accounts of sub-Roman Britain. In the view of historian Thomas Charles-Edwards, \\"at this stage of the enquiry, one can only say that there may well have been an historical Arthur [but ...] the historian can as yet say nothing of value about him\\".[9] These modern admissions of ignorance are a relatively recent trend; earlier generations of historians were less sceptical. The historian John Morris made the putative reign of Arthur the organising principle of his history of sub-Roman Britain and Ireland, The Age of Arthur (1973). Even so, he found little to say about a historical Arthur.[10]\\r\\nPartly in reaction to such theories, another school of thought emerged which argued that Arthur had no historical existence at all. Morris's Age of Arthur prompted the archaeologist Nowell Myres to observe that \\"no figure on the borderline of history and mythology has wasted more of the historian's time\\".[11] Gildas' 6th-century polemic De Excidio et Conquestu Britanniae (On the Ruin and Conquest of Britain), written within living memory of Badon, mentions the battle but does not mention Arthur.[12] Arthur is not mentioned in the Anglo-Saxon Chronicle or named in any surviving manuscript written between 400 and 820.[13] He is absent from Bede's early-8th-century Ecclesiastical History of the English People, another major early source for post-Roman history that mentions Badon.[14] The historian David Dumville has written: \\"I think we can dispose of him [Arthur] quite briefly. He owes his place in our history books to a 'no smoke without fire' school of thought?... The fact of the matter is that there is no historical evidence about Arthur; we must reject him from our histories and, above all, from the titles of our books.\\"[15]\\r\\nSome scholars argue that Arthur was originally a fictional hero of folkloreor even a half-forgotten Celtic deitywho became credited with real deeds in the distant past. They cite parallels with figures such as the Kentish Hengist and Horsa, who may be totemic horse-gods that later became historicised. Bede ascribed to these legendary figures a historical role in the 5th-century Anglo-Saxon conquest of eastern Britain.[16] It is not even certain that Arthur was considered a king in the early texts. Neither the Historia nor the Annales calls him \\"rex\\": the former calls him instead \\"dux bellorum\\" (leader of battles) and \\"miles\\" (soldier).[17]\\r\\nHistorical documents for the post-Roman period are scarce, so a definitive answer to the question of Arthur's historical existence is unlikely. Sites and places have been identified as \\"Arthurian\\" since the 12th century,[18] but archaeology can confidently reveal names only through inscriptions found in secure contexts. The so-called \\"Arthur stone\\", discovered in 1998 among the ruins at Tintagel Castle in Cornwall in securely dated 6th-century contexts, created a brief stir but proved irrelevant.[19] Other inscriptional evidence for Arthur, including the Glastonbury cross, is tainted with the suggestion of forgery.[20] Although several historical figures have been proposed as the basis for Arthur,[21] no convincing evidence for these identifications has emerged.\\r\\nThe origin of the Welsh name \\"Arthur\\" remains a matter of debate. Some suggest it is derived from the Roman nomen gentile (family name) Artorius, of obscure and contested etymology[22] (but possibly of Messapian[23][24][25] or Etruscan origin).[26][27][28] Some scholars have suggested it is relevant to this debate that the legendary King Arthur's name only appears as Arthur, or Arturus, in early Latin Arthurian texts, never as Artrius (though it should be noted that Classical Latin Artrius became Arturius in some Vulgar Latin dialects). However, this may not say anything about the origin of the name Arthur, as Artrius would regularly become Art(h)ur when borrowed into Welsh.[29]\\r\\nAnother possibility is that it is derived from a Brittonic patronym *Arto-rؐg-ios (the root of which, *arto-rؐg- \\"bear-king\\" is to be found in the Old Irish personal name Art-ri) via a Latinized form Artrius.[30] Less likely is the commonly proposed derivation from Welsh arth \\"bear\\" + (g)wr \\"man\\" (earlier *Arto-uiros in Brittonic); there are phonological difficulties with this theorynotably that a Brittonic compound name *Arto-uiros should produce Old Welsh *Artgur and Middle/Modern Welsh *Arthwr and not Arthur (in Welsh poetry the name is always spelled Arthur and is exclusively rhymed with words ending in -ur ÿ never words ending in -wr ÿ which confirms that the second element cannot be [g]wr \\"man\\").[31][32]\\r\\nAn alternative theory, which has gained only limited acceptance among professional scholars, derives the name Arthur from Arcturus, the brightest star in the constellation Bo?tes, near Ursa Major or the Great Bear.[33] Classical Latin Arcturus would also have become Art(h)ur when borrowed into Welsh, and its brightness and position in the sky led people to regard it as the \\"guardian of the bear\\" (which is the meaning of the name in Ancient Greek) and the \\"leader\\" of the other stars in Bo?tes.[34]\\r\\nA similar first name is Old Irish Art~r, which is believed to be derived directly from an early Old Welsh or Cumbric Artur.[35] The earliest historically attested bearer of the name is a son or grandson of edn mac Gabrin (d. AD 609).[36]\\r\\nThe creator of the familiar literary persona of Arthur was Geoffrey of Monmouth, with his pseudo-historical Historia Regum Britanniae (History of the Kings of Britain), written in the 1130s. The textual sources for Arthur are usually divided into those written before Geoffrey's Historia (known as pre-Galfridian texts, from the Latin form of Geoffrey, Galfridus) and those written afterwards, which could not avoid his influence (Galfridian, or post-Galfridian, texts).\\r\\nThe earliest literary references to Arthur come from Welsh and Breton sources. There have been few attempts to define the nature and character of Arthur in the pre-Galfridian tradition as a whole, rather than in a single text or text/story-type. A 2007 academic survey that does attempt this by Thomas Green identifies three key strands to the portrayal of Arthur in this earliest material.[37] The first is that he was a peerless warrior who functioned as the monster-hunting protector of Britain from all internal and external threats. Some of these are human threats, such as the Saxons he fights in the Historia Brittonum, but the majority are supernatural, including giant cat-monsters, destructive divine boars, dragons, dogheads, giants, and witches.[38] The second is that the pre-Galfridian Arthur was a figure of folklore (particularly topographic or onomastic folklore) and localised magical wonder-tales, the leader of a band of superhuman heroes who live in the wilds of the landscape.[39] The third and final strand is that the early Welsh Arthur had a close connection with the Welsh Otherworld Annwn. On the one hand, he launches assaults on Otherworldly fortresses in search of treasure and frees their prisoners. On the other, his warband in the earliest sources includes former pagan gods, and his wife and his possessions are clearly Otherworldly in origin.[40]\\r\\nOne of the most famous Welsh poetic references to Arthur comes in the collection of heroic death-songs known as Y Gododdin (The Gododdin), attributed to 6th-century poet Aneirin. One stanza praises the bravery of a warrior who slew 300 enemies, but says that despite this, \\"he was no Arthur\\" ÿ that is, his feats cannot compare to the valour of Arthur.[41] Y Gododdin is known only from a 13th-century manuscript, so it is impossible to determine whether this passage is original or a later interpolation, but John Koch's view that the passage dates from a 7th-century or earlier version is regarded as unproven; 9th- or 10th-century dates are often proposed for it.[42] Several poems attributed to Taliesin, a poet said to have lived in the 6th century, also refer to Arthur, although these all probably date from between the 8th and 12th centuries.[43] They include \\"Kadeir Teyrnon\\" (\\"The Chair of the Prince\\"),[44] which refers to \\"Arthur the Blessed\\"; \\"Preiddeu Annwn\\" (\\"The Spoils of Annwn\\"),[45] which recounts an expedition of Arthur to the Otherworld; and \\"Marwnat vthyr pen[dragon]\\" (\\"The Elegy of Uther Pen[dragon]\\"),[46] which refers to Arthur's valour and is suggestive of a father-son relationship for Arthur and Uther that pre-dates Geoffrey of Monmouth.\\r\\nOther early Welsh Arthurian texts include a poem found in the Black Book of Carmarthen, \\"Pa gur yv y porthaur?\\" (\\"What man is the gatekeeper?\\").[48] This takes the form of a dialogue between Arthur and the gatekeeper of a fortress he wishes to enter, in which Arthur recounts the names and deeds of himself and his men, notably Cei (Kay) and Bedwyr (Bedivere). The Welsh prose tale Culhwch and Olwen (c.?1100), included in the modern Mabinogion collection, has a much longer list of more than 200 of Arthur's men, though Cei and Bedwyr again take a central place. The story as a whole tells of Arthur helping his kinsman Culhwch win the hand of Olwen, daughter of Ysbaddaden Chief-Giant, by completing a series of apparently impossible tasks, including the hunt for the great semi-divine boar Twrch Trwyth. The 9th-century Historia Brittonum also refers to this tale, with the boar there named Troy(n)t.[49] Finally, Arthur is mentioned numerous times in the Welsh Triads, a collection of short summaries of Welsh tradition and legend which are classified into groups of three linked characters or episodes to assist recall. The later manuscripts of the Triads are partly derivative from Geoffrey of Monmouth and later continental traditions, but the earliest ones show no such influence and are usually agreed to refer to pre-existing Welsh traditions. Even in these, however, Arthur's court has started to embody legendary Britain as a whole, with \\"Arthur's Court\\" sometimes substituted for \\"The Island of Britain\\" in the formula \\"Three XXX of the Island of Britain\\".[50] While it is not clear from the Historia Brittonum and the Annales Cambriae that Arthur was even considered a king, by the time Culhwch and Olwen and the Triads were written he had become Penteyrnedd yr Ynys hon, \\"Chief of the Lords of this Island\\", the overlord of Wales, Cornwall and the North.[51]\\r\\nIn addition to these pre-Galfridian Welsh poems and tales, Arthur appears in some other early Latin texts besides the Historia Brittonum and the Annales Cambriae. In particular, Arthur features in a number of well-known vitae (\\"Lives\\") of post-Roman saints, none of which are now generally considered to be reliable historical sources (the earliest probably dates from the 11th century).[52] According to the Life of Saint Gildas, written in the early 12th century by Caradoc of Llancarfan, Arthur is said to have killed Gildas' brother Hueil and to have rescued his wife Gwenhwyfar from Glastonbury.[53] In the Life of Saint Cadoc, written around 1100 or a little before by Lifris of Llancarfan, the saint gives protection to a man who killed three of Arthur's soldiers, and Arthur demands a herd of cattle as wergeld for his men. Cadoc delivers them as demanded, but when Arthur takes possession of the animals, they turn into bundles of ferns.[54] Similar incidents are described in the medieval biographies of Carannog, Padarn, and Eufflam, probably written around the 12th century. A less obviously legendary account of Arthur appears in the Legenda Sancti Goeznovii, which is often claimed to date from the early 11th century (although the earliest manuscript of this text dates from the 15th century).[55] Also important are the references to Arthur in William of Malmesbury's De Gestis Regum Anglorum and Herman's De Miraculis Sanctae Mariae Laudensis, which together provide the first certain evidence for a belief that Arthur was not actually dead and would at some point return, a theme that is often revisited in post-Galfridian folklore.[56]\\r\\nThe first narrative account of Arthur's life is found in Geoffrey of Monmouth's Latin work Historia Regum Britanniae (History of the Kings of Britain), completed c.?1138.[57] This work is an imaginative and fanciful account of British kings from the legendary Trojan exile Brutus to the 7th-century Welsh king Cadwallader. Geoffrey places Arthur in the same post-Roman period as do Historia Brittonum and Annales Cambriae. He incorporates Arthur's father, Uther Pendragon, his magician advisor Merlin, and the story of Arthur's conception, in which Uther, disguised as his enemy Gorlois by Merlin's magic, sleeps with Gorlois's wife Igerna at Tintagel, and she conceives Arthur. On Uther's death, the fifteen-year-old Arthur succeeds him as King of Britain and fights a series of battles, similar to those in the Historia Brittonum, culminating in the Battle of Bath. He then defeats the Picts and Scots before creating an Arthurian empire through his conquests of Ireland, Iceland and the Orkney Islands. After twelve years of peace, Arthur sets out to expand his empire once more, taking control of Norway, Denmark and Gaul. Gaul is still held by the Roman Empire when it is conquered, and Arthur's victory naturally leads to a further confrontation between his empire and Rome's. Arthur and his warriors, including Kaius (Kay), Beduerus (Bedivere) and Gualguanus (Gawain), defeat the Roman emperor Lucius Tiberius in Gaul but, as he prepares to march on Rome, Arthur hears that his nephew Modredus (Mordred)whom he had left in charge of Britainhas married his wife Guenhuuara (Guinevere) and seized the throne. Arthur returns to Britain and defeats and kills Modredus on the river Camblam in Cornwall, but he is mortally wounded. He hands the crown to his kinsman Constantine and is taken to the isle of Avalon to be healed of his wounds, never to be seen again.[58]\\r\\nHow much of this narrative was Geoffrey's own invention is open to debate. Certainly, Geoffrey seems to have made use of the list of Arthur's twelve battles against the Saxons found in the 9th-century Historia Brittonum, along with the battle of Camlann from the Annales Cambriae and the idea that Arthur was still alive.[60] Arthur's personal status as the king of all Britain would also seem to be borrowed from pre-Galfridian tradition, being found in Culhwch and Olwen, the Triads, and the saints' lives.[61] Finally, Geoffrey borrowed many of the names for Arthur's possessions, close family, and companions from the pre-Galfridian Welsh tradition, including Kaius (Cei), Beduerus (Bedwyr), Guenhuuara (Gwenhwyfar), Uther (Uthyr) and perhaps also Caliburnus (Caledfwlch), the latter becoming Excalibur in subsequent Arthurian tales.[62] However, while names, key events, and titles may have been borrowed, Brynley Roberts has argued that \\"the Arthurian section is Geoffrey's literary creation and it owes nothing to prior narrative.\\"[63] So, for instance, the Welsh Medraut is made the villainous Modredus by Geoffrey, but there is no trace of such a negative character for this figure in Welsh sources until the 16th century.[64] There have been relatively few modern attempts to challenge this notion that the Historia Regum Britanniae is primarily Geoffrey's own work, with scholarly opinion often echoing William of Newburgh's late-12th-century comment that Geoffrey \\"made up\\" his narrative, perhaps through an \\"inordinate love of lying\\".[65] Geoffrey Ashe is one dissenter from this view, believing that Geoffrey's narrative is partially derived from a lost source telling of the deeds of a 5th-century British king named Riotamus, this figure being the original Arthur, although historians and Celticists have been reluctant to follow Ashe in his conclusions.[66]\\r\\nWhatever his sources may have been, the immense popularity of Geoffrey's Historia Regum Britanniae cannot be denied. Well over 200 manuscript copies of Geoffrey's Latin work are known to have survived, and this does not include translations into other languages.[67] Thus, for example, around 60 manuscripts are extant containing Welsh-language versions of the Historia, the earliest of which were created in the 13th century; the old notion that some of these Welsh versions actually underlie Geoffrey's Historia, advanced by antiquarians such as the 18th-century Lewis Morris, has long since been discounted in academic circles.[68] As a result of this popularity, Geoffrey's Historia Regum Britanniae was enormously influential on the later medieval development of the Arthurian legend. While it was by no means the only creative force behind Arthurian romance, many of its elements were borrowed and developed (e.g., Merlin and the final fate of Arthur), and it provided the historical framework into which the romancers' tales of magical and wonderful adventures were inserted.[69]\\r\\nThe popularity of Geoffrey's Historia and its other derivative works (such as Wace's Roman de Brut) is generally agreed to be an important factor in explaining the appearance of significant numbers of new Arthurian works in continental Europe during the 12th and 13th centuries, particularly in France.[70] It was not, however, the only Arthurian influence on the developing \\"Matter of Britain\\". There is clear evidence that Arthur and Arthurian tales were familiar on the Continent before Geoffrey's work became widely known (see for example, the Modena Archivolt),[71] and \\"Celtic\\" names and stories not found in Geoffrey's Historia appear in the Arthurian romances.[72] From the perspective of Arthur, perhaps the most significant effect of this great outpouring of new Arthurian story was on the role of the king himself: much of this 12th-century and later Arthurian literature centres less on Arthur himself than on characters such as Lancelot and Guinevere, Percival, Galahad, Gawain, Ywain, and Tristan and Iseult. Whereas Arthur is very much at the centre of the pre-Galfridian material and Geoffrey's Historia itself, in the romances he is rapidly sidelined.[73] His character also alters significantly. In both the earliest materials and Geoffrey he is a great and ferocious warrior, who laughs as he personally slaughters witches and giants and takes a leading role in all military campaigns,[74] whereas in the continental romances he becomes the roi fainant, the \\"do-nothing king\\", whose \\"inactivity and acquiescence constituted a central flaw in his otherwise ideal society\\".[75] Arthur's role in these works is frequently that of a wise, dignified, even-tempered, somewhat bland, and occasionally feeble monarch. So, he simply turns pale and silent when he learns of Lancelot's affair with Guinevere in the Mort Artu, whilst in Chrtien de Troyes's Yvain, the Knight of the Lion, he is unable to stay awake after a feast and has to retire for a nap.[76] Nonetheless, as Norris J. Lacy has observed, whatever his faults and frailties may be in these Arthurian romances, \\"his prestige is neveror almost nevercompromised by his personal weaknesses?... his authority and glory remain intact.\\"[77]\\r\\nArthur and his retinue appear in some of the Lais of Marie de France,[79] but it was the work of another French poet, Chrtien de Troyes, that had the greatest influence with regard to the development of Arthur's character and legend.[80] Chrtien wrote five Arthurian romances between c.?1170 and 1190. Erec and Enide and Cligs are tales of courtly love with Arthur's court as their backdrop, demonstrating the shift away from the heroic world of the Welsh and Galfridian Arthur, while Yvain, the Knight of the Lion, features Yvain and Gawain in a supernatural adventure, with Arthur very much on the sidelines and weakened. However, the most significant for the development of the Arthurian legend are Lancelot, the Knight of the Cart, which introduces Lancelot and his adulterous relationship with Arthur's queen (Guinevere), extending and popularising the recurring theme of Arthur as a cuckold, and Perceval, the Story of the Grail, which introduces the Holy Grail and the Fisher King and which again sees Arthur having a much reduced role.[81] Chrtien was thus \\"instrumental both in the elaboration of the Arthurian legend and in the establishment of the ideal form for the diffusion of that legend\\",[82] and much of what came after him in terms of the portrayal of Arthur and his world built upon the foundations he had laid. Perceval, although unfinished, was particularly popular: four separate continuations of the poem appeared over the next half century, with the notion of the Grail and its quest being developed by other writers such as Robert de Boron, a fact that helped accelerate the decline of Arthur in continental romance.[83] Similarly, Lancelot and his cuckolding of Arthur with Guinevere became one of the classic motifs of the Arthurian legend, although the Lancelot of the prose Lancelot (c.?1225) and later texts was a combination of Chrtien's character and that of Ulrich von Zatzikhoven's Lanzelet.[84] Chrtien's work even appears to feed back into Welsh Arthurian literature, with the result that the romance Arthur began to replace the heroic, active Arthur in Welsh literary tradition.[85] Particularly significant in this development were the three Welsh Arthurian romances, which are closely similar to those of Chrtien, albeit with some significant differences: Owain, or the Lady of the Fountain is related to Chrtien's Yvain; Geraint and Enid, to Erec and Enide; and Peredur son of Efrawg, to Perceval.[86]\\r\\nUp to c.?1210, continental Arthurian romance was expressed primarily through poetry; after this date the tales began to be told in prose. The most significant of these 13th-century prose romances was the Vulgate Cycle (also known as the Lancelot-Grail Cycle), a series of five Middle French prose works written in the first half of that century.[88] These works were the Estoire del Saint Grail, the Estoire de Merlin, the Lancelot propre (or Prose Lancelot, which made up half the entire Vulgate Cycle on its own), the Queste del Saint Graal and the Mort Artu, which combine to form the first coherent version of the entire Arthurian legend. The cycle continued the trend towards reducing the role played by Arthur in his own legend, partly through the introduction of the character of Galahad and an expansion of the role of Merlin. It also made Mordred the result of an incestuous relationship between Arthur and his sister and established the role of Camelot, first mentioned in passing in Chrtien's Lancelot, as Arthur's primary court.[89] This series of texts was quickly followed by the Post-Vulgate Cycle (c.?1230ÿ40), of which the Suite du Merlin is a part, which greatly reduced the importance of Lancelot's affair with Guinevere but continued to sideline Arthur, and to focus more on the Grail quest.[88] As such, Arthur became even more of a relatively minor character in these French prose romances; in the Vulgate itself he only figures significantly in the Estoire de Merlin and the Mort Artu. During this period, Arthur was made one of the Nine Worthies, a group of three pagan, three Jewish and three Christian exemplars of chivalry. The Worthies were first listed in Jacques de Longuyon's Voeux du Paon in 1312, and subsequently became a common subject in literature and art.[90]\\r\\nThe development of the medieval Arthurian cycle and the character of the \\"Arthur of romance\\" culminated in Le Morte d'Arthur, Thomas Malory's retelling of the entire legend in a single work in English in the late 15th century. Malory based his bookoriginally titled The Whole Book of King Arthur and of His Noble Knights of the Round Tableon the various previous romance versions, in particular the Vulgate Cycle, and appears to have aimed at creating a comprehensive and authoritative collection of Arthurian stories.[91] Perhaps as a result of this, and the fact that Le Morte D'Arthur was one of the earliest printed books in England, published by William Caxton in 1485, most later Arthurian works are derivative of Malory's.[92]\\r\\nThe end of the Middle Ages brought with it a waning of interest in King Arthur. Although Malory's English version of the great French romances was popular, there were increasing attacks upon the truthfulness of the historical framework of the Arthurian romances ÿ established since Geoffrey of Monmouth's time ÿ and thus the legitimacy of the whole Matter of Britain. So, for example, the 16th-century humanist scholar Polydore Vergil famously rejected the claim that Arthur was the ruler of a post-Roman empire, found throughout the post-Galfridian medieval \\"chronicle tradition\\", to the horror of Welsh and English antiquarians.[93] Social changes associated with the end of the medieval period and the Renaissance also conspired to rob the character of Arthur and his associated legend of some of their power to enthrall audiences, with the result that 1634 saw the last printing of Malory's Le Morte d'Arthur for nearly 200 years.[94] King Arthur and the Arthurian legend were not entirely abandoned, but until the early 19th century the material was taken less seriously and was often used simply as a vehicle for allegories of 17th- and 18th-century politics.[95] Thus Richard Blackmore's epics Prince Arthur (1695) and King Arthur (1697) feature Arthur as an allegory for the struggles of William III against James II.[95] Similarly, the most popular Arthurian tale throughout this period seems to have been that of Tom Thumb, which was told first through chapbooks and later through the political plays of Henry Fielding; although the action is clearly set in Arthurian Britain, the treatment is humorous and Arthur appears as a primarily comedic version of his romance character.[96]\\r\\nJohn Dryden's masque King Arthur is still performed, largely thanks to Henry Purcell's music, though seldom unabridged.\\r\\nIn the early 19th century, medievalism, Romanticism, and the Gothic Revival reawakened interest in Arthur and the medieval romances. A new code of ethics for 19th-century gentlemen was shaped around the chivalric ideals embodied in the \\"Arthur of romance\\". This renewed interest first made itself felt in 1816, when Malory's Le Morte d'Arthur was reprinted for the first time since 1634.[98] Initially, the medieval Arthurian legends were of particular interest to poets, inspiring, for example, William Wordsworth to write \\"The Egyptian Maid\\" (1835), an allegory of the Holy Grail.[99] Pre-eminent among these was Alfred Tennyson, whose first Arthurian poem \\"The Lady of Shalott\\" was published in 1832.[100] Arthur himself played a minor role in some of these works, following in the medieval romance tradition. Tennyson's Arthurian work reached its peak of popularity with Idylls of the King, however, which reworked the entire narrative of Arthur's life for the Victorian era. It was first published in 1859 and sold 10,000 copies within the first week.[101] In the Idylls, Arthur became a symbol of ideal manhood who ultimately failed, through human weakness, to establish a perfect kingdom on earth.[102] Tennyson's works prompted a large number of imitators, generated considerable public interest in the legends of Arthur and the character himself, and brought Malory's tales to a wider audience.[103] Indeed, the first modernisation of Malory's great compilation of Arthur's tales was published in 1862, shortly after Idylls appeared, and there were six further editions and five competitors before the century ended.[104]\\r\\nThis interest in the 'Arthur of romance' and his associated stories continued through the 19th century and into the 20th, and influenced poets such as William Morris and Pre-Raphaelite artists including Edward Burne-Jones.[105] Even the humorous tale of Tom Thumb, which had been the primary manifestation of Arthur's legend in the 18th century, was rewritten after the publication of Idylls. While Tom maintained his small stature and remained a figure of comic relief, his story now included more elements from the medieval Arthurian romances and Arthur is treated more seriously and historically in these new versions.[106] The revived Arthurian romance also proved influential in the United States, with such books as Sidney Lanier's The Boy's King Arthur (1880) reaching wide audiences and providing inspiration for Mark Twain's satiric A Connecticut Yankee in King Arthur's Court (1889).[107] Although the 'Arthur of romance' was sometimes central to these new Arthurian works (as he was in Burne-Jones's \\"The Sleep of Arthur in Avalon\\", 1881-1898), on other occasions he reverted to his medieval status and is either marginalized or even missing entirely, with Wagner's Arthurian operas providing a notable instance of the latter.[108] Furthermore, the revival of interest in Arthur and the Arthurian tales did not continue unabated. By the end of the 19th century, it was confined mainly to Pre-Raphaelite imitators,[109] and it could not avoid being affected by World War I, which damaged the reputation of chivalry and thus interest in its medieval manifestations and Arthur as chivalric role model.[110] The romance tradition did, however, remain sufficiently powerful to persuade Thomas Hardy, Laurence Binyon and John Masefield to compose Arthurian plays,[111] and T. S. Eliot alludes to the Arthur myth (but not Arthur) in his poem The Waste Land, which mentions the Fisher King.[112]\\r\\nIn the latter half of the 20th century, the influence of the romance tradition of Arthur continued, through novels such as T. H. White's The Once and Future King (1958) and Marion Zimmer Bradley's The Mists of Avalon (1982) in addition to comic strips such as Prince Valiant (from 1937 onward).[114] Tennyson had reworked the romance tales of Arthur to suit and comment upon the issues of his day, and the same is often the case with modern treatments too. Bradley's tale, for example, takes a feminist approach to Arthur and his legend, in contrast to the narratives of Arthur found in medieval materials,[115] and American authors often rework the story of Arthur to be more consistent with values such as equality and democracy.[116] The romance Arthur has become popular in film and theatre as well. T. H. White's novel was adapted into the Lerner and Loewe stage musical Camelot (1960) and the Disney animated film The Sword in the Stone (1963); Camelot, with its focus on the love of Lancelot and Guinevere and the cuckolding of Arthur, was itself made into a film of the same name in 1967. The romance tradition of Arthur is particularly evident and, according to critics, successfully handled in Robert Bresson's Lancelot du Lac (1974), Eric Rohmer's Perceval le Gallois (1978) and perhaps John Boorman's fantasy film Excalibur (1981); it is also the main source of the material used in the Arthurian spoof Monty Python and the Holy Grail (1975).[117]\\r\\nRe-tellings and re-imaginings of the romance tradition are not the only important aspect of the modern legend of King Arthur. Attempts to portray Arthur as a genuine historical figure of c.?500, stripping away the \\"romance\\", have also emerged. As Taylor and Brewer have noted, this return to the medieval \\"chronicle tradition\\"' of Geoffrey of Monmouth and the Historia Brittonum is a recent trend which became dominant in Arthurian literature in the years following the outbreak of the Second World War, when Arthur's legendary resistance to Germanic invaders struck a chord in Britain.[118] Clemence Dane's series of radio plays, The Saviours (1942), used a historical Arthur to embody the spirit of heroic resistance against desperate odds, and Robert Sherriff's play The Long Sunset (1955) saw Arthur rallying Romano-British resistance against the Germanic invaders.[119] This trend towards placing Arthur in a historical setting is also apparent in historical and fantasy novels published during this period.[120] In recent years the portrayal of Arthur as a real hero of the 5th century has also made its way into film versions of the Arthurian legend, most notably the TV series' Arthur of the Britons (1972ÿ73), Merlin (2008-12), The Legend of King Arthur (1979), and Camelot (2011) [121] and the feature films King Arthur (2004), The Last Legion (2007) and King Arthur: Legend of the Sword (2017).[122]\\r\\nArthur has also been used as a model for modern-day behaviour. In the 1930s, the Order of the Fellowship of the Knights of the Round Table was formed in Britain to promote Christian ideals and Arthurian notions of medieval chivalry.[123] In the United States, hundreds of thousands of boys and girls joined Arthurian youth groups, such as the Knights of King Arthur, in which Arthur and his legends were promoted as wholesome exemplars.[124] However, Arthur's diffusion within contemporary culture goes beyond such obviously Arthurian endeavours, with Arthurian names being regularly attached to objects, buildings, and places. As Norris J. Lacy has observed, \\"The popular notion of Arthur appears to be limited, not surprisingly, to a few motifs and names, but there can be no doubt of the extent to which a legend born many centuries ago is profoundly embedded in modern culture at every level.\\"[125]","input":"Who wrote the first book about king arthur?"},{"output":"California","context":"The almond (Prunus dulcis, syn. Prunus amygdalus) is a species of tree native to the Middle East, the Indian subcontinent and North Africa.\\r\\nAlmond is also the name of the edible and widely cultivated seed of this tree. Within the genus Prunus, it is classified with the peach in the subgenus Amygdalus, distinguished from the other subgenera by corrugations on the shell (endocarp) surrounding the seed.\\r\\nThe fruit of the almond is a drupe, consisting of an outer hull and a hard shell with the seed, which is not a true nut, inside. Shelling almonds refers to removing the shell to reveal the seed. Almonds are sold shelled or unshelled. Blanched almonds are shelled almonds that have been treated with hot water to soften the seedcoat, which is then removed to reveal the white embryo.\\r\\n\\r\\n\\r\\nThe almond is a deciduous tree, growing 4ÿ10?m (13ÿ33?ft) in height, with a trunk of up to 30?cm (12?in) in diameter. The young twigs are green at first, becoming purplish where exposed to sunlight, then grey in their second year. The leaves are 3ÿ5?inches long,[3] with a serrated margin and a 2.5?cm (1?in) petiole. The flowers are white to pale pink, 3ÿ5?cm (1ÿ2?in) diameter with five petals, produced singly or in pairs and appearing before the leaves in early spring.[4][5] Almond grows best in Mediterranean climates with warm, dry summers and mild, wet winters. The optimal temperature for their growth is between 15 and 30?C (59 and 86?F) and the tree buds have a chilling requirement of 300 to 600 hours below 7.2?C (45.0?F) to break dormancy.[6]\\r\\nAlmonds begin bearing an economic crop in the third year after planting. Trees reach full bearing five to six years after planting. The fruit matures in the autumn, 7ÿ8 months after flowering.[5][7]\\r\\nThe almond fruit measures 3.5ÿ6?cm (1ÿ2?in) long. In botanical terms, it is not a nut but a drupe. The outer covering or exocarp, fleshy in other members of Prunus such as the plum and cherry, is instead a thick, leathery, grey-green coat (with a downy exterior), called the hull. Inside the hull is a reticulated, hard, woody shell (like the outside of a peach pit) called the endocarp. Inside the shell is the edible seed, commonly called a nut. Generally, one seed is present, but occasionally two occur.\\r\\nGreen almonds\\r\\nAlmond in shell and shelled\\r\\nBlanched almonds\\r\\nShelled Almond\\r\\nAlmond confections\\r\\nThe almond is native to the Mediterranean climate region of the Middle East, eastward as far as the Yamuna River in India.[9] It was spread by humans in ancient times along the shores of the Mediterranean into northern Africa and southern Europe, and more recently transported to other parts of the world, notably California, United States.[9]\\r\\nThe wild form of domesticated almond grows in parts of the Levant. The fruit of the wild forms contains the glycoside amygdalin, \\"which becomes transformed into deadly prussic acid (hydrogen cyanide) after crushing, chewing, or any other injury to the seed.\\"[10]\\r\\nSelection of the sweet type from the many bitter types in the wild marked the beginning of almond domestication.[11] It is unclear as to which wild ancestor of the almond created the domesticated species. Ladizinsky suggests the taxon Amygdalus fenzliana (Fritsch) Lipsky is the most likely wild ancestor of the almond in part because it is native of Armenia and western Azerbaijan where it was apparently domesticated.\\r\\nWhile wild almond species are toxic, domesticated almonds are not; Jared Diamond argues that a common genetic mutation causes an absence of amygdalin, and this mutant was grown by early farmers, \\"at first unintentionally in the garbage heaps, and later intentionally in their orchards\\".[12]\\r\\nZohary and Hopf believe that almonds were one of the earliest domesticated fruit trees due to \\"the ability of the grower to raise attractive almonds from seed. Thus, in spite of the fact that this plant does not lend itself to propagation from suckers or from cuttings, it could have been domesticated even before the introduction of grafting\\".[10]\\r\\nDomesticated almonds appear in the Early Bronze Age (3000ÿ2000 BC) such as the archaeological sites of Numeria (Jordan),[11] or possibly a little earlier. Another well-known archaeological example of the almond is the fruit found in Tutankhamun's tomb in Egypt (c. 1325 BC), probably imported from the Levant.[10]\\r\\nOf the European countries that the Royal Botanic Garden Edinburgh reported as cultivating almonds, Germany[13] is the northernmost, though the domesticated form can be found as far north as Iceland.[14]\\r\\nThe word \\"almond\\" comes from Old French almande or alemande, Late Latin *amandula, derived through a form amygdala from the Greek ?Ѵ? (amygdl) (cf. amygdala), an almond.[15] The al- in English, for the a- used in other languages may be due a confusion with the Arabic article al, the word having first dropped the a- as in the Italian form mandorla; the British pronunciation ah-mond and the modern Catalan ametlla and modern French amande show a form of the word closer to the original. Other related names of almond include mandel or knackmandel (German), mandorlo (Italian for the tree), mandorla (Italian for the fruit), amndoa (Portuguese), and almendra (Spanish).[16]\\r\\nThe adjective \\"amygdaloid\\" (literally \\"like an almond\\") is used to describe objects which are roughly almond-shaped, particularly a shape which is part way between a triangle and an ellipse. See, for example, the brain structure amygdala, which uses a direct borrowing of the Greek term amygdal.[17]\\r\\nThe pollination of California's almonds is the largest annual managed pollination event in the world, with close to one million hives (nearly half of all beehives in the US) being trucked in February to the almond groves. Much of the pollination is managed by pollination brokers, who contract with migratory beekeepers from at least 49 states for the event. This business has been heavily affected by colony collapse disorder, causing nationwide shortages of honey bees and increasing the price of insect pollination. To partially protect almond growers from the rising cost of insect pollination, researchers at the Agricultural Research Service (ARS) have developed a new line of self-pollinating almond trees.[18] Self-pollinating almond trees, such as the 'Tuono', have been around for a while, but their harvest is not as desirable as the insect-pollinated California 'Nonpareil' almond tree. The 'Nonpareil' tree produces large, smooth almonds and offers 60ÿ65% edible kernel per nut. The Tuono has thicker, hairier shells and offers only 32% of edible kernel per nut, but having a thick shell has advantages. The Tuono's shell protects the nut from threatening pests such as the navel orangeworm. ARS researchers have managed to crossbreed the pest-resistant Tuono tree with the 'Nonpareil, resulting in hybridized cultivars of almond trees that are self-pollinated and maintain a high nut quality.[19] The new, self-pollinating hybrids possess quality skin color, flavor, and oil content, and reduce almond growers' dependency on insect pollination.[18]\\r\\nA grove of almond trees in central California\\r\\nAlmond blossoms in Iran\\r\\nAn almond shaker before and during a harvest of a tree\\r\\nAlmond trees can be attacked by an array of damaging organisms, including insects, fungal pathogens, plant viruses, and bacteria.[20]\\r\\nIn 2014, world production of almonds was 2.7?million tonnes, with United States providing 57% of the total.[21] As the second and third leading producers, Spain and Australia combined contributed 13% of the world total (table).\\r\\nIn the United States, production is concentrated in California where 1,000,000 acres (400,000?ha) and six different almond varieties were under cultivation in 2017, with a yield of 2.25 billion lbs (1 billion kg) of shelled almonds.[22] The value of total US exports of shelled almonds in 2016 was $3.2 billion.[23]\\r\\nSpain has diverse commercial cultivars of almonds grown in Catalonia, Valencia, Murcia, Andalusia, and Arag܇n regions, and the Balearic Islands.[24][25]\\r\\nAustralia is the largest almond production region in the Southern Hemisphere. Most of the almond orchards are located along the Murray River corridor in New South Wales, Victoria, and South Australia.[26][27]\\r\\nThe seeds of Prunus dulcis var. dulcis are predominantly sweet[28][29] but some individual trees produce seeds that are somewhat more bitter. The genetic basis for bitterness involves a single gene, the bitter flavor furthermore being recessive,[30][31] both aspects making this trait easier to domesticate. The fruits from Prunus dulcis var. amara are always bitter, as are the kernels from other species of genus Prunus, such as peach and cherry (although to a lesser extent).\\r\\nThe bitter almond is slightly broader and shorter than the sweet almond and contains about 50% of the fixed oil that occurs in sweet almonds. It also contains the enzyme emulsin which, in the presence of water, acts on soluble glucosides, amygdalin, and prunasin,[32] yielding glucose, cyanide and the essential oil of bitter almonds, which is nearly pure benzaldehyde, the chemical causing the bitter flavor. Bitter almonds may yield 4ÿ9?mg of hydrogen cyanide per almond[33] and contain 42 times higher amounts of cyanide than the trace levels found in sweet almonds.[34] The origin of cyanide content in bitter almonds is via the enzymatic hydrolysis of amygdalin.[34]\\r\\nExtract of bitter almond was once used medicinally but even in small doses, effects are severe or lethal, especially in children; the cyanide must be removed before consumption.[34] The acute oral lethal dose of cyanide for adult humans is reported to be 0.5ÿ3.5?mg/kg of body weight (approximately 50 bitter almonds), whereas for children, consuming 5ÿ10 bitter almonds may be fatal.[34]\\r\\nAll commercially grown almonds sold as food in the United States are sweet cultivars. The US Food and Drug Administration reported in 2010 that some fractions of imported sweet almonds were contaminated with bitter almonds. Eating such almonds could result in vertigo and other typical bitter almond (cyanide) poisoning effects.[35]\\r\\nWhile the almond is often eaten on its own, raw or toasted, it is also a component of various dishes. Almonds are available in many forms, such as whole, sliced (flaked, slivered), and as flour. Almonds yield almond oil and can also be made into almond butter or almond milk. These products can be used in both sweet and savoury dishes.\\r\\nAlong with other nuts, almonds can be sprinkled over breakfasts and desserts, particularly muesli or ice cream-based dishes. Almonds are used in marzipan, nougat, many pastries (including jesuites), cookies (including French macarons, macaroons), and cakes (including financiers), noghl, and other sweets and desserts. They are also used to make almond butter, a spread similar to peanut butter, popular with peanut allergy sufferers and for its naturally sweeter taste. The young, developing fruit of the almond tree can be eaten whole (green almonds) when they are still green and fleshy on the outside and the inner shell has not yet hardened. The fruit is somewhat sour, but is a popular snack in parts of the Middle East, eaten dipped in salt to balance the sour taste. Also in the Middle East they are often eaten with dates. They are available only from mid-April to mid-June in the Northern Hemisphere; pickling or brining extends the fruit's shelf life.\\r\\nAlmond cookies, Chinese almond biscuits, and Italian ricciarelli are made with almonds.\\r\\nThe 'Marcona' almond cultivar is recognizably different from other almonds, and is marketed by name.[38] The kernel is short, round, relatively sweet, and delicate in texture. It has been grown in Spain for a long time and its origin is unknown; the tree is very productive, and the shell of the nut is very hard.[38] 'Marcona' almonds are traditionally served after being lightly fried in oil, and are used by Spanish confectioners to prepare a sweet called turr܇n.\\r\\nCertain natural food stores sell \\"bitter almonds\\" or \\"apricot kernels\\" labeled as such, requiring significant caution by consumers for how to prepare and eat these products.[39]\\r\\nAlmonds can be processed into a milk substitute called almond milk; the nut's soft texture, mild flavor, and light coloring (when skinned) make for an efficient analog to dairy, and a soy-free choice for lactose intolerant people and vegans. Raw, blanched, and lightly toasted almonds work well for different production techniques, some of which are similar to that of soymilk and some of which use no heat, resulting in \\"raw milk\\" (see raw foodism).\\r\\nAlmond flour or ground almond meal combined with sugar or honey as marzipan is often used as a gluten-free alternative to wheat flour in cooking and baking.[40]\\r\\nAlmonds contain polyphenols in their skins consisting of flavonols, flavan-3-ols, hydroxybenzoic acids and flavanones[41] analogous to those of certain fruits and vegetables. These phenolic compounds and almond skin prebiotic dietary fiber have commercial interest as food additives or dietary supplements.[41][42]\\r\\nHistorically, almond syrup was an emulsion of sweet and bitter almonds, usually made with barley syrup (orgeat syrup) or in a syrup of orange flower water and sugar, often flavored with a synthetic aroma of almonds.[34]\\r\\nDue to the cyanide found in bitter almonds, modern syrups generally are produced only from sweet almonds. Such syrup products do not contain significant levels of hydrocyanic acid, so are generally considered safe for human consumption.[34]\\r\\nThe almond is a nutritionally dense food (see chart pictured right) and a 100 gram amount is a rich source (>20% of the Daily value, DV) of the B vitamins riboflavin and niacin, vitamin E, and the essential minerals calcium, iron, magnesium, manganese, phosphorus, and zinc. The same amount is also a good source (10ÿ19% DV) of the B vitamins thiamine, vitamin B6, and folate; choline; and the essential mineral potassium. They are also rich in dietary fiber, monounsaturated fats, and polyunsaturated fats, fats which potentially may lower LDL cholesterol.[43] Typical of nuts and seeds, almonds also contain phytosterols such as beta-sitosterol, stigmasterol, campesterol, sitostanol, and campestanol, which have been associated with cholesterol-lowering properties.[43]\\r\\nPreliminary research associates consumption of almonds with elevated blood levels of high density lipoproteins and lower low density lipoproteins.[43]\\r\\nAlmonds may cause allergy or intolerance. Cross-reactivity is common with peach allergens (lipid transfer proteins) and tree nut allergens. Symptoms range from local signs and symptoms (e.g., oral allergy syndrome, contact urticaria) to systemic signs and symptoms including anaphylaxis (e.g., urticaria, angioedema, gastrointestinal and respiratory symptoms).[44]\\r\\nDuring the digestion process in humans, almond flour may be fermented into short-chain fatty acids, most notably butyrate which is a substrate for cells lining the large intestine.[45][46]\\r\\nAlmonds are a rich source of oil, with 50% of kernel dry mass as fat (table). Almond oil contains 32% monounsaturated oleic acid (an omega-9 fatty acid), 13% linoleic acid (a polyunsaturated omega-6 essential fatty acid), and 5% saturated fatty acid (USDA link in table). Linolenic acid, a polyunsaturated omega-3 fat, is not present (table). Almond oil is a rich source of vitamin E providing 261% of the Daily Value per 100 ml (table).\\r\\nOleum amygdalae, the fixed oil, is prepared from either sweet or bitter almonds, and is a glyceryl oleate with a slight odour and a nutty taste. It is almost insoluble in alcohol but readily soluble in chloroform or ether. Sweet almond oil is obtained from the dried kernel of sweet almonds.[47]\\r\\nThe oil is good for application to the skin as an emollient, and has been traditionally used by massage therapists to lubricate the skin during massage sessions.[48]\\r\\nAlmond oil can also be used as a wood conditioner for certain woodwind instruments such as: the oboe, the clarinet, and the saxophone.[49]\\r\\nAlmonds are susceptible to aflatoxin-producing molds.[50] Aflatoxins are potent carcinogenic chemicals produced by molds such as Aspergillus flavus and Aspergillus parasiticus. The mold contamination may occur from soil, previously infested almonds, and almond pests such as navel-orange worm. High levels of mold growth typically appear as gray to black filament like growth. It is unsafe to eat mold infected tree nuts.\\r\\nSome countries have strict limits on allowable levels of aflatoxin contamination of almonds and require adequate testing before the nuts can be marketed to their citizens. The European Union, for example, introduced a requirement since 2007 that all almond shipments to EU be tested for aflatoxin. If aflatoxin does not meet the strict safety regulations, the entire consignment may be reprocessed to eliminate the aflatoxin or it must be destroyed.[51][52]\\r\\nThe USDA approved a proposal by the Almond Board of California to pasteurize almonds sold to the public, after tracing cases of salmonellosis to almonds. The almond pasteurization program became mandatory for California companies in 2007.[53] Raw, untreated California almonds have not been available in the U.S. since then.\\r\\nCalifornia almonds labeled \\"raw\\" must be steam-pasteurized or chemically treated with propylene oxide (PPO). This does not apply to imported almonds[54] or almonds sold from the grower directly to the consumer in small quantities.[55] The treatment also is not required for raw almonds sold for export outside of North America.\\r\\nThe Almond Board of California states: PPO residue dissipates after treatment. The U.S. EPA has reported: Propylene oxide has been detected in fumigated food products; consumption of contaminated food is another possible route of exposure. PPO is classified by the EPA as a Group B2, probable human carcinogen.\\"[56]\\r\\nThe USDA-approved marketing order was challenged in court by organic farmers organized by the Cornucopia Institute, a Wisconsin-based farm policy research group. According to the Cornucopia Institute, this almond marketing order has imposed significant financial burdens on small-scale and organic growers and damaged domestic almond markets. A federal judge dismissed the lawsuit in the spring of 2009 on procedural grounds. In August 2010, a federal appeals court ruled that the farmers have a right to appeal the USDA regulation. In March 2013, the court vacated the suit on the basis that the objections should have been raised in 2007 when the regulation was first proposed.[57]\\r\\nThe almond is highly revered in some cultures. The tree originated in the Middle East,[58] and is mentioned numerous times in the Bible.\\r\\nIn the Hebrew Bible, the almond was a symbol of watchfulness and promise due to its early flowering. In the Bible the almond is mentioned ten times, beginning with Book of Genesis 43:11, where it is described as \\"among the best of fruits\\". In Numbers 17 Levi is chosen from the other tribes of Israel by Aaron's rod, which brought forth almond flowers. According to tradition, the rod of Aaron bore sweet almonds on one side and bitter on the other; if the Israelites followed the Lord, the sweet almonds would be ripe and edible, but if they were to forsake the path of the Lord, the bitter almonds would predominate. The almond blossom supplied a model for the menorah which stood in the Holy Temple, \\"Three cups, shaped like almond blossoms, were on one branch, with a knob and a flower; and three cups, shaped like almond blossoms, were on the other...on the candlestick itself were four cups, shaped like almond blossoms, with its knobs and flowers\\" (Exodus 25:33ÿ34; 37:19ÿ20).\\r\\nSimilarly, Christian symbolism often uses almond branches as a symbol of the Virgin Birth of Jesus; paintings and icons often include almond-shaped haloes encircling the Christ Child and as a symbol of Mary. The word \\"Luz\\", which appears in Genesis 30:37, sometimes translated as \\"hazel\\", may actually be derived from the Aramaic name for almond (Luz), and is translated as such in some Bible versions such as the NIV.[59] The Arabic name for almond is ??? \\"lauz\\" or \\"lz\\". In some parts of the Levant and North Africa it is pronounced \\"loz\\", which is very close to its Aramaic origin.\\r\\nLa entrada de la flor is an event celebrated on 1 February in Torrent, Spain, in which the clavarios and members of the Confrerie of the Mother of God deliver a branch of the first-blooming almond-tree to the Virgin.[60]","input":"Where are most almonds grown in the us?"},{"output":"May 28, 2008","context":"Executive:\\r\\nFederal Parliament:\\r\\nJudiciary:\\r\\nThe 1st Nepalese Constituent Assembly was a unicameral body of 601 members that served from May 28, 2008 to May 28, 2012. It was formed as a result of the first Constituent Assembly election held on April 10, 2008.[1] The Constituent Assembly was tasked with writing a new constitution,[2] and acting as the interim legislature for a term of two years.[3] 240 members were elected in single seat constituencies, 335 were elected through proportional representation,[4] and the remaining 26 seats were reserved for nominated members.[5]\\r\\nThe Communist Party of Nepal (Maoist) (CPN (M)) was the largest party in the Constituent Assembly, having won half of the constituency seats and about 30% of proportional representation seats.[6] The Constituent Assembly declared a republic at its first meeting on May 28, 2008, abolishing the monarchy.[7]\\r\\nIn late June 2008, the parties agreed to divide the 26 nominated seats in the Constituent Assembly between nine parties: the CPN (M) was to receive nine of these seats, while the Nepali Congress (NC) and the Communist Party of Nepal (Unified Marxist-Leninist) (CPN (UML)) (which respectively placed second and third in the election) would each receive five, the Madhesi Janadhikar Forum would receive two, and the Sadbhavana Party, the Nepal Workers and Peasants Party, Janamorcha Nepal, and the Communist Party of Nepal (Marxist-Leninist) would each receive one nominated seat.[5][8] Due to its failure in drafting a new constitution, the CA was dissolved on May 28, 2012 after its original and extended total tenure of 4 years.[9] The next Nepalese Constituent Assembly elections initially slated for November 22, 2012 [10] were held a year later on November 19, 2013 after being postponed several times.[11]\\r\\n\\r\\n\\r\\nThe official and final list of members elected under the PR system was released on 8 May 2008; this meant the first meeting of the CA (which has to be held within 21 days of the publication of the final result) would be held before the end of May 2008.[12] On 12 May 2008, it was announced that the first session of the CA would be held on 28 May 2008.[13] The members of the CA were sworn in on 27 May 2008.[14]\\r\\nThe composition of CA looked like this:\\r\\nAt the first session of the Constituent Assembly on 28 May, it voted to declare Nepal a federal democratic republic, thereby abolishing the monarchy. 564 members of the Constituent Assembly voted on this motion, with 560 in favor and four opposed.[7] Of all the parties represented in the Constituent Assembly, only the Rastriya Prajatantra Party Nepal (RPP-Nepal) opposed the motion.[15] Koirala said that Nepal was entering a new era and that \\"the nation's dream has come true\\", while celebrations took place in Kathmandu;[16] May 29 and May 30 were declared to be public holidays by the government.[17] The Constituent Assembly also decided that Gyanendra should leave the Narayanhity Palace within 15 days.[7]\\r\\nEarlier on 28 May, the major parties agreed on the creation of the position of President, while the Prime Minister was to hold executive powers;[7] however, they reached no agreement on exactly what powers the President should have or who should become President, and these deliberations led to a delay in the opening of the Constituent Assembly.[18]\\r\\nOn 29 May, the royal standard was removed from Narayanhity Palace and replaced with the national flag.[18] Gyanendra reportedly said on 2 June that he accepted the Constituent Assembly's decision.[19]\\r\\n13 parties, including the CPN (M), the NC, and the CPN (UML), met at the Ministry of Peace and Reconstruction on June 1; no agreement was reached regarding power arrangements. The CPN (M) pressed its demand for both the positions of President and Prime Minister, but the NC and CPN (UML) were unwilling to accept this. The NC wanted these positions to be chosen through a simple majority vote in the Constituent Assembly.[20]\\r\\nAddressing a rally in Gorkha district on June 1, Prachanda gave Koirala an ultimatum to present his resignation to the Constituent Assembly within two or three days, warning that if he failed to do so, the Maoist members of the government would resign and the party would lead street protests.[21]\\r\\nAfter Gyanendra requested that the government make arrangements for his residence on June 1, the government decided on June 4 to give another palace, the Nagarjuna Palace, to Gyanendra.[22] Also on June 4, Prachanda and Koirala met; at this meeting Prachanda demanded that the government step aside by June 5 and again threatened street protests.[23] On the same day, the three main parties held a meeting at which they again failed to reach an agreement, but the parties agreed on the need for a few more days and the CPN (M) postponed its deadline for the government to step aside to allow for this period.[24]\\r\\nOn June 5, the CPN (M) softened its position, deciding at a meeting of its Central Secretariat that it would not press its claim to the presidency and that it would instead favor having a member of civil society become President. The party expressed continued opposition to a proposal that would allow the Prime Minister to be dismissed by a simple majority vote of the Constituent Assembly.[25] Despite the Maoist desire to have a neutral figure as President, the NC proposed Koirala for the position.[26]\\r\\nThe Constituent Assembly held its second sitting on June 5; due to the three major parties' deadlock, this sitting was very brief, lasting less than a half hour, and took no major decisions.[27]\\r\\nOn June 11, Gyanendra gave a brief press conference at Narayanhiti, stating his acceptance of the republic and promising cooperation. He also said that he intended to stay in Nepal, asserted that he held no property outside of Nepal, and expressed his hope that he would be allowed to keep his property.[28] He left Narayanhiti on the same evening and went to his new residence at Nagarjuna.[29]\\r\\nShortly after another meeting between Prachanda and Koirala, the CPN (M) ministers announced their resignations and sent a joint resignation letter to Prachanda on June 12.[30] According to the CPN (M) Minister for Local Development, Dev Gurung, the purpose of the resignations was to \\"accelerate the process of formation of a new government and bring an end to the current transitional period\\". However, some considered the resignations to be a means of increasing pressure on Koirala. The resignations were not immediately submitted to Koirala by the CPN (M), and therefore were not made effective. Gurung said that he expected a coalition government to be formed by June 18.[31]\\r\\nAlso on June 12, CPN (UML) General Secretary Jhala Nath Khanal asserted that a member of his party should become President.[32] On June 14 he blamed the CPN (M) for the deadlock.[33] At a meeting with Communist Party of Nepal (Marxist-Leninist) General Secretary C. P. Mainali on June 14, Koirala stressed the importance of power-sharing according to the popular mandate and consensus. Mainali expressed the view that the Maoists should be allowed to lead the government, while the post of President should go to someone from the Nepali Congress and the post of Chairman of the Constituent Assembly should go to someone from the CPN (UML).[34] Meanwhile, Prachanda rejected the possibility of Koirala becoming President, saying that this would be a \\"dishonour to the people's mandate\\"; he also expressed concern that having Koirala as President could cause the development of a separate power center from the government, in addition to noting Koirala's advanced age and health problems. Prachanda said that the President should come from a smaller party rather than from the Nepali Congress or the CPN (UML).[35]\\r\\nKoirala said on June 15 that he would not \\"run around pleading\\" for the presidency.[36] Prachanda, meanwhile, said that he expected the new government to be formed imminently, calling on other parties to support this and warning that anyone contravening the people's mandate would have a \\"heavy price\\" to pay.[37] Another meeting of the three main parties on June 16 ended in continued disagreement on the key issues,[38] and the CPN (M) said that it would submit the resignations of its ministers to Koirala if the next meeting on June 17 did not produce an agreement.[39]\\r\\nCPN (UML) General Secretary Khanal said on June 17 that it was important for the left-wing parties to work together. While saying that the CPN (UML) and the CPN (M) would cooperate in the future, he noted that it would be necessary for the parties to improve their difficult relationship.[40] The CPN (M) Central Secretariat met on the same day and approved the decision to hold firm on the key issues and for its ministers to resign if an agreement was not reached later in the day. The party chose to support Ramraja Prasad Singh for the presidency.[41] Upendra Yadav, the Coordinator of the Madhesi Janadhikar Forum, also said on June 17 that his party would not participate in the government and would instead be an opposition party, and he stressed the importance of cooperation among the Madhesi parties. Although he criticized the three main parties for focusing on their power struggle, he endorsed the Maoist claim to lead the government, while asserting that some of the key portfolios should be given to other parties.[42]\\r\\nThe Constituent Assembly went into indefinite recess on June 18. The three main parties continued their discussions on that day, but did not reach an agreement. However, CPN (M) spokesman Krishna Bahadur Mahara said that they were getting closer to an agreement, and he said that the party had postponed its deadline to June 19.[43]\\r\\nOn June 19, the three main parties reached an agreement providing for a constitutional amendment that would enable a government to be formed or dismissed by a simple majority vote of the Constituent Assembly, rather than the previously required two-thirds majority vote. An agreement was also concluded on the issue of integrating Maoist fighters into the national army. However, the parties did not yet agree on a way to resolve the question of power-sharing.[44] Later that day, the Seven-Party Alliance held a meeting at which Koirala said that he was prepared to resign at any time.[45]\\r\\nThe CPN (M) and the CPN (UML) leadership met early on June 20, and afterwards Khanal said that the CPN (M) had \\"responded positively\\" to the CPN (UML)'s proposal to have someone from the CPN (UML) as President.[46] A leading member of the CPN (UML) said that the two parties had agreed on the candidacy of Madhav Kumar Nepal, the former General Secretary of the CPN (UML). However, a leading member of the CPN (M) disputed this, saying that the two parties were closer to an agreement but that their party had not agreed to support a CPN (UML) candidate; he said that both Nepal and Sahana Pradhan (whose name was proposed by the CPN (M)) had been discussed as candidates.[47]\\r\\nThe Nepal Federation of Indigenous Nationalities (NFIN) met with Koirala on June 20, seeking a recommendation that indigenous peoples not already represented in the Constituent Assembly be included in it through the 26 nominated seats. Koirala, who was supportive of the NFIN's request, also sharply criticized his rivals on this occasion, saying that they were practicing petty politics and were not respecting the people's mandate to work on a consensus basis.[48] The Maoist ministers submitted their joint resignation at a meeting of the seven parties on June 20.[49]\\r\\nSher Bahadur Deuba of the Nepali Congress said on June 24 that the CPN (M) was responsible for the deadlock and claimed that it was working to divide the Seven-Party Alliance. He also said that Koirala would resign after the election of a President and that the CPN (M) had no authority to demand his resignation before then.[50] Also on June 24, the seven parties agreed on the introduction of a constitutional amendment providing for the election of a President and the formation of a government through simple majority votes. There was, however, disagreement over the Nepali Congress proposal to include a member of the opposition on the National Security Council; the CPN (M) and the CPN (UML) described this as undemocratic. There was also a proposal to include members of each of the seven parties on the National Security Council.[51] Despite the failure to agree about the opposition's inclusion on the National Security Council, it was agreed to give the opposition a place on the Constitutional Council. The parties also reached agreement on a number of issues related to peace, disarmament and reintegration.[52] A decision was also reached with to divide the 26 nominated seats in the Constituent Assembly among nine parties: the CPN (M) was to receive nine of these seats, while the Nepali Congress and the CPN (UML) would each receive five, the Madhesi Janadhikar Forum would receive two, and the Sadbhavana Party, the Nepal Workers and Peasants Party, Janamorcha Nepal, and the Communist Party of Nepal-Marxist Leninist would each receive one nominated seat.[5]\\r\\nThe Council of Ministers approved the constitutional amendment late on June 25.[5] At a meeting of the Constituent Assembly on June 26, Koirala announced his resignation, although it will not be finalized until after the election of a President, to whom the resignation must be submitted.[53] Although it was expected that the constitutional amendment would be approved at the same meeting, it was not introduced after Madhesi members of the Constituent Assembly demanded that the amendment be expanded to incorporate a March 2008 agreement between the Madhesis and the government that provided for Madhesi autonomy, among other things. As a result of this, the Constituent Assembly meeting was suspended until June 28.[54] After meeting with Koirala on June 27, Hridayesh Tripathy of the Terai Madhes Loktantrik Party (TMLP) said that Koirala was in favor of incorporating the Madhesi agreement into the amendment and that he asked the Madhesis not to disrupt the Constituent Assembly again.[55]\\r\\nOn June 28, the seven parties met to discuss the Madhesi demands; although no decision was reached, all of the parties opposed the Madhesi demand for a single province. The Constituent Assembly met later that day and was again disrupted by representatives of the Madhesi parties, forcing the cancellation of the meeting after only a few minutes.[56] The next Constituent Assembly meeting on June 29 was also disrupted by the Madhesis and was cancelled.[57] Jaya Prakash Gupta, a leading figure in the Madhesi Janadhikar Forum (MJF), also warned on June 29 that the Madhesi parties would \\"not only obstruct the Constituent Assembly but also paralyse the entire nation to force [the seven parties] to meet our demands.\\"[58] Prachanda, in an interview on June 30, expressed frustration with the Madhesi parties' disruption, which occurred just after Koirala's resignation, when it appeared the road to forming a new government was clear. He said that he favored Madhesi autonomy, but opposed their demand for all of Terai to become one Madhesi province.[59] Khanal, the CPN (UML) General Secretary, flatly rejected the demand for a single Madhesi province, condemning it as \\"a game to disintegrate the nation\\". He said that the demand ignored the wishes of other ethnic groups in the Terai.[60]\\r\\nThe Constituent Assembly met on June 30 but was again disrupted by the Madhesis and the meeting was cancelled.[61] The three main parties reached an agreement with the three Madhesi parties, the MJF, the TMLP, and the Nepal Sadbhavana Party, on July 1, providing for a supplementary amendment bill that would meet the Madhesi demands.[62] Another meeting of the Constituent Assembly was disrupted by the Madhesis and aborted on July 2, while the CPN (M), Nepali Congress, and CPN (UML) met to decide the draft text of the supplementary amendment bill.[63]\\r\\nThe CPN (M) and MJF met on July 3, and the CPN (M) agreed to include a reference to Madhesi autonomy in the bill, while also saying that it wanted the bill to mention other indigenous groups' desire for autonomy.[64] 13 small parties in the Constituent Assembly said on July 3 that they were completely opposed to the Madhesi demand for a single autonomous province, and they criticized the larger parties for the political deadlock that prevented discussion of the issues from taking place in the Constituent Assembly.[65]\\r\\nSessions of the Constituent Assembly were attempted on July 3 and July 4, but both were immediately disrupted by the Madhesi members and were aborted. On the latter occasion, Kul Bahadur Gurung, who chaired the session, urged the Madhesi members to respect the right of other members to be heard, but they ignored him.[66] On July 4, the CPN (M), Nepali Congress, and CPN (UML) agreed on a draft supplementary amendment bill intended to satisfy the Madhesi demands. The draft requires the State Restructuring Commission to consider the March 2008 agreement between the government and the Madhesis when drawing up Nepal's federal structure. A meeting of the Seven Party Alliance followed the three-party agreement, and at this meeting, the People's Front Nepal, United Left Front, and Nepal Workers and Peasants Party objected to the draft, saying that it would endanger national unity.[67] The cabinet approved the bill late on July 4; at the same time, it decided to nominate the 26 remaining members of the Constituent Assembly, dividing the seats between nine parties in accordance with the parties' earlier agreement and the lists of names they presented.[68]\\r\\nThe Madhesi parties quickly deemed the supplementary amendment bill to be an unacceptable \\"betrayal\\".[69] Khanal, the CPN (UML) General Secretary, said that the bill should satisfy the Madhesi demands, and he warned that opposition to the bill would not be in Madhesi interests or in the interests of any of the peoples of Terai. He called on the Madhesi members to make proposals and engage in discussion in the Constituent Assembly instead of disrupting it.[70]\\r\\nOn July 6, at a meeting between the three main parties and the Madhesi parties, the former agreed to formulate a new bill to replace the one agreed upon two days prior, while the latter agreed to stop disrupting the Constituent Assembly.[71] 23 of the 26 nominated members of the Constituent Assembly were sworn in on July 7; the remaining three were unable to attend the swearing in ceremony.[72] On July 8, the Seven Party Alliance, with the exception of the Nepal Workers and Peasants Party, agreed on the content of a new draft bill, according to which federal structures would be created in line with the wishes of the Madhesis and other ethnic groups.[73]\\r\\nThe Constituent Assembly was able to meet and function on July 9, for the first time since the Madhesis began pressing their demands on June 26.[74] Although they did not disrupt the Constituent Assembly on this occasion, the three Madhesi parties furiously condemned the proposed bill and vowed that their struggle would continue.[75] During the Constituent Assembly meeting, they submitted a protest notice, and when this was rejected, they chose to boycott the Constituent Assembly's proceedings. Narendra Bikram Nemwang, the Minister for Law, Justice and Parliamentary Affairs, tabled the bill regardless.[76]\\r\\nKoirala said on July 11 that forming a government was the responsibility of the CPN (M).[77] However, the CPN (M) criticized the Nepali Congress on July 12 for \\"obstructing the process [of forming a government] for the past three months\\".[78] To protest the amendment bill, the Madhesi parties boycotted the Constituent Assembly meeting held on July 13, at which the amendment bill was considered.[79] The bill was passed on the same day; 442 members of the Constituent Assembly voted for the amendment and seven voted against it. It thus became the interim constitution's Fifth Amendment. The amendment allows for the formation of a government based on a Constituent Assembly majority; it also allows the President, Vice-President, Chairman of the Constituent Assembly, and the Deputy Chairman of the Constituent Assembly to be elected by majority vote if there is no consensus. In addition, the amendment provides for the Leader of the Opposition to become a member of the Constitutional Council; however, the Constituent Assembly rejected a proposal from the cabinet that the Leader of the Opposition be included on the National Defense Council.[80]\\r\\nIn a meeting with the Nepali Congress on July 14, the CPN (M) urged it to participate in the new government. A Nepali Congress leader replied that the party had still not decided whether to participate.[81]\\r\\nAn indirect presidential election was held in Nepal on 19 July 2008 with a presidential run-off on 21 July. The Nepalese Constituent Assembly (CA) elected in April 2008 elected a new President and Vice-President after the Fifth Amendment to the Interim Constitution was passed on July 14.[82] This would be the first President to be elected after Nepal became a republic a few months earlier.\\r\\nIn the newly passed amendment, the majority party will form the government, the CA will elect the new President on the basis of majority and a new provision that the Opposition Leader will be a member of the Constitutional Council.[83] The leading political parties, Nepali Congress, Communist Party of Nepal (Maoist) and Communist Party of Nepal (United Marxist-Leninist) engaged in discussions regarding who would be the new President. The Nepali Congress wanted Prime Minister and interim Head of State Girija Prasad Koirala while the Communist Party of Nepal (United Marxist-Leninist) wanted its former Secretary-General Madhav Kumar Nepal as President. However, the Communist Party of Nepal (Maoist) wants an independent figure as President rather than party figures such as Koirala or Nepal. The Maoists won the most seats in the CA however needs to form a coalition government with the other parties.[84]","input":"When did the constituent assembly hold its first session?"}]`),M={name:"App",components:{PoemCard:C},data(){return{visibleCount:20,poemsData:I}},computed:{visiblePoems(){return this.poemsData.slice(0,this.visibleCount)},hasMorePoems(){return this.visibleCount<this.poemsData.length}},methods:{loadMore(){this.visibleCount+=20}}},B={class:"card-container"};function x(h,t,n,c,u,i){const m=p("PoemCard");return a(),o(l,null,[t[1]||(t[1]=e("section",null,[e("div",{class:"top-Banner"},[e("div",{class:"top-Banner-Title"},[e("div",{class:"top-Banner-Title-Text"},"🎉Q&A Life🥳")])])],-1)),e("section",null,[e("div",B,[(a(!0),o(l,null,g(i.visiblePoems,(r,f)=>(a(),y(m,{key:f,poem:r},null,8,["poem"]))),128))]),i.hasMorePoems?(a(),o("button",{key:0,class:"load-more-button",onClick:t[0]||(t[0]=(...r)=>i.loadMore&&i.loadMore(...r))},"See more")):w("",!0)])],64)}const P=d(M,[["render",x]]),N=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"qapage/30.md","filePath":"qapage/30.md"}'),D={name:"qapage/30.md"},E=Object.assign(D,{setup(h){return(t,n)=>(a(),o("div",null,[b(P)]))}});export{N as __pageData,E as default};
